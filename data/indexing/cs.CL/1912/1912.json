[{"id": "1912.00113", "submitter": "Xuewen Shi", "authors": "Xuewen Shi, Heyan Huang, Shuyang Zhao, Ping Jian, Yi-Kun Tang", "title": "Tag Recommendation by Word-Level Tag Sequence Modeling", "comments": "This is a full length version of the paper in DASFAA 2019", "journal-ref": null, "doi": "10.1007/978-3-030-18590-9_58", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we transform tag recommendation into a word-based text\ngeneration problem and introduce a sequence-to-sequence model. The model\ninherits the advantages of LSTM-based encoder for sequential modeling and\nattention-based decoder with local positional encodings for learning relations\nglobally. Experimental results on Zhihu datasets illustrate the proposed model\noutperforms other state-of-the-art text classification based methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 02:12:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Shi", "Xuewen", ""], ["Huang", "Heyan", ""], ["Zhao", "Shuyang", ""], ["Jian", "Ping", ""], ["Tang", "Yi-Kun", ""]]}, {"id": "1912.00124", "submitter": "Jihyeon Lee", "authors": "Jihyeon Lee, Sho Arora", "title": "A Free Lunch in Generating Datasets: Building a VQG and VQA System with\n  Attention and Humans in the Loop", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their importance in training artificial intelligence systems, large\ndatasets remain challenging to acquire. For example, the ImageNet dataset\nrequired fourteen million labels of basic human knowledge, such as whether an\nimage contains a chair. Unfortunately, this knowledge is so simple that it is\ntedious for human annotators but also tacit enough such that they are\nnecessary. However, human collaborative efforts for tasks like labeling massive\namounts of data are costly, inconsistent, and prone to failure, and this method\ndoes not resolve the issue of the resulting dataset being static in nature.\nWhat if we asked people questions they want to answer and collected their\nresponses as data? This would mean we could gather data at a much lower cost,\nand expanding a dataset would simply become a matter of asking more questions.\nWe focus on the task of Visual Question Answering (VQA) and propose a system\nthat uses Visual Question Generation (VQG) to produce questions, asks them to\nsocial media users, and collects their responses. We present two models that\ncan then parse clean answers from the noisy human responses significantly\nbetter than our baselines, with the goal of eventually incorporating the\nanswers into a Visual Question Answering (VQA) dataset. By demonstrating how\nour system can collect large amounts of data at little to no cost, we envision\nsimilar systems being used to improve performance on other tasks in the future.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 03:45:17 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 17:52:03 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Lee", "Jihyeon", ""], ["Arora", "Sho", ""]]}, {"id": "1912.00127", "submitter": "Chowdhury Rahman", "authors": "Md. Hasibur Rahman, Chowdhury Rafeed Rahman, Ruhul Amin, Md. Habibur\n  Rahman Sifat and Afra Anika", "title": "A Hybrid Approach Towards Two Stage Bengali Question Classification\n  Utilizing Smart Data Balancing Technique", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-52856-0_36", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question classification (QC) is the primary step of the Question Answering\n(QA) system. Question Classification (QC) system classifies the questions in\nparticular classes so that Question Answering (QA) System can provide correct\nanswers for the questions. Our system categorizes the factoid type questions\nasked in natural language after extracting features of the questions. We\npresent a two stage QC system for Bengali. It utilizes one dimensional\nconvolutional neural network for classifying questions into coarse classes in\nthe first stage. Word2vec representation of existing words of the question\ncorpus have been constructed and used for assisting 1D CNN. A smart data\nbalancing technique has been employed for giving data hungry convolutional\nneural network the advantage of a greater number of effective samples to learn\nfrom. For each coarse class, a separate Stochastic Gradient Descent (SGD) based\nclassifier has been used in order to differentiate among the finer classes\nwithin that coarse class. TF-IDF representation of each word has been used as\nfeature for the SGD classifiers implemented as part of second stage\nclassification. Experiments show the effectiveness of our proposed method for\nBengali question classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 04:00:31 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 02:15:32 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 03:53:55 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rahman", "Md. Hasibur", ""], ["Rahman", "Chowdhury Rafeed", ""], ["Amin", "Ruhul", ""], ["Sifat", "Md. Habibur Rahman", ""], ["Anika", "Afra", ""]]}, {"id": "1912.00147", "submitter": "Bin He", "authors": "Bin He, Di Zhou, Jinghui Xiao, Xin jiang, Qun Liu, Nicholas Jing Yuan,\n  Tong Xu", "title": "Integrating Graph Contextualized Knowledge into Pre-trained Language\n  Models", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex node interactions are common in knowledge graphs, and these\ninteractions also contain rich knowledge information. However, traditional\nmethods usually treat a triple as a training unit during the knowledge\nrepresentation learning (KRL) procedure, neglecting contextualized information\nof the nodes in knowledge graphs (KGs). We generalize the modeling object to a\nvery general form, which theoretically supports any subgraph extracted from the\nknowledge graph, and these subgraphs are fed into a novel transformer-based\nmodel to learn the knowledge embeddings. To broaden usage scenarios of\nknowledge, pre-trained language models are utilized to build a model that\nincorporates the learned knowledge representations. Experimental results\ndemonstrate that our model achieves the state-of-the-art performance on several\nmedical NLP tasks, and improvement above TransE indicates that our KRL method\ncaptures the graph contextualized information effectively.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 07:13:25 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 06:36:36 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["He", "Bin", ""], ["Zhou", "Di", ""], ["Xiao", "Jinghui", ""], ["jiang", "Xin", ""], ["Liu", "Qun", ""], ["Yuan", "Nicholas Jing", ""], ["Xu", "Tong", ""]]}, {"id": "1912.00159", "submitter": "Lucy Linder", "authors": "Lucy Linder, Michael Jungo, Jean Hennebert, Claudiu Musat, Andreas\n  Fischer", "title": "Automatic Creation of Text Corpora for Low-Resource Languages from the\n  Internet: The Case of Swiss German", "comments": null, "journal-ref": "Proceedings of The 12th Language Resources and Evaluation\n  Conference, LREC (2020) 2706-2711", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents SwissCrawl, the largest Swiss German text corpus to date.\nComposed of more than half a million sentences, it was generated using a\ncustomized web scraping tool that could be applied to other low-resource\nlanguages as well. The approach demonstrates how freely available web pages can\nbe used to construct comprehensive text corpora, which are of fundamental\nimportance for natural language processing. In an experimental evaluation, we\nshow that using the new corpus leads to significant improvements for the task\nof language modeling. To capture new content, our approach will run\ncontinuously to keep increasing the corpus over time.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 08:42:25 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 18:18:42 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 14:52:55 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Linder", "Lucy", ""], ["Jungo", "Michael", ""], ["Hennebert", "Jean", ""], ["Musat", "Claudiu", ""], ["Fischer", "Andreas", ""]]}, {"id": "1912.00178", "submitter": "Yang Feng", "authors": "Yang Feng, Wanying Xie, Shuhao Gu, Chenze Shao, Wen Zhang, Zhengxin\n  Yang, Dong Yu", "title": "Modeling Fluency and Faithfulness for Diverse Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation models usually adopt the teacher forcing strategy\nfor training which requires the predicted sequence matches ground truth word by\nword and forces the probability of each prediction to approach a 0-1\ndistribution. However, the strategy casts all the portion of the distribution\nto the ground truth word and ignores other words in the target vocabulary even\nwhen the ground truth word cannot dominate the distribution. To address the\nproblem of teacher forcing, we propose a method to introduce an evaluation\nmodule to guide the distribution of the prediction. The evaluation module\naccesses each prediction from the perspectives of fluency and faithfulness to\nencourage the model to generate the word which has a fluent connection with its\npast and future translation and meanwhile tends to form a translation\nequivalent in meaning to the source. The experiments on multiple translation\ntasks show that our method can achieve significant improvements over strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 10:30:46 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Feng", "Yang", ""], ["Xie", "Wanying", ""], ["Gu", "Shuhao", ""], ["Shao", "Chenze", ""], ["Zhang", "Wen", ""], ["Yang", "Zhengxin", ""], ["Yu", "Dong", ""]]}, {"id": "1912.00239", "submitter": "Charlotte Rochereau", "authors": "Charlotte Rochereau, Beno\\^it Sagot, Emmanuel Dupoux", "title": "Neural language modeling of free word order argument structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models trained with a predictive or masked objective have\nproven successful at capturing short and long distance syntactic dependencies.\nHere, we focus on verb argument structure in German, which has the interesting\nproperty that verb arguments may appear in a relatively free order in\nsubordinate clauses. Therefore, checking that the verb argument structure is\ncorrect cannot be done in a strictly sequential fashion, but rather requires to\nkeep track of the arguments' cases irrespective of their orders. We introduce a\nnew probing methodology based on minimal variation sets and show that both\nTransformers and LSTM achieve a score substantially better than chance on this\ntest. As humans, they also show graded judgments preferring canonical word\norders and plausible case assignments. However, we also found unexpected\ndiscrepancies in the strength of these effects, the LSTMs having difficulties\nrejecting ungrammatical sentences containing frequent argument structure types\n(double nominatives), and the Transformers tending to overgeneralize, accepting\nsome infrequent word orders or implausible sentences that humans barely accept.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 17:11:07 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 00:27:37 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Rochereau", "Charlotte", ""], ["Sagot", "Beno\u00eet", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1912.00311", "submitter": "Joel Ruben Antony Moniz", "authors": "Sarthak Garg, Joel Ruben Antony Moniz, Anshu Aviral, Priyatham\n  Bollimpalli", "title": "Learning to Relate from Captions and Bounding Boxes", "comments": "ACL 2019", "journal-ref": null, "doi": "10.18653/v1/P19-1660", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel approach that predicts the relationships\nbetween various entities in an image in a weakly supervised manner by relying\non image captions and object bounding box annotations as the sole source of\nsupervision. Our proposed approach uses a top-down attention mechanism to align\nentities in captions to objects in the image, and then leverage the syntactic\nstructure of the captions to align the relations. We use these alignments to\ntrain a relation classification network, thereby obtaining both grounded\ncaptions and dense relationships. We demonstrate the effectiveness of our model\non the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of\n25% on the relationships present in the image. We also show that the model\nsuccessfully predicts relations that are not present in the corresponding\ncaptions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 03:30:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Garg", "Sarthak", ""], ["Moniz", "Joel Ruben Antony", ""], ["Aviral", "Anshu", ""], ["Bollimpalli", "Priyatham", ""]]}, {"id": "1912.00315", "submitter": "Hanbaek Lyu", "authors": "Yuchen Guo, Nicholas Hanoian, Zhexiao Lin, Nicholas Liskij, Hanbaek\n  Lyu, Deanna Needell, Jiahao Qu, Henry Sojico, Yuliang Wang, Zhe Xiong,\n  Zhenhong Zou", "title": "Topic-aware chatbot using Recurrent Neural Networks and Nonnegative\n  Matrix Factorization", "comments": "14 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for a topic-aware chatbot by combining the\ntraditional Recurrent Neural Network (RNN) encoder-decoder model with a topic\nattention layer based on Nonnegative Matrix Factorization (NMF). After learning\ntopic vectors from an auxiliary text corpus via NMF, the decoder is trained so\nthat it is more likely to sample response words from the most correlated topic\nvectors. One of the main advantages in our architecture is that the user can\neasily switch the NMF-learned topic vectors so that the chatbot obtains desired\ntopic-awareness. We demonstrate our model by training on a single\nconversational data set which is then augmented with topic matrices learned\nfrom different auxiliary data sets. We show that our topic-aware chatbot not\nonly outperforms the non-topic counterpart, but also that each topic-aware\nmodel qualitatively and contextually gives the most relevant answer depending\non the topic of question.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:22:51 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:28:12 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Guo", "Yuchen", ""], ["Hanoian", "Nicholas", ""], ["Lin", "Zhexiao", ""], ["Liskij", "Nicholas", ""], ["Lyu", "Hanbaek", ""], ["Needell", "Deanna", ""], ["Qu", "Jiahao", ""], ["Sojico", "Henry", ""], ["Wang", "Yuliang", ""], ["Xiong", "Zhe", ""], ["Zou", "Zhenhong", ""]]}, {"id": "1912.00336", "submitter": "Lisai Zhang", "authors": "Lisai Zhang, Qingcai Chen, Dongfang Li, Buzhou Tang", "title": "Semi-supervised Visual Feature Integration for Pre-trained Language\n  Models", "comments": "12 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating visual features has been proved useful for natural language\nunderstanding tasks. Nevertheless, in most existing multimodal language models,\nthe alignment of visual and textual data is expensive. In this paper, we\npropose a novel semi-supervised visual integration framework for pre-trained\nlanguage models. In the framework, the visual features are obtained through a\nvisualization and fusion mechanism. The uniqueness includes: 1) the integration\nis conducted via a semi-supervised approach, which does not require aligned\nimages for every sentences 2) the visual features are integrated as an external\ncomponent and can be directly used by pre-trained language models. To verify\nthe efficacy of the proposed framework, we conduct the experiments on both\nnatural language inference and reading comprehension tasks. The results\ndemonstrate that our mechanism brings improvement to two strong baseline\nmodels. Considering that our framework only requires an image database, and no\nnot requires further alignments, it provides an efficient and feasible way for\nmultimodal language learning.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 06:53:23 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 02:59:54 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhang", "Lisai", ""], ["Chen", "Qingcai", ""], ["Li", "Dongfang", ""], ["Tang", "Buzhou", ""]]}, {"id": "1912.00342", "submitter": "Won Ik Cho", "authors": "Won Ik Cho, Young Ki Moon, Sangwhan Moon, Seok Min Kim, Nam Soo Kim", "title": "Machines Getting with the Program: Understanding Intent Arguments of\n  Non-Canonical Directives", "comments": "Findings of ACL: EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern dialog managers face the challenge of having to fulfill human-level\nconversational skills as part of common user expectations, including but not\nlimited to discourse with no clear objective. Along with these requirements,\nagents are expected to extrapolate intent from the user's dialogue even when\nsubjected to non-canonical forms of speech. This depends on the agent's\ncomprehension of paraphrased forms of such utterances. Especially in\nlow-resource languages, the lack of data is a bottleneck that prevents\nadvancements of the comprehension performance for these types of agents. In\nthis regard, here we demonstrate the necessity of extracting the intent\nargument of non-canonical directives in a natural language format, which may\nyield more accurate parsing, and suggest guidelines for building a parallel\ncorpus for this purpose. Following the guidelines, we construct a Korean corpus\nof 50K instances of question/command-intent pairs, including the labels for\nclassification of the utterance type. We also propose a method for mitigating\nclass imbalance, demonstrating the potential applications of the corpus\ngeneration method and its multilingual extensibility.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 07:08:19 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 08:55:30 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cho", "Won Ik", ""], ["Moon", "Young Ki", ""], ["Moon", "Sangwhan", ""], ["Kim", "Seok Min", ""], ["Kim", "Nam Soo", ""]]}, {"id": "1912.00349", "submitter": "Lanqing Xue", "authors": "Lanqing Xue, Xiaopeng Li, Nevin L. Zhang", "title": "Not All Attention Is Needed: Gated Attention Network for Sequence Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks generally have fixed network structures, the\nconcept of dynamic mechanism has drawn more and more attention in recent years.\nAttention mechanisms compute input-dependent dynamic attention weights for\naggregating a sequence of hidden states. Dynamic network configuration in\nconvolutional neural networks (CNNs) selectively activates only part of the\nnetwork at a time for different inputs. In this paper, we combine the two\ndynamic mechanisms for text classification tasks. Traditional attention\nmechanisms attend to the whole sequence of hidden states for an input sentence,\nwhile in most cases not all attention is needed especially for long sequences.\nWe propose a novel method called Gated Attention Network (GA-Net) to\ndynamically select a subset of elements to attend to using an auxiliary\nnetwork, and compute attention weights to aggregate the selected elements. It\navoids a significant amount of unnecessary computation on unattended elements,\nand allows the model to pay attention to important parts of the sequence.\nExperiments in various datasets show that the proposed method achieves better\nperformance compared with all baseline models with global or local attention\nwhile requiring less computation and achieving better interpretability. It is\nalso promising to extend the idea to more complex attention-based models, such\nas transformers and seq-to-seq models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 07:57:41 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Xue", "Lanqing", ""], ["Li", "Xiaopeng", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1912.00380", "submitter": "Yiru Wang", "authors": "Yiru Wang, Pengda Si, Zeyang Lei, Guangxu Xun, Yujiu Yang", "title": "HSCJN: A Holistic Semantic Constraint Joint Network for Diverse Response\n  Generation", "comments": "Nomination for best paper award at 3rd Conversational AI Workshop at\n  Thirty-third Conference on Neural Information Processing Systems(NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sequence-to-sequence (Seq2Seq) model generates target words iteratively\ngiven the previously observed words during decoding process, which results in\nthe loss of the holistic semantics in the target response and the complete\nsemantic relationship between responses and dialogue histories. In this paper,\nwe propose a generic diversity-promoting joint network, called Holistic\nSemantic Constraint Joint Network (HSCJN), enhancing the global sentence\ninformation, and then regularizing the objective function with penalizing the\nlow entropy output. Our network introduces more target information to improve\ndiversity, and captures direct semantic information to better constrain the\nrelevance simultaneously. Moreover, the proposed method can be easily applied\nto any Seq2Seq structure. Extensive experiments on several dialogue corpuses\nshow that our method effectively improves both semantic consistency and\ndiversity of generated responses, and achieves better performance than other\ncompetitive methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 10:41:42 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 16:04:51 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Yiru", ""], ["Si", "Pengda", ""], ["Lei", "Zeyang", ""], ["Xun", "Guangxu", ""], ["Yang", "Yujiu", ""]]}, {"id": "1912.00398", "submitter": "Lei Yang", "authors": "Rujing Yao, Linlin Hou, Lei Yang, Jie Gui, Qing Yin, and Ou Wu", "title": "Deep Human Answer Understanding for Natural Reverse QA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study focuses on a reverse question answering (QA) procedure, in which\nmachines proactively raise questions and humans supply the answers. This\nprocedure exists in many real human-machine interaction applications. However,\na crucial problem in human-machine interaction is answer understanding. The\nexisting solutions have relied on mandatory option term selection to avoid\nautomatic answer understanding. However, these solutions have led to unnatural\nhuman-computer interaction and negatively affected user experience. To this\nend, the current study proposes a novel deep answer understanding network,\ncalled AntNet, for reverse QA. The network consists of three new modules,\nnamely, skeleton attention for questions, relevance-aware representation of\nanswers, and multi-hop based fusion. As answer understanding for reverse QA has\nnot been explored, a new data corpus is compiled in this study. Experimental\nresults indicate that our proposed network is significantly better than\nexisting methods and those modified from classical natural language processing\ndeep models. The effectiveness of the three new modules is also verified.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 13:03:03 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 04:35:12 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yao", "Rujing", ""], ["Hou", "Linlin", ""], ["Yang", "Lei", ""], ["Gui", "Jie", ""], ["Yin", "Qing", ""], ["Wu", "Ou", ""]]}, {"id": "1912.00423", "submitter": "Cody Bumgardner", "authors": "Daniel Cotter and V. K. Cody Bumgardner", "title": "Semantic Enrichment of Streaming Healthcare Data", "comments": "11 pages, 9 figures, adapted from masters project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, the healthcare industry has made significant advances in\nthe digitization of patient information. However, a lack of interoperability\namong healthcare systems still imposes a high cost to patients, hospitals, and\ninsurers. Currently, most systems pass messages using idiosyncratic messaging\nstandards that require specialized knowledge to interpret. This increases the\ncost of systems integration and often puts more advanced uses of data out of\nreach. In this project, we demonstrate how two open standards, FHIR and RDF,\ncan be combined both to integrate data from disparate sources in real-time and\nmake that data queryable and susceptible to automated inference. To validate\nthe effectiveness of the semantic engine, we perform simulations of real-time\ndata feeds and demonstrate how they can be combined and used by client-side\napplications with no knowledge of the underlying sources.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 15:06:44 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Cotter", "Daniel", ""], ["Bumgardner", "V. K. Cody", ""]]}, {"id": "1912.00463", "submitter": "Ivan Smirnov", "authors": "Ivan Smirnov", "title": "Generalizable prediction of academic performance from short texts on\n  social media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has already been established that digital traces can be used to predict\nvarious human attributes. In most cases, however, predictive models rely on\nfeatures that are specific to a particular source of digital trace data. In\ncontrast, short texts written by users $-$ tweets, posts, or comments $-$ are\nubiquitous across multiple platforms. In this paper, we explore the predictive\npower of short texts with respect to the academic performance of their authors.\nWe use data from a representative panel of Russian students that includes\ninformation about their educational outcomes and activity on a popular\nnetworking site, VK. We build a model to predict academic performance from\nusers' posts on VK and then apply it to a different context. In particular, we\nshow that the model could reproduce rankings of schools and universities from\nthe posts of their students on social media. We also find that the same model\ncould predict academic performance from tweets as well as from VK posts. The\ngeneralizability of a model trained on a relatively small data set could be\nexplained by the use of continuous word representations trained on a much\nlarger corpus of social media posts. This also allows for greater\ninterpretability of model predictions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:16:18 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Smirnov", "Ivan", ""]]}, {"id": "1912.00501", "submitter": "Himangi Mittal", "authors": "Himangi Mittal, Ajith Abraham, Anuja Arora", "title": "Interpreting Context of Images using Scene Graphs", "comments": "To appear in International Conference on Big Data Analytics (BDA2019)\n  (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a visual scene incorporates objects, relationships, and\ncontext. Traditional methods working on an image mostly focus on object\ndetection and fail to capture the relationship between the objects.\nRelationships can give rich semantic information about the objects in a scene.\nThe context can be conducive to comprehending an image since it will help us to\nperceive the relation between the objects and thus, give us a deeper insight\ninto the image. Through this idea, our project delivers a model that focuses on\nfinding the context present in an image by representing the image as a graph,\nwhere the nodes will the objects and edges will be the relation between them.\nThe context is found using the visual and semantic cues which are further\nconcatenated and given to the Support Vector Machines (SVM) to detect the\nrelation between two objects. This presents us with the context of the image\nwhich can be further used in applications such as similar image retrieval,\nimage captioning, or story generation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 21:32:11 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mittal", "Himangi", ""], ["Abraham", "Ajith", ""], ["Arora", "Anuja", ""]]}, {"id": "1912.00509", "submitter": "Matheus Werner", "authors": "Matheus Werner, Eduardo Laber", "title": "Speeding up Word Mover's Distance and its variants via properties of\n  distances between embeddings", "comments": "Author's final version, accepted for publication at ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Word Mover's Distance (WMD) proposed by Kusner et al. is a distance\nbetween documents that takes advantage of semantic relations among words that\nare captured by their embeddings. This distance proved to be quite effective,\nobtaining state-of-art error rates for classification tasks, but is also\nimpracticable for large collections/documents due to its computational\ncomplexity. For circumventing this problem, variants of WMD have been proposed.\nAmong them, Relaxed Word Mover's Distance (RWMD) is one of the most successful\ndue to its simplicity, effectiveness, and also because of its fast\nimplementations.\n  Relying on assumptions that are supported by empirical properties of the\ndistances between embeddings, we propose an approach to speed up both WMD and\nRWMD. Experiments over 10 datasets suggest that our approach leads to a\nsignificant speed-up in document classification tasks while maintaining the\nsame error rates.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 22:08:32 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 18:51:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Werner", "Matheus", ""], ["Laber", "Eduardo", ""]]}, {"id": "1912.00512", "submitter": "Ugur Kursuncu", "authors": "Ugur Kursuncu, Manas Gaur and Amit Sheth", "title": "Knowledge Infused Learning (K-IL): Towards Deep Incorporation of\n  Knowledge in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning the underlying patterns in data goes beyond instance-based\ngeneralization to external knowledge represented in structured graphs or\nnetworks. Deep learning that primarily constitutes neural computing stream in\nAI has shown significant advances in probabilistically learning latent patterns\nusing a multi-layered network of computational nodes (i.e., neurons/hidden\nunits). Structured knowledge that underlies symbolic computing approaches and\noften supports reasoning, has also seen significant growth in recent years, in\nthe form of broad-based (e.g., DBPedia, Yago) and domain, industry or\napplication specific knowledge graphs. A common substrate with careful\nintegration of the two will raise opportunities to develop neuro-symbolic\nlearning approaches for AI, where conceptual and probabilistic representations\nare combined. As the incorporation of external knowledge will aid in\nsupervising the learning of features for the model, deep infusion of\nrepresentational knowledge from knowledge graphs within hidden layers will\nfurther enhance the learning process. Although much work remains, we believe\nthat knowledge graphs will play an increasing role in developing hybrid\nneuro-symbolic intelligent systems (bottom-up deep learning with top-down\nsymbolic computing) as well as in building explainable AI systems for which\nknowledge graphs will provide scaffolding for punctuating neural computing. In\nthis position paper, we describe our motivation for such a neuro-symbolic\napproach and framework that combines knowledge graph and neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 22:36:14 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 05:26:23 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kursuncu", "Ugur", ""], ["Gaur", "Manas", ""], ["Sheth", "Amit", ""]]}, {"id": "1912.00544", "submitter": "Qipeng Guo", "authors": "Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, Zheng Zhang", "title": "Multi-Scale Self-Attention for Text Classification", "comments": "Accepted in AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the prior knowledge, multi-scale structure, into\nself-attention modules. We propose a Multi-Scale Transformer which uses\nmulti-scale multi-head self-attention to capture features from different\nscales. Based on the linguistic perspective and the analysis of pre-trained\nTransformer (BERT) on a huge corpus, we further design a strategy to control\nthe scale distribution for each layer. Results of three different kinds of\ntasks (21 datasets) show our Multi-Scale Transformer outperforms the standard\nTransformer consistently and significantly on small and moderate size datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:08:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Guo", "Qipeng", ""], ["Qiu", "Xipeng", ""], ["Liu", "Pengfei", ""], ["Xue", "Xiangyang", ""], ["Zhang", "Zheng", ""]]}, {"id": "1912.00547", "submitter": "Alexey Svyatkovskiy", "authors": "Alexey Svyatkovskiy, Kosuke Imai, Mary Kroeger, Yuki Shiraito", "title": "Large-scale text processing pipeline with Apache Spark", "comments": null, "journal-ref": "Published in Proceedings of Big NLP workshop at the IEEE Big Data\n  Conference 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate Apache Spark for a data-intensive machine learning\nproblem. Our use case focuses on policy diffusion detection across the state\nlegislatures in the United States over time. Previous work on policy diffusion\nhas been unable to make an all-pairs comparison between bills due to\ncomputational intensity. As a substitute, scholars have studied single topic\nareas.\n  We provide an implementation of this analysis workflow as a distributed text\nprocessing pipeline with Spark dataframes and Scala application programming\ninterface. We discuss the challenges and strategies of unstructured data\nprocessing, data formats for storage and efficient access, and graph processing\nat scale.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 02:12:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Svyatkovskiy", "Alexey", ""], ["Imai", "Kosuke", ""], ["Kroeger", "Mary", ""], ["Shiraito", "Yuki", ""]]}, {"id": "1912.00567", "submitter": "Tao Wang", "authors": "Tao Wang and Shaohui Kuang and Deyi Xiong and Ant\\'onio Branco", "title": "Merging External Bilingual Pairs into Neural Machine Translation", "comments": "7 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural machine translation (NMT) is not easily amenable to explicit\ncorrection of errors, incorporating pre-specified translations into NMT is\nwidely regarded as a non-trivial challenge. In this paper, we propose and\nexplore three methods to endow NMT with pre-specified bilingual pairs. Instead,\nfor instance, of modifying the beam search algorithm during decoding or making\ncomplex modifications to the attention mechanism --- mainstream approaches to\ntackling this challenge ---, we experiment with the training data being\nappropriately pre-processed to add information about pre-specified\ntranslations. Extra embeddings are also used to distinguish pre-specified\ntokens from the other tokens. Extensive experimentation and analysis indicate\nthat over 99% of the pre-specified phrases are successfully translated (given a\n85% baseline) and that there is also a substantive improvement in translation\nquality with the methods explored here.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:05:50 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wang", "Tao", ""], ["Kuang", "Shaohui", ""], ["Xiong", "Deyi", ""], ["Branco", "Ant\u00f3nio", ""]]}, {"id": "1912.00569", "submitter": "Kecheng Zheng", "authors": "Kecheng Zheng, Zheng-jun Zha, Wei Wei", "title": "Abstract Reasoning with Distracting Features", "comments": "Published as a conference paper at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction reasoning is a long-standing challenge in artificial\nintelligence. Recent studies suggest that many of the deep architectures that\nhave triumphed over other domains failed to work well in abstract reasoning. In\nthis paper, we first illustrate that one of the main challenges in such a\nreasoning task is the presence of distracting features, which requires the\nlearning algorithm to leverage counterevidence and to reject any of the false\nhypotheses in order to learn the true patterns. We later show that carefully\ndesigned learning trajectory over different categories of training data can\neffectively boost learning performance by mitigating the impacts of distracting\nfeatures. Inspired by this fact, we propose feature robust abstract reasoning\n(FRAR) model, which consists of a reinforcement learning based teacher network\nto determine the sequence of training and a student network for predictions.\nExperimental results demonstrated strong improvements over baseline algorithms\nand we are able to beat the state-of-the-art models by 18.7% in the RAVEN\ndataset and 13.3% in the PGM dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:14:23 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zheng", "Kecheng", ""], ["Zha", "Zheng-jun", ""], ["Wei", "Wei", ""]]}, {"id": "1912.00582", "submitter": "Alex Warstadt", "authors": "Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng,\n  Sheng-Fu Wang, Samuel R. Bowman", "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English", "comments": "To appear in TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce The Benchmark of Linguistic Minimal Pairs (shortened to BLiMP),\na challenge set for evaluating what language models (LMs) know about major\ngrammatical phenomena in English. BLiMP consists of 67 sub-datasets, each\ncontaining 1000 minimal pairs isolating specific contrasts in syntax,\nmorphology, or semantics. The data is automatically generated according to\nexpert-crafted grammars, and aggregate human agreement with the labels is\n96.4%. We use it to evaluate n-gram, LSTM, and Transformer (GPT-2 and\nTransformer-XL) LMs. We find that state-of-the-art models identify\nmorphological contrasts reliably, but they struggle with semantic restrictions\non the distribution of quantifiers and negative polarity items and subtle\nsyntactic phenomena such as extraction islands.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 05:42:41 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 02:07:03 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 20:08:54 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Warstadt", "Alex", ""], ["Parrish", "Alicia", ""], ["Liu", "Haokun", ""], ["Mohananey", "Anhad", ""], ["Peng", "Wei", ""], ["Wang", "Sheng-Fu", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1912.00609", "submitter": "Yabing Zhu", "authors": "Yabing Zhu, Yanfeng Zhang, Huili Yang and Fangjing Wang", "title": "GANCoder: An Automatic Natural Language-to-Programming Language\n  Translation Approach based on GAN", "comments": "10pages, 4 figures", "journal-ref": "NLPCC 2019: Natural Language Processing and Chinese Computing", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GANCoder, an automatic programming approach based on Generative\nAdversarial Networks (GAN), which can generate the same functional and logical\nprogramming language codes conditioned on the given natural language\nutterances. The adversarial training between generator and discriminator helps\ngenerator learn distribution of dataset and improve code generation quality.\nOur experimental results show that GANCoder can achieve comparable accuracy\nwith the state-of-the-art methods and is more stable when programming\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:41:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhu", "Yabing", ""], ["Zhang", "Yanfeng", ""], ["Yang", "Huili", ""], ["Wang", "Fangjing", ""]]}, {"id": "1912.00667", "submitter": "Akansha Bhardwaj", "authors": "Akansha Bhardwaj, Jie Yang, Philippe Cudr\\'e-Mauroux", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation\n  Estimation in Micropost Event Detection", "comments": "Accepted at AAAI, 2020", "journal-ref": "AAAI, 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging platforms such as Twitter are increasingly being used in event\ndetection. Existing approaches mainly use machine learning models and rely on\nevent-related keywords to collect the data for model training. These approaches\nmake strong assumptions on the distribution of the relevant micro-posts\ncontaining the keyword -- referred to as the expectation of the distribution --\nand use it as a posterior regularization parameter during model training. Such\napproaches are, however, limited as they fail to reliably estimate the\ninformativeness of a keyword and its expectation for model training. This paper\nintroduces a Human-AI loop approach to jointly discover informative keywords\nfor model training while estimating their expectation. Our approach iteratively\nleverages the crowd to estimate both keyword specific expectation and the\ndisagreement between the crowd and the model in order to discover new keywords\nthat are most beneficial for model training. These keywords and their\nexpectation not only improve the resulting performance but also make the model\ntraining process more transparent. We empirically demonstrate the merits of our\napproach, both in terms of accuracy and interpretability, on multiple\nreal-world datasets and show that our approach improves the state of the art by\n24.3%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:18:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bhardwaj", "Akansha", ""], ["Yang", "Jie", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1912.00690", "submitter": "Benjamin Clavi\\'e", "authors": "Benjamin Clavi\\'e and Kobi Gal", "title": "EduBERT: Pretrained Deep Language Models for Learning Analytics", "comments": "Accepted for poster presentation at the 10th International Learning\n  Analytics and Knowledge (LAK20) Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of large pretrained neural networks to create contextualized word\nembeddings has drastically improved performance on several natural language\nprocessing (NLP) tasks. These computationally expensive models have begun to be\napplied to domain-specific NLP tasks such as re-hospitalization prediction from\nclinical notes. This paper demonstrates that using large pretrained models\nproduces excellent results on common learning analytics tasks. Pre-training\ndeep language models using student forum data from a wide array of online\ncourses improves performance beyond the state of the art on three text\nclassification tasks. We also show that a smaller, distilled version of our\nmodel produces the best results on two of the three tasks while limiting\ncomputational cost. We make both models available to the research community at\nlarge.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:32:53 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Clavi\u00e9", "Benjamin", ""], ["Gal", "Kobi", ""]]}, {"id": "1912.00698", "submitter": "Yuri Safovich", "authors": "Yuri Safovich and Amos Azaria", "title": "Fiction Sentence Expansion and Enhancement via Focused Objective and\n  Novelty Curve Sampling", "comments": "Accepted at ICTAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the task of sentence expansion and enhancement, in which a\nsentence provided by a human is expanded in some creative way. The expansion\nshould be understandable, believably grammatical, and optimally\nmeaning-preserving. Sentence expansion and enhancement may serve as an\nauthoring tool, or integrate in dynamic media, conversational agents, or\nvariegated advertising.\n  We implement a neural sentence expander trained on sentence compressions\ngenerated from a corpus of modern fiction. We modify an MLE objective to\nsupport the task by focusing on new words, and decode at test time with\ncontrolled curve-like novelty sampling. We run our sentence expander on\nsentences provided by human subjects and have humans evaluate these expansions.\nWe show that, although the generation methods are inferior to professional\nhuman writers, they are comparable to, and as well liked as, our subjects'\noriginal input sentences, and preferred over baselines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:51:57 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 18:22:57 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 21:24:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Safovich", "Yuri", ""], ["Azaria", "Amos", ""]]}, {"id": "1912.00730", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Doris Hoogeveen, Llu\\'is M\\`arquez, Alessandro\n  Moschitti, Hamdy Mubarak, Timothy Baldwin, Karin Verspoor", "title": "SemEval-2017 Task 3: Community Question Answering", "comments": "community question answering, question-question similarity,\n  question-comment similarity, answer reranking, Multi-domain Question\n  Duplicate Detection, StackExchange, English, Arabic", "journal-ref": "SemEval-2017", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe SemEval-2017 Task 3 on Community Question Answering. This year,\nwe reran the four subtasks from SemEval-2016:(A) Question-Comment\nSimilarity,(B) Question-Question Similarity,(C) Question-External Comment\nSimilarity, and (D) Rerank the correct answers for a new question in Arabic,\nproviding all the data from 2015 and 2016 for training, and fresh data for\ntesting. Additionally, we added a new subtask E in order to enable\nexperimentation with Multi-domain Question Duplicate Detection in a\nlarger-scale scenario, using StackExchange subforums. A total of 23 teams\nparticipated in the task, and submitted a total of 85 runs (36 primary and 49\ncontrastive) for subtasks A-D. Unfortunately, no teams participated in subtask\nE. A variety of approaches and features were used by the participating systems\nto address the different subtasks. The best systems achieved an official score\n(MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D,\nrespectively. These scores are better than the baselines, especially for\nsubtasks A-C.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:57:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nakov", "Preslav", ""], ["Hoogeveen", "Doris", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Moschitti", "Alessandro", ""], ["Mubarak", "Hamdy", ""], ["Baldwin", "Timothy", ""], ["Verspoor", "Karin", ""]]}, {"id": "1912.00741", "submitter": "Preslav Nakov", "authors": "Sara Rosenthal, Noura Farra, Preslav Nakov", "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "comments": "sentiment analysis, Twitter, classification, quantification, ranking,\n  English, Arabic", "journal-ref": null, "doi": null, "report-no": "SemEval-2017", "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the fifth year of the Sentiment Analysis in Twitter\ntask. SemEval-2017 Task 4 continues with a rerun of the subtasks of\nSemEval-2016 Task 4, which include identifying the overall sentiment of the\ntweet, sentiment towards a topic with classification on a two-point and on a\nfive-point ordinal scale, and quantification of the distribution of sentiment\ntowards a topic across a number of tweets: again on a two-point and on a\nfive-point ordinal scale. Compared to 2016, we made two changes: (i) we\nintroduced a new language, Arabic, for all subtasks, and (ii)~we made available\ninformation from the profiles of the Twitter users who posted the target\ntweets. The task continues to be very popular, with a total of 48 teams\nparticipating this year.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:04:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Rosenthal", "Sara", ""], ["Farra", "Noura", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.00778", "submitter": "Itay Lieder", "authors": "Itay Lieder, Meirav Segal, Eran Avidan, Asaf Cohen, Tom Hope", "title": "Learning a faceted customer segmentation for discovering new business\n  opportunities at Intel", "comments": "3 pages, 4 figures, Published in proceedings of IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sales and marketing organizations within large enterprises, identifying\nand understanding new markets, customers and partners is a key challenge.\nIntel's Sales and Marketing Group (SMG) faces similar challenges while growing\nin new markets and domains and evolving its existing business. In today's\ncomplex technological and commercial landscape, there is need for intelligent\nautomation supporting a fine-grained understanding of businesses in order to\nhelp SMG sift through millions of companies across many geographies and\nlanguages and identify relevant directions. We present a system developed in\nour company that mines millions of public business web pages, and extracts a\nfaceted customer representation. We focus on two key customer aspects that are\nessential for finding relevant opportunities: industry segments (ranging from\nbroad verticals such as healthcare, to more specific fields such as 'video\nanalytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address\nthe challenge of labeled data collection, we enrich our data with external\ninformation gleaned from Wikipedia, and develop a semi-supervised multi-label,\nmulti-lingual deep learning model that parses customer website texts and\nclassifies them into their respective facets. Our system scans and indexes\ncompanies as part of a large-scale knowledge graph that currently holds tens of\nmillions of connected entities with thousands being fetched, enriched and\nconnected to the graph by the hour in real time, and also supports knowledge\nand insight discovery. In experiments conducted in our company, we are able to\nsignificantly boost the performance of sales personnel in the task of\ndiscovering new customers and commercial partnership opportunities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:48:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Lieder", "Itay", ""], ["Segal", "Meirav", ""], ["Avidan", "Eran", ""], ["Cohen", "Asaf", ""], ["Hope", "Tom", ""]]}, {"id": "1912.00819", "submitter": "Chandrakant Bothe", "authors": "Chandrakant Bothe, Cornelius Weber, Sven Magg, Stefan Wermter", "title": "EDA: Enriching Emotional Dialogue Acts using an Ensemble of Neural\n  Annotators", "comments": "Proceeding of the LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of emotion and dialogue acts enriches conversational analysis\nand help to build natural dialogue systems. Emotion interpretation makes us\nunderstand feelings and dialogue acts reflect the intentions and performative\nfunctions in the utterances. However, most of the textual and multi-modal\nconversational emotion corpora contain only emotion labels but not dialogue\nacts. To address this problem, we propose to use a pool of various recurrent\nneural models trained on a dialogue act corpus, with and without context. These\nneural models annotate the emotion corpora with dialogue act labels, and an\nensemble annotator extracts the final dialogue act label. We annotated two\naccessible multi-modal emotion corpora: IEMOCAP and MELD. We analyzed the\nco-occurrence of emotion and dialogue act labels and discovered specific\nrelations. For example, Accept/Agree dialogue acts often occur with the Joy\nemotion, Apology with Sadness, and Thanking with Joy. We make the Emotional\nDialogue Acts (EDA) corpus publicly available to the research community for\nfurther study and analysis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 14:29:22 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 15:31:54 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 14:37:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bothe", "Chandrakant", ""], ["Weber", "Cornelius", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1912.00835", "submitter": "Sneha Mehta", "authors": "Sneha Mehta, Huzefa Rangwala, Naren Ramakrishnan", "title": "Low Rank Factorization for Compact Multi-Head Self-Attention", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective representation learning from text has been an active area of\nresearch in the fields of NLP and text mining. Attention mechanisms have been\nat the forefront in order to learn contextual sentence representations. Current\nstate-of-the-art approaches for many NLP tasks use large pre-trained language\nmodels such as BERT, XLNet and so on for learning representations. These models\nare based on the Transformer architecture that involves recurrent blocks of\ncomputation consisting of multi-head self-attention and feedforward networks.\nOne of the major bottlenecks largely contributing to the computational\ncomplexity of the Transformer models is the self-attention layer, that is both\ncomputationally expensive and parameter intensive. In this work, we introduce a\nnovel multi-head self-attention mechanism operating on GRUs that is shown to be\ncomputationally cheaper and more parameter efficient than self-attention\nmechanism proposed in Transformers for text classification tasks. The\nefficiency of our approach mainly stems from two optimizations; 1) we use\nlow-rank matrix factorization of the affinity matrix to efficiently get\nmultiple attention distributions instead of having separate parameters for each\nhead 2) attention scores are obtained by querying a global context vector\ninstead of densely querying all the words in the sentence. We evaluate the\nperformance of the proposed model on tasks such as sentiment analysis from\nmovie reviews, predicting business ratings from reviews and classifying news\narticles into topics. We find that the proposed approach matches or outperforms\na series of strong baselines and is more parameter efficient than comparable\nmulti-head approaches. We also perform qualitative analyses to verify that the\nproposed approach is interpretable and captures context-dependent word\nimportance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:01:51 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 03:36:37 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mehta", "Sneha", ""], ["Rangwala", "Huzefa", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1912.00839", "submitter": "Dafang He", "authors": "Ke Yuan, Dafang He, Zhuoren Jiang, Liangcai Gao, Zhi Tang, C. Lee\n  Giles", "title": "Automatic Generation of Headlines for Online Math Questions", "comments": null, "journal-ref": "AAA2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical equations are an important part of dissemination and\ncommunication of scientific information. Students, however, often feel\nchallenged in reading and understanding math content and equations. With the\ndevelopment of the Web, students are posting their math questions online.\nNevertheless, constructing a concise math headline that gives a good\ndescription of the posted detailed math question is nontrivial. In this study,\nwe explore a novel summarization task denoted as geNerating A concise Math\nhEadline from a detailed math question (NAME). Compared to conventional\nsummarization tasks, this task has two extra and essential constraints: 1)\nDetailed math questions consist of text and math equations which require a\nunified framework to jointly model textual and mathematical information; 2)\nUnlike text, math equations contain semantic and structural features, and both\nof them should be captured together. To address these issues, we propose\nMathSum, a novel summarization model which utilizes a pointer mechanism\ncombined with a multi-head attention mechanism for mathematical representation\naugmentation. The pointer mechanism can either copy textual tokens or math\ntokens from source questions in order to generate math headlines. The\nmulti-head attention mechanism is designed to enrich the representation of math\nequations by modeling and integrating both its semantic and structural\nfeatures. For evaluation, we collect and make available two sets of real-world\ndetailed math questions along with human-written math headlines, namely\nEXEQ-300k and OFEQ-10k. Experimental results demonstrate that our model\n(MathSum) significantly outperforms state-of-the-art models for both the\nEXEQ-300k and OFEQ-10k datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 20:37:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Yuan", "Ke", ""], ["He", "Dafang", ""], ["Jiang", "Zhuoren", ""], ["Gao", "Liangcai", ""], ["Tang", "Zhi", ""], ["Giles", "C. Lee", ""]]}, {"id": "1912.00846", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Subhadeep Dey, Hwanhee Lee, Kyomin Jung", "title": "Attentive Modality Hopping Mechanism for Speech Emotion Recognition", "comments": "5 pages, Accepted as a conference paper at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the impact of visual modality in addition to speech\nand text for improving the accuracy of the emotion detection system. The\ntraditional approaches tackle this task by fusing the knowledge from the\nvarious modalities independently for performing emotion classification. In\ncontrast to these approaches, we tackle the problem by introducing an attention\nmechanism to combine the information. In this regard, we first apply a neural\nnetwork to obtain hidden representations of the modalities. Then, the attention\nmechanism is defined to select and aggregate important parts of the video data\nby conditioning on the audio and text data. Furthermore, the attention\nmechanism is again applied to attend important parts of the speech and textual\ndata, by considering other modality. Experiments are performed on the standard\nIEMOCAP dataset using all three modalities (audio, text, and video). The\nachieved results show a significant improvement of 3.65% in terms of weighted\naccuracy compared to the baseline system.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:23:23 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 02:18:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Dey", "Subhadeep", ""], ["Lee", "Hwanhee", ""], ["Jung", "Kyomin", ""]]}, {"id": "1912.00862", "submitter": "Fei Li", "authors": "Fei Li and Hong Yu", "title": "ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated ICD coding, which assigns the International Classification of\nDisease codes to patient visits, has attracted much research attention since it\ncan save time and labor for billing. The previous state-of-the-art model\nutilized one convolutional layer to build document representations for\npredicting ICD codes. However, the lengths and grammar of text fragments, which\nare closely related to ICD coding, vary a lot in different documents.\nTherefore, a flat and fixed-length convolutional architecture may not be\ncapable of learning good document representations. In this paper, we proposed a\nMulti-Filter Residual Convolutional Neural Network (MultiResCNN) for ICD\ncoding. The innovations of our model are two-folds: it utilizes a multi-filter\nconvolutional layer to capture various text patterns with different lengths and\na residual convolutional layer to enlarge the receptive field. We evaluated the\neffectiveness of our model on the widely-used MIMIC dataset. On the full code\nset of MIMIC-III, our model outperformed the state-of-the-art model in 4 out of\n6 evaluation metrics. On the top-50 code set of MIMIC-III and the full code set\nof MIMIC-II, our model outperformed all the existing and state-of-the-art\nmodels in all evaluation metrics. The code is available at\nhttps://github.com/foxlf823/Multi-Filter-Residual-Convolutional-Neural-Network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 11:23:04 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Li", "Fei", ""], ["Yu", "Hong", ""]]}, {"id": "1912.00863", "submitter": "Van Tung Pham", "authors": "Van Tung Pham, Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong\n  Chng, Chongjia Ni, Bin Ma and Haizhou Li", "title": "Independent language modeling architecture for end-to-end ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention-based end-to-end (E2E) automatic speech recognition (ASR)\narchitecture allows for joint optimization of acoustic and language models\nwithin a single network. However, in a vanilla E2E ASR architecture, the\ndecoder sub-network (subnet), which incorporates the role of the language model\n(LM), is conditioned on the encoder output. This means that the acoustic\nencoder and the language model are entangled that doesn't allow language model\nto be trained separately from external text data. To address this problem, in\nthis work, we propose a new architecture that separates the decoder subnet from\nthe encoder output. In this way, the decoupled subnet becomes an independently\ntrainable LM subnet, which can easily be updated using the external text data.\nWe study two strategies for updating the new architecture. Experimental results\nshow that, 1) the independent LM architecture benefits from external text data,\nachieving 9.3% and 22.8% relative character and word error rate reduction on\nMandarin HKUST and English NSC datasets respectively; 2)the proposed\narchitecture works well with external LM and can be generalized to different\namount of labelled data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:35:16 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pham", "Van Tung", ""], ["Xu", "Haihua", ""], ["Khassanov", "Yerbolat", ""], ["Zeng", "Zhiping", ""], ["Chng", "Eng Siong", ""], ["Ni", "Chongjia", ""], ["Ma", "Bin", ""], ["Li", "Haizhou", ""]]}, {"id": "1912.00864", "submitter": "Makoto Nakatsuji Ph. D.", "authors": "Makoto Nakatsuji, Sohei Okui", "title": "Conclusion-Supplement Answer Generation for Non-Factoid Questions", "comments": "AAAI-2020 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the goal of conclusion-supplement answer generation for\nnon-factoid questions, which is a critical issue in the field of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), as users often\nrequire supplementary information before accepting a conclusion. The current\nencoder-decoder framework, however, has difficulty generating such answers,\nsince it may become confused when it tries to learn several different long\nanswers to the same non-factoid question. Our solution, called an ensemble\nnetwork, goes beyond single short sentences and fuses logically connected\nconclusion statements and supplementary statements. It extracts the context\nfrom the conclusion decoder's output sequence and uses it to create\nsupplementary decoder states on the basis of an attention mechanism. It also\nassesses the closeness of the question encoder's output sequence and the\nseparate outputs of the conclusion and supplement decoders as well as their\ncombination. As a result, it generates answers that match the questions and\nhave natural-sounding supplementary sequences in line with the context\nexpressed by the conclusion sequence. Evaluations conducted on datasets\nincluding \"Love Advice\" and \"Arts & Humanities\" categories indicate that our\nmodel outputs much more accurate results than the tested baseline models do.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:06:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nakatsuji", "Makoto", ""], ["Okui", "Sohei", ""]]}, {"id": "1912.00871", "submitter": "Kaden Griffith", "authors": "Kaden Griffith and Jugal Kalita", "title": "Solving Arithmetic Word Problems Automatically Using Transformer and\n  Unambiguous Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing accurate and automatic solvers of math word problems has proven\nto be quite challenging. Prior attempts using machine learning have been\ntrained on corpora specific to math word problems to produce arithmetic\nexpressions in infix notation before answer computation. We find that\ncustom-built neural networks have struggled to generalize well. This paper\noutlines the use of Transformer networks trained to translate math word\nproblems to equivalent arithmetic expressions in infix, prefix, and postfix\nnotations. In addition to training directly on domain-specific corpora, we use\nan approach that pre-trains on a general text corpus to provide foundational\nlanguage abilities to explore if it improves performance. We compare results\nproduced by a large number of neural configurations and find that most\nconfigurations outperform previously reported approaches on three of four\ndatasets with significant increases in accuracy of over 20 percentage points.\nThe best neural approaches boost accuracy by almost 10% on average when\ncompared to the previous state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:42:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Griffith", "Kaden", ""], ["Kalita", "Jugal", ""]]}, {"id": "1912.00879", "submitter": "Xiyao Ma", "authors": "Xiyao Ma, Qile Zhu, Yanlin Zhou, Xiaolin Li, Dapeng Wu", "title": "Improving Question Generation with Sentence-level Semantic Matching and\n  Answer Position Inferring", "comments": "Revised version of paper accepted to Thirty-fourth AAAI Conference on\n  Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking an answer and its context as input, sequence-to-sequence models have\nmade considerable progress on question generation. However, we observe that\nthese approaches often generate wrong question words or keywords and copy\nanswer-irrelevant words from the input. We believe that lacking global question\nsemantics and exploiting answer position-awareness not well are the key root\ncauses. In this paper, we propose a neural question generation model with two\nconcrete modules: sentence-level semantic matching and answer position\ninferring. Further, we enhance the initial state of the decoder by leveraging\nthe answer-aware gated fusion mechanism. Experimental results demonstrate that\nour model outperforms the state-of-the-art (SOTA) models on SQuAD and MARCO\ndatasets. Owing to its generality, our work also improves the existing models\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:57:40 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 21:38:15 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 03:13:40 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ma", "Xiyao", ""], ["Zhu", "Qile", ""], ["Zhou", "Yanlin", ""], ["Li", "Xiaolin", ""], ["Wu", "Dapeng", ""]]}, {"id": "1912.00903", "submitter": "Nora Hollenstein", "authors": "Nora Hollenstein, Marius Troendle, Ce Zhang, Nicolas Langer", "title": "ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading\n  and Annotation", "comments": "Proceedings of the Language Resources and Evaluation Conference (LREC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recorded and preprocessed ZuCo 2.0, a new dataset of simultaneous\neye-tracking and electroencephalography during natural reading and during\nannotation. This corpus contains gaze and brain activity data of 739 sentences,\n349 in a normal reading paradigm and 390 in a task-specific paradigm, in which\nthe 18 participants actively search for a semantic relation type in the given\nsentences as a linguistic annotation task. This new dataset complements ZuCo\n1.0 by providing experiments designed to analyze the differences in cognitive\nprocessing between natural reading and annotation. The data is freely available\nhere: https://osf.io/2urht/.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:30:46 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 14:42:37 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 20:14:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Hollenstein", "Nora", ""], ["Troendle", "Marius", ""], ["Zhang", "Ce", ""], ["Langer", "Nicolas", ""]]}, {"id": "1912.00955", "submitter": "Shubhi Tyagi", "authors": "Shubhi Tyagi, Marco Nicolis, Jonas Rohnke, Thomas Drugman, Jaime\n  Lorenzo-Trueba", "title": "Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven\n  Acoustic Embedding Selection", "comments": null, "journal-ref": "INTERSPEECH 2020: 4407-4411", "doi": "10.21437/Interspeech.2020-1411", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Text-to-Speech (TTS) have improved quality and naturalness\nto near-human capabilities when considering isolated sentences. But something\nwhich is still lacking in order to achieve human-like communication is the\ndynamic variations and adaptability of human speech. This work attempts to\nsolve the problem of achieving a more dynamic and natural intonation in TTS\nsystems, particularly for stylistic speech such as the newscaster speaking\nstyle. We propose a novel embedding selection approach which exploits\nlinguistic information, leveraging the speech variability present in the\ntraining dataset. We analyze the contribution of both semantic and syntactic\nfeatures. Our results show that the approach improves the prosody and\nnaturalness for complex utterances as well as in Long Form Reading (LFR).\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:32:59 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 17:54:56 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 16:47:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Tyagi", "Shubhi", ""], ["Nicolis", "Marco", ""], ["Rohnke", "Jonas", ""], ["Drugman", "Thomas", ""], ["Lorenzo-Trueba", "Jaime", ""]]}, {"id": "1912.00958", "submitter": "Surabhi Punjabi", "authors": "Surabhi Punjabi, Harish Arsikere, Sri Garimella", "title": "Language Model Bootstrapping Using Neural Machine Translation For\n  Conversational Speech Recognition", "comments": "Accepted by IEEE ASRU workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building conversational speech recognition systems for new languages is\nconstrained by the availability of utterances that capture user-device\ninteractions. Data collection is both expensive and limited by the speed of\nmanual transcription. In order to address this, we advocate the use of neural\nmachine translation as a data augmentation technique for bootstrapping language\nmodels. Machine translation (MT) offers a systematic way of incorporating\ncollections from mature, resource-rich conversational systems that may be\navailable for a different language. However, ingesting raw translations from a\ngeneral purpose MT system may not be effective owing to the presence of named\nentities, intra sentential code-switching and the domain mismatch between the\nconversational data being translated and the parallel text used for MT\ntraining. To circumvent this, we explore the following domain adaptation\ntechniques: (a) sentence embedding based data selection for MT training, (b)\nmodel finetuning, and (c) rescoring and filtering translated hypotheses. Using\nHindi as the experimental testbed, we translate US English utterances to\nsupplement the transcribed collections. We observe a relative word error rate\nreduction of 7.8-15.6%, depending on the bootstrapping phase. Fine grained\nanalysis reveals that translation particularly aids the interaction scenarios\nwhich are underrepresented in the transcribed data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 17:42:58 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Punjabi", "Surabhi", ""], ["Arsikere", "Harish", ""], ["Garimella", "Sri", ""]]}, {"id": "1912.00982", "submitter": "Nils Rethmeier", "authors": "Nils Rethmeier and Vageesh Kumar Saxena and Isabelle Augenstein", "title": "TX-Ray: Quantifying and Explaining Model-Knowledge Transfer in\n  (Un-)Supervised NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art NLP explainability (XAI) methods focus on explaining\nper-sample decisions in supervised end or probing tasks, this is insufficient\nto explain and quantify model knowledge transfer during (un-)supervised\ntraining. Thus, for TX-Ray, we modify the established computer vision\nexplainability principle of 'visualizing preferred inputs of neurons' to make\nit usable transfer analysis and NLP. This allows one to analyze, track and\nquantify how self- or supervised NLP models first build knowledge abstractions\nin pretraining (1), and then transfer these abstractions to a new domain (2),\nor adapt them during supervised fine-tuning (3). TX-Ray expresses neurons as\nfeature preference distributions to quantify fine-grained knowledge transfer or\nadaptation and guide human analysis. We find that, similar to Lottery Ticket\nbased pruning, TX-Ray based pruning can improve test set generalization and\nthat it can reveal how early stages of self-supervision automatically learn\nlinguistic abstractions like parts-of-speech.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:21:31 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:18:41 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 14:24:44 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Rethmeier", "Nils", ""], ["Saxena", "Vageesh Kumar", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "1912.00991", "submitter": "Samuel R\\\"onnqvist", "authors": "Nelda Kote, Marenglen Biba, Jenna Kanerva, Samuel R\\\"onnqvist, Filip\n  Ginter", "title": "Morphological Tagging and Lemmatization of Albanian: A Manually\n  Annotated Corpus and Neural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present the first publicly available part-of-speech and\nmorphologically tagged corpus for the Albanian language, as well as a neural\nmorphological tagger and lemmatizer trained on it. There is currently a lack of\navailable NLP resources for Albanian, and its complex grammar and morphology\npresent challenges to their development. We have created an Albanian\npart-of-speech corpus based on the Universal Dependencies schema for\nmorphological annotation, containing about 118,000 tokens of naturally occuring\ntext collected from different text sources, with an addition of 67,000 tokens\nof artificially created simple sentences used only in training. On this corpus,\nwe subsequently train and evaluate segmentation, morphological tagging and\nlemmatization models, using the Turku Neural Parser Pipeline. On the held-out\nevaluation set, the model achieves 92.74% accuracy on part-of-speech tagging,\n85.31% on morphological tagging, and 89.95% on lemmatization. The manually\nannotated corpus, as well as the trained models are available under an open\nlicense.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:50:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kote", "Nelda", ""], ["Biba", "Marenglen", ""], ["Kanerva", "Jenna", ""], ["R\u00f6nnqvist", "Samuel", ""], ["Ginter", "Filip", ""]]}, {"id": "1912.01046", "submitter": "Anthony Colas", "authors": "Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Daisy\n  Zhe Wang, Doo Soon Kim", "title": "TutorialVQA: Question Answering Dataset for Tutorial Videos", "comments": "Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the number of currently available datasets on video question\nanswering, there still remains a need for a dataset involving multi-step and\nnon-factoid answers. Moreover, relying on video transcripts remains an\nunder-explored topic. To adequately address this, We propose a new question\nanswering task on instructional videos, because of their verbose and narrative\nnature. While previous studies on video question answering have focused on\ngenerating a short text as an answer, given a question and video clip, our task\naims to identify a span of a video segment as an answer which contains\ninstructional details with various granularities. This work focuses on\nscreencast tutorial videos pertaining to an image editing program. We introduce\na dataset, TutorialVQA, consisting of about 6,000manually collected triples of\n(video, question, answer span). We also provide experimental results with\nseveral baselines algorithms using the video transcripts. The results indicate\nthat the task is challenging and call for the investigation of new algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:17:57 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 20:28:21 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Colas", "Anthony", ""], ["Kim", "Seokhwan", ""], ["Dernoncourt", "Franck", ""], ["Gupte", "Siddhesh", ""], ["Wang", "Daisy Zhe", ""], ["Kim", "Doo Soon", ""]]}, {"id": "1912.01070", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, Pat Verga, Neha Choudhary, Andrew McCallum", "title": "Simultaneously Linking Entities and Extracting Relations from Biomedical\n  Text Without Mention-level Supervision", "comments": "Accepted in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the meaning of text often involves reasoning about entities and\ntheir relationships. This requires identifying textual mentions of entities,\nlinking them to a canonical concept, and discerning their relationships. These\ntasks are nearly always viewed as separate components within a pipeline, each\nrequiring a distinct model and training data. While relation extraction can\noften be trained with readily available weak or distant supervision, entity\nlinkers typically require expensive mention-level supervision -- which is not\navailable in many domains. Instead, we propose a model which is trained to\nsimultaneously produce entity linking and relation decisions while requiring no\nmention-level annotations. This approach avoids cascading errors that arise\nfrom pipelined methods and more accurately predicts entity relationships from\ntext. We show that our model outperforms a state-of-the art entity linking and\nrelation extraction pipeline on two biomedical datasets and can drastically\nimprove the overall recall of the system.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:37:18 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bansal", "Trapit", ""], ["Verga", "Pat", ""], ["Choudhary", "Neha", ""], ["McCallum", "Andrew", ""]]}, {"id": "1912.01072", "submitter": "Matej Martinc", "authors": "Matej Martinc, Petra Kralj Novak and Senja Pollak", "title": "Leveraging Contextual Embeddings for Detecting Diachronic Semantic Shift", "comments": "Accepted to Language Resources and Evaluation (LREC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new method that leverages contextual embeddings for the task of\ndiachronic semantic shift detection by generating time specific word\nrepresentations from BERT embeddings. The results of our experiments in the\ndomain specific LiverpoolFC corpus suggest that the proposed method has\nperformance comparable to the current state-of-the-art without requiring any\ntime consuming domain adaptation on large corpora. The results on the newly\ncreated Brexit news corpus suggest that the method can be successfully used for\nthe detection of a short-term yearly semantic shift. And lastly, the model also\nshows promising results in a multilingual settings, where the task was to\ndetect differences and similarities between diachronic semantic shifts in\ndifferent languages.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:52:18 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 17:29:39 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Martinc", "Matej", ""], ["Novak", "Petra Kralj", ""], ["Pollak", "Senja", ""]]}, {"id": "1912.01079", "submitter": "Sven Buechel", "authors": "Jo\\~ao Sedoc, Sven Buechel, Yehonathan Nachmany, Anneke Buffone, and\n  Lyle Ungar", "title": "Learning Word Ratings for Empathy and Distress from Document-Level User\n  Responses", "comments": "LREC 2020 camera-ready copy", "journal-ref": "Proceedings of The 12th Language Resources and Evaluation\n  Conference (LREC 2020). Pages 1657-1666", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the excellent performance of black box approaches to modeling\nsentiment and emotion, lexica (sets of informative words and associated\nweights) that characterize different emotions are indispensable to the NLP\ncommunity because they allow for interpretable and robust predictions. Emotion\nanalysis of text is increasing in popularity in NLP; however, manually creating\nlexica for psychological constructs such as empathy has proven difficult. This\npaper automatically creates empathy word ratings from document-level ratings.\nThe underlying problem of learning word ratings from higher-level supervision\nhas to date only been addressed in an ad hoc fashion and has not used deep\nlearning methods. We systematically compare a number of approaches to learning\nword ratings from higher-level supervision against a Mixed-Level Feed Forward\nNetwork (MLFFN), which we find performs best, and use the MLFFN to create the\nfirst-ever empathy lexicon. We then use Signed Spectral Clustering to gain\ninsights into the resulting words. The empathy and distress lexica are publicly\navailable at: http://www.wwbp.org/lexica.html.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:19:22 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 09:47:27 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sedoc", "Jo\u00e3o", ""], ["Buechel", "Sven", ""], ["Nachmany", "Yehonathan", ""], ["Buffone", "Anneke", ""], ["Ungar", "Lyle", ""]]}, {"id": "1912.01109", "submitter": "Ngoc L\\^e", "authors": "Ngoc C. L\\^e, Ngoc-Yen Nguyen, and Anh-Duong Trinh", "title": "On the Vietnamese Name Entity Recognition: A Deep Learning Method\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) plays an important role in text-based\ninformation retrieval. In this paper, we combine Bidirectional Long Short-Term\nMemory (Bi-LSTM) \\cite{hochreiter1997,schuster1997} with Conditional Random\nField (CRF) \\cite{lafferty2001} to create a novel deep learning model for the\nNER problem. Each word as input of the deep learning model is represented by a\nWord2vec-trained vector. A word embedding set trained from about one million\narticles in 2018 collected through a Vietnamese news portal (baomoi.com). In\naddition, we concatenate a Word2Vec\\cite{mikolov2013}-trained vector with\nsemantic feature vector (Part-Of-Speech (POS) tagging, chunk-tag) and hidden\nsyntactic feature vector (extracted by Bi-LSTM nerwork) to achieve the (so far\nbest) result in Vietnamese NER system. The result was conducted on the data set\nVLSP2016 (Vietnamese Language and Speech Processing 2016 \\cite{vlsp2016})\ncompetition.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 13:28:37 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["L\u00ea", "Ngoc C.", ""], ["Nguyen", "Ngoc-Yen", ""], ["Trinh", "Anh-Duong", ""]]}, {"id": "1912.01110", "submitter": "Vijini Supun Keerthisrini Pilana Liyanage", "authors": "Vijini Liyanage and Surangika Ranathunga", "title": "A Multi-language Platform for Generating Algebraic Mathematical Word\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for automatically generating mathematical word problems\nare deprived of customizability and creativity due to the inherent nature of\ntemplate-based mechanisms they employ. We present a solution to this problem\nwith the use of deep neural language generation mechanisms. Our approach uses a\nCharacter Level Long Short Term Memory Network (LSTM) to generate word\nproblems, and uses POS (Part of Speech) tags to resolve the constraints found\nin the generated problems. Our approach is capable of generating Mathematics\nWord Problems in both English and Sinhala languages with an accuracy over 90%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 04:50:45 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Liyanage", "Vijini", ""], ["Ranathunga", "Surangika", ""]]}, {"id": "1912.01111", "submitter": "Jayanta Mandi", "authors": "Dipankar Chakrabarti, Neelam Patodia, Udayan Bhattacharya, Indranil\n  Mitra, Satyaki Roy, Jayanta Mandi, Nandini Roy, Prasun Nandy", "title": "Use of Artificial Intelligence to Analyse Risk in Legal Documents for a\n  Better Decision Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assessing risk for voluminous legal documents such as request for proposal;\ncontracts is tedious and error prone. We have developed \"risk-o-meter\", a\nframework, based on machine learning and natural language processing to review\nand assess risks of any legal document. Our framework uses Paragraph Vector, an\nunsupervised model to generate vector representation of text. This enables the\nframework to learn contextual relations of legal terms and generate sensible\ncontext aware embedding. The framework then feeds the vector space into a\nsupervised classification algorithm to predict whether a paragraph belongs to a\nper-defined risk category or not. The framework thus extracts risk prone\nparagraphs. This technique efficiently overcomes the limitations of\nkeyword-based search. We have achieved an accuracy of 91% for the risk category\nhaving the largest training dataset. This framework will help organizations\noptimize effort to identify risk from large document base with minimal human\nintervention and thus will help to have risk mitigated sustainable growth. Its\nmachine learning capability makes it scalable to uncover relevant information\nfrom any type of document apart from legal documents, provided the library is\nper-populated and rich.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:07:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chakrabarti", "Dipankar", ""], ["Patodia", "Neelam", ""], ["Bhattacharya", "Udayan", ""], ["Mitra", "Indranil", ""], ["Roy", "Satyaki", ""], ["Mandi", "Jayanta", ""], ["Roy", "Nandini", ""], ["Nandy", "Prasun", ""]]}, {"id": "1912.01113", "submitter": "Preslav Nakov", "authors": "Preslav Nakov", "title": "Using the Web as an Implicit Training Set: Application to Noun Compound\n  Syntax and Semantics", "comments": "noun compounds, paraphrasing verbs, semantic interpretation, syntax,\n  multi-word expressions, MWEs, noun compound interpretation, noun compound\n  bracketing, prepositional phrase attachment, noun phrase coordination,\n  machine translation", "journal-ref": "PhD Thesis, University of California at Berkeley, 2007", "doi": null, "report-no": "Technical Report No. UCB/EECS-2007-173", "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important characteristic of English written text is the abundance of noun\ncompounds - sequences of nouns acting as a single noun, e.g., colon cancer\ntumor suppressor protein. While eventually mastered by domain experts, their\ninterpretation poses a major challenge for automated analysis. Understanding\nnoun compounds' syntax and semantics is important for many natural language\napplications, including question answering, machine translation, information\nretrieval, and information extraction. I address the problem of noun compounds\nsyntax by means of novel, highly accurate unsupervised and lightly supervised\nalgorithms using the Web as a corpus and search engines as interfaces to that\ncorpus. Traditionally the Web has been viewed as a source of page hit counts,\nused as an estimate for n-gram word frequencies. I extend this approach by\nintroducing novel surface features and paraphrases, which yield\nstate-of-the-art results for the task of noun compound bracketing. I also show\nhow these kinds of features can be applied to other structural ambiguity\nproblems, like prepositional phrase attachment and noun phrase coordination. I\naddress noun compound semantics by automatically generating paraphrasing verbs\nand prepositions that make explicit the hidden semantic relations between the\nnouns in a noun compound. I also demonstrate how these paraphrasing verbs can\nbe used to solve various relational similarity problems, and how paraphrasing\nnoun compounds can improve machine translation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:33:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Nakov", "Preslav", ""]]}, {"id": "1912.01114", "submitter": "Qingyang Wu", "authors": "Qingyang Wu, Lei Li, Hao Zhou, Ying Zeng, Zhou Yu", "title": "Importance-Aware Learning for Neural Headline Editing", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social media news writers are not professionally trained. Therefore,\nsocial media platforms have to hire professional editors to adjust amateur\nheadlines to attract more readers. We propose to automate this headline editing\nprocess through neural network models to provide more immediate writing support\nfor these social media news writers. To train such a neural headline editing\nmodel, we collected a dataset which contains articles with original headlines\nand professionally edited headlines. However, it is expensive to collect a\nlarge number of professionally edited headlines. To solve this low-resource\nproblem, we design an encoder-decoder model which leverages large scale\npre-trained language models. We further improve the pre-trained model's quality\nby introducing a headline generation task as an intermediate task before the\nheadline editing task. Also, we propose Self Importance-Aware (SIA) loss to\naddress the different levels of editing in the dataset by down-weighting the\nimportance of easily classified tokens and sentences. With the help of\nPre-training, Adaptation, and SIA, the model learns to generate headlines in\nthe professional editor's style. Experimental results show that our method\nsignificantly improves the quality of headline editing comparing against\nprevious methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 06:42:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Wu", "Qingyang", ""], ["Li", "Lei", ""], ["Zhou", "Hao", ""], ["Zeng", "Ying", ""], ["Yu", "Zhou", ""]]}, {"id": "1912.01116", "submitter": "Jeremy Gordon", "authors": "Jeremy Gordon, David Rawlinson, Subutai Ahmad", "title": "Long Distance Relationships without Time Travel: Boosting the\n  Performance of a Sparse Predictive Autoencoder in Sequence Modeling", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequence learning tasks such as language modelling, Recurrent Neural\nNetworks must learn relationships between input features separated by time.\nState of the art models such as LSTM and Transformer are trained by\nbackpropagation of losses into prior hidden states and inputs held in memory.\nThis allows gradients to flow from present to past and effectively learn with\nperfect hindsight, but at a significant memory cost. In this paper we show that\nit is possible to train high performance recurrent networks using information\nthat is local in time, and thereby achieve a significantly reduced memory\nfootprint. We describe a predictive autoencoder called bRSM featuring recurrent\nconnections, sparse activations, and a boosting rule for improved cell\nutilization. The architecture demonstrates near optimal performance on a\nnon-deterministic (stochastic) partially-observable sequence learning task\nconsisting of high-Markov-order sequences of MNIST digits. We find that this\nmodel learns these sequences faster and more completely than an LSTM, and offer\nseveral possible explanations why the LSTM architecture might struggle with the\npartially observable sequence structure in this task. We also apply our model\nto a next word prediction task on the Penn Treebank (PTB) dataset. We show that\na 'flattened' RSM network, when paired with a modern semantic word embedding\nand the addition of boosting, achieves 103.5 PPL (a 20-point improvement over\nthe best N-gram models), beating ordinary RNNs trained with BPTT and\napproaching the scores of early LSTM implementations. This work provides\nencouraging evidence that strong results on challenging tasks such as language\nmodelling may be possible using less memory intensive, biologically-plausible\ntraining regimes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:00:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Gordon", "Jeremy", ""], ["Rawlinson", "David", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1912.01119", "submitter": "Khaled Jedoui", "authors": "Khaled Jedoui, Ranjay Krishna, Michael Bernstein and Li Fei-Fei", "title": "Deep Bayesian Active Learning for Multiple Correct Outputs", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical active learning strategies are designed for tasks, such as\nclassification, with the assumption that the output space is mutually\nexclusive. The assumption that these tasks always have exactly one correct\nanswer has resulted in the creation of numerous uncertainty-based measurements,\nsuch as entropy and least confidence, which operate over a model's outputs.\nUnfortunately, many real-world vision tasks, like visual question answering and\nimage captioning, have multiple correct answers, causing these measurements to\noverestimate uncertainty and sometimes perform worse than a random sampling\nbaseline. In this paper, we propose a new paradigm that estimates uncertainty\nin the model's internal hidden space instead of the model's output space. We\nspecifically study a manifestation of this problem for visual question answer\ngeneration (VQA), where the aim is not to classify the correct answer but to\nproduce a natural language answer, given an image and a question. Our method\novercomes the paraphrastic nature of language. It requires a semantic space\nthat structures the model's output concepts and that enables the usage of\ntechniques like dropout-based Bayesian uncertainty. We build a visual-semantic\nspace that embeds paraphrases close together for any existing VQA model. We\nempirically show state-of-art active learning results on the task of VQA on two\ndatasets, being 5 times more cost-efficient on Visual Genome and 3 times more\ncost-efficient on VQA 2.0.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 23:09:16 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 06:36:35 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jedoui", "Khaled", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1912.01131", "submitter": "Paulo Mann", "authors": "Paulo Mann, Aline Paes, Elton H. Matsushima", "title": "See and Read: Detecting Depression Symptoms in Higher Education Students\n  Using Multimodal Social Media Data", "comments": "This article was accepted (15 November 2019) and will appear in the\n  proceedings of ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mental disorders such as depression and anxiety have been increasing at\nalarming rates in the worldwide population. Notably, the major depressive\ndisorder has become a common problem among higher education students,\naggravated, and maybe even occasioned, by the academic pressures they must\nface. While the reasons for this alarming situation remain unclear (although\nwidely investigated), the student already facing this problem must receive\ntreatment. To that, it is first necessary to screen the symptoms. The\ntraditional way for that is relying on clinical consultations or answering\nquestionnaires. However, nowadays, the data shared at social media is a\nubiquitous source that can be used to detect the depression symptoms even when\nthe student is not able to afford or search for professional care. Previous\nworks have already relied on social media data to detect depression on the\ngeneral population, usually focusing on either posted images or texts or\nrelying on metadata. In this work, we focus on detecting the severity of the\ndepression symptoms in higher education students, by comparing deep learning to\nfeature engineering models induced from both the pictures and their captions\nposted on Instagram. The experimental results show that students presenting a\nBDI score higher or equal than 20 can be detected with 0.92 of recall and 0.69\nof precision in the best case, reached by a fusion model. Our findings show the\npotential of large-scale depression screening, which could shed light upon\nstudents at-risk.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:12:09 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 23:46:04 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Mann", "Paulo", ""], ["Paes", "Aline", ""], ["Matsushima", "Elton H.", ""]]}, {"id": "1912.01140", "submitter": "David Bamman", "authors": "David Bamman, Olivia Lewke and Anya Mansoor", "title": "An Annotated Dataset of Coreference in English Literature", "comments": null, "journal-ref": "Proceedings of the 12th Language Resources and Evaluation\n  Conference (LREC 2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this work a new dataset of coreference annotations for works of\nliterature in English, covering 29,103 mentions in 210,532 tokens from 100\nworks of fiction. This dataset differs from previous coreference datasets in\ncontaining documents whose average length (2,105.3 words) is four times longer\nthan other benchmark datasets (463.7 for OntoNotes), and contains examples of\ndifficult coreference problems common in literature. This dataset allows for an\nevaluation of cross-domain performance for the task of coreference resolution,\nand analysis into the characteristics of long-distance within-document\ncoreference.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:58:01 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 16:37:21 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bamman", "David", ""], ["Lewke", "Olivia", ""], ["Mansoor", "Anya", ""]]}, {"id": "1912.01156", "submitter": "Octavia-Maria ?ulea", "authors": "Octavia-Maria Sulea, Steve Young", "title": "Unsupervised Inflection Generation Using Neural Language Modeling", "comments": "International Work-Conference on Artificial Neural Networks 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Deep Neural Network architectures for Language Modeling has\nrecently seen a tremendous increase in interest in the field of NLP with the\nadvent of transfer learning and the shift in focus from rule-based and\npredictive models (supervised learning) to generative or unsupervised models to\nsolve the long-standing problems in NLP like Information Extraction or Question\nAnswering. While this shift has worked greatly for languages lacking in\ninflectional morphology, such as English, challenges still arise when trying to\nbuild similar systems for morphologically-rich languages, since their\nindividual words shift forms in context more often. In this paper we\ninvestigate the extent to which these new unsupervised or generative techniques\ncan serve to alleviate the type-token ratio disparity in morphologically rich\nlanguages. We apply an off-the-shelf neural language modeling library to the\nnewly introduced task of unsupervised inflection generation in the nominal\ndomain of three morphologically rich languages: Romanian, German, and Finnish.\nWe show that this neural language model architecture can successfully generate\nthe full inflection table of nouns without needing any pre-training on large,\nwikipedia-sized corpora, as long as the model is shown enough inflection\nexamples. In fact, our experiments show that pre-training hinders the\ngeneration performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:25:16 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Sulea", "Octavia-Maria", ""], ["Young", "Steve", ""]]}, {"id": "1912.01214", "submitter": "Zhirui Zhang", "authors": "Baijun Ji, Zhirui Zhang, Xiangyu Duan, Min Zhang, Boxing Chen and\n  Weihua Luo", "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine\n  Translation", "comments": "Accepted as a conference paper at AAAI 2020 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning between different language pairs has shown its\neffectiveness for Neural Machine Translation (NMT) in low-resource scenario.\nHowever, existing transfer methods involving a common target language are far\nfrom success in the extreme scenario of zero-shot translation, due to the\nlanguage space mismatch problem between transferor (the parent model) and\ntransferee (the child model) on the source side. To address this challenge, we\npropose an effective transfer learning approach based on cross-lingual\npre-training. Our key idea is to make all source languages share the same\nfeature space and thus enable a smooth transition for zero-shot translation. To\nthis end, we introduce one monolingual pre-training method and two bilingual\npre-training methods to obtain a universal encoder for different languages.\nOnce the universal encoder is constructed, the parent model built on such\nencoder is trained with large-scale annotated data and then directly applied in\nzero-shot translation scenario. Experiments on two public datasets show that\nour approach significantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:41:03 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ji", "Baijun", ""], ["Zhang", "Zhirui", ""], ["Duan", "Xiangyu", ""], ["Zhang", "Min", ""], ["Chen", "Boxing", ""], ["Luo", "Weihua", ""]]}, {"id": "1912.01218", "submitter": "Daan Van Esch", "authors": "Daan van Esch, Elnaz Sarbar, Tamar Lucassen, Jeremy O'Brien, Theresa\n  Breiner, Manasa Prasad, Evan Crew, Chieu Nguyen, Fran\\c{c}oise Beaufays", "title": "Writing Across the World's Languages: Deep Internationalization for\n  Gboard, the Google Keyboard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report describes our deep internationalization program for\nGboard, the Google Keyboard. Today, Gboard supports 900+ language varieties\nacross 70+ writing systems, and this report describes how and why we have been\nadding support for hundreds of language varieties from around the globe. Many\nlanguages of the world are increasingly used in writing on an everyday basis,\nand we describe the trends we see. We cover technological and logistical\nchallenges in scaling up a language technology product like Gboard to hundreds\nof language varieties, and describe how we built systems and processes to\noperate at scale. Finally, we summarize the key take-aways from user studies we\nran with speakers of hundreds of languages from around the world.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:56:15 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["van Esch", "Daan", ""], ["Sarbar", "Elnaz", ""], ["Lucassen", "Tamar", ""], ["O'Brien", "Jeremy", ""], ["Breiner", "Theresa", ""], ["Prasad", "Manasa", ""], ["Crew", "Evan", ""], ["Nguyen", "Chieu", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1912.01219", "submitter": "Wei Ping", "authors": "Wei Ping, Kainan Peng, Kexin Zhao, Zhao Song", "title": "WaveFlow: A Compact Flow-based Model for Raw Audio", "comments": "Published at ICML 2020. Code and pre-trained models:\n  https://github.com/PaddlePaddle/Parakeet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose WaveFlow, a small-footprint generative flow for raw\naudio, which is directly trained with maximum likelihood. It handles the\nlong-range structure of 1-D waveform with a dilated 2-D convolutional\narchitecture, while modeling the local variations using expressive\nautoregressive functions. WaveFlow provides a unified view of likelihood-based\nmodels for 1-D data, including WaveNet and WaveGlow as special cases. It\ngenerates high-fidelity speech as WaveNet, while synthesizing several orders of\nmagnitude faster as it only requires a few sequential steps to generate very\nlong waveforms with hundreds of thousands of time-steps. Furthermore, it can\nsignificantly reduce the likelihood gap that has existed between autoregressive\nmodels and flow-based models for efficient synthesis. Finally, our\nsmall-footprint WaveFlow has only 5.91M parameters, which is 15$\\times$ smaller\nthan WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6$\\times$\nfaster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered\ninference kernels.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 07:00:13 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 21:45:31 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 21:35:04 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 20:10:12 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Ping", "Wei", ""], ["Peng", "Kainan", ""], ["Zhao", "Kexin", ""], ["Song", "Zhao", ""]]}, {"id": "1912.01220", "submitter": "Jose Camacho-Collados", "authors": "Zied Bouraoui, Jose Camacho-Collados, Luis Espinosa-Anke and Steven\n  Schockaert", "title": "Modelling Semantic Categories using Conceptual Neighborhood", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many methods for learning vector space embeddings have been proposed in\nthe field of Natural Language Processing, these methods typically do not\ndistinguish between categories and individuals. Intuitively, if individuals are\nrepresented as vectors, we can think of categories as (soft) regions in the\nembedding space. Unfortunately, meaningful regions can be difficult to\nestimate, especially since we often have few examples of individuals that\nbelong to a given category. To address this issue, we rely on the fact that\ndifferent categories are often highly interdependent. In particular, categories\noften have conceptual neighbors, which are disjoint from but closely related to\nthe given category (e.g.\\ fruit and vegetable). Our hypothesis is that more\naccurate category representations can be learned by relying on the assumption\nthat the regions representing such conceptual neighbors should be adjacent in\nthe embedding space. We propose a simple method for identifying conceptual\nneighbors and then show that incorporating these conceptual neighbors indeed\nleads to more accurate region based representations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 07:02:38 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bouraoui", "Zied", ""], ["Camacho-Collados", "Jose", ""], ["Espinosa-Anke", "Luis", ""], ["Schockaert", "Steven", ""]]}, {"id": "1912.01252", "submitter": "Sven Banisch", "authors": "Tom Willaert, Sven Banisch, Paul Van Eecke, Katrien Beuls", "title": "Facilitating on-line opinion dynamics by mining expressions of\n  causation. The case of climate change debates on The Guardian", "comments": "This project has received funding from the European Union's Horizon\n  2020 research and innovation programme under grant agreement No 732942\n  (Opinion Dynamics and Cultural Conflict in European Spaces --\n  www.Odycceus.eu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News website comment sections are spaces where potentially conflicting\nopinions and beliefs are voiced. Addressing questions of how to study such\ncultural and societal conflicts through technological means, the present\narticle critically examines possibilities and limitations of machine-guided\nexploration and potential facilitation of on-line opinion dynamics. These\ninvestigations are guided by a discussion of an experimental observatory for\nmining and analyzing opinions from climate change-related user comments on news\narticles from the TheGuardian.com. This observatory combines causal mapping\nmethods with computational text analysis in order to mine beliefs and visualize\nopinion landscapes based on expressions of causation. By (1) introducing\ndigital methods and open infrastructures for data exploration and analysis and\n(2) engaging in debates about the implications of such methods and\ninfrastructures, notably in terms of the leap from opinion observation to\ndebate facilitation, the article aims to make a practical and theoretical\ncontribution to the study of opinion dynamics and conflict in new media\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:20:41 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Willaert", "Tom", ""], ["Banisch", "Sven", ""], ["Van Eecke", "Paul", ""], ["Beuls", "Katrien", ""]]}, {"id": "1912.01385", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Markus Zlabinger, Allan Hanbury", "title": "TU Wien @ TREC Deep Learning '19 -- Simple Contextualization for\n  Re-ranking", "comments": "Presented at TREC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of neural network models puts multiple objectives in conflict with\neach other: Ideally we would like to create a neural model that is effective,\nefficient, and interpretable at the same time. However, in most instances we\nhave to choose which property is most important to us. We used the opportunity\nof the TREC 2019 Deep Learning track to evaluate the effectiveness of a\nbalanced neural re-ranking approach. We submitted results of the TK\n(Transformer-Kernel) model: a neural re-ranking model for ad-hoc search using\nan efficient contextualization mechanism. TK employs a very small number of\nlightweight Transformer layers to contextualize query and document word\nembeddings. To score individual term interactions, we use a document-length\nenhanced kernel-pooling, which enables users to gain insight into the model.\nOur best result for the passage ranking task is: 0.420 MAP, 0.671 nDCG, 0.598\nP@10 (TUW19-p3 full). Our best result for the document ranking task is: 0.271\nMAP, 0.465 nDCG, 0.730 P@10 (TUW19-d3 re-ranking).\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:19:20 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Zlabinger", "Markus", ""], ["Hanbury", "Allan", ""]]}, {"id": "1912.01389", "submitter": "Taesun Moon", "authors": "Taesun Moon, Parul Awasthy, Jian Ni, Radu Florian", "title": "Towards Lingua Franca Named Entity Recognition with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction is an important task in NLP, enabling the automatic\nextraction of data for relational database filling. Historically, research and\ndata was produced for English text, followed in subsequent years by datasets in\nArabic, Chinese (ACE/OntoNotes), Dutch, Spanish, German (CoNLL evaluations),\nand many others. The natural tendency has been to treat each language as a\ndifferent dataset and build optimized models for each. In this paper we\ninvestigate a single Named Entity Recognition model, based on a multilingual\nBERT, that is trained jointly on many languages simultaneously, and is able to\ndecode these languages with better accuracy than models trained only on one\nlanguage. To improve the initial model, we study the use of regularization\nstrategies such as multitask learning and partial gradient updates. In addition\nto being a single model that can tackle multiple languages (including code\nswitch), the model could be used to make zero-shot predictions on a new\nlanguage, even ones for which training data is not available, out of the box.\nThe results show that this model not only performs competitively with\nmonolingual models, but it also achieves state-of-the-art results on the\nCoNLL02 Dutch and Spanish datasets, OntoNotes Arabic and Chinese datasets.\nMoreover, it performs reasonably well on unseen languages, achieving\nstate-of-the-art for zero-shot on three CoNLL languages.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:48:02 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:23:41 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Moon", "Taesun", ""], ["Awasthy", "Parul", ""], ["Ni", "Jian", ""], ["Florian", "Radu", ""]]}, {"id": "1912.01452", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Modar Alfadly, Bernard Ghanem, Marcel Worring", "title": "Assessing the Robustness of Visual Question Answering", "comments": "24 pages, 13 figures, International Journal of Computer Vision (IJCV)\n  [under review]. arXiv admin note: substantial text overlap with\n  arXiv:1711.06232, arXiv:1709.04625", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been playing an essential role in the task of\nVisual Question Answering (VQA). Until recently, their accuracy has been the\nmain focus of research. Now there is a trend toward assessing the robustness of\nthese models against adversarial attacks by evaluating the accuracy of these\nmodels under increasing levels of noisiness in the inputs of VQA models. In\nVQA, the attack can target the image and/or the proposed query question, dubbed\nmain question, and yet there is a lack of proper analysis of this aspect of\nVQA. In this work, we propose a new method that uses semantically related\nquestions, dubbed basic questions, acting as noise to evaluate the robustness\nof VQA models. We hypothesize that as the similarity of a basic question to the\nmain question decreases, the level of noise increases. To generate a reasonable\nnoise level for a given main question, we rank a pool of basic questions based\non their similarity with this main question. We cast this ranking problem as a\nLASSO optimization problem. We also propose a novel robustness measure Rscore\nand two large-scale basic question datasets in order to standardize robustness\nanalysis of VQA models. The experimental results demonstrate that the proposed\nevaluation method is able to effectively analyze the robustness of VQA models.\nTo foster the VQA research, we will publish our proposed datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 09:32:38 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Alfadly", "Modar", ""], ["Ghanem", "Bernard", ""], ["Worring", "Marcel", ""]]}, {"id": "1912.01496", "submitter": "Chao-Chun Hsu", "authors": "Chao-Chun Hsu, Zi-Yuan Chen, Chi-Yang Hsu, Chih-Chia Li, Tzu-Yuan Lin,\n  Ting-Hao 'Kenneth' Huang, Lun-Wei Ku", "title": "Knowledge-Enriched Visual Storytelling", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stories are diverse and highly personalized, resulting in a large possible\noutput space for story generation. Existing end-to-end approaches produce\nmonotonous stories because they are limited to the vocabulary and knowledge in\na single training dataset. This paper introduces KG-Story, a three-stage\nframework that allows the story generation model to take advantage of external\nKnowledge Graphs to produce interesting stories. KG-Story distills a set of\nrepresentative words from the input prompts, enriches the word set by using\nexternal knowledge graphs, and finally generates stories based on the enriched\nword set. This distill-enrich-generate framework allows the use of external\nresources not only for the enrichment phase, but also for the distillation and\ngeneration phases. In this paper, we show the superiority of KG-Story for\nvisual storytelling, where the input prompt is a sequence of five photos and\nthe output is a short story. Per the human ranking evaluation, stories\ngenerated by KG-Story are on average ranked better than that of the\nstate-of-the-art systems. Our code and output stories are available at\nhttps://github.com/zychen423/KE-VIST.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:16:13 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Hsu", "Chao-Chun", ""], ["Chen", "Zi-Yuan", ""], ["Hsu", "Chi-Yang", ""], ["Li", "Chih-Chia", ""], ["Lin", "Tzu-Yuan", ""], ["Huang", "Ting-Hao 'Kenneth'", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1912.01521", "submitter": "Oren Barkan", "authors": "Oren Barkan", "title": "Multiscale Self Attentive Convolutions for Vision and Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self attention mechanisms have become a key building block in many\nstate-of-the-art language understanding models. In this paper, we show that the\nself attention operator can be formulated in terms of 1x1 convolution\noperations. Following this observation, we propose several novel operators:\nFirst, we introduce a 2D version of self attention that is applicable for 2D\nsignals such as images. Second, we present the 1D and 2D Self Attentive\nConvolutions (SAC) operator that generalizes self attention beyond 1x1\nconvolutions to 1xm and nxm convolutions, respectively. While 1D and 2D self\nattention operate on individual words and pixels, SAC operates on m-grams and\nimage patches, respectively. Third, we present a multiscale version of SAC\n(MSAC) which analyzes the input by employing multiple SAC operators that vary\nby filter size, in parallel. Finally, we explain how MSAC can be utilized for\nvision and language modeling, and further harness MSAC to form a cross\nattentive image similarity machinery.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:51:09 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Barkan", "Oren", ""]]}, {"id": "1912.01580", "submitter": "Thanapapas Horsuwan", "authors": "Thanapapas Horsuwan, Kasidis Kanwatchara, Peerapon Vateekul, Boonserm\n  Kijsirikul", "title": "A Comparative Study of Pretrained Language Models on Thai Social Text\n  Categorization", "comments": "12 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ever-growing volume of data of user-generated content on social media\nprovides a nearly unlimited corpus of unlabeled data even in languages where\nresources are scarce. In this paper, we demonstrate that state-of-the-art\nresults on two Thai social text categorization tasks can be realized by\npretraining a language model on a large noisy Thai social media corpus of over\n1.26 billion tokens and later fine-tuned on the downstream classification\ntasks. Due to the linguistically noisy and domain-specific nature of the\ncontent, our unique data preprocessing steps designed for Thai social media\nwere utilized to ease the training comprehension of the model. We compared four\nmodern language models: ULMFiT, ELMo with biLSTM, OpenAI GPT, and BERT. We\nsystematically compared the models across different dimensions including speed\nof pretraining and fine-tuning, perplexity, downstream classification\nbenchmarks, and performance in limited pretraining data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:26:13 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 07:47:56 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Horsuwan", "Thanapapas", ""], ["Kanwatchara", "Kasidis", ""], ["Vateekul", "Peerapon", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1912.01586", "submitter": "Tongfei Chen", "authors": "Yunmo Chen, Tongfei Chen, Seth Ebner, Aaron Steven White, Benjamin Van\n  Durme", "title": "Reading the Manual: Event Extraction as Definition Comprehension", "comments": "Accepted at the EMNLP 2020 Workshop on Structured Prediction for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We ask whether text understanding has progressed to where we may extract\nevent information through incremental refinement of bleached statements derived\nfrom annotation manuals. Such a capability would allow for the trivial\nconstruction and extension of an extraction framework by intended end-users\nthrough declarations such as, \"Some person was born in some location at some\ntime.\" We introduce an example of a model that employs such statements, with\nexperiments illustrating we can extract events under closed ontologies and\ngeneralize to unseen event types simply by reading new definitions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:31:42 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 02:27:24 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chen", "Yunmo", ""], ["Chen", "Tongfei", ""], ["Ebner", "Seth", ""], ["White", "Aaron Steven", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1912.01673", "submitter": "Petra Baran\\v{c}\\'ikov\\'a", "authors": "Petra Barancikova, Ondrej Bojar", "title": "COSTRA 1.0: A Dataset of Complex Sentence Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present COSTRA 1.0, a dataset of complex sentence transformations. The\ndataset is intended for the study of sentence-level embeddings beyond simple\nword alternations or standard paraphrasing. This first version of the dataset\nis limited to sentences in Czech but the construction method is universal and\nwe plan to use it also for other languages. The dataset consist of 4,262 unique\nsentences with average length of 10 words, illustrating 15 types of\nmodifications such as simplification, generalization, or formal and informal\nlanguage variation. The hope is that with this dataset, we should be able to\ntest semantic properties of sentence embeddings and perhaps even to find some\ntopologically interesting 'skeleton' in the sentence embedding space. A\npreliminary analysis using LASER, multi-purpose multi-lingual sentence\nembeddings suggests that the LASER space does not exhibit the desired\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:20:31 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 07:32:00 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Barancikova", "Petra", ""], ["Bojar", "Ondrej", ""]]}, {"id": "1912.01679", "submitter": "Julian Salazar", "authors": "Shaoshi Ling, Yuzong Liu, Julian Salazar, Katrin Kirchhoff", "title": "Deep Contextualized Acoustic Representations For Semi-Supervised Speech\n  Recognition", "comments": "Accepted to ICASSP 2020 (oral)", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053176", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to semi-supervised automatic speech recognition\n(ASR). We first exploit a large amount of unlabeled audio data via\nrepresentation learning, where we reconstruct a temporal slice of filterbank\nfeatures from past and future context frames. The resulting deep contextualized\nacoustic representations (DeCoAR) are then used to train a CTC-based end-to-end\nASR system using a smaller amount of labeled audio data. In our experiments, we\nshow that systems trained on DeCoAR consistently outperform ones trained on\nconventional filterbank features, giving 42% and 19% relative improvement over\nthe baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our\napproach can drastically reduce the amount of labeled data required;\nunsupervised training on LibriSpeech then supervision with 100 hours of labeled\ndata achieves performance on par with training on all 960 hours directly.\nPre-trained models and code will be released online.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:32:50 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 17:55:35 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Ling", "Shaoshi", ""], ["Liu", "Yuzong", ""], ["Salazar", "Julian", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "1912.01682", "submitter": "Lisa Jin", "authors": "Lisa Jin and Daniel Gildea", "title": "AMR-to-Text Generation with Cache Transition Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation from AMR involves emitting sentences that reflect the meaning\nof their AMR annotations. Neural sequence-to-sequence models have successfully\nbeen used to decode strings from flattened graphs (e.g., using depth-first or\nrandom traversal). Such models often rely on attention-based decoders to map\nAMR node to English token sequences. Instead of linearizing AMR, we directly\nencode its graph structure and delegate traversal to the decoder. To enforce a\nsentence-aligned graph traversal and provide local graph context, we predict\ntransition-based parser actions in addition to English words. We present two\nmodel variants: one generates parser actions prior to words, while the other\ninterleaves actions with words.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:45:04 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Jin", "Lisa", ""], ["Gildea", "Daniel", ""]]}, {"id": "1912.01706", "submitter": "Nicolas Garneau", "authors": "Nicolas Garneau, Mathieu Godbout, David Beauchemin, Audrey Durand, Luc\n  Lamontagne", "title": "A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual\n  Mappings of Word Embeddings: Making the Method Robustly Reproducible as Well", "comments": "Accept in REPROLANG@LREC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we reproduce the experiments of Artetxe et al. (2018b)\nregarding the robust self-learning method for fully unsupervised cross-lingual\nmappings of word embeddings. We show that the reproduction of their method is\nindeed feasible with some minor assumptions. We further investigate the\nrobustness of their model by introducing four new languages that are less\nsimilar to English than the ones proposed by the original paper. In order to\nassess the stability of their model, we also conduct a grid search over\nsensible hyperparameters. We then propose key recommendations applicable to any\nresearch project in order to deliver fully reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:07:47 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 14:30:50 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Garneau", "Nicolas", ""], ["Godbout", "Mathieu", ""], ["Beauchemin", "David", ""], ["Durand", "Audrey", ""], ["Lamontagne", "Luc", ""]]}, {"id": "1912.01728", "submitter": "Akshit Tyagi", "authors": "Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang,\n  Zihang Wang, Bill Campbell", "title": "Fast Intent Classification for Spoken Language Understanding", "comments": "Accepted as a conference paper at ICASSP 20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Understanding (SLU) systems consist of several machine\nlearning components operating together (e.g. intent classification, named\nentity recognition and resolution). Deep learning models have obtained state of\nthe art results on several of these tasks, largely attributed to their better\nmodeling capacity. However, an increase in modeling capacity comes with added\ncosts of higher latency and energy usage, particularly when operating on low\ncomplexity devices. To address the latency and computational complexity issues,\nwe explore a BranchyNet scheme on an intent classification scheme within SLU\nsystems. The BranchyNet scheme when applied to a high complexity model, adds\nexit points at various stages in the model allowing early decision making for a\nset of queries to the SLU model. We conduct experiments on the Facebook\nSemantic Parsing dataset with two candidate model architectures for intent\nclassification. Our experiments show that the BranchyNet scheme provides gains\nin terms of computational complexity without compromising model accuracy. We\nalso conduct analytical studies regarding the improvements in the computational\ncost, distribution of utterances that egress from various exit points and the\nimpact of adding more complexity to models with the BranchyNet scheme.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 22:46:59 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 07:58:08 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Tyagi", "Akshit", ""], ["Sharma", "Varun", ""], ["Gupta", "Rahul", ""], ["Samson", "Lynn", ""], ["Zhuang", "Nan", ""], ["Wang", "Zihang", ""], ["Campbell", "Bill", ""]]}, {"id": "1912.01731", "submitter": "Shifeng Liu", "authors": "Shifeng Liu, Yifang Sun, Bing Li, Wei Wang, Xiang Zhao", "title": "HAMNER: Headword Amplified Multi-span Distantly Supervised Method for\n  Domain Specific Named Entity Recognition", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle Named Entity Recognition (NER) tasks, supervised methods need to\nobtain sufficient cleanly annotated data, which is labor and time consuming. On\nthe contrary, distantly supervised methods acquire automatically annotated data\nusing dictionaries to alleviate this requirement. Unfortunately, dictionaries\nhinder the effectiveness of distantly supervised methods for NER due to its\nlimited coverage, especially in specific domains. In this paper, we aim at the\nlimitations of the dictionary usage and mention boundary detection. We\ngeneralize the distant supervision by extending the dictionary with headword\nbased non-exact matching. We apply a function to better weight the matched\nentity mentions. We propose a span-level model, which classifies all the\npossible spans then infers the selected spans with a proposed dynamic\nprogramming algorithm. Experiments on all three benchmark datasets demonstrate\nthat our method outperforms previous state-of-the-art distantly supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 23:00:38 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Liu", "Shifeng", ""], ["Sun", "Yifang", ""], ["Li", "Bing", ""], ["Wang", "Wei", ""], ["Zhao", "Xiang", ""]]}, {"id": "1912.01734", "submitter": "Jesse Thomason", "authors": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson\n  Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday\n  Tasks", "comments": "Computer Vision and Pattern Recognition (CVPR) 2020 ;\n  https://askforalfred.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ALFRED (Action Learning From Realistic Environments and\nDirectives), a benchmark for learning a mapping from natural language\ninstructions and egocentric vision to sequences of actions for household tasks.\nALFRED includes long, compositional tasks with non-reversible state changes to\nshrink the gap between research benchmarks and real-world applications. ALFRED\nconsists of expert demonstrations in interactive visual environments for 25k\nnatural language directives. These directives contain both high-level goals\nlike \"Rinse off a mug and place it in the coffee maker.\" and low-level language\ninstructions like \"Walk to the coffee maker on the right.\" ALFRED tasks are\nmore complex in terms of sequence length, action space, and language than\nexisting vision-and-language task datasets. We show that a baseline model based\non recent embodied vision-and-language tasks performs poorly on ALFRED,\nsuggesting that there is significant room for developing innovative grounded\nvisual language understanding models with this benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 23:18:59 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 01:18:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Shridhar", "Mohit", ""], ["Thomason", "Jesse", ""], ["Gordon", "Daniel", ""], ["Bisk", "Yonatan", ""], ["Han", "Winson", ""], ["Mottaghi", "Roozbeh", ""], ["Zettlemoyer", "Luke", ""], ["Fox", "Dieter", ""]]}, {"id": "1912.01772", "submitter": "Antonios Anastasopoulos", "authors": "Mingjun Duan, Carlos Fasola, Sai Krishna Rallabandi, Rodolfo M. Vega,\n  Antonios Anastasopoulos, Lori Levin, Alan W Black", "title": "A Resource for Computational Experiments on Mapudungun", "comments": "accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a resource for computational experiments on Mapudungun, a\npolysynthetic indigenous language spoken in Chile with upwards of 200 thousand\nspeakers. We provide 142 hours of culturally significant conversations in the\ndomain of medical treatment. The conversations are fully transcribed and\ntranslated into Spanish. The transcriptions also include annotations for\ncode-switching and non-standard pronunciations. We also provide baseline\nresults on three core NLP tasks: speech recognition, speech synthesis, and\nmachine translation between Spanish and Mapudungun. We further explore other\napplications for which the corpus will be suitable, including the study of\ncode-switching, historical orthography change, linguistic structure, and\nsociological and anthropological studies.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 02:26:39 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 03:27:12 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Duan", "Mingjun", ""], ["Fasola", "Carlos", ""], ["Rallabandi", "Sai Krishna", ""], ["Vega", "Rodolfo M.", ""], ["Anastasopoulos", "Antonios", ""], ["Levin", "Lori", ""], ["Black", "Alan W", ""]]}, {"id": "1912.01774", "submitter": "Rongxiang Weng", "authors": "Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, Weihua Luo", "title": "Acquiring Knowledge from Pre-trained Model to Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training and fine-tuning have achieved great success in the natural\nlanguage process field. The standard paradigm of exploiting them includes two\nsteps: first, pre-training a model, e.g. BERT, with a large scale unlabeled\nmonolingual data. Then, fine-tuning the pre-trained model with labeled data\nfrom downstream tasks. However, in neural machine translation (NMT), we address\nthe problem that the training objective of the bilingual task is far different\nfrom the monolingual pre-trained model. This gap leads that only using\nfine-tuning in NMT can not fully utilize prior language knowledge. In this\npaper, we propose an APT framework for acquiring knowledge from the pre-trained\nmodel to NMT. The proposed approach includes two modules: 1). a dynamic fusion\nmechanism to fuse task-specific features adapted from general knowledge into\nNMT network, 2). a knowledge distillation paradigm to learn language knowledge\ncontinuously during the NMT training process. The proposed approach could\nintegrate suitable knowledge from pre-trained models to improve the NMT.\nExperimental results on WMT English to German, German to English and Chinese to\nEnglish machine translation tasks show that our model outperforms strong\nbaselines and the fine-tuning counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 02:54:18 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Weng", "Rongxiang", ""], ["Yu", "Heng", ""], ["Huang", "Shujian", ""], ["Cheng", "Shanbo", ""], ["Luo", "Weihua", ""]]}, {"id": "1912.01777", "submitter": "Ye Bai", "authors": "Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengqi Wen, Zhengkun Tian, Shuai\n  Zhang", "title": "Integrating Knowledge into End-to-End Speech Recognition from External\n  Text-Only Data", "comments": "Submitted TASLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder-decoder (AED) models have achieved promising\nperformance in speech recognition. However, because of the end-to-end training,\nan AED model is usually trained with speech-text paired data. It is challenging\nto incorporate external text-only data into AED models. Another issue of the\nAED model is that it does not use the right context of a text token while\npredicting the token. To alleviate the above two issues, we propose a unified\nmethod called LST (Learn Spelling from Teachers) to integrate knowledge into an\nAED model from the external text-only data and leverage the whole context in a\nsentence. The method is divided into two stages. First, in the representation\nstage, a language model is trained on the text. It can be seen as that the\nknowledge in the text is compressed into the LM. Then, at the transferring\nstage, the knowledge is transferred to the AED model via teacher-student\nlearning. To further use the whole context of the text sentence, we propose an\nLM called causal cloze completer (COR), which estimates the probability of a\ntoken, given both the left context and the right context of it. Therefore, with\nLST training, the AED model can leverage the whole context in the sentence.\nDifferent from fusion based methods, which use LM during decoding, the proposed\nmethod does not increase any extra complexity at the inference stage. We\nconduct experiments on two scales of public Chinese datasets AISHELL-1 and\nAISHELL-2. The experimental results demonstrate the effectiveness of leveraging\nexternal text-only data and the whole context in a sentence with our proposed\nmethod, compared with baseline hybrid systems and AED model based systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 03:12:27 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:44:43 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Bai", "Ye", ""], ["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Wen", "Zhengqi", ""], ["Tian", "Zhengkun", ""], ["Zhang", "Shuai", ""]]}, {"id": "1912.01795", "submitter": "Fanchao Qi", "authors": "Fanchao Qi, Liang Chang, Maosong Sun, Sicong Ouyang, Zhiyuan Liu", "title": "Towards Building a Multilingual Sememe Knowledge Base: Predicting\n  Sememes for BabelNet Synsets", "comments": "Accepted by AAAI Conference on Artificial Intelligence 2020 for oral\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sememe is defined as the minimum semantic unit of human languages. Sememe\nknowledge bases (KBs), which contain words annotated with sememes, have been\nsuccessfully applied to many NLP tasks. However, existing sememe KBs are built\non only a few languages, which hinders their widespread utilization. To address\nthe issue, we propose to build a unified sememe KB for multiple languages based\non BabelNet, a multilingual encyclopedic dictionary. We first build a dataset\nserving as the seed of the multilingual sememe KB. It manually annotates\nsememes for over $15$ thousand synsets (the entries of BabelNet). Then, we\npresent a novel task of automatic sememe prediction for synsets, aiming to\nexpand the seed dataset into a usable KB. We also propose two simple and\neffective models, which exploit different information of synsets. Finally, we\nconduct quantitative and qualitative analyses to explore important factors and\ndifficulties in the task. All the source code and data of this work can be\nobtained on https://github.com/thunlp/BabelNet-Sememe-Prediction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 04:39:32 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Qi", "Fanchao", ""], ["Chang", "Liang", ""], ["Sun", "Maosong", ""], ["Ouyang", "Sicong", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1912.01831", "submitter": "Preslav Nakov", "authors": "Evgeni Stefchov, Galia Angelova, Preslav Nakov", "title": "Towards Constructing a Corpus for Studying the Effects of Treatments and\n  Substances Reported in PubMed Abstracts", "comments": "medical relation extraction, rationale extraction, effects and\n  treatments, bioNLP", "journal-ref": "AIMSA-2016: The 17th International Conference on Artificial\n  Intelligence: Methodology, Systems, Applications", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the construction of an annotated corpus of PubMed abstracts\nreporting about positive, negative or neutral effects of treatments or\nsubstances. Our ultimate goal is to annotate one sentence (rationale) for each\nabstract and to use this resource as a training set for text classification of\neffects discussed in PubMed abstracts. Currently, the corpus consists of 750\nabstracts. We describe the automatic processing that supports the corpus\nconstruction, the manual annotation activities and some features of the medical\nlanguage in the abstracts selected for the annotated corpus. It turns out that\nrecognizing the terminology and the abbreviations is key for determining the\nrationale sentence. The corpus will be applied to improve our classifier, which\ncurrently has accuracy of 78.80% achieved with normalization of the abstract\nterms based on UMLS concepts from specific semantic groups and an SVM with a\nlinear kernel. Finally, we discuss some other possible applications of this\ncorpus.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:22:32 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Stefchov", "Evgeni", ""], ["Angelova", "Galia", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.01852", "submitter": "Chengqi Deng", "authors": "Chengqi Deng, Chengzhu Yu, Heng Lu, Chao Weng, Dong Yu", "title": "PitchNet: Unsupervised Singing Voice Conversion with Pitch Adversarial\n  Network", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing voice conversion is to convert a singer's voice to another one's\nvoice without changing singing content. Recent work shows that unsupervised\nsinging voice conversion can be achieved with an autoencoder-based approach\n[1]. However, the converted singing voice can be easily out of key, showing\nthat the existing approach cannot model the pitch information precisely. In\nthis paper, we propose to advance the existing unsupervised singing voice\nconversion method proposed in [1] to achieve more accurate pitch translation\nand flexible pitch manipulation. Specifically, the proposed PitchNet added an\nadversarially trained pitch regression network to enforce the encoder network\nto learn pitch invariant phoneme representation, and a separate module to feed\npitch extracted from the source audio to the decoder network. Our evaluation\nshows that the proposed method can greatly improve the quality of the converted\nsinging voice (2.92 vs 3.75 in MOS). We also demonstrate that the pitch of\nconverted singing can be easily controlled during generation by changing the\nlevels of the extracted pitch before passing it to the decoder network.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 08:56:13 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 07:20:24 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Deng", "Chengqi", ""], ["Yu", "Chengzhu", ""], ["Lu", "Heng", ""], ["Weng", "Chao", ""], ["Yu", "Dong", ""]]}, {"id": "1912.01858", "submitter": "Hao Wang", "authors": "Qiongxing Tao, Xiangfeng Luo, Hao Wang", "title": "Enhancing Relation Extraction Using Syntactic Indicators and Sentential\n  Contexts", "comments": "Accepted at ICTAI 2019: The IEEE International Conference on Tools\n  with Artificial Intelligence (ICTAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for relation extraction consider the sentential\ncontext by modeling the entire sentence. However, syntactic indicators, certain\nphrases or words like prepositions that are more informative than other words\nand may be beneficial for identifying semantic relations. Other approaches\nusing fixed text triggers capture such information but ignore the lexical\ndiversity. To leverage both syntactic indicators and sentential contexts, we\npropose an indicator-aware approach for relation extraction. Firstly, we\nextract syntactic indicators under the guidance of syntactic knowledge. Then we\nconstruct a neural network to incorporate both syntactic indicators and the\nentire sentences into better relation representations. By this way, the\nproposed model alleviates the impact of noisy information from entire sentences\nand breaks the limit of text triggers. Experiments on the SemEval-2010 Task 8\nbenchmark dataset show that our model significantly outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 09:16:36 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Tao", "Qiongxing", ""], ["Luo", "Xiangfeng", ""], ["Wang", "Hao", ""]]}, {"id": "1912.01957", "submitter": "Chundra Cathcart", "authors": "Chundra A. Cathcart", "title": "A probabilistic assessment of the Indo-Aryan Inner-Outer Hypothesis", "comments": "To appear in Journal of Historical Linguistics 10.1\n  https://www.jbe-platform.com/content/journals/22102124", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a novel data-driven probabilistic approach to address the\ncentury-old Inner-Outer hypothesis of Indo-Aryan. I develop a Bayesian\nhierarchical mixed-membership model to assess the validity of this hypothesis\nusing a large data set of automatically extracted sound changes operating\nbetween Old Indo-Aryan and Modern Indo-Aryan speech varieties. I employ\ndifferent prior distributions in order to model sound change, one of which, the\nlogistic normal distribution, has not received much attention in linguistics\noutside of Natural Language Processing, despite its many attractive features. I\nfind evidence for cohesive dialect groups that have made their imprint on\ncontemporary Indo-Aryan languages, and find that when a logistic normal prior\nis used, the distribution of dialect components across languages is largely\ncompatible with a core-periphery pattern similar to that proposed under the\nInner-Outer hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 20:09:15 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Cathcart", "Chundra A.", ""]]}, {"id": "1912.01972", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Llu\\'is M\\`arquez, Alessandro Moschitti, Walid Magdy,\n  Hamdy Mubarak, Abed Alhakim Freihat, James Glass, Bilal Randeree", "title": "SemEval-2016 Task 3: Community Question Answering", "comments": "community question answering, question-question similarity,\n  question-comment similarity, answer reranking, English, Arabic. arXiv admin\n  note: substantial text overlap with arXiv:1912.00730", "journal-ref": "SemEval-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the SemEval--2016 Task 3 on Community Question\nAnswering, which we offered in English and Arabic. For English, we had three\nsubtasks: Question--Comment Similarity (subtask A), Question--Question\nSimilarity (B), and Question--External Comment Similarity (C). For Arabic, we\nhad another subtask: Rerank the correct answers for a new question (D).\nEighteen teams participated in the task, submitting a total of 95 runs (38\nprimary and 57 contrastive) for the four subtasks. A variety of approaches and\nfeatures were used by the participating systems to address the different\nsubtasks, which are summarized in this paper. The best systems achieved an\nofficial score (MAP) of 79.19, 76.70, 55.41, and 45.83 in subtasks A, B, C, and\nD, respectively. These scores are significantly better than those for the\nbaselines that we provided. For subtask A, the best system improved over the\n2015 winner by 3 points absolute in terms of Accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:30:34 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Nakov", "Preslav", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Moschitti", "Alessandro", ""], ["Magdy", "Walid", ""], ["Mubarak", "Hamdy", ""], ["Freihat", "Abed Alhakim", ""], ["Glass", "James", ""], ["Randeree", "Bilal", ""]]}, {"id": "1912.01973", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani,\n  Veselin Stoyanov", "title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, sentiment towards a topic, quantification,\n  microblog sentiment analysis; Twitter opinion mining. arXiv admin note: text\n  overlap with arXiv:1912.00741", "journal-ref": "SemEval-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the fourth year of the ``Sentiment Analysis in Twitter\nTask''. SemEval-2016 Task 4 comprises five subtasks, three of which represent a\nsignificant departure from previous editions. The first two subtasks are reruns\nfrom prior years and ask to predict the overall sentiment, and the sentiment\ntowards a topic in a tweet. The three new subtasks focus on two variants of the\nbasic ``sentiment classification in Twitter'' task. The first variant adopts a\nfive-point scale, which confers an ordinal character to the classification\ntask. The second variant focuses on the correct estimation of the prevalence of\neach class of interest, a task which has been called quantification in the\nsupervised learning literature. The task continues to be very popular,\nattracting a total of 43 teams.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:46:20 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Nakov", "Preslav", ""], ["Ritter", "Alan", ""], ["Rosenthal", "Sara", ""], ["Sebastiani", "Fabrizio", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.01982", "submitter": "Samet Demir", "authors": "Samet Demir, Uras Mutlu, \\\"Ozgur \\\"Ozdemir", "title": "Neural Academic Paper Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of structured text generation,\nspecifically academic paper generation in $\\LaTeX{}$, inspired by the\nsurprisingly good results of basic character-level language models. Our\nmotivation is using more recent and advanced methods of language modeling on a\nmore complex dataset of $\\LaTeX{}$ source files to generate realistic academic\npapers. Our first contribution is preparing a dataset with $\\LaTeX{}$ source\nfiles on recent open-source computer vision papers. Our second contribution is\nexperimenting with recent methods of language modeling and text generation such\nas Transformer and Transformer-XL to generate consistent $\\LaTeX{}$ code. We\nreport cross-entropy and bits-per-character (BPC) results of the trained\nmodels, and we also discuss interesting points on some examples of the\ngenerated $\\LaTeX{}$ code.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:45:23 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Demir", "Samet", ""], ["Mutlu", "Uras", ""], ["\u00d6zdemir", "\u00d6zgur", ""]]}, {"id": "1912.01987", "submitter": "Edwin D. Simpson", "authors": "Edwin Simpson, Iryna Gurevych", "title": "Scalable Bayesian Preference Learning for Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable Bayesian preference learning method for jointly\npredicting the preferences of individuals as well as the consensus of a crowd\nfrom pairwise labels. Peoples' opinions often differ greatly, making it\ndifficult to predict their preferences from small amounts of personal data.\nIndividual biases also make it harder to infer the consensus of a crowd when\nthere are few labels per item. We address these challenges by combining matrix\nfactorisation with Gaussian processes, using a Bayesian approach to account for\nuncertainty arising from noisy and sparse data. Our method exploits input\nfeatures, such as text embeddings and user metadata, to predict preferences for\nnew items and users that are not in the training set. As previous solutions\nbased on Gaussian processes do not scale to large numbers of users, items or\npairwise labels, we propose a stochastic variational inference approach that\nlimits computational and memory costs. Our experiments on a recommendation task\nshow that our method is competitive with previous approaches despite our\nscalable inference approximation. We demonstrate the method's scalability on a\nnatural language processing task with thousands of users and items, and show\nimprovements over the state of the art on this task. We make our software\npublicly available for future work.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:56:38 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 20:01:44 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1912.02047", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg", "title": "Neural Machine Translation: A Review and Survey", "comments": "Extended version of \"Neural Machine Translation: A Review\" accepted\n  by the Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of machine translation (MT), the automatic translation of written\ntext from one natural language into another, has experienced a major paradigm\nshift in recent years. Statistical MT, which mainly relies on various\ncount-based models and which used to dominate MT research for decades, has\nlargely been superseded by neural machine translation (NMT), which tackles\ntranslation with a single neural network. In this work we will trace back the\norigins of modern NMT architectures to word and sentence embeddings and earlier\nexamples of the encoder-decoder network family. We will conclude with a survey\nof recent trends in the field.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:16:03 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 18:17:04 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Stahlberg", "Felix", ""]]}, {"id": "1912.02077", "submitter": "Rezarta Islamaj", "authors": "Rezarta Islamaj, Lana Yeganova, Won Kim, Natalie Xie, W. John Wilbur,\n  Zhiyong Lu", "title": "PDC -- a probabilistic distributional clustering algorithm: a case study\n  on suicide articles in PubMed", "comments": "AMIA Informatics Summit 2020, 18 pages, Algorithm in the Appendix, 3\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to organize a large collection in a manner that facilitates human\ncomprehension is crucial given the ever-increasing volumes of information. In\nthis work, we present PDC (probabilistic distributional clustering), a novel\nalgorithm that, given a document collection, computes disjoint term sets\nrepresenting topics in the collection. The algorithm relies on probabilities of\nword co-occurrences to partition the set of terms appearing in the collection\nof documents into disjoint groups of related terms. In this work, we also\npresent an environment to visualize the computed topics in the term space and\nretrieve the most related PubMed articles for each group of terms. We\nillustrate the algorithm by applying it to PubMed documents on the topic of\nsuicide. Suicide is a major public health problem identified as the tenth\nleading cause of death in the US. In this application, our goal is to provide a\nglobal view of the mental health literature pertaining to the subject of\nsuicide, and through this, to help create a rich environment of multifaceted\ndata to guide health care researchers in their endeavor to better understand\nthe breadth, depth and scope of the problem. We demonstrate the usefulness of\nthe proposed algorithm by providing a web portal that allows mental health\nresearchers to peruse the suicide-related literature in PubMed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:07:25 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Islamaj", "Rezarta", ""], ["Yeganova", "Lana", ""], ["Kim", "Won", ""], ["Xie", "Natalie", ""], ["Wilbur", "W. John", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1912.02114", "submitter": "Md. Saiful Islam", "authors": "Md. Saiful Islam, Mohammed Eunus Ali, Yong-Bin Kang, Timos Sellis,\n  Farhana M. Choudhury", "title": "Keyword Aware Influential Community Search in Large Attributed Graphs", "comments": "Under review for VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel keyword-aware influential community query KICQ that\nfinds the most influential communities from an attributed graph, where an\ninfluential community is defined as a closely connected group of vertices\nhaving some dominance over other groups of vertices with the expertise (a set\nof keywords) matching with the query terms (words or phrases). We first design\nthe KICQ that facilitates users to issue an influential CS query intuitively by\nusing a set of query terms, and predicates (AND or OR). In this context, we\npropose a novel word-embedding based similarity model that enables semantic\ncommunity search, which substantially alleviates the limitations of exact\nkeyword based community search. Next, we propose a new influence measure for a\ncommunity that considers both the cohesiveness and influence of the community\nand eliminates the need for specifying values of internal parameters of a\nnetwork. Finally, we propose two efficient algorithms for searching influential\ncommunities in large attributed graphs. We present detailed experiments and a\ncase study to demonstrate the effectiveness and efficiency of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:59:40 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Ali", "Mohammed Eunus", ""], ["Kang", "Yong-Bin", ""], ["Sellis", "Timos", ""], ["Choudhury", "Farhana M.", ""]]}, {"id": "1912.02145", "submitter": "Zhucheng Tu", "authors": "Shayne Longpre, Yi Lu, Zhucheng Tu, Chris DuBois", "title": "An Exploration of Data Augmentation and Sampling Techniques for\n  Domain-Agnostic Question Answering", "comments": "Accepted at the 2nd Workshop on Machine Reading for Question\n  Answering", "journal-ref": "In Proceedings of the 2nd Workshop on Machine Reading for Question\n  Answering, pp. 220-227. 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To produce a domain-agnostic question answering model for the Machine Reading\nQuestion Answering (MRQA) 2019 Shared Task, we investigate the relative\nbenefits of large pre-trained language models, various data sampling\nstrategies, as well as query and context paraphrases generated by\nback-translation. We find a simple negative sampling technique to be\nparticularly effective, even though it is typically used for datasets that\ninclude unanswerable questions, such as SQuAD 2.0. When applied in conjunction\nwith per-domain sampling, our XLNet (Yang et al., 2019)-based submission\nachieved the second best Exact Match and F1 in the MRQA leaderboard\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:48:58 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Longpre", "Shayne", ""], ["Lu", "Yi", ""], ["Tu", "Zhucheng", ""], ["DuBois", "Chris", ""]]}, {"id": "1912.02164", "submitter": "Rosanne Liu", "authors": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank,\n  Piero Molino, Jason Yosinski, Rosanne Liu", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text\n  Generation", "comments": "ICLR 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling\nattributes of the generated language (e.g. switching topic or sentiment) is\ndifficult without modifying the model architecture or fine-tuning on\nattribute-specific data and entailing the significant cost of retraining. We\npropose a simple alternative: the Plug and Play Language Model (PPLM) for\ncontrollable language generation, which combines a pretrained LM with one or\nmore simple attribute classifiers that guide text generation without any\nfurther training of the LM. In the canonical scenario we present, the attribute\nmodels are simple classifiers consisting of a user-specified bag of words or a\nsingle learned layer with 100,000 times fewer parameters than the LM. Sampling\nentails a forward and backward pass in which gradients from the attribute model\npush the LM's hidden activations and thus guide the generation. Model samples\ndemonstrate control over a range of topics and sentiment styles, and extensive\nautomated and human annotated evaluations show attribute alignment and fluency.\nPPLMs are flexible in that any combination of differentiable attribute models\nmay be used to steer text generation, which will allow for diverse and creative\napplications beyond the examples given in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:32:15 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 01:02:25 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 06:05:58 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2020 05:33:49 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Dathathri", "Sumanth", ""], ["Madotto", "Andrea", ""], ["Lan", "Janice", ""], ["Hung", "Jane", ""], ["Frank", "Eric", ""], ["Molino", "Piero", ""], ["Yosinski", "Jason", ""], ["Liu", "Rosanne", ""]]}, {"id": "1912.02206", "submitter": "Yunan Zhang", "authors": "Yunan Zhang, Xiang Cheng, Heting Gao, Chengxiang Zhai", "title": "Cooperative Reasoning on Knowledge Graph and Corpus: A\n  Multi-agentReinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge-graph-based reasoning has drawn a lot of attention due to its\ninterpretability. However, previous methods suffer from the incompleteness of\nthe knowledge graph, namely the interested link or entity that can be missing\nin the knowledge graph(explicit missing). Also, most previous models assume the\ndistance between the target and source entity is short, which is not true on a\nreal-world KG like Freebase(implicit missing). The sensitivity to the\nincompleteness of KG and the incapability to capture the long-distance link\nbetween entities have limited the performance of these models on large KG. In\nthis paper, we propose a model that leverages the text corpus to cure such\nlimitations, either the explicit or implicit missing links. We model the\nquestion answering on KG as a cooperative task between two agents, a knowledge\ngraph reasoning agent and an information extraction agent. Each agent learns\nits skill to complete its own task, hopping on KG or select knowledge from the\ncorpus, via maximizing the reward for correctly answering the question. The\nreasoning agent decides how to find an equivalent path for the given entity and\nrelation. The extraction agent provide shortcut for long-distance target entity\nor provide missing relations for explicit missing links with messages from the\nreasoning agent. Through such cooperative reward design, our model can augment\nthe incomplete KG strategically while not introduce much unnecessary noise that\ncould enlarge the search space and lower the performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 19:00:54 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhang", "Yunan", ""], ["Cheng", "Xiang", ""], ["Gao", "Heting", ""], ["Zhai", "Chengxiang", ""]]}, {"id": "1912.02315", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee", "title": "12-in-1: Multi-Task Vision and Language Representation Learning", "comments": "Jiasen Lu and Vedanuj Goswami contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of vision-and-language research focuses on a small but diverse set of\nindependent tasks and supporting datasets often studied in isolation; however,\nthe visually-grounded language understanding skills required for success at\nthese tasks overlap significantly. In this work, we investigate these\nrelationships between vision-and-language tasks by developing a large-scale,\nmulti-task training regime. Our approach culminates in a single model on 12\ndatasets from four broad categories of task including visual question\nanswering, caption-based image retrieval, grounding referring expressions, and\nmulti-modal verification. Compared to independently trained single-task models,\nthis represents a reduction from approximately 3 billion parameters to 270\nmillion while simultaneously improving performance by 2.05 points on average\nacross tasks. We use our multi-task framework to perform in-depth analysis of\nthe effect of joint training diverse tasks. Further, we show that finetuning\ntask-specific models from our single multi-task model can lead to further\nimprovements, achieving performance at or above the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 00:07:35 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 21:39:42 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lu", "Jiasen", ""], ["Goswami", "Vedanuj", ""], ["Rohrbach", "Marcus", ""], ["Parikh", "Devi", ""], ["Lee", "Stefan", ""]]}, {"id": "1912.02367", "submitter": "Jie Zhao", "authors": "Jie Zhao, Xiang Deng, Huan Sun", "title": "Easy-to-Hard: Leveraging Simple Questions for Complex Question\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes one of the first efforts toward automatically generating\ncomplex questions from knowledge graphs. Particularly, we study how to leverage\nexisting simple question datasets for this task, under two separate scenarios:\nusing either sub-questions of the target complex questions, or distantly\nrelated pseudo sub-questions when the former are unavailable. First, a\ncompetitive base model named CoG2Q is designed to map complex query qraphs to\nnatural language questions. Afterwards, we propose two extension models, namely\nCoGSub2Q and CoGSub^m2Q, respectively for the above two scenarios. The former\nencodes and copies from a sub-question, while the latter further scores and\naggregates multiple pseudo sub-questions. Experiment results show that the\nextension models significantly outperform not only base CoG2Q, but also its\naugmented variant using simple questions as additional training examples. This\ndemonstrates the importance of instance-level connections between simple and\ncorresponding complex questions, which may be underexploited by straightforward\ndata augmentation of CoG2Q that builds model-level connections through learned\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 03:44:20 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 06:03:07 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zhao", "Jie", ""], ["Deng", "Xiang", ""], ["Sun", "Huan", ""]]}, {"id": "1912.02379", "submitter": "Vishvak Murahari", "authors": "Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das", "title": "Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art\n  Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work in visual dialog has focused on training deep neural models on\nVisDial in isolation. Instead, we present an approach to leverage pretraining\non related vision-language datasets before transferring to visual dialog. We\nadapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn\nvisually-grounded conversations. Our model is pretrained on the Conceptual\nCaptions and Visual Question Answering datasets, and finetuned on VisDial. Our\nbest single model outperforms prior published work (including model ensembles)\nby more than 1% absolute on NDCG and MRR. Next, we find that additional\nfinetuning using \"dense\" annotations in VisDial leads to even higher NDCG --\nmore than 10% over our base model -- but hurts MRR -- more than 17% below our\nbase model! This highlights a trade-off between the two primary metrics -- NDCG\nand MRR -- which we find is due to dense annotations not correlating well with\nthe original ground-truth answers to questions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:51:11 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 03:12:26 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Murahari", "Vishvak", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Das", "Abhishek", ""]]}, {"id": "1912.02387", "submitter": "Preslav Nakov", "authors": "Sara Rosenthal, Saif M Mohammad, Preslav Nakov, Alan Ritter, Svetlana\n  Kiritchenko, Veselin Stoyanov", "title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, sentiment towards a topic, quantification,\n  microblog sentiment analysis; Twitter opinion mining", "journal-ref": "SemEval-2015", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the 2015 iteration of the SemEval shared task on\nSentiment Analysis in Twitter. This was the most popular sentiment analysis\nshared task to date with more than 40 teams participating in each of the last\nthree years. This year's shared task competition consisted of five sentiment\nprediction subtasks. Two were reruns from previous years: (A) sentiment\nexpressed by a phrase in the context of a tweet, and (B) overall sentiment of a\ntweet. We further included three new subtasks asking to predict (C) the\nsentiment towards a topic in a single tweet, (D) the overall sentiment towards\na topic in a set of tweets, and (E) the degree of prior polarity of a phrase.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:08:36 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Rosenthal", "Sara", ""], ["Mohammad", "Saif M", ""], ["Nakov", "Preslav", ""], ["Ritter", "Alan", ""], ["Kiritchenko", "Svetlana", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.02395", "submitter": "Gang Chen", "authors": "Gang Chen, Yang Liu, Huanbo Luan, Meng Zhang, Qun Liu and Maosong Sun", "title": "Learning to Predict Explainable Plots for Neural Story Generation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story generation is an important natural language processing task that aims\nto generate coherent stories automatically. While the use of neural networks\nhas proven effective in improving story generation, how to learn to generate an\nexplainable high-level plot still remains a major challenge. In this work, we\npropose a latent variable model for neural story generation. The model treats\nan outline, which is a natural language sentence explainable to humans, as a\nlatent variable to represent a high-level plot that bridges the input and\noutput. We adopt an external summarization model to guide the latent variable\nmodel to learn how to generate outlines from training data. Experiments show\nthat our approach achieves significant improvements over state-of-the-art\nmethods in both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:32:41 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 08:22:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Chen", "Gang", ""], ["Liu", "Yang", ""], ["Luan", "Huanbo", ""], ["Zhang", "Meng", ""], ["Liu", "Qun", ""], ["Sun", "Maosong", ""]]}, {"id": "1912.02477", "submitter": "Michael Fell", "authors": "Michael Fell, Elena Cabrio, Elmahdi Korfed, Michel Buffa and Fabien\n  Gandon", "title": "Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the\n  WASABI Song Corpus with Lyrics Annotations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the WASABI Song Corpus, a large corpus of songs enriched with\nmetadata extracted from music databases on the Web, and resulting from the\nprocessing of song lyrics and from audio analysis. More specifically, given\nthat lyrics encode an important part of the semantics of a song, we focus here\non the description of the methods we proposed to extract relevant information\nfrom the lyrics, such as their structure segmentation, their topics, the\nexplicitness of the lyrics content, the salient passages of a song and the\nemotions conveyed. The creation of the resource is still ongoing: so far, the\ncorpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at\ndifferent levels with the output of the above mentioned methods. Such corpus\nlabels and the provided methods can be exploited by music search engines and\nmusic professionals (e.g. journalists, radio presenters) to better handle large\ncollections of lyrics, allowing an intelligent browsing, categorization and\nsegmentation recommendation of songs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:15:13 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 14:01:54 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Fell", "Michael", ""], ["Cabrio", "Elena", ""], ["Korfed", "Elmahdi", ""], ["Buffa", "Michel", ""], ["Gandon", "Fabien", ""]]}, {"id": "1912.02478", "submitter": "Jun Quan", "authors": "Jun Quan and Deyi Xiong", "title": "Effective Data Augmentation Approaches to End-to-End Task-Oriented\n  Dialogue", "comments": "accepted by IALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of task-oriented dialogue systems is often confronted with the\nlack of annotated data. In contrast to previous work which augments training\ndata through expensive crowd-sourcing efforts, we propose four different\nautomatic approaches to data augmentation at both the word and sentence level\nfor end-to-end task-oriented dialogue and conduct an empirical study on their\nimpact. Experimental results on the CamRest676 and KVRET datasets demonstrate\nthat each of the four data augmentation approaches is able to obtain a\nsignificant improvement over a strong baseline in terms of Success F1 score and\nthat the ensemble of the four approaches achieves the state-of-the-art results\nin the two datasets. In-depth analyses further confirm that our methods\nadequately increase the diversity of user utterances, which enables the\nend-to-end model to learn features robustly.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:16:32 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Quan", "Jun", ""], ["Xiong", "Deyi", ""]]}, {"id": "1912.02481", "submitter": "Cristina Espa\\~na-Bonet", "authors": "Jesujoba O. Alabi, Kwabena Amponsah-Kaakyire, David I. Adelani,\n  Cristina Espa\\~na-Bonet", "title": "Massive vs. Curated Word Embeddings for Low-Resourced Languages. The\n  Case of Yor\\`ub\\'a and Twi", "comments": "9 pages, 4 tables. Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of several architectures to learn semantic representations from\nunannotated text and the availability of these kind of texts in online\nmultilingual resources such as Wikipedia has facilitated the massive and\nautomatic creation of resources for multiple languages. The evaluation of such\nresources is usually done for the high-resourced languages, where one has a\nsmorgasbord of tasks and test sets to evaluate on. For low-resourced languages,\nthe evaluation is more difficult and normally ignored, with the hope that the\nimpressive capability of deep learning architectures to learn (multilingual)\nrepresentations in the high-resourced setting holds in the low-resourced\nsetting too. In this paper we focus on two African languages, Yor\\`ub\\'a and\nTwi, and compare the word embeddings obtained in this way, with word embeddings\nobtained from curated corpora and a language-dependent processing. We analyse\nthe noise in the publicly available corpora, collect high quality and noisy\ndata for the two languages and quantify the improvements that depend not only\non the amount of data but on the quality too. We also use different\narchitectures that learn word representations both from surface forms and\ncharacters to further exploit all the available information which showed to be\nimportant for these languages. For the evaluation, we manually translate the\nwordsim-353 word pairs dataset from English into Yor\\`ub\\'a and Twi. As output\nof the work, we provide corpora, embeddings and the test suits for both\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:25:32 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 21:38:21 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Alabi", "Jesujoba O.", ""], ["Amponsah-Kaakyire", "Kwabena", ""], ["Adelani", "David I.", ""], ["Espa\u00f1a-Bonet", "Cristina", ""]]}, {"id": "1912.02545", "submitter": "Yuni Lai", "authors": "Yuni Lai, Linfeng Zhang, Donghong Han, Rui Zhou, Guoren Wang", "title": "Fine-Grained Emotion Classification of Chinese Microblogs Based on Graph\n  Convolution Networks", "comments": "20 pages, 6 figures, submitted to the World Wide Web Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogs are widely used to express people's opinions and feelings in daily\nlife. Sentiment analysis (SA) can timely detect personal sentiment polarities\nthrough analyzing text. Deep learning approaches have been broadly used in SA\nbut still have not fully exploited syntax information. In this paper, we\npropose a syntax-based graph convolution network (GCN) model to enhance the\nunderstanding of diverse grammatical structures of Chinese microblogs. In\naddition, a pooling method based on percentile is proposed to improve the\naccuracy of the model. In experiments, for Chinese microblogs emotion\nclassification categories including happiness, sadness, like, anger, disgust,\nfear, and surprise, the F-measure of our model reaches 82.32% and exceeds the\nstate-of-the-art algorithm by 5.90%. The experimental results show that our\nmodel can effectively utilize the information of dependency parsing to improve\nthe performance of emotion detection. What is more, we annotate a new dataset\nfor Chinese emotion classification, which is open to other researchers.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 12:56:28 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lai", "Yuni", ""], ["Zhang", "Linfeng", ""], ["Han", "Donghong", ""], ["Zhou", "Rui", ""], ["Wang", "Guoren", ""]]}, {"id": "1912.02610", "submitter": "Verena Heu{\\ss}er", "authors": "Verena Heusser, Niklas Freymuth, Stefan Constantin, Alex Waibel", "title": "Bimodal Speech Emotion Recognition Using Pre-Trained Language Models", "comments": "Life-Long Learning for Spoken Language Systems ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech emotion recognition is a challenging task and an important step\ntowards more natural human-machine interaction. We show that pre-trained\nlanguage models can be fine-tuned for text emotion recognition, achieving an\naccuracy of 69.5% on Task 4A of SemEval 2017, improving upon the previous state\nof the art by over 3% absolute. We combine these language models with speech\nemotion recognition, achieving results of 73.5% accuracy when using provided\ntranscriptions and speech data on a subset of four classes of the IEMOCAP\ndataset. The use of noise-induced transcriptions and speech data results in an\naccuracy of 71.4%. For our experiments, we created IEmoNet, a modular and\nadaptable bimodal framework for speech emotion recognition based on pre-trained\nlanguage models. Lastly, we discuss the idea of using an emotional classifier\nas a reward for reinforcement learning as a step towards more successful and\nconvenient human-machine interaction.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 23:25:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Heusser", "Verena", ""], ["Freymuth", "Niklas", ""], ["Constantin", "Stefan", ""], ["Waibel", "Alex", ""]]}, {"id": "1912.02636", "submitter": "Qingxiang Wang", "authors": "Qingxiang Wang, Chad Brown, Cezary Kaliszyk, Josef Urban", "title": "Exploration of Neural Machine Translation in Autoformalization of\n  Mathematics in Mizar", "comments": "The 9th ACM SIGPLAN International Conference on Certified Programs\n  and Proofs", "journal-ref": null, "doi": "10.1145/3372885.3373827", "report-no": null, "categories": "cs.LO cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we share several experiments trying to automatically translate\ninformal mathematics into formal mathematics. In our context informal\nmathematics refers to human-written mathematical sentences in the LaTeX format;\nand formal mathematics refers to statements in the Mizar language. We conducted\nour experiments against three established neural network-based machine\ntranslation models that are known to deliver competitive results on translating\nbetween natural languages. To train these models we also prepared four\ninformal-to-formal datasets. We compare and analyze our results according to\nwhether the model is supervised or unsupervised. In order to augment the data\navailable for auto-formalization and improve the results, we develop a custom\ntype-elaboration mechanism and integrate it in the supervised translation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:13:15 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 09:29:01 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Wang", "Qingxiang", ""], ["Brown", "Chad", ""], ["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1912.02646", "submitter": "Jean Neraud", "authors": "Jean N\\'eraud (LITIS, UNIROUEN)", "title": "Complete Variable-Length Codes: An Excursion into Word Edit Operations", "comments": null, "journal-ref": "LATA 2020, Mar 2020, Milan, Italy", "doi": null, "report-no": null, "categories": "cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an alphabet A and a binary relation $\\tau$ $\\subseteq$ A * x A * , a\nlanguage X $\\subseteq$ A * is $\\tau$-independent if $\\tau$ (X) $\\cap$ X =\n$\\emptyset$; X is $\\tau$-closed if $\\tau$ (X) $\\subseteq$ X. The language X is\ncomplete if any word over A is a factor of some concatenation of words in X.\nGiven a family of languages F containing X, X is maximal in F if no other set\nof F can stricly contain X. A language X $\\subseteq$ A * is a variable-length\ncode if any equation among the words of X is necessarily trivial. The study\ndiscusses the relationship between maximality and completeness in the case of\n$\\tau$-independent or $\\tau$-closed variable-length codes. We focus to the\nbinary relations by which the images of words are computed by deleting,\ninserting, or substituting some characters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:28:05 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["N\u00e9raud", "Jean", "", "LITIS, UNIROUEN"]]}, {"id": "1912.02703", "submitter": "Craig Ganoe", "authors": "Xing Meng, Craig H. Ganoe, Ryan T. Sieberg, Yvonne Y. Cheung, Saeed\n  Hassanpour", "title": "Self-Supervised Contextual Language Representation of Radiology Reports\n  to Improve the Identification of Communication Urgency", "comments": "Accepted in AMIA 2020 Informatics Summit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods have recently achieved high-performance in\nbiomedical text analysis. However, a major bottleneck in the widespread\napplication of these methods is obtaining the required large amounts of\nannotated training data, which is resource intensive and time consuming. Recent\nprogress in self-supervised learning has shown promise in leveraging large text\ncorpora without explicit annotations. In this work, we built a self-supervised\ncontextual language representation model using BERT, a deep bidirectional\ntransformer architecture, to identify radiology reports requiring prompt\ncommunication to the referring physicians. We pre-trained the BERT model on a\nlarge unlabeled corpus of radiology reports and used the resulting contextual\nrepresentations in a final text classifier for communication urgency. Our model\nachieved a precision of 97.0%, recall of 93.3%, and F-measure of 95.1% on an\nindependent test set in identifying radiology reports for prompt communication,\nand significantly outperformed the previous state-of-the-art model based on\nword2vec representations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 16:33:23 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Meng", "Xing", ""], ["Ganoe", "Craig H.", ""], ["Sieberg", "Ryan T.", ""], ["Cheung", "Yvonne Y.", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1912.02761", "submitter": "Joseph Fisher", "authors": "Joseph Fisher, Dave Palfrey, Christos Christodoulopoulos, Arpit Mittal", "title": "Measuring Social Bias in Knowledge Graph Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that word embeddings encode social biases, with a\nharmful impact on downstream tasks. However, to this point there has been no\nsimilar work done in the field of graph embeddings. We present the first study\non social bias in knowledge graph embeddings, and propose a new metric suitable\nfor measuring such bias. We conduct experiments on Wikidata and Freebase, and\nshow that, as with word embeddings, harmful social biases related to\nprofessions are encoded in the embeddings with respect to gender, religion,\nethnicity and nationality. For example, graph embeddings encode the information\nthat men are more likely to be bankers, and women more likely to be\nhomekeepers. As graph embeddings become increasingly utilized, we suggest that\nit is important the existence of such biases are understood and steps taken to\nmitigate their impact.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:53:25 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 14:52:19 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Fisher", "Joseph", ""], ["Palfrey", "Dave", ""], ["Christodoulopoulos", "Christos", ""], ["Mittal", "Arpit", ""]]}, {"id": "1912.02866", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala", "title": "Classifying Diagrams and Their Parts using Graph Neural Networks: A\n  Comparison of Crowd-Sourced and Expert Annotations", "comments": "9 pages; submitted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article compares two multimodal resources that consist of diagrams which\ndescribe topics in elementary school natural sciences. Both resources contain\nthe same diagrams and represent their structure using graphs, but differ in\nterms of their annotation schema and how the annotations have been created -\ndepending on the resource in question - either by crowd-sourced workers or\ntrained experts. This article reports on two experiments that evaluate how\neffectively crowd-sourced and expert-annotated graphs can represent the\nmultimodal structure of diagrams for representation learning using various\ngraph neural networks. The results show that the identity of diagram elements\ncan be learned from their layout features, while the expert annotations provide\nbetter representations of diagram types.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:34:53 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Hiippala", "Tuomo", ""]]}, {"id": "1912.02958", "submitter": "Zhengkun Tian", "authors": "Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai Zhang, Zhengqi\n  Wen", "title": "Synchronous Transformers for End-to-End Speech Recognition", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most of the attention-based sequence-to-sequence models, the decoder\npredicts the output sequence conditioned on the entire input sequence processed\nby the encoder. The asynchronous problem between the encoding and decoding\nmakes these models difficult to be applied for online speech recognition. In\nthis paper, we propose a model named synchronous transformer to address this\nproblem, which can predict the output sequence chunk by chunk. Once a\nfixed-length chunk of the input sequence is processed by the encoder, the\ndecoder begins to predict symbols immediately. During training, a\nforward-backward algorithm is introduced to optimize all the possible alignment\npaths. Our model is evaluated on a Mandarin dataset AISHELL-1. The experiments\nshow that the synchronous transformer is able to perform encoding and decoding\nsynchronously, and achieves a character error rate of 8.91% on the test set.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 03:05:12 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 03:49:45 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Tian", "Zhengkun", ""], ["Yi", "Jiangyan", ""], ["Bai", "Ye", ""], ["Tao", "Jianhua", ""], ["Zhang", "Shuai", ""], ["Wen", "Zhengqi", ""]]}, {"id": "1912.02990", "submitter": "Preslav Nakov", "authors": "Sara Rosenthal, Preslav Nakov, Alan Ritter, Veselin Stoyanov", "title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, microblog sentiment analysis, Twitter opinion\n  mining, sarcasm, LiveJournal, SMS", "journal-ref": "SemEval-2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Sentiment Analysis in Twitter task, ran as part of\nSemEval-2014. It is a continuation of the last year's task that ran\nsuccessfully as part of SemEval-2013. As in 2013, this was the most popular\nSemEval task; a total of 46 teams contributed 27 submissions for subtask A (21\nteams) and 50 submissions for subtask B (44 teams). This year, we introduced\nthree new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii)\nLiveJournal sentences. We further tested on (iv) 2013 tweets, and (v) 2013 SMS\nmessages. The highest F1-score on (i) was achieved by NRC-Canada at 86.63 for\nsubtask A and by TeamX at 70.96 for subtask B.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:23:19 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Rosenthal", "Sara", ""], ["Nakov", "Preslav", ""], ["Ritter", "Alan", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.02998", "submitter": "Preslav Nakov", "authors": "Francisco Guzm\\'an, Llu\\'is M\\`arquez, Preslav Nakov", "title": "Machine Translation Evaluation Meets Community Question Answering", "comments": "community question answering, machine translation evaluation,\n  pairwise ranking, learning to rank", "journal-ref": "Annual meeting of the Association for Computational Linguistics\n  (ACL-2016)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the applicability of machine translation evaluation (MTE) methods\nto a very different problem: answer ranking in community Question Answering. In\nparticular, we adopt a pairwise neural network (NN) architecture, which\nincorporates MTE features, as well as rich syntactic and semantic embeddings,\nand which efficiently models complex non-linear interactions. The evaluation\nresults show state-of-the-art performance, with sizeable contribution from both\nthe MTE features and from the pairwise NN architecture.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:35:21 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Guzm\u00e1n", "Francisco", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.03010", "submitter": "Yu Wu", "authors": "Chengyi Wang, Yu Wu, Yujiao Du, Jinyu Li, Shujie Liu, Liang Lu, Shuo\n  Ren, Guoli Ye, Sheng Zhao, Ming Zhou", "title": "Semantic Mask for Transformer based End-to-End Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder-decoder model has achieved impressive results for\nboth automatic speech recognition (ASR) and text-to-speech (TTS) tasks. This\napproach takes advantage of the memorization capacity of neural networks to\nlearn the mapping from the input sequence to the output sequence from scratch,\nwithout the assumption of prior knowledge such as the alignments. However, this\nmodel is prone to overfitting, especially when the amount of training data is\nlimited. Inspired by SpecAugment and BERT, in this paper, we propose a semantic\nmask based regularization for training such kind of end-to-end (E2E) model. The\nidea is to mask the input features corresponding to a particular output token,\ne.g., a word or a word-piece, in order to encourage the model to fill the token\nbased on the contextual information. While this approach is applicable to the\nencoder-decoder framework with any type of neural network architecture, we\nstudy the transformer-based model for ASR in this work. We perform experiments\non Librispeech 960h and TedLium2 data sets, and achieve the state-of-the-art\nperformance on the test set in the scope of E2E models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 07:55:04 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 02:41:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wang", "Chengyi", ""], ["Wu", "Yu", ""], ["Du", "Yujiao", ""], ["Li", "Jinyu", ""], ["Liu", "Shujie", ""], ["Lu", "Liang", ""], ["Ren", "Shuo", ""], ["Ye", "Guoli", ""], ["Zhao", "Sheng", ""], ["Zhou", "Ming", ""]]}, {"id": "1912.03041", "submitter": "Wenya Wang", "authors": "Wenya Wang and Sinno Jialin Pan", "title": "Integrating Deep Learning with Logic Fusion for Information Extraction", "comments": "Accepted in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction (IE) aims to produce structured information from an\ninput text, e.g., Named Entity Recognition and Relation Extraction. Various\nattempts have been proposed for IE via feature engineering or deep learning.\nHowever, most of them fail to associate the complex relationships inherent in\nthe task itself, which has proven to be especially crucial. For example, the\nrelation between 2 entities is highly dependent on their entity types. These\ndependencies can be regarded as complex constraints that can be efficiently\nexpressed as logical rules. To combine such logic reasoning capabilities with\nlearning capabilities of deep neural networks, we propose to integrate logical\nknowledge in the form of first-order logic into a deep learning system, which\ncan be trained jointly in an end-to-end manner. The integrated framework is\nable to enhance neural outputs with knowledge regularization via logic rules,\nand at the same time update the weights of logic rules to comply with the\ncharacteristics of the training data. We demonstrate the effectiveness and\ngeneralization of the proposed model on multiple IE tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:38:23 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Wang", "Wenya", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1912.03048", "submitter": "Adrien Guille", "authors": "Jean Dupuy and Adrien Guille and Julien Jacques", "title": "Document Network Embedding: Coping for Missing Content and Missing Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Searching through networks of documents is an important task. A promising\npath to improve the performance of information retrieval systems in this\ncontext is to leverage dense node and content representations learned with\nembedding techniques. However, these techniques cannot learn representations\nfor documents that are either isolated or whose content is missing. To tackle\nthis issue, assuming that the topology of the network and the content of the\ndocuments correlate, we propose to estimate the missing node representations\nfrom the available content representations, and conversely. Inspired by recent\nadvances in machine translation, we detail in this paper how to learn a linear\ntransformation from a set of aligned content and node representations. The\nprojection matrix is efficiently calculated in terms of the singular value\ndecomposition. The usefulness of the proposed method is highlighted by the\nimproved ability to predict the neighborhood of nodes whose links are\nunobserved based on the projected content representations, and to retrieve\nsimilar documents when content is missing, based on the projected node\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 10:09:20 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Dupuy", "Jean", ""], ["Guille", "Adrien", ""], ["Jacques", "Julien", ""]]}, {"id": "1912.03063", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec (LIRIS), Grigory Antipov, Moez Baccouche, Christian\n  Wolf (LIRIS)", "title": "Weak Supervision helps Emergence of Word-Object Alignment and improves\n  Vision-Language Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large adoption of the self-attention (i.e. transformer model) and\nBERT-like training principles has recently resulted in a number of high\nperforming models on a large panoply of vision-and-language problems (such as\nVisual Question Answering (VQA), image retrieval, etc.). In this paper we claim\nthat these State-Of-The-Art (SOTA) approaches perform reasonably well in\nstructuring information inside a single modality but, despite their impressive\nperformances , they tend to struggle to identify fine-grained inter-modality\nrelationships. Indeed, such relations are frequently assumed to be implicitly\nlearned during training from application-specific losses, mostly cross-entropy\nfor classification. While most recent works provide inductive bias for\ninter-modality relationships via cross attention modules, in this work, we\ndemonstrate (1) that the latter assumption does not hold, i.e. modality\nalignment does not necessarily emerge automatically, and (2) that adding weak\nsupervision for alignment between visual objects and words improves the quality\nof the learned models on tasks requiring reasoning. In particular , we\nintegrate an object-word alignment loss into SOTA vision-language reasoning\nmodels and evaluate it on two tasks VQA and Language-driven Comparison of\nImages. We show that the proposed fine-grained inter-modality supervision\nsignificantly improves performance on both tasks. In particular, this new\nlearning signal allows obtaining SOTA-level performances on GQA dataset (VQA\ntask) with pre-trained models without finetuning on the task, and a new SOTA on\nNLVR2 dataset (Language-driven Comparison of Images). Finally, we also\nillustrate the impact of the contribution on the models reasoning by\nvisualizing attention distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 11:04:08 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Kervadec", "Corentin", "", "LIRIS"], ["Antipov", "Grigory", "", "LIRIS"], ["Baccouche", "Moez", "", "LIRIS"], ["Wolf", "Christian", "", "LIRIS"]]}, {"id": "1912.03135", "submitter": "Preslav Nakov", "authors": "Francisco Guzman, Shafiq Joty, Lluis Marquez, Preslav Nakov", "title": "Pairwise Neural Machine Translation Evaluation", "comments": "machine translation evaluation, machine translation, pairwise\n  ranking, learning to rank. arXiv admin note: substantial text overlap with\n  arXiv:1710.02095", "journal-ref": "Conference of the Association for Computational Linguistics\n  (ACL'2015)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for machine translation evaluation using neural\nnetworks in a pairwise setting, where the goal is to select the better\ntranslation from a pair of hypotheses, given the reference translation. In this\nframework, lexical, syntactic and semantic information from the reference and\nthe two hypotheses is compacted into relatively small distributed vector\nrepresentations, and fed into a multi-layer neural network that models the\ninteraction between each of the hypotheses and the reference, as well as\nbetween the two hypotheses. These compact representations are in turn based on\nword and sentence embeddings, which are learned using neural networks. The\nframework is flexible, allows for efficient learning and classification, and\nyields correlation with humans that rivals the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:17:05 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Guzman", "Francisco", ""], ["Joty", "Shafiq", ""], ["Marquez", "Lluis", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.03184", "submitter": "Roman Klinger", "authors": "Laura Bostan and Evgeny Kim and Roman Klinger", "title": "GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions,\n  Semantic Roles, and Reader Perception", "comments": "Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on emotion analysis from text focuses on the task of emotion\nclassification or emotion intensity regression. Fewer works address emotions as\na phenomenon to be tackled with structured learning, which can be explained by\nthe lack of relevant datasets. We fill this gap by releasing a dataset of 5000\nEnglish news headlines annotated via crowdsourcing with their associated\nemotions, the corresponding emotion experiencers and textual cues, related\nemotion causes and targets, as well as the reader's perception of the emotion\nof the headline. This annotation task is comparably challenging, given the\nlarge number of classes and roles to be identified. We therefore propose a\nmultiphase annotation procedure in which we first find relevant instances with\nemotional content and then annotate the more fine-grained aspects. Finally, we\ndevelop a baseline for the task of automatic prediction of semantic role\nstructures and discuss the results. The corpus we release enables further\nresearch on emotion classification, emotion intensity prediction, emotion cause\ndetection, and supports further qualitative studies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:30:58 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 10:02:19 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 13:32:42 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Bostan", "Laura", ""], ["Kim", "Evgeny", ""], ["Klinger", "Roman", ""]]}, {"id": "1912.03223", "submitter": "Mahya Ameryan", "authors": "Mahya Ameryan, Lambert Schomaker", "title": "A limited-size ensemble of homogeneous CNN/LSTMs for high-performance\n  word classification", "comments": null, "journal-ref": "Neural Computing and Applications(2021)", "doi": "10.1007/s00521-020-05612-0", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, long short-term memory neural networks (LSTMs) have been\napplied quite successfully to problems in handwritten text recognition.\nHowever, their strength is more located in handling sequences of variable\nlength than in handling geometric variability of the image patterns.\nFurthermore, the best results for LSTMs are often based on large-scale training\nof an ensemble of network instances. In this paper, an end-to-end convolutional\nLSTM Neural Network is used to handle both geometric variation and sequence\nvariability. We show that high performances can be reached on a common\nbenchmark set by using proper data augmentation for just five such networks\nusing a proper coding scheme and a proper voting scheme. The networks have\nsimilar architectures (Convolutional Neural Network (CNN): five layers,\nbidirectional LSTM (BiLSTM): three layers followed by a connectionist temporal\nclassification (CTC) processing step). The approach assumes differently-scaled\ninput images and different feature map sizes. Two datasets are used for\nevaluation of the performance of our algorithm: A standard benchmark RIMES\ndataset (French), and a historical handwritten dataset KdK (Dutch). Final\nperformance obtained for the word-recognition test of RIMES was 96.6%, a clear\nimprovement over other state-of-the-art approaches. On the KdK dataset, our\napproach also shows good results. The proposed approach is deployed in the Monk\nsearch engine for historical-handwriting collections.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:45:52 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ameryan", "Mahya", ""], ["Schomaker", "Lambert", ""]]}, {"id": "1912.03234", "submitter": "Alejandro Mottini", "authors": "Alejandro Mottini, Amber Roy Chowdhury", "title": "What Do You Mean I'm Funny? Personalizing the Joke Skill of a\n  Voice-Controlled Virtual Assistant", "comments": "Presented at the AAAI 2020 Workshop on Interactive and Conversational\n  Recommendation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable part of the success experienced by Voice-controlled virtual\nassistants (VVA) is due to the emotional and personalized experience they\ndeliver, with humor being a key component in providing an engaging interaction.\nIn this paper we describe methods used to improve the joke skill of a VVA\nthrough personalization. The first method, based on traditional NLP techniques,\nis robust and scalable. The others combine self-attentional network and\nmulti-task learning to obtain better results, at the cost of added complexity.\nA significant challenge facing these systems is the lack of explicit user\nfeedback needed to provide labels for the models. Instead, we explore the use\nof two implicit feedback-based labelling strategies. All models were evaluated\non real production data. Online results show that models trained on any of the\nconsidered labels outperform a heuristic method, presenting a positive\nreal-world impact on user satisfaction. Offline results suggest that the\ndeep-learning approaches can improve the joke experience with respect to the\nother considered methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:17:39 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mottini", "Alejandro", ""], ["Chowdhury", "Amber Roy", ""]]}, {"id": "1912.03334", "submitter": "Mitchell Gordon", "authors": "Mitchell A. Gordon, Kevin Duh", "title": "Explaining Sequence-Level Knowledge Distillation as Data-Augmentation\n  for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sequence-level knowledge distillation (SLKD) is a model compression technique\nthat leverages large, accurate teacher models to train smaller,\nunder-parameterized student models. Why does pre-processing MT data with SLKD\nhelp us train smaller models? We test the common hypothesis that SLKD addresses\na capacity deficiency in students by \"simplifying\" noisy data points and find\nit unlikely in our case. Models trained on concatenations of original and\n\"simplified\" datasets generalize just as well as baseline SLKD. We then propose\nan alternative hypothesis under the lens of data augmentation and\nregularization. We try various augmentation strategies and observe that dropout\nregularization can become unnecessary. Our methods achieve BLEU gains of\n0.7-1.2 on TED Talks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 20:27:38 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gordon", "Mitchell A.", ""], ["Duh", "Kevin", ""]]}, {"id": "1912.03363", "submitter": "Ankur Gandhe", "authors": "Ankur Gandhe, Ariya Rastrow", "title": "Audio-attention discriminative language model for ASR rescoring", "comments": "4 pages, 1 figure, Accepted at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches for automatic speech recognition (ASR) benefit from\ndirectly modeling the probability of the word sequence given the input audio\nstream in a single neural network. However, compared to conventional ASR\nsystems, these models typically require more data to achieve comparable\nresults. Well-known model adaptation techniques, to account for domain and\nstyle adaptation, are not easily applicable to end-to-end systems. Conventional\nHMM-based systems, on the other hand, have been optimized for various\nproduction environments and use cases. In this work, we propose to combine the\nbenefits of end-to-end approaches with a conventional system using an\nattention-based discriminative language model that learns to rescore the output\nof a first-pass ASR system. We show that learning to rescore a list of\npotential ASR outputs is much simpler than learning to generate the hypothesis.\nThe proposed model results in 8% improvement in word error rate even when the\namount of training data is a fraction of data used for training the first-pass\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 22:09:07 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 18:03:03 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gandhe", "Ankur", ""], ["Rastrow", "Ariya", ""]]}, {"id": "1912.03366", "submitter": "Shaika Chowdhury", "authors": "Shaika Chowdhury, Chenwei Zhang, Philip S. Yu and Yuan Luo", "title": "Med2Meta: Learning Representations of Medical Concepts with\n  Meta-Embeddings", "comments": "9 pages", "journal-ref": "HEALTHINF 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of medical concepts have been used to support\ndownstream clinical tasks recently. Electronic Health Records (EHR) capture\ndifferent aspects of patients' hospital encounters and serve as a rich source\nfor augmenting clinical decision making by learning robust medical concept\nembeddings. However, the same medical concept can be recorded in different\nmodalities (e.g., clinical notes, lab results)-with each capturing salient\ninformation unique to that modality-and a holistic representation calls for\nrelevant feature ensemble from all information sources. We hypothesize that\nrepresentations learned from heterogeneous data types would lead to performance\nenhancement on various clinical informatics and predictive modeling tasks. To\nthis end, our proposed approach makes use of meta-embeddings, embeddings\naggregated from learned embeddings. Firstly, modality-specific embeddings for\neach medical concept is learned with graph autoencoders. The ensemble of all\nthe embeddings is then modeled as a meta-embedding learning problem to\nincorporate their correlating and complementary information through a joint\nreconstruction. Empirical results of our model on both quantitative and\nqualitative clinical evaluations have shown improvements over state-of-the-art\nembedding models, thus validating our hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 22:11:37 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 17:06:44 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chowdhury", "Shaika", ""], ["Zhang", "Chenwei", ""], ["Yu", "Philip S.", ""], ["Luo", "Yuan", ""]]}, {"id": "1912.03393", "submitter": "Naveen Arivazhagan", "authors": "Naveen Arivazhagan, Colin Cherry, Te I, Wolfgang Macherey, Pallavi\n  Baljekar, George Foster", "title": "Re-Translation Strategies For Long Form, Simultaneous, Spoken Language\n  Translation", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of simultaneous machine translation of long-form\nspeech content. We target a continuous speech-to-text scenario, generating\ntranslated captions for a live audio feed, such as a lecture or play-by-play\ncommentary. As this scenario allows for revisions to our incremental\ntranslations, we adopt a re-translation approach to simultaneous translation,\nwhere the source is repeatedly translated from scratch as it grows. This\napproach naturally exhibits very low latency and high final quality, but at the\ncost of incremental instability as the output is continuously refined. We\nexperiment with a pipeline of industry-grade speech recognition and translation\ntools, augmented with simple inference heuristics to improve stability. We use\nTED Talks as a source of multilingual test data, developing our techniques on\nEnglish-to-German spoken language translation. Our minimalist approach to\nsimultaneous translation allows us to easily scale our final evaluation to six\nmore target languages, dramatically improving incremental stability for all of\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 23:46:37 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 19:25:47 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Arivazhagan", "Naveen", ""], ["Cherry", "Colin", ""], ["I", "Te", ""], ["Macherey", "Wolfgang", ""], ["Baljekar", "Pallavi", ""], ["Foster", "George", ""]]}, {"id": "1912.03441", "submitter": "Tiffany Chien", "authors": "Tiffany Chien, Jugal Kalita", "title": "Adversarial Analysis of Natural Language Inference Systems", "comments": "8 pages, accepted by IEEE ICSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The release of large natural language inference (NLI) datasets like SNLI and\nMNLI have led to rapid development and improvement of completely neural systems\nfor the task. Most recently, heavily pre-trained, Transformer-based models like\nBERT and MT-DNN have reached near-human performance on these datasets. However,\nthese standard datasets have been shown to contain many annotation artifacts,\nallowing models to shortcut understanding using simple fallible heuristics, and\nstill perform well on the test set. So it is no surprise that many adversarial\n(challenge) datasets have been created that cause models trained on standard\ndatasets to fail dramatically. Although extra training on this data generally\nimproves model performance on just that type of data, transferring that\nlearning to unseen examples is still partial at best. This work evaluates the\nfailures of state-of-the-art models on existing adversarial datasets that test\ndifferent linguistic phenomena, and find that even though the models perform\nsimilarly on MNLI, they differ greatly in their robustness to these attacks. In\nparticular, we find syntax-related attacks to be particularly effective across\nall models, so we provide a fine-grained analysis and comparison of model\nperformance on those examples. We draw conclusions about the value of model\nsize and multi-task learning (beyond comparing their standard test set\nperformance), and provide suggestions for more effective training data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:12:03 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chien", "Tiffany", ""], ["Kalita", "Jugal", ""]]}, {"id": "1912.03444", "submitter": "Kelechi Ogueji", "authors": "Kelechi Ogueji and Orevaoghene Ahia", "title": "PidginUNMT: Unsupervised Neural Machine Translation from West African\n  Pidgin to English", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 800 languages are spoken across West Africa. Despite the obvious\ndiversity among people who speak these languages, one language significantly\nunifies them all - West African Pidgin English. There are at least 80 million\nspeakers of West African Pidgin English. However, there is no known natural\nlanguage processing (NLP) work on this language. In this work, we perform the\nfirst NLP work on the most popular variant of the language, providing three\nmajor contributions. First, the provision of a Pidgin corpus of over 56000\nsentences, which is the largest we know of. Secondly, the training of the first\never cross-lingual embedding between Pidgin and English. This aligned embedding\nwill be helpful in the performance of various downstream tasks between English\nand Pidgin. Thirdly, the training of an Unsupervised Neural Machine Translation\nmodel between Pidgin and English which achieves BLEU scores of 7.93 from Pidgin\nto English, and 5.18 from English to Pidgin. In all, this work greatly reduces\nthe barrier of entry for future NLP works on West African Pidgin English.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:30:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Ogueji", "Kelechi", ""], ["Ahia", "Orevaoghene", ""]]}, {"id": "1912.03457", "submitter": "Sebastin Santy", "authors": "Pratik Joshi, Christain Barnes, Sebastin Santy, Simran Khanuja, Sanket\n  Shah, Anirudh Srinivasan, Satwik Bhattamishra, Sunayana Sitaram, Monojit\n  Choudhury, Kalika Bali", "title": "Unsung Challenges of Building and Deploying Language Technologies for\n  Low Resource Language Communities", "comments": "Accepted at ICON 2019; 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine and analyze the challenges associated with\ndeveloping and introducing language technologies to low-resource language\ncommunities. While doing so, we bring to light the successes and failures of\npast work in this area, challenges being faced in doing so, and what they have\nachieved. Throughout this paper, we take a problem-facing approach and describe\nessential factors which the success of such technologies hinges upon. We\npresent the various aspects in a manner which clarify and lay out the different\ntasks involved, which can aid organizations looking to make an impact in this\narea. We take the example of Gondi, an extremely-low resource Indian language,\nto reinforce and complement our discussion.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 07:45:43 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Joshi", "Pratik", ""], ["Barnes", "Christain", ""], ["Santy", "Sebastin", ""], ["Khanuja", "Simran", ""], ["Shah", "Sanket", ""], ["Srinivasan", "Anirudh", ""], ["Bhattamishra", "Satwik", ""], ["Sitaram", "Sunayana", ""], ["Choudhury", "Monojit", ""], ["Bali", "Kalika", ""]]}, {"id": "1912.03502", "submitter": "Jieh-Sheng Lee", "authors": "Jieh-Sheng Lee", "title": "Personalized Patent Claim Generation and Measurement", "comments": "2 figures, 12 pages. Presented at the 32nd International Conference\n  on Legal Knowledge and Information Systems (JURIX 2019) and to be published\n  in the CEUR Workshop Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work-in-progress paper proposes a framework to generate and measure\npersonalized patent claims. The objective is to help inventors conceive better\ninventions by learning from relevant inventors. Patent claim generation is a\nway of \"augmented inventing.\" for inventors. Such patent claim generation\nleverages the recent transfer learning in the Deep Learning field, particularly\nthe state-of-the-art Transformer-based models. In terms of system\nimplementa-tion, it is planned to build an \"auto-complete\" function for patent\nclaim drafting. The auto-complete function is analyzed from four different\nperspectives: extent of generation, generative direction, proximity of\ngeneration, and constraint in generation. Technically, the framework is\ncomposed of two Transformer models. One is for text generation and the other is\nfor quality measurement. Specifically, the patent claim generation is based on\nGPT-2 model and the measurement of personalization is based on BERT model. The\ntraining data is inventor-centric and comes from the Inventors Endpoint API\nprovided by the USPTO.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 13:26:18 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 14:20:19 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Lee", "Jieh-Sheng", ""]]}, {"id": "1912.03553", "submitter": "Spencer Frazier", "authors": "Spencer Frazier, Md Sultan Al Nahian, Mark Riedl, Brent Harrison", "title": "Learning Norms from Stories: A Prior for Value Aligned Agents", "comments": "AIES 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value alignment is a property of an intelligent agent indicating that it can\nonly pursue goals and activities that are beneficial to humans. Traditional\napproaches to value alignment use imitation learning or preference learning to\ninfer the values of humans by observing their behavior. We introduce a\ncomplementary technique in which a value aligned prior is learned from\nnaturally occurring stories which encode societal norms. Training data is\nsourced from the childrens educational comic strip, Goofus and Gallant. In this\nwork, we train multiple machine learning models to classify natural language\ndescriptions of situations found in the comic strip as normative or non\nnormative by identifying if they align with the main characters behavior. We\nalso report the models performance when transferring to two unrelated tasks\nwith little to no additional training on the new task.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 20:12:43 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Frazier", "Spencer", ""], ["Nahian", "Md Sultan Al", ""], ["Riedl", "Mark", ""], ["Harrison", "Brent", ""]]}, {"id": "1912.03627", "submitter": "Hossein Zeinali", "authors": "Hossein Zeinali, Luk\\'a\\v{s} Burget, Jan \"Honza'' \\v{C}ernock\\'y", "title": "A Multi Purpose and Large Scale Speech Corpus in Persian and English for\n  Speaker and Speech Recognition: the DeepMine Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepMine is a speech database in Persian and English designed to build and\nevaluate text-dependent, text-prompted, and text-independent speaker\nverification, as well as Persian speech recognition systems. It contains more\nthan 1850 speakers and 540 thousand recordings overall, more than 480 hours of\nspeech are transcribed. It is the first public large-scale speaker verification\ndatabase in Persian, the largest public text-dependent and text-prompted\nspeaker verification database in English, and the largest public evaluation\ndataset for text-independent speaker verification. It has a good coverage of\nage, gender, and accents. We provide several evaluation protocols for each part\nof the database to allow for research on different aspects of speaker\nverification. We also provide the results of several experiments that can be\nconsidered as baselines: HMM-based i-vectors for text-dependent speaker\nverification, and HMM-based as well as state-of-the-art deep neural network\nbased ASR. We demonstrate that the database can serve for training robust ASR\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:05:36 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zeinali", "Hossein", ""], ["Burget", "Luk\u00e1\u0161", ""], ["\u010cernock\u00fd", "Jan \"Honza''", ""]]}, {"id": "1912.03656", "submitter": "Maurits Bleeker", "authors": "Maurits Bleeker and Maarten de Rijke", "title": "Bidirectional Scene Text Recognition with a Single Decoder", "comments": "8 pages. In 24th European Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Text Recognition (STR) is the problem of recognizing the correct word\nor character sequence in a cropped word image. To obtain more robust output\nsequences, the notion of bidirectional STR has been introduced. So far,\nbidirectional STRs have been implemented by using two separate decoders; one\nfor left-to-right decoding and one for right-to-left. Having two separate\ndecoders for almost the same task with the same output space is undesirable\nfrom a computational and optimization point of view. We introduce the\nbidirectional Scene Text Transformer (Bi-STET), a novel bidirectional STR\nmethod with a single decoder for bidirectional text decoding. With its single\ndecoder, Bi-STET outperforms methods that apply bidirectional decoding by using\ntwo separate decoders while also being more efficient than those methods,\nFurthermore, we achieve or beat state-of-the-art (SOTA) methods on all STR\nbenchmarks with Bi-STET. Finally, we provide analyses and insights into the\nperformance of Bi-STET.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 11:20:35 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 14:44:34 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bleeker", "Maurits", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1912.03720", "submitter": "Wei Zhang", "authors": "Wei Zhang, Chao Dong, Jianhua Yin, Jianyong Wang", "title": "Attentive Representation Learning with Adversarial Training for Short\n  Text Clustering", "comments": "14pages, to appear in IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text clustering has far-reaching effects on semantic analysis, showing\nits importance for multiple applications such as corpus summarization and\ninformation retrieval. However, it inevitably encounters the severe sparsity of\nshort text representations, making the previous clustering approaches still far\nfrom satisfactory. In this paper, we present a novel attentive representation\nlearning model for shot text clustering, wherein cluster-level attention is\nproposed to capture the correlations between text representations and cluster\nrepresentations. Relying on this, the representation learning and clustering\nfor short texts are seamlessly integrated into a unified model. To further\nensure robust model training for short texts, we apply adversarial training to\nthe unsupervised clustering setting, by injecting perturbations into the\ncluster representations. The model parameters and perturbations are optimized\nalternately through a minimax game. Extensive experiments on four real-world\nshort text datasets demonstrate the superiority of the proposed model over\nseveral strong competitors, verifying that robust adversarial training yields\nsubstantial performance gains.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:22:07 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 01:18:30 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhang", "Wei", ""], ["Dong", "Chao", ""], ["Yin", "Jianhua", ""], ["Wang", "Jianyong", ""]]}, {"id": "1912.03804", "submitter": "Mojtaba Heidarysafa", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Tolu Odukoya, Philip Potter,\n  Laura E. Barnes, and Donald E. Brown", "title": "Women in ISIS Propaganda: A Natural Language Processing Analysis of\n  Topics and Emotions in a Comparison with Mainstream Religious Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online propaganda is central to the recruitment strategies of extremist\ngroups and in recent years these efforts have increasingly extended to women.\nTo investigate ISIS' approach to targeting women in their online propaganda and\nuncover implications for counterterrorism, we rely on text mining and natural\nlanguage processing (NLP). Specifically, we extract articles published in Dabiq\nand Rumiyah (ISIS's online English language publications) to identify prominent\ntopics. To identify similarities or differences between these texts and those\nproduced by non-violent religious groups, we extend the analysis to articles\nfrom a Catholic forum dedicated to women. We also perform an emotional analysis\nof both of these resources to better understand the emotional components of\npropaganda. We rely on Depechemood (a lexical-base emotion analysis method) to\ndetect emotions most likely to be evoked in readers of these materials. The\nfindings indicate that the emotional appeal of ISIS and Catholic materials are\nsimilar\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:11:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Odukoya", "Tolu", ""], ["Potter", "Philip", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1912.03832", "submitter": "Tapas Nayak", "authors": "Tapas Nayak and Hwee Tou Ng", "title": "Effective Attention Modeling for Neural Relation Extraction", "comments": "Accepted at CoNLL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction is the task of determining the relation between two\nentities in a sentence. Distantly-supervised models are popular for this task.\nHowever, sentences can be long and two entities can be located far from each\nother in a sentence. The pieces of evidence supporting the presence of a\nrelation between two entities may not be very direct, since the entities may be\nconnected via some indirect links such as a third entity or via co-reference.\nRelation extraction in such scenarios becomes more challenging as we need to\ncapture the long-distance interactions among the entities and other words in\nthe sentence. Also, the words in a sentence do not contribute equally in\nidentifying the relation between the two entities. To address this issue, we\npropose a novel and effective attention model which incorporates syntactic\ninformation of the sentence and a multi-factor attention mechanism. Experiments\non the New York Times corpus show that our proposed model outperforms prior\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 03:38:16 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Nayak", "Tapas", ""], ["Ng", "Hwee Tou", ""]]}, {"id": "1912.03879", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala and Malihe Alikhani and Jonas Haverinen and Timo\n  Kalliokoski and Evanfiya Logacheva and Serafina Orekhova and Aino Tuomainen\n  and Matthew Stone and John A. Bateman", "title": "AI2D-RST: A multimodal corpus of 1000 primary school science diagrams", "comments": "24 pages; revised version submitted to Language Resources &\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces AI2D-RST, a multimodal corpus of 1000\nEnglish-language diagrams that represent topics in primary school natural\nsciences, such as food webs, life cycles, moon phases and human physiology. The\ncorpus is based on the Allen Institute for Artificial Intelligence Diagrams\n(AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which\nwas originally developed to support research on automatic diagram understanding\nand visual question answering. Building on the segmentation of diagram layouts\nin AI2D, the AI2D-RST corpus presents a new multi-layer annotation schema that\nprovides a rich description of their multimodal structure. Annotated by trained\nexperts, the layers describe (1) the grouping of diagram elements into\nperceptual units, (2) the connections set up by diagrammatic elements such as\narrows and lines, and (3) the discourse relations between diagram elements,\nwhich are described using Rhetorical Structure Theory (RST). Each annotation\nlayer in AI2D-RST is represented using a graph. The corpus is freely available\nfor research and teaching.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:22:54 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:03:17 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Hiippala", "Tuomo", ""], ["Alikhani", "Malihe", ""], ["Haverinen", "Jonas", ""], ["Kalliokoski", "Timo", ""], ["Logacheva", "Evanfiya", ""], ["Orekhova", "Serafina", ""], ["Tuomainen", "Aino", ""], ["Stone", "Matthew", ""], ["Bateman", "John A.", ""]]}, {"id": "1912.03884", "submitter": "Chao-I Tuan", "authors": "Chao-I Tuan, Yuan-Kuei Wu, Hung-yi Lee, Yu Tsao", "title": "MITAS: A Compressed Time-Domain Audio Separation Network with Parameter\n  Sharing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning methods have brought substantial advancements in speech\nseparation (SS). Nevertheless, it remains challenging to deploy\ndeep-learning-based models on edge devices. Thus, identifying an effective way\nto compress these large models without hurting SS performance has become an\nimportant research topic. Recently, TasNet and Conv-TasNet have been proposed.\nThey achieved state-of-the-art results on several standardized SS tasks.\nMoreover, their low latency natures make them definitely suitable for real-time\non-device applications. In this study, we propose two parameter-sharing schemes\nto lower the memory consumption on TasNet and Conv-TasNet. Accordingly, we\nderive a novel so-called MiTAS (Mini TasNet). Our experimental results first\nconfirmed the robustness of our MiTAS on two types of perturbations in mixed\naudio. We also designed a series of ablation experiments to analyze the\nrelation between SS performance and the amount of parameters in the model. The\nresults show that MiTAS is able to reduce the model size by a factor of four\nwhile maintaining comparable SS performance with improved stability as compared\nto TasNet and Conv-TasNet. This suggests that MiTAS is more suitable for\nreal-time low latency applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 07:44:32 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Tuan", "Chao-I", ""], ["Wu", "Yuan-Kuei", ""], ["Lee", "Hung-yi", ""], ["Tsao", "Yu", ""]]}, {"id": "1912.04050", "submitter": "Gang Chen", "authors": "Gang Chen, Shengyu He, Haitao Meng, Kai Huang", "title": "PhoneBit: Efficient GPU-Accelerated Binary Neural Network Inference\n  Engine for Mobile Phones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, a great success of deep neural networks (DNNs) has been\nwitnessed in computer vision and other fields. However, performance and power\nconstraints make it still challenging to deploy DNNs on mobile devices due to\ntheir high computational complexity. Binary neural networks (BNNs) have been\ndemonstrated as a promising solution to achieve this goal by using bit-wise\noperations to replace most arithmetic operations. Currently, existing\nGPU-accelerated implementations of BNNs are only tailored for desktop\nplatforms. Due to architecture differences, mere porting of such\nimplementations to mobile devices yields suboptimal performance or is\nimpossible in some cases. In this paper, we propose PhoneBit, a GPU-accelerated\nBNN inference engine for Android-based mobile devices that fully exploits the\ncomputing power of BNNs on mobile GPUs. PhoneBit provides a set of\noperator-level optimizations including locality-friendly data layout, bit\npacking with vectorization and layers integration for efficient binary\nconvolution. We also provide a detailed implementation and parallelization\noptimization for PhoneBit to optimally utilize the memory bandwidth and\ncomputing power of mobile GPUs. We evaluate PhoneBit with AlexNet, YOLOv2 Tiny\nand VGG16 with their binary version. Our experiment results show that PhoneBit\ncan achieve significant speedup and energy efficiency compared with\nstate-of-the-art frameworks for mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:52:24 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Gang", ""], ["He", "Shengyu", ""], ["Meng", "Haitao", ""], ["Huang", "Kai", ""]]}, {"id": "1912.04370", "submitter": "Aparna Balagopalan", "authors": "Aparna Balagopalan, Jekaterina Novikova, Matthew B. A. McDermott, Bret\n  Nestor, Tristan Naumann, Marzyeh Ghassemi", "title": "Cross-Language Aphasia Detection using Optimal Transport Domain\n  Adaptation", "comments": "Accepted to ML4H at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-language speech datasets are scarce and often have small sample sizes\nin the medical domain. Robust transfer of linguistic features across languages\ncould improve rates of early diagnosis and therapy for speakers of low-resource\nlanguages when detecting health conditions from speech. We utilize\nout-of-domain, unpaired, single-speaker, healthy speech data for training\nmultiple Optimal Transport (OT) domain adaptation systems. We learn mappings\nfrom other languages to English and detect aphasia from linguistic\ncharacteristics of speech, and show that OT domain adaptation improves aphasia\ndetection over unilingual baselines for French (6% increased F1) and Mandarin\n(5% increased F1). Further, we show that adding aphasic data to the domain\nadaptation system significantly increases performance for both French and\nMandarin, increasing the F1 scores further (10% and 8% increase in F1 scores\nfor French and Mandarin, respectively, over unilingual baselines).\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 19:48:54 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Balagopalan", "Aparna", ""], ["Novikova", "Jekaterina", ""], ["McDermott", "Matthew B. A.", ""], ["Nestor", "Bret", ""], ["Naumann", "Tristan", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1912.04419", "submitter": "Seid Muhie Yimam", "authors": "Seid Muhie Yimam and Abinew Ali Ayele and Chris Biemann", "title": "Analysis of the Ethiopic Twitter Dataset for Abusive Speech in Amharic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an analysis of the first Ethiopic Twitter Dataset\nfor the Amharic language targeted for recognizing abusive speech. The dataset\nhas been collected since 2014 that is written in Fidel script. Since several\nlanguages can be written using the Fidel script, we have used the existing\nAmharic, Tigrinya and Ge'ez corpora to retain only the Amharic tweets. We have\nanalyzed the tweets for abusive speech content with the following targets:\nAnalyze the distribution and tendency of abusive speech content over time and\ncompare the abusive speech content between a Twitter and general reference\nAmharic corpus.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 23:18:13 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Yimam", "Seid Muhie", ""], ["Ayele", "Abinew Ali", ""], ["Biemann", "Chris", ""]]}, {"id": "1912.04471", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra and Nick Craswell", "title": "Duet at TREC 2019 Deep Learning Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses three submissions based on the Duet architecture to the\nDeep Learning track at TREC 2019. For the document retrieval task, we adapt the\nDuet model to ingest a \"multiple field\" view of documents---we refer to the new\narchitecture as Duet with Multiple Fields (DuetMF). A second submission\ncombines the DuetMF model with other neural and traditional relevance\nestimators in a learning-to-rank framework and achieves improved performance\nover the DuetMF baseline. For the passage retrieval task, we submit a single\nrun based on an ensemble of eight Duet models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:23:05 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""]]}, {"id": "1912.04479", "submitter": "Sawsan Alqahtani", "authors": "Sawsan Alqahtani, Hanan Aldarmaki, Mona Diab", "title": "Homograph Disambiguation Through Selective Diacritic Restoration", "comments": "accepted in WANLP 2019", "journal-ref": null, "doi": "10.18653/v1/W19-4606", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical ambiguity, a challenging phenomenon in all natural languages, is\nparticularly prevalent for languages with diacritics that tend to be omitted in\nwriting, such as Arabic. Omitting diacritics leads to an increase in the number\nof homographs: different words with the same spelling. Diacritic restoration\ncould theoretically help disambiguate these words, but in practice, the\nincrease in overall sparsity leads to performance degradation in NLP\napplications. In this paper, we propose approaches for automatically marking a\nsubset of words for diacritic restoration, which leads to selective homograph\ndisambiguation. Compared to full or no diacritic restoration, these approaches\nyield selectively-diacritized datasets that balance sparsity and lexical\ndisambiguation. We evaluate the various selection strategies extrinsically on\nseveral downstream applications: neural machine translation, part-of-speech\ntagging, and semantic textual similarity. Our experiments on Arabic show\npromising results, where our devised strategies on selective diacritization\nlead to a more balanced and consistent performance in downstream applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:45:42 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Alqahtani", "Sawsan", ""], ["Aldarmaki", "Hanan", ""], ["Diab", "Mona", ""]]}, {"id": "1912.04639", "submitter": "Gustavo Penha", "authors": "Gustavo Penha, Alexandru Balan and Claudia Hauff", "title": "Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is an approach to information retrieval (IR), where\nusers engage in a dialogue with an agent in order to satisfy their information\nneeds. Previous conceptual work described properties and actions a good agent\nshould exhibit. Unlike them, we present a novel conceptual model defined in\nterms of conversational goals, which enables us to reason about current\nresearch practices in conversational search. Based on the literature, we elicit\nhow existing tasks and test collections from the fields of IR, natural language\nprocessing (NLP) and dialogue systems (DS) fit into this model. We describe a\nset of characteristics that an ideal conversational search dataset should have.\nLastly, we introduce MANtIS (the code and dataset are available at\nhttps://guzpenha.github.io/MANtIS/), a large-scale dataset containing\nmulti-domain and grounded information seeking dialogues that fulfill all of our\ndataset desiderata. We provide baseline results for the conversation response\nranking and user intent prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:59:47 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Penha", "Gustavo", ""], ["Balan", "Alexandru", ""], ["Hauff", "Claudia", ""]]}, {"id": "1912.04664", "submitter": "Xiangyang Zhou", "authors": "Lu Li, Zhongheng He, Xiangyang Zhou and Dianhai Yu", "title": "How to Evaluate the Next System: Automatic Dialogue Evaluation from the\n  Perspective of Continual Learning", "comments": "10 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic dialogue evaluation plays a crucial role in open-domain dialogue\nresearch. Previous works train neural networks with limited annotation for\nconducting automatic dialogue evaluation, which would naturally affect the\nevaluation fairness as dialogue systems close to the scope of training corpus\nwould have more preference than the other ones. In this paper, we study\nalleviating this problem from the perspective of continual learning: given an\nexisting neural dialogue evaluator and the next system to be evaluated, we\nfine-tune the learned neural evaluator by selectively forgetting/updating its\nparameters, to jointly fit dialogue systems have been and will be evaluated.\nOur motivation is to seek for a lifelong and low-cost automatic evaluation for\ndialogue systems, rather than to reconstruct the evaluator over and over again.\nExperimental results show that our continual evaluator achieves comparable\nperformance with reconstructing new evaluators, while requires significantly\nlower resources.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 12:27:45 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Li", "Lu", ""], ["He", "Zhongheng", ""], ["Zhou", "Xiangyang", ""], ["Yu", "Dianhai", ""]]}, {"id": "1912.04748", "submitter": "Nikesh Bajaj", "authors": "Nikesh Bajaj, Tracy Goodluck Constance, Marvin Rajwadi, Julie Wall,\n  Mansour Moniri, Cornelius Glackin, Nigel Cannings, Chris Woodruff, James\n  Laird", "title": "Fraud detection in telephone conversations for financial services using\n  linguistic features", "comments": "Published - 33rd Conference on Neural Information Processing Systems\n  (NeurIPS 2019), AI for Social Good Workshop, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting the elements of deception in a conversation is one of the most\nchallenging problems for the AI community. It becomes even more difficult to\ndesign a transparent system, which is fully explainable and satisfies the need\nfor financial and legal services to be deployed. This paper presents an\napproach for fraud detection in transcribed telephone conversations using\nlinguistic features. The proposed approach exploits the syntactic and semantic\ninformation of the transcription to extract both the linguistic markers and the\nsentiment of the customer's response. We demonstrate the results on real-world\nfinancial services data using simple, robust and explainable classifiers such\nas Naive Bayes, Decision Tree, Nearest Neighbours, and Support Vector Machines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:07:48 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bajaj", "Nikesh", ""], ["Constance", "Tracy Goodluck", ""], ["Rajwadi", "Marvin", ""], ["Wall", "Julie", ""], ["Moniri", "Mansour", ""], ["Glackin", "Cornelius", ""], ["Cannings", "Nigel", ""], ["Woodruff", "Chris", ""], ["Laird", "James", ""]]}, {"id": "1912.04778", "submitter": "Marta R. Costa-juss\\`a", "authors": "Marta R. Costa-juss\\`a, Pau Li Lin and Cristina Espa\\~na-Bonet", "title": "GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual\n  Corpus of Wikipedia Biographies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GeBioToolkit, a tool for extracting multilingual parallel\ncorpora at sentence level, with document and gender information from Wikipedia\nbiographies. Despite thegender inequalitiespresent in Wikipedia, the toolkit\nhas been designed to extract corpus balanced in gender. While our toolkit is\ncustomizable to any number of languages (and different domains), in this work\nwe present a corpus of 2,000 sentences in English, Spanish and Catalan, which\nhas been post-edited by native speakers to become a high-quality dataset for\nmachinetranslation evaluation. While GeBioCorpus aims at being one of the first\nnon-synthetic gender-balanced test datasets, GeBioToolkit aims at paving the\npath to standardize procedures to produce gender-balanced datasets\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:50:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Costa-juss\u00e0", "Marta R.", ""], ["Lin", "Pau Li", ""], ["Espa\u00f1a-Bonet", "Cristina", ""]]}, {"id": "1912.04784", "submitter": "Taiyang Zhao", "authors": "Taiyang Zhao", "title": "A Novel Topology for End-to-end Temporal Classification and Segmentation\n  with Recurrent Neural Network", "comments": "4 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectionist temporal classification (CTC) has matured as an alignment free\nto sequence transduction and shows competitive for end-to-end speech\nrecognition. In the CTC topology, the blank symbol occupies more than half of\nthe state trellis, which results the spike phenomenon of the non-blank symbols.\nFor classification task, the spikes work quite well, but as to the segmentation\ntask it does not provide boundaries information. In this paper, a novel\ntopology is introduced to combine the temporal classification and segmentation\nability in one framework.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:53:59 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Zhao", "Taiyang", ""]]}, {"id": "1912.04853", "submitter": "Brandon Carter", "authors": "Angie Boggust, Brandon Carter, Arvind Satyanarayan", "title": "Embedding Comparator: Visualizing Differences in Global Structure and\n  Local Neighborhoods via Small Multiples", "comments": "Equal contribution by first two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings mapping high-dimensional discrete input to lower-dimensional\ncontinuous vector spaces have been widely adopted in machine learning\napplications as a way to capture domain semantics. Interviewing 13 embedding\nusers across disciplines, we find comparing embeddings is a key task for\ndeployment or downstream analysis but unfolds in a tedious fashion that poorly\nsupports systematic exploration. In response, we present the Embedding\nComparator, an interactive system that presents a global comparison of\nembedding spaces alongside fine-grained inspection of local neighborhoods. It\nsystematically surfaces points of comparison by computing the similarity of the\n$k$-nearest neighbors of every embedded object between a pair of spaces.\nThrough case studies, we demonstrate our system rapidly reveals insights, such\nas semantic changes following fine-tuning, language changes over time, and\ndifferences between seemingly similar models. In evaluations with 15\nparticipants, we find our system accelerates comparisons by shifting from\nlaborious manual specification to browsing and manipulating visualizations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:46:43 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 21:28:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Boggust", "Angie", ""], ["Carter", "Brandon", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "1912.04961", "submitter": "Sai Prabhakar Pandi Selvaraj", "authors": "Sai P. Selvaraj, Sandeep Konam", "title": "Medication Regimen Extraction From Medical Conversations", "comments": "Proceedings of International Workshop on Health Intelligence\n  (W3PHIAI) of the 34th AAAI Conference on Artificial Intelligence, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting relevant information from medical conversations and providing it\nto doctors and patients might help in addressing doctor burnout and patient\nforgetfulness. In this paper, we focus on extracting the Medication Regimen\n(dosage and frequency for medications) discussed in a medical conversation. We\nframe the problem as a Question Answering (QA) task and perform comparative\nanalysis over: a QA approach, a new combined QA and Information Extraction\napproach, and other baselines. We use a small corpus of 6,692 annotated\ndoctor-patient conversations for the task. Clinical conversation corpora are\ncostly to create, difficult to handle (because of data privacy concerns), and\nthus scarce. We address this data scarcity challenge through data augmentation\nmethods, using publicly available embeddings and pretrain part of the network\non a related task (summarization) to improve the model's performance. Compared\nto the baseline, our best-performing models improve the dosage and frequency\nextractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94,\nrespectively. Using our best-performing model, we present the first fully\nautomated system that can extract Medication Regimen tags from spontaneous\ndoctor-patient conversations with about $\\approx$71% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:18:39 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 04:03:31 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 18:04:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Selvaraj", "Sai P.", ""], ["Konam", "Sandeep", ""]]}, {"id": "1912.04965", "submitter": "Stamatis Outsios", "authors": "Michalis Lioudakis, Stamatis Outsios, Michalis Vazirgiannis", "title": "An Ensemble Method for Producing Word Representations focusing on the\n  Greek Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new ensemble method, Continuous Bag-of-Skip-grams\n(CBOS), that produces high-quality word representations putting emphasis on the\nmodern Greek language. The CBOS method combines the pioneering approaches for\nlearning word representations: Continuous Bag-of-Words (CBOW) and Continuous\nSkip-gram. These methods are compared through intrinsic and extrinsic\nevaluation tasks on three different sources of data: the English Wikipedia\ncorpus, the modern Greek Wikipedia corpus, and the modern Greek Web Content\ncorpus. By comparing these methods across different tasks and datasets, it is\nevident that the CBOS method achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:20:40 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 19:28:03 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lioudakis", "Michalis", ""], ["Outsios", "Stamatis", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1912.04971", "submitter": "Nitish Gupta", "authors": "Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner", "title": "Neural Module Networks for Reasoning over Text", "comments": "Published in ICLR 2020 (International Conference on Learning\n  Representations, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering compositional questions that require multiple steps of reasoning\nagainst text is challenging, especially when they involve discrete, symbolic\noperations. Neural module networks (NMNs) learn to parse such questions as\nexecutable programs composed of learnable modules, performing well on synthetic\nvisual QA domains. However, we find that it is challenging to learn these\nmodels for non-synthetic questions on open-domain text, where a model needs to\ndeal with the diversity of natural language and perform a broader range of\nreasoning. We extend NMNs by: (a) introducing modules that reason over a\nparagraph of text, performing symbolic reasoning (such as arithmetic, sorting,\ncounting) over numbers and dates in a probabilistic and differentiable manner;\nand (b) proposing an unsupervised auxiliary loss to help extract arguments\nassociated with the events in text. Additionally, we show that a limited amount\nof heuristically-obtained question program and intermediate module output\nsupervision provides sufficient inductive bias for accurate learning. Our\nproposed model significantly outperforms state-of-the-art models on a subset of\nthe DROP dataset that poses a variety of reasoning challenges that are covered\nby our modules.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:36:07 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 19:26:05 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gupta", "Nitish", ""], ["Lin", "Kevin", ""], ["Roth", "Dan", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""]]}, {"id": "1912.04979", "submitter": "Takuya Yoshioka", "authors": "Takuya Yoshioka, Igor Abramovski, Cem Aksoylar, Zhuo Chen, Moshe\n  David, Dimitrios Dimitriadis, Yifan Gong, Ilya Gurvich, Xuedong Huang, Yan\n  Huang, Aviv Hurvitz, Li Jiang, Sharon Koubi, Eyal Krupka, Ido Leichter,\n  Changliang Liu, Partha Parthasarathy, Alon Vinnikov, Lingfeng Wu, Xiong Xiao,\n  Wayne Xiong, Huaming Wang, Zhenghao Wang, Jun Zhang, Yong Zhao, Tianyan Zhou", "title": "Advances in Online Audio-Visual Meeting Transcription", "comments": "To appear in Proc. IEEE ASRU Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a system that generates speaker-annotated transcripts of\nmeetings by using a microphone array and a 360-degree camera. The hallmark of\nthe system is its ability to handle overlapped speech, which has been an\nunsolved problem in realistic settings for over a decade. We show that this\nproblem can be addressed by using a continuous speech separation approach. In\naddition, we describe an online audio-visual speaker diarization method that\nleverages face tracking and identification, sound source localization, speaker\nidentification, and, if available, prior speaker information for robustness to\nvarious real world challenges. All components are integrated in a meeting\ntranscription framework called SRD, which stands for \"separate, recognize, and\ndiarize\". Experimental results using recordings of natural meetings involving\nup to 11 attendees are reported. The continuous speech separation improves a\nword error rate (WER) by 16.1% compared with a highly tuned beamformer. When a\ncomplete list of meeting attendees is available, the discrepancy between WER\nand speaker-attributed WER is only 1.0%, indicating accurate word-to-speaker\nassociation. This increases marginally to 1.6% when 50% of the attendees are\nunknown to the system.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:59:24 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yoshioka", "Takuya", ""], ["Abramovski", "Igor", ""], ["Aksoylar", "Cem", ""], ["Chen", "Zhuo", ""], ["David", "Moshe", ""], ["Dimitriadis", "Dimitrios", ""], ["Gong", "Yifan", ""], ["Gurvich", "Ilya", ""], ["Huang", "Xuedong", ""], ["Huang", "Yan", ""], ["Hurvitz", "Aviv", ""], ["Jiang", "Li", ""], ["Koubi", "Sharon", ""], ["Krupka", "Eyal", ""], ["Leichter", "Ido", ""], ["Liu", "Changliang", ""], ["Parthasarathy", "Partha", ""], ["Vinnikov", "Alon", ""], ["Wu", "Lingfeng", ""], ["Xiao", "Xiong", ""], ["Xiong", "Wayne", ""], ["Wang", "Huaming", ""], ["Wang", "Zhenghao", ""], ["Zhang", "Jun", ""], ["Zhao", "Yong", ""], ["Zhou", "Tianyan", ""]]}, {"id": "1912.05066", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Ronghuo Zheng, Yuezhang Li, Katia Sycara", "title": "Event Outcome Prediction using Sentiment Analysis and Crowd Wisdom in\n  Microblog Feeds", "comments": "9 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis of microblog feeds has attracted considerable interest in\nrecent times. Most of the current work focuses on tweet sentiment\nclassification. But not much work has been done to explore how reliable the\nopinions of the mass (crowd wisdom) in social network microblogs such as\ntwitter are in predicting outcomes of certain events such as election debates.\nIn this work, we investigate whether crowd wisdom is useful in predicting such\noutcomes and whether their opinions are influenced by the experts in the field.\nWe work in the domain of multi-label classification to perform sentiment\nclassification of tweets and obtain the opinion of the crowd. This learnt\nsentiment is then used to predict outcomes of events such as: US Presidential\nDebate winners, Grammy Award winners, Super Bowl Winners. We find that in most\nof the cases, the wisdom of the crowd does indeed match with that of the\nexperts, and in cases where they don't (particularly in the case of debates),\nwe see that the crowd's opinion is actually influenced by that of the experts.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 00:30:24 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Zheng", "Ronghuo", ""], ["Li", "Yuezhang", ""], ["Sycara", "Katia", ""]]}, {"id": "1912.05082", "submitter": "Caroline Schroeder", "authors": "Caroline T. Schroeder, Amir Zeldes", "title": "A Collaborative Ecosystem for Digital Coptic Studies", "comments": "9 pages; paper presented at the Stanford University CESTA Workshop\n  \"Collecting, Preserving and Disseminating Endangered Cultural Heritage for\n  New Understandings Through Multilingual Approaches\"", "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Collecting, Preserving, and Disseminating Endangered Cultural Heritage for\n  New Understandings through Multilingual Approaches (September 23, 2020)\n  jdmdh:6797", "doi": "10.46298/jdmdh.5969", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scholarship on underresourced languages bring with them a variety of\nchallenges which make access to the full spectrum of source materials and their\nevaluation difficult. For Coptic in particular, large scale analyses and any\nkind of quantitative work become difficult due to the fragmentation of\nmanuscripts, the highly fusional nature of an incorporational morphology, and\nthe complications of dealing with influences from Hellenistic era Greek, among\nother concerns. Many of these challenges, however, can be addressed using\nDigital Humanities tools and standards. In this paper, we outline some of the\nlatest developments in Coptic Scriptorium, a DH project dedicated to bringing\nCoptic resources online in uniform, machine readable, and openly available\nformats. Collaborative web-based tools create online 'virtual departments' in\nwhich scholars dispersed sparsely across the globe can collaborate, and natural\nlanguage processing tools counterbalance the scarcity of trained editors by\nenabling machine processing of Coptic text to produce searchable, annotated\ncorpora.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:03:31 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 19:39:30 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 19:22:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Schroeder", "Caroline T.", ""], ["Zeldes", "Amir", ""]]}, {"id": "1912.05124", "submitter": "Xi Chen", "authors": "Xi Chen, Shouyi Yin, Dandan Song, Peng Ouyang, Leibo Liu, Shaojun Wei", "title": "Small-footprint Keyword Spotting with Graph Convolutional Network", "comments": "Accepted by the IEEE Automatic Speech Recognition and Understanding\n  Workshop(ASRU 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of deep neural networks, it remains challenging\nto achieve high precision keyword spotting task (KWS) on resource-constrained\ndevices. In this study, we propose a novel context-aware and compact\narchitecture for keyword spotting task. Based on residual connection and\nbottleneck structure, we design a compact and efficient network for KWS task.\nTo leverage the long range dependencies and global context of the convolutional\nfeature maps, the graph convolutional network is introduced to encode the\nnon-local relations. By evaluated on the Google Speech Command Dataset, the\nproposed method achieves state-of-the-art performance and outperforms the prior\nworks by a large margin with lower computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:44:04 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Chen", "Xi", ""], ["Yin", "Shouyi", ""], ["Song", "Dandan", ""], ["Ouyang", "Peng", ""], ["Liu", "Leibo", ""], ["Wei", "Shaojun", ""]]}, {"id": "1912.05134", "submitter": "Baosong Yang", "authors": "Yu Wan and Baosong Yang and Derek F. Wong and Lidia S. Chao and Haihua\n  Du and Ben C.H. Ao", "title": "Unsupervised Neural Dialect Translation with Commonality and Diversity\n  Modeling", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a special machine translation task, dialect translation has two main\ncharacteristics: 1) lack of parallel training corpus; and 2) possessing similar\ngrammar between two sides of the translation. In this paper, we investigate how\nto exploit the commonality and diversity between dialects thus to build\nunsupervised translation models merely accessing to monolingual data.\nSpecifically, we leverage pivot-private embedding, layer coordination, as well\nas parameter sharing to sufficiently model commonality and diversity among\nsource and target, ranging from lexical, through syntactic, to semantic levels.\nIn order to examine the effectiveness of the proposed models, we collect 20\nmillion monolingual corpus for each of Mandarin and Cantonese, which are\nofficial language and the most widely used dialect in China. Experimental\nresults reveal that our methods outperform rule-based simplified and\ntraditional Chinese conversion and conventional unsupervised translation models\nover 12 BLEU scores.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 06:21:16 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wan", "Yu", ""], ["Yang", "Baosong", ""], ["Wong", "Derek F.", ""], ["Chao", "Lidia S.", ""], ["Du", "Haihua", ""], ["Ao", "Ben C. H.", ""]]}, {"id": "1912.05147", "submitter": "Chengkun Lang", "authors": "Huiwei Zhou, Xuefei Li, Weihong Yao, Zhuang Liu, Shixian Ning,\n  Chengkun Lang, and Lei Du", "title": "Improving Neural Protein-Protein Interaction Extraction with Knowledge\n  Selection", "comments": "Published in Computational Biology and Chemistry; 14 pages, 2 figures", "journal-ref": "Computational Biology and Chemistry, 2019, 83: 107146", "doi": "10.1016/j.compbiolchem.2019.107146", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein-protein interaction (PPI) extraction from published scientific\nliterature provides additional support for precision medicine efforts.\nMeanwhile, knowledge bases (KBs) contain huge amounts of structured information\nof protein entities and their relations, which can be encoded in entity and\nrelation embeddings to help PPI extraction. However, the prior knowledge of\nprotein-protein pairs must be selectively used so that it is suitable for\ndifferent contexts. This paper proposes a Knowledge Selection Model (KSM) to\nfuse the selected prior knowledge and context information for PPI extraction.\nFirstly, two Transformers encode the context sequence of a protein pair\naccording to each protein embedding, respectively. Then, the two outputs are\nfed to a mutual attention to capture the important context features towards the\nprotein pair. Next, the context features are used to distill the relation\nembedding by a knowledge selector. Finally, the selected relation embedding and\nthe context features are concatenated for PPI extraction. Experiments on the\nBioCreative VI PPI dataset show that KSM achieves a new state-of-the-art\nperformance (38.08% F1-score) by adding knowledge selection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:29:59 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Zhou", "Huiwei", ""], ["Li", "Xuefei", ""], ["Yao", "Weihong", ""], ["Liu", "Zhuang", ""], ["Ning", "Shixian", ""], ["Lang", "Chengkun", ""], ["Du", "Lei", ""]]}, {"id": "1912.05156", "submitter": "Lambert Schomaker", "authors": "Lambert Schomaker", "title": "Lifelong learning for text retrieval and recognition in historical\n  handwritten document collections", "comments": "To appear as chapter in book: Handwritten Historical Document\n  Analysis, Recognition, and Retrieval -- State of the Art and Future Trends,\n  in the book series: Series in Machine Perception and Artificial Intelligence\n  World Scientific, ISSN (print): 1793-0839 Original version deposited at\n  Zenodo: https://zenodo.org/record/2346885#.XfCfsq5ytpg on December 17, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides an overview of the problems that need to be dealt with\nwhen constructing a lifelong-learning retrieval, recognition and indexing\nengine for large historical document collections in multiple scripts and\nlanguages, the Monk system. This application is highly variable over time,\nsince the continuous labeling by end users changes the concept of what a\n'ground truth' constitutes. Although current advances in deep learning provide\na huge potential in this application domain, the scale of the problem, i.e.,\nmore than 520 hugely diverse books, documents and manuscripts precludes the\ncurrent meticulous and painstaking human effort which is required in designing\nand developing successful deep-learning systems. The ball-park principle is\nintroduced, which describes the evolution from the sparsely-labeled stage that\ncan only be addressed by traditional methods or nearest-neighbor methods on\nembedded vectors of pre-trained neural networks, up to the other end of the\nspectrum where massive labeling allows reliable training of deep-learning\nmethods. Contents: Introduction, Expectation management, Deep learning, The\nball-park principle, Technical realization, Work flow, Quality and quantity of\nmaterial, Industrialization and scalability, Human effort, Algorithms, Object\nof recognition, Processing pipeline, Performance,Compositionality, Conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:56:31 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Schomaker", "Lambert", ""]]}, {"id": "1912.05200", "submitter": "Jos\\'e A. R. Fonollosa", "authors": "Casimiro Pio Carrino, Marta R. Costa-juss\\`a, Jos\\'e A. R. Fonollosa", "title": "Automatic Spanish Translation of the SQuAD Dataset for Multilingual\n  Question Answering", "comments": "Submitted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, multilingual question answering became a crucial research topic,\nand it is receiving increased interest in the NLP community. However, the\nunavailability of large-scale datasets makes it challenging to train\nmultilingual QA systems with performance comparable to the English ones. In\nthis work, we develop the Translate Align Retrieve (TAR) method to\nautomatically translate the Stanford Question Answering Dataset (SQuAD) v1.1 to\nSpanish. We then used this dataset to train Spanish QA systems by fine-tuning a\nMultilingual-BERT model. Finally, we evaluated our QA models with the recently\nproposed MLQA and XQuAD benchmarks for cross-lingual Extractive QA.\nExperimental results show that our models outperform the previous\nMultilingual-BERT baselines achieving the new state-of-the-art value of 68.1 F1\npoints on the Spanish MLQA corpus and 77.6 F1 and 61.8 Exact Match points on\nthe Spanish XQuAD corpus. The resulting, synthetically generated SQuAD-es v1.1\ncorpora, with almost 100% of data contained in the original English version, to\nthe best of our knowledge, is the first large-scale QA training resource for\nSpanish.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 09:33:21 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 16:05:41 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Carrino", "Casimiro Pio", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1912.05238", "submitter": "Patrick Schramowski", "authors": "Patrick Schramowski, Cigdem Turan, Sophie Jentzsch, Constantin\n  Rothkopf and Kristian Kersting", "title": "BERT has a Moral Compass: Improvements of ethical and moral values of\n  machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allowing machines to choose whether to kill humans would be devastating for\nworld peace and security. But how do we equip machines with the ability to\nlearn ethical or even moral choices? Jentzsch et al.(2019) showed that applying\nmachine learning to human texts can extract deontological ethical reasoning\nabout \"right\" and \"wrong\" conduct by calculating a moral bias score on a\nsentence level using sentence embeddings. The machine learned that it is\nobjectionable to kill living beings, but it is fine to kill time; It is\nessential to eat, yet one might not eat dirt; it is important to spread\ninformation, yet one should not spread misinformation. However, the evaluated\nmoral bias was restricted to simple actions -- one verb -- and a ranking of\nactions with surrounding context. Recently BERT ---and variants such as RoBERTa\nand SBERT--- has set a new state-of-the-art performance for a wide range of NLP\ntasks. But has BERT also a better moral compass? In this paper, we discuss and\nshow that this is indeed the case. Thus, recent improvements of language\nrepresentations also improve the representation of the underlying ethical and\nmoral values of the machine. We argue that through an advanced semantic\nrepresentation of text, BERT allows one to get better insights of moral and\nethical values implicitly represented in text. This enables the Moral Choice\nMachine (MCM) to extract more accurate imprints of moral choices and ethical\nvalues.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:27:06 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Schramowski", "Patrick", ""], ["Turan", "Cigdem", ""], ["Jentzsch", "Sophie", ""], ["Rothkopf", "Constantin", ""], ["Kersting", "Kristian", ""]]}, {"id": "1912.05274", "submitter": "G\\\"ozde G\\\"ul \\c{S}ahin", "authors": "G\\\"ozde G\\\"ul \\c{S}ahin and Iryna Gurevych", "title": "Two Birds with One Stone: Investigating Invertible Neural Networks for\n  Inverse Problems in Morphology", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most problems in natural language processing can be approximated as inverse\nproblems such as analysis and generation at variety of levels from\nmorphological (e.g., cat+Plural <-> cats) to semantic (e.g., (call + 1 2) <->\n\"Calculate one plus two.\"). Although the tasks in both directions are closely\nrelated, general approach in the field has been to design separate models\nspecific for each task. However, having one shared model for both tasks, would\nhelp the researchers exploit the common knowledge among these problems with\nreduced time and memory requirements. We investigate a specific class of neural\nnetworks, called Invertible Neural Networks (INNs) (Ardizzone et al. 2019) that\nenable simultaneous optimization in both directions, hence allow addressing of\ninverse problems via a single model. In this study, we investigate INNs on\nmorphological problems casted as inverse problems. We apply INNs to various\nmorphological tasks with varying ambiguity and show that they provide\ncompetitive performance in both directions. We show that they are able to\nrecover the morphological input parameters, i.e., predicting the lemma (e.g.,\ncat) or the morphological tags (e.g., Plural) when run in the reverse\ndirection, without any significant performance drop in the forward direction,\ni.e., predicting the surface form (e.g., cats).\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 12:50:48 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["\u015eahin", "G\u00f6zde G\u00fcl", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1912.05289", "submitter": "Marius Cotescu", "authors": "Marius Cotescu, Thomas Drugman, Goeric Huybrechts, Jaime\n  Lorenzo-Trueba, Alexis Moinet", "title": "Voice Conversion for Whispered Speech Synthesis", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2019.2961213", "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to synthesize whisper by applying a handcrafted signal\nprocessing recipe and Voice Conversion (VC) techniques to convert normally\nphonated speech to whispered speech. We investigate using Gaussian Mixture\nModels (GMM) and Deep Neural Networks (DNN) to model the mapping between\nacoustic features of normal speech and those of whispered speech. We evaluate\nnaturalness and speaker similarity of the converted whisper on an internal\ncorpus and on the publicly available wTIMIT corpus. We show that applying VC\ntechniques is significantly better than using rule-based signal processing\nmethods and it achieves results that are indistinguishable from copy-synthesis\nof natural whisper recordings. We investigate the ability of the DNN model to\ngeneralize on unseen speakers, when trained with data from multiple speakers.\nWe show that excluding the target speaker from the training set has little or\nno impact on the perceived naturalness and speaker similarity of the converted\nwhisper. The proposed DNN method is used in the newly released Whisper Mode of\nAmazon Alexa.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 13:34:43 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 20:43:49 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Cotescu", "Marius", ""], ["Drugman", "Thomas", ""], ["Huybrechts", "Goeric", ""], ["Lorenzo-Trueba", "Jaime", ""], ["Moinet", "Alexis", ""]]}, {"id": "1912.05308", "submitter": "Mehrdad Valipour", "authors": "Mehrdad Valipour, En-Shiun Annie Lee, Jaime R. Jamacaro, and Carolina\n  Bessega", "title": "Unsupervised Transfer Learning via BERT Neuron Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advancements in language representation models such as BERT have led\nto a rapid improvement in numerous natural language processing tasks. However,\nlanguage models usually consist of a few hundred million trainable parameters\nwith embedding space distributed across multiple layers, thus making them\nchallenging to be fine-tuned for a specific task or to be transferred to a new\ndomain. To determine whether there are task-specific neurons that can be\nexploited for unsupervised transfer learning, we introduce a method for\nselecting the most important neurons to solve a specific classification task.\nThis algorithm is further extended to multi-source transfer learning by\ncomputing the importance of neurons for several single-source transfer learning\nscenarios between different subsets of data sources. Besides, a task-specific\nfingerprint for each data source is obtained based on the percentage of the\nselected neurons in each layer. We perform extensive experiments in\nunsupervised transfer learning for sentiment analysis, natural language\ninference and sentence similarity, and compare our results with the existing\nliterature and baselines. Significantly, we found that the source and target\ndata sources with higher degrees of similarity between their task-specific\nfingerprints demonstrate a better transferability property. We conclude that\nour method can lead to better performance using just a few hundred\ntask-specific and interpretable neurons.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 16:08:26 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Valipour", "Mehrdad", ""], ["Lee", "En-Shiun Annie", ""], ["Jamacaro", "Jaime R.", ""], ["Bessega", "Carolina", ""]]}, {"id": "1912.05320", "submitter": "Carlos S. Armendariz", "authors": "Carlos Santos Armendariz, Matthew Purver, Matej Ul\\v{c}ar, Senja\n  Pollak, Nikola Ljube\\v{s}i\\'c, Marko Robnik-\\v{S}ikonja, Mark\n  Granroth-Wilding, Kristiina Vaik", "title": "CoSimLex: A Resource for Evaluating Graded Word Similarity in Context", "comments": null, "journal-ref": "Proceedings of the 12th Language Resources and Evaluation\n  Conference (2020) 5878-5886", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art natural language processing tools are built on\ncontext-dependent word embeddings, but no direct method for evaluating these\nrepresentations currently exists. Standard tasks and datasets for intrinsic\nevaluation of embeddings are based on judgements of similarity, but ignore\ncontext; standard tasks for word sense disambiguation take account of context\nbut do not provide continuous measures of meaning similarity. This paper\ndescribes an effort to build a new dataset, CoSimLex, intended to fill this\ngap. Building on the standard pairwise similarity task of SimLex-999, it\nprovides context-dependent similarity measures; covers not only discrete\ndifferences in word sense but more subtle, graded changes in meaning; and\ncovers not only a well-resourced language (English) but a number of\nless-resourced languages. We define the task and evaluation metrics, outline\nthe dataset collection methodology, and describe the status of the dataset so\nfar.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:02:59 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 10:33:05 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 15:22:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Armendariz", "Carlos Santos", ""], ["Purver", "Matthew", ""], ["Ul\u010dar", "Matej", ""], ["Pollak", "Senja", ""], ["Ljube\u0161i\u0107", "Nikola", ""], ["Robnik-\u0160ikonja", "Marko", ""], ["Granroth-Wilding", "Mark", ""], ["Vaik", "Kristiina", ""]]}, {"id": "1912.05372", "submitter": "Hang Le", "authors": "Hang Le and Lo\\\"ic Vial and Jibril Frej and Vincent Segonne and\n  Maximin Coavoux and Benjamin Lecouteux and Alexandre Allauzen and Beno\\^it\n  Crabb\\'e and Laurent Besacier and Didier Schwab", "title": "FlauBERT: Unsupervised Language Model Pre-training for French", "comments": "Accepted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language models have become a key step to achieve state-of-the art results in\nmany different Natural Language Processing (NLP) tasks. Leveraging the huge\namount of unlabeled texts nowadays available, they provide an efficient way to\npre-train continuous word representations that can be fine-tuned for a\ndownstream task, along with their contextualization at the sentence level. This\nhas been widely demonstrated for English using contextualized representations\n(Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al.,\n2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and\nshare FlauBERT, a model learned on a very large and heterogeneous French\ncorpus. Models of different sizes are trained using the new CNRS (French\nNational Centre for Scientific Research) Jean Zay supercomputer. We apply our\nFrench language models to diverse NLP tasks (text classification, paraphrasing,\nnatural language inference, parsing, word sense disambiguation) and show that\nmost of the time they outperform other pre-training approaches. Different\nversions of FlauBERT as well as a unified evaluation protocol for the\ndownstream tasks, called FLUE (French Language Understanding Evaluation), are\nshared to the research community for further reproducible experiments in French\nNLP.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 14:59:32 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 18:57:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 23:22:38 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 23:58:56 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Le", "Hang", ""], ["Vial", "Lo\u00efc", ""], ["Frej", "Jibril", ""], ["Segonne", "Vincent", ""], ["Coavoux", "Maximin", ""], ["Lecouteux", "Benjamin", ""], ["Allauzen", "Alexandre", ""], ["Crabb\u00e9", "Beno\u00eet", ""], ["Besacier", "Laurent", ""], ["Schwab", "Didier", ""]]}, {"id": "1912.05467", "submitter": "Xun Wang Dr", "authors": "Rumeng Li, Xun Wang, Hong Yu", "title": "MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low\n  Resource Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating training data leads to robust neural models for MT.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:05:18 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Li", "Rumeng", ""], ["Wang", "Xun", ""], ["Yu", "Hong", ""]]}, {"id": "1912.05493", "submitter": "Hoa T. Le", "authors": "Hoa T. Le, Christophe Cerisara, Claire Gardent", "title": "Quality of syntactic implication of RL-based sentence summarization", "comments": "AAAI-20 Workshop on Engineering Dependable and Secure Machine\n  Learning Systems (EDSMLS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work on summarization has explored both reinforcement learning (RL)\noptimization using ROUGE as a reward and syntax-aware models, such as models\nthose input is enriched with part-of-speech (POS)-tags and dependency\ninformation. However, it is not clear what is the respective impact of these\napproaches beyond the standard ROUGE evaluation metric. Especially, RL-based\nfor summarization is becoming more and more popular. In this paper, we provide\na detailed comparison of these two approaches and of their combination along\nseveral dimensions that relate to the perceived quality of the generated\nsummaries: number of repeated words, distribution of part-of-speech tags,\nimpact of sentence length, relevance and grammaticality. Using the standard\nGigaword sentence summarization task, we compare an RL self-critical sequence\ntraining (SCST) method with syntax-aware models that leverage POS tags and\nDependency information. We show that on all qualitative evaluations, the\ncombined model gives the best results, but also that only training with RL and\nwithout any syntactic information already gives nearly as good results as\nsyntax-aware models with less parameters and faster training convergence.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:47:29 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Le", "Hoa T.", ""], ["Cerisara", "Christophe", ""], ["Gardent", "Claire", ""]]}, {"id": "1912.05525", "submitter": "Leon Lang", "authors": "Benjamin Kolb, Leon Lang, Henning Bartsch, Arwin Gansekoele, Raymond\n  Koopmanschap, Leonardo Romor, David Speck, Mathijs Mul, Elia Bruni", "title": "Learning to Request Guidance in Emergent Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research into agent communication has shown that a pre-trained guide\ncan speed up the learning process of an imitation learning agent. The guide\nachieves this by providing the agent with discrete messages in an emerged\nlanguage about how to solve the task. We extend this one-directional\ncommunication by a one-bit communication channel from the learner back to the\nguide: It is able to ask the guide for help, and we limit the guidance by\npenalizing the learner for these requests. During training, the agent learns to\ncontrol this gate based on its current observation. We find that the amount of\nrequested guidance decreases over time and guidance is requested in situations\nof high uncertainty. We investigate the agent's performance in cases of open\nand closed gates and discuss potential motives for the observed gating\nbehavior.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:48:05 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Kolb", "Benjamin", ""], ["Lang", "Leon", ""], ["Bartsch", "Henning", ""], ["Gansekoele", "Arwin", ""], ["Koopmanschap", "Raymond", ""], ["Romor", "Leonardo", ""], ["Speck", "David", ""], ["Mul", "Mathijs", ""], ["Bruni", "Elia", ""]]}, {"id": "1912.05533", "submitter": "Daniel Park", "authors": "Daniel S. Park, Yu Zhang, Chung-Cheng Chiu, Youzheng Chen, Bo Li,\n  William Chan, Quoc V. Le and Yonghui Wu", "title": "SpecAugment on Large Scale Datasets", "comments": "5 pages, 3 tables; submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, SpecAugment, an augmentation scheme for automatic speech\nrecognition that acts directly on the spectrogram of input utterances, has\nshown to be highly effective in enhancing the performance of end-to-end\nnetworks on public datasets. In this paper, we demonstrate its effectiveness on\ntasks with large scale datasets by investigating its application to the Google\nMultidomain Dataset (Narayanan et al., 2018). We achieve improvement across all\ntest domains by mixing raw training data augmented with SpecAugment and\nnoise-perturbed training data when training the acoustic model. We also\nintroduce a modification of SpecAugment that adapts the time mask size and/or\nmultiplicity depending on the length of the utterance, which can potentially\nbenefit large scale tasks. By using adaptive masking, we are able to further\nimprove the performance of the Listen, Attend and Spell model on LibriSpeech to\n2.2% WER on test-clean and 5.2% WER on test-other.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:58:58 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Park", "Daniel S.", ""], ["Zhang", "Yu", ""], ["Chiu", "Chung-Cheng", ""], ["Chen", "Youzheng", ""], ["Li", "Bo", ""], ["Chan", "William", ""], ["Le", "Quoc V.", ""], ["Wu", "Yonghui", ""]]}, {"id": "1912.05676", "submitter": "Tom Eccles", "authors": "Tom Eccles, Yoram Bachrach, Guy Lever, Angeliki Lazaridou, Thore\n  Graepel", "title": "Biases for Emergent Communication in Multi-agent Reinforcement Learning", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of emergent communication, in which language arises\nbecause speakers and listeners must communicate information in order to solve\ntasks. In temporally extended reinforcement learning domains, it has proved\nhard to learn such communication without centralized training of agents, due in\npart to a difficult joint exploration problem. We introduce inductive biases\nfor positive signalling and positive listening, which ease this problem. In a\nsimple one-step environment, we demonstrate how these biases ease the learning\nproblem. We also apply our methods to a more extended environment, showing that\nagents with these inductive biases achieve better performance, and analyse the\nresulting communication protocols.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:39:51 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Eccles", "Tom", ""], ["Bachrach", "Yoram", ""], ["Lever", "Guy", ""], ["Lazaridou", "Angeliki", ""], ["Graepel", "Thore", ""]]}, {"id": "1912.05728", "submitter": "Fenglin Li", "authors": "Feng-Lin Li, Weijia Chen, Qi Huang, Yikun Guo", "title": "AliMe KBQA: Question Answering over Structured Knowledge for E-commerce\n  Customer Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the rise of knowledge graph (KG), question answering over knowledge base\n(KBQA) has attracted increasing attention in recent years. Despite much\nresearch has been conducted on this topic, it is still challenging to apply\nKBQA technology in industry because business knowledge and real-world questions\ncan be rather complicated. In this paper, we present AliMe-KBQA, a bold attempt\nto apply KBQA in the E-commerce customer service field. To handle real\nknowledge and questions, we extend the classic \"subject-predicate-object (SPO)\"\nstructure with property hierarchy, key-value structure and compound value type\n(CVT), and enhance traditional KBQA with constraints recognition and reasoning\nability. We launch AliMe-KBQA in the Marketing Promotion scenario for merchants\nduring the \"Double 11\" period in 2018 and other such promotional events\nafterwards. Online results suggest that AliMe-KBQA is not only able to gain\nbetter resolution and improve customer satisfaction, but also becomes the\npreferred knowledge management method by business knowledge staffs since it\noffers a more convenient and efficient management experience.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:04:44 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Li", "Feng-Lin", ""], ["Chen", "Weijia", ""], ["Huang", "Qi", ""], ["Guo", "Yikun", ""]]}, {"id": "1912.05846", "submitter": "Jonathan Heras", "authors": "\\'Angela Casado-Garc\\'ia and C\\'esar Dom\\'inguez and J\\'onathan Heras\n  and Eloy Mata and Vico Pascual", "title": "The Benefits of Close-Domain Fine-Tuning for Table Detection in Document\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A correct localisation of tables in a document is instrumental for\ndetermining their structure and extracting their contents; therefore, table\ndetection is a key step in table understanding. Nowadays, the most successful\nmethods for table detection in document images employ deep learning algorithms;\nand, particularly, a technique known as fine-tuning. In this context, such a\ntechnique exports the knowledge acquired to detect objects in natural images to\ndetect tables in document images. However, there is only a vague relation\nbetween natural and document images, and fine-tuning works better when there is\na close relation between the source and target task. In this paper, we show\nthat it is more beneficial to employ fine-tuning from a closer domain. To this\naim, we train different object detection algorithms (namely, Mask R-CNN,\nRetinaNet, SSD and YOLO) using the TableBank dataset (a dataset of images of\nacademic documents designed for table detection and recognition), and fine-tune\nthem for several heterogeneous table detection datasets. Using this approach,\nwe considerably improve the accuracy of the detection models fine-tuned from\nnatural images (in mean a 17%, and, in the best case, up to a 60%).\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 09:30:02 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Casado-Garc\u00eda", "\u00c1ngela", ""], ["Dom\u00ednguez", "C\u00e9sar", ""], ["Heras", "J\u00f3nathan", ""], ["Mata", "Eloy", ""], ["Pascual", "Vico", ""]]}, {"id": "1912.05877", "submitter": "Hinrich Sch\\\"utze", "authors": "James L. McClelland, Felix Hill, Maja Rudolph, Jason Baldridge and\n  Hinrich Sch\\\"utze", "title": "Extending Machine Language Models toward Human-Level Language\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is crucial for human intelligence, but what exactly is its role? We\ntake language to be a part of a system for understanding and communicating\nabout situations. The human ability to understand and communicate about\nsituations emerges gradually from experience and depends on domain-general\nprinciples of biological neural networks: connection-based learning,\ndistributed representation, and context-sensitive, mutual constraint\nsatisfaction-based processing. Current artificial language processing systems\nrely on the same domain general principles, embodied in artificial neural\nnetworks. Indeed, recent progress in this field depends on \\emph{query-based\nattention}, which extends the ability of these systems to exploit context and\nhas contributed to remarkable breakthroughs. Nevertheless, most current models\nfocus exclusively on language-internal tasks, limiting their ability to perform\ntasks that depend on understanding situations. These systems also lack memory\nfor the contents of prior situations outside of a fixed contextual span. We\ndescribe the organization of the brain's distributed understanding system,\nwhich includes a fast learning system that addresses the memory problem. We\nsketch a framework for future models of understanding drawing equally on\ncognitive neuroscience and artificial intelligence and exploiting query-based\nattention. We highlight relevant current directions and consider further\ndevelopments needed to fully capture human-level language understanding in a\ncomputational system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 11:02:30 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 10:17:32 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["McClelland", "James L.", ""], ["Hill", "Felix", ""], ["Rudolph", "Maja", ""], ["Baldridge", "Jason", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1912.05881", "submitter": "Orazio Angelini", "authors": "Orazio Angelini, Alexis Moinet, Kayoko Yanagisawa, Thomas Drugman", "title": "Singing Synthesis: with a little help from my attention", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present UTACO, a singing synthesis model based on an attention-based\nsequence-to-sequence mechanism and a vocoder based on dilated causal\nconvolutions. These two classes of models have significantly affected the field\nof text-to-speech, but have never been thoroughly applied to the task of\nsinging synthesis. UTACO demonstrates that attention can be successfully\napplied to the singing synthesis field and improves naturalness over the state\nof the art. The system requires considerably less explicit modelling of voice\nfeatures such as F0 patterns, vibratos, and note and phoneme durations, than\nprevious models in the literature. Despite this, it shows a strong improvement\nin naturalness with respect to previous neural singing synthesis models. The\nmodel does not require any durations or pitch patterns as inputs, and learns to\ninsert vibrato autonomously according to the musical context. However, we\nobserve that, by completely dispensing with any explicit duration modelling it\nbecomes harder to obtain the fine control of timing needed to exactly match the\ntempo of a song.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 11:17:30 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 12:12:45 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Angelini", "Orazio", ""], ["Moinet", "Alexis", ""], ["Yanagisawa", "Kayoko", ""], ["Drugman", "Thomas", ""]]}, {"id": "1912.05898", "submitter": "Haitong Zhang", "authors": "Haitong Zhang, Yongping Du, Jiaxin Sun, Qingxiao Li", "title": "Improving Interpretability of Word Embeddings by Generating Definition\n  and Usage", "comments": null, "journal-ref": "Expert Systems with Applications, Volume 160, 1 December 2020,\n  113633", "doi": "10.1016/j.eswa.2020.113633", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are substantially successful in capturing semantic relations\namong words. However, these lexical semantics are difficult to be interpreted.\nDefinition modeling provides a more intuitive way to evaluate embeddings by\nutilizing them to generate natural language definitions of corresponding words.\nThis task is of great significance for practical application and in-depth\nunderstanding of word representations. We propose a novel framework for\ndefinition modeling, which can generate reasonable and understandable\ncontext-dependent definitions. Moreover, we introduce usage modeling and study\nwhether it is possible to utilize embeddings to generate example sentences of\nwords. These ways are a more direct and explicit expression of embedding's\nsemantics for better interpretability. We extend the single task model to\nmulti-task setting and investigate several joint multi-task models to combine\nusage modeling and definition modeling together. Experimental results on\nexisting Oxford dataset and a new collected Oxford-2019 dataset show that our\nsingle-task model achieves the state-of-the-art result in definition modeling\nand the multi-task learning methods are helpful for two tasks to improve the\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 12:45:34 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 15:17:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Haitong", ""], ["Du", "Yongping", ""], ["Sun", "Jiaxin", ""], ["Li", "Qingxiao", ""]]}, {"id": "1912.05957", "submitter": "Hamid Mohammadi", "authors": "Hamid Mohammadi, Seyed Hossein Khasteh", "title": "Text as Environment: A Deep Reinforcement Learning Text Readability\n  Assessment Model", "comments": "8 pages, 2 figures, 6 equations, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the readability of a text can significantly facilitate the precise\nexpression of information in a written form. The formulation of text\nreadability assessment demands the identification of meaningful properties of\nthe text and correct conversion of features to the right readability level.\nSophisticated features and models are being used to evaluate the\ncomprehensibility of texts accurately. Still, these models are challenging to\nimplement, heavily language-dependent, and do not perform well on short texts.\nDeep reinforcement learning models are demonstrated to be helpful in further\nimprovement of state-of-the-art text readability assessment models. The main\ncontributions of the proposed approach are the automation of feature\nextraction, loosening the tight language dependency of text readability\nassessment task, and efficient use of text by finding the minimum portion of a\ntext required to assess its readability. The experiments on Weebit, Cambridge\nExams, and Persian readability datasets display the model's state-of-the-art\nprecision, efficiency, and the capability to be applied to other languages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 13:54:09 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 15:46:55 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Mohammadi", "Hamid", ""], ["Khasteh", "Seyed Hossein", ""]]}, {"id": "1912.06174", "submitter": "Marta Skreta", "authors": "Marta Skreta, Aryan Arbabi, Jixuan Wang, Michael Brudno", "title": "Training without training data: Improving the generalizability of\n  automated medical abbreviation disambiguation", "comments": "NeurIPS Machine Learning for Healthcare 2019 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abbreviation disambiguation is important for automated clinical note\nprocessing due to the frequent use of abbreviations in clinical settings.\nCurrent models for automated abbreviation disambiguation are restricted by the\nscarcity and imbalance of labeled training data, decreasing their\ngeneralizability to orthogonal sources. In this work we propose a novel data\naugmentation technique that utilizes information from related medical concepts,\nwhich improves our model's ability to generalize. Furthermore, we show that\nincorporating the global context information within the whole medical note (in\naddition to the traditional local context window), can significantly improve\nthe model's representation for abbreviations. We train our model on a public\ndataset (MIMIC III) and test its performance on datasets from different sources\n(CASI, i2b2). Together, these two techniques boost the accuracy of abbreviation\ndisambiguation by almost 14% on the CASI dataset and 4% on i2b2.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 19:32:41 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Skreta", "Marta", ""], ["Arbabi", "Aryan", ""], ["Wang", "Jixuan", ""], ["Brudno", "Michael", ""]]}, {"id": "1912.06203", "submitter": "Bowen Li", "authors": "Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr", "title": "ManiGAN: Text-Guided Image Manipulation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our paper is to semantically edit parts of an image matching a\ngiven text that describes desired attributes (e.g., texture, colour, and\nbackground), while preserving other contents that are irrelevant to the text.\nTo achieve this, we propose a novel generative adversarial network (ManiGAN),\nwhich contains two key components: text-image affine combination module (ACM)\nand detail correction module (DCM). The ACM selects image regions relevant to\nthe given text and then correlates the regions with corresponding semantic\nwords for effective manipulation. Meanwhile, it encodes original image features\nto help reconstruct text-irrelevant contents. The DCM rectifies mismatched\nattributes and completes missing contents of the synthetic image. Finally, we\nsuggest a new metric for evaluating image manipulation results, in terms of\nboth the generation of new attributes and the reconstruction of text-irrelevant\ncontents. Extensive experiments on the CUB and COCO datasets demonstrate the\nsuperior performance of the proposed method. Code is available at\nhttps://github.com/mrlibw/ManiGAN.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:48:52 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 19:42:35 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Li", "Bowen", ""], ["Qi", "Xiaojuan", ""], ["Lukasiewicz", "Thomas", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1912.06208", "submitter": "Olivier Tieleman", "authors": "Olivier Tieleman, Angeliki Lazaridou, Shibl Mourad, Charles Blundell,\n  Doina Precup", "title": "Shaping representations through communication: community size effect in\n  artificial learning systems", "comments": "NeurIPS 2019 workshop on visually grounded interaction and language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by theories of language and communication that explain why\ncommunities with large numbers of speakers have, on average, simpler languages\nwith more regularity, we cast the representation learning problem in terms of\nlearning to communicate. Our starting point sees the traditional autoencoder\nsetup as a single encoder with a fixed decoder partner that must learn to\ncommunicate. Generalizing from there, we introduce community-based autoencoders\nin which multiple encoders and decoders collectively learn representations by\nbeing randomly paired up on successive training iterations. We find that\nincreasing community sizes reduce idiosyncrasies in the learned codes,\nresulting in representations that better encode concept categories and\ncorrelate with human feature norms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 20:56:59 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Tieleman", "Olivier", ""], ["Lazaridou", "Angeliki", ""], ["Mourad", "Shibl", ""], ["Blundell", "Charles", ""], ["Precup", "Doina", ""]]}, {"id": "1912.06214", "submitter": "Kuldeep Singh", "authors": "Isaiah Onando Mulang, Kuldeep Singh, Akhilesh Vyas, Saeedeh\n  Shekarpour, Maria Esther Vidal, Jens Lehmann, Soren Auer", "title": "Encoding Knowledge Graph Entity Aliases in Attentive Neural Network for\n  Wikidata Entity Linking", "comments": "15 pages", "journal-ref": "WISE 2020 (21st International Conference on Web Information\n  Systems Engineering)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collaborative knowledge graphs such as Wikidata excessively rely on the\ncrowd to author the information. Since the crowd is not bound to a standard\nprotocol for assigning entity titles, the knowledge graph is populated by\nnon-standard, noisy, long or even sometimes awkward titles. The issue of long,\nimplicit, and nonstandard entity representations is a challenge in Entity\nLinking (EL) approaches for gaining high precision and recall. Underlying KG,\nin general, is the source of target entities for EL approaches, however, it\noften contains other relevant information, such as aliases of entities (e.g.,\nObama and Barack Hussein Obama are aliases for the entity Barack Obama). EL\nmodels usually ignore such readily available entity attributes. In this paper,\nwe examine the role of knowledge graph context on an attentive neural network\napproach for entity linking on Wikidata. Our approach contributes by exploiting\nthe sufficient context from a KG as a source of background knowledge, which is\nthen fed into the neural network. This approach demonstrates merit to address\nchallenges associated with entity titles (multi-word, long, implicit,\ncase-sensitive). Our experimental study shows approx 8% improvements over the\nbaseline approach, and significantly outperform an end to end approach for\nWikidata entity linking.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 21:11:56 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 13:16:16 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 16:03:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Mulang", "Isaiah Onando", ""], ["Singh", "Kuldeep", ""], ["Vyas", "Akhilesh", ""], ["Shekarpour", "Saeedeh", ""], ["Vidal", "Maria Esther", ""], ["Lehmann", "Jens", ""], ["Auer", "Soren", ""]]}, {"id": "1912.06262", "submitter": "Yue Zhao", "authors": "Yue Zhao and John Handley", "title": "Extracting clinical concepts from user queries", "comments": "8 pages, 4 figures. Added references. Corrected wording and typos,\n  results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical concept extraction often begins with clinical Named Entity\nRecognition (NER). Often trained on annotated clinical notes, clinical NER\nmodels tend to struggle with tagging clinical entities in user queries because\nof the structural differences between clinical notes and user queries. User\nqueries, unlike clinical notes, are often ungrammatical and incoherent. In many\ncases, user queries are compounded of multiple clinical entities, without comma\nor conjunction words separating them. By using as dataset a mixture of\nannotated clinical notes and synthesized user queries, we adapt a clinical NER\nmodel based on the BiLSTM-CRF architecture for tagging clinical entities in\nuser queries. Our contribution are the following: 1) We found that when trained\non a mixture of synthesized user queries and clinical notes, the NER model\nperforms better on both user queries and clinical notes. 2) We provide an\nend-to-end and easy-to-implement framework for clinical concept extraction from\nuser queries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:18:16 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 19:10:03 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhao", "Yue", ""], ["Handley", "John", ""]]}, {"id": "1912.06311", "submitter": "Hossein Zeinali", "authors": "Hossein Zeinali and Kong Aik Lee and Jahangir Alam and Lukas Burget", "title": "Short-duration Speaker Verification (SdSV) Challenge 2021: the Challenge\n  Evaluation Plan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the Short-duration Speaker Verification (SdSV)\nChallenge 2021. The main goal of the challenge is to evaluate new technologies\nfor text-dependent (TD) and text-independent (TI) speaker verification (SV) in\na short duration scenario. The proposed challenge evaluates SdSV with varying\ndegree of phonetic overlap between the enrollment and test utterances\n(cross-lingual). It is the first challenge with a broad focus on systematic\nbenchmark and analysis on varying degrees of phonetic variability on\nshort-duration speaker recognition. We expect that modern methods (deep neural\nnetworks in particular) will play a key role.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:38:14 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 18:03:11 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 21:07:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zeinali", "Hossein", ""], ["Lee", "Kong Aik", ""], ["Alam", "Jahangir", ""], ["Burget", "Lukas", ""]]}, {"id": "1912.06598", "submitter": "Radina Dobreva", "authors": "Radina Dobreva, Jie Zhou, Rachel Bawden", "title": "Document Sub-structure in Neural Machine Translation", "comments": "Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to machine translation (MT) either translate sentences in\nisolation, disregarding the context they appear in, or model context at the\nlevel of the full document, without a notion of any internal structure the\ndocument may have. In this work we consider the fact that documents are rarely\nhomogeneous blocks of text, but rather consist of parts covering different\ntopics. Some documents, such as biographies and encyclopedia entries, have\nhighly predictable, regular structures in which sections are characterised by\ndifferent topics. We draw inspiration from Louis and Webber (2014) who use this\ninformation to improve statistical MT and transfer their proposal into the\nframework of neural MT. We compare two different methods of including\ninformation about the topic of the section within which each sentence is found:\none using side constraints and the other using a cache-based model. We create\nand release the data on which we run our experiments - parallel corpora for\nthree language pairs (Chinese-English, French-English, Bulgarian-English) from\nWikipedia biographies, which we extract automatically, preserving the\nboundaries of sections within the articles.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 16:49:16 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 12:19:52 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Dobreva", "Radina", ""], ["Zhou", "Jie", ""], ["Bawden", "Rachel", ""]]}, {"id": "1912.06638", "submitter": "James Tian", "authors": "James Yi Tian, Alexander P. Kreuzer, Pai-Hung Chen, Hans-Martin Will", "title": "WaLDORf: Wasteless Language-model Distillation On Reading-comprehension", "comments": "Added Figure, minor edits for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and\nRoBERTa, have recently shown tremendous performance on a large variety of\nNatural Language Understanding (NLU) tasks. However, due to their size, these\nVLLMs are extremely resource intensive and cumbersome to deploy at production\ntime. Several recent publications have looked into various ways to distil\nknowledge from a transformer based VLLM (most commonly BERT-Base) into a\nsmaller model which can run much faster at inference time. Here, we propose a\nnovel set of techniques which together produce a task-specific hybrid\nconvolutional and transformer model, WaLDORf, that achieves state-of-the-art\ninference speed while still being more accurate than previous distilled models.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 18:15:37 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 19:08:58 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Tian", "James Yi", ""], ["Kreuzer", "Alexander P.", ""], ["Chen", "Pai-Hung", ""], ["Will", "Hans-Martin", ""]]}, {"id": "1912.06670", "submitter": "Josh Meyer", "authors": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael\n  Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, Gregor\n  Weber", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "comments": "Accepted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Common Voice corpus is a massively-multilingual collection of transcribed\nspeech intended for speech technology research and development. Common Voice is\ndesigned for Automatic Speech Recognition purposes but can be useful in other\ndomains (e.g. language identification). To achieve scale and sustainability,\nthe Common Voice project employs crowdsourcing for both data collection and\ndata validation. The most recent release includes 29 languages, and as of\nNovember 2019 there are a total of 38 languages collecting data. Over 50,000\nindividuals have participated so far, resulting in 2,500 hours of collected\naudio. To our knowledge this is the largest audio corpus in the public domain\nfor speech recognition, both in terms of number of hours and number of\nlanguages. As an example use case for Common Voice, we present speech\nrecognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By\napplying transfer learning from a source English model, we find an average\nCharacter Error Rate improvement of 5.99 +/- 5.48 for twelve target languages\n(German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton,\nTatar, Chuvash, and Kabyle). For most of these languages, these are the first\never published results on end-to-end Automatic Speech Recognition.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:22:44 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 20:37:08 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Ardila", "Rosana", ""], ["Branson", "Megan", ""], ["Davis", "Kelly", ""], ["Henretty", "Michael", ""], ["Kohler", "Michael", ""], ["Meyer", "Josh", ""], ["Morais", "Reuben", ""], ["Saunders", "Lindsay", ""], ["Tyers", "Francis M.", ""], ["Weber", "Gregor", ""]]}, {"id": "1912.06719", "submitter": "Susan Zhang", "authors": "Jonathan Raiman, Susan Zhang, Christy Dennison", "title": "Neural Network Surgery with Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost to train machine learning models has been increasing exponentially,\nmaking exploration and research into the correct features and architecture a\ncostly or intractable endeavor at scale. However, using a technique named\n\"surgery\" OpenAI Five was continuously trained to play the game DotA 2 over the\ncourse of 10 months through 20 major changes in features and architecture.\nSurgery transfers trained weights from one network to another after a selection\nprocess to determine which sections of the model are unchanged and which must\nbe re-initialized. In the past, the selection process relied on heuristics,\nmanual labor, or pre-existing boundaries in the structure of the model,\nlimiting the ability to salvage experiments after modifications of the feature\nset or input reorderings.\n  We propose a solution to automatically determine which components of a neural\nnetwork model should be salvaged and which require retraining. We achieve this\nby allowing the model to operate over discrete sets of features and use\nset-based operations to determine the exact relationship between inputs and\noutputs, and how they change across tweaks in model architecture. In this\npaper, we introduce the methodology for enabling neural networks to operate on\nsets, derive two methods for detecting feature-parameter interaction maps, and\nshow their equivalence. We empirically validate that we can surgery weights\nacross feature and architecture changes to the OpenAI Five model.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 21:41:39 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 17:16:00 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Raiman", "Jonathan", ""], ["Zhang", "Susan", ""], ["Dennison", "Christy", ""]]}, {"id": "1912.06721", "submitter": "Susan Zhang", "authors": "Jonathan Raiman, Susan Zhang, Filip Wolski", "title": "Long-Term Planning and Situational Awareness in OpenAI Five", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how knowledge about the world is represented within model-free\ndeep reinforcement learning methods is a major challenge given the black box\nnature of its learning process within high-dimensional observation and action\nspaces. AlphaStar and OpenAI Five have shown that agents can be trained without\nany explicit hierarchical macro-actions to reach superhuman skill in games that\nrequire taking thousands of actions before reaching the final goal. Assessing\nthe agent's plans and game understanding becomes challenging given the lack of\nhierarchy or explicit representations of macro-actions in these models, coupled\nwith the incomprehensible nature of the internal representations.\n  In this paper, we study the distributed representations learned by OpenAI\nFive to investigate how game knowledge is gradually obtained over the course of\ntraining. We also introduce a general technique for learning a model from the\nagent's hidden states to identify the formation of plans and subgoals. We show\nthat the agent can learn situational similarity across actions, and find\nevidence of planning towards accomplishing subgoals minutes before they are\nexecuted. We perform a qualitative analysis of these predictions during the\ngames against the DotA 2 world champions OG in April 2019.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 21:49:30 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Raiman", "Jonathan", ""], ["Zhang", "Susan", ""], ["Wolski", "Filip", ""]]}, {"id": "1912.06728", "submitter": "Sheena Panthaplackel", "authors": "Sheena Panthaplackel, Milos Gligoric, Raymond J. Mooney, Junyi Jessy\n  Li", "title": "Associating Natural Language Comment and Source Code Entities", "comments": "Accepted in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comments are an integral part of software development; they are natural\nlanguage descriptions associated with source code elements. Understanding\nexplicit associations can be useful in improving code comprehensibility and\nmaintaining the consistency between code and comments. As an initial step\ntowards this larger goal, we address the task of associating entities in\nJavadoc comments with elements in Java source code. We propose an approach for\nautomatically extracting supervised data using revision histories of open\nsource projects and present a manually annotated evaluation dataset for this\ntask. We develop a binary classifier and a sequence labeling model by crafting\na rich feature set which encompasses various aspects of code, comments, and the\nrelationships between them. Experiments show that our systems outperform\nseveral baselines learning from the proposed supervision.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 22:06:59 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Panthaplackel", "Sheena", ""], ["Gligoric", "Milos", ""], ["Mooney", "Raymond J.", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "1912.06745", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Katia Sycara", "title": "An Unsupervised Domain-Independent Framework for Automated Detection of\n  Persuasion Tactics in Text", "comments": "19 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing growth of social media, people have started relying\nheavily on the information shared therein to form opinions and make decisions.\nWhile such a reliance is motivation for a variety of parties to promote\ninformation, it also makes people vulnerable to exploitation by slander,\nmisinformation, terroristic and predatorial advances. In this work, we aim to\nunderstand and detect such attempts at persuasion. Existing works on detecting\npersuasion in text make use of lexical features for detecting persuasive\ntactics, without taking advantage of the possible structures inherent in the\ntactics used. We formulate the task as a multi-class classification problem and\npropose an unsupervised, domain-independent machine learning framework for\ndetecting the type of persuasion used in text, which exploits the inherent\nsentence structure present in the different persuasion tactics. Our work shows\npromising results as compared to existing work.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 23:32:38 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Sycara", "Katia", ""]]}, {"id": "1912.06806", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin\n  Stoyanov, Theresa Wilson", "title": "SemEval-2013 Task 2: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, microblog sentiment analysis, Twitter opinion\n  mining, SMS", "journal-ref": "SemEval-2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sentiment analysis in social media has attracted a lot of\nresearch interest and has been used for a number of applications.\nUnfortunately, research has been hindered by the lack of suitable datasets,\ncomplicating the comparison between approaches. To address this issue, we have\nproposed SemEval-2013 Task 2: Sentiment Analysis in Twitter, which included two\nsubtasks: A, an expression-level subtask, and B, a message-level subtask. We\nused crowdsourcing on Amazon Mechanical Turk to label a large Twitter training\ndataset along with additional test sets of Twitter and SMS messages for both\nsubtasks. All datasets used in the evaluation are released to the research\ncommunity. The task attracted significant interest and a total of 149\nsubmissions from 44 teams. The best-performing team achieved an F1 of 88.9% and\n69% for subtasks A and B, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:44:18 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Nakov", "Preslav", ""], ["Kozareva", "Zornitsa", ""], ["Ritter", "Alan", ""], ["Rosenthal", "Sara", ""], ["Stoyanov", "Veselin", ""], ["Wilson", "Theresa", ""]]}, {"id": "1912.06810", "submitter": "Preslav Nakov", "authors": "Alberto Barr\\'on-Cede\\~no, Giovanni Da San Martino, Israa Jaradat,\n  Preslav Nakov", "title": "Proppy: A System to Unmask Propaganda in Online News", "comments": "propaganda, disinformation, fake news", "journal-ref": "Thirty-Third AAAI Conference on Artificial Intelligence\n  (AAAI-2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present proppy, the first publicly available real-world, real-time\npropaganda detection system for online news, which aims at raising awareness,\nthus potentially limiting the impact of propaganda and helping fight\ndisinformation. The system constantly monitors a number of news sources,\ndeduplicates and clusters the news into events, and organizes the articles\nabout an event on the basis of the likelihood that they contain propagandistic\ncontent. The system is trained on known propaganda sources using a variety of\nstylistic features. The evaluation results on a standard dataset show\nstate-of-the-art results for propaganda detection.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:58:01 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Martino", "Giovanni Da San", ""], ["Jaradat", "Israa", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.06813", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Tomoki Hayashi, Yi-Chiao Wu, Hirokazu Kameoka, Tomoki\n  Toda", "title": "Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using\n  Transformer with Text-to-Speech Pretraining", "comments": "Preprint. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC)\nmodel based on the Transformer architecture with text-to-speech (TTS)\npretraining. Seq2seq VC models are attractive owing to their ability to convert\nprosody. While seq2seq models based on recurrent neural networks (RNNs) and\nconvolutional neural networks (CNNs) have been successfully applied to VC, the\nuse of the Transformer network, which has shown promising results in various\nspeech processing tasks, has not yet been investigated. Nonetheless, their\ndata-hungry property and the mispronunciation of converted speech make seq2seq\nmodels far from practical. To this end, we propose a simple yet effective\npretraining technique to transfer knowledge from learned TTS models, which\nbenefit from large-scale, easily accessible TTS corpora. VC models initialized\nwith such pretrained model parameters are able to generate effective hidden\nrepresentations for high-fidelity, highly intelligible converted speech.\nExperimental results show that such a pretraining scheme can facilitate\ndata-efficient training and outperform an RNN-based seq2seq VC model in terms\nof intelligibility, naturalness, and similarity.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 09:30:52 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Hayashi", "Tomoki", ""], ["Wu", "Yi-Chiao", ""], ["Kameoka", "Hirokazu", ""], ["Toda", "Tomoki", ""]]}, {"id": "1912.06825", "submitter": "Hongwei Zeng", "authors": "Qinghua Zheng, Jun Liu, Hongwei Zeng, Zhaotong Guo, Bei Wu, Bifan Wei", "title": "Knowledge forest: a novel model to organize knowledge fragments", "comments": "Accepted for publication in Science China Information Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of knowledge, it shows a steady trend of knowledge\nfragmentization. Knowledge fragmentization manifests as that the knowledge\nrelated to a specific topic in a course is scattered in isolated and autonomous\nknowledge sources. We term the knowledge of a facet in a specific topic as a\nknowledge fragment. The problem of knowledge fragmentization brings two\nchallenges: First, knowledge is scattered in various knowledge sources, which\nexerts users' considerable efforts to search for the knowledge of their\ninterested topics, thereby leading to information overload. Second, learning\ndependencies which refer to the precedence relationships between topics in the\nlearning process are concealed by the isolation and autonomy of knowledge\nsources, thus causing learning disorientation. To solve the knowledge\nfragmentization problem, we propose a novel knowledge organization model,\nknowledge forest, which consists of facet trees and learning dependencies.\nFacet trees can organize knowledge fragments with facet hyponymy to alleviate\ninformation overload. Learning dependencies can organize disordered topics to\ncope with learning disorientation. We conduct extensive experiments on three\nmanually constructed datasets from the Data Structure, Data Mining, and\nComputer Network courses, and the experimental results show that knowledge\nforest can effectively organize knowledge fragments, and alleviate information\noverload and learning disorientation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 11:02:17 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 20:41:24 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zheng", "Qinghua", ""], ["Liu", "Jun", ""], ["Zeng", "Hongwei", ""], ["Guo", "Zhaotong", ""], ["Wu", "Bei", ""], ["Wei", "Bifan", ""]]}, {"id": "1912.06858", "submitter": "Neslihan Suzen", "authors": "Neslihan Suzen, Evgeny M. Mirkes, Alexander N. Gorban", "title": "LScDC-new large scientific dictionary", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a scientific corpus of abstracts of academic papers\nin English -- Leicester Scientific Corpus (LSC). The LSC contains 1,673,824\nabstracts of research articles and proceeding papers indexed by Web of Science\n(WoS) in which publication year is 2014. Each abstract is assigned to at least\none of 252 subject categories. Paper metadata include these categories and the\nnumber of citations. We then develop scientific dictionaries named Leicester\nScientific Dictionary (LScD) and Leicester Scientific Dictionary-Core (LScDC),\nwhere words are extracted from the LSC. The LScD is a list of 974,238 unique\nwords (lemmas). The LScDC is a core list (sub-list) of the LScD with 104,223\nlemmas. It was created by removing LScD words appearing in not greater than 10\ntexts in the LSC. LScD and LScDC are available online. Both the corpus and\ndictionaries are developed to be later used for quantification of meaning in\nacademic texts.\n  Finally, the core list LScDC was analysed by comparing its words and word\nfrequencies with a classic academic word list 'New Academic Word List (NAWL)'\ncontaining 963 word families, which is also sampled from an academic corpus.\nThe major sources of the corpus where NAWL is extracted are Cambridge English\nCorpus (CEC), oral sources and textbooks. We investigate whether two\ndictionaries are similar in terms of common words and ranking of words. Our\ncomparison leads us to main conclusion: most of words of NAWL (99.6%) are\npresent in the LScDC but two lists differ in word ranking. This difference is\nmeasured.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 14:55:59 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Suzen", "Neslihan", ""], ["Mirkes", "Evgeny M.", ""], ["Gorban", "Alexander N.", ""]]}, {"id": "1912.06859", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko", "title": "Knowledge-based Conversational Search", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational interfaces that allow for intuitive and comprehensive access\nto digitally stored information remain an ambitious goal. In this thesis, we\nlay foundations for designing conversational search systems by analyzing the\nrequirements and proposing concrete solutions for automating some of the basic\ncomponents and tasks that such systems should support. We describe several\ninterdependent studies that were conducted to analyse the design requirements\nfor more advanced conversational search systems able to support complex\nhuman-like dialogue interactions and provide access to vast knowledge\nrepositories. In the first two research chapters, we focus on analyzing the\nstructures common to information-seeking dialogues by capturing recurrent\npatterns in terms of both domain-independent functional relations between\nutterances as well as domain-specific implicit semantic relations from shared\nbackground knowledge.\n  Our results show that question answering is one of the key components\nrequired for efficient information access but it is not the only type of\ndialogue interactions that a conversational search system should support. In\nthe third research chapter, we propose a novel approach for complex question\nanswering from a knowledge graph that surpasses the current state-of-the-art\nresults in terms of both efficacy and efficiency. In the last research chapter,\nwe turn our attention towards an alternative interaction mode, which we termed\nconversational browsing, in which, unlike question answering, the\nconversational system plays a more pro-active role in the course of a dialogue\ninteraction. We show that this approach helps users to discover relevant items\nthat are difficult to retrieve using only question answering due to the\nvocabulary mismatch problem.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 14:59:38 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Vakulenko", "Svitlana", ""]]}, {"id": "1912.06872", "submitter": "Antonios Anastasopoulos", "authors": "Keita Kurita, Anna Belova and Antonios Anastasopoulos", "title": "Towards Robust Toxic Content Classification", "comments": "to appear at EDSMLS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxic content detection aims to identify content that can offend or harm its\nrecipients. Automated classifiers of toxic content need to be robust against\nadversaries who deliberately try to bypass filters. We propose a method of\ngenerating realistic model-agnostic attacks using a lexicon of toxic tokens,\nwhich attempts to mislead toxicity classifiers by diluting the toxicity signal\neither by obfuscating toxic tokens through character-level perturbations, or by\ninjecting non-toxic distractor tokens. We show that these realistic attacks\nreduce the detection recall of state-of-the-art neural toxicity detectors,\nincluding those using ELMo and BERT, by more than 50% in some cases. We explore\ntwo approaches for defending against such attacks. First, we examine the effect\nof training on synthetically noised data. Second, we propose the Contextual\nDenoising Autoencoder (CDAE): a method for learning robust representations that\nuses character-level and contextual information to denoise perturbed tokens. We\nshow that the two approaches are complementary, improving robustness to both\ncharacter-level perturbations and distractors, recovering a considerable\nportion of the lost accuracy. Finally, we analyze the robustness\ncharacteristics of the most competitive methods and outline practical\nconsiderations for improving toxicity detectors.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:03:15 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kurita", "Keita", ""], ["Belova", "Anna", ""], ["Anastasopoulos", "Antonios", ""]]}, {"id": "1912.06889", "submitter": "Harshavardhan Kamarthi", "authors": "Aakash Srinivasan, Harshavardhan Kamarthi, Devi Ganesan and Sutanu\n  Chakraborti", "title": "Integrating Lexical Knowledge in Word Embeddings using Sprinkling and\n  Retrofitting", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based word embeddings, such as Word2Vec and GloVe, are purely\ndata driven in that they capture the distributional information about words\nfrom the training corpus. Past works have attempted to improve these embeddings\nby incorporating semantic knowledge from lexical resources like WordNet. Some\ntechniques like retrofitting modify word embeddings in the post-processing\nstage while some others use a joint learning approach by modifying the\nobjective function of neural networks. In this paper, we discuss two novel\napproaches for incorporating semantic knowledge into word embeddings. In the\nfirst approach, we take advantage of Levy et al's work which showed that using\nSVD based methods on co-occurrence matrix provide similar performance to neural\nnetwork based embeddings. We propose a 'sprinkling' technique to add semantic\nrelations to the co-occurrence matrix directly before factorization. In the\nsecond approach, WordNet similarity scores are used to improve the retrofitting\nmethod. We evaluate the proposed methods in both intrinsic and extrinsic tasks\nand observe significant improvements over the baselines in many of the\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 17:38:46 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 17:19:46 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Srinivasan", "Aakash", ""], ["Kamarthi", "Harshavardhan", ""], ["Ganesan", "Devi", ""], ["Chakraborti", "Sutanu", ""]]}, {"id": "1912.06900", "submitter": "Sawsan Alqahtani", "authors": "Sawsan Alqahtani, Ajay Mishra, and Mona Diab", "title": "Efficient Convolutional Neural Networks for Diacritic Restoration", "comments": "accepted in EMNLP 2019", "journal-ref": "EMNLP 2019", "doi": "10.18653/v1/D19-1151", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diacritic restoration has gained importance with the growing need for\nmachines to understand written texts. The task is typically modeled as a\nsequence labeling problem and currently Bidirectional Long Short Term Memory\n(BiLSTM) models provide state-of-the-art results. Recently, Bai et al. (2018)\nshow the advantages of Temporal Convolutional Neural Networks (TCN) over\nRecurrent Neural Networks (RNN) for sequence modeling in terms of performance\nand computational resources. As diacritic restoration benefits from both\nprevious as well as subsequent timesteps, we further apply and evaluate a\nvariant of TCN, Acausal TCN (A-TCN), which incorporates context from both\ndirections (previous and future) rather than strictly incorporating previous\ncontext as in the case of TCN. A-TCN yields significant improvement over TCN\nfor diacritization in three different languages: Arabic, Yoruba, and\nVietnamese. Furthermore, A-TCN and BiLSTM have comparable performance, making\nA-TCN an efficient alternative over BiLSTM since convolutions can be trained in\nparallel. A-TCN is significantly faster than BiLSTM at inference time\n(270%-334% improvement in the amount of text diacritized per minute).\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 18:28:02 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Alqahtani", "Sawsan", ""], ["Mishra", "Ajay", ""], ["Diab", "Mona", ""]]}, {"id": "1912.06905", "submitter": "Mirko Bernardoni", "authors": "Lulu Wan, George Papageorgiou, Michael Seddon, Mirko Bernardoni", "title": "Long-length Legal Document Classification", "comments": "8 pages, 5 figures, 4 equations, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the principal tasks of machine learning with major applications is\ntext classification. This paper focuses on the legal domain and, in particular,\non the classification of lengthy legal documents. The main challenge that this\nstudy addresses is the limitation that current models impose on the length of\nthe input text. In addition, the present paper shows that dividing the text\ninto segments and later combining the resulting embeddings with a BiLSTM\narchitecture to form a single document embedding can improve results. These\nadvancements are achieved by utilising a simpler structure, rather than an\nincreasingly complex one, which is often the case in NLP research. The dataset\nused in this paper is obtained from an online public database containing\nlengthy legal documents with highly domain-specific vocabulary and thus, the\ncomparison of our results to the ones produced by models implemented on the\ncommonly used datasets would be unjustified. This work provides the foundation\nfor future work in document classification in the legal field.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 19:13:44 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Wan", "Lulu", ""], ["Papageorgiou", "George", ""], ["Seddon", "Michael", ""], ["Bernardoni", "Mirko", ""]]}, {"id": "1912.06927", "submitter": "Debanjan Mahata", "authors": "Akash Gautam, Puneet Mathur, Rakesh Gosangi, Debanjan Mahata, Ramit\n  Sawhney, Rajiv Ratn Shah", "title": "#MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo\n  Movement", "comments": "Preprint of paper accepted at ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a dataset containing 9,973 tweets related to the\nMeToo movement that were manually annotated for five different linguistic\naspects: relevance, stance, hate speech, sarcasm, and dialogue acts. We present\na detailed account of the data collection and annotation processes. The\nannotations have a very high inter-annotator agreement (0.79 to 0.93 k-alpha)\ndue to the domain expertise of the annotators and clear annotation\ninstructions. We analyze the data in terms of geographical distribution, label\ncorrelations, and keywords. Lastly, we present some potential use cases of this\ndataset. We expect this dataset would be of great interest to psycholinguists,\nsocio-linguists, and computational linguists to study the discursive space of\ndigitally mobilized social movements on sensitive issues like sexual\nharassment.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 20:57:29 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 11:19:31 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Gautam", "Akash", ""], ["Mathur", "Puneet", ""], ["Gosangi", "Rakesh", ""], ["Mahata", "Debanjan", ""], ["Sawhney", "Ramit", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1912.06979", "submitter": "Jon Gillick", "authors": "Jon Gillick, David Bamman", "title": "Breaking Speech Recognizers to Imagine Lyrics", "comments": "3 pages", "journal-ref": "NeurIPS 2019 Workshop on Machine Learning for Creativity and\n  Design", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for generating text, and in particular song lyrics,\nbased on the speech-like acoustic qualities of a given audio file. We repurpose\na vocal source separation algorithm and an acoustic model trained to recognize\nisolated speech, instead inputting instrumental music or environmental sounds.\nFeeding the \"mistakes\" of the vocal separator into the recognizer, we obtain a\ntranscription of words \\emph{imagined} to be spoken in the input audio. We\ndescribe the key components of our approach, present initial analysis, and\ndiscuss the potential of the method for machine-in-the-loop collaboration in\ncreative applications.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:34:45 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Gillick", "Jon", ""], ["Bamman", "David", ""]]}, {"id": "1912.07025", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Abhishek Prusty, Sowmya Aitha, Abhishek Trivedi, Ravi Kiran\n  Sarvadevabhatla", "title": "Indiscapes: Instance Segmentation Networks for Layout Parsing of\n  Historical Indic Manuscripts", "comments": "Oral presentation at International Conference on Document Analysis\n  and Recognition (ICDAR) - 2019. For dataset, pre-trained networks and\n  additional details, visit project page at http://ihdia.iiit.ac.in/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical palm-leaf manuscript and early paper documents from Indian\nsubcontinent form an important part of the world's literary and cultural\nheritage. Despite their importance, large-scale annotated Indic manuscript\nimage datasets do not exist. To address this deficiency, we introduce\nIndiscapes, the first ever dataset with multi-regional layout annotations for\nhistorical Indic manuscripts. To address the challenge of large diversity in\nscripts and presence of dense, irregular layout elements (e.g. text lines,\npictures, multiple documents per image), we adapt a Fully Convolutional Deep\nNeural Network architecture for fully automatic, instance-level spatial layout\nparsing of manuscript images. We demonstrate the effectiveness of proposed\narchitecture on images from the Indiscapes dataset. For annotation flexibility\nand keeping the non-technical nature of domain experts in mind, we also\ncontribute a custom, web-based GUI annotation tool and a dashboard-style\nanalytics portal. Overall, our contributions set the stage for enabling\ndownstream applications such as OCR and word-spotting in historical Indic\nmanuscripts at scale.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 11:42:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Prusty", "Abhishek", ""], ["Aitha", "Sowmya", ""], ["Trivedi", "Abhishek", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "1912.07050", "submitter": "Dafydd Gibbon", "authors": "Dafydd Gibbon", "title": "Computational Induction of Prosodic Structure", "comments": "29 pages, 10 figures, code appendix, to appear in \"Studies in\n  Prosodic Grammar\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study has two goals relating to the grammar of prosody,\nunderstood as the rhythms and melodies of speech. First, an overview is\nprovided of the computable grammatical and phonetic approaches to prosody\nanalysis which use hypothetico-deductive methods and are based on learned\nhermeneutic intuitions about language. Second, a proposal is presented for an\ninductive grounding in the physical signal, in which prosodic structure is\ninferred using a language-independent method from the low-frequency spectrum of\nthe speech signal. The overview includes a discussion of computational aspects\nof standard generative and post-generative models, and suggestions for\nreformulating these to form inductive approaches. Also included is a discussion\nof linguistic phonetic approaches to analysis of annotations (pairs of speech\nunit labels with time-stamps) of recorded spoken utterances. The proposal\nintroduces the inductive approach of Rhythm Formant Theory (RFT) and the\nassociated Rhythm Formant Analysis (RFA) method are introduced, with the aim of\ncompleting a gap in the linguistic hypothetico-deductive cycle by grounding in\na language-independent inductive procedure of speech signal analysis. The\nvalidity of the method is demonstrated and applied to rhythm patterns in\nread-aloud Mandarin Chinese, finding differences from English which are related\nto lexical and grammatical differences between the languages, as well as\nindividual variation. The overall conclusions are (1) that normative\nlanguage-to-language phonological or phonetic comparisons of rhythm, for\nexample of Mandarin and English, are too simplistic, in view of diverse\nlanguage-internal factors due to genre and style differences as well as\nutterance dynamics, and (2) that language-independent empirical grounding of\nrhythm in the physical signal is called for.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 14:38:58 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Gibbon", "Dafydd", ""]]}, {"id": "1912.07076", "submitter": "Sampo Pyysalo", "authors": "Antti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani\n  Luotolahti, Tapio Salakoski, Filip Ginter, Sampo Pyysalo", "title": "Multilingual is not enough: BERT for Finnish", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based language models pretrained on large unannotated text\ncorpora have been demonstrated to allow efficient transfer learning for natural\nlanguage processing, with recent approaches such as the transformer-based BERT\nmodel advancing the state of the art across a variety of tasks. While most work\non these models has focused on high-resource languages, in particular English,\na number of recent efforts have introduced multilingual models that can be\nfine-tuned to address tasks in a large number of different languages. However,\nwe still lack a thorough understanding of the capabilities of these models, in\nparticular for lower-resourced languages. In this paper, we focus on Finnish\nand thoroughly evaluate the multilingual BERT model on a range of tasks,\ncomparing it with a new Finnish BERT model trained from scratch. The new\nlanguage-specific model is shown to systematically and clearly outperform the\nmultilingual. While the multilingual model largely fails to reach the\nperformance of previously proposed methods, the custom Finnish BERT model\nestablishes new state-of-the-art results on all corpora for all reference\ntasks: part-of-speech tagging, named entity recognition, and dependency\nparsing. We release the model and all related resources created for this study\nwith open licenses at https://turkunlp.org/finbert .\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 17:50:56 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Virtanen", "Antti", ""], ["Kanerva", "Jenna", ""], ["Ilo", "Rami", ""], ["Luoma", "Jouni", ""], ["Luotolahti", "Juhani", ""], ["Salakoski", "Tapio", ""], ["Ginter", "Filip", ""], ["Pyysalo", "Sampo", ""]]}, {"id": "1912.07095", "submitter": "Stephen Mayhew", "authors": "Stephen Mayhew, Nitish Gupta, Dan Roth", "title": "Robust Named Entity Recognition with Truecasing Pretraining", "comments": "Accepted to AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although modern named entity recognition (NER) systems show impressive\nperformance on standard datasets, they perform poorly when presented with noisy\ndata. In particular, capitalization is a strong signal for entities in many\nlanguages, and even state of the art models overfit to this feature, with\ndrastically lower performance on uncapitalized text. In this work, we address\nthe problem of robustness of NER systems in data with noisy or uncertain\ncasing, using a pretraining objective that predicts casing in text, or a\ntruecaser, leveraging unlabeled data. The pretrained truecaser is combined with\na standard BiLSTM-CRF model for NER by appending output distributions to\ncharacter embeddings. In experiments over several datasets of varying domain\nand casing quality, we show that our new model improves performance in uncased\ntext, even adding value to uncased BERT embeddings. Our method achieves a new\nstate of the art on the WNUT17 shared task dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 19:33:25 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Mayhew", "Stephen", ""], ["Gupta", "Nitish", ""], ["Roth", "Dan", ""]]}, {"id": "1912.07199", "submitter": "Robert Hawkins", "authors": "Robert D. Hawkins, Michael C. Frank, Noah D. Goodman", "title": "Characterizing the dynamics of learning in repeated reference games", "comments": "Accepted at Cognitive Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The language we use over the course of conversation changes as we establish\ncommon ground and learn what our partner finds meaningful. Here we draw upon\nrecent advances in natural language processing to provide a finer-grained\ncharacterization of the dynamics of this learning process. We release an open\ncorpus (>15,000 utterances) of extended dyadic interactions in a classic\nrepeated reference game task where pairs of participants had to coordinate on\nhow to refer to initially difficult-to-describe tangram stimuli. We find that\ndifferent pairs discover a wide variety of idiosyncratic but efficient and\nstable solutions to the problem of reference. Furthermore, these conventions\nare shaped by the communicative context: words that are more discriminative in\nthe initial context (i.e. that are used for one target more than others) are\nmore likely to persist through the final repetition. Finally, we find\nsystematic structure in how a speaker's referring expressions become more\nefficient over time: syntactic units drop out in clusters following positive\nfeedback from the listener, eventually leaving short labels containing\nopen-class parts of speech. These findings provide a higher resolution look at\nthe quantitative dynamics of ad hoc convention formation and support further\ndevelopment of computational models of learning in communication.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 05:25:07 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 00:14:28 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Hawkins", "Robert D.", ""], ["Frank", "Michael C.", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1912.07225", "submitter": "Yongjing Yin", "authors": "Yongjing Yin and Linfeng Song and Jinsong Su and Jiali Zeng and Chulun\n  Zhou and Jiebo Luo", "title": "Graph-based Neural Sentence Ordering", "comments": "IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence ordering is to restore the original paragraph from a set of\nsentences. It involves capturing global dependencies among sentences regardless\nof their input order. In this paper, we propose a novel and flexible\ngraph-based neural sentence ordering model, which adopts graph recurrent\nnetwork \\cite{Zhang:acl18} to accurately learn semantic representations of the\nsentences. Instead of assuming connections between all pairs of input\nsentences, we use entities that are shared among multiple sentences to make\nmore expressive graph representations with less noise. Experimental results\nshow that our proposed model outperforms the existing state-of-the-art systems\non several benchmark datasets, demonstrating the effectiveness of our model. We\nalso conduct a thorough analysis on how entities help the performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 07:23:21 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yin", "Yongjing", ""], ["Song", "Linfeng", ""], ["Su", "Jinsong", ""], ["Zeng", "Jiali", ""], ["Zhou", "Chulun", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.07239", "submitter": "Yongjing Yin", "authors": "Jiali Zeng, Yang Liu, Jinsong Su, Yubin Ge, Yaojie Lu, Yongjing Yin,\n  Jiebo Luo", "title": "Iterative Dual Domain Adaptation for Neural Machine Translation", "comments": "EMNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies on the domain adaptation for neural machine translation\n(NMT) mainly focus on the one-pass transferring out-of-domain translation\nknowledge to in-domain NMT model. In this paper, we argue that such a strategy\nfails to fully extract the domain-shared translation knowledge, and repeatedly\nutilizing corpora of different domains can lead to better distillation of\ndomain-shared translation knowledge. To this end, we propose an iterative dual\ndomain adaptation framework for NMT. Specifically, we first pre-train in-domain\nand out-of-domain NMT models using their own training corpora respectively, and\nthen iteratively perform bidirectional translation knowledge transfer (from\nin-domain to out-of-domain and then vice versa) based on knowledge distillation\nuntil the in-domain NMT model convergences. Furthermore, we extend the proposed\nframework to the scenario of multiple out-of-domain training corpora, where the\nabove-mentioned transfer is performed sequentially between the in-domain and\neach out-of-domain NMT models in the ascending order of their domain\nsimilarities. Empirical results on Chinese-English and English-German\ntranslation tasks demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:21:28 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zeng", "Jiali", ""], ["Liu", "Yang", ""], ["Su", "Jinsong", ""], ["Ge", "Yubin", ""], ["Lu", "Yaojie", ""], ["Yin", "Yongjing", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.07240", "submitter": "Yuchen Liu", "authors": "Yuchen Liu, Jiajun Zhang, Hao Xiong, Long Zhou, Zhongjun He, Hua Wu,\n  Haifeng Wang, and Chengqing Zong", "title": "Synchronous Speech Recognition and Speech-to-Text Translation with\n  Interactive Decoding", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-to-text translation (ST), which translates source language speech into\ntarget language text, has attracted intensive attention in recent years.\nCompared to the traditional pipeline system, the end-to-end ST model has\npotential benefits of lower latency, smaller model size, and less error\npropagation. However, it is notoriously difficult to implement such a model\nwithout transcriptions as intermediate. Existing works generally apply\nmulti-task learning to improve translation quality by jointly training\nend-to-end ST along with automatic speech recognition (ASR). However, different\ntasks in this method cannot utilize information from each other, which limits\nthe improvement. Other works propose a two-stage model where the second model\ncan use the hidden state from the first one, but its cascade manner greatly\naffects the efficiency of training and inference process. In this paper, we\npropose a novel interactive attention mechanism which enables ASR and ST to\nperform synchronously and interactively in a single model. Specifically, the\ngeneration of transcriptions and translations not only relies on its previous\noutputs but also the outputs predicted in the other task. Experiments on TED\nspeech translation corpora have shown that our proposed model can outperform\nstrong baselines on the quality of speech translation and achieve better speech\nrecognition performances as well.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:22:43 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Yuchen", ""], ["Zhang", "Jiajun", ""], ["Xiong", "Hao", ""], ["Zhou", "Long", ""], ["He", "Zhongjun", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""], ["Zong", "Chengqing", ""]]}, {"id": "1912.07419", "submitter": "Elaheh Momeni", "authors": "Patrick Kiss and Elaheh Momeni", "title": "Optimized Tracking of Topic Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic evolution modeling has been researched for a long time and has gained\nconsiderable interest. A state-of-the-art method has been recently using word\nmodeling algorithms in combination with community detection mechanisms to\nachieve better results in a more effective way. We analyse results of this\napproach and discuss the two major challenges that this approach still faces.\nAlthough the topics that have resulted from the recent algorithm are good in\ngeneral, they are very noisy due to many topics that are very unimportant\nbecause of their size, words, or ambiguity. Additionally, the number of words\ndefining each topic is too large, making it difficult to analyse them in their\nunsorted state. In this paper, we propose approaches to tackle these challenges\nby adding topic filtering and network analysis metrics to define the importance\nof a topic. We test different combinations of these metrics to see which\ncombination yields the best results. Furthermore, we add word filtering and\nranking to each topic to identify the words with the highest novelty\nautomatically. We evaluate our enhancement methods in two ways: human\nqualitative evaluation and automatic quantitative evaluation. Moreover, we\ncreated two case studies to test the quality of the clusters and words. In the\nquantitative evaluation, we use the pairwise mutual information score to test\nthe coherency of topics. The quantitative evaluation also includes an analysis\nof execution times for each part of the program. The results of the\nexperimental evaluations show that the two evaluation methods agree on the\npositive feasibility of the algorithm. We then show possible extensions in the\nform of usability and future improvements to the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:43:11 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kiss", "Patrick", ""], ["Momeni", "Elaheh", ""]]}, {"id": "1912.07421", "submitter": "Frejus Laleye", "authors": "Fr\\'ejus A. A. Laleye, Antonia Blani\\'e, Antoine Brouquet, Dan\n  Behnamou, Ga\\\"el de Chalendar", "title": "Semantic Similarity To Improve Question Understanding in a Virtual\n  Patient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medicine, a communicating virtual patient or doctor allows students to\ntrain in medical diagnosis and develop skills to conduct a medical\nconsultation. In this paper, we describe a conversational virtual standardized\npatient system to allow medical students to simulate a diagnosis strategy of an\nabdominal surgical emergency. We exploited the semantic properties captured by\ndistributed word representations to search for similar questions in the virtual\npatient dialogue system. We created two dialogue systems that were evaluated on\ndatasets collected during tests with students. The first system based on\nhand-crafted rules obtains $92.29\\%$ as $F1$-score on the studied clinical case\nwhile the second system that combines rules and semantic similarity achieves\n$94.88\\%$. It represents an error reduction of $9.70\\%$ as compared to the\nrules-only-based system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:45:56 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Laleye", "Fr\u00e9jus A. A.", ""], ["Blani\u00e9", "Antonia", ""], ["Brouquet", "Antoine", ""], ["Behnamou", "Dan", ""], ["de Chalendar", "Ga\u00ebl", ""]]}, {"id": "1912.07478", "submitter": "Dawei Zhu", "authors": "Dawei Zhu, Aditya Mogadala, Dietrich Klakow", "title": "Image Manipulation with Natural Language using Two-sidedAttentive\n  Conditional Generative Adversarial Network", "comments": "Submitted to Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Altering the content of an image with photo editing tools is a tedious task\nfor an inexperienced user. Especially, when modifying the visual attributes of\na specific object in an image without affecting other constituents such as\nbackground etc. To simplify the process of image manipulation and to provide\nmore control to users, it is better to utilize a simpler interface like natural\nlanguage. Therefore, in this paper, we address the challenge of manipulating\nimages using natural language description. We propose the Two-sidEd Attentive\nconditional Generative Adversarial Network (TEA-cGAN) to generate semantically\nmanipulated images while preserving other contents such as background intact.\nTEA-cGAN uses fine-grained attention both in the generator and discriminator of\nGenerative Adversarial Network (GAN) based framework at different scales.\nExperimental results show that TEA-cGAN which generates 128x128 and 256x256\nresolution images outperforms existing methods on CUB and Oxford-102 datasets\nboth quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:21:13 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhu", "Dawei", ""], ["Mogadala", "Aditya", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1912.07491", "submitter": "Jian Wang", "authors": "Jian Wang, Junhao Liu, Wei Bi, Xiaojiang Liu, Kejing He, Ruifeng Xu,\n  Min Yang", "title": "Improving Knowledge-aware Dialogue Generation via Knowledge Base\n  Question Answering", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models usually suffer from the challenge of incorporating\ncommonsense knowledge into the open-domain dialogue systems. In this paper, we\npropose a novel knowledge-aware dialogue generation model (called TransDG),\nwhich transfers question representation and knowledge matching abilities from\nknowledge base question answering (KBQA) task to facilitate the utterance\nunderstanding and factual knowledge selection for dialogue generation. In\naddition, we propose a response guiding attention and a multi-step decoding\nstrategy to steer our model to focus on relevant features for response\ngeneration. Experiments on two benchmark datasets demonstrate that our model\nhas robust superiority over compared methods in generating informative and\nfluent dialogues. Our code is available at https://github.com/siat-nlp/TransDG.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:39:01 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Wang", "Jian", ""], ["Liu", "Junhao", ""], ["Bi", "Wei", ""], ["Liu", "Xiaojiang", ""], ["He", "Kejing", ""], ["Xu", "Ruifeng", ""], ["Yang", "Min", ""]]}, {"id": "1912.07506", "submitter": "Aakash Sarkar", "authors": "Aakash Sarkar, Marc Howard", "title": "Scale-dependent Relationships in Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language exhibits statistical dependencies at a wide range of scales.\nFor instance, the mutual information between words in natural language decays\nlike a power law with the temporal lag between them. However, many statistical\nlearning models applied to language impose a sampling scale while extracting\nstatistical structure. For instance, Word2Vec constructs a vector embedding\nthat maximizes the prediction between a target word and the context words that\nappear nearby in the corpus. The size of the context is chosen by the user and\ndefines a strong scale; relationships over much larger temporal scales would be\ninvisible to the algorithm. This paper examines the family of Word2Vec\nembeddings generated while systematically manipulating the sampling scale used\nto define the context around each word. The primary result is that different\nlinguistic relationships are preferentially encoded at different scales.\nDifferent scales emphasize different syntactic and semantic relations between\nwords.Moreover, the neighborhoods of a given word in the embeddings change\nsignificantly depending on the scale. These results suggest that any individual\nscale can only identify a subset of the meaningful relationships a word might\nhave, and point toward the importance of developing scale-free models of\nsemantic meaning.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:12:00 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sarkar", "Aakash", ""], ["Howard", "Marc", ""]]}, {"id": "1912.07538", "submitter": "Rakshith Shetty", "authors": "Vedika Agarwal and Rakshith Shetty and Mario Fritz", "title": "Towards Causal VQA: Revealing and Reducing Spurious Correlations by\n  Invariant and Covariant Semantic Editing", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant success in Visual Question Answering (VQA), VQA models\nhave been shown to be notoriously brittle to linguistic variations in the\nquestions. Due to deficiencies in models and datasets, today's models often\nrely on correlations rather than predictions that are causal w.r.t. data. In\nthis paper, we propose a novel way to analyze and measure the robustness of the\nstate of the art models w.r.t semantic visual variations as well as propose\nways to make models more robust against spurious correlations. Our method\nperforms automated semantic image manipulations and tests for consistency in\nmodel predictions to quantify the model robustness as well as generate\nsynthetic data to counter these problems. We perform our analysis on three\ndiverse, state of the art VQA models and diverse question types with a\nparticular focus on challenging counting questions. In addition, we show that\nmodels can be made significantly more robust against inconsistent predictions\nusing our edited data. Finally, we show that results also translate to\nreal-world error cases of state of the art models, which results in improved\noverall performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:45:01 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 21:22:34 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 08:58:11 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Agarwal", "Vedika", ""], ["Shetty", "Rakshith", ""], ["Fritz", "Mario", ""]]}, {"id": "1912.07575", "submitter": "Th\\'eodore Bluche", "authors": "Theodore Bluche and Thibault Gisselbrecht", "title": "Predicting detection filters for small footprint open-vocabulary keyword\n  spotting", "comments": "Submtted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully-neural approach to open-vocabulary keyword\nspotting, that allows the users to include a customizable voice interface to\ntheir device and that does not require task-specific data. We present a keyword\ndetection neural network weighing less than 250KB, in which the topmost layer\nperforming keyword detection is predicted by an auxiliary network, that may be\nrun offline to generate a detector for any keyword. We show that the proposed\nmodel outperforms acoustic keyword spotting baselines by a large margin on two\ntasks of detecting keywords in utterances and three tasks of detecting isolated\nspeech commands. We also propose a method to fine-tune the model when specific\ntraining data is available for some keywords, which yields a performance\nsimilar to a standard speech command neural network while keeping the ability\nof the model to be applied to new keywords.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:41:49 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 09:09:15 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Bluche", "Theodore", ""], ["Gisselbrecht", "Thibault", ""]]}, {"id": "1912.07747", "submitter": "William Hsu", "authors": "Huichen Yang, Carlos A. Aguirre, Maria F. De La Torre, Derek\n  Christensen, Luis Bobadilla, Emily Davich, Jordan Roth, Lei Luo, Yihong\n  Theis, Alice Lam, T. Yong-Jin Han, David Buttler, William H. Hsu", "title": "Pipelines for Procedural Information Extraction from Scientific\n  Literature: Towards Recipes using Machine Learning and Data Science", "comments": "15th International Conference on Document Analysis and Recognition\n  Workshops (ICDARW 2019)", "journal-ref": null, "doi": "10.1109/ICDARW.2019.10037", "report-no": "2019-1", "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a machine learning and data science pipeline for\nstructured information extraction from documents, implemented as a suite of\nopen-source tools and extensions to existing tools. It centers around a\nmethodology for extracting procedural information in the form of recipes,\nstepwise procedures for creating an artifact (in this case synthesizing a\nnanomaterial), from published scientific literature. From our overall goal of\nproducing recipes from free text, we derive the technical objectives of a\nsystem consisting of pipeline stages: document acquisition and filtering,\npayload extraction, recipe step extraction as a relationship extraction task,\nrecipe assembly, and presentation through an information retrieval interface\nwith question answering (QA) functionality. This system meets computational\ninformation and knowledge management (CIKM) requirements of metadata-driven\npayload extraction, named entity extraction, and relationship extraction from\ntext. Functional contributions described in this paper include semi-supervised\nmachine learning methods for PDF filtering and payload extraction tasks,\nfollowed by structured extraction and data transformation tasks beginning with\nsection extraction, recipe steps as information tuples, and finally assembled\nrecipes. Measurable objective criteria for extraction quality include precision\nand recall of recipe steps, ordering constraints, and QA accuracy, precision,\nand recall. Results, key novel contributions, and significant open problems\nderived from this work center around the attribution of these holistic quality\nmeasures to specific machine learning and inference stages of the pipeline,\neach with their performance measures. The desired recipes contain identified\npreconditions, material inputs, and operations, and constitute the overall\noutput generated by our computational information and knowledge management\n(CIKM) system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:04:03 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Yang", "Huichen", ""], ["Aguirre", "Carlos A.", ""], ["De La Torre", "Maria F.", ""], ["Christensen", "Derek", ""], ["Bobadilla", "Luis", ""], ["Davich", "Emily", ""], ["Roth", "Jordan", ""], ["Luo", "Lei", ""], ["Theis", "Yihong", ""], ["Lam", "Alice", ""], ["Han", "T. Yong-Jin", ""], ["Buttler", "David", ""], ["Hsu", "William H.", ""]]}, {"id": "1912.07840", "submitter": "Zihan Wang", "authors": "Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth", "title": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has exhibited the surprising cross-lingual abilities of\nmultilingual BERT (M-BERT) -- surprising since it is trained without any\ncross-lingual objective and with no aligned data. In this work, we provide a\ncomprehensive study of the contribution of different components in M-BERT to\nits cross-lingual ability. We study the impact of linguistic properties of the\nlanguages, the architecture of the model, and the learning objectives. The\nexperimental study is done in the context of three typologically different\nlanguages -- Spanish, Hindi, and Russian -- and using two conceptually\ndifferent NLP tasks, textual entailment and named entity recognition. Among our\nkey conclusions is the fact that the lexical overlap between languages plays a\nnegligible role in the cross-lingual success, while the depth of the network is\nan integral part of it. All our models and implementations can be found on our\nproject page: http://cogcomp.org/page/publication_view/900 .\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 06:53:05 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 18:48:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["K", "Karthikeyan", ""], ["Wang", "Zihan", ""], ["Mayhew", "Stephen", ""], ["Roth", "Dan", ""]]}, {"id": "1912.07875", "submitter": "Jacob Kahn", "authors": "Jacob Kahn, Morgane Rivi\\`ere, Weiyi Zheng, Evgeny Kharitonov,\n  Qiantong Xu, Pierre-Emmanuel Mazar\\'e, Julien Karadayi, Vitaliy Liptchinsky,\n  Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve,\n  Armand Joulin, Abdelrahman Mohamed, Emmanuel Dupoux", "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9052942", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new collection of spoken English audio suitable for training\nspeech recognition systems under limited or no supervision. It is derived from\nopen-source audio books from the LibriVox project. It contains over 60K hours\nof audio, which is, to our knowledge, the largest freely-available corpus of\nspeech. The audio has been segmented using voice activity detection and is\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\nbaseline systems and evaluation metrics working under three settings: (1) the\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\nstandard LibriSpeech dev and test sets for comparison with the supervised\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:47:30 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kahn", "Jacob", ""], ["Rivi\u00e8re", "Morgane", ""], ["Zheng", "Weiyi", ""], ["Kharitonov", "Evgeny", ""], ["Xu", "Qiantong", ""], ["Mazar\u00e9", "Pierre-Emmanuel", ""], ["Karadayi", "Julien", ""], ["Liptchinsky", "Vitaliy", ""], ["Collobert", "Ronan", ""], ["Fuegen", "Christian", ""], ["Likhomanenko", "Tatiana", ""], ["Synnaeve", "Gabriel", ""], ["Joulin", "Armand", ""], ["Mohamed", "Abdelrahman", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1912.07911", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Jing Chen, Haonan Sun, Keyang Xu", "title": "A Heterogeneous Graphical Model to Understand User-Level Sentiments in\n  Social Media", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Media has seen a tremendous growth in the last decade and is\ncontinuing to grow at a rapid pace. With such adoption, it is increasingly\nbecoming a rich source of data for opinion mining and sentiment analysis. The\ndetection and analysis of sentiment in social media is thus a valuable topic\nand attracts a lot of research efforts. Most of the earlier efforts focus on\nsupervised learning approaches to solve this problem, which require expensive\nhuman annotations and therefore limits their practical use. In our work, we\npropose a semi-supervised approach to predict user-level sentiments for\nspecific topics. We define and utilize a heterogeneous graph built from the\nsocial networks of the users with the knowledge that connected users in social\nnetworks typically share similar sentiments. Compared with the previous works,\nwe have several novelties: (1) we incorporate the influences/authoritativeness\nof the users into the model, 2) we include comment-based and like-based\nuser-user links to the graph, 3) we superimpose multiple heterogeneous graphs\ninto one thereby allowing multiple types of links to exist between two users.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:29:26 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Chen", "Jing", ""], ["Sun", "Haonan", ""], ["Xu", "Keyang", ""]]}, {"id": "1912.07915", "submitter": "Fengshi Jing", "authors": "Fengshi Jing and Qingpeng Zhang", "title": "Knowledge-Enhanced Attentive Learning for Answer Selection in Community\n  Question Answering Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the community question answering (CQA) system, the answer selection task\naims to identify the best answer for a specific question, and thus is playing a\nkey role in enhancing the service quality through recommending appropriate\nanswers for new questions. Recent advances in CQA answer selection focus on\nenhancing the performance by incorporating the community information,\nparticularly the expertise (previous answers) and authority (position in the\nsocial network) of an answerer. However, existing approaches for incorporating\nsuch information are limited in (a) only considering either the expertise or\nthe authority, but not both; (b) ignoring the domain knowledge to differentiate\ntopics of previous answers; and (c) simply using the authority information to\nadjust the similarity score, instead of fully utilizing it in the process of\nmeasuring the similarity between segments of the question and the answer. We\npropose the Knowledge-enhanced Attentive Answer Selection (KAAS) model, which\nenhances the performance through (a) considering both the expertise and the\nauthority of the answerer; (b) utilizing the human-labeled tags, the taxonomy\nof the tags, and the votes as the domain knowledge to infer the expertise of\nthe answer; (c) using matrix decomposition of the social network (formed by\nfollowing-relationship) to infer the authority of the answerer and\nincorporating such information in the process of evaluating the similarity\nbetween segments. Besides, for vertical community, we incorporate an external\nknowledge graph to capture more professional information for vertical CQA\nsystems. Then we adopt the attention mechanism to integrate the analysis of the\ntext of questions and answers and the aforementioned community information.\nExperiments with both vertical and general CQA sites demonstrate the superior\nperformance of the proposed KAAS model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:33:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jing", "Fengshi", ""], ["Zhang", "Qingpeng", ""]]}, {"id": "1912.07940", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk", "title": "To What Extent are Name Variants Used as Named Entities in Turkish\n  Tweets?", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media texts differ from regular texts in various aspects. One of the\nmain differences is the common use of informal name variants instead of\nwell-formed named entities in social media compared to regular texts. These\nname variants may come in the form of abbreviations, nicknames, contractions,\nand hypocoristic uses, in addition to names distorted due to capitalization and\nwriting errors. In this paper, we present an analysis of the named entities in\na publicly-available tweet dataset in Turkish with respect to their being name\nvariants belonging to different categories. We also provide finer-grained\nannotations of the named entities as well-formed names and different categories\nof name variants, where these annotations are made publicly-available. The\nanalysis presented and the accompanying annotations will contribute to related\nresearch on the treatment of named entities in social media.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:43:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""]]}, {"id": "1912.07942", "submitter": "Santiago Zanella-Beguelin", "authors": "Marc Brockschmidt, Boris K\\\"opf, Olga Ohrimenko, Andrew Paverd, Victor\n  R\\\"uhle, Shruti Tople, Lukas Wutschitz, Santiago Zanella-B\\'eguelin", "title": "Analyzing Information Leakage of Updates to Natural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---differential score and\ndifferential rank---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:46:08 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 13:28:16 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 21:41:35 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Brockschmidt", "Marc", ""], ["K\u00f6pf", "Boris", ""], ["Ohrimenko", "Olga", ""], ["Paverd", "Andrew", ""], ["R\u00fchle", "Victor", ""], ["Tople", "Shruti", ""], ["Wutschitz", "Lukas", ""], ["Zanella-B\u00e9guelin", "Santiago", ""]]}, {"id": "1912.07943", "submitter": "Hazrat Ali", "authors": "Hazrat Ali, Ahsan Ullah, Talha Iqbal, Shahid Khattak", "title": "Pioneer dataset and automatic recognition of Urdu handwritten characters\n  using a deep autoencoder and convolutional neural network", "comments": "SN Applied Sciences, December 2019", "journal-ref": null, "doi": "10.1007/s42452-019-1914-1", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic recognition of Urdu handwritten digits and characters, is a\nchallenging task. It has applications in postal address reading, bank's cheque\nprocessing, and digitization and preservation of handwritten manuscripts from\nold ages. While there exists a significant work for automatic recognition of\nhandwritten English characters and other major languages of the world, the work\ndone for Urdu lan-guage is extremely insufficient. This paper has two goals.\nFirstly, we introduce a pioneer dataset for handwritten digits and characters\nof Urdu, containing samples from more than 900 individuals. Secondly, we report\nresults for automatic recog-nition of handwritten digits and characters as\nachieved by using deep auto-encoder network and convolutional neural network.\nMore specifically, we use a two-layer and a three-layer deep autoencoder\nnetwork and convolutional neural network and evaluate the two frameworks in\nterms of recognition accuracy. The proposed framework of deep autoencoder can\nsuccessfully recognize digits and characters with an accuracy of 97% for digits\nonly, 81% for characters only and 82% for both digits and characters\nsimultaneously. In comparison, the framework of convolutional neural network\nhas accuracy of 96.7% for digits only, 86.5% for characters only and 82.7% for\nboth digits and characters simultaneously. These frameworks can serve as\nbaselines for future research on Urdu handwritten text.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:49:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Ali", "Hazrat", ""], ["Ullah", "Ahsan", ""], ["Iqbal", "Talha", ""], ["Khattak", "Shahid", ""]]}, {"id": "1912.07946", "submitter": "Giuseppe Antonio Di Luna", "authors": "Fiorella Artuso, Giuseppe Antonio Di Luna, Luca Massarelli and\n  Leonardo Querzoni", "title": "In Nomine Function: Naming Functions in Stripped Binaries with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of automatically naming pieces of\nassembly code. Where by naming we mean assigning to an assembly function a\nstring of words that would likely be assigned by a human reverse engineer. We\nformally and precisely define the framework in which our investigation takes\nplace. That is we define the problem, we provide reasonable justifications for\nthe choices that we made for the design of training and the tests. We performed\nan analysis on a large real-world corpora constituted by nearly 9 millions of\nfunctions taken from more than 22k softwares. In such framework we test\nbaselines coming from the field of Natural Language Processing (e.g., Seq2Seq\nnetworks and Transformer). Interestingly, our evaluation shows promising\nresults beating the state-of-the-art and reaching good performance. We\ninvestigate the applicability of tine-tuning (i.e., taking a model already\ntrained on a large generic corpora and retraining it for a specific task). Such\ntechnique is popular and well-known in the NLP field. Our results confirm that\nfine-tuning is effective even when neural networks are applied to binaries. We\nshow that a model, pre-trained on the aforementioned corpora, when fine-tuned\nhas higher performances on specific domains (such as predicting names in system\nutilites, malware, etc).\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 11:59:41 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 16:17:59 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 09:31:56 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Artuso", "Fiorella", ""], ["Di Luna", "Giuseppe Antonio", ""], ["Massarelli", "Luca", ""], ["Querzoni", "Leonardo", ""]]}, {"id": "1912.07976", "submitter": "Heng Yang", "authors": "Heng Yang, Biqing Zeng, JianHao Yang, Youwei Song and Ruyang Xu", "title": "A Multi-task Learning Model for Chinese-oriented Aspect Polarity\n  Classification and Aspect Term Extraction", "comments": "Submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis (ABSA) task is a multi-grained task of\nnatural language processing and consists of two subtasks: aspect term\nextraction (ATE) and aspect polarity classification (APC). Most of the existing\nwork focuses on the subtask of aspect term polarity inferring and ignores the\nsignificance of aspect term extraction. Besides, the existing researches do not\npay attention to the research of the Chinese-oriented ABSA task. Based on the\nlocal context focus (LCF) mechanism, this paper firstly proposes a multi-task\nlearning model for Chinese-oriented aspect-based sentiment analysis, namely\nLCF-ATEPC. Compared with existing models, this model equips the capability of\nextracting aspect term and inferring aspect term polarity synchronously,\nmoreover, this model is effective to analyze both Chinese and English comments\nsimultaneously and the experiment on a multilingual mixed dataset proved its\navailability. By integrating the domain-adapted BERT model, the LCF-ATEPC model\nachieved the state-of-the-art performance of aspect term extraction and aspect\npolarity classification in four Chinese review datasets. Besides, the\nexperimental results on the most commonly used SemEval-2014 task4 Restaurant\nand Laptop datasets outperform the state-of-the-art performance on the ATE and\nAPC subtask.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:47:33 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 01:38:38 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 09:20:28 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Yang", "Heng", ""], ["Zeng", "Biqing", ""], ["Yang", "JianHao", ""], ["Song", "Youwei", ""], ["Xu", "Ruyang", ""]]}, {"id": "1912.08011", "submitter": "Xin Feng", "authors": "Xin Feng and Lei Wang", "title": "Application of Word2vec in Phoneme Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present how to hybridize a Word2vec model and an\nattention-based end-to-end speech recognition model. We build a phoneme\nrecognition system based on Listen, Attend and Spell model. And the phoneme\nrecognition model uses a word2vec model to initialize the embedding matrix for\nthe improvement of the performance, which can increase the distance among the\nphoneme vectors. At the same time, in order to solve the problem of overfitting\nin the 61 phoneme recognition model on TIMIT dataset, we propose a new training\nmethod. A 61-39 phoneme mapping comparison table is used to inverse map the\nphonemes of the dataset to generate more 61 phoneme training data. At the end\nof training, replace the dataset with a standard dataset for corrective\ntraining. Our model can achieve the best result under the TIMIT dataset which\nis 16.5% PER (Phoneme Error Rate).\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:45:13 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 12:40:45 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Feng", "Xin", ""], ["Wang", "Lei", ""]]}, {"id": "1912.08084", "submitter": "Preslav Nakov", "authors": "Pepa Gencheva, Ivan Koychev, Llu\\'is M\\`arquez, Alberto\n  Barr\\'on-Cede\\~no, Preslav Nakov", "title": "A Context-Aware Approach for Detecting Check-Worthy Claims in Political\n  Debates", "comments": "Check-worthiness; Fact-Checking; Veracity; Neural Networks. arXiv\n  admin note: substantial text overlap with arXiv:1908.01328", "journal-ref": "RANLP-2017", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of investigative journalism, we address the problem of\nautomatically identifying which claims in a given document are most worthy and\nshould be prioritized for fact-checking. Despite its importance, this is a\nrelatively understudied problem. Thus, we create a new dataset of political\ndebates, containing statements that have been fact-checked by nine reputable\nsources, and we train machine learning models to predict which claims should be\nprioritized for fact-checking, i.e., we model the problem as a ranking task.\nUnlike previous work, which has looked primarily at sentences in isolation, in\nthis paper we focus on a rich input representation modeling the context:\nrelationship between the target statement and the larger context of the debate,\ninteraction between the opponents, and reaction by the moderator and by the\npublic. Our experiments show state-of-the-art results, outperforming a strong\nrivaling system by a margin, while also confirming the importance of the\ncontextual information.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 10:29:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Gencheva", "Pepa", ""], ["Koychev", "Ivan", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.08226", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara", "title": "Meshed-Memory Transformer for Image Captioning", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based architectures represent the state of the art in sequence\nmodeling tasks like machine translation and language understanding. Their\napplicability to multi-modal contexts like image captioning, however, is still\nlargely under-explored. With the aim of filling this gap, we present M$^2$ - a\nMeshed Transformer with Memory for Image Captioning. The architecture improves\nboth the image encoding and the language generation steps: it learns a\nmulti-level representation of the relationships between image regions\nintegrating learned a priori knowledge, and uses a mesh-like connectivity at\ndecoding stage to exploit low- and high-level features. Experimentally, we\ninvestigate the performance of the M$^2$ Transformer and different\nfully-attentive models in comparison with recurrent ones. When tested on COCO,\nour proposal achieves a new state of the art in single-model and ensemble\nconfigurations on the \"Karpathy\" test split and on the online test server. We\nalso assess its performances when describing objects unseen in the training\nset. Trained models and code for reproducing the experiments are publicly\navailable at: https://github.com/aimagelab/meshed-memory-transformer.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:03:23 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 19:29:14 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Cornia", "Marcella", ""], ["Stefanini", "Matteo", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1912.08259", "submitter": "Sarkhan Badirli", "authors": "Sarkhan Badirli, Mary Borgo Ton, Abdulmecit Gungor, Murat Dundar", "title": "Open Set Authorship Attribution toward Demystifying Victorian\n  Periodicals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Existing research in computational authorship attribution (AA) has primarily\nfocused on attribution tasks with a limited number of authors in a closed-set\nconfiguration. This restricted set-up is far from being realistic in dealing\nwith highly entangled real-world AA tasks that involve a large number of\ncandidate authors for attribution during test time. In this paper, we study AA\nin historical texts using anew data set compiled from the Victorian literature.\nWe investigate the predictive capacity of most common English words in\ndistinguishing writings of most prominent Victorian novelists. We challenged\nthe closed-set classification assumption and discussed the limitations of\nstandard machine learning techniques in dealing with the open set AA task. Our\nexperiments suggest that a linear classifier can achieve near perfect\nattribution accuracy under closed set assumption yet, the need for more robust\napproaches becomes evident once a large candidate pool has to be considered in\nthe open-set classification setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 20:15:34 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Badirli", "Sarkhan", ""], ["Ton", "Mary Borgo", ""], ["Gungor", "Abdulmecit", ""], ["Dundar", "Murat", ""]]}, {"id": "1912.08282", "submitter": "Yi Zhou", "authors": "Yi Zhou, Xiaoqing Zheng, Xuanjing Huang", "title": "Chinese Named Entity Recognition Augmented with Lexicon Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by a concept of content-addressable retrieval from cognitive\nscience, we propose a novel fragment-based model augmented with a lexicon-based\nmemory for Chinese NER, in which both the character-level and word-level\nfeatures are combined to generate better feature representations for possible\nname candidates. It is observed that locating the boundary information of\nentity names is useful in order to classify them into pre-defined categories.\nPosition-dependent features, including prefix and suffix are introduced for NER\nin the form of distributed representation. The lexicon-based memory is used to\nhelp generate such position-dependent features and deal with the problem of\nout-of-vocabulary words. Experimental results showed that the proposed model,\ncalled LEMON, achieved state-of-the-art on four datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:38:22 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 18:09:12 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhou", "Yi", ""], ["Zheng", "Xiaoqing", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1912.08290", "submitter": "Jefferson Pena Torres JAPeTo", "authors": "Jefferson A. Pe\\~na Torres, Raul Ernesto Gutierrez, Victor A. Bucheli,\n  Fabio A. Gonzalez O", "title": "The performance evaluation of Multi-representation in the Deep Learning\n  models for Relation Extraction Task", "comments": "11 pages, 3 tables, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Single implementing, concatenating, adding or replacing of the\nrepresentations has yielded significant improvements on many NLP tasks. Mainly\nin Relation Extraction where static, contextualized and others representations\nthat are capable of explaining word meanings through the linguistic features\nthat these incorporates. In this work addresses the question of how is improved\nthe relation extraction using different types of representations generated by\npretrained language representation models. We benchmarked our approach using\npopular word representation models, replacing and concatenating static,\ncontextualized and others representations of hand-extracted features. The\nexperiments show that representation is a crucial element to choose when DL\napproach is applied. Word embeddings from Flair and BERT can be well\ninterpreted by a deep learning model for RE task, and replacing static word\nembeddings with contextualized word representations could lead to significant\nimprovements. While, the hand-created representations requires is\ntime-consuming and not is ensure a improve in combination with others\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 21:58:51 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Torres", "Jefferson A. Pe\u00f1a", ""], ["Gutierrez", "Raul Ernesto", ""], ["Bucheli", "Victor A.", ""], ["O", "Fabio A. Gonzalez", ""]]}, {"id": "1912.08320", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah\n  Tang, Jenny Huang", "title": "Garbage In, Garbage Out? Do Machine Learning Application Papers in\n  Social Computing Report Where Human-Labeled Training Data Comes From?", "comments": "18 pages, includes appendix", "journal-ref": "Proc ACM FAT* 2020", "doi": "10.1145/3351095.3372862", "report-no": null, "categories": "cs.CY cs.CL cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many machine learning projects for new application areas involve teams of\nhumans who label data for a particular purpose, from hiring crowdworkers to the\npaper's authors labeling the data themselves. Such a task is quite similar to\n(or a form of) structured content analysis, which is a longstanding methodology\nin the social sciences and humanities, with many established best practices. In\nthis paper, we investigate to what extent a sample of machine learning\napplication papers in social computing --- specifically papers from ArXiv and\ntraditional publications performing an ML classification task on Twitter data\n--- give specific details about whether such best practices were followed. Our\nteam conducted multiple rounds of structured content analysis of each paper,\nmaking determinations such as: Does the paper report who the labelers were,\nwhat their qualifications were, whether they independently labeled the same\nitems, whether inter-rater reliability metrics were disclosed, what level of\ntraining and/or instructions were given to labelers, whether compensation for\ncrowdworkers is disclosed, and if the training data is publicly available. We\nfind a wide divergence in whether such practices were followed and documented.\nMuch of machine learning research and education focuses on what is done once a\n\"gold standard\" of training data is available, but we discuss issues around the\nequally-important aspect of whether such data is reliable in the first place.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:49:19 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Geiger", "R. Stuart", ""], ["Yu", "Kevin", ""], ["Yang", "Yanlai", ""], ["Dai", "Mindy", ""], ["Qiu", "Jie", ""], ["Tang", "Rebekah", ""], ["Huang", "Jenny", ""]]}, {"id": "1912.08360", "submitter": "Feilong Chen", "authors": "Feilong Chen, Fandong Meng, Jiaming Xu, Peng Li, Bo Xu and Jie Zhou", "title": "DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Dialog is a vision-language task that requires an AI agent to engage\nin a conversation with humans grounded in an image. It remains a challenging\ntask since it requires the agent to fully understand a given question before\nmaking an appropriate response not only from the textual dialog history, but\nalso from the visually-grounded information. While previous models typically\nleverage single-hop reasoning or single-channel reasoning to deal with this\ncomplex multimodal reasoning task, which is intuitively insufficient. In this\npaper, we thus propose a novel and more powerful Dual-channel Multi-hop\nReasoning Model for Visual Dialog, named DMRM. DMRM synchronously captures\ninformation from the dialog history and the image to enrich the semantic\nrepresentation of the question by exploiting dual-channel reasoning.\nSpecifically, DMRM maintains a dual channel to obtain the question- and\nhistory-aware image features and the question- and image-aware dialog history\nfeatures by a mulit-hop reasoning process in each channel. Additionally, we\nalso design an effective multimodal attention to further enhance the decoder to\ngenerate more accurate responses. Experimental results on the VisDial v0.9 and\nv1.0 datasets demonstrate that the proposed model is effective and outperforms\ncompared models by a significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 03:09:12 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chen", "Feilong", ""], ["Meng", "Fandong", ""], ["Xu", "Jiaming", ""], ["Li", "Peng", ""], ["Xu", "Bo", ""], ["Zhou", "Jie", ""]]}, {"id": "1912.08374", "submitter": "Somak Aditya", "authors": "Somak Aditya, Atanu Sinha", "title": "Uncovering Relations for Marketing Knowledge Representation", "comments": "8 pages, 1 figure, 8 tables (2 page Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online behaviors of consumers and marketers generate massive marketing data,\nwhich ever more sophisticated models attempt to turn into insights and aid\ndecisions by marketers. Yet, in making decisions human managers bring to bear\nmarketing knowledge which reside outside of data and models. Thus, it behooves\ncreation of an automated marketing knowledge base that can interact with data\nand models. Currently, marketing knowledge is dispersed in large corpora, but\nno definitive knowledge base for marketing exists. Out of the two broad aspects\nof marketing knowledge - representation and reasoning - this treatise focuses\non the former. Specifically, we focus on creation of marketing knowledge graph\nfrom corpora, which requires identification of entities and relations. The\nrelation identification task is particularly challenging in marketing, because\nof the non-factoid nature of much marketing knowledge, and the difficulty of\nforming rules that govern relations. Specifically, we define a set of relations\nto capture marketing knowledge, propose a pipeline for creating the knowledge\ngraph from text and propose a rule-guided semi-supervised relation prediction\nalgorithm to extract relations between marketing entities from sentences.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 04:32:18 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 09:05:59 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 03:49:50 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Aditya", "Somak", ""], ["Sinha", "Atanu", ""]]}, {"id": "1912.08404", "submitter": "Weixin Zeng", "authors": "Weixin Zeng, Xiang Zhao, Jiuyang Tang, and Xuemin Lin", "title": "Collective Entity Alignment via Adaptive Features", "comments": "ICDE20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) identifies entities that refer to the same real-world\nobject but locate in different knowledge graphs (KGs), and has been harnessed\nfor KG construction and integration. When generating EA results, current\nsolutions treat entities independently and fail to take into account the\ninterdependence between entities. To fill this gap, we propose a collective EA\nframework. We first employ three representative features, i.e., structural,\nsemantic and string signals, which are adapted to capture different aspects of\nthe similarity between entities in heterogeneous KGs. In order to make\ncollective EA decisions, we formulate EA as the classical stable matching\nproblem, which is further effectively solved by deferred acceptance algorithm.\nOur proposal is evaluated on both cross-lingual and mono-lingual EA benchmarks\nagainst state-of-the-art solutions, and the empirical results verify its\neffectiveness and superiority.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 06:25:12 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 13:50:21 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 08:49:39 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zeng", "Weixin", ""], ["Zhao", "Xiang", ""], ["Tang", "Jiuyang", ""], ["Lin", "Xuemin", ""]]}, {"id": "1912.08441", "submitter": "Fanchao Qi", "authors": "Lei Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun", "title": "Multi-channel Reverse Dictionary Model", "comments": "Accepted by AAAI Conference on Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reverse dictionary takes the description of a target word as input and\noutputs the target word together with other words that match the description.\nExisting reverse dictionary methods cannot deal with highly variable input\nqueries and low-frequency target words successfully. Inspired by the\ndescription-to-word inference process of humans, we propose the multi-channel\nreverse dictionary model, which can mitigate the two problems simultaneously.\nOur model comprises a sentence encoder and multiple predictors. The predictors\nare expected to identify different characteristics of the target word from the\ninput query. We evaluate our model on English and Chinese datasets including\nboth dictionary definitions and human-written descriptions. Experimental\nresults show that our model achieves the state-of-the-art performance, and even\noutperforms the most popular commercial reverse dictionary system on the\nhuman-written description dataset. We also conduct quantitative analyses and a\ncase study to demonstrate the effectiveness and robustness of our model. All\nthe code and data of this work can be obtained on\nhttps://github.com/thunlp/MultiRD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:13:43 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 02:12:09 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zhang", "Lei", ""], ["Qi", "Fanchao", ""], ["Liu", "Zhiyuan", ""], ["Wang", "Yasheng", ""], ["Liu", "Qun", ""], ["Sun", "Maosong", ""]]}, {"id": "1912.08442", "submitter": "Xinting Huang", "authors": "Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang", "title": "MALA: Cross-Domain Dialogue Generation with Action Learning", "comments": "Update: Accepted to Proceedings of AAAI 2020", "journal-ref": null, "doi": "10.1609/aaai.v34i05.6306", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response generation for task-oriented dialogues involves two basic\ncomponents: dialogue planning and surface realization. These two components,\nhowever, have a discrepancy in their objectives, i.e., task completion and\nlanguage quality. To deal with such discrepancy, conditioned response\ngeneration has been introduced where the generation process is factorized into\naction decision and language generation via explicit action representations. To\nobtain action representations, recent studies learn latent actions in an\nunsupervised manner based on the utterance lexical similarity. Such an action\nlearning approach is prone to diversities of language surfaces, which may\nimpinge task completion and language quality. To address this issue, we propose\nmulti-stage adaptive latent action learning (MALA) that learns semantic latent\nactions by distinguishing the effects of utterances on dialogue progress. We\nmodel the utterance effect using the transition of dialogue states caused by\nthe utterance and develop a semantic similarity measurement that estimates\nwhether utterances have similar effects. For learning semantic actions on\ndomains without dialogue states, MsALA extends the semantic similarity\nmeasurement across domains progressively, i.e., from aligning shared actions to\nlearning domain-specific actions. Experiments using multi-domain datasets, SMD\nand MultiWOZ, show that our proposed model achieves consistent improvements\nover the baselines models in terms of both task completion and language\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:14:10 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:33:38 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Huang", "Xinting", ""], ["Qi", "Jianzhong", ""], ["Sun", "Yu", ""], ["Zhang", "Rui", ""]]}, {"id": "1912.08462", "submitter": "Thilo von Neumann", "authors": "Thilo von Neumann, Keisuke Kinoshita, Lukas Drude, Christoph\n  Boeddeker, Marc Delcroix, Tomohiro Nakatani, Reinhold Haeb-Umbach", "title": "End-to-end training of time domain audio separation and recognition", "comments": "5 pages, 1 figure, to appear in ICASSP 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053461", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising interest in single-channel multi-speaker speech separation sparked\ndevelopment of End-to-End (E2E) approaches to multi-speaker speech recognition.\nHowever, up until now, state-of-the-art neural network-based time domain source\nseparation has not yet been combined with E2E speech recognition. We here\ndemonstrate how to combine a separation module based on a Convolutional Time\ndomain Audio Separation Network (Conv-TasNet) with an E2E speech recognizer and\nhow to train such a model jointly by distributing it over multiple GPUs or by\napproximating truncated back-propagation for the convolutional front-end. To\nput this work into perspective and illustrate the complexity of the design\nspace, we provide a compact overview of single-channel multi-speaker\nrecognition systems. Our experiments show a word error rate of 11.0% on\nWSJ0-2mix and indicate that our joint time domain model can yield substantial\nimprovements over cascade DNN-HMM and monolithic E2E frequency domain systems\nproposed so far.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:08:05 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 17:14:07 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 11:39:14 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["von Neumann", "Thilo", ""], ["Kinoshita", "Keisuke", ""], ["Drude", "Lukas", ""], ["Boeddeker", "Christoph", ""], ["Delcroix", "Marc", ""], ["Nakatani", "Tomohiro", ""], ["Haeb-Umbach", "Reinhold", ""]]}, {"id": "1912.08492", "submitter": "Hrituraj Singh", "authors": "Kushal Chawla, Hrituraj Singh, Arijit Pramanik, Mithlesh Kumar, Balaji\n  Vasan Srinivasan", "title": "Generating summaries tailored to target characteristics", "comments": "Appeared in CiCLing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, research efforts have gained pace to cater to varied user\npreferences while generating text summaries. While there have been attempts to\nincorporate a few handpicked characteristics such as length or entities, a\nholistic view around these preferences is missing and crucial insights on why\ncertain characteristics should be incorporated in a specific manner are absent.\nWith this objective, we provide a categorization around these characteristics\nrelevant to the task of text summarization: one, focusing on what content needs\nto be generated and second, focusing on the stylistic aspects of the output\nsummaries. We use our insights to provide guidelines on appropriate methods to\nincorporate various classes characteristics in sequence-to-sequence\nsummarization framework. Our experiments with incorporating topics, readability\nand simplicity indicate the viability of the proposed prescriptions\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 10:05:49 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Chawla", "Kushal", ""], ["Singh", "Hrituraj", ""], ["Pramanik", "Arijit", ""], ["Kumar", "Mithlesh", ""], ["Srinivasan", "Balaji Vasan", ""]]}, {"id": "1912.08494", "submitter": "Sameen Maruf", "authors": "Sameen Maruf, Fahimeh Saleh and Gholamreza Haffari", "title": "A Survey on Document-level Neural Machine Translation: Methods and\n  Evaluation", "comments": "Accepted for publication by ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) is an important task in natural language processing\n(NLP) as it automates the translation process and reduces the reliance on human\ntranslators. With the resurgence of neural networks, the translation quality\nsurpasses that of the translations obtained using statistical techniques for\nmost language-pairs. Up until a few years ago, almost all of the neural\ntranslation models translated sentences independently, without incorporating\nthe wider document-context and inter-dependencies among the sentences. The aim\nof this survey paper is to highlight the major works that have been undertaken\nin the space of document-level machine translation after the neural revolution,\nso that researchers can recognise the current state and future directions of\nthis field. We provide an organisation of the literature based on novelties in\nmodelling and architectures as well as training and decoding strategies. In\naddition, we cover evaluation strategies that have been introduced to account\nfor the improvements in document MT, including automatic metrics and\ndiscourse-targeted test sets. We conclude by presenting possible avenues for\nfuture exploration in this research field.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 10:07:20 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 23:10:22 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 00:31:53 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Maruf", "Sameen", ""], ["Saleh", "Fahimeh", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "1912.08555", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "Curriculum Learning Strategies for IR: An Empirical Study on\n  Conversation Response Ranking", "comments": "Accepted for publication in the 42nd European Conference on\n  Information Retrieval (ECIR'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural ranking models are traditionally trained on a series of random\nbatches, sampled uniformly from the entire training set. Curriculum learning\nhas recently been shown to improve neural models' effectiveness by sampling\nbatches non-uniformly, going from easy to difficult instances during training.\nIn the context of neural Information Retrieval (IR) curriculum learning has not\nbeen explored yet, and so it remains unclear (1) how to measure the difficulty\nof training instances and (2) how to transition from easy to difficult\ninstances during training. To address both challenges and determine whether\ncurriculum learning is beneficial for neural ranking models, we need\nlarge-scale datasets and a retrieval task that allows us to conduct a wide\nrange of experiments. For this purpose, we resort to the task of conversation\nresponse ranking: ranking responses given the conversation history. In order to\ndeal with challenge (1), we explore scoring functions to measure the difficulty\nof conversations based on different input spaces. To address challenge (2) we\nevaluate different pacing functions, which determine the velocity in which we\ngo from easy to difficult instances. We find that, overall, by just\nintelligently sorting the training data (i.e., by performing curriculum\nlearning) we can improve the retrieval effectiveness by up to 2%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:13:30 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "1912.08582", "submitter": "Giorgio Maria Di Nunzio", "authors": "Nataliya Sira, Giorgio Maria Di Nunzio, Viviana Nosilia", "title": "Towards an automatic recognition of mixed languages: The\n  Ukrainian-Russian hybrid language Surzhyk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language interference is common in today's multilingual societies where more\nlanguages are being in contact and as a global final result leads to the\ncreation of hybrid languages. These, together with doubts on their right to be\nofficially recognised made emerge in the area of computational linguistics the\nproblem of their automatic identification and further elaboration. In this\npaper, we propose a first attempt to identify the elements of a\nUkrainian-Russian hybrid language, Surzhyk, through the adoption of the\nexample-based rules created with the instruments of programming language R. Our\nexample-based study consists of: 1) analysis of spoken samples of Surzhyk\nregistered by Del Gaudio (2010) in Kyiv area and creation of the written\ncorpus; 2) production of specific rules on the identification of Surzhyk\npatterns and their implementation; 3) testing the code and analysing the\neffectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:13:29 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Sira", "Nataliya", ""], ["Di Nunzio", "Giorgio Maria", ""], ["Nosilia", "Viviana", ""]]}, {"id": "1912.08633", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara,\n  Georgios Paliouras", "title": "iASiS Open Data Graph: Automated Semantic Integration of\n  Disease-Specific Knowledge", "comments": "6 pages, 2 figures, accepted in IEEE 33rd International Symposium on\n  Computer Based Medical Systems (CBMS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical research, unified access to up-to-date domain-specific\nknowledge is crucial, as such knowledge is continuously accumulated in\nscientific literature and structured resources. Identifying and extracting\nspecific information is a challenging task and computational analysis of\nknowledge bases can be valuable in this direction. However, for\ndisease-specific analyses researchers often need to compile their own datasets,\nintegrating knowledge from different resources, or reuse existing datasets,\nthat can be out-of-date. In this study, we propose a framework to automatically\nretrieve and integrate disease-specific knowledge into an up-to-date semantic\ngraph, the iASiS Open Data Graph. This disease-specific semantic graph provides\naccess to knowledge relevant to specific concepts and their individual aspects,\nin the form of concept relations and attributes. The proposed approach is\nimplemented as an open-source framework and applied to three diseases (Lung\nCancer, Dementia, and Duchenne Muscular Dystrophy). Exemplary queries are\npresented, investigating the potential of this automatically generated semantic\ngraph as a basis for retrieval and analysis of disease-specific knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:33:05 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 10:47:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Bougiatiotis", "Konstantinos", ""], ["Krithara", "Anastasia", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1912.08777", "submitter": "Jingqing Zhang", "authors": "Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\n  Summarization", "comments": "Added results from mixed+stochastic model, test-set overlapping\n  analysis; Code link added; Accepted for ICML 2020. arXiv admin note: text\n  overlap with arXiv:1605.06560, arXiv:1205.2395, arXiv:0902.4351,\n  arXiv:1610.09932, arXiv:nucl-ex/0512029 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work pre-training Transformers with self-supervised objectives on\nlarge text corpora has shown great success when fine-tuned on downstream NLP\ntasks including text summarization. However, pre-training objectives tailored\nfor abstractive text summarization have not been explored. Furthermore there is\na lack of systematic evaluation across diverse domains. In this work, we\npropose pre-training large Transformer-based encoder-decoder models on massive\ntext corpora with a new self-supervised objective. In PEGASUS, important\nsentences are removed/masked from an input document and are generated together\nas one output sequence from the remaining sentences, similar to an extractive\nsummary. We evaluated our best PEGASUS model on 12 downstream summarization\ntasks spanning news, science, stories, instructions, emails, patents, and\nlegislative bills. Experiments demonstrate it achieves state-of-the-art\nperformance on all 12 downstream datasets measured by ROUGE scores. Our model\nalso shows surprising performance on low-resource summarization, surpassing\nprevious state-of-the-art results on 6 datasets with only 1000 examples.\nFinally we validated our results using human evaluation and show that our model\nsummaries achieve human performance on multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:16:20 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 17:24:23 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 11:07:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Jingqing", ""], ["Zhao", "Yao", ""], ["Saleh", "Mohammad", ""], ["Liu", "Peter J.", ""]]}, {"id": "1912.08830", "submitter": "Dave Zhenyu Chen", "authors": "Dave Zhenyu Chen, Angel X. Chang, Matthias Nie{\\ss}ner", "title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language", "comments": "Project page: https://daveredrum.github.io/ScanRefer/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of 3D object localization in RGB-D scans using natural\nlanguage descriptions. As input, we assume a point cloud of a scanned 3D scene\nalong with a free-form description of a specified target object. To address\nthis task, we propose ScanRefer, learning a fused descriptor from 3D object\nproposals and encoded sentence embeddings. This fused descriptor correlates\nlanguage expressions with geometric features, enabling regression of the 3D\nbounding box of a target object. We also introduce the ScanRefer dataset,\ncontaining 51,583 descriptions of 11,046 objects from 800 ScanNet scenes.\nScanRefer is the first large-scale effort to perform object localization via\nnatural language expression directly in 3D.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 19:00:49 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 21:41:53 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 09:33:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chen", "Dave Zhenyu", ""], ["Chang", "Angel X.", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.08868", "submitter": "Rashid Mehdiyev Dr", "authors": "Rashid Mehdiyev, Jean Nava, Karan Sodhi, Saurav Acharya, Annie Ibrahim\n  Rana", "title": "Topic subject creation using unsupervised learning for topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the use of Non-Negative Matrix Factorization (NMF) and Latent\nDirichlet Allocation (LDA) algorithms to perform topic mining and labelling\napplied to retail customer communications in attempt to characterize the\nsubject of customers inquiries. In this paper we compare both algorithms in the\ntopic mining performance and propose methods to assign topic subject labels in\nan automated way.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:11:03 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Mehdiyev", "Rashid", ""], ["Nava", "Jean", ""], ["Sodhi", "Karan", ""], ["Acharya", "Saurav", ""], ["Rana", "Annie Ibrahim", ""]]}, {"id": "1912.08904", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Nick Craswell", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational information seeking (CIS) has been recognized as a major\nemerging research area in information retrieval. Such research will require\ndata and tools, to allow the implementation and study of conversational\nsystems. This paper introduces Macaw, an open-source framework with a modular\narchitecture for CIS research. Macaw supports multi-turn, multi-modal, and\nmixed-initiative interactions, and enables research for tasks such as document\nretrieval, question answering, recommendation, and structured data exploration.\nIt has a modular design to encourage the study of new CIS algorithms, which can\nbe evaluated in batch mode. It can also integrate with a user interface, which\nallows user studies and data collection in an interactive mode, where the back\nend can be fully algorithmic or a wizard of oz setup. Macaw is distributed\nunder the MIT License.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 21:51:22 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "1912.08960", "submitter": "Huiyuan Xie", "authors": "Huiyuan Xie, Tom Sherborne, Alexander Kuhnle, Ann Copestake", "title": "Going Beneath the Surface: Evaluating Image Captioning for\n  Grammaticality, Truthfulness and Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning as a multimodal task has drawn much interest in recent\nyears. However, evaluation for this task remains a challenging problem.\nExisting evaluation metrics focus on surface similarity between a candidate\ncaption and a set of reference captions, and do not check the actual relation\nbetween a caption and the underlying visual content. We introduce a new\ndiagnostic evaluation framework for the task of image captioning, with the goal\nof directly assessing models for grammaticality, truthfulness and diversity\n(GTD) of generated captions. We demonstrate the potential of our evaluation\nframework by evaluating existing image captioning models on a wide ranging set\nof synthetic datasets that we construct for diagnostic evaluation. We\nempirically show how the GTD evaluation framework, in combination with\ndiagnostic datasets, can provide insights into model capabilities and\nlimitations to supplement standard evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 00:27:40 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Xie", "Huiyuan", ""], ["Sherborne", "Tom", ""], ["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1912.08981", "submitter": "Hoang-Quoc Nguyen-Son", "authors": "Hoang-Quoc Nguyen-Son, Tran Phuong Thao, Seira Hidano, and Shinsaku\n  Kiyomoto", "title": "Identifying Adversarial Sentences by Analyzing Text Complexity", "comments": "PACLIC 2019, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attackers create adversarial text to deceive both human perception and the\ncurrent AI systems to perform malicious purposes such as spam product reviews\nand fake political posts. We investigate the difference between the adversarial\nand the original text to prevent the risk. We prove that the text written by a\nhuman is more coherent and fluent. Moreover, the human can express the idea\nthrough the flexible text with modern words while a machine focuses on\noptimizing the generated text by the simple and common words. We also suggest a\nmethod to identify the adversarial text by extracting the features related to\nour findings. The proposed method achieves high performance with 82.0% of\naccuracy and 18.4% of equal error rate, which is better than the existing\nmethods whose the best accuracy is 77.0% corresponding to the error rate 22.8%.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 01:45:36 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Nguyen-Son", "Hoang-Quoc", ""], ["Thao", "Tran Phuong", ""], ["Hidano", "Seira", ""], ["Kiyomoto", "Shinsaku", ""]]}, {"id": "1912.09003", "submitter": "Xiaoxiao Miao", "authors": "Xiaoxiao Miao and Ian McLoughlin", "title": "LSTM-TDNN with convolutional front-end for Dialect Identification in the\n  2019 Multi-Genre Broadcast Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Dialect Identification (DID) system developed for\nthe Fifth Edition of the Multi-Genre Broadcast challenge, the task of\nFine-grained Arabic Dialect Identification (MGB-5 ADI Challenge). The system\nimproves upon traditional DNN x-vector performance by employing a Convolutional\nand Long Short Term Memory-Recurrent (CLSTM) architecture to combine the\nbenefits of a convolutional neural network front-end for feature extraction and\na back-end recurrent neural to capture longer temporal dependencies.\nFurthermore we investigate intensive augmentation of one low resource dialect\nin the highly unbalanced training set using time-scale modification (TSM). This\nconverts an utterance to several time-stretched or time-compressed versions,\nsubsequently used to train the CLSTM system without using any other corpus. In\nthis paper, we also investigate speech augmentation using MUSAN and the RIR\ndatasets to increase the quantity and diversity of the existing training data\nin the normal way. Results show firstly that the CLSTM architecture outperforms\na traditional DNN x-vector implementation. Secondly, adopting TSM-based speed\nperturbation yields a small performance improvement for the unbalanced data,\nfinally that traditional data augmentation techniques yield further benefit, in\nline with evidence from related speaker and language recognition tasks. Our\nsystem achieved 2nd place ranking out of 15 entries in the MGB-5 ADI challenge,\npresented at ASRU 2019.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 03:20:33 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Miao", "Xiaoxiao", ""], ["McLoughlin", "Ian", ""]]}, {"id": "1912.09008", "submitter": "Yiming Cui", "authors": "Yiming Cui, Wanxiang Che, Wei-Nan Zhang, Ting Liu, Shijin Wang,\n  Guoping Hu", "title": "Discriminative Sentence Modeling for Story Ending Prediction", "comments": "8 pages, accepted as a conference paper at AAAI 2020", "journal-ref": null, "doi": "10.1609/aaai.v34i05.6260", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story Ending Prediction is a task that needs to select an appropriate ending\nfor the given story, which requires the machine to understand the story and\nsometimes needs commonsense knowledge. To tackle this task, we propose a new\nneural network called Diff-Net for better modeling the differences of each\nending in this task. The proposed model could discriminate two endings in three\nsemantic levels: contextual representation, story-aware representation, and\ndiscriminative representation. Experimental results on the Story Cloze Test\ndataset show that the proposed model siginificantly outperforms various systems\nby a large margin, and detailed ablation studies are given for better\nunderstanding our model. We also carefully examine the traditional and\nBERT-based models on both SCT v1.0 and v1.5 with interesting findings that may\npotentially help future studies.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 03:48:05 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cui", "Yiming", ""], ["Che", "Wanxiang", ""], ["Zhang", "Wei-Nan", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1912.09084", "submitter": "Jiali Zeng", "authors": "Jiali Zeng, Linfeng Song, Jinsong Su, Jun Xie, Wei Song, Jiebo Luo", "title": "Neural Simile Recognition with Cyclic Multitask Learning and Local\n  Attention", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simile recognition is to detect simile sentences and to extract simile\ncomponents, i.e., tenors and vehicles. It involves two subtasks: {\\it simile\nsentence classification} and {\\it simile component extraction}. Recent work has\nshown that standard multitask learning is effective for Chinese simile\nrecognition, but it is still uncertain whether the mutual effects between the\nsubtasks have been well captured by simple parameter sharing. We propose a\nnovel cyclic multitask learning framework for neural simile recognition, which\nstacks the subtasks and makes them into a loop by connecting the last to the\nfirst. It iteratively performs each subtask, taking the outputs of the previous\nsubtask as additional inputs to the current one, so that the interdependence\nbetween the subtasks can be better explored. Extensive experiments show that\nour framework significantly outperforms the current state-of-the-art model and\nour carefully designed baselines, and the gains are still remarkable using\nBERT.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 09:40:19 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zeng", "Jiali", ""], ["Song", "Linfeng", ""], ["Su", "Jinsong", ""], ["Xie", "Jun", ""], ["Song", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.09152", "submitter": "Fernando Sanchez Leon", "authors": "Fernando S\\'anchez Le\\'on and Ana Gonz\\'alez Ledesma", "title": "Annotating and normalizing biomedical NEs with limited knowledge", "comments": "8 pages; unpublished contribution to the PharmaCoNER shared task held\n  as part of BioNLP-OST 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Named entity recognition (NER) is the very first step in the linguistic\nprocessing of any new domain. It is currently a common process in BioNLP on\nEnglish clinical text. However, it is still in its infancy in other major\nlanguages, as it is the case for Spanish. Presented under the umbrella of the\nPharmaCoNER shared task, this paper describes a very simple method for the\nannotation and normalization of pharmacological, chemical and, ultimately,\nbiomedical named entities in clinical cases. The system developed for the\nshared task is based on limited knowledge, collected, structured and munged in\na way that clearly outperforms scores obtained by similar dictionary-based\nsystems for English in the past. Along with this recovering of the\nknowledge-based methods for NER in subdomains, the paper also highlights the\nkey contribution of resource-based systems in the validation and consolidation\nof both the annotation guidelines and the human annotation practices. In this\nsense, some of the authors discoverings on the overall quality of human\nannotated datasets question the above-mentioned `official' results obtained by\nthis system, that ranked second (0.91 F1-score) and first (0.916 F1-score),\nrespectively, in the two PharmaCoNER subtasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 12:05:36 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Le\u00f3n", "Fernando S\u00e1nchez", ""], ["Ledesma", "Ana Gonz\u00e1lez", ""]]}, {"id": "1912.09156", "submitter": "Xingyi Duan", "authors": "Xingyi Duan, Baoxin Wang, Ziyue Wang, Wentao Ma, Yiming Cui, Dayong\n  Wu, Shijin Wang, Ting Liu, Tianxiang Huo, Zhen Hu, Heng Wang, Zhiyuan Liu", "title": "CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial\n  Reading Comprehension", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32381-3_36", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Chinese judicial reading comprehension (CJRC) dataset which\ncontains approximately 10K documents and almost 50K questions with answers. The\ndocuments come from judgment documents and the questions are annotated by law\nexperts. The CJRC dataset can help researchers extract elements by reading\ncomprehension technology. Element extraction is an important task in the legal\nfield. However, it is difficult to predefine the element types completely due\nto the diversity of document types and causes of action. By contrast, machine\nreading comprehension technology can quickly extract elements by answering\nvarious questions from the long document. We build two strong baseline models\nbased on BERT and BiDAF. The experimental results show that there is enough\nspace for improvement compared to human annotators.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 12:17:38 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Duan", "Xingyi", ""], ["Wang", "Baoxin", ""], ["Wang", "Ziyue", ""], ["Ma", "Wentao", ""], ["Cui", "Yiming", ""], ["Wu", "Dayong", ""], ["Wang", "Shijin", ""], ["Liu", "Ting", ""], ["Huo", "Tianxiang", ""], ["Hu", "Zhen", ""], ["Wang", "Heng", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1912.09253", "submitter": "Eduardo Paluzo-Hidalgo", "authors": "Eduardo Paluzo-Hidalgo, Rocio Gonzalez-Diaz, Miguel A.\n  Guti\\'errez-Naranjo", "title": "Towards a Philological Metric through a Topological Data Analysis\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The canon of the baroque Spanish literature has been thoroughly studied with\nphilological techniques. The major representatives of the poetry of this epoch\nare Francisco de Quevedo and Luis de G\\'ongora y Argote. They are commonly\nclassified by the literary experts in two different streams: Quevedo belongs to\nthe Conceptismo and G\\'ongora to the Culteranismo. Besides, traditionally, even\nif Quevedo is considered the most representative of the Conceptismo, Lope de\nVega is also considered to be, at least, closely related to this literary\ntrend. In this paper, we use Topological Data Analysis techniques to provide a\nfirst approach to a metric distance between the literary style of these poets.\nAs a consequence, we reach results that are under the literary experts'\ncriteria, locating the literary style of Lope de Vega, closer to the one of\nQuevedo than to the one of G\\'ongora.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:04:24 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 19:20:21 GMT"}, {"version": "v3", "created": "Sat, 11 Jan 2020 17:26:25 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Paluzo-Hidalgo", "Eduardo", ""], ["Gonzalez-Diaz", "Rocio", ""], ["Guti\u00e9rrez-Naranjo", "Miguel A.", ""]]}, {"id": "1912.09257", "submitter": "Nick Rossenbach", "authors": "Nick Rossenbach, Albert Zeyer, Ralf Schl\\\"uter, Hermann Ney", "title": "Generating Synthetic Audio Data for Attention-Based Speech Recognition\n  Systems", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in text-to-speech (TTS) led to the development of flexible\nmulti-speaker end-to-end TTS systems. We extend state-of-the-art\nattention-based automatic speech recognition (ASR) systems with synthetic audio\ngenerated by a TTS system trained only on the ASR corpora itself. ASR and TTS\nsystems are built separately to show that text-only data can be used to enhance\nexisting end-to-end ASR systems without the necessity of parameter or\narchitecture changes. We compare our method with language model integration of\nthe same text data and with simple data augmentation methods like SpecAugment\nand show that performance improvements are mostly independent. We achieve\nimprovements of up to 33% relative in word-error-rate (WER) over a strong\nbaseline with data-augmentation in a low-resource environment\n(LibriSpeech-100h), closing the gap to a comparable oracle experiment by more\nthan 50\\%. We also show improvements of up to 5% relative WER over our most\nrecent ASR baseline on LibriSpeech-960h.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:09:07 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 14:08:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rossenbach", "Nick", ""], ["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1912.09297", "submitter": "Yue Ma", "authors": "Yue Ma, Zengfeng Zeng, Dawei Zhu, Xuan Li, Yiying Yang, Xiaoyuan Yao,\n  Kaijie Zhou, Jianping Shen", "title": "An End-to-End Dialogue State Tracking System with Machine Reading\n  Comprehension and Wide & Deep Classification", "comments": "The Thirty-Fourth AAAI Conference on Artificial Intelligence DSTC 8\n  Workshop (AAAI-20, DSTC 8 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach in DSTC 8 Track 4: Schema-Guided Dialogue\nState Tracking. The goal of this task is to predict the intents and slots in\neach user turn to complete the dialogue state tracking (DST) based on the\ninformation provided by the task's schema. Different from traditional\nstage-wise DST, we propose an end-to-end DST system to avoid error accumulation\nbetween the dialogue turns. The DST system consists of a machine reading\ncomprehension (MRC) model for non-categorical slots and a Wide & Deep model for\ncategorical slots. As far as we know, this is the first time that MRC and Wide\n& Deep model are applied to DST problem in a fully end-to-end way. Experimental\nresults show that our framework achieves an excellent performance on the test\ndataset including 50% zero-shot services with a joint goal accuracy of 0.8652\nand a slot tagging F1-Score of 0.9835.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:32:19 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 05:16:12 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ma", "Yue", ""], ["Zeng", "Zengfeng", ""], ["Zhu", "Dawei", ""], ["Li", "Xuan", ""], ["Yang", "Yiying", ""], ["Yao", "Xiaoyuan", ""], ["Zhou", "Kaijie", ""], ["Shen", "Jianping", ""]]}, {"id": "1912.09551", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro and Vinay P. Namboodiri", "title": "Deep Exemplar Networks for VQA and VQG", "comments": "This work is an extension of CVPR-2018 accepted paper\n  arXiv:1804.00298 and EMNLP-2018 accepted paper arXiv:1808.03986", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we consider the problem of solving semantic tasks such as\n`Visual Question Answering' (VQA), where one aims to answers related to an\nimage and `Visual Question Generation' (VQG), where one aims to generate a\nnatural question pertaining to an image. Solutions for VQA and VQG tasks have\nbeen proposed using variants of encoder-decoder deep learning based frameworks\nthat have shown impressive performance. Humans however often show\ngeneralization by relying on exemplar based approaches. For instance, the work\nby Tversky and Kahneman suggests that humans use exemplars when making\ncategorizations and decisions. In this work, we propose the incorporation of\nexemplar based approaches towards solving these problems. Specifically, we\nincorporate exemplar based approaches and show that an exemplar based module\ncan be incorporated in almost any of the deep learning architectures proposed\nin the literature and the addition of such a block results in improved\nperformance for solving these tasks. Thus, just as the incorporation of\nattention is now considered de facto useful for solving these tasks, similarly,\nincorporating exemplars also can be considered to improve any proposed\narchitecture for solving this task. We provide extensive empirical analysis for\nthe same through various architectures, ablations, and state of the art\ncomparisons.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:29:22 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Patro", "Badri N.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1912.09558", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Alfonso Medina-Urrea and Juan-Manuel Torres-Moreno", "title": "RIMAX: Ranking Semantic Rhymes by calculating Definition Similarity", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents RIMAX, a new system for detecting semantic rhymes, using\na Comprehensive Mexican Spanish Dictionary (DEM) and its Rhyming Dictionary\n(REM). We use the Vector Space Model to calculate the similarity of the\ndefinition of a query with the definitions corresponding to the assonant and\nconsonant rhymes of the query. The preliminary results using a manual\nevaluation are very encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:47:08 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 11:36:04 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Medina-Urrea", "Alfonso", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "1912.09582", "submitter": "Andreas Van Cranenburgh", "authors": "Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso\n  Caselli, Gertjan van Noord, Malvina Nissim", "title": "BERTje: A Dutch BERT Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The transformer-based pre-trained language model BERT has helped to improve\nstate-of-the-art performance on many natural language processing (NLP) tasks.\nUsing the same architecture and parameters, we developed and evaluated a\nmonolingual Dutch BERT model called BERTje. Compared to the multilingual BERT\nmodel, which includes Dutch but is only based on Wikipedia text, BERTje is\nbased on a large and diverse dataset of 2.4 billion tokens. BERTje consistently\noutperforms the equally-sized multilingual BERT model on downstream NLP tasks\n(part-of-speech tagging, named-entity recognition, semantic role labeling, and\nsentiment analysis). Our pre-trained Dutch BERT model is made available at\nhttps://github.com/wietsedv/bertje.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 22:59:26 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["de Vries", "Wietse", ""], ["van Cranenburgh", "Andreas", ""], ["Bisazza", "Arianna", ""], ["Caselli", "Tommaso", ""], ["van Noord", "Gertjan", ""], ["Nissim", "Malvina", ""]]}, {"id": "1912.09637", "submitter": "Wenhan Xiong", "authors": "Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov", "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language\n  Model", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs of pretrained language models have shown the\neffectiveness of self-supervised learning for a wide range of natural language\nprocessing (NLP) tasks. In addition to standard syntactic and semantic NLP\ntasks, pretrained models achieve strong improvements on tasks that involve\nreal-world knowledge, suggesting that large-scale language modeling could be an\nimplicit method to capture knowledge. In this work, we further investigate the\nextent to which pretrained models such as BERT capture knowledge using a\nzero-shot fact completion task. Moreover, we propose a simple yet effective\nweakly supervised pretraining objective, which explicitly forces the model to\nincorporate knowledge about real-world entities. Models trained with our new\nobjective yield significant improvements on the fact completion task. When\napplied to downstream tasks, our model consistently outperforms BERT on four\nentity-related question answering datasets (i.e., WebQuestions, TriviaQA,\nSearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard\nfine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 04:25:48 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Xiong", "Wenhan", ""], ["Du", "Jingfei", ""], ["Wang", "William Yang", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.09713", "submitter": "Marc van Zee", "authors": "Daniel Keysers, Nathanael Sch\\\"arli, Nathan Scales, Hylke Buisman,\n  Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz\n  Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, Olivier\n  Bousquet", "title": "Measuring Compositional Generalization: A Comprehensive Method on\n  Realistic Data", "comments": "Accepted for publication at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine learning methods exhibit limited compositional\ngeneralization. At the same time, there is a lack of realistic benchmarks that\ncomprehensively measure this ability, which makes it challenging to find and\nevaluate improvements. We introduce a novel method to systematically construct\nsuch benchmarks by maximizing compound divergence while guaranteeing a small\natom divergence between train and test sets, and we quantitatively compare this\nmethod to other approaches for creating compositional generalization\nbenchmarks. We present a large and realistic natural language question\nanswering dataset that is constructed according to this method, and we use it\nto analyze the compositional generalization ability of three machine learning\narchitectures. We find that they fail to generalize compositionally and that\nthere is a surprisingly strong negative correlation between compound divergence\nand accuracy. We also demonstrate how our method can be used to create new\ncompositionality benchmarks on top of the existing SCAN dataset, which confirms\nthese findings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:32:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 06:38:53 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Keysers", "Daniel", ""], ["Sch\u00e4rli", "Nathanael", ""], ["Scales", "Nathan", ""], ["Buisman", "Hylke", ""], ["Furrer", "Daniel", ""], ["Kashubin", "Sergii", ""], ["Momchev", "Nikola", ""], ["Sinopalnikov", "Danila", ""], ["Stafiniak", "Lukasz", ""], ["Tihon", "Tibor", ""], ["Tsarkov", "Dmitry", ""], ["Wang", "Xiao", ""], ["van Zee", "Marc", ""], ["Bousquet", "Olivier", ""]]}, {"id": "1912.09723", "submitter": "Pavel Efimov", "authors": "Pavel Efimov, Andrey Chertok, Leonid Boytsov, Pavel Braslavski", "title": "SberQuAD -- Russian Reading Comprehension Dataset: Description and\n  Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58219-7_1", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SberQuAD -- a large scale analog of Stanford SQuAD in the Russian language -\nis a valuable resource that has not been properly presented to the scientific\ncommunity. We fill this gap by providing a description, a thorough analysis,\nand baseline experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:44:42 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 10:44:08 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 19:55:06 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Efimov", "Pavel", ""], ["Chertok", "Andrey", ""], ["Boytsov", "Leonid", ""], ["Braslavski", "Pavel", ""]]}, {"id": "1912.09879", "submitter": "Tian Lan", "authors": "Tian Lan, Xianling Mao, Heyan Huang, Wei Wei", "title": "When to Talk: Chatbot Controls the Timing of Talking during Multi-turn\n  Open-domain Dialogue Generation", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the multi-turn open-domain dialogue systems have attracted more and\nmore attention and made great progress, the existing dialogue systems are still\nvery boring. Nearly all the existing dialogue models only provide a response\nwhen the user's utterance is accepted. But during daily conversations, humans\nalways decide whether to continue to utter an utterance based on the context.\nIntuitively, a dialogue model that can control the timing of talking\nautonomously based on the conversation context can chat with humans more\nnaturally. In this paper, we explore the dialogue system that automatically\ncontrols the timing of talking during the conversation. Specifically, we adopt\nthe decision module for the existing dialogue models. Furthermore, modeling\nconversation context effectively is very important for controlling the timing\nof talking. So we also adopt the graph neural networks to process the context\nwith the natural graph structure. Extensive experiments on two benchmarks show\nthat controlling the timing of talking can effectively improve the quality of\ndialogue generation, and the proposed methods significantly improve the\naccuracy of the timing of talking. In addition, we have publicly released the\ncodes of our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:25:57 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Lan", "Tian", ""], ["Mao", "Xianling", ""], ["Huang", "Heyan", ""], ["Wei", "Wei", ""]]}, {"id": "1912.09913", "submitter": "Minh Nguyen", "authors": "Minh Nguyen, Gia H. Ngo, Nancy F. Chen", "title": "Hierarchical Character Embeddings: Learning Phonological and Semantic\n  Representations in Languages of Logographic Origin using Recursive Neural\n  Networks", "comments": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing. Copyright 2019 IEEE", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 28, pp. 461-473, 2020", "doi": "10.1109/TASLP.2019.2955246", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logographs (Chinese characters) have recursive structures (i.e. hierarchies\nof sub-units in logographs) that contain phonological and semantic information,\nas developmental psychology literature suggests that native speakers leverage\non the structures to learn how to read. Exploiting these structures could\npotentially lead to better embeddings that can benefit many downstream tasks.\nWe propose building hierarchical logograph (character) embeddings from\nlogograph recursive structures using treeLSTM, a recursive neural network.\nUsing recursive neural network imposes a prior on the mapping from logographs\nto embeddings since the network must read in the sub-units in logographs\naccording to the order specified by the recursive structures. Based on human\nbehavior in language learning and reading, we hypothesize that modeling\nlogographs' structures using recursive neural network should be beneficial. To\nverify this claim, we consider two tasks (1) predicting logographs' Cantonese\npronunciation from logographic structures and (2) language modeling. Empirical\nresults show that the proposed hierarchical embeddings outperform baseline\napproaches. Diagnostic analysis suggests that hierarchical embeddings\nconstructed using treeLSTM is less sensitive to distractors, thus is more\nrobust, especially on complex logographs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:24:23 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 00:18:10 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Nguyen", "Minh", ""], ["Ngo", "Gia H.", ""], ["Chen", "Nancy F.", ""]]}, {"id": "1912.10000", "submitter": "Luca Costabello", "authors": "Pedro Tabacof, Luca Costabello", "title": "Probability Calibration for Knowledge Graph Embedding Models", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding research has overlooked the problem of probability\ncalibration. We show popular embedding models are indeed uncalibrated. That\nmeans probability estimates associated to predicted triples are unreliable. We\npresent a novel method to calibrate a model when ground truth negatives are not\navailable, which is the usual case in knowledge graphs. We propose to use Platt\nscaling and isotonic regression alongside our method. Experiments on three\ndatasets with ground truth negatives show our contribution leads to\nwell-calibrated models when compared to the gold standard of using negatives.\nWe get significantly better results than the uncalibrated models from all\ncalibration methods. We show isotonic regression offers the best the\nperformance overall, not without trade-offs. We also show that calibrated\nmodels reach state-of-the-art accuracy without the need to define\nrelation-specific decision thresholds.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:31:33 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 10:38:54 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Tabacof", "Pedro", ""], ["Costabello", "Luca", ""]]}, {"id": "1912.10011", "submitter": "Cl\\'ement Rebuffel", "authors": "Cl\\'ement Rebuffel and Laure Soulier and Geoffrey Scoutheeten and\n  Patrick Gallinari", "title": "A Hierarchical Model for Data-to-Text Generation", "comments": "Accepted at the 42nd European Conference on IR Research, ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcribing structured data into natural language descriptions has emerged\nas a challenging task, referred to as \"data-to-text\". These structures\ngenerally regroup multiple elements, as well as their attributes. Most attempts\nrely on translation encoder-decoder methods which linearize elements into a\nsequence. This however loses most of the structure contained in the data. In\nthis work, we propose to overpass this limitation with a hierarchical model\nthat encodes the data-structure at the element-level and the structure level.\nEvaluations on RotoWire show the effectiveness of our model w.r.t. qualitative\nand quantitative metrics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:41:32 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Rebuffel", "Cl\u00e9ment", ""], ["Soulier", "Laure", ""], ["Scoutheeten", "Geoffrey", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1912.10128", "submitter": "Chengzhu Yu", "authors": "Liqiang Zhang, Chengzhu Yu, Heng Lu, Chao Weng, Yusong Wu, Xiang Xie,\n  Zijin Li, Dong Yu", "title": "Learning Singing From Speech", "comments": "Submitted to ICASSP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm that is capable of synthesizing high quality target\nspeaker's singing voice given only their normal speech samples. The proposed\nalgorithm first integrate speech and singing synthesis into a unified\nframework, and learns universal speaker embeddings that are shareable between\nspeech and singing synthesis tasks. Specifically, the speaker embeddings\nlearned from normal speech via the speech synthesis objective are shared with\nthose learned from singing samples via the singing synthesis objective in the\nunified training framework. This makes the learned speaker embedding a\ntransferable representation for both speaking and singing. We evaluate the\nproposed algorithm on singing voice conversion task where the content of\noriginal singing is covered with the timbre of another speaker's voice learned\npurely from their normal speech samples. Our experiments indicate that the\nproposed algorithm generates high-quality singing voices that sound highly\nsimilar to target speaker's voice given only his or her normal speech samples.\nWe believe that proposed algorithm will open up new opportunities for singing\nsynthesis and conversion for broader users and applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:45:23 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhang", "Liqiang", ""], ["Yu", "Chengzhu", ""], ["Lu", "Heng", ""], ["Weng", "Chao", ""], ["Wu", "Yusong", ""], ["Xie", "Xiang", ""], ["Li", "Zijin", ""], ["Yu", "Dong", ""]]}, {"id": "1912.10130", "submitter": "Eda Okur", "authors": "Saurav Sahay, Shachi H Kumar, Eda Okur, Haroon Syed, Lama Nachman", "title": "Modeling Intent, Dialog Policies and Response Adaptation for\n  Goal-Oriented Interactions", "comments": "Presented as a full-paper at the 23rd Workshop on the Semantics and\n  Pragmatics of Dialogue (SemDial 2019 - LondonLogue), Sep 4-6, 2019, London,\n  UK", "journal-ref": "Proceedings of the 23rd Workshop on the Semantics and Pragmatics\n  of Dialogue (SEMDIAL), pp. 146-155, London, United Kingdom, September 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a machine learning driven spoken dialog system for goal-oriented\ninteractions involves careful design of intents and data collection along with\ndevelopment of intent recognition models and dialog policy learning algorithms.\nThe models should be robust enough to handle various user distractions during\nthe interaction flow and should steer the user back into an engaging\ninteraction for successful completion of the interaction. In this work, we have\ndesigned a goal-oriented interaction system where children can engage with\nagents for a series of interactions involving `Meet \\& Greet' and `Simon Says'\ngame play. We have explored various feature extractors and models for improved\nintent recognition and looked at leveraging previous user and system\ninteractions in novel ways with attention models. We have also looked at dialog\nadaptation methods for entrained response selection. Our bootstrapped models\nfrom limited training data perform better than many baseline approaches we have\nlooked at for intent recognition and dialog action prediction.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:53:18 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Sahay", "Saurav", ""], ["Kumar", "Shachi H", ""], ["Okur", "Eda", ""], ["Syed", "Haroon", ""], ["Nachman", "Lama", ""]]}, {"id": "1912.10131", "submitter": "Eda Okur", "authors": "Shachi H Kumar, Eda Okur, Saurav Sahay, Jonathan Huang, Lama Nachman", "title": "Leveraging Topics and Audio Features with Multimodal Attention for Audio\n  Visual Scene-Aware Dialog", "comments": "Presented at the 3rd Visually Grounded Interaction and Language\n  (ViGIL) Workshop, NeurIPS 2019, Vancouver, Canada. arXiv admin note:\n  substantial text overlap with arXiv:1812.08407, arXiv:1912.10132", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancements in Artificial Intelligence (AI), Intelligent\nVirtual Assistants (IVA) such as Alexa, Google Home, etc., have become a\nubiquitous part of many homes. Currently, such IVAs are mostly audio-based, but\ngoing forward, we are witnessing a confluence of vision, speech and dialog\nsystem technologies that are enabling the IVAs to learn audio-visual groundings\nof utterances. This will enable agents to have conversations with users about\nthe objects, activities and events surrounding them. In this work, we present\nthree main architectural explorations for the Audio Visual Scene-Aware Dialog\n(AVSD): 1) investigating `topics' of the dialog as an important contextual\nfeature for the conversation, 2) exploring several multimodal attention\nmechanisms during response generation, 3) incorporating an end-to-end audio\nclassification ConvNet, AclNet, into our architecture. We discuss detailed\nanalysis of the experimental results and show that our model variations\noutperform the baseline system presented for the AVSD task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:55:40 GMT"}], "update_date": "2019-12-27", "authors_parsed": [["Kumar", "Shachi H", ""], ["Okur", "Eda", ""], ["Sahay", "Saurav", ""], ["Huang", "Jonathan", ""], ["Nachman", "Lama", ""]]}, {"id": "1912.10132", "submitter": "Eda Okur", "authors": "Shachi H Kumar, Eda Okur, Saurav Sahay, Jonathan Huang, Lama Nachman", "title": "Exploring Context, Attention and Audio Features for Audio Visual\n  Scene-Aware Dialog", "comments": "Presented at the Visual Question Answering and Dialog Workshop, CVPR\n  2019, Long Beach, USA. arXiv admin note: substantial text overlap with\n  arXiv:1912.10131", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are witnessing a confluence of vision, speech and dialog system\ntechnologies that are enabling the IVAs to learn audio-visual groundings of\nutterances and have conversations with users about the objects, activities and\nevents surrounding them. Recent progress in visual grounding techniques and\nAudio Understanding are enabling machines to understand shared semantic\nconcepts and listen to the various sensory events in the environment. With\naudio and visual grounding methods, end-to-end multimodal SDS are trained to\nmeaningfully communicate with us in natural language about the real dynamic\naudio-visual sensory world around us. In this work, we explore the role of\n`topics' as the context of the conversation along with multimodal attention\ninto such an end-to-end audio-visual scene-aware dialog system architecture. We\nalso incorporate an end-to-end audio classification ConvNet, AclNet, into our\nmodels. We develop and test our approaches on the Audio Visual Scene-Aware\nDialog (AVSD) dataset released as a part of the DSTC7. We present the analysis\nof our experiments and show that some of our model variations outperform the\nbaseline system released for AVSD.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:56:54 GMT"}], "update_date": "2019-12-27", "authors_parsed": [["Kumar", "Shachi H", ""], ["Okur", "Eda", ""], ["Sahay", "Saurav", ""], ["Huang", "Jonathan", ""], ["Nachman", "Lama", ""]]}, {"id": "1912.10160", "submitter": "Gaurav Kumar", "authors": "Gaurav Kumar, Rishabh Joshi, Jaspreet Singh, Promod Yenigalla", "title": "AMUSED: A Multi-Stream Vector Representation Method for Use in Natural\n  Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of building a coherent and non-monotonous conversational agent\nwith proper discourse and coverage is still an area of open research. Current\narchitectures only take care of semantic and contextual information for a given\nquery and fail to completely account for syntactic and external knowledge which\nare crucial for generating responses in a chit-chat system. To overcome this\nproblem, we propose an end to end multi-stream deep learning architecture which\nlearns unified embeddings for query-response pairs by leveraging contextual\ninformation from memory networks and syntactic information by incorporating\nGraph Convolution Networks (GCN) over their dependency parse. A stream of this\nnetwork also utilizes transfer learning by pre-training a bidirectional\ntransformer to extract semantic representation for each input sentence and\nincorporates external knowledge through the the neighborhood of the entities\nfrom a Knowledge Base (KB). We benchmark these embeddings on next sentence\nprediction task and significantly improve upon the existing techniques.\nFurthermore, we use AMUSED to represent query and responses along with its\ncontext to develop a retrieval based conversational agent which has been\nvalidated by expert linguists to have comprehensive engagement with humans.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:35:03 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kumar", "Gaurav", ""], ["Joshi", "Rishabh", ""], ["Singh", "Jaspreet", ""], ["Yenigalla", "Promod", ""]]}, {"id": "1912.10161", "submitter": "Maria Becker", "authors": "Maria Becker, Katharina Korfhage, Anette Frank", "title": "Implicit Knowledge in Argumentative Texts: An Annotated Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When speaking or writing, people omit information that seems clear and\nevident, such that only part of the message is expressed in words. Especially\nin argumentative texts it is very common that (important) parts of the argument\nare implied and omitted. We hypothesize that for argument analysis it will be\nbeneficial to reconstruct this implied information. As a starting point for\nfilling such knowledge gaps, we build a corpus consisting of high-quality human\nannotations of missing and implied information in argumentative texts. To learn\nmore about the characteristics of both the argumentative texts and the added\ninformation, we further annotate the data with semantic clause types and\ncommonsense knowledge relations. The outcome of our work is a carefully\nde-signed and richly annotated dataset, for which we then provide an in-depth\nanalysis by investigating characteristic distributions and correlations of the\nassigned labels. We reveal interesting patterns and intersections between the\nannotation categories and properties of our dataset, which enable insights into\nthe characteristics of both argumentative texts and implicit knowledge in terms\nof structural features and semantic information. The results of our analysis\ncan help to assist automated argument analysis and can guide the process of\nrevealing implicit information in argumentative texts automatically.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:43:46 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Becker", "Maria", ""], ["Korfhage", "Katharina", ""], ["Frank", "Anette", ""]]}, {"id": "1912.10162", "submitter": "Stavros Vologiannidis", "authors": "Eleni Partalidou, Eleftherios Spyromitros-Xioufis, Stavros Doropoulos,\n  Stavros Vologiannidis, Konstantinos I. Diamantaras", "title": "Design and implementation of an open source Greek POS Tagger and Entity\n  Recognizer using spaCy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a machine learning approach to part-of-speech tagging and\nnamed entity recognition for Greek, focusing on the extraction of morphological\nfeatures and classification of tokens into a small set of classes for named\nentities. The architecture model that was used is introduced. The greek version\nof the spaCy platform was added into the source code, a feature that did not\nexist before our contribution, and was used for building the models.\nAdditionally, a part of speech tagger was trained that can detect the\nmorphology of the tokens and performs higher than the state-of-the-art results\nwhen classifying only the part of speech. For named entity recognition using\nspaCy, a model that extends the standard ENAMEX type (organization, location,\nperson) was built. Certain experiments that were conducted indicate the need\nfor flexibility in out-of-vocabulary words and there is an effort for resolving\nthis issue. Finally, the evaluation results are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 13:29:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Partalidou", "Eleni", ""], ["Spyromitros-Xioufis", "Eleftherios", ""], ["Doropoulos", "Stavros", ""], ["Vologiannidis", "Stavros", ""], ["Diamantaras", "Konstantinos I.", ""]]}, {"id": "1912.10163", "submitter": "Makoto Nakatsuji Ph. D.", "authors": "Makoto Nakatsuji", "title": "Can AI Generate Love Advice?: Toward Neural Answer Generation for\n  Non-Factoid Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods that extract answers for non-factoid questions from QA\nsites are seen as critical since they can assist users in reaching their next\ndecisions through conversations with AI systems. The current methods, however,\nhave the following two problems: (1) They can not understand the ambiguous use\nof words in the questions as word usage can strongly depend on the context. As\na result, the accuracies of their answer selections are not good enough. (2)\nThe current methods can only select from among the answers held by QA sites and\ncan not generate new ones. Thus, they can not answer the questions that are\nsomewhat different with those stored in QA sites. Our solution, Neural Answer\nConstruction Model, tackles these problems as it: (1) Incorporates the biases\nof semantics behind questions into word embeddings while also computing them\nregardless of the semantics. As a result, it can extract answers that suit the\ncontexts of words used in the question as well as following the common usage of\nwords across semantics. This improves the accuracy of answer selection. (2)\nUses biLSTM to compute the embeddings of questions as well as those of the\nsentences often used to form answers. It then simultaneously learns the optimum\ncombination of those sentences as well as the closeness between the question\nand those sentences. As a result, our model can construct an answer that\ncorresponds to the situation that underlies the question; it fills the gap\nbetween answer selection and generation and is the first model to move beyond\nthe current simple answer selection model for non-factoid QAs. Evaluations\nusing datasets created for love advice stored in the Japanese QA site, Oshiete\ngoo, indicate that our model achieves 20% higher accuracy in answer creation\nthan the strong baselines. Our model is practical and has already been applied\nto the love advice service in Oshiete goo.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:57:22 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Nakatsuji", "Makoto", ""]]}, {"id": "1912.10164", "submitter": "Markus J. Hofmann", "authors": "Markus J. Hofmann, Mareike A. Kleemann, Andre Roelke, Christian\n  Vorstius, and Ralph Radach", "title": "Decomposing predictability: Semantic feature overlap between words and\n  the dynamics of reading for meaning", "comments": "Journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study uses a computational approach to examine the role of\nsemantic constraints in normal reading. This methodology avoids confounds\ninherent in conventional measures of predictability, allowing for theoretically\ndeeper accounts of semantic processing. We start from a definition of\nassociations between words based on the significant log likelihood that two\nwords co-occur frequently together in the sentences of a large text corpus.\nDirect associations between stimulus words were controlled, and semantic\nfeature overlap between prime and target words was manipulated by their common\nassociates. The stimuli consisted of sentences of the form pronoun, verb,\narticle, adjective and noun, followed by a series of closed class words, e. g.\n\"She rides the grey elephant on one of her many exploratory voyages\". The\nresults showed that verb-noun overlap reduces single and first fixation\ndurations of the target noun and adjective-noun overlap reduces go-past\ndurations. A dynamic spreading of activation account suggests that associates\nof the prime words take some time to become activated: The verb can act on the\ntarget noun's early eye-movement measures presented three words later, while\nthe adjective is presented immediately prior to the target, which induces\nsentence re-examination after a difficult adjective-noun semantic integration.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:36:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hofmann", "Markus J.", ""], ["Kleemann", "Mareike A.", ""], ["Roelke", "Andre", ""], ["Vorstius", "Christian", ""], ["Radach", "Ralph", ""]]}, {"id": "1912.10165", "submitter": "Raul Puri", "authors": "Raul Puri and Bryan Catanzaro", "title": "Zero-shot Text Classification With Generative Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the use of natural language to enable zero-shot model\nadaptation to new tasks. We use text and metadata from social commenting\nplatforms as a source for a simple pretraining task. We then provide the\nlanguage model with natural language descriptions of classification tasks as\ninput and train it to generate the correct answer in natural language via a\nlanguage modeling objective. This allows the model to generalize to new\nclassification tasks without the need for multiple multitask classification\nheads. We show the zero-shot performance of these generative language models,\ntrained with weak supervision, on six benchmark text classification datasets\nfrom the torchtext library. Despite no access to training data, we achieve up\nto a 45% absolute improvement in classification accuracy over random or\nmajority class baselines. These results show that natural language can serve as\nsimple and powerful descriptors for task adaptation. We believe this points the\nway to new metalearning strategies for text problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:01:34 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Puri", "Raul", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1912.10166", "submitter": "Zeljko Kraljevic", "authors": "Zeljko Kraljevic, Daniel Bean, Aurelie Mascio, Lukasz Roguski, Amos\n  Folarin, Angus Roberts, Rebecca Bendayan, Richard Dobson", "title": "MedCAT -- Medical Concept Annotation Tool", "comments": "Preprint, 25 pages, 5 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical documents such as Electronic Health Records (EHRs) contain a large\namount of information in an unstructured format. The data in EHRs is a hugely\nvaluable resource documenting clinical narratives and decisions, but whilst the\ntext can be easily understood by human doctors it is challenging to use in\nresearch and clinical applications. To uncover the potential of biomedical\ndocuments we need to extract and structure the information they contain. The\ntask at hand is Named Entity Recognition and Linking (NER+L). The number of\nentities, ambiguity of words, overlapping and nesting make the biomedical area\nsignificantly more difficult than many others. To overcome these difficulties,\nwe have developed the Medical Concept Annotation Tool (MedCAT), an open-source\nunsupervised approach to NER+L. MedCAT uses unsupervised machine learning to\ndisambiguate entities. It was validated on MIMIC-III (a freely accessible\ncritical care database) and MedMentions (Biomedical papers annotated with\nmentions from the Unified Medical Language System). In case of NER+L, the\ncomparison with existing tools shows that MedCAT improves the previous best\nwith only unsupervised learning (F1=0.848 vs 0.691 for disease detection;\nF1=0.710 vs. 0.222 for general concept detection). A qualitative analysis of\nthe vector embeddings learnt by MedCAT shows that it captures latent medical\nknowledge available in EHRs (MIMIC-III). Unsupervised learning can improve the\nperformance of large scale entity extraction, but it has some limitations when\nworking with only a couple of entities and a small dataset. In that case\noptions are supervised learning or active learning, both of which are supported\nin MedCAT via the MedCATtrainer extension. Our approach can detect and link\nmillions of different biomedical concepts with state-of-the-art performance,\nwhilst being lightweight, fast and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:42:31 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kraljevic", "Zeljko", ""], ["Bean", "Daniel", ""], ["Mascio", "Aurelie", ""], ["Roguski", "Lukasz", ""], ["Folarin", "Amos", ""], ["Roberts", "Angus", ""], ["Bendayan", "Rebecca", ""], ["Dobson", "Richard", ""]]}, {"id": "1912.10167", "submitter": "Marco Berlot", "authors": "Marco Berlot, Evan Kaplan", "title": "Machine Translation with Cross-lingual Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Learning word embeddings using distributional information is a task that has\nbeen studied by many researchers, and a lot of studies are reported in the\nliterature. On the contrary, less studies were done for the case of multiple\nlanguages. The idea is to focus on a single representation for a pair of\nlanguages such that semantically similar words are closer to one another in the\ninduced representation irrespective of the language. In this way, when data are\nmissing for a particular language, classifiers from another language can be\nused.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:50:35 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 13:50:37 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Berlot", "Marco", ""], ["Kaplan", "Evan", ""]]}, {"id": "1912.10168", "submitter": "Blaine Cole", "authors": "Blaine Cole", "title": "Two Way Adversarial Unsupervised Word Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word translation is a problem in machine translation that seeks to build\nmodels that recover word level correspondence between languages. Recent\napproaches to this problem have shown that word translation models can learned\nwith very small seeding dictionaries, and even without any starting\nsupervision. In this paper we propose a method to jointly find translations\nbetween a pair of languages. Not only does our method learn translations in\nboth directions but it improves accuracy of those translations over past\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 21:21:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Cole", "Blaine", ""]]}, {"id": "1912.10169", "submitter": "Niels Van Der Heijden", "authors": "Niels van der Heijden, Samira Abnar and Ekaterina Shutova", "title": "A Comparison of Architectures and Pretraining Methods for Contextualized\n  Multilingual Word Embeddings", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of annotated data in many languages is a well-known challenge within\nthe field of multilingual natural language processing (NLP). Therefore, many\nrecent studies focus on zero-shot transfer learning and joint training across\nlanguages to overcome data scarcity for low-resource languages. In this work we\n(i) perform a comprehensive comparison of state-ofthe-art multilingual word and\nsentence encoders on the tasks of named entity recognition (NER) and part of\nspeech (POS) tagging; and (ii) propose a new method for creating multilingual\ncontextualized word embeddings, compare it to multiple baselines and show that\nit performs at or above state-of-theart level in zero-shot transfer settings.\nFinally, we show that our method allows for better knowledge sharing across\nlanguages in a joint training setting.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 11:42:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["van der Heijden", "Niels", ""], ["Abnar", "Samira", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "1912.10170", "submitter": "Joeran Beel", "authors": "Dominika Tkaczyk, Andrew Collins, Joeran Beel", "title": "Na\\\"iveRole: Author-Contribution Extraction and Parsing from Biomedical\n  Manuscripts", "comments": "arXiv admin note: substantial text overlap with arXiv:1802.01174", "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the contributions of individual authors to scientific\npublications is important for assessing authors' achievements. Some biomedical\npublications have a short section that describes authors' roles and\ncontributions. It is usually written in natural language and hence author\ncontributions cannot be trivially extracted in a machine-readable format. In\nthis paper, we present 1) A statistical analysis of roles in author\ncontributions sections, and 2) Na\\\"iveRole, a novel approach to extract\nstructured authors' roles from author contribution sections. For the first\npart, we used co-clustering techniques, as well as Open Information Extraction,\nto semi-automatically discover the popular roles within a corpus of 2,000\ncontributions sections from PubMed Central. The discovered roles were used to\nautomatically build a training set for Na\\\"iveRole, our role extractor\napproach, based on Na\\\"ive Bayes. Na\\\"iveRole extracts roles with a\nmicro-averaged precision of 0.68, recall of 0.48 and F1 of 0.57. It is, to the\nbest of our knowledge, the first attempt to automatically extract author roles\nfrom research papers. This paper is an extended version of a previous poster\npublished at JCDL 2018.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 14:37:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.10204", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Carolyn Penstein Rose", "title": "A Machine Learning Framework for Authorship Identification From Texts", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship identification is a process in which the author of a text is\nidentified. Most known literary texts can easily be attributed to a certain\nauthor because they are, for example, signed. Yet sometimes we find unfinished\npieces of work or a whole bunch of manuscripts with a wide variety of possible\nauthors. In order to assess the importance of such a manuscript, it is vital to\nknow who wrote it. In this work, we aim to develop a machine learning framework\nto effectively determine authorship. We formulate the task as a single-label\nmulti-class text categorization problem and propose a supervised machine\nlearning framework incorporating stylometric features. This task is highly\ninterdisciplinary in that it takes advantage of machine learning, information\nretrieval, and natural language processing. We present an approach and a model\nwhich learns the differences in writing style between $50$ different authors\nand is able to predict the author of a new text with high accuracy. The\naccuracy is seen to increase significantly after introducing certain linguistic\nstylometric features along with text features.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:47:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Rose", "Carolyn Penstein", ""]]}, {"id": "1912.10306", "submitter": "Xiong Liu", "authors": "Xiong Liu, Yu Chen, Jay Bae, Hu Li, Joseph Johnston, Todd Sanger", "title": "Predicting Heart Failure Readmission from Clinical Notes Using Deep\n  Learning", "comments": "IEEE BIBM 2019", "journal-ref": "2019 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart failure hospitalization is a severe burden on healthcare. How to\npredict and therefore prevent readmission has been a significant challenge in\noutcomes research. To address this, we propose a deep learning approach to\npredict readmission from clinical notes. Unlike conventional methods that use\nstructured data for prediction, we leverage the unstructured clinical notes to\ntrain deep learning models based on convolutional neural networks (CNN). We\nthen use the trained models to classify and predict potentially high-risk\nadmissions/patients. For evaluation, we trained CNNs using the discharge\nsummary notes in the MIMIC III database. We also trained regular machine\nlearning models based on random forest using the same datasets. The result\nshows that deep learning models outperform the regular models in prediction\ntasks. CNN method achieves a F1 score of 0.756 in general readmission\nprediction and 0.733 in 30-day readmission prediction, while random forest only\nachieves a F1 score of 0.674 and 0.656 respectively. We also propose a\nchi-square test based method to interpret key features associated with deep\nlearning predicted readmissions. It reveals clinical insights about readmission\nembedded in the clinical notes. Collectively, our method can make the human\nevaluation process more efficient and potentially facilitate the reduction of\nreadmission rates.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 17:49:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liu", "Xiong", ""], ["Chen", "Yu", ""], ["Bae", "Jay", ""], ["Li", "Hu", ""], ["Johnston", "Joseph", ""], ["Sanger", "Todd", ""]]}, {"id": "1912.10308", "submitter": "Lei Kang", "authors": "Lei Kang, Pau Riba, Mauricio Villegas, Alicia Forn\\'es, Mar\\c{c}al\n  Rusi\\~nol", "title": "Candidate Fusion: Integrating Language Modelling into a\n  Sequence-to-Sequence Handwritten Word Recognition Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence models have recently become very popular for tackling\nhandwritten word recognition problems. However, how to effectively integrate an\nexternal language model into such recognizer is still a challenging problem.\nThe main challenge faced when training a language model is to deal with the\nlanguage model corpus which is usually different to the one used for training\nthe handwritten word recognition system. Thus, the bias between both word\ncorpora leads to incorrectness on the transcriptions, providing similar or even\nworse performances on the recognition task. In this work, we introduce\nCandidate Fusion, a novel way to integrate an external language model to a\nsequence-to-sequence architecture. Moreover, it provides suggestions from an\nexternal language knowledge, as a new input to the sequence-to-sequence\nrecognizer. Hence, Candidate Fusion provides two improvements. On the one hand,\nthe sequence-to-sequence recognizer has the flexibility not only to combine the\ninformation from itself and the language model, but also to choose the\nimportance of the information provided by the language model. On the other\nhand, the external language model has the ability to adapt itself to the\ntraining corpus and even learn the most commonly errors produced from the\nrecognizer. Finally, by conducting comprehensive experiments, the Candidate\nFusion proves to outperform the state-of-the-art language models for\nhandwritten word recognition tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 18:14:32 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kang", "Lei", ""], ["Riba", "Pau", ""], ["Villegas", "Mauricio", ""], ["Forn\u00e9s", "Alicia", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""]]}, {"id": "1912.10337", "submitter": "Mingyuan Zhou", "authors": "Dandan Guo, Bo Chen, Ruiying Lu, Mingyuan Zhou", "title": "Recurrent Hierarchical Topic-Guided RNN for Language Generation", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To simultaneously capture syntax and global semantics from a text corpus, we\npropose a new larger-context recurrent neural network (RNN) based language\nmodel, which extracts recurrent hierarchical semantic structure via a dynamic\ndeep topic model to guide natural language generation. Moving beyond a\nconventional RNN-based language model that ignores long-range word dependencies\nand sentence order, the proposed model captures not only intra-sentence word\ndependencies, but also temporal transitions between sentences and\ninter-sentence topic dependencies. For inference, we develop a hybrid of\nstochastic-gradient Markov chain Monte Carlo and recurrent autoencoding\nvariational Bayes. Experimental results on a variety of real-world text corpora\ndemonstrate that the proposed model not only outperforms larger-context\nRNN-based language models, but also learns interpretable recurrent multilayer\ntopics and generates diverse sentences and paragraphs that are syntactically\ncorrect and semantically coherent.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 21:11:35 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 22:22:58 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Guo", "Dandan", ""], ["Chen", "Bo", ""], ["Lu", "Ruiying", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1912.10375", "submitter": "Boxin Wang", "authors": "Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, Bo Li", "title": "T3: Tree-Autoencoder Constrained Adversarial Text Generation for\n  Targeted Attack", "comments": "Accepted to EMNLP 2020 as a long paper. 17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks against natural language processing systems, which\nperform seemingly innocuous modifications to inputs, can induce arbitrary\nmistakes to the target models. Though raised great concerns, such adversarial\nattacks can be leveraged to estimate the robustness of NLP models. Compared\nwith the adversarial example generation in continuous data domain (e.g.,\nimage), generating adversarial text that preserves the original meaning is\nchallenging since the text space is discrete and non-differentiable. To handle\nthese challenges, we propose a target-controllable adversarial attack framework\nT3, which is applicable to a range of NLP tasks. In particular, we propose a\ntree-based autoencoder to embed the discrete text data into a continuous\nrepresentation space, upon which we optimize the adversarial perturbation. A\nnovel tree-based decoder is then applied to regularize the syntactic\ncorrectness of the generated text and manipulate it on either sentence\n(T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP\ntasks: sentiment analysis and question answering (QA). Extensive experimental\nresults and human studies show that T3 generated adversarial texts can\nsuccessfully manipulate the NLP models to output the targeted incorrect answer\nwithout misleading the human. Moreover, we show that the generated adversarial\ntexts have high transferability which enables the black-box attacks in\npractice. Our work sheds light on an effective and general way to examine the\nrobustness of NLP models. Our code is publicly available at\nhttps://github.com/AI-secure/T3/.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 03:02:42 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:29:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Boxin", ""], ["Pei", "Hengzhi", ""], ["Pan", "Boyuan", ""], ["Chen", "Qian", ""], ["Wang", "Shuohang", ""], ["Li", "Bo", ""]]}, {"id": "1912.10434", "submitter": "Andreas Hanselowski Dr.", "authors": "Andreas Hanselowski, Iryna Gurevych", "title": "Analyzing Structures in the Semantic Vector Space: A Framework for\n  Decomposing Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are rich word representations, which in combination with deep\nneural networks, lead to large performance gains for many NLP tasks. However,\nword embeddings are represented by dense, real-valued vectors and they are\ntherefore not directly interpretable. Thus, computational operations based on\nthem are also not well understood. In this paper, we present an approach for\nanalyzing structures in the semantic vector space to get a better understanding\nof the underlying semantic encoding principles. We present a framework for\ndecomposing word embeddings into smaller meaningful units which we call\nsub-vectors. The framework opens up a wide range of possibilities analyzing\nphenomena in vector space semantics, as well as solving concrete NLP problems:\nWe introduce the category completion task and show that a sub-vector based\napproach is superior to supervised techniques; We present a sub-vector based\nmethod for solving the word analogy task, which substantially outperforms\ndifferent variants of the traditional vector-offset method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 09:01:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Hanselowski", "Andreas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1912.10435", "submitter": "Ankit Chadha Mr.", "authors": "Ankit Chadha and Rewa Sood", "title": "BERTQA -- Attention on Steroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we extend the Bidirectional Encoder Representations from\nTransformers (BERT) with an emphasis on directed coattention to obtain an\nimproved F1 performance on the SQUAD2.0 dataset. The Transformer architecture\non which BERT is based places hierarchical global attention on the\nconcatenation of the context and query. Our additions to the BERT architecture\naugment this attention with a more focused context to query (C2Q) and query to\ncontext (Q2C) attention via a set of modified Transformer encoder units. In\naddition, we explore adding convolution-based feature extraction within the\ncoattention architecture to add localized information to self-attention. We\nfound that coattention significantly improves the no answer F1 by 4 points in\nthe base and 1 point in the large architecture. After adding skip connections\nthe no answer F1 improved further without causing an additional loss in has\nanswer F1. The addition of localized feature extraction added to attention\nproduced an overall dev F1 of 77.03 in the base architecture. We applied our\nfindings to the large BERT model which contains twice as many layers and\nfurther used our own augmented version of the SQUAD 2.0 dataset created by back\ntranslation, which we have named SQUAD 2.Q. Finally, we performed\nhyperparameter tuning and ensembled our best models for a final F1/EM of\n82.317/79.442 (Attention on Steroids, PCE Test Leaderboard).\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 06:44:12 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chadha", "Ankit", ""], ["Sood", "Rewa", ""]]}, {"id": "1912.10458", "submitter": "Haresh Rengaraj Rajamohan", "authors": "Kannan Venkataramanan and Haresh Rengaraj Rajamohan", "title": "Emotion Recognition from Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we conduct an extensive comparison of various approaches to\nspeech based emotion recognition systems. The analyses were carried out on\naudio recordings from Ryerson Audio-Visual Database of Emotional Speech and\nSong (RAVDESS). After pre-processing the raw audio files, features such as\nLog-Mel Spectrogram, Mel-Frequency Cepstral Coefficients (MFCCs), pitch and\nenergy were considered. The significance of these features for emotion\nclassification was compared by applying methods such as Long Short Term Memory\n(LSTM), Convolutional Neural Networks (CNNs), Hidden Markov Models (HMMs) and\nDeep Neural Networks (DNNs). On the 14-class (2 genders x 7 emotions)\nclassification task, an accuracy of 68% was achieved with a 4-layer 2\ndimensional CNN using the Log-Mel Spectrogram features. We also observe that,\nin emotion recognition, the choice of audio features impacts the results much\nmore than the model complexity.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 14:43:14 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Venkataramanan", "Kannan", ""], ["Rajamohan", "Haresh Rengaraj", ""]]}, {"id": "1912.10514", "submitter": "Idris Abdulmumin", "authors": "Idris Abdulmumin, Bashir Shehu Galadanci and Aliyu Garba", "title": "Tag-less Back-Translation", "comments": "29 pages, 4 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An effective method to generate a large number of parallel sentences for\ntraining improved neural machine translation (NMT) systems is the use of the\nback-translations of the target-side monolingual data. The standard\nback-translation method has been shown to be unable to efficiently utilize the\navailable huge amount of existing monolingual data because of the inability of\ntranslation models to differentiate between the authentic and synthetic\nparallel data during training. Tagging, or using gates, has been used to enable\ntranslation models to distinguish between synthetic and authentic data,\nimproving standard back-translation and also enabling the use of iterative\nback-translation on language pairs that underperformed using standard\nback-translation. In this work, we approach back-translation as a domain\nadaptation problem, eliminating the need for explicit tagging. In the approach\n-- \\emph{tag-less back-translation} -- the synthetic and authentic parallel\ndata are treated as out-of-domain and in-domain data respectively and, through\npre-training and fine-tuning, the translation model is shown to be able to\nlearn more efficiently from them during training. Experimental results have\nshown that the approach outperforms the standard and tagged back-translation\napproaches on low resource English-Vietnamese and English-German neural machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 19:20:10 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 09:07:53 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 15:53:24 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Abdulmumin", "Idris", ""], ["Galadanci", "Bashir Shehu", ""], ["Garba", "Aliyu", ""]]}, {"id": "1912.10546", "submitter": "Jincheng Sun", "authors": "T. Chen, J. Sun, H. Lin, Y. Liu", "title": "Hybrid Machine Learning Models of Classifying Residential Requests for\n  Smart Dispatching", "comments": "22 pages, Cyberspace Data and Intelligence, and Cyber-Living,\n  Syndrome, and Health. Springer, Singapore, 2019. 357-378", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hybrid machine learning method of classifying\nresidential requests in natural language to responsible departments that\nprovide timely responses back to residents under the vision of digital\ngovernment services in smart cities. Residential requests in natural language\ndescriptions cover almost every aspect of a city's daily operation. Hence the\nresponsible departments are fine-grained to even the level of local\ncommunities. There are no specific general categories or labels for each\nrequest sample. This causes two issues for supervised classification solutions,\nnamely (1) the request sample data is unbalanced and (2) lack of specific\nlabels for training. To solve these issues, we investigate a hybrid machine\nlearning method that generates meta-class labels by means of unsupervised\nclustering algorithms; applies two-word embedding methods with three\nclassifiers (including two hierarchical classifiers and one residual\nconvolutional neural network); and selects the best performing classifier as\nthe classification result. We demonstrate our approach performing better\nclassification tasks compared to two benchmarking machine learning models,\nNaive Bayes classifier and a Multiple Layer Perceptron (MLP). In addition, the\nhierarchical classification method provides insights into the source of\nclassification errors.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 21:47:05 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chen", "T.", ""], ["Sun", "J.", ""], ["Lin", "H.", ""], ["Liu", "Y.", ""]]}, {"id": "1912.10554", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\\'es R\\'issola,\n  Fabio Crestani", "title": "Harnessing Evolution of Multi-Turn Conversations for Effective Answer\n  Retrieval", "comments": "To appear in ACM CHIIR 2020, Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3343413.3377968", "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the improvements in speech recognition and voice generation technologies\nover the last years, a lot of companies have sought to develop conversation\nunderstanding systems that run on mobile phones or smart home devices through\nnatural language interfaces. Conversational assistants, such as Google\nAssistant and Microsoft Cortana, can help users to complete various types of\ntasks. This requires an accurate understanding of the user's information need\nas the conversation evolves into multiple turns. Finding relevant context in a\nconversation's history is challenging because of the complexity of natural\nlanguage and the evolution of a user's information need. In this work, we\npresent an extensive analysis of language, relevance, dependency of user\nutterances in a multi-turn information-seeking conversation. To this aim, we\nhave annotated relevant utterances in the conversations released by the TREC\nCaST 2019 track. The annotation labels determine which of the previous\nutterances in a conversation can be used to improve the current one.\nFurthermore, we propose a neural utterance relevance model based on BERT\nfine-tuning, outperforming competitive baselines. We study and compare the\nperformance of multiple retrieval models, utilizing different strategies to\nincorporate the user's context. The experimental results on both classification\nand retrieval tasks show that our proposed approach can effectively identify\nand incorporate the conversation context. We show that processing the current\nutterance using the predicted relevant utterance leads to a 38% relative\nimprovement in terms of nDCG@20. Finally, to foster research in this area, we\nhave released the dataset of the annotations.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:39:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 15:41:03 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Chakraborty", "Manajit", ""], ["R\u00edssola", "Esteban Andr\u00e9s", ""], ["Crestani", "Fabio", ""]]}, {"id": "1912.10590", "submitter": "Chengkun Lang", "authors": "Huiwei Zhou, Chengkun Lang, Zhuang Liu, Shixian Ning, Yingyu Lin and\n  Lei Du", "title": "Knowledge-guided Convolutional Networks for Chemical-Disease Relation\n  Extraction", "comments": "Published on BMC Bioinformatics, 16 pages, 5 figures", "journal-ref": "BMC Bioinformatics, 2019, 20(1):260", "doi": "10.1186/s12859-019-2873-7", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Automatic extraction of chemical-disease relations (CDR) from\nunstructured text is of essential importance for disease treatment and drug\ndevelopment. Meanwhile, biomedical experts have built many highly-structured\nknowledge bases (KBs), which contain prior knowledge about chemicals and\ndiseases. Prior knowledge provides strong support for CDR extraction. How to\nmake full use of it is worth studying. Results: This paper proposes a novel\nmodel called \"Knowledge-guided Convolutional Networks (KCN)\" to leverage prior\nknowledge for CDR extraction. The proposed model first learns knowledge\nrepresentations including entity embeddings and relation embeddings from KBs.\nThen, entity embeddings are used to control the propagation of context features\ntowards a chemical-disease pair with gated convolutions. After that, relation\nembeddings are employed to further capture the weighted context features by a\nshared attention pooling. Finally, the weighted context features containing\nadditional knowledge information are used for CDR extraction. Experiments on\nthe BioCreative V CDR dataset show that the proposed KCN achieves 71.28%\nF1-score, which outperforms most of the state-of-the-art systems. Conclusions:\nThis paper proposes a novel CDR extraction model KCN to make full use of prior\nknowledge. Experimental results demonstrate that KCN could effectively\nintegrate prior knowledge and contexts for the performance improvement.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 02:28:10 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhou", "Huiwei", ""], ["Lang", "Chengkun", ""], ["Liu", "Zhuang", ""], ["Ning", "Shixian", ""], ["Lin", "Yingyu", ""], ["Du", "Lei", ""]]}, {"id": "1912.10604", "submitter": "Chengkun Lang", "authors": "Huiwei Zhou, Yunlong Yang, Shixian Ning, Zhuang Liu, Chengkun Lang,\n  Yingyu Lin, Degen Huang", "title": "Combining Context and Knowledge Representations for Chemical-Disease\n  Relation Extraction", "comments": "Published on IEEE/ACM Transactions on Computational Biology and\n  Bioinformatics, 11 pages, 5 figures", "journal-ref": "IEEE/ACM TCBB,2018,16(6):1879-1889", "doi": "10.1109/TCBB.2018.2838661", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically extracting the relationships between chemicals and diseases is\nsignificantly important to various areas of biomedical research and health\ncare. Biomedical experts have built many large-scale knowledge bases (KBs) to\nadvance the development of biomedical research. KBs contain huge amounts of\nstructured information about entities and relationships, therefore plays a\npivotal role in chemical-disease relation (CDR) extraction. However, previous\nresearches pay less attention to the prior knowledge existing in KBs. This\npaper proposes a neural network-based attention model (NAM) for CDR extraction,\nwhich makes full use of context information in documents and prior knowledge in\nKBs. For a pair of entities in a document, an attention mechanism is employed\nto select important context words with respect to the relation representations\nlearned from KBs. Experiments on the BioCreative V CDR dataset show that\ncombining context and knowledge representations through the attention\nmechanism, could significantly improve the CDR extraction performance while\nachieve comparable results with state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:34:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhou", "Huiwei", ""], ["Yang", "Yunlong", ""], ["Ning", "Shixian", ""], ["Liu", "Zhuang", ""], ["Lang", "Chengkun", ""], ["Lin", "Yingyu", ""], ["Huang", "Degen", ""]]}, {"id": "1912.10616", "submitter": "Mark Dras", "authors": "Chakaveh Saedi and Mark Dras", "title": "Siamese Networks for Large-Scale Author Identification", "comments": "28 pages. Accepted by Computer Speech and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution is the process of identifying the author of a text.\nApproaches to tackling it have been conventionally divided into\nclassification-based ones, which work well for small numbers of candidate\nauthors, and similarity-based methods, which are applicable for larger numbers\nof authors or for authors beyond the training set; these existing\nsimilarity-based methods have only embodied static notions of similarity. Deep\nlearning methods, which blur the boundaries between classification-based and\nsimilarity-based approaches, are promising in terms of ability to learn a\nnotion of similarity, but have previously only been used in a conventional\nsmall-closed-class classification setup.\n  Siamese networks have been used to develop learned notions of similarity in\none-shot image tasks, and also for tasks of mostly semantic relatedness in NLP.\nWe examine their application to the stylistic task of authorship attribution on\ndatasets with large numbers of authors, looking at multiple energy functions\nand neural network architectures, and show that they can substantially\noutperform previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 04:38:00 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 06:48:40 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 08:23:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Saedi", "Chakaveh", ""], ["Dras", "Mark", ""]]}, {"id": "1912.10729", "submitter": "Yujing Wang", "authors": "Yujing Wang, Yaming Yang, Yiren Chen, Jing Bai, Ce Zhang, Guinan Su,\n  Xiaoyu Kou, Yunhai Tong, Mao Yang, Lidong Zhou", "title": "TextNAS: A Neural Architecture Search Space tailored for Text\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning text representation is crucial for text classification and other\nlanguage related tasks. There are a diverse set of text representation networks\nin the literature, and how to find the optimal one is a non-trivial problem.\nRecently, the emerging Neural Architecture Search (NAS) techniques have\ndemonstrated good potential to solve the problem. Nevertheless, most of the\nexisting works of NAS focus on the search algorithms and pay little attention\nto the search space. In this paper, we argue that the search space is also an\nimportant human prior to the success of NAS in different applications. Thus, we\npropose a novel search space tailored for text representation. Through\nautomatic search, the discovered network architecture outperforms\nstate-of-the-art models on various public datasets on text classification and\nnatural language inference tasks. Furthermore, some of the design principles\nfound in the automatic network agree well with human intuition.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:51:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wang", "Yujing", ""], ["Yang", "Yaming", ""], ["Chen", "Yiren", ""], ["Bai", "Jing", ""], ["Zhang", "Ce", ""], ["Su", "Guinan", ""], ["Kou", "Xiaoyu", ""], ["Tong", "Yunhai", ""], ["Yang", "Mao", ""], ["Zhou", "Lidong", ""]]}, {"id": "1912.10785", "submitter": "Hao Feng", "authors": "Chi Xu, Hao Feng, Guoxin Yu, Min Yang, Xiting Wang, Xiang Ao", "title": "Discovering Protagonist of Sentiment with Aspect Reconstructed Capsule\n  Network", "comments": "7pages, 3figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent existing aspect-term level sentiment analysis (ATSA) approaches\ncombined various neural network models with delicately carved attention\nmechanisms built upon given aspect and context to generate refined sentence\nrepresentations for better predictions. In these methods, aspect terms are\nalways provided in both training and testing process which may degrade\naspect-level analysis into sentence-level prediction. However, the annotated\naspect term might be unavailable in real-world scenarios which may challenge\nthe applicability of the existing methods. In this paper, we aim to improve\nATSA by discovering the potential aspect terms of the predicted sentiment\npolarity when the aspect terms of a test sentence are unknown. We access this\ngoal by proposing a capsule network based model named CAPSAR. In CAPSAR,\nsentiment categories are denoted by capsules and aspect term information is\ninjected into sentiment capsules through a sentiment-aspect reconstruction\nprocedure during the training. As a result, coherent patterns between aspects\nand sentimental expressions are encapsulated by these sentiment capsules.\nExperiments on three widely used benchmarks demonstrate these patterns have\npotential in exploring aspect terms from test sentence when only feeding the\nsentence to the model. Meanwhile, the proposed CAPSAR can clearly outperform\nSOTA methods in standard ATSA tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 13:14:40 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 01:47:29 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xu", "Chi", ""], ["Feng", "Hao", ""], ["Yu", "Guoxin", ""], ["Yang", "Min", ""], ["Wang", "Xiting", ""], ["Ao", "Xiang", ""]]}, {"id": "1912.10806", "submitter": "Yinchuan Li", "authors": "Xinyi Li, Yinchuan Li, Hongyang Yang, Liuqing Yang, Xiao-Yang Liu", "title": "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using\n  Financial News", "comments": "arXiv admin note: text overlap with arXiv:1908.01112", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock price prediction is important for value investments in the stock\nmarket. In particular, short-term prediction that exploits financial news\narticles is promising in recent years. In this paper, we propose a novel deep\nneural network DP-LSTM for stock price prediction, which incorporates the news\narticles as hidden information and integrates difference news sources through\nthe differential privacy mechanism. First, based on the autoregressive moving\naverage model (ARMA), a sentiment-ARMA is formulated by taking into\nconsideration the information of financial news articles in the model. Then, an\nLSTM-based deep neural network is designed, which consists of three components:\nLSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM\nscheme can reduce prediction errors and increase the robustness. Extensive\nexperiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32%\nimprovement in mean MPA of prediction result, and (ii) for the prediction of\nthe market index S&P 500, we achieve up to 65.79% improvement in MSE.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 02:52:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Li", "Xinyi", ""], ["Li", "Yinchuan", ""], ["Yang", "Hongyang", ""], ["Yang", "Liuqing", ""], ["Liu", "Xiao-Yang", ""]]}, {"id": "1912.10809", "submitter": "Christian Otto", "authors": "Hang Zhou, Christian Otto, Ralph Ewerth", "title": "Visual Summarization of Scholarly Videos using Word Embeddings and\n  Keyphrase Extraction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-3-030-30760-8_28", "report-no": null, "categories": "cs.MM cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning with audiovisual content depends on many factors. Besides\nthe quality of the learning resource's content, it is essential to discover the\nmost relevant and suitable video in order to support the learning process most\neffectively. Video summarization techniques facilitate this goal by providing a\nquick overview over the content. It is especially useful for longer recordings\nsuch as conference presentations or lectures. In this paper, we present an\napproach that generates a visual summary of video content based on semantic\nword embeddings and keyphrase extraction. For this purpose, we exploit video\nannotations that are automatically generated by speech recognition and video\nOCR (optical character recognition).\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:02:15 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhou", "Hang", ""], ["Otto", "Christian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1912.10818", "submitter": "Daniel Acuna", "authors": "Lizhen Liang and Daniel E. Acuna", "title": "Artificial mental phenomena: Psychophysics as a framework to detect\n  perception biases in AI models", "comments": "FAT Conference 2020", "journal-ref": null, "doi": "10.1145/3351095.3375623", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting biases in artificial intelligence has become difficult because of\nthe impenetrable nature of deep learning. The central difficulty is in relating\nunobservable phenomena deep inside models with observable, outside quantities\nthat we can measure from inputs and outputs. For example, can we detect\ngendered perceptions of occupations (e.g., female librarian, male electrician)\nusing questions to and answers from a word embedding-based system? Current\ntechniques for detecting biases are often customized for a task, dataset, or\nmethod, affecting their generalization. In this work, we draw from\nPsychophysics in Experimental Psychology---meant to relate quantities from the\nreal world (i.e., \"Physics\") into subjective measures in the mind (i.e.,\n\"Psyche\")---to propose an intellectually coherent and generalizable framework\nto detect biases in AI. Specifically, we adapt the two-alternative forced\nchoice task (2AFC) to estimate potential biases and the strength of those\nbiases in black-box models. We successfully reproduce previously-known biased\nperceptions in word embeddings and sentiment analysis predictions. We discuss\nhow concepts in experimental psychology can be naturally applied to\nunderstanding artificial mental phenomena, and how psychophysics can form a\nuseful methodological foundation to study fairness in AI.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 19:48:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liang", "Lizhen", ""], ["Acuna", "Daniel E.", ""]]}, {"id": "1912.10819", "submitter": "Joeran Beel", "authors": "Conor O'Sullivan and Joeran Beel", "title": "Predicting the Outcome of Judicial Decisions made by the European Court\n  of Human Rights", "comments": null, "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science. 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, machine learning models were constructed to predict whether\njudgments made by the European Court of Human Rights (ECHR) would lead to a\nviolation of an Article in the Convention on Human Rights. The problem is\nframed as a binary classification task where a judgment can lead to a\n\"violation\" or \"non-violation\" of a particular Article. Using auto-sklearn, an\nautomated algorithm selection package, models were constructed for 12 Articles\nin the Convention. To train these models, textual features were obtained from\nthe ECHR Judgment documents using N-grams, word embeddings and paragraph\nembeddings. Additional documents, from the ECHR, were incorporated into the\nmodels through the creation of a word embedding (echr2vec) and a doc2vec model.\nThe features obtained using the echr2vec embedding provided the highest\ncross-validation accuracy for 5 of the Articles. The overall test accuracy,\nacross the 12 Articles, was 68.83%. As far as we could tell, this is the first\nestimate of the accuracy of such machine learning models using a realistic test\nset. This provides an important benchmark for future work. As a baseline, a\nsimple heuristic of always predicting the most common outcome in the past was\nused. The heuristic achieved an overall test accuracy of 86.68% which is 29.7%\nhigher than the models. Again, this was seemingly the first study that included\nsuch a heuristic with which to compare model results. The higher accuracy\nachieved by the heuristic highlights the importance of including such a\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:42:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["O'Sullivan", "Conor", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.10824", "submitter": "Pasquale Minervini", "authors": "Pasquale Minervini, Matko Bo\\v{s}njak, Tim Rockt\\\"aschel, Sebastian\n  Riedel, Edward Grefenstette", "title": "Differentiable Reasoning on Large Knowledge Bases and Natural Language", "comments": "Accepted at the 34th AAAI Conference on Artificial Intelligence\n  (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning with knowledge expressed in natural language and Knowledge Bases\n(KBs) is a major challenge for Artificial Intelligence, with applications in\nmachine reading, dialogue, and question answering. General neural architectures\nthat jointly learn representations and transformations of text are very\ndata-inefficient, and it is hard to analyse their reasoning process. These\nissues are addressed by end-to-end differentiable reasoning systems such as\nNeural Theorem Provers (NTPs), although they can only be used with small-scale\nsymbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension\nto NTPs addressing their complexity and scalability limitations, thus making\nthem applicable to real-world datasets. This result is achieved by dynamically\nconstructing the computation graph of NTPs and including only the most\npromising proof paths during inference, thus obtaining orders of magnitude more\nefficient models. Then, we propose a novel approach for jointly reasoning over\nKBs and textual mentions, by embedding logic facts and natural language\nsentences in a shared embedding space. We show that GNTPs perform on par with\nNTPs at a fraction of their cost while achieving competitive link prediction\nresults on large datasets, providing explanations for predictions, and inducing\ninterpretable models. Source code, datasets, and supplementary material are\navailable online at https://github.com/uclnlp/gntp.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:01:54 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Minervini", "Pasquale", ""], ["Bo\u0161njak", "Matko", ""], ["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1912.10839", "submitter": "Sanjaye Ramgoolam", "authors": "Sanjaye Ramgoolam, Mehrnoosh Sadrzadeh, Lewis Sword", "title": "Gaussianity and typicality in matrix distributional semantics", "comments": "38 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": "QMUL-PH-19-29", "categories": "hep-th cs.CL math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructions in type-driven compositional distributional semantics associate\nlarge collections of matrices of size $D$ to linguistic corpora. We develop the\nproposal of analysing the statistical characteristics of this data in the\nframework of permutation invariant matrix models. The observables in this\nframework are permutation invariant polynomial functions of the matrix entries,\nwhich correspond to directed graphs. Using the general 13-parameter permutation\ninvariant Gaussian matrix models recently solved, we find, using a dataset of\nmatrices constructed via standard techniques in distributional semantics, that\nthe expectation values of a large class of cubic and quartic observables show\nhigh gaussianity at levels between 90 to 99 percent. Beyond expectation values,\nwhich are averages over words, the dataset allows the computation of standard\ndeviations for each observable, which can be viewed as a measure of typicality\nfor each observable. There is a wide range of magnitudes in the measures of\ntypicality. The permutation invariant matrix models, considered as functions of\nrandom couplings, give a very good prediction of the magnitude of the\ntypicality for different observables. We find evidence that observables with\nsimilar matrix model characteristics of Gaussianity and typicality also have\nhigh degrees of correlation between the ranked lists of words associated to\nthese observables.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 15:55:42 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Ramgoolam", "Sanjaye", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Sword", "Lewis", ""]]}, {"id": "1912.10846", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Kyubum Lee, Shankai Yan, Sun Kim, Chih-Hsuan Wei, and\n  Zhiyong Lu", "title": "BioConceptVec: creating and evaluating literature-based biomedical\n  concept embeddings on a large scale", "comments": "33 pages, 6 figures, 7 tables, accepted by PLOS Computational Biology", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007617", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the semantics of related biological concepts, such as genes and\nmutations, is of significant importance to many research tasks in computational\nbiology such as protein-protein interaction detection, gene-drug association\nprediction, and biomedical literature-based discovery. Here, we propose to\nleverage state-of-the-art text mining tools and machine learning models to\nlearn the semantics via vector representations (aka. embeddings) of over\n400,000 biological concepts mentioned in the entire PubMed abstracts. Our\nlearned embeddings, namely BioConceptVec, can capture related concepts based on\ntheir surrounding contextual information in the literature, which is beyond\nexact term match or co-occurrence-based methods. BioConceptVec has been\nthoroughly evaluated in multiple bioinformatics tasks consisting of over 25\nmillion instances from nine different biological datasets. The evaluation\nresults demonstrate that BioConceptVec has better performance than existing\nmethods in all tasks. Finally, BioConceptVec is made freely available to the\nresearch community and general public via\nhttps://github.com/ncbi-nlp/BioConceptVec.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:46:46 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chen", "Qingyu", ""], ["Lee", "Kyubum", ""], ["Yan", "Shankai", ""], ["Kim", "Sun", ""], ["Wei", "Chih-Hsuan", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1912.10847", "submitter": "Preeti Sah", "authors": "Preeti Sah and Ernest Fokou\\'e", "title": "What do Asian Religions Have in Common? An Unsupervised Text Analytics\n  Exploration", "comments": "18 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main source of various religious teachings is their sacred texts which\nvary from religion to religion based on different factors like the geographical\nlocation or time of the birth of a particular religion. Despite these\ndifferences, there could be similarities between the sacred texts based on what\nlessons it teaches to its followers. This paper attempts to find the similarity\nusing text mining techniques. The corpus consisting of Asian (Tao Te Ching,\nBuddhism, Yogasutra, Upanishad) and non-Asian (four Bible texts) is used to\nexplore findings of similarity measures like Euclidean, Manhattan, Jaccard and\nCosine on raw Document Term Frequency [DTM], normalized DTM which reveals\nsimilarity based on word usage. The performance of Supervised learning\nalgorithms like K-Nearest Neighbor [KNN], Support Vector Machine [SVM] and\nRandom Forest is measured based on its accuracy to predict correct scared text\nfor any given chapter in the corpus. The K-means clustering visualizations on\nEuclidean distances of raw DTM reveals that there exists a pattern of\nsimilarity among these sacred texts with Upanishads and Tao Te Ching is the\nmost similar text in the corpus.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:28:29 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Sah", "Preeti", ""], ["Fokou\u00e9", "Ernest", ""]]}, {"id": "1912.10858", "submitter": "Xuan-Hong Dang", "authors": "Xuan-Hong Dang, Syed Yousaf Shah, Petros Zerfos", "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities\n  for Automated Financial Information Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal analysis that uses numerical time series and textual corpora as\ninput data sources is becoming a promising approach, especially in the\nfinancial industry. However, the main focus of such analysis has been on\nachieving high prediction accuracy while little effort has been spent on the\nimportant task of understanding the association between the two data\nmodalities. Performance on the time series hence receives little explanation\nthough human-understandable textual information is available. In this work, we\naddress the problem of given a numerical time series, and a general corpus of\ntextual stories collected in the same period of the time series, the task is to\ntimely discover a succinct set of textual stories associated with that time\nseries. Towards this goal, we propose a novel multi-modal neural model called\nMSIN that jointly learns both numerical time series and categorical text\narticles in order to unearth the association between them. Through multiple\nsteps of data interrelation between the two data modalities, MSIN learns to\nfocus on a small subset of text articles that best align with the performance\nin the time series. This succinct set is timely discovered and presented as\nrecommended documents, acting as automated information filtering, for the given\ntime series. We empirically evaluate the performance of our model on\ndiscovering relevant news articles for two stock time series from Apple and\nGoogle companies, along with the daily news articles collected from the Thomson\nReuters over a period of seven consecutive years. The experimental results\ndemonstrate that MSIN achieves up to 84.9% and 87.2% in recalling the ground\ntruth articles respectively to the two examined time series, far more superior\nto state-of-the-art algorithms that rely on conventional attention mechanism in\ndeep learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:37:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Dang", "Xuan-Hong", ""], ["Shah", "Syed Yousaf", ""], ["Zerfos", "Petros", ""]]}, {"id": "1912.10915", "submitter": "Jian Zhu", "authors": "Jian Zhu", "title": "Probing the phonetic and phonological knowledge of tones in Mandarin TTS\n  models", "comments": "Submitted to Speech Prosody 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study probes the phonetic and phonological knowledge of lexical tones in\nTTS models through two experiments. Controlled stimuli for testing tonal\ncoarticulation and tone sandhi in Mandarin were fed into Tacotron 2 and\nWaveGlow to generate speech samples, which were subject to acoustic analysis\nand human evaluation. Results show that both baseline Tacotron 2 and Tacotron 2\nwith BERT embeddings capture the surface tonal coarticulation patterns well but\nfail to consistently apply the Tone-3 sandhi rule to novel sentences.\nIncorporating pre-trained BERT embeddings into Tacotron 2 improves the\nnaturalness and prosody performance, and yields better generalization of Tone-3\nsandhi rules to novel complex sentences, although the overall accuracy for\nTone-3 sandhi was still low. Given that TTS models do capture some linguistic\nphenomena, it is argued that they can be used to generate and validate certain\nlinguistic hypotheses. On the other hand, it is also suggested that\nlinguistically informed stimuli should be included in the training and the\nevaluation of TTS models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 15:25:45 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Zhu", "Jian", ""]]}, {"id": "1912.11046", "submitter": "Pengcheng Liao", "authors": "Pengcheng Liao, Chuang Zhang, Xiaojun Chen and Xiaofei Zhou", "title": "Improving Abstractive Text Summarization with History Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural sequence to sequence models have provided feasible solutions\nfor abstractive summarization. However, such models are still hard to tackle\nlong text dependency in the summarization task. A high-quality summarization\nsystem usually depends on strong encoder which can refine important information\nfrom long input texts so that the decoder can generate salient summaries from\nthe encoder's memory. In this paper, we propose an aggregation mechanism based\non the Transformer model to address the challenge of long text representation.\nOur model can review history information to make encoder hold more memory\ncapacity. Empirically, we apply our aggregation mechanism to the Transformer\nmodel and experiment on CNN/DailyMail dataset to achieve higher quality\nsummaries compared to several strong baseline models on the ROUGE metrics.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 03:34:58 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Liao", "Pengcheng", ""], ["Zhang", "Chuang", ""], ["Chen", "Xiaojun", ""], ["Zhou", "Xiaofei", ""]]}, {"id": "1912.11078", "submitter": "Deven Santosh Shah", "authors": "Deven Shah, H. Andrew Schwartz, Dirk Hovy", "title": "Predictive Biases in Natural Language Processing Models: A Conceptual\n  Framework and Overview", "comments": "9 pages excluding references, 1 figure, 3 pages for appendix", "journal-ref": "Association for Computational Linguistics. (2020) 5248--5264", "doi": "10.18653/v1/2020.acl-main.468", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of works in natural language processing have addressed\nthe effect of bias on the predicted outcomes, introducing mitigation techniques\nthat act on different parts of the standard NLP pipeline (data and models).\nHowever, these works have been conducted in isolation, without a unifying\nframework to organize efforts within the field. This leads to repetitive\napproaches, and puts an undue focus on the effects of bias, rather than on\ntheir origins. Research focused on bias symptoms rather than the underlying\norigins could limit the development of effective countermeasures. In this\npaper, we propose a unifying conceptualization: the predictive bias framework\nfor NLP. We summarize the NLP literature and propose a general mathematical\ndefinition of predictive bias in NLP along with a conceptual framework,\ndifferentiating four main origins of biases: label bias, selection bias, model\noveramplification, and semantic bias. We discuss how past work has countered\neach bias origin. Our framework serves to guide an introductory overview of\npredictive bias in NLP, integrating existing work into a single structure and\nopening avenues for future research.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 23:53:19 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 19:56:10 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Shah", "Deven", ""], ["Schwartz", "H. Andrew", ""], ["Hovy", "Dirk", ""]]}, {"id": "1912.11151", "submitter": "Rupayan Chakraborty", "authors": "Sri Harsha Dumpala, Imran Sheikh, Rupayan Chakraborty, Sunil Kumar\n  Kopparapu", "title": "A Cycle-GAN Approach to Model Natural Perturbations in Speech for ASR\n  Applications", "comments": "7 pages, 3 figures, ICASSP-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturally introduced perturbations in audio signal, caused by emotional and\nphysical states of the speaker, can significantly degrade the performance of\nAutomatic Speech Recognition (ASR) systems. In this paper, we propose a\nfront-end based on Cycle-Consistent Generative Adversarial Network (CycleGAN)\nwhich transforms naturally perturbed speech into normal speech, and hence\nimproves the robustness of an ASR system. The CycleGAN model is trained on\nnon-parallel examples of perturbed and normal speech. Experiments on\nspontaneous laughter-speech and creaky-speech datasets show that the\nperformance of four different ASR systems improve by using speech obtained from\nCycleGAN based front-end, as compared to directly using the original perturbed\nspeech. Visualization of the features of the laughter perturbed speech and\nthose generated by the proposed front-end further demonstrates the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:26:38 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Dumpala", "Sri Harsha", ""], ["Sheikh", "Imran", ""], ["Chakraborty", "Rupayan", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1912.11270", "submitter": "Ahmad Sakor", "authors": "Ahmad Sakor, Kuldeep Singh, Anery Patel, Maria-Esther Vidal", "title": "Falcon 2.0: An Entity and Relation Linking Tool over Wikidata", "comments": "CIKM 2020 Paper 8 pages", "journal-ref": null, "doi": "10.1145/3340531.3412777", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Natural Language Processing (NLP) community has significantly contributed\nto the solutions for entity and relation recognition from the text, and\npossibly linking them to proper matches in Knowledge Graphs (KGs). Considering\nWikidata as the background KG, still, there are limited tools to link knowledge\nwithin the text to Wikidata. In this paper, we present Falcon 2.0, first joint\nentity, and relation linking tool over Wikidata. It receives a short natural\nlanguage text in the English language and outputs a ranked list of entities and\nrelations annotated with the proper candidates in Wikidata. The candidates are\nrepresented by their Internationalized Resource Identifier (IRI) in Wikidata.\nFalcon 2.0 resorts to the English language model for the recognition task\n(e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach\nfor linking task. We have empirically studied the performance of Falcon 2.0 on\nWikidata and concluded that it outperforms all the existing baselines. Falcon\n2.0 is public and can be reused by the community; all the required instructions\nof Falcon 2.0 are well-documented at our GitHub repository. We also demonstrate\nan online API, which can be run without any technical expertise. Falcon 2.0 and\nits background knowledge bases are available as resources at\nhttps://labs.tib.eu/falcon/falcon2/.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 09:53:27 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 16:22:04 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 13:40:06 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2020 18:32:14 GMT"}, {"version": "v5", "created": "Mon, 27 Jul 2020 15:12:14 GMT"}, {"version": "v6", "created": "Mon, 3 Aug 2020 15:24:22 GMT"}, {"version": "v7", "created": "Mon, 31 Aug 2020 21:24:10 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Sakor", "Ahmad", ""], ["Singh", "Kuldeep", ""], ["Patel", "Anery", ""], ["Vidal", "Maria-Esther", ""]]}, {"id": "1912.11334", "submitter": "Triet Chau", "authors": "Minh Triet Chau, Diego Esteves, Jens Lehmann", "title": "Open-domain Event Extraction and Embedding for Natural Gas Market\n  Prediction", "comments": null, "journal-ref": "CLEOPATRA 2020", "doi": null, "report-no": "urn:nbn:de:0074-2611-9", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to predict the natural gas price in several days using\nhistorical price data and events extracted from news headlines. Most previous\nmethods treats price as an extrapolatable time series, those analyze the\nrelation between prices and news either trim their price data correspondingly\nto a public news dataset, manually annotate headlines or use off-the-shelf\ntools. In comparison to off-the-shelf tools, our event extraction method\ndetects not only the occurrence of phenomena but also the changes in\nattribution and characteristics from public sources. Instead of using sentence\nembedding as a feature, we use every word of the extracted events, encode and\norganize them before feeding to the learning models. Empirical results show\nfavorable results, in terms of prediction performance, money saved and\nscalability.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 10:19:40 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Chau", "Minh Triet", ""], ["Esteves", "Diego", ""], ["Lehmann", "Jens", ""]]}, {"id": "1912.11585", "submitter": "Yi Liu", "authors": "Yi Liu, Tianyu Liang, Can Xu, Xianwei Zhang, Xianhong Chen, Wei-Qiang\n  Zhang, Liang He, Dandan song, Ruyun Li, Yangcheng Wu, Peng Ouyang, Shouyi Yin", "title": "THUEE system description for NIST 2019 SRE CTS Challenge", "comments": "This is the system description of THUEE submitted to NIST SRE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the systems submitted by the department of electronic\nengineering, institute of microelectronics of Tsinghua university and\nTsingMicro Co. Ltd. (THUEE) to the NIST 2019 speaker recognition evaluation CTS\nchallenge. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet,\nmultitask and c-vector are developed in this evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 03:44:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Liu", "Yi", ""], ["Liang", "Tianyu", ""], ["Xu", "Can", ""], ["Zhang", "Xianwei", ""], ["Chen", "Xianhong", ""], ["Zhang", "Wei-Qiang", ""], ["He", "Liang", ""], ["song", "Dandan", ""], ["Li", "Ruyun", ""], ["Wu", "Yangcheng", ""], ["Ouyang", "Peng", ""], ["Yin", "Shouyi", ""]]}, {"id": "1912.11600", "submitter": "Artyom Kovalevskii", "authors": "Anik Chakrabarty, Mikhail Chebunin, Artyom Kovalevskii, Ilya Pupyshev,\n  Natalia Zakrevskaya, Qianqian Zhou", "title": "A statistical test for correspondence of texts to the Zipf-Mandelbrot\n  law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CL stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse correspondence of a text to a simple probabilistic model. The\nmodel assumes that the words are selected independently from an infinite\ndictionary. The probability distribution correspond to the Zipf---Mandelbrot\nlaw. We count sequentially the numbers of different words in the text and get\nthe process of the numbers of different words. Then we estimate\nZipf---Mandelbrot law parameters using the same sequence and construct an\nestimate of the expectation of the number of different words in the text. Then\nwe subtract the corresponding values of the estimate from the sequence and\nnormalize along the coordinate axes, obtaining a random process on a segment\nfrom 0 to 1. We prove that this process (the empirical text bridge) converges\nweakly in the uniform metric on $C (0,1)$ to a centered Gaussian process with\ncontinuous a.s. paths. We develop and implement an algorithm for approximate\ncalculation of eigenvalues of the covariance function of the limit Gaussian\nprocess, and then an algorithm for calculating the probability distribution of\nthe integral of the square of this process. We use the algorithm to analyze\nuniformity of texts in English, French, Russian and Chinese.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 05:59:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Chakrabarty", "Anik", ""], ["Chebunin", "Mikhail", ""], ["Kovalevskii", "Artyom", ""], ["Pupyshev", "Ilya", ""], ["Zakrevskaya", "Natalia", ""], ["Zhou", "Qianqian", ""]]}, {"id": "1912.11602", "submitter": "Chenguang Zhu", "authors": "Chenguang Zhu, Ziyi Yang, Robert Gmyr, Michael Zeng, Xuedong Huang", "title": "Leveraging Lead Bias for Zero-shot Abstractive News Summarization", "comments": "Published in ACM SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical journalistic convention in news articles is to deliver the most\nsalient information in the beginning, also known as the lead bias. While this\nphenomenon can be exploited in generating a summary, it has a detrimental\neffect on teaching a model to discriminate and extract important information in\ngeneral. We propose that this lead bias can be leveraged in our favor in a\nsimple and effective way to pre-train abstractive news summarization models on\nlarge-scale unlabeled news corpora: predicting the leading sentences using the\nrest of an article. We collect a massive news corpus and conduct data cleaning\nand filtering via statistical analysis. We then apply self-supervised\npre-training on this dataset to existing generation models BART and T5 for\ndomain adaptation. Via extensive experiments on six benchmark datasets, we show\nthat this approach can dramatically improve the summarization quality and\nachieve state-of-the-art results for zero-shot news summarization without any\nfine-tuning. For example, in the DUC2003 dataset, the ROUGE-1 score of BART\nincreases 13.7% after the lead-bias pre-training. We deploy the model in\nMicrosoft News and provide public APIs as well as a demo website for\nmulti-lingual news summarization.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 06:05:44 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 04:04:03 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 23:43:44 GMT"}, {"version": "v4", "created": "Thu, 15 Apr 2021 18:07:05 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Chenguang", ""], ["Yang", "Ziyi", ""], ["Gmyr", "Robert", ""], ["Zeng", "Michael", ""], ["Huang", "Xuedong", ""]]}, {"id": "1912.11612", "submitter": "Md Ataur Rahman", "authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui", "title": "N-gram Statistical Stemmer for Bangla Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 07:31:44 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sadia", "Rabeya", ""], ["Rahman", "Md Ataur", ""], ["Seddiqui", "Md Hanif", ""]]}, {"id": "1912.11625", "submitter": "Xu Tan", "authors": "Xu Tan, Yichong Leng, Jiale Chen, Yi Ren, Tao Qin, Tie-Yan Liu", "title": "A Study of Multilingual Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual neural machine translation (NMT) has recently been investigated\nfrom different aspects (e.g., pivot translation, zero-shot translation,\nfine-tuning, or training from scratch) and in different settings (e.g., rich\nresource and low resource, one-to-many, and many-to-one translation). This\npaper concentrates on a deep understanding of multilingual NMT and conducts a\ncomprehensive study on a multilingual dataset with more than 20 languages. Our\nresults show that (1) low-resource language pairs benefit much from\nmultilingual training, while rich-resource language pairs may get hurt under\nlimited model capacity and training with similar languages benefits more than\ndissimilar languages; (2) fine-tuning performs better than training from\nscratch in the one-to-many setting while training from scratch performs better\nin the many-to-one setting; (3) the bottom layers of the encoder and top layers\nof the decoder capture more language-specific information, and just fine-tuning\nthese parts can achieve good accuracy for low-resource language pairs; (4)\ndirect translation is better than pivot translation when the source language is\nsimilar to the target language (e.g., in the same language branch), even when\nthe size of direct training data is much smaller; (5) given a fixed training\ndata budget, it is better to introduce more languages into multilingual\ntraining for zero-shot translation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 08:50:45 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Tan", "Xu", ""], ["Leng", "Yichong", ""], ["Chen", "Jiale", ""], ["Ren", "Yi", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1912.11637", "submitter": "Guangxiang Zhao", "authors": "Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu\n  Sun", "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention based Transformer has demonstrated the state-of-the-art\nperformances in a number of natural language processing tasks. Self-attention\nis able to model long-term dependencies, but it may suffer from the extraction\nof irrelevant information in the context. To tackle the problem, we propose a\nnovel model called \\textbf{Explicit Sparse Transformer}. Explicit Sparse\nTransformer is able to improve the concentration of attention on the global\ncontext through an explicit selection of the most relevant segments. Extensive\nexperimental results on a series of natural language processing and computer\nvision tasks, including neural machine translation, image captioning, and\nlanguage modeling, all demonstrate the advantages of Explicit Sparse\nTransformer in model performance. We also show that our proposed sparse\nattention method achieves comparable or better results than the previous sparse\nattention method, but significantly reduces training and testing time. For\nexample, the inference speed is twice that of sparsemax in Transformer model.\nCode will be available at\n\\url{https://github.com/lancopku/Explicit-Sparse-Transformer}\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 10:59:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhao", "Guangxiang", ""], ["Lin", "Junyang", ""], ["Zhang", "Zhiyuan", ""], ["Ren", "Xuancheng", ""], ["Su", "Qi", ""], ["Sun", "Xu", ""]]}, {"id": "1912.11668", "submitter": "Jianhao Shen", "authors": "Yikai Zhu, Jianhao Shen, Ming Zhang", "title": "Learning to Answer Ambiguous Questions with Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of factoid question answering over knowledge base, many questions\nhave more than one plausible interpretation. Previous works on SimpleQuestions\nassume only one interpretation as the ground truth for each question, so they\nlack the ability to answer ambiguous questions correctly. In this paper, we\npresent a new way to utilize the dataset that takes into account the existence\nof ambiguous questions. Then we introduce a simple and effective model which\ncombines local knowledge subgraph with attention mechanism. Our experimental\nresults show that our approach achieves outstanding performance in this task.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 13:43:44 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhu", "Yikai", ""], ["Shen", "Jianhao", ""], ["Zhang", "Ming", ""]]}, {"id": "1912.11688", "submitter": "Abhishek Singh", "authors": "Abhishek Kumar Singh, Manish Gupta, Vasudeva Varma", "title": "Unity in Diversity: Learning Distributed Heterogeneous Sentence\n  Representation for Extractive Summarization", "comments": "Accepted in AAAI Conference on Artificial Intelligence, 2018.\n  Retrieved from\n  https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16977", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated multi-document extractive text summarization is a widely studied\nresearch problem in the field of natural language understanding. Such\nextractive mechanisms compute in some form the worthiness of a sentence to be\nincluded into the summary. While the conventional approaches rely on human\ncrafted document-independent features to generate a summary, we develop a\ndata-driven novel summary system called HNet, which exploits the various\nsemantic and compositional aspects latent in a sentence to capture document\nindependent features. The network learns sentence representation in a way that,\nsalient sentences are closer in the vector space than non-salient sentences.\nThis semantic and compositional feature vector is then concatenated with the\ndocument-dependent features for sentence ranking. Experiments on the DUC\nbenchmark datasets (DUC-2001, DUC-2002 and DUC-2004) indicate that our model\nshows significant performance gain of around 1.5-2 points in terms of ROUGE\nscore compared with the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 16:25:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Singh", "Abhishek Kumar", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1912.11701", "submitter": "Abhishek Singh", "authors": "Abhishek Kumar Singh, Manish Gupta, Vasudeva Varma", "title": "Hybrid MemNet for Extractive Summarization", "comments": "Accepted in CIKM '17 Proceedings of the 2017 ACM on Conference on\n  Information and Knowledge Management", "journal-ref": "In Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management (CIKM '17). ACM, New York, NY, USA, pages 2303-2306", "doi": "10.1145/3132847.3133127", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive text summarization has been an extensive research problem in the\nfield of natural language understanding. While the conventional approaches rely\nmostly on manually compiled features to generate the summary, few attempts have\nbeen made in developing data-driven systems for extractive summarization. To\nthis end, we present a fully data-driven end-to-end deep network which we call\nas Hybrid MemNet for single document summarization task. The network learns the\ncontinuous unified representation of a document before generating its summary.\nIt jointly captures local and global sentential information along with the\nnotion of summary worthy sentences. Experimental results on two different\ncorpora confirm that our model shows significant performance gains compared\nwith the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 17:48:09 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Singh", "Abhishek Kumar", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1912.11720", "submitter": "Qing Ping", "authors": "Qing Ping, Chaomei Chen", "title": "Convolutional Quantum-Like Language Model with Mutual-Attention for\n  Product Rating Prediction", "comments": "Accepted at MAISON workshop at ICTIR 19'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are designed to help mitigate information overload users\nexperience during online shopping. Recent work explores neural language models\nto learn user and item representations from user reviews and combines such\nrepresentations with rating information. Most existing convolutional-based\nneural models take pooling immediately after convolution and loses the\ninteraction information between the latent dimension of convolutional feature\nvectors along the way. Moreover, these models usually take all feature vectors\nat higher levels as equal and do not take into consideration that some features\nare more relevant to this specific user-item context. To bridge these gaps,\nthis paper proposes a convolutional quantum-like language model with\nmutual-attention for rating prediction (ConQAR). By introducing a quantum-like\ndensity matrix layer, interactions between latent dimensions of convolutional\nfeature vectors are well captured. With the attention weights learned from the\nmutual-attention layer, final representations of a user and an item absorb\ninformation from both itself and its counterparts for making rating prediction.\nExperiments on two large datasets show that our model outperforms multiple\nstate-of-the-art CNN-based models. We also perform an ablation test to analyze\nthe independent effects of the two components of our model. Moreover, we\nconduct a case study and present visualizations of the quantum probabilistic\ndistributions in one user and one item review document to show that the learned\ndistributions capture meaningful information about this user and item, and can\nbe potentially used as textual profiling of the user and item.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 22:01:59 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ping", "Qing", ""], ["Chen", "Chaomei", ""]]}, {"id": "1912.11739", "submitter": "Haiyue Song", "authors": "Haiyue Song, Raj Dabre, Atsushi Fujita, Sadao Kurohashi", "title": "Coursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures\n  Translation", "comments": "10 pages, 1 figure, 9 tables, under review by LREC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lectures translation is a case of spoken language translation and there is a\nlack of publicly available parallel corpora for this purpose. To address this,\nwe examine a language independent framework for parallel corpus mining which is\na quick and effective way to mine a parallel corpus from publicly available\nlectures at Coursera. Our approach determines sentence alignments, relying on\nmachine translation and cosine similarity over continuous-space sentence\nrepresentations. We also show how to use the resulting corpora in a multistage\nfine-tuning based domain adaptation for high-quality lectures translation. For\nJapanese--English lectures translation, we extracted parallel data of\napproximately 40,000 lines and created development and test sets through manual\nfiltering for benchmarking translation performance. We demonstrate that the\nmined corpus greatly enhances the quality of translation when used in\nconjunction with out-of-domain parallel corpora via multistage training. This\npaper also suggests some guidelines to gather and clean corpora, mine parallel\nsentences, address noise in the mined data, and create high-quality evaluation\nsplits. For the sake of reproducibility, we will release our code for parallel\ndata creation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 01:12:31 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 03:16:24 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Song", "Haiyue", ""], ["Dabre", "Raj", ""], ["Fujita", "Atsushi", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "1912.11959", "submitter": "Thomas Dowdell BCom(Hons)", "authors": "Thomas Dowdell and Hongyu Zhang", "title": "Is Attention All What You Need? -- An Empirical Investigation on\n  Convolution-Based Active Memory and Self-Attention", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The key to a Transformer model is the self-attention mechanism, which allows\nthe model to analyze an entire sequence in a computationally efficient manner.\nRecent work has suggested the possibility that general attention mechanisms\nused by RNNs could be replaced by active-memory mechanisms. In this work, we\nevaluate whether various active-memory mechanisms could replace self-attention\nin a Transformer. Our experiments suggest that active-memory alone achieves\ncomparable results to the self-attention mechanism for language modelling, but\noptimal results are mostly achieved by using both active-memory and\nself-attention mechanisms together. We also note that, for some specific\nalgorithmic tasks, active-memory mechanisms alone outperform both\nself-attention and a combination of the two.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:01:13 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 09:01:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Dowdell", "Thomas", ""], ["Zhang", "Hongyu", ""]]}, {"id": "1912.11973", "submitter": "Muhammad Haroon Shakeel", "authors": "Muhammad Haroon Shakeel, Turki Alghamidi, Safi Faizullah, Imdadullah\n  Khan", "title": "Language Independent Sentiment Analysis", "comments": null, "journal-ref": "International Conference on Advances in the Emerging Computing\n  Technologies (AECT), 2020", "doi": "10.1109/AECT47998.2020.9194186", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms and online forums generate rapid and increasing amount\nof textual data. Businesses, government agencies, and media organizations seek\nto perform sentiment analysis on this rich text data. The results of these\nanalytics are used for adapting marketing strategies, customizing products,\nsecurity and various other decision makings. Sentiment analysis has been\nextensively studied and various methods have been developed for it with great\nsuccess. These methods, however apply to texts written in a specific language.\nThis limits applicability to a limited demographic and a specific geographic\nregion. In this paper we propose a general approach for sentiment analysis on\ndata containing texts from multiple languages. This enables all the\napplications to utilize the results of sentiment analysis in a language\noblivious or language-independent fashion.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 03:20:48 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 12:55:41 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Shakeel", "Muhammad Haroon", ""], ["Alghamidi", "Turki", ""], ["Faizullah", "Safi", ""], ["Khan", "Imdadullah", ""]]}, {"id": "1912.11975", "submitter": "Kexin Huang", "authors": "Kexin Huang, Abhishek Singh, Sitong Chen, Edward T. Moseley, Chih-ying\n  Deng, Naomi George, Charlotta Lindvall", "title": "Clinical XLNet: Modeling Sequential Clinical Notes and Predicting\n  Prolonged Mechanical Ventilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical notes contain rich data, which is unexploited in predictive modeling\ncompared to structured data. In this work, we developed a new text\nrepresentation Clinical XLNet for clinical notes which also leverages the\ntemporal information of the sequence of the notes. We evaluated our models on\nprolonged mechanical ventilation prediction problem and our experiments\ndemonstrated that Clinical XLNet outperforms the best baselines consistently.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 03:40:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Huang", "Kexin", ""], ["Singh", "Abhishek", ""], ["Chen", "Sitong", ""], ["Moseley", "Edward T.", ""], ["Deng", "Chih-ying", ""], ["George", "Naomi", ""], ["Lindvall", "Charlotta", ""]]}, {"id": "1912.11980", "submitter": "Zuchao Li", "authors": "Zuchao Li, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita,\n  Zhuosheng Zhang, Hai Zhao", "title": "Explicit Sentence Compression for Neural Machine Translation", "comments": "Working in progress, part of this work is accepted in AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art Transformer-based neural machine translation (NMT) systems\nstill follow a standard encoder-decoder framework, in which source sentence\nrepresentation can be well done by an encoder with self-attention mechanism.\nThough Transformer-based encoder may effectively capture general information in\nits resulting source sentence representation, the backbone information, which\nstands for the gist of a sentence, is not specifically focused on. In this\npaper, we propose an explicit sentence compression method to enhance the source\nsentence representation for NMT. In practice, an explicit sentence compression\ngoal used to learn the backbone information in a sentence. We propose three\nways, including backbone source-side fusion, target-side fusion, and both-side\nfusion, to integrate the compressed sentence into NMT. Our empirical tests on\nthe WMT English-to-French and English-to-German translation tasks show that the\nproposed sentence compression method significantly improves the translation\nperformances over strong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 04:14:06 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Li", "Zuchao", ""], ["Wang", "Rui", ""], ["Chen", "Kehai", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "1912.12010", "submitter": "Chengzhu Yu", "authors": "Yusong Wu, Shengchen Li, Chengzhu Yu, Heng Lu, Chao Weng, Liqiang\n  Zhang, Dong Yu", "title": "Synthesising Expressiveness in Peking Opera via Duration Informed\n  Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method that generates expressive singing voice of\nPeking opera. The synthesis of expressive opera singing usually requires pitch\ncontours to be extracted as the training data, which relies on techniques and\nis not able to be manually labeled. With the Duration Informed Attention\nNetwork (DurIAN), this paper makes use of musical note instead of pitch\ncontours for expressive opera singing synthesis. The proposed method enables\nhuman annotation being combined with automatic extracted features to be used as\ntraining data thus the proposed method gives extra flexibility in data\ncollection for Peking opera singing synthesis. Comparing with the expressive\nsinging voice of Peking opera synthesised by pitch contour based system, the\nproposed musical note based system produces comparable singing voice in Peking\nopera with expressiveness in various aspects.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 07:28:11 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Wu", "Yusong", ""], ["Li", "Shengchen", ""], ["Yu", "Chengzhu", ""], ["Lu", "Heng", ""], ["Weng", "Chao", ""], ["Zhang", "Liqiang", ""], ["Yu", "Dong", ""]]}, {"id": "1912.12014", "submitter": "Pengcheng Yang", "authors": "Pengcheng Yang, Boxing Chen, Pei Zhang, Xu Sun", "title": "Visual Agreement Regularized Training for Multi-Modal Machine\n  Translation", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal machine translation aims at translating the source sentence into\na different language in the presence of the paired image. Previous work\nsuggests that additional visual information only provides dispensable help to\ntranslation, which is needed in several very special cases such as translating\nambiguous words. To make better use of visual information, this work presents\nvisual agreement regularized training. The proposed approach jointly trains the\nsource-to-target and target-to-source translation models and encourages them to\nshare the same focus on the visual information when generating semantically\nequivalent visual words (e.g. \"ball\" in English and \"ballon\" in French).\nBesides, a simple yet effective multi-head co-attention model is also\nintroduced to capture interactions between visual and textual features. The\nresults show that our approaches can outperform competitive baselines by a\nlarge margin on the Multi30k dataset. Further analysis demonstrates that the\nproposed regularized training can effectively improve the agreement of\nattention on the image, leading to better use of visual information.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 07:46:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yang", "Pengcheng", ""], ["Chen", "Boxing", ""], ["Zhang", "Pei", ""], ["Sun", "Xu", ""]]}, {"id": "1912.12068", "submitter": "Muhammad Haroon Shakeel", "authors": "Muhammad Haroon Shakeel, Asim Karim, Imdadullah Khan", "title": "A Multi-cascaded Model with Data Augmentation for Enhanced Paraphrase\n  Detection in Short Texts", "comments": null, "journal-ref": "Information Processing & Management, Volume 57, Issue 3, May 2020,\n  102204", "doi": "10.1016/j.ipm.2020.102204", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paraphrase detection is an important task in text analytics with numerous\napplications such as plagiarism detection, duplicate question identification,\nand enhanced customer support helpdesks. Deep models have been proposed for\nrepresenting and classifying paraphrases. These models, however, require large\nquantities of human-labeled data, which is expensive to obtain. In this work,\nwe present a data augmentation strategy and a multi-cascaded model for improved\nparaphrase detection in short texts. Our data augmentation strategy considers\nthe notions of paraphrases and non-paraphrases as binary relations over the set\nof texts. Subsequently, it uses graph theoretic concepts to efficiently\ngenerate additional paraphrase and non-paraphrase pairs in a sound manner. Our\nmulti-cascaded model employs three supervised feature learners (cascades) based\non CNN and LSTM networks with and without soft-attention. The learned features,\ntogether with hand-crafted linguistic features, are then forwarded to a\ndiscriminator network for final classification. Our model is both wide and deep\nand provides greater robustness across clean and noisy short texts. We evaluate\nour approach on three benchmark datasets and show that it produces a comparable\nor state-of-the-art performance on all three.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 12:10:10 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Shakeel", "Muhammad Haroon", ""], ["Karim", "Asim", ""], ["Khan", "Imdadullah", ""]]}, {"id": "1912.12214", "submitter": "Tin Huynh Van", "authors": "Tin Van Huynh, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen, Anh Gia-Tuan\n  Nguyen", "title": "Job Prediction: From Deep Neural Network Models to Applications", "comments": "Accepted by IEEE RIVF 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Determining the job is suitable for a student or a person looking for work\nbased on their job's descriptions such as knowledge and skills that are\ndifficult, as well as how employers must find ways to choose the candidates\nthat match the job they require. In this paper, we focus on studying the job\nprediction using different deep neural network models including TextCNN,\nBi-GRU-LSTM-CNN, and Bi-GRU-CNN with various pre-trained word embeddings on the\nIT Job dataset. In addition, we also proposed a simple and effective ensemble\nmodel combining different deep neural network models. The experimental results\nillustrated that our proposed ensemble model achieved the highest result with\nan F1 score of 72.71%. Moreover, we analyze these experimental results to have\ninsights about this problem to find better solutions in the future.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 16:13:43 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 09:36:49 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Van Huynh", "Tin", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""], ["Nguyen", "Anh Gia-Tuan", ""]]}, {"id": "1912.12333", "submitter": "Benyou Wang", "authors": "Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang,\n  Jakob Grue Simonsen", "title": "Encoding word order in complex embeddings", "comments": "15 pages, 3 figures, ICLR 2020 spotlight paper. A typo on Ablation\n  Table was revised thanks to Jingquan Zeng from SCUT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential word order is important when processing text. Currently, neural\nnetworks (NNs) address this by modeling word position using position\nembeddings. The problem is that position embeddings capture the position of\nindividual words, but not the ordered relationship (e.g., adjacency or\nprecedence) between individual word positions. We present a novel and\nprincipled solution for modeling both the global absolute positions of words\nand their order relationships. Our solution generalizes word embeddings,\npreviously defined as independent vectors, to continuous word functions over a\nvariable (position). The benefit of continuous functions over variable\npositions is that word representations shift smoothly with increasing\npositions. Hence, word representations in different positions can correlate\nwith each other in a continuous function. The general solution of these\nfunctions is extended to complex-valued domain due to richer representations.\nWe extend CNN, RNN and Transformer NNs to complex-valued versions to\nincorporate our complex embedding (we make all code available). Experiments on\ntext classification, machine translation and language modeling show gains over\nboth classical word embeddings and position-enriched word embeddings. To our\nknowledge, this is the first work in NLP to link imaginary numbers in\ncomplex-valued representations to concrete meanings (i.e., word order).\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 20:59:38 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 04:37:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Benyou", ""], ["Zhao", "Donghao", ""], ["Lioma", "Christina", ""], ["Li", "Qiuchi", ""], ["Zhang", "Peng", ""], ["Simonsen", "Jakob Grue", ""]]}, {"id": "1912.12362", "submitter": "Simone Santini", "authors": "Maria Rojo Gonz\\'alez and Simone Santini", "title": "Structural characterization of musical harmonies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structural characteristics of harmony is essential for an\neffective use of music as a communication medium. Of the three expressive axes\nof music (melody, rhythm, harmony), harmony is the foundation on which the\nemotional content is built, and its understanding is important in areas such as\nmultimedia and affective computing. The common tool for studying this kind of\nstructure in computing science is the formal grammar but, in the case of music,\ngrammars run into problems due to the ambiguous nature of some of the concepts\ndefined in music theory. In this paper, we consider one of such constructs:\nmodulation, that is, the change of key in the middle of a musical piece, an\nimportant tool used by many authors to enhance the capacity of music to express\nemotions. We develop a hybrid method in which an evidence-gathering numerical\nmethod detects modulation and then, based on the detected tonalities, a\nnon-ambiguous grammar can be used for analyzing the structure of each tonal\ncomponent. Experiments with music from the XVII and XVIII centuries show that\nwe can detect the precise point of modulation with an error of at most two\nchords in almost 97% of the cases. Finally, we show examples of complete\nmodulation and structural analysis of musical harmonies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 23:15:49 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Gonz\u00e1lez", "Maria Rojo", ""], ["Santini", "Simone", ""]]}, {"id": "1912.12380", "submitter": "Yingcheng Sun", "authors": "Yingcheng Sun, Kenneth Loparo", "title": "Knowledge-guided Text Structuring in Clinical Trials", "comments": null, "journal-ref": "2019 19th Industrial Conference on Data Mining", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trial records are variable resources or the analysis of patients and\ndiseases. Information extraction from free text such as eligibility criteria\nand summary of results and conclusions in clinical trials would better support\ncomputer-based eligibility query formulation and electronic patient screening.\nPrevious research has focused on extracting information from eligibility\ncriteria, with usually a single pair of medical entity and attribute, but\nseldom considering other kinds of free text with multiple entities, attributes\nand relations that are more complex for parsing. In this paper, we propose a\nknowledge-guided text structuring framework with an automatically generated\nknowledge base as training corpus and word dependency relations as context\ninformation to transfer free text into formal, computer-interpretable\nrepresentations. Experimental results show that our method can achieve overall\nhigh precision and recall, demonstrating the effectiveness and efficiency of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 01:12:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sun", "Yingcheng", ""], ["Loparo", "Kenneth", ""]]}, {"id": "1912.12394", "submitter": "Da Ju", "authors": "Da Ju, Kurt Shuster, Y-Lan Boureau, Jason Weston", "title": "All-in-One Image-Grounded Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As single-task accuracy on individual language and image tasks has improved\nsubstantially in the last few years, the long-term goal of a generally skilled\nagent that can both see and talk becomes more feasible to explore. In this\nwork, we focus on leveraging individual language and image tasks, along with\nresources that incorporate both vision and language towards that objective. We\ndesign an architecture that combines state-of-the-art Transformer and ResNeXt\nmodules fed into a novel attentive multimodal module to produce a combined\nmodel trained on many tasks. We provide a thorough analysis of the components\nof the model, and transfer performance when training on one, some, or all of\nthe tasks. Our final models provide a single system that obtains good results\non all vision and language tasks considered, and improves the state-of-the-art\nin image-grounded conversational applications.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 03:51:52 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 23:10:55 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Ju", "Da", ""], ["Shuster", "Kurt", ""], ["Boureau", "Y-Lan", ""], ["Weston", "Jason", ""]]}, {"id": "1912.12397", "submitter": "Saptarshi Purkayastha", "authors": "Siddhartha Nuthakki, Sunil Neela, Judy W. Gichoya, Saptarshi\n  Purkayastha", "title": "Natural language processing of MIMIC-III clinical notes for identifying\n  diagnosis and procedures with neural networks", "comments": "This is a shortened version of the Capstone Project that was accepted\n  by the Faculty of Indiana University, in partial fulfillment of the\n  requirements for the degree of Master of Science in Health Informatics in Dec\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coding diagnosis and procedures in medical records is a crucial process in\nthe healthcare industry, which includes the creation of accurate billings,\nreceiving reimbursements from payers, and creating standardized patient care\nrecords. In the United States, Billing and Insurance related activities cost\naround $471 billion in 2012 which constitutes about 25% of all the U.S hospital\nspending. In this paper, we report the performance of a natural language\nprocessing model that can map clinical notes to medical codes, and predict\nfinal diagnosis from unstructured entries of history of present illness,\nsymptoms at the time of admission, etc. Previous studies have demonstrated that\ndeep learning models perform better at such mapping when compared to\nconventional machine learning models. Therefore, we employed state-of-the-art\ndeep learning method, ULMFiT on the largest emergency department clinical notes\ndataset MIMIC III which has 1.2M clinical notes to select for the top-10 and\ntop-50 diagnosis and procedure codes. Our models were able to predict the\ntop-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the\ntop-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and\n63.9% accuracy. Prediction of diagnosis and procedures from unstructured\nclinical notes benefit human coders to save time, eliminate errors and minimize\ncosts. With promising scores from our present model, the next step would be to\ndeploy this on a small-scale real-world scenario and compare it with human\ncoders as the gold standard. We believe that further research of this approach\ncan create highly accurate predictions that can ease the workflow in a clinical\nsetting.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 04:05:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nuthakki", "Siddhartha", ""], ["Neela", "Sunil", ""], ["Gichoya", "Judy W.", ""], ["Purkayastha", "Saptarshi", ""]]}, {"id": "1912.12422", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai and Cong Yao", "title": "TextScanner: Reading Characters in Order for Robust Scene Text\n  Recognition", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by deep learning and the large volume of data, scene text recognition\nhas evolved rapidly in recent years. Formerly, RNN-attention based methods have\ndominated this field, but suffer from the problem of \\textit{attention drift}\nin certain situations. Lately, semantic segmentation based algorithms have\nproven effective at recognizing text of different forms (horizontal, oriented\nand curved). However, these methods may produce spurious characters or miss\ngenuine characters, as they rely heavily on a thresholding procedure operated\non segmentation maps. To tackle these challenges, we propose in this paper an\nalternative approach, called TextScanner, for scene text recognition.\nTextScanner bears three characteristics: (1) Basically, it belongs to the\nsemantic segmentation family, as it generates pixel-wise, multi-channel\nsegmentation maps for character class, position and order; (2) Meanwhile, akin\nto RNN-attention based methods, it also adopts RNN for context modeling; (3)\nMoreover, it performs paralleled prediction for character position and class,\nand ensures that characters are transcripted in correct order. The experiments\non standard benchmark datasets demonstrate that TextScanner outperforms the\nstate-of-the-art methods. Moreover, TextScanner shows its superiority in\nrecognizing more difficult text such Chinese transcripts and aligning with\ntarget characters.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 07:52:00 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 10:18:26 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Wan", "Zhaoyi", ""], ["He", "Minghang", ""], ["Chen", "Haoran", ""], ["Bai", "Xiang", ""], ["Yao", "Cong", ""]]}, {"id": "1912.12481", "submitter": "Prakhar Gupta", "authors": "Ali Sabet, Prakhar Gupta, Jean-Baptiste Cordonnier, Robert West,\n  Martin Jaggi", "title": "Robust Cross-lingual Embeddings from Parallel Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cross-lingual word embeddings have primarily relied on\nmapping-based methods, which project pretrained word embeddings from different\nlanguages into a shared space through a linear transformation. However, these\napproaches assume word embedding spaces are isomorphic between different\nlanguages, which has been shown not to hold in practice (S{\\o}gaard et al.,\n2018), and fundamentally limits their performance. This motivates investigating\njoint learning methods which can overcome this impediment, by simultaneously\nlearning embeddings across languages via a cross-lingual term in the training\nobjective. We propose a bilingual extension of the CBOW method which leverages\nsentence-aligned corpora to obtain robust cross-lingual word and sentence\nrepresentations. Our approach significantly improves cross-lingual sentence\nretrieval performance over all other approaches while maintaining parity with\nthe current state-of-the-art methods on word-translation. It also achieves\nparity with a deep RNN method on a zero-shot cross-lingual document\nclassification task, requiring far fewer computational resources for training\nand inference. As an additional advantage, our bilingual method leads to a much\nmore pronounced improvement in the the quality of monolingual word vectors\ncompared to other competing methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 16:18:33 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 17:02:33 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Sabet", "Ali", ""], ["Gupta", "Prakhar", ""], ["Cordonnier", "Jean-Baptiste", ""], ["West", "Robert", ""], ["Jaggi", "Martin", ""]]}, {"id": "1912.12514", "submitter": "Ali Fadel", "authors": "Ali Fadel, Ibraheem Tuffaha, Mahmoud Al-Ayyoub", "title": "Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our team's effort on the semantic text question\nsimilarity task of NSURL 2019. Our top performing system utilizes several\ninnovative data augmentation techniques to enlarge the training data. Then, it\ntakes ELMo pre-trained contextual embeddings of the data and feeds them into an\nON-LSTM network with self-attention. This results in sequence representation\nvectors that are used to predict the relation between the question pairs. The\nmodel is ranked in the 1st place with 96.499 F1-score (same as the second place\nF1-score) and the 2nd place with 94.848 F1-score (differs by 1.076 F1-score\nfrom the first place) on the public and private leaderboards, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 20:11:33 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Fadel", "Ali", ""], ["Tuffaha", "Ibraheem", ""], ["Al-Ayyoub", "Mahmoud", ""]]}, {"id": "1912.12520", "submitter": "Yaqing Wang", "authors": "Yaqing Wang, Weifeng Yang, Fenglong Ma, Jin Xu, Bin Zhong, Qiang Deng,\n  Jing Gao", "title": "Weak Supervision for Fake News Detection via Reinforcement Learning", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today social media has become the primary source for news. Via social media\nplatforms, fake news travel at unprecedented speeds, reach global audiences and\nput users and communities at great risk. Therefore, it is extremely important\nto detect fake news as early as possible. Recently, deep learning based\napproaches have shown improved performance in fake news detection. However, the\ntraining of such models requires a large amount of labeled data, but manual\nannotation is time-consuming and expensive. Moreover, due to the dynamic nature\nof news, annotated samples may become outdated quickly and cannot represent the\nnews articles on newly emerged events. Therefore, how to obtain fresh and\nhigh-quality labeled samples is the major challenge in employing deep learning\nmodels for fake news detection. In order to tackle this challenge, we propose a\nreinforced weakly-supervised fake news detection framework, i.e., WeFEND, which\ncan leverage users' reports as weak supervision to enlarge the amount of\ntraining data for fake news detection. The proposed framework consists of three\nmain components: the annotator, the reinforced selector and the fake news\ndetector. The annotator can automatically assign weak labels for unlabeled news\nbased on users' reports. The reinforced selector using reinforcement learning\ntechniques chooses high-quality samples from the weakly labeled data and\nfilters out those low-quality ones that may degrade the detector's prediction\nperformance. The fake news detector aims to identify fake news based on the\nnews content. We tested the proposed framework on a large collection of news\narticles published via WeChat official accounts and associated user reports.\nExtensive experiments on this dataset show that the proposed WeFEND model\nachieves the best performance compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:20:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 03:03:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Yaqing", ""], ["Yang", "Weifeng", ""], ["Ma", "Fenglong", ""], ["Xu", "Jin", ""], ["Zhong", "Bin", ""], ["Deng", "Qiang", ""], ["Gao", "Jing", ""]]}, {"id": "1912.12598", "submitter": "Dheeru Dua", "authors": "Dheeru Dua, Ananth Gottumukkala, Alon Talmor, Sameer Singh, and Matt\n  Gardner", "title": "ORB: An Open Reading Benchmark for Comprehensive Evaluation of Machine\n  Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading comprehension is one of the crucial tasks for furthering research in\nnatural language understanding. A lot of diverse reading comprehension datasets\nhave recently been introduced to study various phenomena in natural language,\nranging from simple paraphrase matching and entity typing to entity tracking\nand understanding the implications of the context. Given the availability of\nmany such datasets, comprehensive and reliable evaluation is tedious and\ntime-consuming for researchers working on this problem. We present an\nevaluation server, ORB, that reports performance on seven diverse reading\ncomprehension datasets, encouraging and facilitating testing a single model's\ncapability in understanding a wide variety of reading phenomena. The evaluation\nserver places no restrictions on how models are trained, so it is a suitable\ntest bed for exploring training paradigms and representation learning for\ngeneral reading facility. As more suitable datasets are released, they will be\nadded to the evaluation server. We also collect and include synthetic\naugmentations for these datasets, testing how well models can handle\nout-of-domain questions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 07:27:23 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Dua", "Dheeru", ""], ["Gottumukkala", "Ananth", ""], ["Talmor", "Alon", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""]]}, {"id": "1912.12602", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Baris Bozkurt, Thierry Dutoit", "title": "Complex Cepstrum-based Decomposition of Speech for Glottal Source\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homomorphic analysis is a well-known method for the separation of\nnon-linearly combined signals. More particularly, the use of complex cepstrum\nfor source-tract deconvolution has been discussed in various articles. However\nthere exists no study which proposes a glottal flow estimation methodology\nbased on cepstrum and reports effective results. In this paper, we show that\ncomplex cepstrum can be effectively used for glottal flow estimation by\nseparating the causal and anticausal components of a windowed speech signal as\ndone by the Zeros of the Z-Transform (ZZT) decomposition. Based on exactly the\nsame principles presented for ZZT decomposition, windowing should be applied\nsuch that the windowed speech signals exhibit mixed-phase characteristics which\nconform the speech production model that the anticausal component is mainly due\nto the glottal flow open phase. The advantage of the complex cepstrum-based\napproach compared to the ZZT decomposition is its much higher speed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 07:58:18 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Drugman", "Thomas", ""], ["Bozkurt", "Baris", ""], ["Dutoit", "Thierry", ""]]}, {"id": "1912.12604", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Paavo Alku, Abeer Alwan, Bayya Yegnanarayana", "title": "Glottal Source Processing: from Analysis to Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The great majority of current voice technology applications relies on\nacoustic features characterizing the vocal tract response, such as the widely\nused MFCC of LPC parameters. Nonetheless, the airflow passing through the vocal\nfolds, and called glottal flow, is expected to exhibit a relevant\ncomplementarity. Unfortunately, glottal analysis from speech recordings\nrequires specific and more complex processing operations, which explains why it\nhas been generally avoided. This review gives a general overview of techniques\nwhich have been designed for glottal source processing. Starting from\nfundamental analysis tools of pitch tracking, glottal closure instant\ndetection, glottal flow estimation and modelling, this paper then highlights\nhow these solutions can be properly integrated within various voice technology\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 08:13:58 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Drugman", "Thomas", ""], ["Alku", "Paavo", ""], ["Alwan", "Abeer", ""], ["Yegnanarayana", "Bayya", ""]]}, {"id": "1912.12628", "submitter": "Jose Mena Rold\\'an", "authors": "Jos\\'e Mena, Oriol Pujol, Jordi Vitri\\`a", "title": "Dirichlet uncertainty wrappers for actionable algorithm accuracy\n  accountability and auditability", "comments": "13 pages, 5 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the use of machine learning models is becoming a utility in many\napplications. Companies deliver pre-trained models encapsulated as application\nprogramming interfaces (APIs) that developers combine with third party\ncomponents and their own models and data to create complex data products to\nsolve specific problems. The complexity of such products and the lack of\ncontrol and knowledge of the internals of each component used cause unavoidable\neffects, such as lack of transparency, difficulty in auditability, and\nemergence of potential uncontrolled risks. They are effectively black-boxes.\nAccountability of such solutions is a challenge for the auditors and the\nmachine learning community. In this work, we propose a wrapper that given a\nblack-box model enriches its output prediction with a measure of uncertainty.\nBy using this wrapper, we make the black-box auditable for the accuracy risk\n(risk derived from low quality or uncertain decisions) and at the same time we\nprovide an actionable mechanism to mitigate that risk in the form of decision\nrejection; we can choose not to issue a prediction when the risk or uncertainty\nin that decision is significant. Based on the resulting uncertainty measure, we\nadvocate for a rejection system that selects the more confident predictions,\ndiscarding those more uncertain, leading to an improvement in the trustability\nof the resulting system. We showcase the proposed technique and methodology in\na practical scenario where a simulated sentiment analysis API based on natural\nlanguage processing is applied to different domains. Results demonstrate the\neffectiveness of the uncertainty computed by the wrapper and its high\ncorrelation to bad quality predictions and misclassifications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:05:47 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mena", "Jos\u00e9", ""], ["Pujol", "Oriol", ""], ["Vitri\u00e0", "Jordi", ""]]}, {"id": "1912.12635", "submitter": "Konstantinos Kogkalidis", "authors": "Konstantinos Kogkalidis and Michael Moortgat and Richard Moot", "title": "\\AE THEL: Automatically Extracted Typelogical Derivations for Dutch", "comments": "8 pages plus abstract, LREC 2020", "journal-ref": "Proceedings of The 12th Language Resources and Evaluation\n  Conference (2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present {\\AE}THEL, a semantic compositionality dataset for written Dutch.\n{\\AE}THEL consists of two parts. First, it contains a lexicon of supertags for\nabout 900 000 words in context. The supertags correspond to types of the simply\ntyped linear lambda-calculus, enhanced with dependency decorations that capture\ngrammatical roles supplementary to function-argument structures. On the basis\nof these types, {\\AE}THEL further provides 72 192 validated derivations,\npresented in four formats: natural-deduction and sequent-style proofs, linear\nlogic proofnets and the associated programs (lambda terms) for meaning\ncomposition. {\\AE}THEL's types and derivations are obtained by means of an\nextraction algorithm applied to the syntactic analyses of LASSY Small, the gold\nstandard corpus of written Dutch. We discuss the extraction algorithm and show\nhow `virtual elements' in the original LASSY annotation of unbounded\ndependencies and coordination phenomena give rise to higher-order types. We\nsuggest some example usecases highlighting the benefits of a type-driven\napproach at the syntax semantics interface. The following resources are\nopen-sourced with {\\AE}THEL: the lexical mappings between words and types, a\nsubset of the dataset consisting of 7 924 semantic parses, and the Python code\nthat implements the extraction algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:31:11 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 15:26:59 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Kogkalidis", "Konstantinos", ""], ["Moortgat", "Michael", ""], ["Moot", "Richard", ""]]}, {"id": "1912.12800", "submitter": "Varun Gangal", "authors": "Varun Gangal, Abhinav Arora, Arash Einolghozati, Sonal Gupta", "title": "Likelihood Ratios and Generative Classifiers for Unsupervised\n  Out-of-Domain Detection In Task Oriented Dialog", "comments": "Accepted for AAAI-2020 Main Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of identifying out-of-domain (OOD) input examples directly at\ntest-time has seen renewed interest recently due to increased real world\ndeployment of models. In this work, we focus on OOD detection for natural\nlanguage sentence inputs to task-based dialog systems. Our findings are\nthree-fold: First, we curate and release ROSTD (Real Out-of-Domain Sentences\nFrom Task-oriented Dialog) - a dataset of 4K OOD examples for the publicly\navailable dataset from (Schuster et al. 2019). In contrast to existing settings\nwhich synthesize OOD examples by holding out a subset of classes, our examples\nwere authored by annotators with apriori instructions to be out-of-domain with\nrespect to the sentences in an existing dataset. Second, we explore likelihood\nratio based approaches as an alternative to currently prevalent paradigms.\nSpecifically, we reformulate and apply these approaches to natural language\ninputs. We find that they match or outperform the latter on all datasets, with\nlarger improvements on non-artificial OOD benchmarks such as our dataset. Our\nablations validate that specifically using likelihood ratios rather than plain\nlikelihood is necessary to discriminate well between OOD and in-domain data.\nThird, we propose learning a generative classifier and computing a marginal\nlikelihood (ratio) for OOD detection. This allows us to use a principled\nlikelihood while at the same time exploiting training-time labels. We find that\nthis approach outperforms both simple likelihood (ratio) based and other prior\napproaches. We are hitherto the first to investigate the use of generative\nclassifiers for OOD detection at test-time.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 03:31:17 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gangal", "Varun", ""], ["Arora", "Abhinav", ""], ["Einolghozati", "Arash", ""], ["Gupta", "Sonal", ""]]}, {"id": "1912.12843", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Baris Bozkurt, Thierry Dutoit", "title": "Causal-Anticausal Decomposition of Speech using Complex Cepstrum for\n  Glottal Source Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex cepstrum is known in the literature for linearly separating causal\nand anticausal components. Relying on advances achieved by the Zeros of the\nZ-Transform (ZZT) technique, we here investigate the possibility of using\ncomplex cepstrum for glottal flow estimation on a large-scale database. Via a\nsystematic study of the windowing effects on the deconvolution quality, we show\nthat the complex cepstrum causal-anticausal decomposition can be effectively\nused for glottal flow estimation when specific windowing criteria are met. It\nis also shown that this complex cepstral decomposition gives similar glottal\nestimates as obtained with the ZZT method. However, as complex cepstrum uses\nFFT operations instead of requiring the factoring of high-degree polynomials,\nthe method benefits from a much higher speed. Finally in our tests on a large\ncorpus of real expressive speech, we show that the proposed method has the\npotential to be used for voice quality analysis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 08:12:03 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Drugman", "Thomas", ""], ["Bozkurt", "Baris", ""], ["Dutoit", "Thierry", ""]]}, {"id": "1912.12887", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Alexis Moinet, Thierry Dutoit, Geoffrey Wilfart", "title": "Using a Pitch-Synchronous Residual Codebook for Hybrid HMM/Frame\n  Selection Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to improve the quality delivered by statistical\nparametric speech synthesizers. For this, we use a codebook of\npitch-synchronous residual frames, so as to construct a more realistic source\nsignal. First a limited codebook of typical excitations is built from some\ntraining database. During the synthesis part, HMMs are used to generate filter\nand source coefficients. The latter coefficients contain both the pitch and a\ncompact representation of target residual frames. The source signal is obtained\nby concatenating excitation frames picked up from the codebook, based on a\nselection criterion and taking target residual coefficients as input.\nSubjective results show a relevant improvement compared to the basic technique.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 11:34:39 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Drugman", "Thomas", ""], ["Moinet", "Alexis", ""], ["Dutoit", "Thierry", ""], ["Wilfart", "Geoffrey", ""]]}, {"id": "1912.12999", "submitter": "Laura Kinkead", "authors": "Laura Kinkead, Ahmed Allam, Michael Krauthammer", "title": "AutoDiscern: Rating the Quality of Online Health Information with\n  Hierarchical Encoder Attention-based Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients increasingly turn to search engines and online content before, or in\nplace of, talking with a health professional. Low quality health information,\nwhich is common on the internet, presents risks to the patient in the form of\nmisinformation and a possibly poorer relationship with their physician. To\naddress this, the DISCERN criteria (developed at University of Oxford) are used\nto evaluate the quality of online health information. However, patients are\nunlikely to take the time to apply these criteria to the health websites they\nvisit. We built an automated implementation of the DISCERN instrument (Brief\nversion) using machine learning models. We compared the performance of a\ntraditional model (Random Forest) with that of a hierarchical encoder\nattention-based neural network (HEA) model using two language embeddings, BERT\nand BioBERT. The HEA BERT and BioBERT models achieved average F1-macro scores\nacross all criteria of 0.75 and 0.74, respectively, outperforming the Random\nForest model (average F1-macro = 0.69). Overall, the neural network based\nmodels achieved 81% and 86% average accuracy at 100% and 80% coverage,\nrespectively, compared to 94% manual rating accuracy. The attention mechanism\nimplemented in the HEA architectures not only provided 'model explainability'\nby identifying reasonable supporting sentences for the documents fulfilling the\nBrief DISCERN criteria, but also boosted F1 performance by 0.05 compared to the\nsame architecture without an attention mechanism. Our research suggests that it\nis feasible to automate online health information quality assessment, which is\nan important step towards empowering patients to become informed partners in\nthe healthcare process.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 16:44:41 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 13:52:19 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 16:01:39 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kinkead", "Laura", ""], ["Allam", "Ahmed", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1912.13072", "submitter": "Chiyu Zhang", "authors": "Muhammad Abdul-Mageed, Chiyu Zhang, Azadeh Hashemi, El Moatez Billah\n  Nagoudi", "title": "AraNet: A Deep Learning Toolkit for Arabic Social Media", "comments": "Accepted by The 4th Workshop on Open-Source Arabic Corpora and\n  Processing Tools (OSACT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe AraNet, a collection of deep learning Arabic social media\nprocessing tools. Namely, we exploit an extensive host of publicly available\nand novel social media datasets to train bidirectional encoders from\ntransformer models (BERT) to predict age, dialect, gender, emotion, irony, and\nsentiment. AraNet delivers state-of-the-art performance on a number of the\ncited tasks and competitively on others. In addition, AraNet has the advantage\nof being exclusively based on a deep learning framework and hence feature\nengineering free. To the best of our knowledge, AraNet is the first to performs\npredictions across such a wide range of tasks for Arabic NLP and thus meets a\ncritical needs. We publicly release AraNet to accelerate research and\nfacilitate comparisons across the different tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:05:37 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 18:31:26 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Zhang", "Chiyu", ""], ["Hashemi", "Azadeh", ""], ["Nagoudi", "El Moatez Billah", ""]]}, {"id": "1912.13080", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Luca Soldaini, Nazli Goharian", "title": "Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using\n  Zero-shot Learning", "comments": "ECIR 2020 (short)", "journal-ref": null, "doi": "10.1007/978-3-030-45442-5_31", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While billions of non-English speaking users rely on search engines every\nday, the problem of ad-hoc information retrieval is rarely studied for\nnon-English languages. This is primarily due to a lack of data set that are\nsuitable to train ranking algorithms. In this paper, we tackle the lack of data\nby leveraging pre-trained multilingual language models to transfer a retrieval\nsystem trained on English collections to non-English queries and documents. Our\nmodel is evaluated in a zero-shot setting, meaning that we use them to predict\nrelevance scores for query-document pairs in languages never seen during\ntraining. Our results show that the proposed approach can significantly\noutperform unsupervised retrieval techniques for Arabic, Chinese Mandarin, and\nSpanish. We also show that augmenting the English training collection with some\nexamples from the target language can sometimes improve performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:46:38 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["MacAvaney", "Sean", ""], ["Soldaini", "Luca", ""], ["Goharian", "Nazli", ""]]}, {"id": "1912.13082", "submitter": "Makarand Tapaswi", "authors": "Atef Chaudhury, Makarand Tapaswi, Seung Wook Kim, Sanja Fidler", "title": "The Shmoop Corpus: A Dataset of Stories with Loosely Aligned Summaries", "comments": "Project page: http://www.cs.toronto.edu/~makarand/shmoop/ Dataset at:\n  https://github.com/achaudhury/shmoop-corpus/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding stories is a challenging reading comprehension problem for\nmachines as it requires reading a large volume of text and following long-range\ndependencies. In this paper, we introduce the Shmoop Corpus: a dataset of 231\nstories that are paired with detailed multi-paragraph summaries for each\nindividual chapter (7,234 chapters), where the summary is chronologically\naligned with respect to the story chapter. From the corpus, we construct a set\nof common NLP tasks, including Cloze-form question answering and a simplified\nform of abstractive summarization, as benchmarks for reading comprehension on\nstories. We then show that the chronological alignment provides a strong\nsupervisory signal that learning-based methods can exploit leading to\nsignificant improvements on these tasks. We believe that the unique structure\nof this corpus provides an important foothold towards making machine story\ncomprehension more approachable.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 21:03:59 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 16:06:48 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chaudhury", "Atef", ""], ["Tapaswi", "Makarand", ""], ["Kim", "Seung Wook", ""], ["Fidler", "Sanja", ""]]}, {"id": "1912.13106", "submitter": "Xiaotong Liu", "authors": "Xiaotong Liu, Yingbei Tong, Anbang Xu, Rama Akkiraju", "title": "An Empirical Study of Factors Affecting Language-Independent Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling existing applications and solutions to multiple human languages has\ntraditionally proven to be difficult, mainly due to the language-dependent\nnature of preprocessing and feature engineering techniques employed in\ntraditional approaches. In this work, we empirically investigate the factors\naffecting language-independent models built with multilingual representations,\nincluding task type, language set and data resource. On two most representative\nNLP tasks -- sentence classification and sequence labeling, we show that\nlanguage-independent models can be comparable to or even outperforms the models\ntrained using monolingual data, and they are generally more effective on\nsentence classification. We experiment language-independent models with many\ndifferent languages and show that they are more suitable for typologically\nsimilar languages. We also explore the effects of different data sizes when\ntraining and testing language-independent models, and demonstrate that they are\nnot only suitable for high-resource languages, but also very effective in\nlow-resource languages.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 22:41:57 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Liu", "Xiaotong", ""], ["Tong", "Yingbei", ""], ["Xu", "Anbang", ""], ["Akkiraju", "Rama", ""]]}, {"id": "1912.13109", "submitter": "Vivek Gupta", "authors": "Vivek Kumar Gupta", "title": "\"Hinglish\" Language -- Modeling a Messy Code-Mixed Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a sharp rise in fluency and users of \"Hinglish\" in linguistically\ndiverse country, India, it has increasingly become important to analyze social\ncontent written in this language in platforms such as Twitter, Reddit,\nFacebook. This project focuses on using deep learning techniques to tackle a\nclassification problem in categorizing social content written in Hindi-English\ninto Abusive, Hate-Inducing and Not offensive categories. We utilize\nbi-directional sequence models with easy text augmentation techniques such as\nsynonym replacement, random insertion, random swap, and random deletion to\nproduce a state of the art classifier that outperforms the previous work done\non analyzing this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 23:01:28 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gupta", "Vivek Kumar", ""]]}, {"id": "1912.13149", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Dev Chauhan, Vinod K. Kurmi, Vinay P. Namboodiri", "title": "Revisiting Paraphrase Question Generator using Pairwise Discriminator", "comments": "This work is an extension of our COLING-2018 paper arXiv:1806.00807", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a method for obtaining sentence-level embeddings.\nWhile the problem of securing word-level embeddings is very well studied, we\npropose a novel method for obtaining sentence-level embeddings. This is\nobtained by a simple method in the context of solving the paraphrase generation\ntask. If we use a sequential encoder-decoder model for generating paraphrase,\nwe would like the generated paraphrase to be semantically close to the original\nsentence. One way to ensure this is by adding constraints for true paraphrase\nembeddings to be close and unrelated paraphrase candidate sentence embeddings\nto be far. This is ensured by using a sequential pair-wise discriminator that\nshares weights with the encoder that is trained with a suitable loss function.\nOur loss function penalizes paraphrase sentence embedding distances from being\ntoo large. This loss is used in combination with a sequential encoder-decoder\nnetwork. We also validated our method by evaluating the obtained embeddings for\na sentiment analysis task. The proposed method results in semantic embeddings\nand outperforms the state-of-the-art on the paraphrase generation and sentiment\nanalysis task on standard datasets. These results are also shown to be\nstatistically significant.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 02:46:29 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 14:06:19 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Patro", "Badri N.", ""], ["Chauhan", "Dev", ""], ["Kurmi", "Vinod K.", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1912.13161", "submitter": "Ibrahim Gashaw", "authors": "Ibrahim Gashaw and H L Shashirekha", "title": "Amharic-Arabic Neural Machine Translation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automatic translation works have been addressed between major European\nlanguage pairs, by taking advantage of large scale parallel corpora, but very\nfew research works are conducted on the Amharic-Arabic language pair due to its\nparallel data scarcity. Two Long Short-Term Memory (LSTM) and Gated Recurrent\nUnits (GRU) based Neural Machine Translation (NMT) models are developed using\nAttention-based Encoder-Decoder architecture which is adapted from the\nopen-source OpenNMT system. In order to perform the experiment, a small\nparallel Quranic text corpus is constructed by modifying the existing\nmonolingual Arabic text and its equivalent translation of Amharic language text\ncorpora available on Tanzile. LSTM and GRU based NMT models and Google\nTranslation system are compared and found that LSTM based OpenNMT outperforms\nGRU based OpenNMT and Google Translation system, with a BLEU score of 12%, 11%,\nand 6% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 15:41:35 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gashaw", "Ibrahim", ""], ["Shashirekha", "H L", ""]]}, {"id": "1912.13194", "submitter": "Jialong Han", "authors": "Jialong Han, Aixin Sun, Haisong Zhang, Chenliang Li, Shuming Shi", "title": "CASE: Context-Aware Semantic Expansion", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define and study a new task called Context-Aware Semantic\nExpansion (CASE). Given a seed term in a sentential context, we aim to suggest\nother terms that well fit the context as the seed. CASE has many interesting\napplications such as query suggestion, computer-assisted writing, and word\nsense disambiguation, to name a few. Previous explorations, if any, only\ninvolve some similar tasks, and all require human annotations for evaluation.\nIn this study, we demonstrate that annotations for this task can be harvested\nat scale from existing corpora, in a fully automatic manner. On a dataset of\n1.8 million sentences thus derived, we propose a network architecture that\nencodes the context and seed term separately before suggesting alternative\nterms. The context encoder in this architecture can be easily extended by\nincorporating seed-aware attention. Our experiments demonstrate that\ncompetitive results are achieved with appropriate choices of context encoder\nand attention scoring function.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:38:57 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Han", "Jialong", ""], ["Sun", "Aixin", ""], ["Zhang", "Haisong", ""], ["Li", "Chenliang", ""], ["Shi", "Shuming", ""]]}, {"id": "1912.13283", "submitter": "Alon Talmor", "authors": "Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant", "title": "oLMpics -- On what Language Model Pre-training Captures", "comments": "TACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent success of pre-trained language models (LMs) has spurred widespread\ninterest in the language capabilities that they possess. However, efforts to\nunderstand whether LM representations are useful for symbolic reasoning tasks\nhave been limited and scattered. In this work, we propose eight reasoning\ntasks, which conceptually require operations such as comparison, conjunction,\nand composition. A fundamental challenge is to understand whether the\nperformance of a LM on a task should be attributed to the pre-trained\nrepresentations or to the process of fine-tuning on the task data. To address\nthis, we propose an evaluation protocol that includes both zero-shot evaluation\n(no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to\nthe learning curve of multiple controls, which paints a rich picture of the LM\ncapabilities. Our main findings are that: (a) different LMs exhibit\nqualitatively different reasoning abilities, e.g., RoBERTa succeeds in\nreasoning tasks where BERT fails completely; (b) LMs do not reason in an\nabstract manner and are context-dependent, e.g., while RoBERTa can compare\nages, it can do so only when the ages are in the typical range of human ages;\n(c) On half of our reasoning tasks all models fail completely. Our findings and\ninfrastructure can help future work on designing new datasets, models and\nobjective functions for pre-training.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:11:35 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 08:24:06 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Talmor", "Alon", ""], ["Elazar", "Yanai", ""], ["Goldberg", "Yoav", ""], ["Berant", "Jonathan", ""]]}, {"id": "1912.13318", "submitter": "Lei Cui", "authors": "Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou", "title": "LayoutLM: Pre-training of Text and Layout for Document Image\n  Understanding", "comments": "KDD 2020", "journal-ref": null, "doi": "10.1145/3394486.3403172", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training techniques have been verified successfully in a variety of NLP\ntasks in recent years. Despite the widespread use of pre-training models for\nNLP applications, they almost exclusively focus on text-level manipulation,\nwhile neglecting layout and style information that is vital for document image\nunderstanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model\ninteractions between text and layout information across scanned document\nimages, which is beneficial for a great number of real-world document image\nunderstanding tasks such as information extraction from scanned documents.\nFurthermore, we also leverage image features to incorporate words' visual\ninformation into LayoutLM. To the best of our knowledge, this is the first time\nthat text and layout are jointly learned in a single framework for\ndocument-level pre-training. It achieves new state-of-the-art results in\nseveral downstream tasks, including form understanding (from 70.72 to 79.27),\nreceipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly\navailable at \\url{https://aka.ms/layoutlm}.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 14:31:29 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 06:35:54 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 13:00:42 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 05:37:32 GMT"}, {"version": "v5", "created": "Tue, 16 Jun 2020 09:52:05 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Xu", "Yiheng", ""], ["Li", "Minghao", ""], ["Cui", "Lei", ""], ["Huang", "Shaohan", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "1912.13321", "submitter": "Xavier Marjou", "authors": "Xavier Marjou", "title": "OTEANN: Estimating the Transparency of Orthographies with an Artificial\n  Neural Network", "comments": "11 pages, 9 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To transcribe spoken language to written medium, most alphabets enable an\nunambiguous sound-to-letter rule. However, some writing systems have distanced\nthemselves from this simple concept and little work exists on measuring such\ndistance. In this study, we use an Artificial Neural Network (ANN) model to\nevaluate the transparency between written words and their pronunciation, hence\nits name Orthographic Transparency Estimation with an ANN (OTEANN). Based on\ndatasets derived from Wikimedia dictionaries, we trained and tested this model\nto score the percentage of false predictions in phoneme-to-grapheme and\ngrapheme-to-phoneme translation tasks. The scores obtained on 17 orthographies\nwere in line with the estimations of other studies. Interestingly, the model\nalso provided insight into typical mistakes made by learners who only consider\nthe phonemic rule in reading and writing.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 14:36:45 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 06:03:39 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 09:46:00 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Marjou", "Xavier", ""]]}, {"id": "1912.13337", "submitter": "Kyle Richardson", "authors": "Kyle Richardson and Ashish Sabharwal", "title": "What Does My QA Model Know? Devising Controlled Probes using Expert\n  Knowledge", "comments": "TACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain question answering (QA) is known to involve several underlying\nknowledge and reasoning challenges, but are models actually learning such\nknowledge when trained on benchmark tasks? To investigate this, we introduce\nseveral new challenge tasks that probe whether state-of-the-art QA models have\ngeneral knowledge about word definitions and general taxonomic reasoning, both\nof which are fundamental to more complex forms of reasoning and are widespread\nin benchmark datasets. As an alternative to expensive crowd-sourcing, we\nintroduce a methodology for automatically building datasets from various types\nof expert knowledge (e.g., knowledge graphs and lexical taxonomies), allowing\nfor systematic control over the resulting probes and for a more comprehensive\nevaluation. We find automatically constructing probes to be vulnerable to\nannotation artifacts, which we carefully control for. Our evaluation confirms\nthat transformer-based QA models are already predisposed to recognize certain\ntypes of structural lexical knowledge. However, it also reveals a more nuanced\npicture: their performance degrades substantially with even a slight increase\nin the number of hops in the underlying taxonomic hierarchy, or as more\nchallenging distractor candidate answers are introduced. Further, even when\nthese models succeed at the standard instance-level evaluation, they leave much\nroom for improvement when assessed at the level of clusters of semantically\nconnected probes (e.g., all Isa questions about a concept).\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:05:54 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 22:24:44 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Richardson", "Kyle", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "1912.13362", "submitter": "Behnam Kiani Kalejahi", "authors": "Umid Suleymanov, Behnam Kiani Kalejahi, Elkhan Amrahov, Rashid\n  Badirkhanli", "title": "Text Classification for Azerbaijani Language Using Machine Learning and\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text classification systems will help to solve the text clustering problem in\nthe Azerbaijani language. There are some text-classification applications for\nforeign languages, but we tried to build a newly developed system to solve this\nproblem for the Azerbaijani language. Firstly, we tried to find out potential\npractice areas. The system will be useful in a lot of areas. It will be mostly\nused in news feed categorization. News websites can automatically categorize\nnews into classes such as sports, business, education, science, etc. The system\nis also used in sentiment analysis for product reviews. For example, the\ncompany shares a photo of a new product on Facebook and the company receives a\nthousand comments for new products. The systems classify the comments into\ncategories like positive or negative. The system can also be applied in\nrecommended systems, spam filtering, etc. Various machine learning techniques\nsuch as Naive Bayes, SVM, Decision Trees have been devised to solve the text\nclassification problem in Azerbaijani language.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:38:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Suleymanov", "Umid", ""], ["Kalejahi", "Behnam Kiani", ""], ["Amrahov", "Elkhan", ""], ["Badirkhanli", "Rashid", ""]]}, {"id": "1912.13413", "submitter": "Maxat Tezekbayev", "authors": "Maxat Tezekbayev, Zhenisbek Assylbekov, Rustem Takhanov", "title": "Semantics- and Syntax-related Subvectors in the Skip-gram Embeddings", "comments": "2 pages, 1 figure, Student Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the skip-gram embedding of any word can be decomposed into two\nsubvectors which roughly correspond to semantic and syntactic roles of the\nword.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:01:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Tezekbayev", "Maxat", ""], ["Assylbekov", "Zhenisbek", ""], ["Takhanov", "Rustem", ""]]}, {"id": "1912.13415", "submitter": "John Giorgi", "authors": "John Giorgi, Xindi Wang, Nicola Sahar, Won Young Shin, Gary D. Bader,\n  Bo Wang", "title": "End-to-end Named Entity Recognition and Relation Extraction using\n  Pre-trained Language Models", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Named entity recognition (NER) and relation extraction (RE) are two important\ntasks in information extraction and retrieval (IE \\& IR). Recent work has\ndemonstrated that it is beneficial to learn these tasks jointly, which avoids\nthe propagation of error inherent in pipeline-based systems and improves\nperformance. However, state-of-the-art joint models typically rely on external\nnatural language processing (NLP) tools, such as dependency parsers, limiting\ntheir usefulness to domains (e.g. news) where those tools perform well. The few\nneural, end-to-end models that have been proposed are trained almost completely\nfrom scratch. In this paper, we propose a neural, end-to-end model for jointly\nextracting entities and their relations which does not rely on external NLP\ntools and which integrates a large, pre-trained language model. Because the\nbulk of our model's parameters are pre-trained and we eschew recurrence for\nself-attention, our model is fast to train. On 5 datasets across 3 domains, our\nmodel matches or exceeds state-of-the-art performance, sometimes by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:47:56 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Giorgi", "John", ""], ["Wang", "Xindi", ""], ["Sahar", "Nicola", ""], ["Shin", "Won Young", ""], ["Bader", "Gary D.", ""], ["Wang", "Bo", ""]]}, {"id": "1912.13455", "submitter": "Christoph Treude", "authors": "Sarah Nadi, Christoph Treude", "title": "Essential Sentences for Navigating Stack Overflow Answers", "comments": "to appear as full paper at SANER 2020, the 27th IEEE International\n  Conference on Software Analysis, Evolution and Reengineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack Overflow (SO) has become an essential resource for software\ndevelopment. Despite its success and prevalence, navigating SO remains a\nchallenge. Ideally, SO users could benefit from highlighted navigational cues\nthat help them decide if an answer is relevant to their task and context. Such\nnavigational cues could be in the form of essential sentences that help the\nsearcher decide whether they want to read the answer or skip over it. In this\npaper, we compare four potential approaches for identifying essential\nsentences. We adopt two existing approaches and develop two new approaches\nbased on the idea that contextual information in a sentence (e.g., \"if using\nwindows\") could help identify essential sentences. We compare the four\ntechniques using a survey of 43 participants. Our participants indicate that it\nis not always easy to figure out what the best solution for their specific\nproblem is, given the options, and that they would indeed like to easily spot\ncontextual information that may narrow down the search. Our quantitative\ncomparison of the techniques shows that there is no single technique sufficient\nfor identifying essential sentences that can serve as navigational cues, while\nour qualitative analysis shows that participants valued explanations and\nspecific conditions, and did not value filler sentences or speculations. Our\nwork sheds light on the importance of navigational cues, and our findings can\nbe used to guide future research to find the best combination of techniques to\nidentify such cues.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:52:05 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nadi", "Sarah", ""], ["Treude", "Christoph", ""]]}]