[{"id": "1407.0167", "submitter": "Moritz Schubotz", "authors": "Robert Pagael and Moritz Schubotz", "title": "Mathematical Language Processing Project", "comments": "8 pages, one figure, Conferences on Intelligent Computer Mathematics\n  (CICM) 2014", "journal-ref": null, "doi": null, "report-no": "urn:nbn:de:0074-1186-1", "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language, words and phrases themselves imply the semantics. In\ncontrast, the meaning of identifiers in mathematical formulae is undefined.\nThus scientists must study the context to decode the meaning. The Mathematical\nLanguage Processing (MLP) project aims to support that process. In this paper,\nwe compare two approaches to discover identifier-definition tuples. At first we\nuse a simple pattern matching approach. Second, we present the MLP approach\nthat uses part-of-speech tag based distances as well as sentence positions to\ncalculate identifier-definition probabilities. The evaluation of our\nprototypical system, applied on the Wikipedia text corpus, shows that our\napproach augments the user experience substantially. While hovering the\nidentifiers in the formula, tool-tips with the most probable definitions occur.\nTests with random samples show that the displayed definitions provide a good\nmatch with the actual meaning of the identifiers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 09:56:42 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Pagael", "Robert", ""], ["Schubotz", "Moritz", ""]]}, {"id": "1407.1165", "submitter": "Prashant  Borde", "authors": "Prashant Bordea, Amarsinh Varpeb, Ramesh Manzac, Pravin Yannawara", "title": "Recognition of Isolated Words using Zernike and MFCC features for Audio\n  Visual Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) by machine is an attractive research topic\nin signal processing domain and has attracted many researchers to contribute in\nthis area. In recent year, there have been many advances in automatic speech\nreading system with the inclusion of audio and visual speech features to\nrecognize words under noisy conditions. The objective of audio-visual speech\nrecognition system is to improve recognition accuracy. In this paper we\ncomputed visual features using Zernike moments and audio feature using Mel\nFrequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of\nIndependent Standard Words) dataset which contains collection of isolated set\nof city names of 10 speakers. The visual features were normalized and dimension\nof features set was reduced by Principal Component Analysis (PCA) in order to\nrecognize the isolated word utterance on PCA space.The performance of\nrecognition of isolated words based on visual only and audio only features\nresults in 63.88 and 100 respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 09:32:10 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Bordea", "Prashant", ""], ["Varpeb", "Amarsinh", ""], ["Manzac", "Ramesh", ""], ["Yannawara", "Pravin", ""]]}, {"id": "1407.1605", "submitter": "Denis Maurel", "authors": "\\'Emeline Lecuit (LLL), Denis Maurel (LI), Dusko Vitas", "title": "Les noms propres se traduisent-ils ? \\'Etude d'un corpus multilingue", "comments": "in French", "journal-ref": "Corpus 10 (2011) 201-218", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of the translation of proper names. We\nintroduce our hypothesis according to which proper names can be translated more\noften than most people seem to think. Then, we describe the construction of a\nparallel multilingual corpus used to illustrate our point. We eventually\nevaluate both the advantages and limits of this corpus in our study.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 07:08:07 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Lecuit", "\u00c9meline", "", "LLL"], ["Maurel", "Denis", "", "LI"], ["Vitas", "Dusko", ""]]}, {"id": "1407.1640", "submitter": "Bin Gao", "authors": "Bin Gao, Jiang Bian, and Tie-Yan Liu", "title": "WordRep: A Benchmark for Research on Learning Word Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  WordRep is a benchmark collection for the research on learning distributed\nword representations (or word embeddings), released by Microsoft Research. In\nthis paper, we describe the details of the WordRep collection and show how to\nuse it in different types of machine learning research related to word\nembedding. Specifically, we describe how the evaluation tasks in WordRep are\nselected, how the data are sampled, and how the evaluation tool is built. We\nthen compare several state-of-the-art word representations on WordRep, report\ntheir evaluation performance, and make discussions on the results. After that,\nwe discuss new potential research topics that can be supported by WordRep, in\naddition to algorithm comparison. We hope that this paper can help people gain\ndeeper understanding of WordRep, and enable more interesting research on\nlearning distributed word representations and related topics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 09:31:21 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1407.1687", "submitter": "Bin Gao", "authors": "Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, and Tie-Yan Liu", "title": "KNET: A General Framework for Learning Word Embedding using\n  Morphological Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Neural network techniques are widely applied to obtain high-quality\ndistributed representations of words, i.e., word embeddings, to address text\nmining, information retrieval, and natural language processing tasks. Recently,\nefficient methods have been proposed to learn word embeddings from context that\ncaptures both semantic and syntactic relationships between words. However, it\nis challenging to handle unseen words or rare words with insufficient context.\nIn this paper, inspired by the study on word recognition process in cognitive\npsychology, we propose to take advantage of seemingly less obvious but\nessentially important morphological knowledge to address these challenges. In\nparticular, we introduce a novel neural network architecture called KNET that\nleverages both contextual information and morphological word similarity built\nbased on morphological knowledge to learn word embeddings. Meanwhile, the\nlearning architecture is also able to refine the pre-defined morphological\nknowledge and obtain more accurate word similarity. Experiments on an\nanalogical reasoning task and a word similarity task both demonstrate that the\nproposed KNET framework can greatly enhance the effectiveness of word\nembeddings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 12:45:10 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 16:03:47 GMT"}, {"version": "v3", "created": "Fri, 5 Sep 2014 15:58:35 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Cui", "Qing", ""], ["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Qiu", "Siyu", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1407.1933", "submitter": "Adam Saulwick", "authors": "Adam Saulwick", "title": "Lexpresso: a Controlled Natural Language", "comments": "12 pages, 2 figures, 4th Workshop on Controlled Natural Language 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an overview of `Lexpresso', a Controlled Natural Language\ndeveloped at the Defence Science & Technology Organisation as a bidirectional\nnatural language interface to a high-level information fusion system. The paper\ndescribes Lexpresso's main features including lexical coverage, expressiveness\nand range of linguistic syntactic and semantic structures. It also touches on\nits tight integration with a formal semantic formalism and tentatively\nclassifies it against the PENS system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 02:53:29 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Saulwick", "Adam", ""]]}, {"id": "1407.1976", "submitter": "Shanta Phani", "authors": "Shanta Phani, Shibamouli Lahiri and Arindam Biswas", "title": "Inter-Rater Agreement Study on Readability Assessment in Bengali", "comments": "6 pages, 4 tables, Accepted in ICCONAC, 2014", "journal-ref": "International Journal on Natural Language Computing (IJNLC), 3(3),\n  2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An inter-rater agreement study is performed for readability assessment in\nBengali. A 1-7 rating scale was used to indicate different levels of\nreadability. We obtained moderate to fair agreement among seven independent\nannotators on 30 text passages written by four eminent Bengali authors. As a by\nproduct of our study, we obtained a readability-annotated ground truth dataset\nin Bengali. .\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 07:35:16 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Phani", "Shanta", ""], ["Lahiri", "Shibamouli", ""], ["Biswas", "Arindam", ""]]}, {"id": "1407.2019", "submitter": "Pranjal Das", "authors": "Kalyanee Kanchan Baruah, Pranjal Das, Abdul Hannan, Shikhar Kr. Sarma", "title": "Assamese-English Bilingual Machine Translation", "comments": "In the proceedings of International Conference of Natural Language\n  Processing and Cognitive Computing (ICONACC)-2014, pp. 227-231", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  3, No.3, June 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is the process of translating text from one language to\nanother. In this paper, Statistical Machine Translation is done on Assamese and\nEnglish language by taking their respective parallel corpus. A statistical\nphrase based translation toolkit Moses is used here. To develop the language\nmodel and to align the words we used two another tools IRSTLM, GIZA\nrespectively. BLEU score is used to check our translation system performance,\nhow good it is. A difference in BLEU scores is obtained while translating\nsentences from Assamese to English and vice-versa. Since Indian languages are\nmorphologically very rich hence translation is relatively harder from English\nto Assamese resulting in a low BLEU score. A statistical transliteration system\nis also introduced with our translation system to deal basically with proper\nnouns, OOV (out of vocabulary) words which are not present in our corpus.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 10:04:07 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Baruah", "Kalyanee Kanchan", ""], ["Das", "Pranjal", ""], ["Hannan", "Abdul", ""], ["Sarma", "Shikhar Kr.", ""]]}, {"id": "1407.2694", "submitter": "Pooja Gupta", "authors": "Pooja Gupta, Nisheeth Joshi, Iti Mathur", "title": "Quality Estimation Of Machine Translation Outputs Through Stemming", "comments": null, "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.4, No.3, June 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Machine Translation is the challenging problem for Indian languages. Every\nday we can see some machine translators being developed, but getting a high\nquality automatic translation is still a very distant dream . The correct\ntranslated sentence for Hindi language is rarely found. In this paper, we are\nemphasizing on English-Hindi language pair, so in order to preserve the correct\nMT output we present a ranking system, which employs some machine learning\ntechniques and morphological features. In ranking no human intervention is\nrequired. We have also validated our results by comparing it with human\nranking.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 05:26:14 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Gupta", "Pooja", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1407.2918", "submitter": "Pranjal Protim  Borah", "authors": "Gitimoni Talukdar, Pranjal Protim Borah, Arup Baruah", "title": "A Survey of Named Entity Recognition in Assamese and other Indian\n  Languages", "comments": "ICONACC-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition is always important when dealing with major Natural\nLanguage Processing tasks such as information extraction, question-answering,\nmachine translation, document summarization etc so in this paper we put forward\na survey of Named Entities in Indian Languages with particular reference to\nAssamese. There are various rule-based and machine learning approaches\navailable for Named Entity Recognition. At the very first of the paper we give\nan idea of the available approaches for Named Entity Recognition and then we\ndiscuss about the related research in this field. Assamese like other Indian\nlanguages is agglutinative and suffers from lack of appropriate resources as\nNamed Entity Recognition requires large data sets, gazetteer list, dictionary\netc and some useful feature like capitalization as found in English cannot be\nfound in Assamese. Apart from this we also describe some of the issues faced in\nAssamese while doing Named Entity Recognition.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 13:59:27 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Talukdar", "Gitimoni", ""], ["Borah", "Pranjal Protim", ""], ["Baruah", "Arup", ""]]}, {"id": "1407.2989", "submitter": "Manoj Jayaweera", "authors": "A.J.P.M.P. Jayaweera and N.G.J. Dias", "title": "Hidden Markov Model Based Part of Speech Tagger for Sinhala Language", "comments": "This paper contains 15 Pages, the paper was presented at ICONACC\n  2014, organized by Manipur University, Imphal, India", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  3, No.3, June 2014. Pages 9-23", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fundamental lexical semantics of Sinhala language\nand a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala\nlanguage. In any Natural Language processing task, Part of Speech is a very\nvital topic, which involves analysing of the construction, behaviour and the\ndynamics of the language, which the knowledge could utilized in computational\nlinguistics analysis and automation applications. Though Sinhala is a\nmorphologically rich and agglutinative language, in which words are inflected\nwith various grammatical features, tagging is very essential for further\nanalysis of the language. Our research is based on statistical based approach,\nin which the tagging process is done by computing the tag sequence probability\nand the word-likelihood probability from the given corpus, where the linguistic\nknowledge is automatically extracted from the annotated corpus. The current\ntagger could reach more than 90% of accuracy for known words.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 23:57:54 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Jayaweera", "A. J. P. M. P.", ""], ["Dias", "N. G. J.", ""]]}, {"id": "1407.3636", "submitter": "Ana Mestrovic", "authors": "Sabina \\v{S}i\\v{s}ovi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c and Ana\n  Me\\v{s}trovi\\'c", "title": "Toward Network-based Keyword Extraction from Multitopic Web Documents", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse the selectivity measure calculated from the complex\nnetwork in the task of the automatic keyword extraction. Texts, collected from\ndifferent web sources (portals, forums), are represented as directed and\nweighted co-occurrence complex networks of words. Words are nodes and links are\nestablished between two nodes if they are directly co-occurring within the\nsentence. We test different centrality measures for ranking nodes - keyword\ncandidates. The promising results are achieved using the selectivity measure.\nThen we propose an approach which enables extracting word pairs according to\nthe values of the in/out selectivity and weight measures combined with\nfiltering.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 13:22:36 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["\u0160i\u0161ovi\u0107", "Sabina", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""], ["Me\u0161trovi\u0107", "Ana", ""]]}, {"id": "1407.3751", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Chase Dowling", "title": "Benchmarking Named Entity Disambiguation approaches for Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": "PNNL-23455 2014-05, Pacific Northwest National Laboratory, Richland,\n  WA", "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Named Entity Disambiaguation (NED) is a central task for applications dealing\nwith natural language text. Assume that we have a graph based knowledge base\n(subsequently referred as Knowledge Graph) where nodes represent various real\nworld entities such as people, location, organization and concepts. Given data\nsources such as social media streams and web pages Entity Linking is the task\nof mapping named entities that are extracted from the data to those present in\nthe Knowledge Graph. This is an inherently difficult task due to several\nreasons. Almost all these data sources are generated without any formal\nontology; the unstructured nature of the input, limited context and the\nambiguity involved when multiple entities are mapped to the same name make this\na hard task. This report looks at two state of the art systems employing two\ndistinctive approaches: graph based Accurate Online Disambiguation of Entities\n(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a\nstatistical inference approach. We compare both approaches using the data set\nand queries provided by the Knowledge Base Population (KBP) track at 2011 NIST\nText Analytics Conference (TAC). This report begins with an overview of the\nrespective approaches, followed by detailed description of the experimental\nsetup. It concludes with our findings from the benchmarking exercise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 18:01:08 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Dowling", "Chase", ""]]}, {"id": "1407.4610", "submitter": "Bernat Corominas-Murtra BCM", "authors": "Stefan Thurner, Rudolf Hanel, Bo Liu and Bernat Corominas-Murtra", "title": "Understanding Zipf's law of word frequencies through sample-space\n  collapse in sentence formation", "comments": "7 pages, 4 figures. Accepted for publication in the Journal of the\n  Royal Society Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The formation of sentences is a highly structured and history-dependent\nprocess. The probability of using a specific word in a sentence strongly\ndepends on the 'history' of word-usage earlier in that sentence. We study a\nsimple history-dependent model of text generation assuming that the\nsample-space of word usage reduces along sentence formation, on average. We\nfirst show that the model explains the approximate Zipf law found in word\nfrequencies as a direct consequence of sample-space reduction. We then\nempirically quantify the amount of sample-space reduction in the sentences of\nten famous English books, by analysis of corresponding word-transition tables\nthat capture which words can follow any given word in a text. We find a highly\nnested structure in these transition tables and show that this `nestedness' is\ntightly related to the power law exponents of the observed word frequency\ndistributions. With the proposed model it is possible to understand that the\nnestedness of a text can be the origin of the actual scaling exponent, and that\ndeviations from the exact Zipf law can be understood by variations of the\ndegree of nestedness on a book-by-book basis. On a theoretical level we are\nable to show that in case of weak nesting, Zipf's law breaks down in a fast\ntransition. Unlike previous attempts to understand Zipf's law in language the\nsample-space reducing model is not based on assumptions of multiplicative,\npreferential, or self-organised critical mechanisms behind language formation,\nbut simply used the empirically quantifiable parameter 'nestedness' to\nunderstand the statistics of word frequencies.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 09:38:07 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 07:42:38 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Thurner", "Stefan", ""], ["Hanel", "Rudolf", ""], ["Liu", "Bo", ""], ["Corominas-Murtra", "Bernat", ""]]}, {"id": "1407.4723", "submitter": "Ana Mestrovic", "authors": "Slobodan Beliga, Ana Me\\v{s}trovi\\'c, Sanda\n  Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Toward Selectivity Based Keyword Extraction for Croatian News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preliminary report on network based keyword extraction for Croatian is an\nunsupervised method for keyword extraction from the complex network. We build\nour approach with a new network measure the node selectivity, motivated by the\nresearch of the graph based centrality approaches. The node selectivity is\ndefined as the average weight distribution on the links of the single node. We\nextract nodes (keyword candidates) based on the selectivity value. Furthermore,\nwe expand extracted nodes to word-tuples ranked with the highest in/out\nselectivity values. Selectivity based extraction does not require linguistic\nknowledge while it is purely derived from statistical and structural\ninformation en-compassed in the source text which is reflected into the\nstructure of the network. Obtained sets are evaluated on a manually annotated\nkeywords: for the set of extracted keyword candidates average F1 score is\n24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates\naverage F1 score is 25,9% and average F2 score is 24,47%.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 16:12:04 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Beliga", "Slobodan", ""], ["Me\u0161trovi\u0107", "Ana", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1407.6027", "submitter": "Cristina Martinez Ramirez", "authors": "Alberto Besana, Cristina Mart\\'inez", "title": "Modeling languages from graph networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model and compute the probability distribution of the letters in random\ngenerated words in a language by using the theory of set partitions, Young\ntableaux and graph theoretical representation methods. This has been of\ninterest for several application areas such as network systems, bioinformatics,\ninternet search, data mining and computacional linguistics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 20:25:40 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Besana", "Alberto", ""], ["Mart\u00ednez", "Cristina", ""]]}, {"id": "1407.6099", "submitter": "Andrew Connor", "authors": "S.G. Macdonell, K. Min, A.M. Connor", "title": "Autonomous requirements specification processing using natural language\n  processing", "comments": "Proceedings of the ISCA 14th International Conferenceon Intelligent\n  and Adaptive Systems and Software Engineering (IASSE 2005)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our ongoing research that centres on the application of natural\nlanguage processing (NLP) to software engineering and systems development\nactivities. In particular, this paper addresses the use of NLP in the\nrequirements analysis and systems design processes. We have developed a\nprototype toolset that can assist the systems analyst or software engineer to\nselect and verify terms relevant to a project. In this paper we describe the\nprocesses employed by the system to extract and classify objects of interest\nfrom requirements documents. These processes are illustrated using a small\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 03:29:44 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Macdonell", "S. G.", ""], ["Min", "K.", ""], ["Connor", "A. M.", ""]]}, {"id": "1407.6439", "submitter": "Ce Zhang", "authors": "Christopher R\\'e, Amir Abbas Sadeghian, Zifei Shan, Jaeho Shin, Feiran\n  Wang, Sen Wu, Ce Zhang", "title": "Feature Engineering for Knowledge Base Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge base construction (KBC) is the process of populating a knowledge\nbase, i.e., a relational database together with inference rules, with\ninformation extracted from documents and structured sources. KBC blurs the\ndistinction between two traditional database problems, information extraction\nand information integration. For the last several years, our group has been\nbuilding knowledge bases with scientific collaborators. Using our approach, we\nhave built knowledge bases that have comparable and sometimes better quality\nthan those constructed by human volunteers. In contrast to these knowledge\nbases, which took experts a decade or more human years to construct, many of\nour projects are constructed by a single graduate student.\n  Our approach to KBC is based on joint probabilistic inference and learning,\nbut we do not see inference as either a panacea or a magic bullet: inference is\na tool that allows us to be systematic in how we construct, debug, and improve\nthe quality of such systems. In addition, inference allows us to construct\nthese systems in a more loosely coupled way than traditional approaches. To\nsupport this idea, we have built the DeepDive system, which has the design goal\nof letting the user \"think about features---not algorithms.\" We think of\nDeepDive as declarative in that one specifies what they want but not how to get\nit. We describe our approach with a focus on feature engineering, which we\nargue is an understudied problem relative to its importance to end-to-end\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 03:34:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 17:00:00 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 14:38:06 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["R\u00e9", "Christopher", ""], ["Sadeghian", "Amir Abbas", ""], ["Shan", "Zifei", ""], ["Shin", "Jaeho", ""], ["Wang", "Feiran", ""], ["Wu", "Sen", ""], ["Zhang", "Ce", ""]]}, {"id": "1407.6639", "submitter": "Torsten Timm", "authors": "Torsten Timm", "title": "How the Voynich Manuscript was created", "comments": "96 pages, 17 figures, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Voynich manuscript is a medieval book written in an unknown script. This\npaper studies the relation between similarly spelled words in the Voynich\nmanuscript. By means of a detailed analysis of similar spelled words it was\npossible to reveal the text generation method used for the Voynich manuscript.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 16:26:45 GMT"}, {"version": "v2", "created": "Fri, 12 Sep 2014 17:20:37 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 23:38:21 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Timm", "Torsten", ""]]}, {"id": "1407.6853", "submitter": "Volkan Cirik", "authors": "Volkan Cirik and Deniz Yuret", "title": "Substitute Based SCODE Word Embeddings in Supervised NLP Tasks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a word embedding method in supervised tasks. It maps words on a\nsphere such that words co-occurring in similar contexts lie closely. The\nsimilarity of contexts is measured by the distribution of substitutes that can\nfill them. We compared word embeddings, including more recent representations,\nin Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine\nour framework in multilingual dependency parsing as well. The results show that\nthe proposed method achieves as good as or better results compared to the other\nword embeddings in the tasks we investigate. It achieves state-of-the-art\nresults in multilingual dependency parsing. Word embeddings in 7 languages are\navailable for public use.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 11:17:28 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Cirik", "Volkan", ""], ["Yuret", "Deniz", ""]]}, {"id": "1407.6872", "submitter": "Ivan Ivek", "authors": "Ivan Ivek", "title": "Interpretable Low-Rank Document Representations with Label-Dependent\n  Sparsity Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In context of document classification, where in a corpus of documents their\nlabel tags are readily known, an opportunity lies in utilizing label\ninformation to learn document representation spaces with better discriminative\nproperties. To this end, in this paper application of a Variational Bayesian\nSupervised Nonnegative Matrix Factorization (supervised vbNMF) with\nlabel-driven sparsity structure of coefficients is proposed for learning of\ndiscriminative nonsubtractive latent semantic components occuring in TF-IDF\ndocument representations. Constraints are such that the components pursued are\nmade to be frequently occuring in a small set of labels only, making it\npossible to yield document representations with distinctive label-specific\nsparse activation patterns. A simple measure of quality of this kind of\nsparsity structure, dubbed inter-label sparsity, is introduced and\nexperimentally brought into tight connection with classification performance.\nRepresenting a great practical convenience, inter-label sparsity is shown to be\neasily controlled in supervised vbNMF by a single parameter.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 12:46:18 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Ivek", "Ivan", ""]]}, {"id": "1407.7094", "submitter": "Bruno Gon\\c{c}alves", "authors": "Bruno Gon\\c{c}alves and David S\\'anchez", "title": "Crowdsourcing Dialect Characterization through Twitter", "comments": "10 pages, 5 figures", "journal-ref": "PLoS One 9, E112074 (2014)", "doi": "10.1371/journal.pone.0112074", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a large-scale analysis of language diatopic variation using\ngeotagged microblogging datasets. By collecting all Twitter messages written in\nSpanish over more than two years, we build a corpus from which a carefully\nselected list of concepts allows us to characterize Spanish varieties on a\nglobal scale. A cluster analysis proves the existence of well defined\nmacroregions sharing common lexical properties. Remarkably enough, we find that\nSpanish language is split into two superdialects, namely, an urban speech used\nacross major American and Spanish citites and a diverse form that encompasses\nrural areas and small towns. The latter can be further clustered into smaller\nvarieties with a stronger regional character.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 04:16:31 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1407.7169", "submitter": "Matilde Marcolli", "authors": "Matilde Marcolli", "title": "Principles and Parameters: a coding theory perspective", "comments": "11 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to Longobardi's parametric comparison method (PCM) via\nthe theory of error-correcting codes. One associates to a collection of\nlanguages to be analyzed with the PCM a binary (or ternary) code with one code\nwords for each language in the family and each word consisting of the binary\nvalues of the syntactic parameters of the language, with the ternary case\nallowing for an additional parameter state that takes into account phenomena of\nentailment of parameters. The code parameters of the resulting code can be\ncompared with some classical bounds in coding theory: the asymptotic bound, the\nGilbert-Varshamov bound, etc. The position of the code parameters with respect\nto some of these bounds provides quantitative information on the variability of\nsyntactic parameters within and across historical-linguistic families. While\ncomputations carried out for languages belonging to the same family yield codes\nbelow the GV curve, comparisons across different historical families can give\nexamples of isolated codes lying above the asymptotic bound.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 23:48:32 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Marcolli", "Matilde", ""]]}, {"id": "1407.7357", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Philippe Lenca", "title": "Text Classification Using Association Rules, Dependency Pruning and\n  Hyperonymization", "comments": "16 pages, 2 figures, presented at DMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for pruning and enhancing item- sets for text\nclassification via association rule mining. Pruning methods are based on\ndependency syntax and enhancing methods are based on replacing words by their\nhyperonyms of various orders. We discuss the impact of these methods, compared\nto pruning based on tfidf rank of words.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 09:00:57 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Haralambous", "Yannis", ""], ["Lenca", "Philippe", ""]]}, {"id": "1407.7736", "submitter": "Xiangju Qin", "authors": "Xiangju Qin, Derek Greene, and P\\'adraig Cunningham", "title": "A Latent Space Analysis of Editor Lifecycles in Wikipedia", "comments": "16 pages, In Proc. of 5th International Workshop on Mining Ubiquitous\n  and Social Environments (MUSE) at ECML/PKDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborations such as Wikipedia are a key part of the value of the modern\nInternet. At the same time there is concern that these collaborations are\nthreatened by high levels of member turnover. In this paper we borrow ideas\nfrom topic analysis to editor activity on Wikipedia over time into a latent\nspace that offers an insight into the evolving patterns of editor behavior.\nThis latent space representation reveals a number of different categories of\neditor (e.g. content experts, social networkers) and we show that it does\nprovide a signal that predicts an editor's departure from the community. We\nalso show that long term editors gradually diversify their participation by\nshifting edit preference from one or two namespaces to multiple namespaces and\nexperience relatively soft evolution in their editor profiles, while short term\neditors generally distribute their contribution randomly among the namespaces\nand experience considerably fluctuated evolution in their editor profiles.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 14:25:41 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Qin", "Xiangju", ""], ["Greene", "Derek", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1407.8215", "submitter": "Vanessa Wei Feng Ms.", "authors": "Vanessa Wei Feng and Graeme Hirst", "title": "Two-pass Discourse Segmentation with Pairing and Global Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous attempts at RST-style discourse segmentation typically adopt\nfeatures centered on a single token to predict whether to insert a boundary\nbefore that token. In contrast, we develop a discourse segmenter utilizing a\nset of pairing features, which are centered on a pair of adjacent tokens in the\nsentence, by equally taking into account the information from both tokens.\nMoreover, we propose a novel set of global features, which encode\ncharacteristics of the segmentation as a whole, once we have an initial\nsegmentation. We show that both the pairing and global features are useful on\ntheir own, and their combination achieved an $F_1$ of 92.6% of identifying\nin-sentence discourse boundaries, which is a 17.8% error-rate reduction over\nthe state-of-the-art performance, approaching 95% of human performance. In\naddition, similar improvement is observed across different classification\nframeworks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 21:00:25 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Feng", "Vanessa Wei", ""], ["Hirst", "Graeme", ""]]}, {"id": "1407.8322", "submitter": "Ramon Ferrer i Cancho", "authors": "Alvaro Corral, Gemma Boleda and Ramon Ferrer-i-Cancho", "title": "Zipf's law for word frequencies: word forms versus lemmas in long texts", "comments": null, "journal-ref": "PLoS ONE 10 (7), e0129031", "doi": "10.1371/journal.pone.0129031", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law is a fundamental paradigm in the statistics of written and spoken\nnatural language as well as in other communication systems. We raise the\nquestion of the elementary units for which Zipf's law should hold in the most\nnatural way, studying its validity for plain word forms and for the\ncorresponding lemma forms. In order to have as homogeneous sources as possible,\nwe analyze some of the longest literary texts ever written, comprising four\ndifferent languages, with different levels of morphological complexity. In all\ncases Zipf's law is fulfilled, in the sense that a power-law distribution of\nword or lemma frequencies is valid for several orders of magnitude. We\ninvestigate the extent to which the word-lemma transformation preserves two\nparameters of Zipf's law: the exponent and the low-frequency cut-off. We are\nnot able to demonstrate a strict invariance of the tail, as for a few texts\nboth exponents deviate significantly, but we conclude that the exponents are\nvery similar, despite the remarkable transformation that going from words to\nlemmas represents, considerably affecting all ranges of frequencies. In\ncontrast, the low-frequency cut-offs are less stable.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 09:02:15 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 13:58:27 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Corral", "Alvaro", ""], ["Boleda", "Gemma", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}]