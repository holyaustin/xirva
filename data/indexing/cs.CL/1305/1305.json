[{"id": "1305.0194", "submitter": "Chantal Cherifi", "authors": "Cihan Aksoy, Vincent Labatut, Chantal Cherifi, Jean-Fran\\c{c}ois\n  Santucci", "title": "MATAWS: A Multimodal Approach for Automatic WS Semantic Annotation", "comments": null, "journal-ref": "In International Conference on Networked Digital Technologies,\n  Springer CCIS 136),China (2011)", "doi": "10.1007/978-3-642-22185-9_27", "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works aim at developing methods and tools for the processing of\nsemantic Web services. In order to be properly tested, these tools must be\napplied to an appropriate benchmark, taking the form of a collection of\nsemantic WS descriptions. However, all of the existing publicly available\ncollections are limited by their size or their realism (use of randomly\ngenerated or resampled descriptions). Larger and realistic syntactic (WSDL)\ncollections exist, but their semantic annotation requires a certain level of\nautomation, due to the number of operations to be processed. In this article,\nwe propose a fully automatic method to semantically annotate such large WS\ncollections. Our approach is multimodal, in the sense it takes advantage of the\nlatent semantics present not only in the parameter names, but also in the type\nnames and structures. Concept-to-word association is performed by using Sigma,\na mapping of WordNet to the SUMO ontology. After having described in details\nour annotation method, we apply it to the larger collection of real-world\nsyntactic WS descriptions we could find, and assess its efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 15:07:18 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Aksoy", "Cihan", ""], ["Labatut", "Vincent", ""], ["Cherifi", "Chantal", ""], ["Santucci", "Jean-Fran\u00e7ois", ""]]}, {"id": "1305.0556", "submitter": "Edward Grefenstette", "authors": "Stephen Clark, Bob Coecke, Edward Grefenstette, Stephen Pulman and\n  Mehrnoosh Sadrzadeh", "title": "A quantum teleportation inspired algorithm produces sentence meaning\n  from word meaning and grammatical structure", "comments": "10 pages, many pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an algorithm which produces the meaning of a sentence given\nmeanings of its words, and its resemblance to quantum teleportation. In fact,\nthis protocol was the main source of inspiration for this algorithm which has\nmany applications in the area of Natural Language Processing.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 18:49:01 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2013 17:18:51 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Clark", "Stephen", ""], ["Coecke", "Bob", ""], ["Grefenstette", "Edward", ""], ["Pulman", "Stephen", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1305.0625", "submitter": "Kamlesh Sharma", "authors": "Kamlesh Sharma, Dr. T. V. Prasad", "title": "CONATION: English Command Input/Output System for Computers", "comments": "6 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this information technology age, a convenient and user friendly interface\nis required to operate the computer system on very fast rate. In the human\nbeing, speech being a natural mode of communication has potential to being a\nfast and convenient mode of interaction with computer. Speech recognition will\nplay an important role in taking technology to them. It is the need of this era\nto access the information within seconds. This paper describes the design and\ndevelopment of speaker independent and English command interpreted system for\ncomputers. HMM model is used to represent the phoneme like speech commands.\nExperiments have been done on real world data and system has been trained in\nnormal condition for real world subject.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 06:25:18 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Sharma", "Kamlesh", ""], ["Prasad", "Dr. T. V.", ""]]}, {"id": "1305.1145", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, V M Thakare", "title": "Techniques for Feature Extraction In Speech Recognition System : A\n  Comparative Study", "comments": "Pages: 9 Figures : 3", "journal-ref": "International Journal Of Computer Applications In Engineering,\n  Technology and Sciences (IJCAETS),ISSN 0974-3596,2010,pp 412-418", "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time domain waveform of a speech signal carries all of the auditory\ninformation. From the phonological point of view, it little can be said on the\nbasis of the waveform itself. However, past research in mathematics, acoustics,\nand speech technology have provided many methods for converting data that can\nbe considered as information if interpreted correctly. In order to find some\nstatistically relevant information from incoming data, it is important to have\nmechanisms for reducing the information of each segment in the audio signal\ninto a relatively small number of parameters, or features. These features\nshould describe each segment in such a characteristic way that other similar\nsegments can be grouped together by comparing their features. There are\nenormous interesting and exceptional ways to describe the speech signal in\nterms of parameters. Though, they all have their strengths and weaknesses, we\nhave presented some of the most used methods with their importance.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 10:42:34 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Thakare", "V M", ""]]}, {"id": "1305.1319", "submitter": "David Bamman", "authors": "David Bamman and Noah A. Smith", "title": "New Alignment Methods for Discriminative Book Summarization", "comments": "This paper reflects work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the unsupervised alignment of the full text of a book with a\nhuman-written summary. This presents challenges not seen in other text\nalignment problems, including a disparity in length and, consequent to this, a\nviolation of the expectation that individual words and phrases should align,\nsince large passages and chapters can be distilled into a single summary\nphrase. We present two new methods, based on hidden Markov models, specifically\ntargeted to this problem, and demonstrate gains on an extractive book\nsummarization task. While there is still much room for improvement,\nunsupervised alignment holds intrinsic value in offering insight into what\nfeatures of a book are deemed worthy of summarization.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 20:27:55 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Bamman", "David", ""], ["Smith", "Noah A.", ""]]}, {"id": "1305.1343", "submitter": "Arnim Bleier", "authors": "Arnim Bleier and Andreas Strotmann", "title": "Towards an Author-Topic-Term-Model Visualization of 100 Years of German\n  Sociological Society Proceedings", "comments": "Accepted: 14th International Society of Scientometrics and\n  Informetrics Conference, Vienna Austria 15-19th July 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author co-citation studies employ factor analysis to reduce high-dimensional\nco-citation matrices to low-dimensional and possibly interpretable factors, but\nthese studies do not use any information from the text bodies of publications.\nWe hypothesise that term frequencies may yield useful information for\nscientometric analysis. In our work we ask if word features in combination with\nBayesian analysis allow well-founded science mapping studies. This work goes\nback to the roots of Mosteller and Wallace's (1964) statistical text analysis\nusing word frequency features and a Bayesian inference approach, tough with\ndifferent goals. To answer our research question we (i) introduce a new data\nset on which the experiments are carried out, (ii) describe the Bayesian model\nemployed for inference and (iii) present first results of the analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 22:24:20 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Bleier", "Arnim", ""], ["Strotmann", "Andreas", ""]]}, {"id": "1305.1426", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, V. M. Thakare", "title": "Speech Enhancement Modeling Towards Robust Speech Recognition System", "comments": "Pages: 04; Conference Proceedings International Conference on Advance\n  Computing (ICAC-2008), India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Form about four decades human beings have been dreaming of an intelligent\nmachine which can master the natural speech. In its simplest form, this machine\nshould consist of two subsystems, namely automatic speech recognition (ASR) and\nspeech understanding (SU). The goal of ASR is to transcribe natural speech\nwhile SU is to understand the meaning of the transcription. Recognizing and\nunderstanding a spoken sentence is obviously a knowledge-intensive process,\nwhich must take into account all variable information about the speech\ncommunication process, from acoustics to semantics and pragmatics. While\ndeveloping an Automatic Speech Recognition System, it is observed that some\nadverse conditions degrade the performance of the Speech Recognition System. In\nthis contribution, speech enhancement system is introduced for enhancing speech\nsignals corrupted by additive noise and improving the performance of Automatic\nSpeech Recognizers in noisy conditions. Automatic speech recognition\nexperiments show that replacing noisy speech signals by the corresponding\nenhanced speech signals leads to an improvement in the recognition accuracies.\nThe amount of improvement varies with the type of the corrupting noise.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 07:21:06 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Thakare", "V. M.", ""]]}, {"id": "1305.1925", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, Anjali Mahajan", "title": "Speech: A Challenge to Digital Signal Processing Technology for\n  Human-to-Computer Interaction", "comments": "Pages: 06 Figures : 06. arXiv admin note: text overlap with\n  arXiv:1305.1429, arXiv:1305.1428", "journal-ref": "Conference Proceedings National Conference on Recent Trends in\n  Electronics & Information Technology (RTEIT),2006,pp 206-212", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This software project based paper is for a vision of the near future in which\ncomputer interaction is characterized by natural face-to-face conversations\nwith lifelike characters that speak, emote, and gesture. The first step is\nspeech. The dream of a true virtual reality, a complete human-computer\ninteraction system will not come true unless we try to give some perception to\nmachine and make it perceive the outside world as humans communicate with each\nother. This software project is under development for listening and replying\nmachine (Computer) through speech. The Speech interface is developed to convert\nspeech input into some parametric form (Speech-to-Text) for further processing\nand the results, text output to speech synthesis (Text-to-Speech)\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 05:55:50 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Mahajan", "Anjali", ""]]}, {"id": "1305.2352", "submitter": "Urmila Shrawankar Ms", "authors": "Rashmi Makhijani, Urmila Shrawankar, V M Thakare", "title": "Speech Enhancement Using Pitch Detection Approach For Noisy Environment", "comments": "Pages: 06 Figures : 05", "journal-ref": "International Journal of Engineering Science and Technology\n  (IJEST), 2011, ISSN : 0975-5462 Vol. 3 No. 2, pp 1764-1769", "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustical mismatch among training and testing phases degrades outstandingly\nspeech recognition results. This problem has limited the development of\nreal-world nonspecific applications, as testing conditions are highly variant\nor even unpredictable during the training process. Therefore the background\nnoise has to be removed from the noisy speech signal to increase the signal\nintelligibility and to reduce the listener fatigue. Enhancement techniques\napplied, as pre-processing stages; to the systems remarkably improve\nrecognition results. In this paper, a novel approach is used to enhance the\nperceived quality of the speech signal when the additive noise cannot be\ndirectly controlled. Instead of controlling the background noise, we propose to\nreinforce the speech signal so that it can be heard more clearly in noisy\nenvironments. The subjective evaluation shows that the proposed method improves\nperceptual quality of speech in various noisy environments. As in some cases\nspeaking may be more convenient than typing, even for rapid typists: many\nmathematical symbols are missing from the keyboard but can be easily spoken and\nrecognized. Therefore, the proposed system can be used in an application\ndesigned for mathematical symbol recognition (especially symbols not available\non the keyboard) in schools.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 08:39:11 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Makhijani", "Rashmi", ""], ["Shrawankar", "Urmila", ""], ["Thakare", "V M", ""]]}, {"id": "1305.2680", "submitter": "Sulaiman AlDahri", "authors": "Sulaiman S. AlDahri", "title": "A study for the effect of the Emphaticness and language and dialect for\n  Voice Onset Time (VOT) in Modern Standard Arabic (MSA)", "comments": "18 pages, Signal & Image Processing:An International Journal (SIPIJ),\n  2013 April", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The signal sound contains many different features, including Voice Onset Time\n(VOT), which is a very important feature of stop sounds in many languages. The\nonly application of VOT values is stopping phoneme subsets. This subset of\nconsonant sounds is stop phonemes exist in the Arabic language, and in fact,\nall languages. The pronunciation of these sounds is hard and unique especially\nfor less-educated Arabs and non-native Arabic speakers. VOT can be utilized by\nthe human auditory system to distinguish between voiced and unvoiced stops such\nas /p/ and /b/ in English.This search focuses on computing and analyzing VOT of\nModern Standard Arabic (MSA), within the Arabic language, for all pairs of\nnon-emphatic (namely, /d/ and /t/) and emphatic pairs (namely, /d?/ and /t?/)\ndepending on carrier words. This research uses a database built by ourselves,\nand uses the carrier words syllable structure: CV-CV-CV. One of the main\noutcomes always found is the emphatic sounds (/d?/, /t?/) are less than 50% of\nnon-emphatic (counter-part) sounds ( /d/, /t/).Also, VOT can be used to\nclassify or detect for a dialect ina language.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 06:38:34 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["AlDahri", "Sulaiman S.", ""]]}, {"id": "1305.2846", "submitter": "Urmila Shrawankar Ms", "authors": "Rashmi Makhijani, Urmila Shrawankar, V M Thakare", "title": "Opportunities & Challenges In Automatic Speech Recognition", "comments": "Pages: 05 Figures : 01 Proceedings of the International Conference\n  BEATS 2010, NIT Jalandhar, INDIA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition enables a wide range of current and emerging\napplications such as automatic transcription, multimedia content analysis, and\nnatural human-computer interfaces. This paper provides a glimpse of the\nopportunities and challenges that parallelism provides for automatic speech\nrecognition and related application research from the point of view of speech\nresearchers. The increasing parallelism in computing platforms opens three\nmajor possibilities for speech recognition systems: improving recognition\naccuracy in non-ideal, everyday noisy environments; increasing recognition\nthroughput in batch processing of speech data; and reducing recognition latency\nin realtime usage scenarios. This paper describes technical challenges,\napproaches taken, and possible directions for future research to guide the\ndesign of efficient parallel software and hardware infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 08:42:26 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Makhijani", "Rashmi", ""], ["Shrawankar", "Urmila", ""], ["Thakare", "V M", ""]]}, {"id": "1305.2847", "submitter": "Urmila Shrawankar Ms", "authors": "Neema Mishra, Urmila Shrawankar, V M Thakare", "title": "An Overview of Hindi Speech Recognition", "comments": "Pages: 05 Figures : 04 Tables : 03 Proceedings of the International\n  Conference ICCSCT 2010, Tirunelveli, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this age of information technology, information access in a convenient\nmanner has gained importance. Since speech is a primary mode of communication\namong human beings, it is natural for people to expect to be able to carry out\nspoken dialogue with computer. Speech recognition system permits ordinary\npeople to speak to the computer to retrieve information. It is desirable to\nhave a human computer dialogue in local language. Hindi being the most widely\nspoken Language in India is the natural primary human language candidate for\nhuman machine interaction. There are five pairs of vowels in Hindi languages;\none member is longer than the other one. This paper describes an overview of\nspeech recognition system that includes how speech is produced and the\nproperties and characteristics of Hindi Phoneme.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 08:44:58 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Mishra", "Neema", ""], ["Shrawankar", "Urmila", ""], ["Thakare", "V M", ""]]}, {"id": "1305.2959", "submitter": "Urmila Shrawankar Ms", "authors": "Neema Mishra, Urmila Shrawankar, V M Thakare", "title": "Automatic Speech Recognition Using Template Model for Man-Machine\n  Interface", "comments": "Pages: 05 Figures : 01 Tables : 03 Proceedings of the International\n  Conference ICAET 2010, Chennai, India. arXiv admin note: text overlap with\n  arXiv:1305.2847", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is a natural form of communication for human beings, and computers\nwith the ability to understand speech and speak with a human voice are expected\nto contribute to the development of more natural man-machine interfaces.\nComputers with this kind of ability are gradually becoming a reality, through\nthe evolution of speech recognition technologies. Speech is being an important\nmode of interaction with computers. In this paper Feature extraction is\nimplemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern\nmatching is done using Dynamic time warping (DTW) algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 08:47:47 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Mishra", "Neema", ""], ["Shrawankar", "Urmila", ""], ["Thakare", "V M", ""]]}, {"id": "1305.3107", "submitter": "Miles Osborne", "authors": "Sasa Petrovic, Miles Osborne and Victor Lavrenko", "title": "I Wish I Didn't Say That! Analyzing and Predicting Deleted Messages in\n  Twitter", "comments": "Unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has become a major source of data for social media researchers. One\nimportant aspect of Twitter not previously considered are {\\em deletions} --\nremoval of tweets from the stream. Deletions can be due to a multitude of\nreasons such as privacy concerns, rashness or attempts to undo public\nstatements. We show how deletions can be automatically predicted ahead of time\nand analyse which tweets are likely to be deleted and how.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 10:49:16 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Petrovic", "Sasa", ""], ["Osborne", "Miles", ""], ["Lavrenko", "Victor", ""]]}, {"id": "1305.3882", "submitter": "Daniel Christen Mr.", "authors": "Daniel Christen", "title": "Rule-Based Semantic Tagging. An Application Undergoing Dictionary\n  Glosses", "comments": "12 pages, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The project presented in this article aims to formalize criteria and\nprocedures in order to extract semantic information from parsed dictionary\nglosses. The actual purpose of the project is the generation of a semantic\nnetwork (nearly an ontology) issued from a monolingual Italian dictionary,\nthrough unsupervised procedures. Since the project involves rule-based Parsing,\nSemantic Tagging and Word Sense Disambiguation techniques, its outcomes may\nfind an interest also beyond this immediate intent. The cooperation of both\nsyntactic and semantic features in meaning construction are investigated, and\nprocedures which allows a translation of syntactic dependencies in semantic\nrelations are discussed. The procedures that rise from this project can be\napplied also to other text types than dictionary glosses, as they convert the\noutput of a parsing process into a semantic representation. In addition some\nmechanism are sketched that may lead to a kind of procedural semantics, through\nwhich multiple paraphrases of an given expression can be generated. Which means\nthat these techniques may find an application also in 'query expansion'\nstrategies, interesting Information Retrieval, Search Engines and Question\nAnswering Systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 18:09:21 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 15:09:26 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Christen", "Daniel", ""]]}, {"id": "1305.3981", "submitter": "Kaixu Zhang", "authors": "Kaixu Zhang, Can Wang, Maosong Sun", "title": "Binary Tree based Chinese Word Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Chinese word segmentation is a fundamental task for Chinese language\nprocessing. The granularity mismatch problem is the main cause of the errors.\nThis paper showed that the binary tree representation can store outputs with\ndifferent granularity. A binary tree based framework is also designed to\novercome the granularity mismatch problem. There are two steps in this\nframework, namely tree building and tree pruning. The tree pruning step is\nspecially designed to focus on the granularity problem. Previous work for\nChinese word segmentation such as the sequence tagging can be easily employed\nin this framework. This framework can also provide quantitative error analysis\nmethods. The experiments showed that after using a more sophisticated tree\npruning function for a state-of-the-art conditional random field based\nbaseline, the error reduction can be up to 20%.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 05:14:43 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Zhang", "Kaixu", ""], ["Wang", "Can", ""], ["Sun", "Maosong", ""]]}, {"id": "1305.4561", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Random crossings in dependency trees", "comments": "changes of format and language; some corrections in Appendix A; in\n  press in Glottometrics", "journal-ref": "Glottometrics 37, 1-12 (2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.DM cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been hypothesized that the rather small number of crossings in real\nsyntactic dependency trees is a side-effect of pressure for dependency length\nminimization. Here we answer a related important research question: what would\nbe the expected number of crossings if the natural order of a sentence was lost\nand replaced by a random ordering? We show that this number depends only on the\nnumber of vertices of the dependency tree (the sentence length) and the second\nmoment about zero of vertex degrees. The expected number of crossings is\nminimum for a star tree (crossings are impossible) and maximum for a linear\ntree (the number of crossings is of the order of the square of the sequence\nlength).\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 15:44:54 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 15:02:57 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 14:12:54 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1305.5566", "submitter": "Taha Yasseri", "authors": "Taha Yasseri, Anselm Spoerri, Mark Graham, and J\\'anos Kert\\'esz", "title": "The most controversial topics in Wikipedia: A multilingual and\n  geographical analysis", "comments": "This is a draft of a book chapter to be published in 2014 by\n  Scarecrow Press. Please cite as: Yasseri T., Spoerri A., Graham M., and\n  Kert\\'esz J., The most controversial topics in Wikipedia: A multilingual and\n  geographical analysis. In: Fichman P., Hara N., editors, Global\n  Wikipedia:International and cross-cultural issues in online collaboration.\n  Scarecrow Press (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.DL cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present, visualize and analyse the similarities and differences between\nthe controversial topics related to \"edit wars\" identified in 10 different\nlanguage versions of Wikipedia. After a brief review of the related work we\ndescribe the methods developed to locate, measure, and categorize the\ncontroversial topics in the different languages. Visualizations of the degree\nof overlap between the top 100 lists of most controversial articles in\ndifferent languages and the content related to geographical locations will be\npresented. We discuss what the presented analysis and visualizations can tell\nus about the multicultural aspects of Wikipedia and practices of\npeer-production. Our results indicate that Wikipedia is more than just an\nencyclopaedia; it is also a window into convergent and divergent social-spatial\npriorities, interests and preferences.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 21:09:51 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2013 13:24:36 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Yasseri", "Taha", ""], ["Spoerri", "Anselm", ""], ["Graham", "Mark", ""], ["Kert\u00e9sz", "J\u00e1nos", ""]]}, {"id": "1305.5753", "submitter": "Peter Bruza", "authors": "Peter D. Bruza and Kirsty Kitto and Brentyn J. Ramm and Laurianne\n  Sitbon", "title": "A probabilistic framework for analysing the compositionality of\n  conceptual combinations", "comments": "Revisions (Journal of Mathematical Psychology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptual combination performs a fundamental role in creating the broad\nrange of compound phrases utilized in everyday language. This article provides\na novel probabilistic framework for assessing whether the semantics of\nconceptual combinations are compositional, and so can be considered as a\nfunction of the semantics of the constituent concepts, or not. While the\nsystematicity and productivity of language provide a strong argument in favor\nof assuming compositionality, this very assumption is still regularly\nquestioned in both cognitive science and philosophy. Additionally, the\nprinciple of semantic compositionality is underspecified, which means that\nnotions of both \"strong\" and \"weak\" compositionality appear in the literature.\nRather than adjudicating between different grades of compositionality, the\nframework presented here contributes formal methods for determining a clear\ndividing line between compositional and non-compositional semantics. In\naddition, we suggest that the distinction between these is contextually\nsensitive. Utilizing formal frameworks developed for analyzing composite\nsystems in quantum theory, we present two methods that allow the semantics of\nconceptual combinations to be classified as \"compositional\" or\n\"non-compositional\". Compositionality is first formalised by factorising the\njoint probability distribution modeling the combination, where the terms in the\nfactorisation correspond to individual concepts. This leads to the necessary\nand sufficient condition for the joint probability distribution to exist. A\nfailure to meet this condition implies that the underlying concepts cannot be\nmodeled in a single probability space when considering their combination, and\nthe combination is thus deemed \"non-compositional\". The formal analysis methods\nare demonstrated by applying them to an empirical study of twenty-four\nnon-lexicalised conceptual combinations.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 03:14:50 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2013 00:27:07 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 02:15:45 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Bruza", "Peter D.", ""], ["Kitto", "Kirsty", ""], ["Ramm", "Brentyn J.", ""], ["Sitbon", "Laurianne", ""]]}, {"id": "1305.5785", "submitter": "Vivek Srikumar", "authors": "Vivek Srikumar and Dan Roth", "title": "An Inventory of Preposition Relations", "comments": "Supplementary material for Srikumar and Roth, 2013. Modeling Semantic\n  Relations Expressed by Prepositions, TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an inventory of semantic relations that are expressed by\nprepositions. We define these relations by building on the word sense\ndisambiguation task for prepositions and propose a mapping from preposition\nsenses to the relation labels by collapsing semantically related senses across\nprepositions.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 16:34:22 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Srikumar", "Vivek", ""], ["Roth", "Dan", ""]]}, {"id": "1305.5918", "submitter": "Kaixu Zhang", "authors": "Kaixu Zhang, Maosong Sun", "title": "Reduce Meaningless Words for Joint Chinese Word Segmentation and\n  Part-of-speech Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Conventional statistics-based methods for joint Chinese word segmentation and\npart-of-speech tagging (S&T) have generalization ability to recognize new words\nthat do not appear in the training data. An undesirable side effect is that a\nnumber of meaningless words will be incorrectly created. We propose an\neffective and efficient framework for S&T that introduces features to\nsignificantly reduce meaningless words generation. A general lexicon, Wikepedia\nand a large-scale raw corpus of 200 billion characters are used to generate\nword-based features for the wordhood. The word-lattice based framework consists\nof a character-based model and a word-based model in order to employ our\nword-based features. Experiments on Penn Chinese treebank 5 show that this\nmethod has a 62.9% reduction of meaningless word generation in comparison with\nthe baseline. As a result, the F1 measure for segmentation is increased to\n0.984.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2013 13:20:31 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Zhang", "Kaixu", ""], ["Sun", "Maosong", ""]]}, {"id": "1305.6143", "submitter": "Vivek Narayanan", "authors": "Vivek Narayanan, Ishan Arora, Arjun Bhatia", "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes\n  model", "comments": "8 pages, 2 figures", "journal-ref": "Intelligent Data Engineering and Automated Learning IDEAL 2013\n  Lecture Notes in Computer Science Volume 8206, 2013, pp 194-201", "doi": "10.1007/978-3-642-41278-3_24", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 08:37:26 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 05:36:29 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Narayanan", "Vivek", ""], ["Arora", "Ishan", ""], ["Bhatia", "Arjun", ""]]}, {"id": "1305.6211", "submitter": "Nisheeth Joshi", "authors": "Snigdha Paul, Nisheeth Joshi, Iti Mathur", "title": "Development of a Hindi Lemmatizer", "comments": "International Journal of Computational Linguistics and Natural\n  Language Processing, Vol 2, Issue 5, 2013", "journal-ref": "International Journal of Computational Linguistics and Natural\n  Language Processing, Vol 2, Issue 5, 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a translingual society, in order to communicate with people from\ndifferent parts of the world we need to have an expertise in their respective\nlanguages. Learning all these languages is not at all possible; therefore we\nneed a mechanism which can do this task for us. Machine translators have\nemerged as a tool which can perform this task. In order to develop a machine\ntranslator we need to develop several different rules. The very first module\nthat comes in machine translation pipeline is morphological analysis. Stemming\nand lemmatization comes under morphological analysis. In this paper we have\ncreated a lemmatizer which generates rules for removing the affixes along with\nthe addition of rules for creating a proper root word.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 18:01:34 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 16:02:42 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Paul", "Snigdha", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1305.6238", "submitter": "Richard Moot", "authors": "Richard Moot (LaBRI)", "title": "Extended Lambek calculi and first-order linear logic", "comments": "Logic and Language, Allemagne (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order multiplicative intuitionistic linear logic (MILL1) can be seen as\nan extension of the Lambek calculus. In addition to the fragment of MILL1 which\ncorresponds to the Lambek calculus (of Moot & Piazza 2001), I will show\nfragments of MILL1 which generate the multiple context-free languages and which\ncorrespond to the Displacement calculus of Morrilll e.a.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 14:36:53 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Moot", "Richard", "", "LaBRI"]]}, {"id": "1305.7014", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Tweets Miner for Stock Market Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a software package for the data mining of Twitter\nmicroblogs for the purpose of using them for the stock market analysis. The\npackage is written in R langauge using apropriate R packages. The model of\ntweets has been considered. We have also compared stock market charts with\nfrequent sets of keywords in Twitter microblogs messages.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 06:35:52 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}]