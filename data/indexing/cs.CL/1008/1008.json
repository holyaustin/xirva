[{"id": "1008.0170", "submitter": "Michael Moortgat", "authors": "Michael Moortgat", "title": "Symmetric categorial grammar: residuation and Galois connections", "comments": "Submitted to the Jim Lambek Festschrift of Linguistic Analysis (LA,\n  volume 36, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lambek-Grishin calculus is a symmetric extension of the Lambek calculus:\nin addition to the residuated family of product, left and right division\noperations of Lambek's original calculus, one also considers a family of\ncoproduct, right and left difference operations, related to the former by an\narrow-reversing duality. Communication between the two families is implemented\nin terms of linear distributivity principles. The aim of this paper is to\ncomplement the symmetry between (dual) residuated type-forming operations with\nan orthogonal opposition that contrasts residuated and Galois connected\noperations. Whereas the (dual) residuated operations are monotone, the Galois\nconnected operations (and their duals) are antitone. We discuss the algebraic\nproperties of the (dual) Galois connected operations, and generalize the\n(co)product distributivity principles to include the negative operations. We\ngive a continuation-passing-style translation for the new type-forming\noperations, and discuss some linguistic applications.\n", "versions": [{"version": "v1", "created": "Sun, 1 Aug 2010 12:45:58 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Moortgat", "Michael", ""]]}, {"id": "1008.0706", "submitter": "Allen Lavoie", "authors": "Allen Lavoie and Mukkai Krishnamoorthy", "title": "Algorithmic Detection of Computer Generated Text", "comments": null, "journal-ref": null, "doi": null, "report-no": "10-07", "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer generated academic papers have been used to expose a lack of\nthorough human review at several computer science conferences. We assess the\nproblem of classifying such documents. After identifying and evaluating several\nquantifiable features of academic papers, we apply methods from machine\nlearning to build a binary classifier. In tests with two hundred papers, the\nresulting classifier correctly labeled papers either as human written or as\ncomputer generated with no false classifications of computer generated papers\nas human and a 2% false classification rate for human papers as computer\ngenerated. We believe generalizations of these features are applicable to\nsimilar classification problems. While most current text-based spam detection\ntechniques focus on the keyword-based classification of email messages, a new\ngeneration of unsolicited computer-generated advertisements masquerade as\nlegitimate postings in online groups, message boards and social news sites. Our\nresults show that taking the formatting and contextual clues offered by these\nenvironments into account may be of central importance when selecting features\nwith which to identify such unwanted postings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Aug 2010 06:00:21 GMT"}], "update_date": "2010-08-05", "authors_parsed": [["Lavoie", "Allen", ""], ["Krishnamoorthy", "Mukkai", ""]]}, {"id": "1008.1394", "submitter": "Zeeshan Ahmed Mr.", "authors": "Zeeshan Ahmed, Saman Majeed, Thomas Dandekar", "title": "Towards Design and Implementation of a Language Technology based\n  Information Processor for PDM Systems", "comments": null, "journal-ref": "IST Transactions of Information Technology- Theory and\n  Applications, Vol. 1, No. 1 (2),ISSN 1913-8822, pp. 1-7, 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product Data Management (PDM) aims to provide 'Systems' contributing in\nindustries by electronically maintaining organizational data, improving data\nrepository system, facilitating with easy access to CAD and providing\nadditional information engineering and management modules to access, store,\nintegrate, secure, recover and manage information. Targeting one of the\nunresolved issues i.e., provision of natural language based processor for the\nimplementation of an intelligent record search mechanism, an approach is\nproposed and discussed in detail in this manuscript. Designing an intelligent\napplication capable of reading and analyzing user's structured and unstructured\nnatural language based text requests and then extracting desired concrete and\noptimized results from knowledge base is still a challenging task for the\ndesigners because it is still very difficult to completely extract Meta data\nout of raw data. Residing within the limited scope of current research and\ndevelopment; we present an approach capable of reading user's natural language\nbased input text, understanding the semantic and extracting results from\nrepositories. To evaluate the effectiveness of implemented prototyped version\nof proposed approach, it is compared with some existing PDM Systems, in the end\nthe discussion is concluded with an abstract presentation of resultant\ncomparison amongst implemented prototype and some existing PDM Systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Aug 2010 09:38:38 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Ahmed", "Zeeshan", ""], ["Majeed", "Saman", ""], ["Dandekar", "Thomas", ""]]}, {"id": "1008.1673", "submitter": "Alex Berka Mr", "authors": "Alex V Berka", "title": "Space and the Synchronic A-Ram", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space is a circuit oriented, spatial programming language designed to exploit\nthe massive parallelism available in a novel formal model of computation called\nthe Synchronic A-Ram, and physically related FPGA and reconfigurable\narchitectures. Space expresses variable grained MIMD parallelism, is modular,\nstrictly typed, and deterministic. Barring operations associated with memory\nallocation and compilation, modules cannot access global variables, and are\nreferentially transparent. At a high level of abstraction, modules exhibit a\nsmall, sequential state transition system, aiding verification. Space deals\nwith communication, scheduling, and resource contention issues in parallel\ncomputing, by resolving them explicitly in an incremental manner, module by\nmodule, whilst ascending the ladder of abstraction. Whilst the Synchronic A-Ram\nmodel was inspired by linguistic considerations, it is also put forward as a\nformal model for reconfigurable digital circuits. A programming environment has\nbeen developed, that incorporates a simulator and compiler that transform Space\nprograms into Synchronic A-Ram machine code, consisting of only three bit-level\ninstructions, and a marking instruction. Space and the Synchronic A-Ram point\nto novel routes out of the parallel computing crisis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 09:46:21 GMT"}, {"version": "v2", "created": "Sat, 28 Aug 2010 10:33:08 GMT"}], "update_date": "2010-08-31", "authors_parsed": [["Berka", "Alex V", ""]]}, {"id": "1008.1986", "submitter": "Lillian Lee", "authors": "Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian\n  Lee", "title": "For the sake of simplicity: Unsupervised extraction of lexical\n  simplifications from Wikipedia", "comments": "4 pp; data available at\n  http://www.cs.cornell.edu/home/llee/data/simple/", "journal-ref": "Proceedings of the NAACL, pp. 365-368, 2010. Short paper", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on work in progress on extracting lexical simplifications (e.g.,\n\"collaborate\" -> \"work together\"), focusing on utilizing edit histories in\nSimple English Wikipedia for this task. We consider two main approaches: (1)\nderiving simplification probabilities via an edit model that accounts for a\nmixture of different operations, and (2) using metadata to focus on edits that\nare more likely to be simplification operations. We find our methods to\noutperform a reasonable baseline and yield many high-quality lexical\nsimplifications not included in an independently-created manually prepared\nlist.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 20:01:59 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Yatskar", "Mark", ""], ["Pang", "Bo", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Lee", "Lillian", ""]]}, {"id": "1008.3169", "submitter": "Lillian Lee", "authors": "Cristian Danescu-Niculescu-Mizil and Lillian Lee", "title": "Don't 'have a clue'? Unsupervised co-learning of downward-entailing\n  operators", "comments": "pp 1-6 are identical to the ACL 2010 published version; pp. 7-8 are\n  the \"externally-available appendices\". Revision contains an additional\n  appendix correcting the origin of the term \"pseudo-polarity item\"", "journal-ref": "Proceedings of the ACL Short Papers, pp. 247-252, 2010", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in textual entailment have begun to consider inferences involving\n'downward-entailing operators', an interesting and important class of lexical\nitems that change the way inferences are made. Recent work proposed a method\nfor learning English downward-entailing operators that requires access to a\nhigh-quality collection of 'negative polarity items' (NPIs). However, English\nis one of the very few languages for which such a list exists. We propose the\nfirst approach that can be applied to the many languages for which there is no\npre-existing high-precision database of NPIs. As a case study, we apply our\nmethod to Romanian and show that our method yields good results. Also, we\nperform a cross-linguistic analysis that suggests interesting connections to\nsome findings in linguistic typology.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 20:08:22 GMT"}, {"version": "v2", "created": "Sat, 27 Nov 2010 01:51:03 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Danescu-Niculescu-Mizil", "Cristian", ""], ["Lee", "Lillian", ""]]}, {"id": "1008.3667", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay, Yicheng Wen and Asok Ray", "title": "Pattern Classification In Symbolic Streams via Semantic Annihilation of\n  Information", "comments": "15 pages, 7 figures (Under Review Elsewhere: Journal Reference Will\n  Be Provided When Available )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for pattern classification in symbolic streams via\nselective erasure of observed symbols, in cases where the patterns of interest\nare represented as Probabilistic Finite State Automata (PFSA). We define an\nadditive abelian group for a slightly restricted subset of probabilistic finite\nstate automata (PFSA), and the group sum is used to formulate pattern-specific\nsemantic annihilators. The annihilators attempt to identify pre-specified\npatterns via removal of essentially all inter-symbol correlations from observed\nsequences, thereby turning them into symbolic white noise. Thus a perfect\nannihilation corresponds to a perfect pattern match. This approach of\nclassification via information annihilation is shown to be strictly\nadvantageous, with theoretical guarantees, for a large class of PFSA models.\nThe results are supported by simulation experiments.\n", "versions": [{"version": "v1", "created": "Sat, 21 Aug 2010 21:24:42 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Wen", "Yicheng", ""], ["Ray", "Asok", ""]]}, {"id": "1008.5287", "submitter": "Srivatsan Laxman", "authors": "Dipak Chaudhari, Om P. Damani, and Srivatsan Laxman", "title": "Lexical Co-occurrence, Statistical Significance, and Word Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical co-occurrence is an important cue for detecting word associations. We\npresent a theoretical framework for discovering statistically significant\nlexical co-occurrences from a given corpus. In contrast with the prevalent\npractice of giving weightage to unigram frequencies, we focus only on the\ndocuments containing both the terms (of a candidate bigram). We detect biases\nin span distributions of associated words, while being agnostic to variations\nin global unigram frequencies. Our framework has the fidelity to distinguish\ndifferent classes of lexical co-occurrences, based on strengths of the document\nand corpuslevel cues of co-occurrence in the data. We perform extensive\nexperiments on benchmark data sets to study the performance of various\nco-occurrence measures that are currently known in literature. We find that a\nrelatively obscure measure called Ochiai, and a newly introduced measure CSA\ncapture the notion of lexical co-occurrence best, followed next by LLR, Dice,\nand TTest, while another popular measure, PMI, suprisingly, performs poorly in\nthe context of lexical co-occurrence.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 11:37:32 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Chaudhari", "Dipak", ""], ["Damani", "Om P.", ""], ["Laxman", "Srivatsan", ""]]}]