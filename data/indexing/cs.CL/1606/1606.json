[{"id": "1606.00025", "submitter": "Sushrut Thorat", "authors": "Sushrut Thorat, Varad Choudhari", "title": "Implementing a Reverse Dictionary, based on word definitions, using a\n  Node-Graph Architecture", "comments": "Included publication information", "journal-ref": "Proceedings of COLING 2016, the 26th International Conference on\n  Computational Linguistics: Technical Papers, pages 2797-2806, Osaka, Japan,\n  December 11-17 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we outline an approach to build graph-based reverse\ndictionaries using word definitions. A reverse dictionary takes a phrase as an\ninput and outputs a list of words semantically similar to that phrase. It is a\nsolution to the Tip-of-the-Tongue problem. We use a distance-based similarity\nmeasure, computed on a graph, to assess the similarity between a word and the\ninput phrase. We compare the performance of our approach with the Onelook\nReverse Dictionary and a distributional semantics method based on word2vec, and\nshow that our approach is much better than the distributional semantics method,\nand as good as Onelook, on a 3k lexicon. This simple approach sets a new\nperformance baseline for reverse dictionaries.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 20:09:59 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 22:57:17 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 08:55:35 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 13:48:31 GMT"}, {"version": "v5", "created": "Sat, 17 Dec 2016 22:36:15 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Thorat", "Sushrut", ""], ["Choudhari", "Varad", ""]]}, {"id": "1606.00061", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "comments": "11 pages, 7 figures, 3 tables in 2016 Conference on Neural\n  Information Processing Systems (NIPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works have proposed attention models for Visual Question\nAnswering (VQA) that generate spatial maps highlighting image regions relevant\nto answering the question. In this paper, we argue that in addition to modeling\n\"where to look\" or visual attention, it is equally important to model \"what\nwords to listen to\" or question attention. We present a novel co-attention\nmodel for VQA that jointly reasons about image and question attention. In\naddition, our model reasons about the question (and consequently the image via\nthe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional\nconvolution neural networks (CNN). Our model improves the state-of-the-art on\nthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA\ndataset. By using ResNet, the performance is further improved to 62.1% for VQA\nand 65.4% for COCO-QA.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 22:02:01 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 01:51:13 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 02:15:57 GMT"}, {"version": "v4", "created": "Fri, 13 Jan 2017 16:18:03 GMT"}, {"version": "v5", "created": "Thu, 19 Jan 2017 05:03:33 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Lu", "Jiasen", ""], ["Yang", "Jianwei", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.00189", "submitter": "Shamil Chollampatt", "authors": "Shamil Chollampatt, Kaveh Taghipour and Hwee Tou Ng", "title": "Neural Network Translation Models for Grammatical Error Correction", "comments": "Accepted for presentation at IJCAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phrase-based statistical machine translation (SMT) systems have previously\nbeen used for the task of grammatical error correction (GEC) to achieve\nstate-of-the-art accuracy. The superiority of SMT systems comes from their\nability to learn text transformations from erroneous to corrected text, without\nexplicitly modeling error types. However, phrase-based SMT systems suffer from\nlimitations of discrete word representation, linear mapping, and lack of global\ncontext. In this paper, we address these limitations by using two different yet\ncomplementary neural network models, namely a neural network global lexicon\nmodel and a neural network joint model. These neural networks can generalize\nbetter by using continuous space representation of words and learn non-linear\nmappings. Moreover, they can leverage contextual information from the source\nsentence more effectively. By adding these two components, we achieve\nstatistically significant improvement in accuracy for grammatical error\ncorrection over a state-of-the-art GEC system.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 09:31:00 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Chollampatt", "Shamil", ""], ["Taghipour", "Kaveh", ""], ["Ng", "Hwee Tou", ""]]}, {"id": "1606.00210", "submitter": "Duc Tam Hoang", "authors": "Duc Tam Hoang and Shamil Chollampatt and Hwee Tou Ng", "title": "Exploiting N-Best Hypotheses to Improve an SMT Approach to Grammatical\n  Error Correction", "comments": "Accepted for presentation at IJCAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical error correction (GEC) is the task of detecting and correcting\ngrammatical errors in texts written by second language learners. The\nstatistical machine translation (SMT) approach to GEC, in which sentences\nwritten by second language learners are translated to grammatically correct\nsentences, has achieved state-of-the-art accuracy. However, the SMT approach is\nunable to utilize global context. In this paper, we propose a novel approach to\nimprove the accuracy of GEC, by exploiting the n-best hypotheses generated by\nan SMT approach. Specifically, we build a classifier to score the edits in the\nn-best hypotheses. The classifier can be used to select appropriate edits or\nre-rank the n-best hypotheses. We apply these methods to a state-of-the-art GEC\nsystem that uses the SMT approach. Our experiments show that our methods\nachieve statistically significant improvements in accuracy over the best\npublished results on a benchmark test dataset on GEC.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 10:32:28 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Hoang", "Duc Tam", ""], ["Chollampatt", "Shamil", ""], ["Ng", "Hwee Tou", ""]]}, {"id": "1606.00253", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Massih-Reza Amini, Marianne Clausel", "title": "On a Topic Model for Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic topic models are generative models that describe the content of\ndocuments by discovering the latent topics underlying them. However, the\nstructure of the textual input, and for instance the grouping of words in\ncoherent text spans such as sentences, contains much information which is\ngenerally lost with these models. In this paper, we propose sentenceLDA, an\nextension of LDA whose goal is to overcome this limitation by incorporating the\nstructure of the text in the generative and inference processes. We illustrate\nthe advantages of sentenceLDA by comparing it with LDA using both intrinsic\n(perplexity) and extrinsic (text classification) evaluation tasks on different\ntext collections.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:34:50 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""], ["Clausel", "Marianne", ""]]}, {"id": "1606.00294", "submitter": "Jessica Ficler", "authors": "Jessica Ficler and Yoav Goldberg", "title": "Improved Parsing for Argument-Clusters Coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic parsers perform poorly in prediction of Argument-Cluster\nCoordination (ACC). We change the PTB representation of ACC to be more suitable\nfor learning by a statistical PCFG parser, affecting 125 trees in the training\nset. Training on the modified trees yields a slight improvement in EVALB scores\non sections 22 and 23. The main evaluation is on a corpus of 4th grade science\nexams, in which ACC structures are prevalent. On this corpus, we obtain an\nimpressive x2.7 improvement in recovering ACC structures compared to a parser\ntrained on the original PTB trees.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 14:06:41 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Ficler", "Jessica", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1606.00372", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou and Marc Pickett and Javier Snaider and Yun-hsuan Sung\n  and Brian Strope and Ray Kurzweil", "title": "Conversational Contextual Cues: The Case of Personalization and History\n  for Response Ranking", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of modeling open-domain, multi-turn, unstructured,\nmulti-participant, conversational dialogue. We specifically study the effect of\nincorporating different elements of the conversation. Unlike previous efforts,\nwhich focused on modeling messages and responses, we extend the modeling to\nlong context and participant's history. Our system does not rely on handwritten\nrules or engineered features; instead, we train deep neural networks on a large\nconversational dataset. In particular, we exploit the structure of Reddit\ncomments and posts to extract 2.1 billion messages and 133 million\nconversations. We evaluate our models on the task of predicting the next\nresponse in a conversation, and we find that modeling both context and\nparticipants improves prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 18:01:14 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Pickett", "Marc", ""], ["Snaider", "Javier", ""], ["Sung", "Yun-hsuan", ""], ["Strope", "Brian", ""], ["Kurzweil", "Ray", ""]]}, {"id": "1606.00411", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Elaine O. Nsoesie, Emily Cohn,\n  Sumiko R. Mekaru, John S. Brownstein and Naren Ramakrishnan", "title": "Temporal Topic Modeling to Assess Associations between News Trends and\n  Infectious Disease Outbreaks", "comments": "This paper has been submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retrospective assessments, internet news reports have been shown to\ncapture early reports of unknown infectious disease transmission prior to\nofficial laboratory confirmation. In general, media interest and reporting\npeaks and wanes during the course of an outbreak. In this study, we quantify\nthe extent to which media interest during infectious disease outbreaks is\nindicative of trends of reported incidence. We introduce an approach that uses\nsupervised temporal topic models to transform large corpora of news articles\ninto temporal topic trends. The key advantages of this approach include,\napplicability to a wide range of diseases, and ability to capture disease\ndynamics - including seasonality, abrupt peaks and troughs. We evaluated the\nmethod using data from multiple infectious disease outbreaks reported in the\nUnited States of America (U.S.), China and India. We noted that temporal topic\ntrends extracted from disease-related news reports successfully captured the\ndynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue\noutbreaks in India (2013) and China (2014). Our observations also suggest that\nefficient modeling of temporal topic trends using time-series regression\ntechniques can estimate disease case counts with increased precision before\nofficial reports by health organizations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 19:30:07 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Nsoesie", "Elaine O.", ""], ["Cohn", "Emily", ""], ["Mekaru", "Sumiko R.", ""], ["Brownstein", "John S.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1606.00414", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "On a Possible Similarity between Gene and Semantic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several domains such as linguistics, molecular biology or social sciences,\nholistic effects are hardly well-defined by modeling with single units, but\nmore and more studies tend to understand macro structures with the help of\nmeaningful and useful associations in fields such as social networks, systems\nbiology or semantic web. A stochastic multi-agent system offers both accurate\ntheoretical framework and operational computing implementations to model\nlarge-scale associations, their dynamics and patterns extraction. We show that\nclustering around a target object in a set of associations of object prove some\nsimilarity in specific data and two case studies about gene-gene and term-term\nrelationships leading to an idea of a common organizing principle of cognition\nwith random and deterministic effects.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:24:50 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1606.00499", "submitter": "Graham Neubig", "authors": "Graham Neubig and Chris Dyer", "title": "Generalizing and Hybridizing Count-based and Neural Language Models", "comments": "Presented at EMNLP2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) are statistical models that calculate probabilities\nover sequences of words or other discrete symbols. Currently two major\nparadigms for language modeling exist: count-based n-gram models, which have\nadvantages of scalability and test-time speed, and neural LMs, which often\nachieve superior modeling performance. We demonstrate how both varieties of\nmodels can be unified in a single modeling framework that defines a set of\nprobability distributions over the vocabulary of words, and then dynamically\ncalculates mixture weights over these distributions. This formulation allows us\nto create novel hybrid models that combine the desirable features of\ncount-based and neural LMs, and experiments demonstrate the advantages of these\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 23:26:20 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 01:48:57 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Neubig", "Graham", ""], ["Dyer", "Chris", ""]]}, {"id": "1606.00577", "submitter": "Justin Wood", "authors": "Justin Wood, Patrick Tan, Wei Wang, Corey Arnold", "title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge\n  sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:15:15 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 05:15:36 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 21:03:06 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Wood", "Justin", ""], ["Tan", "Patrick", ""], ["Wang", "Wei", ""], ["Arnold", "Corey", ""]]}, {"id": "1606.00589", "submitter": "Katharina Kann", "authors": "Katharina Kann and Hinrich Sch\\\"utze", "title": "Single-Model Encoder-Decoder with Explicit Morphological Representation\n  for Reinflection", "comments": "Accepted at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological reinflection is the task of generating a target form given a\nsource form, a source tag and a target tag. We propose a new way of modeling\nthis task with neural encoder-decoder models. Our approach reduces the amount\nof required training data for this architecture and achieves state-of-the-art\nresults, making encoder-decoder models applicable to morphological reinflection\neven for low-resource languages. We further present a new automatic correction\nmethod for the outputs based on edit trees.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:57:14 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Kann", "Katharina", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1606.00739", "submitter": "Stefan Riezler", "authors": "Artem Sokolov and Julia Kreutzer and Christopher Lo and Stefan Riezler", "title": "Stochastic Structured Prediction under Bandit Feedback", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic structured prediction under bandit feedback follows a learning\nprotocol where on each of a sequence of iterations, the learner receives an\ninput, predicts an output structure, and receives partial feedback in form of a\ntask loss evaluation of the predicted structure. We present applications of\nthis learning scenario to convex and non-convex objectives for structured\nprediction and analyze them as stochastic first-order methods. We present an\nexperimental evaluation on problems of natural language processing over\nexponential output spaces, and compare convergence speed across different\nobjectives under the practical criterion of optimal task performance on\ndevelopment data and the optimization-theoretic criterion of minimal squared\ngradient norm. Best results under both criteria are obtained for a non-convex\nobjective for pairwise preference learning under bandit feedback.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 16:06:29 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 16:29:42 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sokolov", "Artem", ""], ["Kreutzer", "Julia", ""], ["Lo", "Christopher", ""], ["Riezler", "Stefan", ""]]}, {"id": "1606.00776", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula,\n  Bowen Zhou, Yoshua Bengio, Aaron Courville", "title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue\n  Response Generation", "comments": "21 pages, 2 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens. There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\nSuch procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\nthe standard log- likelihood objective w.r.t. natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions. We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy. On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics. Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 17:37:31 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 02:01:16 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Klinger", "Tim", ""], ["Tesauro", "Gerald", ""], ["Talamadupula", "Kartik", ""], ["Zhou", "Bowen", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1606.00819", "submitter": "Alexandre Salle", "authors": "Alexandre Salle and Marco Idiart and Aline Villavicencio", "title": "Matrix Factorization using Window Sampling and Negative Sampling for\n  Improved Word Representations", "comments": "Converted paper size from A4 to US Letter to avoid margin issues on\n  arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose LexVec, a new method for generating distributed\nword representations that uses low-rank, weighted factorization of the Positive\nPoint-wise Mutual Information matrix via stochastic gradient descent, employing\na weighting scheme that assigns heavier penalties for errors on frequent\nco-occurrences while still accounting for negative co-occurrence. Evaluation on\nword similarity and analogy tasks shows that LexVec matches and often\noutperforms state-of-the-art methods on many of these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 19:35:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 02:20:23 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Salle", "Alexandre", ""], ["Idiart", "Marco", ""], ["Villavicencio", "Aline", ""]]}, {"id": "1606.00979", "submitter": "Kang Liu", "authors": "Yuanzhe Zhang, Kang Liu, Shizhu He, Guoliang Ji, Zhanyi Liu, Hua Wu,\n  Jun Zhao", "title": "Question Answering over Knowledge Base with Neural Attention Combining\n  Global Knowledge Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of knowledge bases (KBs) on the web, how to take full\nadvantage of them becomes increasingly important. Knowledge base-based question\nanswering (KB-QA) is one of the most promising approaches to access the\nsubstantial knowledge. Meantime, as the neural network-based (NN-based) methods\ndevelop, NN-based KB-QA has already achieved impressive results. However,\nprevious work did not put emphasis on question representation, and the question\nis converted into a fixed vector regardless of its candidate answers. This\nsimple representation strategy is unable to express the proper information of\nthe question. Hence, we present a neural attention-based model to represent the\nquestions dynamically according to the different focuses of various candidate\nanswer aspects. In addition, we leverage the global knowledge inside the\nunderlying KB, aiming at integrating the rich KB information into the\nrepresentation of the answers. And it also alleviates the out of vocabulary\n(OOV) problem, which helps the attention model to represent the question more\nprecisely. The experimental results on WEBQUESTIONS demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 06:40:14 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Zhang", "Yuanzhe", ""], ["Liu", "Kang", ""], ["He", "Shizhu", ""], ["Ji", "Guoliang", ""], ["Liu", "Zhanyi", ""], ["Wu", "Hua", ""], ["Zhao", "Jun", ""]]}, {"id": "1606.01151", "submitter": "Joshua Snoke", "authors": "Alexander G. Ororbia II and Fridolin Linder and Joshua Snoke", "title": "Using Neural Generative Models to Release Synthetic Twitter Corpora with\n  Reduced Stylometric Identifiability of Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for generating synthetic versions of Twitter data using\nneural generative models. The goal is protecting individuals in the source data\nfrom stylometric re-identification attacks while still releasing data that\ncarries research value. Specifically, we generate tweet corpora that maintain\nuser-level word distributions by augmenting the neural language models with\nuser-specific components. We compare our approach to two standard text data\nprotection methods: redaction and iterative translation. We evaluate the three\nmethods on measures of risk and utility. We define risk following the\nstylometric models of re-identification, and we define utility based on two\ngeneral word distribution measures and two common text analysis research tasks.\nWe find that neural models are able to significantly lower risk over previous\nmethods with little cost to utility. We also demonstrate that the neural models\nallow data providers to actively control the risk-utility trade-off through\nmodel tuning parameters. This work presents promising results for a new tool\naddressing the problem of privacy for free text and sharing social media data\nin a way that respects privacy and is ethically responsible.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 15:43:15 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 03:34:37 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 22:14:38 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 14:12:39 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Ororbia", "Alexander G.", "II"], ["Linder", "Fridolin", ""], ["Snoke", "Joshua", ""]]}, {"id": "1606.01161", "submitter": "Jiang Guo", "authors": "Jiang Guo, Wanxiang Che, Haifeng Wang and Ting Liu", "title": "Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task\n  Learning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various treebanks have been released for dependency parsing. Despite that\ntreebanks may belong to different languages or have different annotation\nschemes, they contain syntactic knowledge that is potential to benefit each\nother. This paper presents an universal framework for exploiting these\nmulti-typed treebanks to improve parsing with deep multi-task learning. We\nconsider two kinds of treebanks as source: the multilingual universal treebanks\nand the monolingual heterogeneous treebanks. Multiple treebanks are trained\njointly and interacted with multi-level parameter sharing. Experiments on\nseveral benchmark datasets in various languages demonstrate that our approach\ncan make effective use of arbitrary source treebanks to improve target parsing\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 16:09:52 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Guo", "Jiang", ""], ["Che", "Wanxiang", ""], ["Wang", "Haifeng", ""], ["Liu", "Ting", ""]]}, {"id": "1606.01219", "submitter": "Steven H. H. Ding", "authors": "Steven H. H. Ding, Benjamin C. M. Fung, Farkhund Iqbal, William K.\n  Cheung", "title": "Learning Stylometric Representations for Authorship Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship analysis (AA) is the study of unveiling the hidden properties of\nauthors from a body of exponentially exploding textual data. It extracts an\nauthor's identity and sociolinguistic characteristics based on the reflected\nwriting styles in the text. It is an essential process for various areas, such\nas cybercrime investigation, psycholinguistics, political socialization, etc.\nHowever, most of the previous techniques critically depend on the manual\nfeature engineering process. Consequently, the choice of feature set has been\nshown to be scenario- or dataset-dependent. In this paper, to mimic the human\nsentence composition process using a neural network approach, we propose to\nincorporate different categories of linguistic features into distributed\nrepresentation of words in order to learn simultaneously the writing style\nrepresentations based on unlabeled texts for authorship analysis. In\nparticular, the proposed models allow topical, lexical, syntactical, and\ncharacter-level feature vectors of each document to be extracted as\nstylometrics. We evaluate the performance of our approach on the problems of\nauthorship characterization and authorship verification with the Twitter,\nnovel, and essay datasets. The experiments suggest that our proposed text\nrepresentation outperforms the bag-of-lexical-n-grams, Latent Dirichlet\nAllocation, Latent Semantic Analysis, PVDM, PVDBOW, and word2vec\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 18:42:14 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Ding", "Steven H. H.", ""], ["Fung", "Benjamin C. M.", ""], ["Iqbal", "Farkhund", ""], ["Cheung", "William K.", ""]]}, {"id": "1606.01269", "submitter": "Jason Williams", "authors": "Jason D. Williams and Geoffrey Zweig", "title": "End-to-end LSTM-based dialog control optimized with supervised and\n  reinforcement learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a model for end-to-end learning of task-oriented dialog\nsystems. The main component of the model is a recurrent neural network (an\nLSTM), which maps from raw dialog history directly to a distribution over\nsystem actions. The LSTM automatically infers a representation of dialog\nhistory, which relieves the system developer of much of the manual feature\nengineering of dialog state. In addition, the developer can provide software\nthat expresses business rules and provides access to programmatic APIs,\nenabling the LSTM to take actions in the real world on behalf of the user. The\nLSTM can be optimized using supervised learning (SL), where a domain expert\nprovides example dialogs which the LSTM should imitate; or using reinforcement\nlearning (RL), where the system improves by interacting directly with end\nusers. Experiments show that SL and RL are complementary: SL alone can derive a\nreasonable initial policy from a small number of training dialogs; and starting\nRL optimization with a policy trained with SL substantially accelerates the\nlearning rate of RL.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 20:32:52 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Williams", "Jason D.", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "1606.01280", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Jianpeng Cheng and Mirella Lapata", "title": "Dependency Parsing as Head Selection", "comments": "to appear in EACL 2017; Our code is available at\n  http://github.com/XingxingZhang/dense_parser", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional graph-based dependency parsers guarantee a tree structure both\nduring training and inference. Instead, we formalize dependency parsing as the\nproblem of independently selecting the head of each word in a sentence. Our\nmodel which we call \\textsc{DeNSe} (as shorthand for {\\bf De}pendency {\\bf\nN}eural {\\bf Se}lection) produces a distribution over possible heads for each\nword using features obtained from a bidirectional recurrent neural network.\nWithout enforcing structural constraints during training, \\textsc{DeNSe}\ngenerates (at inference time) trees for the overwhelming majority of sentences,\nwhile non-tree outputs can be adjusted with a maximum spanning tree algorithm.\nWe evaluate \\textsc{DeNSe} on four languages (English, Chinese, Czech, and\nGerman) with varying degrees of non-projectivity. Despite the simplicity of the\napproach, our parsers are on par with the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 21:27:03 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 20:25:02 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 22:22:10 GMT"}, {"version": "v4", "created": "Thu, 22 Dec 2016 15:28:34 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Zhang", "Xingxing", ""], ["Cheng", "Jianpeng", ""], ["Lapata", "Mirella", ""]]}, {"id": "1606.01283", "submitter": "Alexandre Salle", "authors": "Alexandre Salle and Marco Idiart and Aline Villavicencio", "title": "Enhancing the LexVec Distributed Word Representation Model Using\n  Positional Contexts and External Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we take a state-of-the-art model for distributed word\nrepresentation that explicitly factorizes the positive pointwise mutual\ninformation (PPMI) matrix using window sampling and negative sampling and\naddress two of its shortcomings. We improve syntactic performance by using\npositional contexts, and solve the need to store the PPMI matrix in memory by\nworking on aggregate data in external memory. The effectiveness of both\nmodifications is shown using word similarity and analogy tasks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 21:39:42 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Salle", "Alexandre", ""], ["Idiart", "Marco", ""], ["Villavicencio", "Aline", ""]]}, {"id": "1606.01292", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao and Baolin Peng and Geoffrey Zweig and Kam-Fai Wong", "title": "An Attentional Neural Conversation Model with Improved Specificity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a neural conversation model for conducting\ndialogues. We demonstrate the use of this model to generate help desk\nresponses, where users are asking questions about PC applications. Our model is\ndistinguished by two characteristics. First, it models intention across turns\nwith a recurrent network, and incorporates an attention model that is\nconditioned on the representation of intention. Secondly, it avoids generating\nnon-specific responses by incorporating an IDF term in the objective function.\nThe model is evaluated both as a pure generation model in which a help-desk\nresponse is generated from scratch, and as a retrieval model with performance\nmeasured using recall rates of the correct response. Experimental results\nindicate that the model outperforms previously proposed neural conversation\narchitectures, and that using specificity in the objective function\nsignificantly improves performances for both generation and retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 22:26:01 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Yao", "Kaisheng", ""], ["Peng", "Baolin", ""], ["Zweig", "Geoffrey", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1606.01305", "submitter": "David Krueger", "authors": "David Krueger, Tegan Maharaj, J\\'anos Kram\\'ar, Mohammad Pezeshki,\n  Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron\n  Courville, Chris Pal", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "comments": "David Krueger and Tegan Maharaj contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose zoneout, a novel method for regularizing RNNs. At each timestep,\nzoneout stochastically forces some hidden units to maintain their previous\nvalues. Like dropout, zoneout uses random noise to train a pseudo-ensemble,\nimproving generalization. But by preserving instead of dropping hidden units,\ngradient information and state information are more readily propagated through\ntime, as in feedforward stochastic depth networks. We perform an empirical\ninvestigation of various RNN regularizers, and find that zoneout gives\nsignificant performance improvements across tasks. We achieve competitive\nresults with relatively simple models in character- and word-level language\nmodelling on the Penn Treebank and Text8 datasets, and combining with recurrent\nbatch normalization yields state-of-the-art results on permuted sequential\nMNIST.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 23:31:47 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 18:59:48 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 03:12:03 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 20:43:09 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Krueger", "David", ""], ["Maharaj", "Tegan", ""], ["Kram\u00e1r", "J\u00e1nos", ""], ["Pezeshki", "Mohammad", ""], ["Ballas", "Nicolas", ""], ["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Pal", "Chris", ""]]}, {"id": "1606.01323", "submitter": "Kevin Clark", "authors": "Kevin Clark and Christopher D. Manning", "title": "Improving Coreference Resolution by Learning Entity-Level Distributed\n  Representations", "comments": "Accepted for publication at the Association for Computational\n  Linguistics (ACL), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing challenge in coreference resolution has been the\nincorporation of entity-level information - features defined over clusters of\nmentions instead of mention pairs. We present a neural network based\ncoreference system that produces high-dimensional vector representations for\npairs of coreference clusters. Using these representations, our system learns\nwhen combining clusters is desirable. We train the system with a\nlearning-to-search algorithm that teaches it which local decisions (cluster\nmerges) will lead to a high-scoring final coreference partition. The system\nsubstantially outperforms the current state-of-the-art on the English and\nChinese portions of the CoNLL 2012 Shared Task dataset despite using few\nhand-engineered features.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 04:08:45 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 21:11:13 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Clark", "Kevin", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1606.01341", "submitter": "Sonse Shimaoka", "authors": "Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel", "title": "Neural Architectures for Fine-grained Entity Type Classification", "comments": "10 pages, 3 figures, accepted at EACL2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we investigate several neural network architectures for\nfine-grained entity type classification. Particularly, we consider extensions\nto a recently proposed attentive neural architecture and make three key\ncontributions. Previous work on attentive neural architectures do not consider\nhand-crafted features, we combine learnt and hand-crafted features and observe\nthat they complement each other. Additionally, through quantitative analysis we\nestablish that the attention mechanism is capable of learning to attend over\nsyntactic heads and the phrase containing the mention, where both are known\nstrong hand-crafted features for our task. We enable parameter sharing through\na hierarchical label encoding method, that in low-dimensional projections show\nclear clusters for each type hierarchy. Lastly, despite using the same\nevaluation dataset, the literature frequently compare models trained using\ndifferent data. We establish that the choice of training data has a drastic\nimpact on performance, with decreases by as much as 9.85% loose micro F1 score\nfor a previously proposed method. Despite this, our best model achieves\nstate-of-the-art results with 75.36% loose micro F1 score on the well-\nestablished FIGER (GOLD) dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 07:52:22 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 06:49:42 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Shimaoka", "Sonse", ""], ["Stenetorp", "Pontus", ""], ["Inui", "Kentaro", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1606.01404", "submitter": "Tim Rockt\\\"aschel", "authors": "Vladyslav Kolesnyk, Tim Rockt\\\"aschel, Sebastian Riedel", "title": "Generating Natural Language Inference Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to reason with natural language is a fundamental prerequisite for\nmany NLP tasks such as information extraction, machine translation and question\nanswering. To quantify this ability, systems are commonly tested whether they\ncan recognize textual entailment, i.e., whether one sentence can be inferred\nfrom another one. However, in most NLP applications only single source\nsentences instead of sentence pairs are available. Hence, we propose a new task\nthat measures how well a model can generate an entailed sentence from a source\nsentence. We take entailment-pairs of the Stanford Natural Language Inference\ncorpus and train an LSTM with attention. On a manually annotated test set we\nfound that 82% of generated sentences are correct, an improvement of 10.3% over\nan LSTM baseline. A qualitative analysis shows that this model is not only\ncapable of shortening input sentences, but also inferring new statements via\nparaphrasing and phrase entailment. We then apply this model recursively to\ninput-output pairs, thereby generating natural language inference chains that\ncan be used to automatically construct an entailment graph from source\nsentences. Finally, by swapping source and target sentences we can also train a\nmodel that given an input sentence invents additional information to generate a\nnew sentence.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 18:34:51 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Kolesnyk", "Vladyslav", ""], ["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1606.01433", "submitter": "Jason Fries", "authors": "Jason Alan Fries", "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint\n  Inference for Clinical Temporal Information Extraction", "comments": "NAACL HLT 2016, SemEval-2016 Task 12 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval\nchallenge, participating in Phase 1, where we identified text spans of time and\nevent expressions in clinical notes and Phase 2, where we predicted a relation\nbetween an event and its parent document creation time.\n  For temporal entity extraction, we find that a joint inference-based approach\nusing structured prediction outperforms a vanilla recurrent neural network that\nincorporates word embeddings trained on a variety of large clinical document\nsets. For document creation time relations, we find that a combination of date\ncanonicalization and distant supervision rules for predicting relations on both\nevents and time expressions improves classification, though gains are limited,\nlikely due to the small scale of training data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2016 23:22:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Fries", "Jason Alan", ""]]}, {"id": "1606.01515", "submitter": "EPTCS", "authors": "Dimitri Kartsaklis (Queen Mary University of London)", "title": "Coordination in Categorical Compositional Distributional Semantics", "comments": "In Proceedings SLPCS 2016, arXiv:1608.01018", "journal-ref": "EPTCS 221, 2016, pp. 29-38", "doi": "10.4204/EPTCS.221.4", "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open problem with categorical compositional distributional semantics is\nthe representation of words that are considered semantically vacuous from a\ndistributional perspective, such as determiners, prepositions, relative\npronouns or coordinators. This paper deals with the topic of coordination\nbetween identical syntactic types, which accounts for the majority of\ncoordination cases in language. By exploiting the compact closed structure of\nthe underlying category and Frobenius operators canonically induced over the\nfixed basis of finite-dimensional vector spaces, we provide a morphism as\nrepresentation of a coordinator tensor, and we show how it lifts from atomic\ntypes to compound types. Linguistic intuitions are provided, and the importance\nof the Frobenius operators as an addition to the compact closed setting with\nregard to language is discussed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 14:26:56 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 00:41:00 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Kartsaklis", "Dimitri", "", "Queen Mary University of London"]]}, {"id": "1606.01541", "submitter": "Jiwei Li", "authors": "Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan\n  Jurafsky", "title": "Deep Reinforcement Learning for Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent neural models of dialogue generation offer great promise for\ngenerating responses for conversational agents, but tend to be shortsighted,\npredicting utterances one at a time while ignoring their influence on future\noutcomes. Modeling the future direction of a dialogue is crucial to generating\ncoherent, interesting dialogues, a need which led traditional NLP models of\ndialogue to draw on reinforcement learning. In this paper, we show how to\nintegrate these goals, applying deep reinforcement learning to model future\nreward in chatbot dialogue. The model simulates dialogues between two virtual\nagents, using policy gradient methods to reward sequences that display three\nuseful conversational properties: informativity (non-repetitive turns),\ncoherence, and ease of answering (related to forward-looking function). We\nevaluate our model on diversity, length as well as with human judges, showing\nthat the proposed algorithm generates more interactive responses and manages to\nfoster a more sustained conversation in dialogue simulation. This work marks a\nfirst step towards learning a neural conversational model based on the\nlong-term success of dialogues.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 17:59:23 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 05:44:47 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 04:49:39 GMT"}, {"version": "v4", "created": "Thu, 29 Sep 2016 15:02:32 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Li", "Jiwei", ""], ["Monroe", "Will", ""], ["Ritter", "Alan", ""], ["Galley", "Michel", ""], ["Gao", "Jianfeng", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1606.01545", "submitter": "Jiwei Li", "authors": "Jiwei Li, Dan Jurafsky", "title": "Neural Net Models for Open-Domain Discourse Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse coherence is strongly associated with text quality, making it\nimportant to natural language generation and understanding. Yet existing models\nof coherence focus on measuring individual aspects of coherence (lexical\noverlap, rhetorical structure, entity centering) in narrow domains.\n  In this paper, we describe domain-independent neural models of discourse\ncoherence that are capable of measuring multiple aspects of coherence in\nexisting sentences and can maintain coherence while generating new sentences.\nWe study both discriminative models that learn to distinguish coherent from\nincoherent discourse, and generative models that produce coherent text,\nincluding a novel neural latent-variable Markovian generative model that\ncaptures the latent discourse dependencies between sentences in a text.\n  Our work achieves state-of-the-art performance on multiple coherence\nevaluations, and marks an initial step in generating coherent texts given\ndiscourse contexts.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 18:29:45 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 00:21:43 GMT"}, {"version": "v3", "created": "Sun, 24 Sep 2017 01:38:11 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Jiwei", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1606.01549", "submitter": "Bhuwan Dhingra", "authors": "Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W. Cohen, Ruslan\n  Salakhutdinov", "title": "Gated-Attention Readers for Text Comprehension", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of answering cloze-style questions over\ndocuments. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop\narchitecture with a novel attention mechanism, which is based on multiplicative\ninteractions between the query embedding and the intermediate states of a\nrecurrent neural network document reader. This enables the reader to build\nquery-specific representations of tokens in the document for accurate answer\nselection. The GA Reader obtains state-of-the-art results on three benchmarks\nfor this task--the CNN \\& Daily Mail news stories and the Who Did What dataset.\nThe effectiveness of multiplicative interaction is demonstrated by an ablation\nstudy, and by comparing to alternative compositional operators for implementing\nthe gated-attention. The code is available at\nhttps://github.com/bdhingra/ga-reader.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 19:30:39 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 19:27:42 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 18:50:05 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Dhingra", "Bhuwan", ""], ["Liu", "Hanxiao", ""], ["Yang", "Zhilin", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1606.01603", "submitter": "Yiming Cui", "authors": "Ting Liu, Yiming Cui, Qingyu Yin, Weinan Zhang, Shijin Wang and\n  Guoping Hu", "title": "Generating and Exploiting Large-scale Pseudo Training Data for Zero\n  Pronoun Resolution", "comments": "8+2 pages, published as a conference paper at ACL2017 (long paper)", "journal-ref": null, "doi": "10.18653/v1/P17-1010", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing approaches for zero pronoun resolution are heavily relying on\nannotated data, which is often released by shared task organizers. Therefore,\nthe lack of annotated data becomes a major obstacle in the progress of zero\npronoun resolution task. Also, it is expensive to spend manpower on labeling\nthe data for better performance. To alleviate the problem above, in this paper,\nwe propose a simple but novel approach to automatically generate large-scale\npseudo training data for zero pronoun resolution. Furthermore, we successfully\ntransfer the cloze-style reading comprehension neural network model into zero\npronoun resolution task and propose a two-step training mechanism to overcome\nthe gap between the pseudo training data and the real one. Experimental results\nshow that the proposed approach significantly outperforms the state-of-the-art\nsystems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 02:45:47 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 09:18:35 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 02:47:41 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Liu", "Ting", ""], ["Cui", "Yiming", ""], ["Yin", "Qingyu", ""], ["Zhang", "Weinan", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1606.01614", "submitter": "Xilun Chen", "authors": "Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie and Kilian\n  Weinberger", "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment\n  Classification", "comments": "TACL journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years great success has been achieved in sentiment classification\nfor English, thanks in part to the availability of copious annotated resources.\nUnfortunately, most languages do not enjoy such an abundance of labeled data.\nTo tackle the sentiment classification problem in low-resource languages\nwithout adequate annotated data, we propose an Adversarial Deep Averaging\nNetwork (ADAN) to transfer the knowledge learned from labeled data on a\nresource-rich source language to low-resource languages where only unlabeled\ndata exists. ADAN has two discriminative branches: a sentiment classifier and\nan adversarial language discriminator. Both branches take input from a shared\nfeature extractor to learn hidden representations that are simultaneously\nindicative for the classification task and invariant across languages.\nExperiments on Chinese and Arabic sentiment classification demonstrate that\nADAN significantly outperforms state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 05:04:23 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 15:28:02 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 01:30:30 GMT"}, {"version": "v4", "created": "Mon, 17 Apr 2017 18:48:19 GMT"}, {"version": "v5", "created": "Sat, 18 Aug 2018 21:41:42 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Chen", "Xilun", ""], ["Sun", "Yu", ""], ["Athiwaratkun", "Ben", ""], ["Cardie", "Claire", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1606.01700", "submitter": "Yasumasa Miyamoto", "authors": "Yasumasa Miyamoto and Kyunghyun Cho", "title": "Gated Word-Character Recurrent Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a recurrent neural network language model (RNN-LM) with long\nshort-term memory (LSTM) units that utilizes both character-level and\nword-level inputs. Our model has a gate that adaptively finds the optimal\nmixture of the character-level and word-level inputs. The gate creates the\nfinal vector representation of a word by combining two distinct representations\nof the word. The character-level inputs are converted into vector\nrepresentations of words using a bidirectional LSTM. The word-level inputs are\nprojected into another high-dimensional space by a word lookup table. The final\nvector representations of words are used in the LSTM language model which\npredicts the next word given all the preceding words. Our model with the gating\nmechanism effectively utilizes the character-level inputs for rare and\nout-of-vocabulary words and outperforms word-level language models on several\nEnglish corpora.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 11:43:28 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 03:26:43 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Miyamoto", "Yasumasa", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1606.01720", "submitter": "Richard Moot", "authors": "Richard Moot (LaBRI)", "title": "Proof nets for the Displacement calculus", "comments": "Formal Grammar, Aug 2016, Bolzano, Italy. Springer, Proceedings of\n  Formal Grammar 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof net calculus for the Displacement calculus and show its\ncorrectness. This is the first proof net calculus which models the Displacement\ncalculus directly and not by some sort of translation into another formalism.\nThe proof net calculus opens up new possibilities for parsing and proof search\nwith the Displacement calculus.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 12:55:57 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Moot", "Richard", "", "LaBRI"]]}, {"id": "1606.01781", "submitter": "Alexis Conneau", "authors": "Alexis Conneau, Holger Schwenk, Lo\\\"ic Barrault, Yann Lecun", "title": "Very Deep Convolutional Networks for Text Classification", "comments": "10 pages, EACL 2017, camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach for many NLP tasks are recurrent neural networks, in\nparticular LSTMs, and convolutional neural networks. However, these\narchitectures are rather shallow in comparison to the deep convolutional\nnetworks which have pushed the state-of-the-art in computer vision. We present\na new architecture (VDCNN) for text processing which operates directly at the\ncharacter level and uses only small convolutions and pooling operations. We are\nable to show that the performance of this model increases with depth: using up\nto 29 convolutional layers, we report improvements over the state-of-the-art on\nseveral public text classification tasks. To the best of our knowledge, this is\nthe first time that very deep convolutional nets have been applied to text\nprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:14:50 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 12:49:11 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Conneau", "Alexis", ""], ["Schwenk", "Holger", ""], ["Barrault", "Lo\u00efc", ""], ["Lecun", "Yann", ""]]}, {"id": "1606.01792", "submitter": "Yaohua Tang", "authors": "Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li, Philip L.H. Yu", "title": "Neural Machine Translation with External Phrase Memory", "comments": "8 figures, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose phraseNet, a neural machine translator with a\nphrase memory which stores phrase pairs in symbolic form, mined from corpus or\nspecified by human experts. For any given source sentence, phraseNet scans the\nphrase memory to determine the candidate phrase pairs and integrates tagging\ninformation in the representation of source sentence accordingly. The decoder\nutilizes a mixture of word-generating component and phrase-generating\ncomponent, with a specifically designed strategy to generate a sequence of\nmultiple words all at once. The phraseNet not only approaches one step towards\nincorporating external knowledge into neural machine translation, but also\nmakes an effort to extend the word-by-word generation mechanism of recurrent\nneural network. Our empirical study on Chinese-to-English translation shows\nthat, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU\nimprovement over the generic neural machine translator.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 15:45:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Tang", "Yaohua", ""], ["Meng", "Fandong", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Yu", "Philip L. H.", ""]]}, {"id": "1606.01847", "submitter": "Marcus Rohrbach", "authors": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor\n  Darrell, and Marcus Rohrbach", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and\n  Visual Grounding", "comments": "Accepted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling textual or visual information with vector representations trained\nfrom large language or visual datasets has been successfully explored in recent\nyears. However, tasks such as visual question answering require combining these\nvector representations with each other. Approaches to multimodal pooling\ninclude element-wise product or sum, as well as concatenation of the visual and\ntextual representations. We hypothesize that these methods are not as\nexpressive as an outer product of the visual and textual vectors. As the outer\nproduct is typically infeasible due to its high dimensionality, we instead\npropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and\nexpressively combine multimodal features. We extensively evaluate MCB on the\nvisual question answering and grounding tasks. We consistently show the benefit\nof MCB over ablations without MCB. For visual question answering, we present an\narchitecture which uses MCB twice, once for predicting attention over spatial\nfeatures and again to combine the attended representation with the question\nrepresentation. This model outperforms the state-of-the-art on the Visual7W\ndataset and the VQA challenge.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 17:59:56 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 19:52:41 GMT"}, {"version": "v3", "created": "Sat, 24 Sep 2016 01:58:59 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Fukui", "Akira", ""], ["Park", "Dong Huk", ""], ["Yang", "Daylen", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1606.01933", "submitter": "Ankur Parikh", "authors": "Ankur P. Parikh, Oscar T\\\"ackstr\\\"om, Dipanjan Das, Jakob Uszkoreit", "title": "A Decomposable Attention Model for Natural Language Inference", "comments": "7 pages, 1 figure, Proceeedings of EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple neural architecture for natural language inference. Our\napproach uses attention to decompose the problem into subproblems that can be\nsolved separately, thus making it trivially parallelizable. On the Stanford\nNatural Language Inference (SNLI) dataset, we obtain state-of-the-art results\nwith almost an order of magnitude fewer parameters than previous work and\nwithout relying on any word-order information. Adding intra-sentence attention\nthat takes a minimum amount of order into account yields further improvements.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 20:30:57 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 23:52:45 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Parikh", "Ankur P.", ""], ["T\u00e4ckstr\u00f6m", "Oscar", ""], ["Das", "Dipanjan", ""], ["Uszkoreit", "Jakob", ""]]}, {"id": "1606.01990", "submitter": "Attapol Rutherford", "authors": "Attapol T. Rutherford, Vera Demberg, Nianwen Xue", "title": "Neural Network Models for Implicit Discourse Relation Classification in\n  English and Chinese without Surface Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring implicit discourse relations in natural language text is the most\ndifficult subtask in discourse parsing. Surface features achieve good\nperformance, but they are not readily applicable to other languages without\nsemantic lexicons. Previous neural models require parses, surface features, or\na small label set to work well. Here, we propose neural network models that are\nbased on feedforward and long-short term memory architecture without any\nsurface features. To our surprise, our best configured feedforward architecture\noutperforms LSTM-based model in most cases despite thorough tuning. Under\nvarious fine-grained label sets and a cross-linguistic setting, our feedforward\nmodels perform consistently better or at least just as well as systems that\nrequire hand-crafted surface features. Our models present the first neural\nChinese discourse parser in the style of Chinese Discourse Treebank, showing\nthat our results hold cross-linguistically.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 01:17:00 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Rutherford", "Attapol T.", ""], ["Demberg", "Vera", ""], ["Xue", "Nianwen", ""]]}, {"id": "1606.01994", "submitter": "Zihang Dai", "authors": "Zihang Dai, Lei Li, Wei Xu", "title": "CFO: Conditional Focused Neural Question Answering with Large-scale\n  Knowledge Bases", "comments": "Accepted by ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we enable computers to automatically answer questions like \"Who\ncreated the character Harry Potter\"? Carefully built knowledge bases provide\nrich sources of facts. However, it remains a challenge to answer factoid\nquestions raised in natural language due to numerous expressions of one\nquestion. In particular, we focus on the most common questions --- ones that\ncan be answered with a single fact in the knowledge base. We propose CFO, a\nConditional Focused neural-network-based approach to answering factoid\nquestions with knowledge bases. Our approach first zooms in a question to find\nmore probable candidate subject mentions, and infers the final answers with a\nunified conditional probabilistic framework. Powered by deep recurrent neural\nnetworks and neural embeddings, our proposed CFO achieves an accuracy of 75.7%\non a dataset of 108k questions - the largest public one to date. It outperforms\nthe current state of the art by an absolute margin of 11.8%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 01:36:07 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 03:04:38 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Dai", "Zihang", ""], ["Li", "Lei", ""], ["Xu", "Wei", ""]]}, {"id": "1606.02003", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang, Zhengdong Lu, Hang Li and Qun Liu", "title": "Memory-enhanced Decoder for Neural Machine Translation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to enhance the RNN decoder in a neural machine translator (NMT)\nwith external memory, as a natural but powerful extension to the state in the\ndecoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At\neach time during decoding, \\textsc{MemDec} will read from this memory and write\nto this memory once, both with content-based addressing. Unlike the unbounded\nmemory in previous work\\cite{RNNsearch} to store the representation of source\nsentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size\ndesigned to better capture the information important for the decoding process\nat each time step. Our empirical study on Chinese-English translation shows\nthat it can improve by $4.8$ BLEU upon Groundhog and $5.3$ BLEU upon on Moses,\nyielding the best performance achieved with the same training set.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 02:28:19 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Liu", "Qun", ""]]}, {"id": "1606.02006", "submitter": "Philip Arthur", "authors": "Philip Arthur, Graham Neubig, Satoshi Nakamura", "title": "Incorporating Discrete Translation Lexicons into Neural Machine\n  Translation", "comments": "Accepted at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) often makes mistakes in translating\nlow-frequency content words that are essential to understanding the meaning of\nthe sentence. We propose a method to alleviate this problem by augmenting NMT\nsystems with discrete translation lexicons that efficiently encode translations\nof these low-frequency words. We describe a method to calculate the lexicon\nprobability of the next word in the translation candidate by using the\nattention vector of the NMT model to select which source word lexical\nprobabilities the model should focus on. We test two methods to combine this\nprobability with the standard NMT probability: (1) using it as a bias, and (2)\nlinear interpolation. Experiments on two corpora show an improvement of 2.0-2.3\nBLEU and 0.13-0.44 NIST score, and faster convergence time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 02:40:42 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 02:46:39 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Arthur", "Philip", ""], ["Neubig", "Graham", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1606.02012", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Masha Esipova", "title": "Can neural machine translation do simultaneous translation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the potential of attention-based neural machine translation in\nsimultaneous translation. We introduce a novel decoding algorithm, called\nsimultaneous greedy decoding, that allows an existing neural machine\ntranslation model to begin translating before a full source sentence is\nreceived. This approach is unique from previous works on simultaneous\ntranslation in that segmentation and translation are done jointly to maximize\nthe translation quality and that translating each segment is strongly\nconditioned on all the previous segments. This paper presents a first step\ntoward building a full simultaneous translation system based on neural machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 03:38:46 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Esipova", "Masha", ""]]}, {"id": "1606.02126", "submitter": "Chenhui Chu", "authors": "Chenhui Chu and Sadao Kurohashi", "title": "Supervised Syntax-based Alignment between English Sentences and Abstract\n  Meaning Representation Graphs", "comments": "Updated the paper with AMR parsing results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As alignment links are not given between English sentences and Abstract\nMeaning Representation (AMR) graphs in the AMR annotation, automatic alignment\nbecomes indispensable for training an AMR parser. Previous studies formalize it\nas a string-to-string problem and solve it in an unsupervised way, which\nsuffers from data sparseness due to the small size of training data for\nEnglish-AMR alignment. In this paper, we formalize it as a syntax-based\nalignment problem and solve it in a supervised manner based on syntax trees,\nwhich can address the data sparseness problem by generalizing English-AMR\ntokens to syntax tags. Experiments verify the effectiveness of the proposed\nmethod not only for English-AMR alignment, but also for AMR parsing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 13:00:48 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 07:26:13 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 01:02:41 GMT"}, {"version": "v4", "created": "Sat, 18 Feb 2017 01:53:58 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Chu", "Chenhui", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "1606.02245", "submitter": "Alessandro Sordoni", "authors": "Alessandro Sordoni and Philip Bachman and Adam Trischler and Yoshua\n  Bengio", "title": "Iterative Alternating Neural Attention for Machine Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural attention architecture to tackle machine\ncomprehension tasks, such as answering Cloze-style queries with respect to a\ndocument. Unlike previous models, we do not collapse the query into a single\nvector, instead we deploy an iterative alternating attention mechanism that\nallows a fine-grained exploration of both the query and the document. Our model\noutperforms state-of-the-art baselines in standard machine comprehension\nbenchmarks such as CNN news articles and the Children's Book Test (CBT)\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 18:25:48 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 18:17:03 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 21:16:56 GMT"}, {"version": "v4", "created": "Wed, 9 Nov 2016 18:11:09 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Sordoni", "Alessandro", ""], ["Bachman", "Philip", ""], ["Trischler", "Adam", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1606.02270", "submitter": "Adam Trischler", "authors": "Adam Trischler, Zheng Ye, Xingdi Yuan, Kaheer Suleman", "title": "Natural Language Comprehension with the EpiReader", "comments": "8 pages plus references. Submitted to EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the EpiReader, a novel model for machine comprehension of text.\nMachine comprehension of unstructured, real-world text is a major research goal\nfor natural language processing. Current tests of machine comprehension pose\nquestions whose answers can be inferred from some supporting text, and evaluate\na model's response to the questions. The EpiReader is an end-to-end neural\nmodel comprising two components: the first component proposes a small set of\ncandidate answers after comparing a question to its supporting text, and the\nsecond component formulates hypotheses using the proposed candidates and the\nquestion, then reranks the hypotheses based on their estimated concordance with\nthe supporting text. We present experiments demonstrating that the EpiReader\nsets a new state-of-the-art on the CNN and Children's Book Test machine\ncomprehension benchmarks, outperforming previous neural models by a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:27:04 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 15:43:51 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Trischler", "Adam", ""], ["Ye", "Zheng", ""], ["Yuan", "Xingdi", ""], ["Suleman", "Kaheer", ""]]}, {"id": "1606.02276", "submitter": "Mercan Topkara", "authors": "Nikolaos Pappas, Miriam Redi, Mercan Topkara, Brendan Jou, Hongyi Liu,\n  Tao Chen, Shih-Fu Chang", "title": "Multilingual Visual Sentiment Concept Matching", "comments": null, "journal-ref": "Proceedings ICMR '16 Proceedings of the 2016 ACM on International\n  Conference on Multimedia Retrieval Pages 151-158", "doi": "10.1145/2911996.2912016", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:40:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Jou", "Brendan", ""], ["Liu", "Hongyi", ""], ["Chen", "Tao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1606.02342", "submitter": "Shashi Narayan", "authors": "Shashi Narayan and Shay B. Cohen", "title": "Optimizing Spectral Learning for Parsing", "comments": "11 pages, ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a search algorithm for optimizing the number of latent states\nwhen estimating latent-variable PCFGs with spectral methods. Our results show\nthat contrary to the common belief that the number of latent states for each\nnonterminal in an L-PCFG can be decided in isolation with spectral methods,\nparsing results significantly improve if the number of latent states for each\nnonterminal is globally optimized, while taking into account interactions\nbetween the different nonterminals. In addition, we contribute an empirical\nanalysis of spectral algorithms on eight morphologically rich languages:\nBasque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our\nresults show that our estimation consistently performs better or close to\ncoarse-to-fine expectation-maximization techniques for these languages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 21:58:41 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 08:34:12 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 13:10:41 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Narayan", "Shashi", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1606.02440", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO), Lawrence Muchemi (TAO)", "title": "On the Place of Text Data in Lifelogs, and Text Analysis via Semantic\n  Facets", "comments": null, "journal-ref": "iConference 2016 SIE on Lifelogging, Mar 2016, Philadelphia,\n  United States. iConference 2016 SIE on Lifelogging, 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research in lifelog data has not paid enough attention to analysis of\ncognitive activities in comparison to physical activities. We argue that as we\nlook into the future, wearable devices are going to be cheaper and more\nprevalent and textual data will play a more significant role. Data captured by\nlifelogging devices will increasingly include speech and text, potentially\nuseful in analysis of intellectual activities. Analyzing what a person hears,\nreads, and sees, we should be able to measure the extent of cognitive activity\ndevoted to a certain topic or subject by a learner. Test-based lifelog records\ncan benefit from semantic analysis tools developed for natural language\nprocessing. We show how semantic analysis of such text data can be achieved\nthrough the use of taxonomic subject facets and how these facets might be\nuseful in quantifying cognitive activity devoted to various topics in a\nperson's day. We are currently developing a method to automatically create\ntaxonomic topic vocabularies that can be applied to this detection of\nintellectual activity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 08:11:54 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"], ["Muchemi", "Lawrence", "", "TAO"]]}, {"id": "1606.02447", "submitter": "Sida Wang", "authors": "Sida I. Wang and Percy Liang and Christopher D. Manning", "title": "Learning Language Games through Interaction", "comments": "11 pages, ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new language learning setting relevant to building adaptive\nnatural language interfaces. It is inspired by Wittgenstein's language games: a\nhuman wishes to accomplish some task (e.g., achieving a certain configuration\nof blocks), but can only communicate with a computer, who performs the actual\nactions (e.g., removing all red blocks). The computer initially knows nothing\nabout language and therefore must learn it from scratch through interaction,\nwhile the human adapts to the computer's capabilities. We created a game in a\nblocks world and collected interactions from 100 people playing it. First, we\nanalyze the humans' strategies, showing that using compositionality and\navoiding synonyms correlates positively with task performance. Second, we\ncompare computer strategies, showing how to quickly learn a semantic parsing\nmodel from scratch, and that modeling pragmatics further accelerates learning\nfor successful players.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 08:27:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Wang", "Sida I.", ""], ["Liang", "Percy", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1606.02461", "submitter": "Ran Tian", "authors": "Ran Tian and Naoaki Okazaki and Kentaro Inui", "title": "Learning Semantically and Additively Compositional Distributional\n  Representations", "comments": "to appear in ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper connects a vector-based composition model to a formal semantics,\nthe Dependency-based Compositional Semantics (DCS). We show theoretical\nevidence that the vector compositions in our model conform to the logic of DCS.\nExperimentally, we show that vector-based composition brings a strong ability\nto calculate similar phrases as similar vectors, achieving near\nstate-of-the-art on a wide range of phrase similarity tasks and relation\nclassification; meanwhile, DCS can guide building vectors for structured\nqueries that can be directly executed. We evaluate this utility on sentence\ncompletion task and report a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 09:12:17 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Tian", "Ran", ""], ["Okazaki", "Naoaki", ""], ["Inui", "Kentaro", ""]]}, {"id": "1606.02514", "submitter": "Luis Espinosa-Anke", "authors": "Luis Espinosa-Anke, Roberto Carlini, Horacio Saggion, Francesco\n  Ronzano", "title": "DefExt: A Semi Supervised Definition Extraction Tool", "comments": "GLOBALEX 2016 Lexicographic Resources for Human Language Technology\n  Workshop Programme (p. 24) (2016, May)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present DefExt, an easy to use semi supervised Definition Extraction Tool.\nDefExt is designed to extract from a target corpus those textual fragments\nwhere a term is explicitly mentioned together with its core features, i.e. its\ndefinition. It works on the back of a Conditional Random Fields based\nsequential labeling algorithm and a bootstrapping approach. Bootstrapping\nenables the model to gradually become more aware of the idiosyncrasies of the\ntarget corpus. In this paper we describe the main components of the toolkit as\nwell as experimental results stemming from both automatic and manual\nevaluation. We release DefExt as open source along with the necessary files to\nrun it in any Unix machine. We also provide access to training and test data\nfor immediate use.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 11:22:12 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Espinosa-Anke", "Luis", ""], ["Carlini", "Roberto", ""], ["Saggion", "Horacio", ""], ["Ronzano", "Francesco", ""]]}, {"id": "1606.02529", "submitter": "Jessica Ficler", "authors": "Jessica Ficler and Yoav Goldberg", "title": "Coordination Annotation Extension in the Penn Tree Bank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination is an important and common syntactic construction which is not\nhandled well by state of the art parsers. Coordinations in the Penn Treebank\nare missing internal structure in many cases, do not include explicit marking\nof the conjuncts and contain various errors and inconsistencies. In this work,\nwe initiated manual annotation process for solving these issues. We identify\nthe different elements in a coordination phrase and label each element with its\nfunction. We add phrase boundaries when these are missing, unify\ninconsistencies, and fix errors. The outcome is an extension of the PTB that\nincludes consistent and detailed structures for coordinations. We make the\ncoordination annotation publicly available, in hope that they will facilitate\nfurther research into coordination disambiguation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 12:44:51 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Ficler", "Jessica", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1606.02555", "submitter": "Marco Dinarelli", "authors": "Marco Dinarelli and Isabelle Tellier", "title": "Improving Recurrent Neural Networks For Sequence Labelling", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study different types of Recurrent Neural Networks (RNN) for\nsequence labeling tasks. We propose two new variants of RNNs integrating\nimprovements for sequence labeling, and we compare them to the more traditional\nElman and Jordan RNNs. We compare all models, either traditional or new, on\nfour distinct tasks of sequence labeling: two on Spoken Language Understanding\n(ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the\nPenn Treebank (PTB) corpora. The results show that our new variants of RNNs are\nalways more effective than the others.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 13:47:18 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Dinarelli", "Marco", ""], ["Tellier", "Isabelle", ""]]}, {"id": "1606.02560", "submitter": "Tiancheng Zhao", "authors": "Tiancheng Zhao and Maxine Eskenazi", "title": "Towards End-to-End Learning for Dialog State Tracking and Management\n  using Deep Reinforcement Learning", "comments": "In proceeding of SIGDIAL 2016. Added changes based-on peer review,\n  including: 1. Added references, 2. fixed typos in text and figures, 3. added\n  minor change to introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an end-to-end framework for task-oriented dialog systems\nusing a variant of Deep Recurrent Q-Networks (DRQN). The model is able to\ninterface with a relational database and jointly learn policies for both\nlanguage understanding and dialog strategy. Moreover, we propose a hybrid\nalgorithm that combines the strength of reinforcement learning and supervised\nlearning to achieve faster learning speed. We evaluated the proposed model on a\n20 Question Game conversational game simulator. Results show that the proposed\nmethod outperforms the modular-based baseline and learns a distributed\nrepresentation of the latent dialog state.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:03:25 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 21:50:30 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Zhao", "Tiancheng", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1606.02562", "submitter": "Tiancheng Zhao", "authors": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi", "title": "DialPort: Connecting the Spoken Dialog Research Community to Real User\n  Data", "comments": "Under Peer Review of SigDial 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a new spoken dialog portal that connects systems\nproduced by the spoken dialog academic research community and gives them access\nto real users. We introduce a distributed, multi-modal, multi-agent prototype\ndialog framework that affords easy integration with various remote resources,\nranging from end-to-end dialog systems to external knowledge APIs. To date, the\nDialPort portal has successfully connected to the multi-domain spoken dialog\nsystem at Cambridge University, the NOAA (National Oceanic and Atmospheric\nAdministration) weather API and the Yelp API.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 14:08:21 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Zhao", "Tiancheng", ""], ["Lee", "Kyusong", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1606.02601", "submitter": "Kris Cao", "authors": "Kris Cao and Marek Rei", "title": "A Joint Model for Word Embedding and Word Morphology", "comments": "Submission for first Representation Learning for NLP workshop at\n  ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a joint model for performing unsupervised morphological\nanalysis on words, and learning a character-level composition function from\nmorphemes to word embeddings. Our model splits individual words into segments,\nand weights each segment according to its ability to predict context words. Our\nmorphological analysis is comparable to dedicated morphological analyzers at\nthe task of morpheme boundary recovery, and also performs better than\nword-based embedding models at the task of syntactic analogy answering.\nFinally, we show that incorporating morphology explicitly into character-level\nmodels help them produce embeddings for unseen words which correlate better\nwith human judgments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 15:24:22 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Cao", "Kris", ""], ["Rei", "Marek", ""]]}, {"id": "1606.02638", "submitter": "Preethi Raghavan", "authors": "Chaitanya Shivade, Preethi Raghavan, Siddharth Patwardhan", "title": "Addressing Limited Data for Textual Entailment Across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to address the lack of labeled data (and high cost of annotation) for\ntextual entailment in some domains. To that end, we first create (for\nexperimental purposes) an entailment dataset for the clinical domain, and a\nhighly competitive supervised entailment system, ENT, that is effective (out of\nthe box) on two domains. We then explore self-training and active learning\nstrategies to address the lack of labeled data. With self-training, we\nsuccessfully exploit unlabeled data to improve over ENT by 15% F-score on the\nnewswire domain, and 13% F-score on clinical data. On the other hand, our\nactive learning experiments demonstrate that we can match (and even beat) ENT\nusing only 6.6% of the training data in the clinical domain, and only 5.8% of\nthe training data in the newswire domain.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 16:56:19 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Shivade", "Chaitanya", ""], ["Raghavan", "Preethi", ""], ["Patwardhan", "Siddharth", ""]]}, {"id": "1606.02680", "submitter": "Amjad Almahairi", "authors": "Amjad Almahairi, Kyunghyun Cho, Nizar Habash and Aaron Courville", "title": "First Result on Arabic Neural Machine Translation", "comments": "EMNLP submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation has become a major alternative to widely used\nphrase-based statistical machine translation. We notice however that much of\nresearch on neural machine translation has focused on European languages\ndespite its language agnostic nature. In this paper, we apply neural machine\ntranslation to the task of Arabic translation (Ar<->En) and compare it against\na standard phrase-based translation system. We run extensive comparison using\nvarious configurations in preprocessing Arabic script and show that the\nphrase-based and neural translation systems perform comparably to each other\nand that proper preprocessing of Arabic script has a similar effect on both of\nthe systems. We however observe that the neural machine translation\nsignificantly outperform the phrase-based system on an out-of-domain test set,\nmaking it attractive for real-world deployment.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 18:36:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Almahairi", "Amjad", ""], ["Cho", "Kyunghyun", ""], ["Habash", "Nizar", ""], ["Courville", "Aaron", ""]]}, {"id": "1606.02689", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan\n  Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young", "title": "Continuously Learning Neural Dialogue Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a two-step approach for dialogue management in task-oriented\nspoken dialogue systems. A unified neural network framework is proposed to\nenable the system to first learn by supervision from a set of dialogue data and\nthen continuously improve its behaviour via reinforcement learning, all using\ngradient-based algorithms on one single model. The experiments demonstrate the\nsupervised model's effectiveness in the corpus-based evaluation, with user\nsimulation, and with paid human subjects. The use of reinforcement learning\nfurther improves the model's performance in both interactive settings,\nespecially under higher-noise conditions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2016 19:03:06 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Su", "Pei-Hao", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Rojas-Barahona", "Lina", ""], ["Ultes", "Stefan", ""], ["Vandyke", "David", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1606.02785", "submitter": "Lu Wang", "authors": "Lu Wang and Wang Ling", "title": "Neural Network-Based Abstract Generation for Opinions and Arguments", "comments": "NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of generating abstractive summaries for opinionated\ntext. We propose an attention-based neural network model that is able to absorb\ninformation from multiple text units to construct informative, concise, and\nfluent summaries. An importance-based sampling method is designed to allow the\nencoder to integrate information from an important subset of input. Automatic\nevaluation indicates that our system outperforms state-of-the-art abstractive\nand extractive summarization systems on two newly collected datasets of movie\nreviews and arguments. Our system summaries are also rated as more informative\nand grammatical in human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 00:15:23 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Wang", "Lu", ""], ["Ling", "Wang", ""]]}, {"id": "1606.02820", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Kevin Clark, Jure Leskovec, Dan Jurafsky", "title": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora", "comments": "11 pages, 5 figures, EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A word's sentiment depends on the domain in which it is used. Computational\nsocial science research thus requires sentiment lexicons that are specific to\nthe domains being studied. We combine domain-specific word embeddings with a\nlabel propagation framework to induce accurate domain-specific sentiment\nlexicons using small sets of seed words, achieving state-of-the-art performance\ncompetitive with approaches that rely on hand-curated resources. Using our\nframework we perform two large-scale empirical studies to quantify the extent\nto which sentiment varies across time and between communities. We induce and\nrelease historical sentiment lexicons for 150 years of English and\ncommunity-specific sentiment lexicons for 250 online communities from the\nsocial media forum Reddit. The historical lexicons show that more than 5% of\nsentiment-bearing (non-neutral) English words completely switched polarity\nduring the last 150 years, and the community-specific lexicons highlight how\nsentiment varies drastically between different communities.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 04:28:10 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 03:12:09 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Hamilton", "William L.", ""], ["Clark", "Kevin", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1606.02821", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Jure Leskovec, Dan Jurafsky", "title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures\n  of Semantic Change", "comments": "5 pages, 3 figures, EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words shift in meaning for many reasons, including cultural factors like new\ntechnologies and regular linguistic processes like subjectification.\nUnderstanding the evolution of language and culture requires disentangling\nthese underlying causes. Here we show how two different distributional measures\ncan be used to detect two different types of semantic change. The first\nmeasure, which has been used in many previous works, analyzes global shifts in\na word's distributional semantics, it is sensitive to changes due to regular\nprocesses of linguistic drift, such as the semantic generalization of promise\n(\"I promise.\" -> \"It promised to be exciting.\"). The second measure, which we\ndevelop here, focuses on local changes to a word's nearest semantic neighbors;\nit is more sensitive to cultural shifts, such as the change in the meaning of\ncell (\"prison cell\" -> \"cell phone\"). Comparing measurements made by these two\nmethods allows researchers to determine whether changes are more cultural or\nlinguistic in nature, a distinction that is essential for work in the digital\nhumanities and historical linguistics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 04:42:12 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 03:45:27 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1606.02858", "submitter": "Danqi Chen", "authors": "Danqi Chen, Jason Bolton, Christopher D. Manning", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "comments": "ACL 2016, updated results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling a computer to understand a document so that it can answer\ncomprehension questions is a central, yet unsolved goal of NLP. A key factor\nimpeding its solution by machine learned systems is the limited availability of\nhuman-annotated data. Hermann et al. (2015) seek to solve this problem by\ncreating over a million training examples by pairing CNN and Daily Mail news\narticles with their summarized bullet points, and show that a neural network\ncan then be trained to give good performance on this task. In this paper, we\nconduct a thorough examination of this new reading comprehension task. Our\nprimary aim is to understand what depth of language understanding is required\nto do well on this task. We approach this from one side by doing a careful\nhand-analysis of a small subset of the problems and from the other by showing\nthat simple, carefully designed systems can obtain accuracies of 73.6% and\n76.6% on these two datasets, exceeding current state-of-the-art results by\n7-10% and approaching what we believe is the ceiling for performance on this\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 08:19:16 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 21:21:19 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Chen", "Danqi", ""], ["Bolton", "Jason", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1606.02891", "submitter": "Rico Sennrich", "authors": "Rico Sennrich, Barry Haddow, Alexandra Birch", "title": "Edinburgh Neural Machine Translation Systems for WMT 16", "comments": "WMT16 shared task system description - final version with human\n  evaluation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participated in the WMT 2016 shared news translation task by building\nneural translation systems for four language pairs, each trained in both\ndirections: English<->Czech, English<->German, English<->Romanian and\nEnglish<->Russian. Our systems are based on an attentional encoder-decoder,\nusing BPE subword segmentation for open-vocabulary translation with a fixed\nvocabulary. We experimented with using automatic back-translations of the\nmonolingual News corpus as additional training data, pervasive dropout, and\ntarget-bidirectional models. All reported methods give substantial\nimprovements, and we see improvements of 4.3--11.2 BLEU over our baseline\nsystems. In the human evaluation, our systems were the (tied) best constrained\nsystem for 7 out of 8 translation directions in which we participated.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 10:06:28 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 23:02:24 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Sennrich", "Rico", ""], ["Haddow", "Barry", ""], ["Birch", "Alexandra", ""]]}, {"id": "1606.02892", "submitter": "Rico Sennrich", "authors": "Rico Sennrich, Barry Haddow", "title": "Linguistic Input Features Improve Neural Machine Translation", "comments": "WMT16 final version; new EN-RO results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation has recently achieved impressive results, while\nusing little in the way of external linguistic information. In this paper we\nshow that the strong learning capability of neural MT models does not make\nlinguistic features redundant; they can be easily incorporated to provide\nfurther improvements in performance. We generalize the embedding layer of the\nencoder in the attentional encoder--decoder architecture to support the\ninclusion of arbitrary features, in addition to the baseline word feature. We\nadd morphological features, part-of-speech tags, and syntactic dependency\nlabels as input features to English<->German, and English->Romanian neural\nmachine translation systems. In experiments on WMT16 training and test sets, we\nfind that linguistic input features improve model quality according to three\nmetrics: perplexity, BLEU and CHRF3. An open-source implementation of our\nneural MT system is available, as are sample files and configurations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 10:12:36 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 23:11:51 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Sennrich", "Rico", ""], ["Haddow", "Barry", ""]]}, {"id": "1606.02960", "submitter": "Sam Wiseman", "authors": "Sam Wiseman and Alexander M. Rush", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "comments": "EMNLP 2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 13:29:34 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 03:45:30 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wiseman", "Sam", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1606.02976", "submitter": "Gayo Diallo", "authors": "Khadim Dram\\'e (UB), Fleur Mougin (UB), Gayo Diallo (UB)", "title": "Large scale biomedical texts classification: a kNN and an ESA-based\n  approaches", "comments": "Journal of Biomedical Semantics, BioMed Central, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large and increasing volume of textual data, automated methods for\nidentifying significant topics to classify textual documents have received a\ngrowing interest. While many efforts have been made in this direction, it still\nremains a real challenge. Moreover, the issue is even more complex as full\ntexts are not always freely available. Then, using only partial information to\nannotate these documents is promising but remains a very ambitious issue.\nMethodsWe propose two classification methods: a k-nearest neighbours\n(kNN)-based approach and an explicit semantic analysis (ESA)-based approach.\nAlthough the kNN-based approach is widely used in text classification, it needs\nto be improved to perform well in this specific classification problem which\ndeals with partial information. Compared to existing kNN-based methods, our\nmethod uses classical Machine Learning (ML) algorithms for ranking the labels.\nAdditional features are also investigated in order to improve the classifiers'\nperformance. In addition, the combination of several learning algorithms with\nvarious techniques for fixing the number of relevant topics is performed. On\nthe other hand, ESA seems promising for this classification task as it yielded\ninteresting results in related issues, such as semantic relatedness computation\nbetween texts and text classification. Unlike existing works, which use ESA for\nenriching the bag-of-words approach with additional knowledge-based features,\nour ESA-based method builds a standalone classifier. Furthermore, we\ninvestigate if the results of this method could be useful as a complementary\nfeature of our kNN-based approach.ResultsExperimental evaluations performed on\nlarge standard annotated datasets, provided by the BioASQ organizers, show that\nthe kNN-based method with the Random Forest learning algorithm achieves good\nperformances compared with the current state-of-the-art methods, reaching a\ncompetitive f-measure of 0.55% while the ESA-based approach surprisingly\nyielded reserved results.ConclusionsWe have proposed simple classification\nmethods suitable to annotate textual documents using only partial information.\nThey are therefore adequate for large multi-label classification and\nparticularly in the biomedical domain. Thus, our work contributes to the\nextraction of relevant information from unstructured documents in order to\nfacilitate their automated processing. Consequently, it could be used for\nvarious purposes, including document indexing, information retrieval, etc.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:32:50 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Dram\u00e9", "Khadim", "", "UB"], ["Mougin", "Fleur", "", "UB"], ["Diallo", "Gayo", "", "UB"]]}, {"id": "1606.02979", "submitter": "Shaohua Li", "authors": "Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao", "title": "Generative Topic Embedding: a Continuous Representation of Documents\n  (Extended Version with Proofs)", "comments": "13 pages. The original version has been accepted in ACL 2016 as a\n  long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding maps words into a low-dimensional continuous embedding space\nby exploiting the local word collocation patterns in a small context window. On\nthe other hand, topic modeling maps documents onto a low-dimensional topic\nspace, by utilizing the global word collocation patterns in the same document.\nThese two types of patterns are complementary. In this paper, we propose a\ngenerative topic embedding model to combine the two types of patterns. In our\nmodel, topics are represented by embedding vectors, and are shared across\ndocuments. The probability of each word is influenced by both its local context\nand its topic. A variational inference method yields the topic embeddings as\nwell as the topic mixing proportions for each document. Jointly they represent\nthe document in a low-dimensional continuous space. In two document\nclassification tasks, our method performs better than eight existing methods,\nwith fewer features. In addition, we illustrate with an example that our method\ncan generate coherent topics even based on only one document.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:45:39 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:49:07 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Li", "Shaohua", ""], ["Chua", "Tat-Seng", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1606.03002", "submitter": "Dirk Weissenborn", "authors": "Dirk Weissenborn and Tim Rockt\\\"aschel", "title": "MuFuRU: The Multi-Function Recurrent Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks such as the GRU and LSTM found wide adoption in\nnatural language processing and achieve state-of-the-art results for many\ntasks. These models are characterized by a memory state that can be written to\nand read from by applying gated composition operations to the current input and\nthe previous state. However, they only cover a small subset of potentially\nuseful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that\nallow for arbitrary differentiable functions as composition operations.\nFurthermore, MuFuRUs allow for an input- and state-dependent choice of these\ncomposition operations that is learned. Our experiments demonstrate that the\nadditional functionality helps in different sequence modeling tasks, including\nthe evaluation of propositional logic formulae, language modeling and sentiment\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 15:41:17 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Weissenborn", "Dirk", ""], ["Rockt\u00e4schel", "Tim", ""]]}, {"id": "1606.03126", "submitter": "Jason  Weston", "authors": "Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi,\n  Antoine Bordes, Jason Weston", "title": "Key-Value Memory Networks for Directly Reading Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directly reading documents and being able to answer questions from them is an\nunsolved challenge. To avoid its inherent difficulty, question answering (QA)\nhas been directed towards using Knowledge Bases (KBs) instead, which has proven\neffective. Unfortunately KBs often suffer from being too restrictive, as the\nschema cannot support certain types of answers, and too sparse, e.g. Wikipedia\ncontains much more information than Freebase. In this work we introduce a new\nmethod, Key-Value Memory Networks, that makes reading documents more viable by\nutilizing different encodings in the addressing and output stages of the memory\nread operation. To compare using KBs, information extraction or Wikipedia\ndocuments directly in a single framework we construct an analysis tool,\nWikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in\nthe domain of movies. Our method reduces the gap between all three settings. It\nalso achieves state-of-the-art results on the existing WikiQA benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 21:33:55 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 20:14:10 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Miller", "Alexander", ""], ["Fisch", "Adam", ""], ["Dodge", "Jesse", ""], ["Karimi", "Amir-Hossein", ""], ["Bordes", "Antoine", ""], ["Weston", "Jason", ""]]}, {"id": "1606.03143", "submitter": "Shibamouli Lahiri", "authors": "Saeid Parvandeh, Shibamouli Lahiri, Fahimeh Boroumand", "title": "PerSum: Novel Systems for Document Summarization in Persian", "comments": "42 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the problem of document summarization in Persian\nlanguage from two distinct angles. In our first approach, we modify a popular\nand widely cited Persian document summarization framework to see how it works\non a realistic corpus of news articles. Human evaluation on generated summaries\nshows that graph-based methods perform better than the modified systems. We\ncarry this intuition forward in our second approach, and probe deeper into the\nnature of graph-based systems by designing several summarizers based on\ncentrality measures. Ad hoc evaluation using ROUGE score on these summarizers\nsuggests that there is a small class of centrality measures that perform better\nthan three strong unsupervised baselines.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 23:32:41 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Parvandeh", "Saeid", ""], ["Lahiri", "Shibamouli", ""], ["Boroumand", "Fahimeh", ""]]}, {"id": "1606.03144", "submitter": "Marek Rei", "authors": "Marek Rei and Ronan Cummins", "title": "Sentence Similarity Measures for Fine-Grained Estimation of Topical\n  Relevance in Learner Essays", "comments": "Accepted for publication at BEA-2016", "journal-ref": null, "doi": "10.18653/v1/W16-0533", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of assessing sentence-level prompt relevance in\nlearner essays. Various systems using word overlap, neural embeddings and\nneural compositional models are evaluated on two datasets of learner writing.\nWe propose a new method for sentence-level similarity calculation, which learns\nto adjust the weights of pre-trained word embeddings for a specific task,\nachieving substantially higher accuracy compared to other relevant baselines.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 23:42:45 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rei", "Marek", ""], ["Cummins", "Ronan", ""]]}, {"id": "1606.03152", "submitter": "Mehdi Fatemi", "authors": "Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, Kaheer Suleman", "title": "Policy Networks with Two-Stage Training for Dialogue Systems", "comments": "SIGDial 2016 (Submitted: May 2016; Accepted: Jun 30, 2016)", "journal-ref": "Proceedings of the SIGDIAL 2016 Conference, pages 101--110, Los\n  Angeles, USA, 13-15 September 2016. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to use deep policy networks which are trained with\nan advantage actor-critic method for statistically optimised dialogue systems.\nFirst, we show that, on summary state and action spaces, deep Reinforcement\nLearning (RL) outperforms Gaussian Processes methods. Summary state and action\nspaces lead to good performance but require pre-engineering effort, RL\nknowledge, and domain expertise. In order to remove the need to define such\nsummary spaces, we show that deep RL can also be trained efficiently on the\noriginal state and action spaces. Dialogue systems based on partially\nobservable Markov decision processes are known to require many dialogues to\ntrain, which makes them unappealing for practical deployment. We show that a\ndeep RL method based on an actor-critic architecture can exploit a small amount\nof data very efficiently. Indeed, with only a few hundred dialogues collected\nwith a handcrafted policy, the actor-critic deep learner is considerably\nbootstrapped from a combination of supervised and batch RL. In addition,\nconvergence to an optimal policy is significantly sped up compared to other\ndeep RL methods initialized on the data with batch RL. All experiments are\nperformed on a restaurant domain derived from the Dialogue State Tracking\nChallenge 2 (DSTC2) dataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 01:02:19 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 16:20:18 GMT"}, {"version": "v3", "created": "Sat, 20 Aug 2016 21:20:21 GMT"}, {"version": "v4", "created": "Mon, 12 Sep 2016 16:23:42 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Fatemi", "Mehdi", ""], ["Asri", "Layla El", ""], ["Schulz", "Hannes", ""], ["He", "Jing", ""], ["Suleman", "Kaheer", ""]]}, {"id": "1606.03153", "submitter": "Furong Huang", "authors": "Furong Huang, Animashree Anandkumar", "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via\n  Convolutional Tensor Decomposition", "comments": "There was an error in section 3, there is a bug in the experiment\n  section. We would like to take it down", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised text embeddings extraction is crucial for text understanding in\nmachine learning. Word2Vec and its variants have received substantial success\nin mapping words with similar syntactic or semantic meaning to vectors close to\neach other. However, extracting context-aware word-sequence embedding remains a\nchallenging task. Training over large corpus is difficult as labels are\ndifficult to get. More importantly, it is challenging for pre-trained models to\nobtain word-sequence embeddings that are universally good for all downstream\ntasks or for any new datasets. We propose a two-phased ConvDic+DeconvDec\nframework to solve the problem by combining a word-sequence dictionary learning\nmodel with a word-sequence embedding decode model. We propose a convolutional\ntensor decomposition mechanism to learn good word-sequence phrase dictionary in\nthe learning phase. It is proved to be more accurate and much more efficient\nthan the popular alternating minimization method. In the decode phase, we\nintroduce a deconvolution framework that is immune to the problem of varying\nsentence lengths. The word-sequence embeddings we extracted using\nConvDic+DeconvDec are universally good for a few downstream tasks we test on.\nThe framework requires neither pre-training nor prior/outside information.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 01:22:32 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 22:32:17 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 19:22:09 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Huang", "Furong", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1606.03192", "submitter": "Shaohua Li", "authors": "Shaohua Li, Jun Zhu, Chunyan Miao", "title": "PSDVec: a Toolbox for Incremental and Scalable Word Embedding", "comments": "12 pages, accepted by Neurocomputing, Software Track on Original\n  Software Publications", "journal-ref": null, "doi": "10.1016/j.neucom.2016.05.093", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping\nof words in a natural language to continuous vectors which encode the\nsemantic/syntactic regularities between the words. PSDVec implements a word\nembedding learning method based on a weighted low-rank positive semidefinite\napproximation. To scale up the learning process, we implement a blockwise\nonline learning algorithm to learn the embeddings incrementally. This strategy\ngreatly reduces the learning time of word embeddings on a large vocabulary, and\ncan learn the embeddings of new words without re-learning the whole vocabulary.\nOn 9 word similarity/analogy benchmark sets and 2 Natural Language Processing\n(NLP) tasks, PSDVec produces embeddings that has the best average performance\namong popular word embedding tools. PSDVec provides a new option for NLP\npractitioners.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 05:55:58 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Li", "Shaohua", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1606.03207", "submitter": "Hwaran Lee", "authors": "Hwaran Lee, Geonmin Kim, Ho-Gyeong Kim, Sang-Hoon Oh, and Soo-Young\n  Lee", "title": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to\n  Spectral Variations", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2016.2589962", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) with convolutional and pooling\noperations along the frequency axis have been proposed to attain invariance to\nfrequency shifts of features. However, this is inappropriate with regard to the\nfact that acoustic features vary in frequency. In this paper, we contend that\nconvolution along the time axis is more effective. We also propose the addition\nof an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each\ngroup extract common but spectrally variant features, then the layer pools the\nfeature maps of each group. As a result, the proposed IMP CNN can achieve\ninsensitivity to spectral variations characteristic of different speakers and\nutterances. The effectiveness of the IMP CNN architecture is demonstrated on\nseveral LVCSR tasks. Even without speaker adaptation techniques, the\narchitecture achieved a WER of 12.7% on the SWB part of the Hub5'2000\nevaluation test set, which is competitive with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 06:44:21 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 07:23:53 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Lee", "Hwaran", ""], ["Kim", "Geonmin", ""], ["Kim", "Ho-Gyeong", ""], ["Oh", "Sang-Hoon", ""], ["Lee", "Soo-Young", ""]]}, {"id": "1606.03254", "submitter": "Verena Rieser", "authors": "Dimitra Gkatzia and Oliver Lemon and Verena Rieser", "title": "Natural Language Generation enhances human decision-making with\n  uncertain information", "comments": "54th annual meeting of the Association for Computational Linguistics\n  (ACL), Berlin 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making is often dependent on uncertain data, e.g. data associated\nwith confidence scores or probabilities. We present a comparison of different\ninformation presentations for uncertain data and, for the first time, measure\ntheir effects on human decision-making. We show that the use of Natural\nLanguage Generation (NLG) improves decision-making under uncertainty, compared\nto state-of-the-art graphical-based representation methods. In a task-based\nstudy with 442 adults, we found that presentations using NLG lead to 24% better\ndecision-making on average than the graphical presentations, and to 44% better\ndecision-making when NLG is combined with graphics. We also show that women\nachieve significantly better results when presented with NLG output (an 87%\nincrease on average compared to graphical presentations).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 10:12:13 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 10:11:49 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Gkatzia", "Dimitra", ""], ["Lemon", "Oliver", ""], ["Rieser", "Verena", ""]]}, {"id": "1606.03333", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain", "title": "Automatic Genre and Show Identification of Broadcast Media", "comments": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge amounts of digital videos are being produced and broadcast every day,\nleading to giant media archives. Effective techniques are needed to make such\ndata accessible further. Automatic meta-data labelling of broadcast media is an\nessential task for multimedia indexing, where it is standard to use multi-modal\ninput for such purposes. This paper describes a novel method for automatic\ndetection of media genre and show identities using acoustic features, textual\nfeatures or a combination thereof. Furthermore the inclusion of available\nmeta-data, such as time of broadcast, is shown to lead to very high\nperformance. Latent Dirichlet Allocation is used to model both acoustics and\ntext, yielding fixed dimensional representations of media recordings that can\nthen be used in Support Vector Machines based classification. Experiments are\nconducted on more than 1200 hours of TV broadcasts from the British\nBroadcasting Corporation (BBC), where the task is to categorise the broadcasts\ninto 8 genres or 133 show identities. On a 200-hour test set, accuracies of\n98.6% and 85.7% were achieved for genre and show identification respectively,\nusing a combination of acoustic and textual features with meta-data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:09:32 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Ng", "Raymond W. M.", ""], ["Hain", "Thomas", ""]]}, {"id": "1606.03335", "submitter": "Roman Bartusiak", "authors": "Roman Bartusiak, {\\L}ukasz Augustyniak, Tomasz Kajdanowicz,\n  Przemys{\\l}aw Kazienko, Maciej Piasecki", "title": "WordNet2Vec: Corpora Agnostic Word Vectorization Method", "comments": "29 pages, 16 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex nature of big data resources demands new methods for structuring\nespecially for textual content. WordNet is a good knowledge source for\ncomprehensive abstraction of natural language as its good implementations exist\nfor many languages. Since WordNet embeds natural language in the form of a\ncomplex network, a transformation mechanism WordNet2Vec is proposed in the\npaper. It creates vectors for each word from WordNet. These vectors encapsulate\ngeneral position - role of a given word towards all other words in the natural\nlanguage. Any list or set of such vectors contains knowledge about the context\nof its component within the whole language. Such word representation can be\neasily applied to many analytic tasks like classification or clustering. The\nusefulness of the WordNet2Vec method was demonstrated in sentiment analysis,\ni.e. classification with transfer learning for the real Amazon opinion textual\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:12:47 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Bartusiak", "Roman", ""], ["Augustyniak", "\u0141ukasz", ""], ["Kajdanowicz", "Tomasz", ""], ["Kazienko", "Przemys\u0142aw", ""], ["Piasecki", "Maciej", ""]]}, {"id": "1606.03352", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona,\n  Pei-Hao Su, Stefan Ultes, David Vandyke, Steve Young", "title": "Conditional Generation and Snapshot Learning in Neural Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a variety of LSTM-based conditional language models (LM) have been\napplied across a range of language generation tasks. In this work we study\nvarious model architectures and different ways to represent and aggregate the\nsource information in an end-to-end neural dialogue system framework. A method\ncalled snapshot learning is also proposed to facilitate learning from\nsupervised sequential signals by applying a companion cross-entropy objective\nfunction to the conditioning vector. The experimental and analytical results\ndemonstrate firstly that competition occurs between the conditioning vector and\nthe LM, and the differing architectures provide different trade-offs between\nthe two. Secondly, the discriminative power and transparency of the\nconditioning vector is key to providing both model interpretability and better\nperformance. Thirdly, snapshot learning leads to consistent performance\nimprovements independent of which architecture is used.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:56:19 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Rojas-Barahona", "Lina M.", ""], ["Su", "Pei-Hao", ""], ["Ultes", "Stefan", ""], ["Vandyke", "David", ""], ["Young", "Steve", ""]]}, {"id": "1606.03391", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, Hinrich Sch\\\"utze", "title": "Simple Question Answering by Attentive Convolutional Neural Network", "comments": "Accepted as an oral long paper by COLING'2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on answering single-relation factoid questions over\nFreebase. Each question can acquire the answer from a single fact of form\n(subject, predicate, object) in Freebase. This task, simple question answering\n(SimpleQA), can be addressed via a two-step pipeline: entity linking and fact\nselection. In fact selection, we match the subject entity in a fact candidate\nwith the entity mention in the question by a character-level convolutional\nneural network (char-CNN), and match the predicate in that fact with the\nquestion by a word-level CNN (word-CNN). This work makes two main\ncontributions. (i) A simple and effective entity linker over Freebase is\nproposed. Our entity linker outperforms the state-of-the-art entity linker over\nSimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so\nthat the predicate representation can be matched with the predicate-focused\nquestion representation more effectively. Experiments show that our system sets\nnew state-of-the-art in this task.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 16:54:51 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 14:32:13 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Yin", "Wenpeng", ""], ["Yu", "Mo", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1606.03398", "submitter": "Lidong Bing", "authors": "Lidong Bing, Bhuwan Dhingra, Kathryn Mazaitis, Jong Hyuk Park, William\n  W. Cohen", "title": "Bootstrapping Distantly Supervised IE using Joint Learning and Small\n  Well-structured Corpora", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to improve performance of distantly-supervised\nrelation extraction, by jointly learning to solve two related tasks:\nconcept-instance extraction and relation extraction. We combine this with a\nnovel use of document structure: in some small, well-structured corpora,\nsections can be identified that correspond to relation arguments, and\ndistantly-labeled examples from such sections tend to have good precision.\nUsing these as seeds we extract additional relation examples by applying label\npropagation on a graph composed of noisy examples extracted from a large\nunstructured testing corpus. Combined with the soft constraint that concept\nexamples should have the same type as the second argument of the relation, we\nget significant improvements over several state-of-the-art approaches to\ndistantly-supervised relation extraction.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 17:14:11 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 01:22:30 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Bing", "Lidong", ""], ["Dhingra", "Bhuwan", ""], ["Mazaitis", "Kathryn", ""], ["Park", "Jong Hyuk", ""], ["Cohen", "William W.", ""]]}, {"id": "1606.03402", "submitter": "Pavel Sountsov", "authors": "Pavel Sountsov, Sunita Sarawagi", "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder networks are popular for modeling sequences probabilistically\nin many applications. These models use the power of the Long Short-Term Memory\n(LSTM) architecture to capture the full dependence among variables, unlike\nearlier models like CRFs that typically assumed conditional independence among\nnon-adjacent variables. However in practice encoder-decoder models exhibit a\nbias towards short sequences that surprisingly gets worse with increasing beam\nsize.\n  In this paper we show that such phenomenon is due to a discrepancy between\nthe full sequence margin and the per-element margin enforced by the locally\nconditioned training objective of a encoder-decoder model. The discrepancy more\nadversely impacts long sequences, explaining the bias towards predicting short\nsequences.\n  For the case where the predicted sequences come from a closed set, we show\nthat a globally conditioned model alleviates the above problems of\nencoder-decoder models. From a practical point of view, our proposed model also\neliminates the need for a beam-search during inference, which reduces to an\nefficient dot-product based search in a vector-space.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 17:30:46 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 17:33:58 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Sountsov", "Pavel", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "1606.03475", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner, Peter Szolovits", "title": "De-identification of Patient Notes with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Patient notes in electronic health records (EHRs) may contain\ncritical information for medical investigations. However, the vast majority of\nmedical investigators can only access de-identified notes, in order to protect\nthe confidentiality of patients. In the United States, the Health Insurance\nPortability and Accountability Act (HIPAA) defines 18 types of protected health\ninformation (PHI) that needs to be removed to de-identify patient notes. Manual\nde-identification is impractical given the size of EHR databases, the limited\nnumber of researchers with access to the non-de-identified notes, and the\nfrequent mistakes of human annotators. A reliable automated de-identification\nsystem would consequently be of high value.\n  Materials and Methods: We introduce the first de-identification system based\non artificial neural networks (ANNs), which requires no handcrafted features or\nrules, unlike existing systems. We compare the performance of the system with\nstate-of-the-art systems on two datasets: the i2b2 2014 de-identification\nchallenge dataset, which is the largest publicly available de-identification\ndataset, and the MIMIC de-identification dataset, which we assembled and is\ntwice as large as the i2b2 2014 dataset.\n  Results: Our ANN model outperforms the state-of-the-art systems. It yields an\nF1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision\nof 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with\na recall 99.25 and a precision of 99.06.\n  Conclusion: Our findings support the use of ANNs for de-identification of\npatient notes, as they show better performance than previously published\nsystems while requiring no feature engineering.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 20:45:30 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Uzuner", "Ozlem", ""], ["Szolovits", "Peter", ""]]}, {"id": "1606.03556", "submitter": "Abhishek Das", "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv\n  Batra", "title": "Human Attention in Visual Question Answering: Do Humans and Deep\n  Networks Look at the Same Regions?", "comments": "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 05:41:10 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 04:39:01 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Das", "Abhishek", ""], ["Agrawal", "Harsh", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1606.03568", "submitter": "Mikael K{\\aa}geb\\\"ack", "authors": "Mikael K{\\aa}geb\\\"ack, Hans Salomonsson", "title": "Word Sense Disambiguation using a Bidirectional LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a clean, yet effective, model for word sense\ndisambiguation. Our approach leverage a bidirectional long short-term memory\nnetwork which is shared between all words. This enables the model to share\nstatistical strength and to scale well with vocabulary size. The model is\ntrained end-to-end, directly from the raw text to sense labels, and makes\neffective use of word order. We evaluate our approach on two standard datasets,\nusing identical hyperparameter settings, which are in turn tuned on a third set\nof held out data. We employ no external resources (e.g. knowledge graphs,\npart-of-speech tagging, etc), language specific features, or hand crafted\nrules, but still achieve statistically equivalent results to the best\nstate-of-the-art systems, that employ no such limitations.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 08:12:02 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 22:47:07 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["K\u00e5geb\u00e4ck", "Mikael", ""], ["Salomonsson", "Hans", ""]]}, {"id": "1606.03622", "submitter": "Robin Jia", "authors": "Robin Jia and Percy Liang", "title": "Data Recombination for Neural Semantic Parsing", "comments": "ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling crisp logical regularities is crucial in semantic parsing, making it\ndifficult for neural models with no task-specific prior knowledge to achieve\ngood results. In this paper, we introduce data recombination, a novel framework\nfor injecting such prior knowledge into a model. From the training data, we\ninduce a high-precision synchronous context-free grammar, which captures\nimportant conditional independence properties commonly found in semantic\nparsing. We then train a sequence-to-sequence recurrent network (RNN) model\nwith a novel attention-based copying mechanism on datapoints sampled from this\ngrammar, thereby teaching the model about these structural properties. Data\nrecombination improves the accuracy of our RNN model on three semantic parsing\ndatasets, leading to new state-of-the-art performance on the standard GeoQuery\ndataset for models with comparable supervision.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 20:34:09 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Jia", "Robin", ""], ["Liang", "Percy", ""]]}, {"id": "1606.03632", "submitter": "Shikhar Sharma", "authors": "Shikhar Sharma and Jing He and Kaheer Suleman and Hannes Schulz and\n  Philip Bachman", "title": "Natural Language Generation in Dialogue using Lexicalized and\n  Delexicalized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language generation plays a critical role in spoken dialogue systems.\nWe present a new approach to natural language generation for task-oriented\ndialogue using recurrent neural networks in an encoder-decoder framework. In\ncontrast to previous work, our model uses both lexicalized and delexicalized\ncomponents i.e. slot-value pairs for dialogue acts, with slots and\ncorresponding values aligned together. This allows our model to learn from all\navailable data including the slot-value pairing, rather than being restricted\nto delexicalized slots. We show that this helps our model generate more natural\nsentences with better grammar. We further improve our model's performance by\ntransferring weights learnt from a pretrained sentence auto-encoder. Human\nevaluation of our best-performing model indicates that it generates sentences\nwhich users find more appealing.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 21:24:43 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 19:13:02 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 19:11:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Sharma", "Shikhar", ""], ["He", "Jing", ""], ["Suleman", "Kaheer", ""], ["Schulz", "Hannes", ""], ["Bachman", "Philip", ""]]}, {"id": "1606.03667", "submitter": "Ji He", "authors": "Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen, Jianfeng Gao, Lihong\n  Li, Li Deng", "title": "Deep Reinforcement Learning with a Combinatorial Action Space for\n  Predicting Popular Reddit Threads", "comments": "To be published in EMNLP 2016, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an online popularity prediction and tracking task as a benchmark\ntask for reinforcement learning with a combinatorial, natural language action\nspace. A specified number of discussion threads predicted to be popular are\nrecommended, chosen from a fixed window of recent comments to track. Novel deep\nreinforcement learning architectures are studied for effective modeling of the\nvalue function associated with actions comprised of interdependent sub-actions.\nThe proposed model, which represents dependence between sub-actions through a\nbi-directional LSTM, gives the best performance across different experimental\nconfigurations and domains, and it also generalizes well with varying numbers\nof recommendation requests.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 05:38:20 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 22:31:36 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 06:38:20 GMT"}, {"version": "v4", "created": "Sat, 17 Sep 2016 00:52:43 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["He", "Ji", ""], ["Ostendorf", "Mari", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Deng", "Li", ""]]}, {"id": "1606.03676", "submitter": "Benoit Sagot", "authors": "Beno\\^it Sagot (ALPAGE)", "title": "External Lexical Information for Multilingual Part-of-Speech Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphosyntactic lexicons and word vector representations have both proven\nuseful for improving the accuracy of statistical part-of-speech taggers. Here\nwe compare the performances of four systems on datasets covering 16 languages,\ntwo of these systems being feature-based (MEMMs and CRFs) and two of them being\nneural-based (bi-LSTMs). We show that, on average, all four approaches perform\nsimilarly and reach state-of-the-art results. Yet better performances are\nobtained with our feature-based models on lexically richer datasets (e.g. for\nmorphologically rich languages), whereas neural-based results are higher on\ndatasets with less lexical variability (e.g. for English). These conclusions\nhold in particular for the MEMM models relying on our system MElt, which\nbenefited from newly designed features. This shows that, under certain\nconditions, feature-based approaches enriched with morphosyntactic lexicons are\ncompetitive with respect to neural methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 08:06:55 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 08:41:46 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Sagot", "Beno\u00eet", "", "ALPAGE"]]}, {"id": "1606.03777", "submitter": "Nikola Mrk\\v{s}i\\'c", "authors": "Nikola Mrk\\v{s}i\\'c and Diarmuid \\'O S\\'eaghdha and Tsung-Hsien Wen\n  and Blaise Thomson and Steve Young", "title": "Neural Belief Tracker: Data-Driven Dialogue State Tracking", "comments": "Accepted as a long paper for the 55th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core components of modern spoken dialogue systems is the belief\ntracker, which estimates the user's goal at every step of the dialogue.\nHowever, most current approaches have difficulty scaling to larger, more\ncomplex dialogue domains. This is due to their dependency on either: a) Spoken\nLanguage Understanding models that require large amounts of annotated training\ndata; or b) hand-crafted lexicons for capturing some of the linguistic\nvariation in users' language. We propose a novel Neural Belief Tracking (NBT)\nframework which overcomes these problems by building on recent advances in\nrepresentation learning. NBT models reason over pre-trained word vectors,\nlearning to compose them into distributed representations of user utterances\nand dialogue context. Our evaluation on two datasets shows that this approach\nsurpasses past limitations, matching the performance of state-of-the-art models\nwhich rely on hand-crafted semantic lexicons and outperforming them when such\nlexicons are not provided.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 22:59:14 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 15:15:03 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Wen", "Tsung-Hsien", ""], ["Thomson", "Blaise", ""], ["Young", "Steve", ""]]}, {"id": "1606.03783", "submitter": "Thomas Lampert", "authors": "Pedro Chahuara, Thomas Lampert, Pierre Gancarski", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives\n  Using Topic Modelling and Topic Distribution Regression", "comments": "International Conference on Theory and Practice of Digital Libraries\n  2016 (accepted)", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_4", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented herein is a novel model for similar question ranking within\ncollaborative question answer platforms. The presented approach integrates a\nregression stage to relate topics derived from questions to those derived from\nquestion-answer pairs. This helps to avoid problems caused by the differences\nin vocabulary used within questions and answers, and the tendency for questions\nto be shorter than answers. The performance of the model is shown to outperform\ntranslation methods and topic modelling (without regression) on several\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 23:50:19 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chahuara", "Pedro", ""], ["Lampert", "Thomas", ""], ["Gancarski", "Pierre", ""]]}, {"id": "1606.03784", "submitter": "Guido Zarrella", "authors": "Guido Zarrella and Amy Marsh", "title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection", "comments": "International Workshop on Semantic Evaluation 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance\nin Tweets. This effort achieved the top score in Task A on supervised stance\ndetection, producing an average F1 score of 67.8 when assessing whether a tweet\nauthor was in favor or against a topic. We employed a recurrent neural network\ninitialized with features learned via distant supervision on two large\nunlabeled datasets. We trained embeddings of words and phrases with the\nword2vec skip-gram method, then used those features to learn sentence\nrepresentations via a hashtag prediction auxiliary task. These sentence vectors\nwere then fine-tuned for stance detection on several hundred labeled examples.\nThe result was a high performing system that used transfer learning to maximize\nthe value of the available training data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 00:12:49 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Zarrella", "Guido", ""], ["Marsh", "Amy", ""]]}, {"id": "1606.03821", "submitter": "Will Monroe", "authors": "Will Monroe, Noah D. Goodman, Christopher Potts", "title": "Learning to Generate Compositional Color Descriptions", "comments": "6 pages, 4 figures, 3 tables. EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production of color language is essential for grounded language\ngeneration. Color descriptions have many challenging properties: they can be\nvague, compositionally complex, and denotationally rich. We present an\neffective approach to generating color descriptions using recurrent neural\nnetworks and a Fourier-transformed color representation. Our model outperforms\nprevious work on a conditional language modeling task over a large corpus of\nnaturalistic color descriptions. In addition, probing the model's output\nreveals that it can accurately produce not only basic color terms but also\ndescriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\",\n\"dull\"), and compositional phrases (\"faded teal\") not seen in training.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 06:17:32 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 18:28:12 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Monroe", "Will", ""], ["Goodman", "Noah D.", ""], ["Potts", "Christopher", ""]]}, {"id": "1606.03864", "submitter": "Dirk Weissenborn", "authors": "Dirk Weissenborn", "title": "Neural Associative Memory for Dual-Sequence Modeling", "comments": "To appear in RepL4NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important NLP problems can be posed as dual-sequence or\nsequence-to-sequence modeling tasks. Recent advances in building end-to-end\nneural architectures have been highly successful in solving such tasks. In this\nwork we propose a new architecture for dual-sequence modeling that is based on\nassociative memory. We derive AM-RNNs, a recurrent associative memory (AM)\nwhich augments generic recurrent neural networks (RNN). This architecture is\nextended to the Dual AM-RNN which operates on two AMs at once. Our models\nachieve very competitive results on textual entailment. A qualitative analysis\ndemonstrates that long range dependencies between source and target-sequence\ncan be bridged effectively using Dual AM-RNNs. However, an initial experiment\non auto-encoding reveals that these benefits are not exploited by the system\nwhen learning to solve sequence-to-sequence tasks which indicates that\nadditional supervision or regularization is needed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 09:08:04 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 07:59:18 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Weissenborn", "Dirk", ""]]}, {"id": "1606.04052", "submitter": "Julien Perez", "authors": "Julien Perez and Fei Liu", "title": "Dialog state tracking, a machine reading approach using Memory Network", "comments": "10 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an end-to-end dialog system, the aim of dialog state tracking is to\naccurately estimate a compact representation of the current dialog status from\na sequence of noisy observations produced by the speech recognition and the\nnatural language understanding modules. This paper introduces a novel method of\ndialog state tracking based on the general paradigm of machine reading and\nproposes to solve it using an End-to-End Memory Network, MemN2N, a\nmemory-enhanced neural network architecture. We evaluate the proposed approach\non the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has\nbeen converted for the occasion in order to frame the hidden state variable\ninference as a question-answering task based on a sequence of utterances\nextracted from a dialog. We show that the proposed tracker gives encouraging\nresults. Then, we propose to extend the DSTC-2 dataset with specific reasoning\ncapabilities requirement like counting, list maintenance, yes-no question\nanswering and indefinite knowledge management. Finally, we present encouraging\nresults using our proposed MemN2N based tracking model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 18:09:40 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 06:42:04 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 00:07:41 GMT"}, {"version": "v4", "created": "Thu, 13 Oct 2016 19:23:00 GMT"}, {"version": "v5", "created": "Thu, 2 Mar 2017 20:17:23 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Perez", "Julien", ""], ["Liu", "Fei", ""]]}, {"id": "1606.04081", "submitter": "Pedro Mota", "authors": "Pedro Mota, Maxine Eskenazi, Luisa Coheur", "title": "Graph-Community Detection for Cross-Document Topic Segment Relationship\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a graph-community detection approach to identify\ncross-document relationships at the topic segment level. Given a set of related\ndocuments, we automatically find these relationships by clustering segments\nwith similar content (topics). In this context, we study how different\nweighting mechanisms influence the discovery of word communities that relate to\nthe different topics found in the documents. Finally, we test different mapping\nfunctions to assign topic segments to word communities, determining which topic\nsegments are considered equivalent.\n  By performing this task it is possible to enable efficient multi-document\nbrowsing, since when a user finds relevant content in one document we can\nprovide access to similar topics in other documents. We deploy our approach in\ntwo different scenarios. One is an educational scenario where equivalence\nrelationships between learning materials need to be found. The other consists\nof a series of dialogs in a social context where students discuss commonplace\ntopics. Results show that our proposed approach better discovered equivalence\nrelationships in learning material documents and obtained close results in the\nsocial speech domain, where the best performing approach was a clustering\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:41:56 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Mota", "Pedro", ""], ["Eskenazi", "Maxine", ""], ["Coheur", "Luisa", ""]]}, {"id": "1606.04155", "submitter": "Tao Lei", "authors": "Tao Lei, Regina Barzilay and Tommi Jaakkola", "title": "Rationalizing Neural Predictions", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications -- rationales -- that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:10:23 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 20:26:20 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Lei", "Tao", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1606.04164", "submitter": "Orhan Firat", "authors": "Orhan Firat and Baskaran Sankaran and Yaser Al-Onaizan and Fatos T.\n  Yarman Vural and Kyunghyun Cho", "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel finetuning algorithm for the recently\nintroduced multi-way, mulitlingual neural machine translate that enables\nzero-resource machine translation. When used together with novel many-to-one\ntranslation strategies, we empirically show that this finetuning algorithm\nallows the multi-way, multilingual model to translate a zero-resource language\npair (1) as well as a single-pair neural translation model trained with up to\n1M direct parallel sentences of the same language pair and (2) better than\npivot-based translation strategy, while keeping only one additional copy of\nattention-related parameters.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 22:40:33 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Firat", "Orhan", ""], ["Sankaran", "Baskaran", ""], ["Al-Onaizan", "Yaser", ""], ["Vural", "Fatos T. Yarman", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1606.04199", "submitter": "Jie Zhou", "authors": "Jie Zhou and Ying Cao and Xuguang Wang and Peng Li and Wei Xu", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine\n  Translation", "comments": "TACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) aims at solving machine translation (MT)\nproblems using neural networks and has exhibited promising results in recent\nyears. However, most of the existing NMT models are shallow and there is still\na performance gap between a single NMT model and the best conventional MT\nsystem. In this work, we introduce a new type of linear connections, named\nfast-forward connections, based on deep Long Short-Term Memory (LSTM) networks,\nand an interleaved bi-directional architecture for stacking the LSTM layers.\nFast-forward connections play an essential role in propagating the gradients\nand building a deep topology of depth 16. On the WMT'14 English-to-French task,\nwe achieve BLEU=37.7 with a single attention model, which outperforms the\ncorresponding single shallow model by 6.2 BLEU points. This is the first time\nthat a single NMT model achieves state-of-the-art performance and outperforms\nthe best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3\neven without using an attention mechanism. After special handling of unknown\nwords and model ensembling, we obtain the best score reported to date on this\ntask with BLEU=40.4. Our models are also validated on the more difficult WMT'14\nEnglish-to-German task.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 03:53:00 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 04:21:03 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 13:14:17 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Zhou", "Jie", ""], ["Cao", "Ying", ""], ["Wang", "Xuguang", ""], ["Li", "Peng", ""], ["Xu", "Wei", ""]]}, {"id": "1606.04212", "submitter": "Ye Zhang", "authors": "Ye Zhang, Matthew Lease, Byron C. Wallace", "title": "Active Discriminative Text Representation Learning", "comments": "This paper got accepted by AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning (AL) method for text classification with\nconvolutional neural networks (CNNs). In AL, one selects the instances to be\nmanually labeled with the aim of maximizing model performance with minimal\neffort. Neural models capitalize on word embeddings as representations\n(features), tuning these to the task at hand. We argue that AL strategies for\nmulti-layered neural models should focus on selecting instances that most\naffect the embedding space (i.e., induce discriminative word representations).\nThis is in contrast to traditional AL approaches (e.g., entropy-based\nuncertainty sampling), which specify higher level objectives. We propose a\nsimple approach for sentence classification that selects instances containing\nwords whose embeddings are likely to be updated with the greatest magnitude,\nthereby rapidly learning discriminative, task-specific embeddings. We extend\nthis approach to document classification by jointly considering: (1) the\nexpected changes to the constituent word representations; and (2) the model's\ncurrent overall uncertainty regarding the instance. The relative emphasis\nplaced on these criteria is governed by a stochastic process that favors\nselecting instances likely to improve representations at the outset of\nlearning, and then shifts toward general uncertainty sampling as AL progresses.\nEmpirical results show that our method outperforms baseline AL approaches on\nboth sentence and document classification tasks. We also show that, as\nexpected, the method quickly learns discriminative word embeddings. To the best\nof our knowledge, this is the first work on AL addressing neural models for\ntext classification.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 06:49:40 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 19:31:07 GMT"}, {"version": "v3", "created": "Sun, 27 Nov 2016 05:58:25 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2016 18:53:32 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Zhang", "Ye", ""], ["Lease", "Matthew", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1606.04217", "submitter": "Ekaterina Vylomova", "authors": "Ekaterina Vylomova, Trevor Cohn, Xuanli He and Gholamreza Haffari", "title": "Word Representation Models for Morphologically Rich Languages in Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with the complex word forms in morphologically rich languages is an\nopen problem in language processing, and is particularly important in\ntranslation. In contrast to most modern neural systems of translation, which\ndiscard the identity for rare words, in this paper we propose several\narchitectures for learning word representations from character and morpheme\nlevel word decompositions. We incorporate these representations in a novel\nmachine translation model which jointly learns word alignments and translations\nvia a hard attention mechanism. Evaluating on translating from several\nmorphologically rich languages into English, we show consistent improvements\nover strong baseline methods, of between 1 and 1.5 BLEU points.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:04:37 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Vylomova", "Ekaterina", ""], ["Cohn", "Trevor", ""], ["He", "Xuanli", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "1606.04279", "submitter": "Jan Buys", "authors": "Jan Buys and Jan A. Botha", "title": "Cross-Lingual Morphological Tagging for Low-Resource Languages", "comments": "11 pages. ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphologically rich languages often lack the annotated linguistic resources\nrequired to develop accurate natural language processing tools. We propose\nmodels suitable for training morphological taggers with rich tagsets for\nlow-resource languages without using direct supervision. Our approach extends\nexisting approaches of projecting part-of-speech tags across languages, using\nbitext to infer constraints on the possible tags for a given word type or\ntoken. We propose a tagging model using Wsabie, a discriminative\nembedding-based model with rank-based learning. In our evaluation on 11\nlanguages, on average this model performs on par with a baseline\nweakly-supervised HMM, while being more scalable. Multilingual experiments show\nthat the method performs best when projecting between related language pairs.\nDespite the inherently lossy projection, we show that the morphological tags\npredicted by our models improve the downstream performance of a parser by +0.6\nLAS on average.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:43:36 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Buys", "Jan", ""], ["Botha", "Jan A.", ""]]}, {"id": "1606.04289", "submitter": "Dimitrios Alikaniotis", "authors": "Dimitrios Alikaniotis and Helen Yannakoudakis and Marek Rei", "title": "Automatic Text Scoring Using Neural Networks", "comments": "11 pages, 3 figures, 2 tables, ACL-2016", "journal-ref": null, "doi": "10.18653/v1/P16-1068", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Text Scoring (ATS) provides a cost-effective and consistent\nalternative to human marking. However, in order to achieve good performance,\nthe predictive features of the system need to be manually engineered by human\nexperts. We introduce a model that forms word representations by learning the\nextent to which specific words contribute to the text's score. Using Long-Short\nTerm Memory networks to represent the meaning of texts, we demonstrate that a\nfully automated framework is able to achieve excellent results over similar\napproaches. In an attempt to make our results more interpretable, and inspired\nby recent advances in visualizing neural networks, we introduce a novel method\nfor identifying the regions of the text that the model has found more\ndiscriminative.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 10:17:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 16:30:33 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Alikaniotis", "Dimitrios", ""], ["Yannakoudakis", "Helen", ""], ["Rei", "Marek", ""]]}, {"id": "1606.04300", "submitter": "Deng Cai", "authors": "Deng Cai and Hai Zhao", "title": "Neural Word Segmentation Learning for Chinese", "comments": "ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous approaches to Chinese word segmentation formalize this problem\nas a character-based sequence labeling task where only contextual information\nwithin fixed sized local windows and simple interactions between adjacent tags\ncan be captured. In this paper, we propose a novel neural framework which\nthoroughly eliminates context windows and can utilize complete segmentation\nhistory. Our model employs a gated combination neural network over characters\nto produce distributed representations of word candidates, which are then given\nto a long short-term memory (LSTM) language scoring model. Experiments on the\nbenchmark datasets show that without the help of feature engineering as most\nexisting approaches, our models achieve competitive or better performances with\nprevious state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 10:52:21 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 08:06:10 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cai", "Deng", ""], ["Zhao", "Hai", ""]]}, {"id": "1606.04351", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Massih-Reza Amini", "title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the participation of the team \"TwiSE\" in the SemEval\n2016 challenge. Specifically, we participated in Task 4, namely \"Sentiment\nAnalysis in Twitter\" for which we implemented sentiment classification systems\nfor subtasks A, B, C and D. Our approach consists of two steps. In the first\nstep, we generate and validate diverse feature sets for twitter sentiment\nevaluation, inspired by the work of participants of previous editions of such\nchallenges. In the second step, we focus on the optimization of the evaluation\nmeasures of the different subtasks. To this end, we examine different learning\nstrategies by validating them on the data provided by the task organisers. For\nour final submissions we used an ensemble learning approach (stacked\ngeneralization) for Subtask A and single linear models for the rest of the\nsubtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14\nfor subtasks A, B, C and D respectively.\\footnote{We make the code available\nfor research purposes at\n\\url{https://github.com/balikasg/SemEval2016-Twitter\\_Sentiment\\_Evaluation}.}\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:36:00 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1606.04429", "submitter": "Arkaitz Zubiaga", "authors": "Alberto P. Garc\\'ia-Plaza and V\\'ictor Fresno and Raquel Mart\\'inez\n  and Arkaitz Zubiaga", "title": "Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation", "comments": "This is the accepted version of an article accepted for publication\n  in IEEE Transactions on Fuzzy Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of a suitable document representation approach plays a crucial\nrole in the performance of a document clustering task. Being able to pick out\nrepresentative words within a document can lead to substantial improvements in\ndocument clustering. In the case of web documents, the HTML markup that defines\nthe layout of the content provides additional structural information that can\nbe further exploited to identify representative words. In this paper we\nintroduce a fuzzy term weighing approach that makes the most of the HTML\nstructure for document clustering. We set forth and build on the hypothesis\nthat a good representation can take advantage of how humans skim through\ndocuments to extract the most representative words. The authors of web pages\nmake use of HTML tags to convey the most important message of a web page\nthrough page elements that attract the readers' attention, such as page titles\nor emphasized elements. We define a set of criteria to exploit the information\nprovided by these page elements, and introduce a fuzzy combination of these\ncriteria that we evaluate within the context of a web page clustering task. Our\nproposed approach, called Abstract Fuzzy Combination of Criteria (AFCC), can\nadapt to datasets whose features are distributed differently, achieving good\nresults compared to other similar fuzzy logic based approaches and TF-IDF\nacross different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:44:52 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Garc\u00eda-Plaza", "Alberto P.", ""], ["Fresno", "V\u00edctor", ""], ["Mart\u00ednez", "Raquel", ""], ["Zubiaga", "Arkaitz", ""]]}, {"id": "1606.04503", "submitter": "Jacob Eisenstein", "authors": "Akanksha and Jacob Eisenstein", "title": "Shallow Discourse Parsing Using Distributed Argument Representations and\n  Bayesian Optimization", "comments": "describes our system at the CoNLL 2016 shared task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Georgia Tech team's approach to the CoNLL-2016\nsupplementary evaluation on discourse relation sense classification. We use\nlong short-term memories (LSTM) to induce distributed representations of each\nargument, and then combine these representations with surface features in a\nneural network. The architecture of the neural network is determined by\nBayesian hyperparameter search.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 19:00:59 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Akanksha", "", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1606.04582", "submitter": "Minjoon Seo", "authors": "Minjoon Seo, Sewon Min, Ali Farhadi, Hannaneh Hajishirzi", "title": "Query-Reduction Networks for Question Answering", "comments": "Published as a conference paper at ICLR 2017. Title of the paper has\n  changed from \"Query-Regression Networks for Machine Comprehension\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of question answering when reasoning over\nmultiple facts is required. We propose Query-Reduction Network (QRN), a variant\nof Recurrent Neural Network (RNN) that effectively handles both short-term\n(local) and long-term (global) sequential dependencies to reason over multiple\nfacts. QRN considers the context sentences as a sequence of state-changing\ntriggers, and reduces the original query to a more informed query as it\nobserves each trigger (context sentence) through time. Our experiments show\nthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and\nin a real goal-oriented dialog dataset. In addition, QRN formulation allows\nparallelization on RNN's time axis, saving an order of magnitude in time\ncomplexity for training and inference.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 21:54:46 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 21:54:45 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 10:07:22 GMT"}, {"version": "v4", "created": "Fri, 9 Dec 2016 00:05:06 GMT"}, {"version": "v5", "created": "Tue, 7 Feb 2017 22:04:54 GMT"}, {"version": "v6", "created": "Fri, 24 Feb 2017 19:59:01 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Seo", "Minjoon", ""], ["Min", "Sewon", ""], ["Farhadi", "Ali", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1606.04596", "submitter": "Yang Liu", "authors": "Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun and Yang\n  Liu", "title": "Semi-Supervised Learning for Neural Machine Translation", "comments": "Corrected a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While end-to-end neural machine translation (NMT) has made remarkable\nprogress recently, NMT systems only rely on parallel corpora for parameter\nestimation. Since parallel corpora are usually limited in quantity, quality,\nand coverage, especially for low-resource languages, it is appealing to exploit\nmonolingual corpora to improve NMT. We propose a semi-supervised approach for\ntraining NMT models on the concatenation of labeled (parallel corpora) and\nunlabeled (monolingual corpora) data. The central idea is to reconstruct the\nmonolingual corpora using an autoencoder, in which the source-to-target and\ntarget-to-source translation models serve as the encoder and decoder,\nrespectively. Our approach can not only exploit the monolingual corpora of the\ntarget language, but also of the source language. Experiments on the\nChinese-English dataset show that our approach achieves significant\nimprovements over state-of-the-art SMT and NMT systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 00:22:27 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 19:08:20 GMT"}, {"version": "v3", "created": "Sat, 10 Dec 2016 20:02:52 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Cheng", "Yong", ""], ["Xu", "Wei", ""], ["He", "Zhongjun", ""], ["He", "Wei", ""], ["Wu", "Hua", ""], ["Sun", "Maosong", ""], ["Liu", "Yang", ""]]}, {"id": "1606.04597", "submitter": "Yang Liu", "authors": "Chunyang Liu, Yang Liu, Huanbo Luan, Maosong Sun and Heng Yu", "title": "Agreement-based Learning of Parallel Lexicons and Phrases from\n  Non-Parallel Corpora", "comments": "Accepted for publication in the Proceedings of ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an agreement-based approach to learning parallel lexicons and\nphrases from non-parallel corpora. The basic idea is to encourage two\nasymmetric latent-variable translation models (i.e., source-to-target and\ntarget-to-source) to agree on identifying latent phrase and word alignments.\nThe agreement is defined at both word and phrase levels. We develop a Viterbi\nEM algorithm for jointly training the two unidirectional models efficiently.\nExperiments on the Chinese-English dataset show that agreement-based learning\nsignificantly improves both alignment and translation performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 00:28:51 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Liu", "Chunyang", ""], ["Liu", "Yang", ""], ["Luan", "Huanbo", ""], ["Sun", "Maosong", ""], ["Yu", "Heng", ""]]}, {"id": "1606.04631", "submitter": "Fumin Shen Dr.", "authors": "Yi Bin, Yang Yang, Zi Huang, Fumin Shen, Xing Xu, Heng Tao Shen", "title": "Bidirectional Long-Short Term Memory for Video Description", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning has been attracting broad research attention in multimedia\ncommunity. However, most existing approaches either ignore temporal information\namong video frames or just employ local contextual temporal knowledge. In this\nwork, we propose a novel video captioning framework, termed as\n\\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures\nbidirectional global temporal structure in video. Specifically, we first devise\na joint visual modelling approach to encode video data by combining a forward\nLSTM pass, a backward LSTM pass, together with visual features from\nConvolutional Neural Networks (CNNs). Then, we inject the derived video\nrepresentation into the subsequent language model for initialization. The\nbenefits are in two folds: 1) comprehensively preserving sequential and visual\ninformation; and 2) adaptively learning dense visual features and sparse\nsemantic representations for videos and sentences, respectively. We verify the\neffectiveness of our proposed video captioning framework on a commonly-used\nbenchmark, i.e., Microsoft Video Description (MSVD) corpus, and the\nexperimental results demonstrate that the superiority of the proposed approach\nas compared to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 03:26:53 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Bin", "Yi", ""], ["Yang", "Yang", ""], ["Huang", "Zi", ""], ["Shen", "Fumin", ""], ["Xu", "Xing", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1606.04640", "submitter": "Tom Kenter", "authors": "Tom Kenter, Alexey Borisov, Maarten de Rijke", "title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "comments": "Accepted as full paper at ACL 2016, Berlin. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural\nnetwork for efficient estimation of high-quality sentence embeddings. Averaging\nthe embeddings of words in a sentence has proven to be a surprisingly\nsuccessful and efficient way of obtaining sentence embeddings. However, word\nembeddings trained with the methods currently available are not optimized for\nthe task of sentence representation, and, thus, likely to be suboptimal.\nSiamese CBOW handles this problem by training word embeddings directly for the\npurpose of being averaged. The underlying neural network learns word embeddings\nby predicting, from a sentence representation, its surrounding sentences. We\nshow the robustness of the Siamese CBOW model by evaluating it on 20 datasets\nstemming from a wide variety of sources.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 04:47:43 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Kenter", "Tom", ""], ["Borisov", "Alexey", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1606.04672", "submitter": "Lars Roemheld", "authors": "Allen Huang, Lars Roemheld", "title": "Constitutional Precedent of Amicus Briefs", "comments": "Stanford University \"Law, Order, and Algorithms\" Final Project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate shared language between U.S. Supreme Court majority opinions\nand interest groups' corresponding amicus briefs. Specifically, we evaluate\nwhether language that originated in an amicus brief acquired legal precedent\nstatus by being cited in the Court's opinion. Using plagiarism detection\nsoftware, automated querying of a large legal database, and manual analysis, we\nestablish seven instances where interest group amici were able to formulate\nconstitutional case law, setting binding legal precedent. We discuss several\nsuch instances for their implications in the Supreme Court's creation of case\nlaw.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 08:21:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 10:20:35 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Huang", "Allen", ""], ["Roemheld", "Lars", ""]]}, {"id": "1606.04686", "submitter": "Verena Rieser", "authors": "Verena Rieser and Oliver Lemon", "title": "Natural Language Generation as Planning under Uncertainty Using\n  Reinforcement Learning", "comments": "published EACL 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate a new model for Natural Language Generation (NLG) in\nSpoken Dialogue Systems, based on statistical planning, given noisy feedback\nfrom the current generation context (e.g. a user and a surface realiser). We\nstudy its use in a standard NLG problem: how to present information (in this\ncase a set of search results) to users, given the complex trade- offs between\nutterance length, amount of information conveyed, and cognitive load. We set\nthese trade-offs by analysing existing MATCH data. We then train a NLG pol- icy\nusing Reinforcement Learning (RL), which adapts its behaviour to noisy feed-\nback from the current generation context. This policy is compared to several\nbase- lines derived from previous work in this area. The learned policy\nsignificantly out- performs all the prior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 09:05:56 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Rieser", "Verena", ""], ["Lemon", "Oliver", ""]]}, {"id": "1606.04721", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Personality Traits and Echo Chambers on Facebook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online social networks, users tend to select information that adhere to\ntheir system of beliefs and to form polarized groups of like minded people.\nPolarization as well as its effects on online social interactions have been\nextensively investigated. Still, the relation between group formation and\npersonality traits remains unclear. A better understanding of the cognitive and\npsychological determinants of online social dynamics might help to design more\nefficient communication strategies and to challenge the digital misinformation\nthreat. In this work, we focus on users commenting posts published by US\nFacebook pages supporting scientific and conspiracy-like narratives, and we\nclassify the personality traits of those users according to their online\nbehavior. We show that different and conflicting communities are populated by\nusers showing similar psychological profiles, and that the dominant personality\nmodel is the same in both scientific and conspiracy echo chambers. Moreover, we\nobserve that the permanence within echo chambers slightly shapes users'\npsychological profiles. Our results suggest that the presence of specific\npersonality traits in individuals lead to their considerable involvement in\nsupporting narratives inside virtual echo chambers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 11:08:24 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1606.04754", "submitter": "Sarath Chandar", "authors": "Amrita Saha, Mitesh M. Khapra, Sarath Chandar, Janarthanan Rajendran,\n  Kyunghyun Cho", "title": "A Correlational Encoder Decoder Architecture for Pivot Based Sequence\n  Generation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interlingua based Machine Translation (MT) aims to encode multiple languages\ninto a common linguistic representation and then decode sentences in multiple\ntarget languages from this representation. In this work we explore this idea in\nthe context of neural encoder decoder architectures, albeit on a smaller scale\nand without MT as the end goal. Specifically, we consider the case of three\nlanguages or modalities X, Z and Y wherein we are interested in generating\nsequences in Y starting from information available in X. However, there is no\nparallel training data available between X and Y but, training data is\navailable between X & Z and Z & Y (as is often the case in many real world\napplications). Z thus acts as a pivot/bridge. An obvious solution, which is\nperhaps less elegant but works very well in practice is to train a two stage\nmodel which first converts from X to Z and then from Z to Y. Instead we explore\nan interlingua inspired solution which jointly learns to do the following (i)\nencode X and Z to a common representation and (ii) decode Y from this common\nrepresentation. We evaluate our model on two tasks: (i) bridge transliteration\nand (ii) bridge captioning. We report promising results in both these\napplications and believe that this is a right step towards truly interlingua\ninspired encoder decoder architectures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 13:27:16 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Saha", "Amrita", ""], ["Khapra", "Mitesh M.", ""], ["Chandar", "Sarath", ""], ["Rajendran", "Janarthanan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1606.04835", "submitter": "Qi Li", "authors": "Qi Li, Tianshi Li, Baobao Chang", "title": "Learning Word Sense Embeddings from Word Sense Definitions", "comments": "To appear at NLPCC-ICCPOL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings play a significant role in many modern NLP systems. Since\nlearning one representation per word is problematic for polysemous words and\nhomonymous words, researchers propose to use one embedding per word sense.\nTheir approaches mainly train word sense embeddings on a corpus. In this paper,\nwe propose to use word sense definitions to learn one embedding per word sense.\nExperimental results on word similarity tasks and a word sense disambiguation\ntask show that word sense embeddings produced by our approach are of high\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 16:14:09 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 14:59:47 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 13:03:12 GMT"}, {"version": "v4", "created": "Mon, 24 Oct 2016 00:56:54 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Li", "Qi", ""], ["Li", "Tianshi", ""], ["Chang", "Baobao", ""]]}, {"id": "1606.04870", "submitter": "Karol Kurach", "authors": "Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kaufmann, Andrew\n  Tomkins, Balint Miklos, Greg Corrado, Laszlo Lukacs, Marina Ganea, Peter\n  Young, Vivek Ramavajjala", "title": "Smart Reply: Automated Response Suggestion for Email", "comments": "Accepted to KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and investigate a novel end-to-end method for\nautomatically generating short email responses, called Smart Reply. It\ngenerates semantically diverse suggestions that can be used as complete email\nresponses with just one tap on mobile. The system is currently used in Inbox by\nGmail and is responsible for assisting with 10% of all mobile responses. It is\ndesigned to work at very high throughput and process hundreds of millions of\nmessages daily. The system exploits state-of-the-art, large-scale deep\nlearning.\n  We describe the architecture of the system as well as the challenges that we\nfaced while building it, like response diversity and scalability. We also\nintroduce a new method for semantic clustering of user-generated content that\nrequires only a modest amount of explicitly labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 17:23:12 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Kannan", "Anjuli", ""], ["Kurach", "Karol", ""], ["Ravi", "Sujith", ""], ["Kaufmann", "Tobias", ""], ["Tomkins", "Andrew", ""], ["Miklos", "Balint", ""], ["Corrado", "Greg", ""], ["Lukacs", "Laszlo", ""], ["Ganea", "Marina", ""], ["Young", "Peter", ""], ["Ramavajjala", "Vivek", ""]]}, {"id": "1606.04963", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg, Eva Hasler and Bill Byrne", "title": "The Edit Distance Transducer in Action: The University of Cambridge\n  English-German System at WMT16", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the University of Cambridge submission to WMT16.\nMotivated by the complementary nature of syntactical machine translation and\nneural machine translation (NMT), we exploit the synergies of Hiero and NMT in\ndifferent combination schemes. Starting out with a simple neural lattice\nrescoring approach, we show that the Hiero lattices are often too narrow for\nNMT ensembles. Therefore, instead of a hard restriction of the NMT search space\nto the lattice, we propose to loosely couple NMT and Hiero by composition with\na modified version of the edit distance transducer. The loose combination\noutperforms lattice rescoring, especially when using multiple NMT systems in an\nensemble.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 20:08:01 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Stahlberg", "Felix", ""], ["Hasler", "Eva", ""], ["Byrne", "Bill", ""]]}, {"id": "1606.05007", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Tofigh Naghibi, Beat Pfister", "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep\n  Neural Networks", "comments": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phonemic or phonetic sub-word units are the most commonly used atomic\nelements to represent speech signals in modern ASRs. However they are not the\noptimal choice due to several reasons such as: large amount of effort required\nto handcraft a pronunciation dictionary, pronunciation variations, human\nmistakes and under-resourced dialects and languages. Here, we propose a\ndata-driven pronunciation estimation and acoustic modeling method which only\ntakes the orthographic transcription to jointly estimate a set of sub-word\nunits and a reliable dictionary. Experimental results show that the proposed\nmethod which is based on semi-supervised training of a deep neural network\nlargely outperforms phoneme based continuous speech recognition on the TIMIT\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 23:45:33 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Takahashi", "Naoya", ""], ["Naghibi", "Tofigh", ""], ["Pfister", "Beat", ""]]}, {"id": "1606.05029", "submitter": "Ferhan Ture", "authors": "Ferhan Ture and Oliver Jojic", "title": "No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for\n  Answering \"Simple\" Questions)", "comments": "7 pages, to appear in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order factoid question answering assumes that the question can be\nanswered by a single fact in a knowledge base (KB). While this does not seem\nlike a challenging task, many recent attempts that apply either complex\nlinguistic reasoning or deep neural networks achieve 65%-76% accuracy on\nbenchmark sets. Our approach formulates the task as two machine learning\nproblems: detecting the entities in the question, and classifying the question\nas one of the relation types in the KB. We train a recurrent neural network to\nsolve each problem. On the SimpleQuestions dataset, our approach yields\nsubstantial improvements over previously published results --- even neural\nnetworks based on much more complex architectures. The simplicity of our\napproach also has practical advantages, such as efficiency and modularity, that\nare valuable especially in an industry setting. In fact, we present a\npreliminary analysis of the performance of our model on real queries from\nComcast's X1 entertainment platform with millions of users every day.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 02:20:04 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 15:28:01 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Ture", "Ferhan", ""], ["Jojic", "Oliver", ""]]}, {"id": "1606.05250", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "comments": "To appear in Proceedings of the 2016 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Stanford Question Answering Dataset (SQuAD), a new reading\ncomprehension dataset consisting of 100,000+ questions posed by crowdworkers on\na set of Wikipedia articles, where the answer to each question is a segment of\ntext from the corresponding reading passage. We analyze the dataset to\nunderstand the types of reasoning required to answer the questions, leaning\nheavily on dependency and constituency trees. We build a strong logistic\nregression model, which achieves an F1 score of 51.0%, a significant\nimprovement over a simple baseline (20%). However, human performance (86.8%) is\nmuch higher, indicating that the dataset presents a good challenge problem for\nfuture research.\n  The dataset is freely available at https://stanford-qa.com\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 16:36:00 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 03:48:29 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 02:42:36 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Zhang", "Jian", ""], ["Lopyrev", "Konstantin", ""], ["Liang", "Percy", ""]]}, {"id": "1606.05286", "submitter": "Julien Perez", "authors": "Julien Perez", "title": "Spectral decomposition method of dialog state tracking via collective\n  matrix factorization", "comments": "13 pages, 3 figures, 1 Table. arXiv admin note: substantial text\n  overlap with arXiv:1606.04052", "journal-ref": "Dialogue & Discourse 7(3) (2016)", "doi": "10.5087/dad.2016.304", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of dialog management is commonly decomposed into two sequential\nsubtasks: dialog state tracking and dialog policy learning. In an end-to-end\ndialog system, the aim of dialog state tracking is to accurately estimate the\ntrue dialog state from noisy observations produced by the speech recognition\nand the natural language understanding modules. The state tracking task is\nprimarily meant to support a dialog policy. From a probabilistic perspective,\nthis is achieved by maintaining a posterior distribution over hidden dialog\nstates composed of a set of context dependent variables. Once a dialog policy\nis learned, it strives to select an optimal dialog act given the estimated\ndialog state and a defined reward function. This paper introduces a novel\nmethod of dialog state tracking based on a bilinear algebric decomposition\nmodel that provides an efficient inference schema through collective matrix\nfactorization. We evaluate the proposed approach on the second Dialog State\nTracking Challenge (DSTC-2) dataset and we show that the proposed tracker gives\nencouraging results compared to the state-of-the-art trackers that participated\nin this standard benchmark. Finally, we show that the prediction schema is\ncomputationally efficient in comparison to the previous approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 17:31:13 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Perez", "Julien", ""]]}, {"id": "1606.05320", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna, Finale Doshi-Velez", "title": "Increasing the Interpretability of Recurrent Neural Networks Using\n  Hidden Markov Models", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks (RNNs), state of the art\nmodels in speech recognition and translation. Our approach to increasing\ninterpretability is by combining an RNN with a hidden Markov model (HMM), a\nsimpler and more transparent model. We explore various combinations of RNNs and\nHMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained\nfirst, then a small LSTM is given HMM state distributions and trained to fill\nin gaps in the HMM's performance; and a jointly trained hybrid model. We find\nthat the LSTM and HMM learn complementary information about the features in the\ntext.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 19:13:52 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 22:20:39 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1606.05378", "submitter": "Reginald Long", "authors": "Reginald Long, Panupong Pasupat, Percy Liang", "title": "Simpler Context-Dependent Logical Forms via Model Projections", "comments": "10 pages, ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning a context-dependent mapping from utterances\nto denotations. With only denotations at training time, we must search over a\ncombinatorially large space of logical forms, which is even larger with\ncontext-dependent utterances. To cope with this challenge, we perform\nsuccessive projections of the full model onto simpler models that operate over\nequivalence classes of logical forms. Though less expressive, we find that\nthese simpler models are much faster and can be surprisingly effective.\nMoreover, they can be used to bootstrap the full model. Finally, we collected\nthree new context-dependent semantic parsing datasets, and develop a new\nleft-to-right parser.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 21:57:11 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Long", "Reginald", ""], ["Pasupat", "Panupong", ""], ["Liang", "Percy", ""]]}, {"id": "1606.05409", "submitter": "Linfeng Song", "authors": "Linfeng Song, Zhiguo Wang, Haitao Mi and Daniel Gildea", "title": "Sense Embedding Learning for Word Sense Induction", "comments": "6 pages, no figures in *SEM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional word sense induction (WSI) methods usually represent each\ninstance with discrete linguistic features or cooccurrence features, and train\na model for each polysemous word individually. In this work, we propose to\nlearn sense embeddings for the WSI task. In the training stage, our method\ninduces several sense centroids (embedding) for each polysemous word. In the\ntesting stage, our method represents each instance as a contextual vector, and\ninduces its sense by finding the nearest sense centroid in the embedding space.\nThe advantages of our method are (1) distributed sense vectors are taken as the\nknowledge representations which are trained discriminatively, and usually have\nbetter performance than traditional count-based distributional models, and (2)\na general model for the whole vocabulary is jointly trained to induce sense\ncentroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI\ndataset, our method outperforms all participants and most of the recent\nstate-of-the-art methods. We further verify the two advantages by comparing\nwith carefully designed baselines.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 02:49:52 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 04:59:08 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Song", "Linfeng", ""], ["Wang", "Zhiguo", ""], ["Mi", "Haitao", ""], ["Gildea", "Daniel", ""]]}, {"id": "1606.05464", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein and Tim Rockt\\\"aschel and Andreas Vlachos and\n  Kalina Bontcheva", "title": "Stance Detection with Bidirectional Conditional Encoding", "comments": "10 pages", "journal-ref": "EMNLP 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance detection is the task of classifying the attitude expressed in a text\ntowards a target such as Hillary Clinton to be \"positive\", negative\" or\n\"neutral\". Previous work has assumed that either the target is mentioned in the\ntext or that training data for every target is given. This paper considers the\nmore challenging version of this task, where targets are not always mentioned\nand no training data is available for the test targets. We experiment with\nconditional LSTM encoding, which builds a representation of the tweet that is\ndependent on the target, and demonstrate that it outperforms encoding the tweet\nand the target independently. Performance is improved further when the\nconditional model is augmented with bidirectional encoding. We evaluate our\napproach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving\nperformance second best only to a system trained on semi-automatically labelled\ntweets for the test target. When such weak supervision is added, our approach\nachieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 09:39:47 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 20:49:16 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Rockt\u00e4schel", "Tim", ""], ["Vlachos", "Andreas", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1606.05467", "submitter": "Juergen Mueller", "authors": "Juergen Mueller and Gerd Stumme", "title": "Gender Inference using Statistical Name Characteristics in Twitter", "comments": "9 pages (8 pages in actual proceedings), 2 figures, 8 tables,\n  conference: MISNC, SI, DS '16, August 15 - 17, 2016, Union, NJ, USA", "journal-ref": null, "doi": "10.1145/2955129.2955182", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much attention has been given to the task of gender inference of Twitter\nusers. Although names are strong gender indicators, the names of Twitter users\nare rarely used as a feature; probably due to the high number of ill-formed\nnames, which cannot be found in any name dictionary. Instead of relying solely\non a name database, we propose a novel name classifier. Our approach extracts\ncharacteristics from the user names and uses those in order to assign the names\nto a gender. This enables us to classify international first names as well as\nill-formed names.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 10:24:29 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 12:38:04 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Mueller", "Juergen", ""], ["Stumme", "Gerd", ""]]}, {"id": "1606.05491", "submitter": "Ond\\v{r}ej Du\\v{s}ek", "authors": "Ond\\v{r}ej Du\\v{s}ek and Filip Jur\\v{c}\\'i\\v{c}ek", "title": "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax\n  Trees and Strings", "comments": "Accepted as a short paper for ACL 2016", "journal-ref": "Proceedings of the 54th Annual Meeting of the Association for\n  Computational Linguistics, pages 45-51, Berlin, Germany, August 7-12, 2016", "doi": "10.18653/v1/P16-2008", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a natural language generator based on the sequence-to-sequence\napproach that can be trained to produce natural language strings as well as\ndeep syntax dependency trees from input dialogue acts, and we use it to\ndirectly compare two-step generation with separate sentence planning and\nsurface realization stages to a joint, one-step approach. We were able to train\nboth setups successfully using very little training data. The joint setup\noffers better performance, surpassing state-of-the-art with regards to\nn-gram-based scores while providing more relevant outputs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 11:51:25 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Du\u0161ek", "Ond\u0159ej", ""], ["Jur\u010d\u00ed\u010dek", "Filip", ""]]}, {"id": "1606.05545", "submitter": "David Vilares", "authors": "David Vilares and Carlos G\\'omez-Rodr\\'iguez and Miguel A. Alonso", "title": "Universal, Unsupervised (Rule-Based), Uncovered Sentiment Analysis", "comments": "19 pages, 5 Tables, 6 Figures. This is the authors version of a work\n  that was accepted for publication in Knowledge-Based Systems", "journal-ref": "Knowledge-Based Systems, 118:45-55, 2017", "doi": "10.1016/j.knosys.2016.11.014", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised approach for multilingual sentiment analysis\ndriven by compositional syntax-based rules. On the one hand, we exploit some of\nthe main advantages of unsupervised algorithms: (1) the interpretability of\ntheir output, in contrast with most supervised models, which behave as a black\nbox and (2) their robustness across different corpora and domains. On the other\nhand, by introducing the concept of compositional operations and exploiting\nsyntactic information in the form of universal dependencies, we tackle one of\ntheir main drawbacks: their rigidity on data that are structured differently\ndepending on the language concerned. Experiments show an improvement both over\nexisting unsupervised methods, and over state-of-the-art supervised models when\nevaluating outside their corpus of origin. Experiments also show how the same\ncompositional operations can be shared across languages. The system is\navailable at http://www.grupolys.org/software/UUUSA/\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 14:53:02 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 10:33:11 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Vilares", "David", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""], ["Alonso", "Miguel A.", ""]]}, {"id": "1606.05554", "submitter": "Noura Al Moubayed", "authors": "Noura Al Moubayed, Toby Breckon, Peter Matthews, and A. Stephen\n  McGough", "title": "SMS Spam Filtering using Probabilistic Topic Modelling and Stacked\n  Denoising Autoencoder", "comments": "Paper was accepted to the 25th International Conference on Artificial\n  Neural Networks (ICANN 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In This paper we present a novel approach to spam filtering and demonstrate\nits applicability with respect to SMS messages. Our approach requires minimum\nfeatures engineering and a small set of la- belled data samples. Features are\nextracted using topic modelling based on latent Dirichlet allocation, and then\na comprehensive data model is created using a Stacked Denoising Autoencoder\n(SDA). Topic modelling summarises the data providing ease of use and high\ninterpretability by visualising the topics using word clouds. Given that the\nSMS messages can be regarded as either spam (unwanted) or ham (wanted), the SDA\nis able to model the messages and accurately discriminate between the two\nclasses without the need for a pre-labelled training set. The results are\ncompared against the state-of-the-art spam detection algorithms with our\nproposed approach achieving over 97% accuracy which compares favourably to the\nbest reported algorithms presented in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 15:15:18 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Moubayed", "Noura Al", ""], ["Breckon", "Toby", ""], ["Matthews", "Peter", ""], ["McGough", "A. Stephen", ""]]}, {"id": "1606.05611", "submitter": "Karsten Schmidt", "authors": "Tim Zimmermann and Leo Kotschenreuther and Karsten Schmidt", "title": "Data-driven HR - R\\'esum\\'e Analysis Based on Natural Language\n  Processing and Machine Learning", "comments": "Research Prototype, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recruiters usually spend less than a minute looking at each r\\'esum\\'e when\ndeciding whether it's worth continuing the recruitment process with the\ncandidate. Recruiters focus on keywords, and it's almost impossible to\nguarantee a fair process of candidate selection. The main scope of this paper\nis to tackle this issue by introducing a data-driven approach that shows how to\nprocess r\\'esum\\'es automatically and give recruiters more time to only examine\npromising candidates. Furthermore, we show how to leverage Machine Learning and\nNatural Language Processing in order to extract all required information from\nthe r\\'esum\\'es. Once the information is extracted, a ranking score is\ncalculated. The score describes how well the candidates fit based on their\neducation, work experience and skills. Later this paper illustrates a prototype\napplication that shows how this novel approach can increase the productivity of\nrecruiters. The application enables them to filter and rank candidates based on\npredefined job descriptions. Guided by the ranking, recruiters can get deeper\ninsights from candidate profiles and validate why and how the application\nranked them. This application shows how to improve the hiring process by giving\nan unbiased hiring decision support.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 17:52:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 20:48:05 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Zimmermann", "Tim", ""], ["Kotschenreuther", "Leo", ""], ["Schmidt", "Karsten", ""]]}, {"id": "1606.05679", "submitter": "Haoruo Peng", "authors": "Haoruo Peng, Dan Roth", "title": "Two Discourse Driven Language Models for Semantics", "comments": "To appear in ACL 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding often requires deep semantic knowledge.\nExpanding on previous proposals, we suggest that some important aspects of\nsemantic knowledge can be modeled as a language model if done at an appropriate\nlevel of abstraction. We develop two distinct models that capture semantic\nframe chains and discourse information while abstracting over the specific\nmentions of predicates and entities. For each model, we investigate four\nimplementations: a \"standard\" N-gram language model and three discriminatively\ntrained \"neural\" language models that generate embeddings for semantic frames.\nThe quality of the semantic language models (SemLM) is evaluated both\nintrinsically, using perplexity and a narrative cloze test and extrinsically -\nwe show that our SemLM helps improve performance on semantic natural language\nprocessing tasks such as co-reference resolution and discourse parsing.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 21:19:35 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 06:20:52 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Peng", "Haoruo", ""], ["Roth", "Dan", ""]]}, {"id": "1606.05694", "submitter": "Soroush Vosoughi Dr", "authors": "Prashanth Vijayaraghavan, Ivan Sysoev, Soroush Vosoughi and Deb Roy", "title": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using\n  Character and Word-Level CNNs", "comments": "SemEval 2016, San Diego, California. In Proceedings of the 10th\n  International Workshop on Semantic Evaluation (SemEval-2016). San Diego,\n  California", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach for the Detecting Stance in Tweets task\n(SemEval-2016 Task 6). We utilized recent advances in short text categorization\nusing deep learning to create word-level and character-level models. The choice\nbetween word-level and character-level models in each particular case was\ninformed through validation performance. Our final system is a combination of\nclassifiers using word-level or character-level models. We also employed novel\ndata augmentation techniques to expand and diversify our training dataset, thus\nmaking our system more robust. Our system achieved a macro-average precision,\nrecall and F1-scores of 0.67, 0.61 and 0.635 respectively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:32:50 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vijayaraghavan", "Prashanth", ""], ["Sysoev", "Ivan", ""], ["Vosoughi", "Soroush", ""], ["Roy", "Deb", ""]]}, {"id": "1606.05699", "submitter": "Lu Wang", "authors": "Lu Wang and Claire Cardie and Galen Marchetti", "title": "Socially-Informed Timeline Generation for Complex Events", "comments": "NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing timeline generation systems for complex events consider only\ninformation from traditional media, ignoring the rich social context provided\nby user-generated content that reveals representative public interests or\ninsightful opinions. We instead aim to generate socially-informed timelines\nthat contain both news article summaries and selected user comments. We present\nan optimization framework designed to balance topical cohesion between the\narticle and comment summaries along with their informativeness and coverage of\nthe event. Automatic evaluations on real-world datasets that cover four complex\nevents show that our system produces more informative timelines than\nstate-of-the-art systems. In human evaluation, the associated comment summaries\nare furthermore rated more insightful than editor's picks and comments ranked\nhighly by users.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 22:52:09 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Lu", ""], ["Cardie", "Claire", ""], ["Marchetti", "Galen", ""]]}, {"id": "1606.05702", "submitter": "Lu Wang", "authors": "Lu Wang and Hema Raghavan and Claire Cardie and Vittorio Castelli", "title": "Query-Focused Opinion Summarization for User-Generated Content", "comments": "COLING 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a submodular function-based framework for query-focused opinion\nsummarization. Within our framework, relevance ordering produced by a\nstatistical ranker, and information coverage with respect to topic distribution\nand diverse viewpoints are both encoded as submodular functions. Dispersion\nfunctions are utilized to minimize the redundancy. We are the first to evaluate\ndifferent metrics of text similarity for submodularity-based summarization\nmethods. By experimenting on community QA and blog summarization, we show that\nour system outperforms state-of-the-art approaches in both automatic evaluation\nand human evaluation. A human evaluation task is conducted on Amazon Mechanical\nTurk with scale, and shows that our systems are able to generate summaries of\nhigh overall quality and information diversity.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:05:41 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Lu", ""], ["Raghavan", "Hema", ""], ["Cardie", "Claire", ""], ["Castelli", "Vittorio", ""]]}, {"id": "1606.05704", "submitter": "Lu Wang", "authors": "Lu Wang and Claire Cardie", "title": "A Piece of My Mind: A Sentiment Analysis Approach for Online Dispute\n  Detection", "comments": "ACL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the novel task of online dispute detection and propose a\nsentiment analysis solution to the problem: we aim to identify the sequence of\nsentence-level sentiments expressed during a discussion and to use them as\nfeatures in a classifier that predicts the DISPUTE/NON-DISPUTE label for the\ndiscussion as a whole. We evaluate dispute detection approaches on a newly\ncreated corpus of Wikipedia Talk page disputes and find that classifiers that\nrely on our sentiment tagging features outperform those that do not. The best\nmodel achieves a very promising F1 score of 0.78 and an accuracy of 0.80.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:22:39 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Lu", ""], ["Cardie", "Claire", ""]]}, {"id": "1606.05706", "submitter": "Lu Wang", "authors": "Lu Wang and Claire Cardie", "title": "Improving Agreement and Disagreement Identification in Online\n  Discussions with A Socially-Tuned Sentiment Lexicon", "comments": "ACL WASSA workshop 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of agreement and disagreement detection in online\ndiscussions. An isotonic Conditional Random Fields (isotonic CRF) based\nsequential model is proposed to make predictions on sentence- or segment-level.\nWe automatically construct a socially-tuned lexicon that is bootstrapped from\nexisting general-purpose sentiment lexicons to further improve the performance.\nWe evaluate our agreement and disagreement tagging model on two disparate\nonline discussion corpora -- Wikipedia Talk pages and online debates. Our model\nis shown to outperform the state-of-the-art approaches in both datasets. For\nexample, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for\nagreement and disagreement detection, when a linear chain CRF obtains 0.58 and\n0.56 for the discussions on Wikipedia Talk pages.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:29:11 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Lu", ""], ["Cardie", "Claire", ""]]}, {"id": "1606.05759", "submitter": "Nadir Durrani Dr", "authors": "Hassan Sajjad, Nadir Durrani, Francisco Guzman, Preslav Nakov, Ahmed\n  Abdelali, Stephan Vogel, Wael Salloum, Ahmed El Kholy, Nizar Habash", "title": "Egyptian Arabic to English Statistical Machine Translation System for\n  NIST OpenMT'2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the Egyptian Arabic-to-English statistical machine\ntranslation (SMT) system that the QCRI-Columbia-NYUAD (QCN) group submitted to\nthe NIST OpenMT'2015 competition. The competition focused on informal dialectal\nArabic, as used in SMS, chat, and speech. Thus, our efforts focused on\nprocessing and standardizing Arabic, e.g., using tools such as 3arrib and\nMADAMIRA. We further trained a phrase-based SMT system using state-of-the-art\nfeatures and components such as operation sequence model, class-based language\nmodel, sparse features, neural network joint model, genre-based\nhierarchically-interpolated language model, unsupervised transliteration\nmining, phrase-table merging, and hypothesis combination. Our system ranked\nsecond on all three genres.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 14:34:07 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Sajjad", "Hassan", ""], ["Durrani", "Nadir", ""], ["Guzman", "Francisco", ""], ["Nakov", "Preslav", ""], ["Abdelali", "Ahmed", ""], ["Vogel", "Stephan", ""], ["Salloum", "Wael", ""], ["Kholy", "Ahmed El", ""], ["Habash", "Nizar", ""]]}, {"id": "1606.05804", "submitter": "Patrick Verga", "authors": "Patrick Verga, Arvind Neelakantan, Andrew McCallum", "title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal\n  Schema", "comments": "EACL 2017. arXiv admin note: text overlap with arXiv:1604.06361", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal schema predicts the types of entities and relations in a knowledge\nbase (KB) by jointly embedding the union of all available schema types---not\nonly types from multiple structured databases (such as Freebase or Wikipedia\ninfoboxes), but also types expressed as textual patterns from raw text. This\nprediction is typically modeled as a matrix completion problem, with one type\nper column, and either one or two entities per row (in the case of entity types\nor binary relation types, respectively). Factorizing this sparsely observed\nmatrix yields a learned vector embedding for each row and each column. In this\npaper we explore the problem of making predictions for entities or entity-pairs\nunseen at training time (and hence without a pre-learned row embedding). We\npropose an approach having no per-row parameters at all; rather we produce a\nrow vector on the fly using a learned aggregation function of the vectors of\nthe observed columns for that row. We experiment with various aggregation\nfunctions, including neural network attention models. Our approach can be\nunderstood as a natural language database, in that questions about KB entities\nare answered by attending to textual or database evidence. In experiments\npredicting both relations and entity types, we demonstrate that despite having\nan order of magnitude fewer parameters than traditional universal schema, we\ncan match the accuracy of the traditional model, and more importantly, we can\nnow make predictions about unseen rows with nearly the same accuracy as rows\navailable at training time.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2016 20:38:42 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 21:46:47 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Verga", "Patrick", ""], ["Neelakantan", "Arvind", ""], ["McCallum", "Andrew", ""]]}, {"id": "1606.05829", "submitter": "Tianyi Luo", "authors": "Qixin Wang, Tianyi Luo, Dong Wang", "title": "Can Machine Generate Traditional Chinese Poetry? A Feigenbaum Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in neural learning demonstrated that machines can do well in\nregularized tasks, e.g., the game of Go. However, artistic activities such as\npoem generation are still widely regarded as human's special capability. In\nthis paper, we demonstrate that a simple neural model can imitate human in some\ntasks of art generation. We particularly focus on traditional Chinese poetry,\nand show that machines can do as well as many contemporary poets and weakly\npass the Feigenbaum Test, a variant of Turing test in professional domains. Our\nmethod is based on an attention-based recurrent neural network, which accepts a\nset of keywords as the theme and generates poems by looking at each keyword\nduring the generation. A number of techniques are proposed to improve the\nmodel, including character vector initialization, attention to input and\nhybrid-style training. Compared to existing poetry generation methods, our\nmodel can generate much more theme-consistent and semantic-rich poems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 03:17:29 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Qixin", ""], ["Luo", "Tianyi", ""], ["Wang", "Dong", ""]]}, {"id": "1606.05854", "submitter": "Dong Xu", "authors": "Dong Xu and Wu-Jun Li", "title": "Full-Time Supervision based Bidirectional RNN for Factoid Question\n  Answering", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, bidirectional recurrent neural network (BRNN) has been widely used\nfor question answering (QA) tasks with promising performance. However, most\nexisting BRNN models extract the information of questions and answers by\ndirectly using a pooling operation to generate the representation for loss or\nsimilarity calculation. Hence, these existing models don't put supervision\n(loss or similarity calculation) at every time step, which will lose some\nuseful information. In this paper, we propose a novel BRNN model called\nfull-time supervision based BRNN (FTS-BRNN), which can put supervision at every\ntime step. Experiments on the factoid QA task show that our FTS-BRNN can\noutperform other baselines to achieve the state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 11:03:47 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 01:47:06 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Xu", "Dong", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1606.05925", "submitter": "Vikrant Singh Tomar", "authors": "Vikrant Singh Tomar and Richard C. Rose", "title": "Graph based manifold regularized deep neural networks for automatic\n  speech recognition", "comments": "12 pages including citations, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been successfully applied to a wide variety\nof acoustic modeling tasks in recent years. These include the applications of\nDNNs either in a discriminative feature extraction or in a hybrid acoustic\nmodeling scenario. Despite the rapid progress in this area, a number of\nchallenges remain in training DNNs. This paper presents an effective way of\ntraining DNNs using a manifold learning based regularization framework. In this\nframework, the parameters of the network are optimized to preserve underlying\nmanifold based relationships between speech feature vectors while minimizing a\nmeasure of loss between network outputs and targets. This is achieved by\nincorporating manifold based locality constraints in the objective criterion of\nDNNs. Empirical evidence is provided to demonstrate that training a network\nwith manifold constraints preserves structural compactness in the hidden layers\nof the network. Manifold regularization is applied to train bottleneck DNNs for\nfeature extraction in hidden Markov model (HMM) based speech recognition. The\nexperiments in this work are conducted on the Aurora-2 spoken digits and the\nAurora-4 read news large vocabulary continuous speech recognition tasks. The\nperformance is measured in terms of word error rate (WER) on these tasks. It is\nshown that the manifold regularized DNNs result in up to 37% reduction in WER\nrelative to standard DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 23:40:51 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Tomar", "Vikrant Singh", ""], ["Rose", "Richard C.", ""]]}, {"id": "1606.05967", "submitter": "Amir Hossein Harati Nejad Torbati", "authors": "Amir Hossein Harati Nejad Torbati, Joseph Picone", "title": "A Nonparametric Bayesian Approach for Spoken Term detection by Example\n  Query", "comments": "interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art speech recognition systems use data-intensive\ncontext-dependent phonemes as acoustic units. However, these approaches do not\ntranslate well to low resourced languages where large amounts of training data\nis not available. For such languages, automatic discovery of acoustic units is\ncritical. In this paper, we demonstrate the application of nonparametric\nBayesian models to acoustic unit discovery. We show that the discovered units\nare correlated with phonemes and therefore are linguistically meaningful. We\nalso present a spoken term detection (STD) by example query algorithm based on\nthese automatically learned units. We show that our proposed system produces a\nP@N of 61.2% and an EER of 13.95% on the TIMIT dataset. The improvement in the\nEER is 5% while P@N is only slightly lower than the best reported system in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 04:06:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Torbati", "Amir Hossein Harati Nejad", ""], ["Picone", "Joseph", ""]]}, {"id": "1606.05994", "submitter": "Normunds Gruzitis", "authors": "Normunds Gruzitis and Guntis Barzdins", "title": "The Role of CNL and AMR in Scalable Abstractive Summarization for\n  Multilingual Media Monitoring", "comments": "Proceedings of the 5th Workshop on Controlled Natural Language, 2016\n  (to appear)", "journal-ref": "Controlled Natural Language, Lecture Notes in Computer Science,\n  Vol. 9767, Springer, 2016, pp. 127-130", "doi": "10.1007/978-3-319-41498-0", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Big Data and Deep Learning, there is a common view that machine\nlearning approaches are the only way to cope with the robust and scalable\ninformation extraction and summarization. It has been recently proposed that\nthe CNL approach could be scaled up, building on the concept of embedded CNL\nand, thus, allowing for CNL-based information extraction from e.g. normative or\nmedical texts that are rather controlled by nature but still infringe the\nboundaries of CNL. Although it is arguable if CNL can be exploited to approach\nthe robust wide-coverage semantic parsing for use cases like media monitoring,\nits potential becomes much more obvious in the opposite direction: generation\nof story highlights from the summarized AMR graphs, which is in the focus of\nthis position paper.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 07:15:55 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Gruzitis", "Normunds", ""], ["Barzdins", "Guntis", ""]]}, {"id": "1606.06031", "submitter": "Sandro Pezzelle", "authors": "Denis Paperno (1), Germ\\'an Kruszewski (1), Angeliki Lazaridou (1),\n  Quan Ngoc Pham (1), Raffaella Bernardi (1), Sandro Pezzelle (1), Marco Baroni\n  (1), Gemma Boleda (1), Raquel Fern\\'andez (2) ((1) CIMeC - Center for\n  Mind/Brain Sciences, University of Trento, (2) Institute for Logic, Language\n  & Computation, University of Amsterdam)", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "comments": "10 pages, Accepted as a long paper for ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 09:37:17 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Paperno", "Denis", ""], ["Kruszewski", "Germ\u00e1n", ""], ["Lazaridou", "Angeliki", ""], ["Pham", "Quan Ngoc", ""], ["Bernardi", "Raffaella", ""], ["Pezzelle", "Sandro", ""], ["Baroni", "Marco", ""], ["Boleda", "Gemma", ""], ["Fern\u00e1ndez", "Raquel", ""]]}, {"id": "1606.06061", "submitter": "Heiga Zen", "authors": "Heiga Zen and Yannis Agiomyrgiannakis and Niels Egberts and Fergus\n  Henderson and Przemys{\\l}aw Szczepaniak", "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric\n  Speech Synthesizers for Mobile Devices", "comments": "13 pages, 3 figures, Interspeech 2016 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic models based on long short-term memory recurrent neural networks\n(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and\nshowed significant improvements in naturalness and latency over those based on\nhidden Markov models (HMMs). This paper describes further optimizations of\nLSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,\nmulti-frame inference, and robust inference using an {\\epsilon}-contaminated\nGaussian loss function. Experimental results in subjective listening tests show\nthat these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based\nSPSS in runtime speed while maintaining naturalness. Evaluations between\nLSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 10:54:51 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 15:11:30 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Zen", "Heiga", ""], ["Agiomyrgiannakis", "Yannis", ""], ["Egberts", "Niels", ""], ["Henderson", "Fergus", ""], ["Szczepaniak", "Przemys\u0142aw", ""]]}, {"id": "1606.06083", "submitter": "Vivek Gupta", "authors": "Vivek Gupta, Harish Karnick, Ashendra Bansal, Pradhuman Jhala", "title": "Product Classification in E-Commerce using Distributional Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Product classification is the task of automatically predicting a taxonomy\npath for a product in a predefined taxonomy hierarchy given a textual product\ndescription or title. For efficient product classification we require a\nsuitable representation for a document (the textual description of a product)\nfeature vector and efficient and fast algorithms for prediction. To address the\nabove challenges, we propose a new distributional semantics representation for\ndocument vector formation. We also develop a new two-level ensemble approach\nutilizing (with respect to the taxonomy tree) a path-wise, node-wise and\ndepth-wise classifiers for error reduction in the final product classification.\nOur experiments show the effectiveness of the distributional representation and\nthe ensemble approach on data sets from a leading e-commerce platform and\nachieve better results on various evaluation metrics compared to earlier\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 12:26:21 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 10:38:52 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Gupta", "Vivek", ""], ["Karnick", "Harish", ""], ["Bansal", "Ashendra", ""], ["Jhala", "Pradhuman", ""]]}, {"id": "1606.06086", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Mihai Lupu, Allan Hanbury", "title": "Uncertainty in Neural Network Word Embedding: Exploration of Threshold\n  for Similarity", "comments": "Neu-IR Workshop at the ACM Conference on Research and Development in\n  Information Retrieval (NeuIR-SIGIR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding, specially with its recent developments, promises a\nquantification of the similarity between terms. However, it is not clear to\nwhich extent this similarity value can be genuinely meaningful and useful for\nsubsequent tasks. We explore how the similarity score obtained from the models\nis really indicative of term relatedness. We first observe and quantify the\nuncertainty factor of the word embedding models regarding to the similarity\nvalue. Based on this factor, we introduce a general threshold on various\ndimensions which effectively filters the highly related terms. Our evaluation\non four information retrieval collections supports the effectiveness of our\napproach as the results of the introduced threshold are significantly better\nthan the baseline while being equal to or statistically indistinguishable from\nthe optimal results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 12:31:13 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 07:33:59 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Lupu", "Mihai", ""], ["Hanbury", "Allan", ""]]}, {"id": "1606.06121", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Quantifying and Reducing Stereotypes in Word Embeddings", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are optimized to model statistical properties of\nthe training data. If the input data reflects stereotypes and biases of the\nbroader society, then the output of the learning algorithm also captures these\nstereotypes. In this paper, we initiate the study of gender stereotypes in {\\em\nword embedding}, a popular framework to represent text data. As their use\nbecomes increasingly common, applications can inadvertently amplify unwanted\nstereotypes. We show across multiple datasets that the embeddings contain\nsignificant gender stereotypes, especially with regard to professions. We\ncreated a novel gender analogy task and combined it with crowdsourcing to\nsystematically quantify the gender bias in a given embedding. We developed an\nefficient algorithm that reduces gender stereotype using just a handful of\ntraining examples while preserving the useful geometric properties of the\nembedding. We evaluated our algorithm on several metrics. While we focus on\nmale/female stereotypes, our framework may be applicable to other types of\nembedding biases.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 13:58:45 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1606.06125", "submitter": "Jirka Marsik", "authors": "Jirka Mar\\v{s}\\'ik (SEMAGRAMME), Maxime Amblard (MSH Lorraine,\n  SEMAGRAMME)", "title": "Introducing a Calculus of Effects and Handlers for Natural Language\n  Semantics", "comments": null, "journal-ref": "Formal Grammar 2016, Aug 2016, Bozen-Bolzano, Italy", "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compositional model-theoretic semantics, researchers assemble\ntruth-conditions or other kinds of denotations using the lambda calculus. It\nwas previously observed that the lambda terms and/or the denotations studied\ntend to follow the same pattern: they are instances of a monad. In this paper,\nwe present an extension of the simply-typed lambda calculus that exploits this\nuniformity using the recently discovered technique of effect handlers. We prove\nthat our calculus exhibits some of the key formal properties of the lambda\ncalculus and we use it to construct a modular semantics for a small fragment\nthat involves multiple distinct semantic phenomena.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:05:59 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 07:14:44 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Mar\u0161\u00edk", "Jirka", "", "SEMAGRAMME"], ["Amblard", "Maxime", "", "MSH Lorraine,\n  SEMAGRAMME"]]}, {"id": "1606.06137", "submitter": "Markus Koskela", "authors": "Petri Luukkonen, Markus Koskela, and Patrik Flor\\'een", "title": "LSTM-Based Predictions for Proactive Information Retrieval", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for proactive information retrieval targeted at\nretrieving relevant information during a writing task. In our method, the\ncurrent task and the needs of the user are estimated, and the potential next\nsteps are unobtrusively predicted based on the user's past actions. We focus on\nthe task of writing, in which the user is coalescing previously collected\ninformation into a text. Our proactive system automatically recommends the user\nrelevant background information. The proposed system incorporates text input\nprediction using a long short-term memory (LSTM) network. We present\nsimulations, which show that the system is able to reach higher precision\nvalues in an exploratory search setting compared to both a baseline and a\ncomparison system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:26:33 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Luukkonen", "Petri", ""], ["Koskela", "Markus", ""], ["Flor\u00e9en", "Patrik", ""]]}, {"id": "1606.06142", "submitter": "Gergely Tib\\'ely", "authors": "Gergely Tib\\'ely, David Sousa-Rodrigues, P\\'eter Pollner, Gergely\n  Palla", "title": "Comparing the hierarchy of keywords in on-line news portals", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0165728", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tagging of on-line content with informative keywords is a widespread\nphenomenon from scientific article repositories through blogs to on-line news\nportals. In most of the cases, the tags on a given item are free words chosen\nby the authors independently. Therefore, relations among keywords in a\ncollection of news items is unknown. However, in most cases the topics and\nconcepts described by these keywords are forming a latent hierarchy, with the\nmore general topics and categories at the top, and more specialised ones at the\nbottom. Here we apply a recent, cooccurrence-based tag hierarchy extraction\nmethod to sets of keywords obtained from four different on-line news portals.\nThe resulting hierarchies show substantial differences not just in the topics\nrendered as important (being at the top of the hierarchy) or of less interest\n(categorised low in the hierarchy), but also in the underlying network\nstructure. This reveals discrepancies between the plausible keyword association\nframeworks in the studied news portals.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:34:55 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Tib\u00e9ly", "Gergely", ""], ["Sousa-Rodrigues", "David", ""], ["Pollner", "P\u00e9ter", ""], ["Palla", "Gergely", ""]]}, {"id": "1606.06164", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg, Roser Morante, Desmond Elliott", "title": "Pragmatic factors in image description: the case of negations", "comments": "Accepted as a short paper for the 5th Workshop on Vision and\n  Language, collocated with ACL 2016, Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a qualitative analysis of the descriptions containing negations\n(no, not, n't, nobody, etc) in the Flickr30K corpus, and a categorization of\nnegation uses. Based on this analysis, we provide a set of requirements that an\nimage description system should have in order to generate negation sentences.\nAs a pilot experiment, we used our categorization to manually annotate\nsentences containing negations in the Flickr30K corpus, with an agreement score\nof K=0.67. With this paper, we hope to open up a broader discussion of\nsubjective language in image descriptions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 15:08:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 09:06:28 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["van Miltenburg", "Emiel", ""], ["Morante", "Roser", ""], ["Elliott", "Desmond", ""]]}, {"id": "1606.06259", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Rowan Zellers, Eli Pincus, Louis-Philippe Morency", "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis\n  in Online Opinion Videos", "comments": "Accepted as Journal Publication in IEEE Intelligent Systems", "journal-ref": "IEEE Intelligent Systems 31.6 (2016): 82-88", "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are sharing their opinions, stories and reviews through online video\nsharing websites every day. Studying sentiment and subjectivity in these\nopinion videos is experiencing a growing attention from academia and industry.\nWhile sentiment analysis has been successful for text, it is an understudied\nresearch question for videos and multimedia content. The biggest setbacks for\nstudies in this direction are lack of a proper dataset, methodology, baselines\nand statistical analysis of how information from different modality sources\nrelate to each other. This paper introduces to the scientific community the\nfirst opinion-level annotated corpus of sentiment and subjectivity analysis in\nonline videos called Multimodal Opinion-level Sentiment Intensity dataset\n(MOSI). The dataset is rigorously annotated with labels for subjectivity,\nsentiment intensity, per-frame and per-opinion annotated visual features, and\nper-milliseconds annotated audio features. Furthermore, we present baselines\nfor future studies in this direction as well as a new multimodal fusion\napproach that jointly models spoken words and visual gestures.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 19:23:53 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 02:39:40 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zadeh", "Amir", ""], ["Zellers", "Rowan", ""], ["Pincus", "Eli", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1606.06274", "submitter": "Vivek Datla V", "authors": "Vivek Datla, David Lin, Max Louwerse and Abhinav Vishnu", "title": "A Data-Driven Approach for Semantic Role Labeling from Induced Grammar\n  Structures in Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic roles play an important role in extracting knowledge from text.\nCurrent unsupervised approaches utilize features from grammar structures, to\ninduce semantic roles. The dependence on these grammars, however, makes it\ndifficult to adapt to noisy and new languages. In this paper we develop a\ndata-driven approach to identifying semantic roles, the approach is entirely\nunsupervised up to the point where rules need to be learned to identify the\nposition the semantic role occurs. Specifically we develop a modified-ADIOS\nalgorithm based on ADIOS Solan et al. (2005) to learn grammar structures, and\nuse these grammar structures to learn the rules for identifying the semantic\nroles based on the context in which the grammar structures appeared. The\nresults obtained are comparable with the current state-of-art models that are\ninherently dependent on human annotated data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 19:53:53 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Datla", "Vivek", ""], ["Lin", "David", ""], ["Louwerse", "Max", ""], ["Vishnu", "Abhinav", ""]]}, {"id": "1606.06352", "submitter": "Brendan O'Connor", "authors": "Abram Handler, Su Lin Blodgett, Brendan O'Connor", "title": "Visualizing textual models with in-text and word-as-pixel highlighting", "comments": "Presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore two techniques which use color to make sense of statistical text\nmodels. One method uses in-text annotations to illustrate a model's view of\nparticular tokens in particular documents. Another uses a high-level,\n\"words-as-pixels\" graphic to display an entire corpus. Together, these methods\noffer both zoomed-in and zoomed-out perspectives into a model's understanding\nof text. We show how these interconnected methods help diagnose a classifier's\npoor performance on Twitter slang, and make sense of a topic model on\nhistorical political texts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 22:30:19 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Handler", "Abram", ""], ["Blodgett", "Su Lin", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1606.06361", "submitter": "Abulhair Saparov", "authors": "Abulhair Saparov, Tom M. Mitchell", "title": "A Probabilistic Generative Grammar for Semantic Parsing", "comments": "[manuscript draft]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework that couples the syntax and semantics of natural\nlanguage sentences in a generative model, in order to develop a semantic parser\nthat jointly infers the syntactic, morphological, and semantic representations\nof a given sentence under the guidance of background knowledge. To generate a\nsentence in our framework, a semantic statement is first sampled from a prior,\nsuch as from a set of beliefs in a knowledge base. Given this semantic\nstatement, a grammar probabilistically generates the output sentence. A joint\nsemantic-syntactic parser is derived that returns the $k$-best semantic and\nsyntactic parses for a given sentence. The semantic prior is flexible, and can\nbe used to incorporate background knowledge during parsing, in ways unlike\nprevious semantic parsing approaches. For example, semantic statements\ncorresponding to beliefs in a knowledge base can be given higher prior\nprobability, type-correct statements can be given somewhat lower probability,\nand beliefs outside the knowledge base can be given lower probability. The\nconstruction of our grammar invokes a novel application of hierarchical\nDirichlet processes (HDPs), which in turn, requires a novel and efficient\ninference approach. We present experimental results showing, for a simple\ngrammar, that our parser outperforms a state-of-the-art CCG semantic parser and\nscales to knowledge bases with millions of beliefs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:29:55 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Saparov", "Abulhair", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "1606.06368", "submitter": "Fereshte Khani", "authors": "Fereshte Khani, Martin Rinard, Percy Liang", "title": "Unanimous Prediction for 100% Precision with Application to Learning\n  Semantic Mappings", "comments": "ACL 2016, Removed the duplicate author name of the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we train a system that, on any new input, either says \"don't know\" or\nmakes a prediction that is guaranteed to be correct? We answer the question in\nthe affirmative provided our model family is well-specified. Specifically, we\nintroduce the unanimity principle: only predict when all models consistent with\nthe training data predict the same output. We operationalize this principle for\nsemantic parsing, the task of mapping utterances to logical forms. We develop a\nsimple, efficient method that reasons over the infinite set of all consistent\nmodels by only checking two of the models. We prove that our method obtains\n100% precision even with a modest amount of training data from a possibly\nadversarial distribution. Empirically, we demonstrate the effectiveness of our\napproach on the standard GeoQuery dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 23:59:25 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 07:33:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Khani", "Fereshte", ""], ["Rinard", "Martin", ""], ["Liang", "Percy", ""]]}, {"id": "1606.06406", "submitter": "James Cross", "authors": "James Cross and Liang Huang", "title": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM", "comments": "Pre-print of paper appearing in ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural network approaches for parsing have largely automated the\ncombination of individual features, but still rely on (often a larger number\nof) atomic features created from human linguistic intuition, and potentially\nomitting important global context. To further reduce feature engineering to the\nbare minimum, we use bi-directional LSTM sentence representations to model a\nparser state with only three sentence positions, which automatically identifies\nimportant aspects of the entire sentence. This model achieves state-of-the-art\nresults among greedy dependency parsers for English. We also introduce a novel\ntransition system for constituency parsing which does not require binarization,\nand together with the above architecture, achieves state-of-the-art results\namong greedy parsers for both English and Chinese.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 03:20:59 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Cross", "James", ""], ["Huang", "Liang", ""]]}, {"id": "1606.06424", "submitter": "Tanmay Basu", "authors": "Tanmay Basu, Shraman Kumar, Abhishek Kalyan, Priyanka Jayaswal, Pawan\n  Goyal, Stephen Pettifer and Siddhartha R. Jonnalagadda", "title": "A Novel Framework to Expedite Systematic Reviews by Automatically\n  Building Information Extraction Training Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic review identifies and collates various clinical studies and\ncompares data elements and results in order to provide an evidence based answer\nfor a particular clinical question. The process is manual and involves lot of\ntime. A tool to automate this process is lacking. The aim of this work is to\ndevelop a framework using natural language processing and machine learning to\nbuild information extraction algorithms to identify data elements in a new\nprimary publication, without having to go through the expensive task of manual\nannotation to build gold standards for each data element type. The system is\ndeveloped in two stages. Initially, it uses information contained in existing\nsystematic reviews to identify the sentences from the PDF files of the included\nreferences that contain specific data elements of interest using a modified\nJaccard similarity measure. These sentences have been treated as labeled data.A\nSupport Vector Machine (SVM) classifier is trained on this labeled data to\nextract data elements of interests from a new article. We conducted experiments\non Cochrane Database systematic reviews related to congestive heart failure\nusing inclusion criteria as an example data element. The empirical results show\nthat the proposed system automatically identifies sentences containing the data\nelement of interest with a high recall (93.75%) and reasonable precision\n(27.05% - which means the reviewers have to read only 3.7 sentences on\naverage). The empirical results suggest that the tool is retrieving valuable\ninformation from the reference articles, even when it is time-consuming to\nidentify them manually. Thus we hope that the tool will be useful for automatic\ndata extraction from biomedical research publications. The future scope of this\nwork is to generalize this information framework for all types of systematic\nreviews.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 04:56:33 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Basu", "Tanmay", ""], ["Kumar", "Shraman", ""], ["Kalyan", "Abhishek", ""], ["Jayaswal", "Priyanka", ""], ["Goyal", "Pawan", ""], ["Pettifer", "Stephen", ""], ["Jonnalagadda", "Siddhartha R.", ""]]}, {"id": "1606.06461", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu and Mark Johnson", "title": "Neighborhood Mixture Model for Knowledge Base Completion", "comments": "V1: In Proceedings of the 20th SIGNLL Conference on Computational\n  Natural Language Learning, CoNLL 2016. V2: Corrected citation to (Krompa{\\ss}\n  et al., 2015). V3: A revised version of our CoNLL 2016 paper to update latest\n  related work", "journal-ref": null, "doi": "10.18653/v1/K16-1005", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are useful resources for many natural language processing\ntasks, however, they are far from complete. In this paper, we define a novel\nentity representation as a mixture of its neighborhood in the knowledge base\nand apply this technique on TransE-a well-known embedding model for knowledge\nbase completion. Experimental results show that the neighborhood information\nsignificantly helps to improve the results of the TransE model, leading to\nbetter performance than obtained by other state-of-the-art embedding models on\nthree benchmark datasets for triple classification, entity prediction and\nrelation prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:54:35 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 16:08:32 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 12:51:31 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Sirts", "Kairit", ""], ["Qu", "Lizhen", ""], ["Johnson", "Mark", ""]]}, {"id": "1606.06622", "submitter": "Gordon Christie", "authors": "Arijit Ray, Gordon Christie, Mohit Bansal, Dhruv Batra, Devi Parikh", "title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise\n  Questions", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is the task of answering natural-language\nquestions about images. We introduce the novel problem of determining the\nrelevance of questions to images in VQA. Current VQA models do not reason about\nwhether a question is even related to the given image (e.g. What is the capital\nof Argentina?) or if it requires information from external resources to answer\ncorrectly. This can break the continuity of a dialogue in human-machine\ninteraction. Our approaches for determining relevance are composed of two\nstages. Given an image and a question, (1) we first determine whether the\nquestion is visual or not, (2) if visual, we determine whether the question is\nrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA\nmodel uncertainty, and caption-question similarity, are able to outperform\nstrong baselines on both relevance tasks. We also present human studies showing\nthat VQA models augmented with such question relevance reasoning are perceived\nas more intelligent, reasonable, and human-like.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:38:27 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 02:56:00 GMT"}, {"version": "v3", "created": "Mon, 26 Sep 2016 15:24:28 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Ray", "Arijit", ""], ["Christie", "Gordon", ""], ["Bansal", "Mohit", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.06623", "submitter": "Georgios Balikas", "authors": "Georgios Balikas and Massih-Reza Amini", "title": "An empirical study on large scale text classification with skip-gram\n  embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the integration of word embeddings as classification features\nin the setting of large scale text classification. Such representations have\nbeen used in a plethora of tasks, however their application in classification\nscenarios with thousands of classes has not been extensively researched,\npartially due to hardware limitations. In this work, we examine efficient\ncomposition functions to obtain document-level from word-level embeddings and\nwe subsequently investigate their combination with the traditional\none-hot-encoding representations. By presenting empirical evidence on large,\nmulti-class, multi-label classification problems, we demonstrate the efficiency\nand the performance benefits of this combination.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:39:35 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1606.06640", "submitter": "Georg Heigold", "authors": "Georg Heigold, Guenter Neumann, Josef van Genabith", "title": "Neural Morphological Tagging from Characters for Morphologically Rich\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates neural character-based morphological tagging for\nlanguages with complex morphology and large tag sets. We systematically explore\na variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain\ncharacter-based word vectors combined with bidirectional LSTMs to model\nacross-word context in an end-to-end setting. We explore supplementary use of\nword-based vectors trained on large amounts of unlabeled data. Our experiments\nfor morphological tagging suggest that for \"simple\" model configurations, the\nchoice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or\nthe augmentation with pre-trained word embeddings can be important and clearly\nimpact the accuracy. Increasing the model capacity by adding depth, for\nexample, and carefully optimizing the neural networks can lead to substantial\nimprovements, and the differences in accuracy (but not training time) become\nmuch smaller or even negligible. Overall, our best morphological taggers for\nGerman and Czech outperform the best results reported in the literature by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 16:25:31 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Heigold", "Georg", ""], ["Neumann", "Guenter", ""], ["van Genabith", "Josef", ""]]}, {"id": "1606.06710", "submitter": "Yulia Tsvetkov", "authors": "Yulia Tsvetkov, Manaal Faruqui, Chris Dyer", "title": "Correlation-based Intrinsic Evaluation of Word Vector Representations", "comments": "RepEval 2016, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce QVEC-CCA--an intrinsic evaluation metric for word vector\nrepresentations based on correlations of learned vectors with features\nextracted from linguistic resources. We show that QVEC-CCA scores are an\neffective proxy for a range of extrinsic semantic and syntactic tasks. We also\nshow that the proposed evaluation obtains higher and more consistent\ncorrelations with downstream tasks, compared to existing approaches to\nintrinsic evaluation of word vectors that are based on word similarity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 19:12:01 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tsvetkov", "Yulia", ""], ["Faruqui", "Manaal", ""], ["Dyer", "Chris", ""]]}, {"id": "1606.06737", "submitter": "Max Tegmark", "authors": "Henry W. Lin (Harvard), Max Tegmark (MIT)", "title": "Criticality in Formal Languages and Statistical Physics", "comments": "Replaced to match final published version. Discussion improved,\n  references added", "journal-ref": "Entropy, 19, 299 (2017)", "doi": "10.3390/e19070299", "report-no": null, "categories": "cond-mat.dis-nn cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the mutual information between two symbols, as a function of the\nnumber of symbols between the two, decays exponentially in any probabilistic\nregular grammar, but can decay like a power law for a context-free grammar.\nThis result about formal languages is closely related to a well-known result in\nclassical statistical mechanics that there are no phase transitions in\ndimensions fewer than two. It is also related to the emergence of power-law\ncorrelations in turbulence and cosmological inflation through recursive\ngenerative processes. We elucidate these physics connections and comment on\npotential applications of our results to machine learning tasks like training\nartificial recurrent neural networks. Along the way, we introduce a useful\nquantity which we dub the rational mutual information and discuss\ngeneralizations of our claims involving more complicated Bayesian networks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 20:00:01 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 16:23:06 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 18:25:00 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Lin", "Henry W.", "", "Harvard"], ["Tegmark", "Max", "", "MIT"]]}, {"id": "1606.06820", "submitter": "Ryan Gallagher", "authors": "Ryan J. Gallagher, Andrew J. Reagan, Christopher M. Danforth, Peter\n  Sheridan Dodds", "title": "Divergent discourse between protests and counter-protests:\n  #BlackLivesMatter and #AllLivesMatter", "comments": "26 pages, 27 figures", "journal-ref": "PLoS ONE, 2018", "doi": "10.1371/journal.pone.0195644", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the shooting of Black teenager Michael Brown by White police officer\nDarren Wilson in Ferguson, Missouri, the protest hashtag #BlackLivesMatter has\namplified critiques of extrajudicial killings of Black Americans. In response\nto #BlackLivesMatter, other Twitter users have adopted #AllLivesMatter, a\ncounter-protest hashtag whose content argues that equal attention should be\ngiven to all lives regardless of race. Through a multi-level analysis of over\n860,000 tweets, we study how these protests and counter-protests diverge by\nquantifying aspects of their discourse. We find that #AllLivesMatter\nfacilitates opposition between #BlackLivesMatter and hashtags such as\n#PoliceLivesMatter and #BlueLivesMatter in such a way that historically echoes\nthe tension between Black protesters and law enforcement. In addition, we show\nthat a significant portion of #AllLivesMatter use stems from hijacking by\n#BlackLivesMatter advocates. Beyond simply injecting #AllLivesMatter with\n#BlackLivesMatter content, these hijackers use the hashtag to directly confront\nthe counter-protest notion of \"All lives matter.\" Our findings suggest that\nBlack Lives Matter movement was able to grow, exhibit diverse conversations,\nand avoid derailment on social media by making discussion of counter-protest\nopinions a central topic of #AllLivesMatter, rather than the movement itself.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 05:23:01 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 15:52:06 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 17:28:11 GMT"}, {"version": "v4", "created": "Wed, 19 Oct 2016 02:41:31 GMT"}, {"version": "v5", "created": "Sat, 20 May 2017 03:36:05 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Gallagher", "Ryan J.", ""], ["Reagan", "Andrew J.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1606.06864", "submitter": "Stefan Braun", "authors": "Stefan Braun, Daniel Neil, Shih-Chii Liu", "title": "A Curriculum Learning Method for Improved Noise Robustness in Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of automatic speech recognition systems under noisy\nenvironments still leaves room for improvement. Speech enhancement or feature\nenhancement techniques for increasing noise robustness of these systems usually\nadd components to the recognition system that need careful optimization. In\nthis work, we propose the use of a relatively simple curriculum training\nstrategy called accordion annealing (ACCAN). It uses a multi-stage training\nschedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are\nfirst added and samples at increasing higher SNR values are gradually added up\nto an SNR value of 50dB. We also use a method called per-epoch noise mixing\n(PEM) that generates noisy training samples online during training and thus\nenables dynamically changing the SNR of our training data. Both the ACCAN and\nthe PEM methods are evaluated on a end-to-end speech recognition pipeline on\nthe Wall Street Journal corpus. ACCAN decreases the average word error rate\n(WER) on the 20dB to -10dB SNR range by up to 31.4% when compared to a\nconventional multi-condition training method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 09:29:40 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 15:20:39 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Braun", "Stefan", ""], ["Neil", "Daniel", ""], ["Liu", "Shih-Chii", ""]]}, {"id": "1606.06871", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Patrick Doetsch, Paul Voigtlaender, Ralf Schl\\\"uter,\n  Hermann Ney", "title": "A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic\n  Modeling in Speech Recognition", "comments": "published on ICASSP 2017 conference, New Orleans, USA", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952599", "report-no": null, "categories": "cs.NE cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study of deep bidirectional long short-term memory\n(LSTM) recurrent neural network (RNN) based acoustic models for automatic\nspeech recognition (ASR). We study the effect of size and depth and train\nmodels of up to 8 layers. We investigate the training aspect and study\ndifferent variants of optimization methods, batching, truncated\nbackpropagation, different regularization techniques such as dropout and $L_2$\nregularization, and different gradient clipping variants.\n  The major part of the experimental analysis was performed on the Quaero\ncorpus. Additional experiments also were performed on the Switchboard corpus.\nOur best LSTM model has a relative improvement in word error rate of over 14\\%\ncompared to our best feed-forward neural network (FFNN) baseline on the Quaero\ntask. On this task, we get our best result with an 8 layer bidirectional LSTM\nand we show that a pretraining scheme with layer-wise construction helps for\ndeep LSTMs.\n  Finally we compare the training calculation time of many of the presented\nexperiments in relation with recognition performance.\n  All the experiments were done with RETURNN, the RWTH extensible training\nframework for universal recurrent neural networks in combination with RASR, the\nRWTH ASR toolkit.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 10:00:14 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 08:08:29 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Zeyer", "Albert", ""], ["Doetsch", "Patrick", ""], ["Voigtlaender", "Paul", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "1606.06900", "submitter": "Panupong Pasupat", "authors": "Panupong Pasupat and Percy Liang", "title": "Inferring Logical Forms From Denotations", "comments": "Published at the Association for Computational Linguistics (ACL)\n  conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core problem in learning semantic parsers from denotations is picking out\nconsistent logical forms--those that yield the correct denotation--from a\ncombinatorially large space. To control the search space, previous work relied\non restricted set of rules, which limits expressivity. In this paper, we\nconsider a much more expressive class of logical forms, and show how to use\ndynamic programming to efficiently represent the complete set of consistent\nlogical forms. Expressivity also introduces many more spurious logical forms\nwhich are consistent with the correct denotation but do not represent the\nmeaning of the utterance. To address this, we generate fictitious worlds and\nuse crowdsourced denotations on these worlds to filter out spurious logical\nforms. On the WikiTableQuestions dataset, we increase the coverage of\nanswerable questions from 53.5% to 76%, and the additional crowdsourced\nsupervision lets us rule out 92.1% of spurious logical forms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 11:07:43 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 21:24:08 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Pasupat", "Panupong", ""], ["Liang", "Percy", ""]]}, {"id": "1606.06905", "submitter": "Ying Wen", "authors": "Ying Wen, Weinan Zhang, Rui Luo, Jun Wang", "title": "Learning text representation using recurrent convolutional neural\n  network with highway layers", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the rapid development of word embedding and neural networks has\nbrought new inspiration to various NLP and IR tasks. In this paper, we describe\na staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN)\nwith highway layers. The highway network module is incorporated in the middle\ntakes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module\nin the first stage and provides the Convolutional Neural Network (CNN) module\nin the last stage with the input. The experiment shows that our model\noutperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment\nanalysis task. Besides, the analysis of how sequence length influences the RCNN\nwith highway layers shows that our model could learn good representation for\nthe long text.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 11:30:47 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 16:17:05 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Wen", "Ying", ""], ["Zhang", "Weinan", ""], ["Luo", "Rui", ""], ["Wang", "Jun", ""]]}, {"id": "1606.06950", "submitter": "Herman Kamper", "authors": "Herman Kamper, Aren Jansen, Sharon Goldwater", "title": "A segmental framework for fully-unsupervised large-vocabulary speech\n  recognition", "comments": "15 pages, 6 figures, 8 tables", "journal-ref": "Comput. Speech Lang. 46 (2017) 154-174", "doi": "10.1016/j.csl.2017.04.008", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-resource speech technology is a growing research area that aims to\ndevelop methods for speech processing in the absence of transcriptions,\nlexicons, or language modelling text. Early term discovery systems focused on\nidentifying isolated recurring patterns in a corpus, while more recent\nfull-coverage systems attempt to completely segment and cluster the audio into\nword-like units---effectively performing unsupervised speech recognition. This\narticle presents the first attempt we are aware of to apply such a system to\nlarge-vocabulary multi-speaker data. Our system uses a Bayesian modelling\nframework with segmental word representations: each word segment is represented\nas a fixed-dimensional acoustic embedding obtained by mapping the sequence of\nfeature frames to a single embedding vector. We compare our system on English\nand Xitsonga datasets to state-of-the-art baselines, using a variety of\nmeasures including word error rate (obtained by mapping the unsupervised output\nto ground truth transcriptions). Very high word error rates are reported---in\nthe order of 70--80% for speaker-dependent and 80--95% for speaker-independent\nsystems---highlighting the difficulty of this task. Nevertheless, in terms of\ncluster quality and word segmentation metrics, we show that by imposing a\nconsistent top-down segmentation while also using bottom-up knowledge from\ndetected syllable boundaries, both single-speaker and multi-speaker versions of\nour system outperform a purely bottom-up single-speaker syllable-based\napproach. We also show that the discovered clusters can be made less speaker-\nand gender-specific by using an unsupervised autoencoder-like feature extractor\nto learn better frame-level features (prior to embedding). Our system's\ndiscovered clusters are still less pure than those of unsupervised term\ndiscovery systems, but provide far greater coverage.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 13:51:57 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 09:36:02 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kamper", "Herman", ""], ["Jansen", "Aren", ""], ["Goldwater", "Sharon", ""]]}, {"id": "1606.06991", "submitter": "Nawal Ould Amer", "authors": "Nawal Ould-Amer and Philippe Mulhem and Mathias Gery", "title": "Toward Word Embedding for Personalized Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents preliminary works on using Word Embedding (word2vec) for\nquery expansion in the context of Personalized Information Retrieval.\nTraditionally, word embeddings are learned on a general corpus, like Wikipedia.\nIn this work we try to personalize the word embeddings learning, by achieving\nthe learning on the user's profile. The word embeddings are then in the same\ncontext than the user interests. Our proposal is evaluated on the CLEF Social\nBook Search 2016 collection. The results obtained show that some efforts should\nbe made in the way to apply Word Embedding in the context of Personalized\nInformation Retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:53:29 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Ould-Amer", "Nawal", ""], ["Mulhem", "Philippe", ""], ["Gery", "Mathias", ""]]}, {"id": "1606.06996", "submitter": "Christian Bentz", "authors": "Christian Bentz and Dimitrios Alikaniotis", "title": "The word entropy of natural languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average uncertainty associated with words is an information-theoretic\nconcept at the heart of quantitative and computational linguistics. The entropy\nhas been established as a measure of this average uncertainty - also called\naverage information content. We here use parallel texts of 21 languages to\nestablish the number of tokens at which word entropies converge to stable\nvalues. These convergence points are then used to select texts from a massively\nparallel corpus, and to estimate word entropies across more than 1000\nlanguages. Our results help to establish quantitative language comparisons, to\nunderstand the performance of multilingual translation systems, and to\nnormalize semantic similarity measures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 16:00:52 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Bentz", "Christian", ""], ["Alikaniotis", "Dimitrios", ""]]}, {"id": "1606.07006", "submitter": "Xiao Yang", "authors": "Xiao Yang, Craig Macdonald, Iadh Ounis", "title": "Using Word Embeddings in Twitter Election Classification", "comments": "NeuIR Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings and convolutional neural networks (CNN) have attracted\nextensive attention in various classification tasks for Twitter, e.g. sentiment\nclassification. However, the effect of the configuration used to train and\ngenerate the word embeddings on the classification performance has not been\nstudied in the existing literature. In this paper, using a Twitter election\nclassification task that aims to detect election-related tweets, we investigate\nthe impact of the background dataset used to train the embedding models, the\ncontext window size and the dimensionality of word embeddings on the\nclassification performance. By comparing the classification results of two word\nembedding models, which are trained using different background corpora (e.g.\nWikipedia articles and Twitter microposts), we show that the background data\ntype should align with the Twitter classification dataset to achieve a better\nperformance. Moreover, by evaluating the results of word embeddings models\ntrained using various context window sizes and dimensionalities, we found that\nlarge context window and dimension sizes are preferable to improve the\nperformance. Our experimental results also show that using word embeddings and\nCNN leads to statistically significant improvements over various baselines such\nas random, SVM with TF-IDF and SVM with word embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 16:37:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 10:22:17 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 18:29:49 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Yang", "Xiao", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1606.07043", "submitter": "David Kale", "authors": "Kyle Reing, David C. Kale, Greg Ver Steeg, Aram Galstyan", "title": "Toward Interpretable Topic Discovery via Anchored Correlation\n  Explanation", "comments": "presented at 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications, New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many predictive tasks, such as diagnosing a patient based on their medical\nchart, are ultimately defined by the decisions of human experts. Unfortunately,\nencoding experts' knowledge is often time consuming and expensive. We propose a\nsimple way to use fuzzy and informal knowledge from experts to guide discovery\nof interpretable latent topics in text. The underlying intuition of our\napproach is that latent factors should be informative about both correlations\nin the data and a set of relevance variables specified by an expert.\nMathematically, this approach is a combination of the information bottleneck\nand Total Correlation Explanation (CorEx). We give a preliminary evaluation of\nAnchored CorEx, showing that it produces more coherent and interpretable topics\non two distinct corpora.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 19:00:38 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Reing", "Kyle", ""], ["Kale", "David C.", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1606.07046", "submitter": "Jayant Krishnamurthy", "authors": "Jayant Krishnamurthy and Oyvind Tafjord and Aniruddha Kembhavi", "title": "Semantic Parsing to Probabilistic Programs for Situated Question\n  Answering", "comments": "EMNLP 2016, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situated question answering is the problem of answering questions about an\nenvironment such as an image or diagram. This problem requires jointly\ninterpreting a question and an environment using background knowledge to select\nthe correct answer. We present Parsing to Probabilistic Programs (P3), a novel\nsituated question answering model that can use background knowledge and global\nfeatures of the question/environment interpretation while retaining efficient\napproximate inference. Our key insight is to treat semantic parses as\nprobabilistic programs that execute nondeterministically and whose possible\nexecutions represent environmental uncertainty. We evaluate our approach on a\nnew, publicly-released data set of 5000 science diagram questions,\noutperforming several competitive classical and neural baselines.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 19:19:29 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 00:37:43 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Krishnamurthy", "Jayant", ""], ["Tafjord", "Oyvind", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "1606.07056", "submitter": "Abhay Prakash", "authors": "Abhay Prakash, Chris Brockett, Puneet Agrawal", "title": "Emulating Human Conversations using Convolutional Neural Network-based\n  IR", "comments": "5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval,\n  July 21, 2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents (\"bots\") are beginning to be widely used in\nconversational interfaces. To design a system that is capable of emulating\nhuman-like interactions, a conversational layer that can serve as a fabric for\nchat-like interaction with the agent is needed. In this paper, we introduce a\nmodel that employs Information Retrieval by utilizing convolutional deep\nstructured semantic neural network-based features in the ranker to present\nhuman-like responses in ongoing conversation with a user. In conversations,\naccounting for context is critical to the retrieval model; we show that our\ncontext-sensitive approach using a Convolutional Deep Structured Semantic Model\n(cDSSM) with character trigrams significantly outperforms several conventional\nbaselines in terms of the relevance of responses retrieved.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 19:55:24 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Prakash", "Abhay", ""], ["Brockett", "Chris", ""], ["Agrawal", "Puneet", ""]]}, {"id": "1606.07103", "submitter": "Sai Praneeth Suggu", "authors": "Sai Praneeth Suggu, Kushwanth N. Goutham, Manoj K. Chinnakotla and\n  Manish Shrivastava", "title": "Deep Feature Fusion Network for Answer Quality Prediction in Community\n  Question Answering", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering (cQA) forums have become a popular medium for\nsoliciting direct answers to specific questions of users from experts or other\nexperienced users on a given topic. However, for a given question, users\nsometimes have to sift through a large number of low-quality or irrelevant\nanswers to find out the answer which satisfies their information need. To\nalleviate this, the problem of Answer Quality Prediction (AQP) aims to predict\nthe quality of an answer posted in response to a forum question. Current AQP\nsystems either learn models using - a) various hand-crafted features (HCF) or\nb) use deep learning (DL) techniques which automatically learn the required\nfeature representations.\n  In this paper, we propose a novel approach for AQP known as - \"Deep Feature\nFusion Network (DFFN)\" which leverages the advantages of both hand-crafted\nfeatures and deep learning based systems. Given a question-answer pair along\nwith its metadata, DFFN independently - a) learns deep features using a\nConvolutional Neural Network (CNN) and b) computes hand-crafted features using\nvarious external resources and then combines them using a deep neural network\ntrained to predict the final answer quality. DFFN achieves state-of-the-art\nperformance on the standard SemEval-2015 and SemEval-2016 benchmark datasets\nand outperforms baseline approaches which individually employ either HCF or DL\nbased techniques alone.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:58:08 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 05:54:51 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Suggu", "Sai Praneeth", ""], ["Goutham", "Kushwanth N.", ""], ["Chinnakotla", "Manoj K.", ""], ["Shrivastava", "Manish", ""]]}, {"id": "1606.07137", "submitter": "Abeed Sarker", "authors": "Abeed Sarker", "title": "Automated Extraction of Number of Subjects in Randomised Controlled\n  Trials", "comments": "unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple approach for automatically extracting the number of\nsubjects involved in randomised controlled trials (RCT). Our approach first\napplies a set of rule-based techniques to extract candidate study sizes from\nthe abstracts of the articles. Supervised classification is then performed over\nthe candidates with support vector machines, using a small set of lexical,\nstructural, and contextual features. With only a small annotated training set\nof 201 RCTs, we obtained an accuracy of 88\\%. We believe that this system will\naid complex medical text processing tasks such as summarisation and question\nanswering.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 23:35:59 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Sarker", "Abeed", ""]]}, {"id": "1606.07189", "submitter": "Mihajlo Grbovic", "authors": "Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan\n  Bhamidipati, Ananth Nagarajan", "title": "Gender and Interest Targeting for Sponsored Post Advertising at Tumblr", "comments": "10 pages, 9 figures, Proceedings of the 21th ACM SIGKDD International\n  Conference on Knowledge Discovery and Data Mining (KDD 2015), Sydney,\n  Australia", "journal-ref": "Proceedings of the 21th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD 2015), Sydney, Australia", "doi": "10.1145/2783258.2788616.", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the leading platforms for creative content, Tumblr offers\nadvertisers a unique way of creating brand identity. Advertisers can tell their\nstory through images, animation, text, music, video, and more, and promote that\ncontent by sponsoring it to appear as an advertisement in the streams of Tumblr\nusers. In this paper we present a framework that enabled one of the key\ntargeted advertising components for Tumblr, specifically gender and interest\ntargeting. We describe the main challenges involved in development of the\nframework, which include creating the ground truth for training gender\nprediction models, as well as mapping Tumblr content to an interest taxonomy.\nFor purposes of inferring user interests we propose a novel semi-supervised\nneural language model for categorization of Tumblr content (i.e., post tags and\npost keywords). The model was trained on a large-scale data set consisting of\n6.8 billion user posts, with very limited amount of categorized keywords, and\nwas shown to have superior performance over the bag-of-words model. We\nsuccessfully deployed gender and interest targeting capability in Yahoo\nproduction systems, delivering inference for users that cover more than 90% of\ndaily activities at Tumblr. Online performance results indicate advantages of\nthe proposed approach, where we observed 20% lift in user engagement with\nsponsored posts as compared to untargeted campaigns.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 05:03:03 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Grbovic", "Mihajlo", ""], ["Radosavljevic", "Vladan", ""], ["Djuric", "Nemanja", ""], ["Bhamidipati", "Narayan", ""], ["Nagarajan", "Ananth", ""]]}, {"id": "1606.07211", "submitter": "Gia-Hung Nguyen", "authors": "Gia-Hung Nguyen, Lynda Tamine, Laure Soulier, Nathalie Bricon-Souf", "title": "Toward a Deep Neural Approach for Knowledge-Based IR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of the semantic gap between a document and a\nquery within an ad-hoc information retrieval task. In this context, knowledge\nbases (KBs) have already been acknowledged as valuable means since they allow\nthe representation of explicit relations between entities. However, they do not\nnecessarily represent implicit relations that could be hidden in a corpora.\nThis latter issue is tackled by recent works dealing with deep representation\nlearn ing of texts. With this in mind, we argue that embedding KBs within deep\nneural architectures supporting documentquery matching would give rise to\nfine-grained latent representations of both words and their semantic relations.\nIn this paper, we review the main approaches of neural-based document ranking\nas well as those approaches for latent representation of entities and relations\nvia KBs. We then propose some avenues to incorporate KBs in deep neural\napproaches for document ranking. More particularly, this paper advocates that\nKBs can be used either to support enhanced latent representations of queries\nand documents based on both distributional and relational semantics or to serve\nas a semantic translator between their latent distributional representations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 07:21:28 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Nguyen", "Gia-Hung", ""], ["Tamine", "Lynda", ""], ["Soulier", "Laure", ""], ["Bricon-Souf", "Nathalie", ""]]}, {"id": "1606.07287", "submitter": "Andrea Esuli", "authors": "Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio Falchi, Alejandro\n  Moreo Fern\\'andez", "title": "Picture It In Your Mind: Generating High Level Visual Representations\n  From Textual Descriptions", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": "10.1007/s10791-017-9318-6", "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of image search when the query is a short\ntextual description of the image the user is looking for. We choose to\nimplement the actual search process as a similarity search in a visual feature\nspace, by learning to translate a textual query into a visual representation.\nSearching in the visual feature space has the advantage that any update to the\ntranslation model does not require to reprocess the, typically huge, image\ncollection on which the search is performed. We propose Text2Vis, a neural\nnetwork that generates a visual representation, in the visual feature space of\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\noptimizes two loss functions, using a stochastic loss-selection method. A\nvisual-focused loss is aimed at learning the actual text-to-visual feature\nmapping, while a text-focused loss is aimed at modeling the higher-level\nsemantic concepts expressed in language and countering the overfit on\nnon-relevant visual components of the visual loss. We report preliminary\nresults on the MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:25:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Carrara", "Fabio", ""], ["Esuli", "Andrea", ""], ["Fagni", "Tiziano", ""], ["Falchi", "Fabrizio", ""], ["Fern\u00e1ndez", "Alejandro Moreo", ""]]}, {"id": "1606.07298", "submitter": "Wojciech Samek", "authors": "Leila Arras and Franziska Horn and Gr\\'egoire Montavon and\n  Klaus-Robert M\\\"uller and Wojciech Samek", "title": "Explaining Predictions of Non-Linear Classifiers in NLP", "comments": "7 pages, 3 figures, Paper accepted for 1st Workshop on Representation\n  Learning for NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:53:31 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1606.07356", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh", "title": "Analyzing the Behavior of Visual Question Answering Models", "comments": "13 pages, 20 figures; To appear in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a number of deep-learning based models have been proposed for the\ntask of Visual Question Answering (VQA). The performance of most models is\nclustered around 60-70%. In this paper we propose systematic methods to analyze\nthe behavior of these models as a first step towards recognizing their\nstrengths and weaknesses, and identifying the most fruitful directions for\nprogress. We analyze two models, one each from two major classes of VQA models\n-- with-attention and without-attention and show the similarities and\ndifferences in the behavior of these models. We also analyze the winning entry\nof the VQA Challenge 2016.\n  Our behavior analysis reveals that despite recent progress, today's VQA\nmodels are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump\nto conclusions\" (converge on a predicted answer after 'listening' to just half\nthe question), and are \"stubborn\" (do not change their answers across images).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 16:05:16 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 19:56:22 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1606.07461", "submitter": "Alexander M. Rush", "authors": "Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M.\n  Rush", "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in\n  Recurrent Neural Networks", "comments": "InfoVis 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks, and in particular long short-term memory (LSTM)\nnetworks, are a remarkably effective tool for sequence modeling that learn a\ndense black-box hidden representation of their sequential input. Researchers\ninterested in better understanding these models have studied the changes in\nhidden state representations over time and noticed some interpretable patterns\nbut also significant noise. In this work, we present LSTMVIS, a visual analysis\ntool for recurrent neural networks with a focus on understanding these hidden\nstate dynamics. The tool allows users to select a hypothesis input range to\nfocus on local state changes, to match these states changes to similar patterns\nin a large data set, and to align these results with structural annotations\nfrom their domain. We show several use cases of the tool for analyzing specific\nhidden state properties on dataset containing nesting, phrase structure, and\nchord progressions, and demonstrate how the tool can be used to isolate\npatterns for further statistical analysis. We characterize the domain, the\ndifferent stakeholders, and their goals and tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 20:20:39 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:11:54 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Strobelt", "Hendrik", ""], ["Gehrmann", "Sebastian", ""], ["Pfister", "Hanspeter", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1606.07470", "submitter": "Shankar Kumar", "authors": "Babak Damavandi, Shankar Kumar, Noam Shazeer and Antoine Bruguier", "title": "NN-grams: Unifying neural network and n-gram language models for Speech\n  Recognition", "comments": "To be published in the proceedings of INTERSPEECH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NN-grams, a novel, hybrid language model integrating n-grams and\nneural networks (NN) for speech recognition. The model takes as input both word\nhistories as well as n-gram counts. Thus, it combines the memorization capacity\nand scalability of an n-gram model with the generalization ability of neural\nnetworks. We report experiments where the model is trained on 26B words.\nNN-grams are efficient at run-time since they do not include an output soft-max\nlayer. The model is trained using noise contrastive estimation (NCE), an\napproach that transforms the estimation problem of neural networks into one of\nbinary classification between data samples and noise samples. We present\nresults with noise samples derived from either an n-gram distribution or from\nspeech recognition lattices. NN-grams outperforms an n-gram model on an Italian\nspeech recognition dictation task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 20:37:06 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Damavandi", "Babak", ""], ["Kumar", "Shankar", ""], ["Shazeer", "Noam", ""], ["Bruguier", "Antoine", ""]]}, {"id": "1606.07481", "submitter": "Jind\\v{r}ich Libovick\\'y", "authors": "Jind\\v{r}ich Libovick\\'y, Jind\\v{r}ich Helcl, Marek Tlust\\'y, Pavel\n  Pecina, Ond\\v{r}ej Bojar", "title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation\n  Tasks", "comments": "Accepted to the First Conference of Machine Translation (WMT16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence to sequence learning recently became a very promising\nparadigm in machine translation, achieving competitive results with statistical\nphrase-based systems. In this system description paper, we attempt to utilize\nseveral recently published methods used for neural sequential learning in order\nto build systems for WMT 2016 shared tasks of Automatic Post-Editing and\nMultimodal Machine Translation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:23:29 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Helcl", "Jind\u0159ich", ""], ["Tlust\u00fd", "Marek", ""], ["Pecina", "Pavel", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1606.07493", "submitter": "Arjun Chandrasekaran", "authors": "Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, Mohit\n  Bansal", "title": "Sort Story: Sorting Jumbled Images and Captions into Stories", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal common sense has applications in AI tasks such as QA, multi-document\nsummarization, and human-AI communication. We propose the task of sequencing --\ngiven a jumbled set of aligned image-caption pairs that belong to a story, the\ntask is to sort them such that the output sequence forms a coherent story. We\npresent multiple approaches, via unary (position) and pairwise (order)\npredictions, and their ensemble-based combinations, achieving strong results on\nthis task. We use both text-based and image-based features, which depict\ncomplementary improvements. Using qualitative examples, we demonstrate that our\nmodels have learnt interesting aspects of temporal common sense.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 21:54:44 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 05:26:43 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 19:56:36 GMT"}, {"version": "v4", "created": "Sat, 24 Sep 2016 00:37:27 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 18:48:13 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Agrawal", "Harsh", ""], ["Chandrasekaran", "Arjun", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Bansal", "Mohit", ""]]}, {"id": "1606.07496", "submitter": "Roberto Camacho Barranco", "authors": "Roberto Camacho Barranco (1), Laura M. Rodriguez (1), Rebecca Urbina\n  (1), and M. Shahriar Hossain (1) ((1) The University of Texas at El Paso)", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "comments": "10 pages, 11 figures, \"for associated results, see\n  http://http://auto-captioning.herokuapp.com/\" \"submitted to DLRS 2016\n  workshop\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 22:04:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Barranco", "Roberto Camacho", "", "The University of Texas at El Paso"], ["Rodriguez", "Laura M.", "", "The University of Texas at El Paso"], ["Urbina", "Rebecca", "", "The University of Texas at El Paso"], ["Hossain", "M. Shahriar", "", "The University of Texas at El Paso"]]}, {"id": "1606.07545", "submitter": "Camille Jandot", "authors": "Camille Jandot, Patrice Simard, Max Chickering, David Grangier, Jina\n  Suh", "title": "Interactive Semantic Featuring for Text Classification", "comments": "presented at 2016 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In text classification, dictionaries can be used to define\nhuman-comprehensible features. We propose an improvement to dictionary features\ncalled smoothed dictionary features. These features recognize document contexts\ninstead of n-grams. We describe a principled methodology to solicit dictionary\nfeatures from a teacher, and present results showing that models built using\nthese human-comprehensible features are competitive with models trained with\nBag of Words features.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 02:28:24 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Jandot", "Camille", ""], ["Simard", "Patrice", ""], ["Chickering", "Max", ""], ["Grangier", "David", ""], ["Suh", "Jina", ""]]}, {"id": "1606.07548", "submitter": "Lu Wang", "authors": "Lu Wang and Hema Raghavan and Vittorio Castelli and Radu Florian and\n  Claire Cardie", "title": "A Sentence Compression Based Framework to Query-Focused Multi-Document\n  Summarization", "comments": "ACL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using sentence compression techniques to\nfacilitate query-focused multi-document summarization. We present a\nsentence-compression-based framework for the task, and design a series of\nlearning-based compression models built on parse trees. An innovative beam\nsearch decoder is proposed to efficiently find highly probable compressions.\nUnder this framework, we show how to integrate various indicative metrics such\nas linguistic motivation and query relevance into the compression process by\nderiving a novel formulation of a compression scoring function. Our best model\nachieves statistically significant improvement over the state-of-the-art\nsystems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2\nrespectively) for the DUC 2006 and 2007 summarization task.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 02:57:04 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Wang", "Lu", ""], ["Raghavan", "Hema", ""], ["Castelli", "Vittorio", ""], ["Florian", "Radu", ""], ["Cardie", "Claire", ""]]}, {"id": "1606.07565", "submitter": "Daniel Cohen", "authors": "Daniel Cohen, Qingyao Ai, W. Bruce Croft", "title": "Adaptability of Neural Networks on Varying Granularity IR Tasks", "comments": "4 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval,\n  July 21, 2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in Information Retrieval (IR) using Deep Learning models has\nyielded state of the art results on a variety of IR tasks. Deep neural networks\n(DNN) are capable of learning ideal representations of data during the training\nprocess, removing the need for independently extracting features. However, the\nstructures of these DNNs are often tailored to perform on specific datasets. In\naddition, IR tasks deal with text at varying levels of granularity from single\nfactoids to documents containing thousands of words. In this paper, we examine\nthe role of the granularity on the performance of common state of the art DNN\nstructures in IR.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 04:40:48 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Cohen", "Daniel", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1606.07572", "submitter": "S Subhashree", "authors": "Subhashree S and P Sreenivasa Kumar", "title": "Enriching Linked Datasets with New Object Properties", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several RDF knowledge bases are available through the LOD\ninitiative, the ontology schema of such linked datasets is not very rich. In\nparticular, they lack object properties. The problem of finding new object\nproperties (and their instances) between any two given classes has not been\ninvestigated in detail in the context of Linked Data. In this paper, we present\nDART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an\nunsupervised solution to enrich the LOD cloud with new object properties\nbetween two given classes. DART exploits contextual similarity to identify text\npatterns from the web corpus that can potentially represent relations between\nindividuals. These text patterns are then clustered by means of paraphrase\ndetection to capture the object properties between the two given LOD classes.\nDART also performs fully automated mapping of the discovered relations to the\nproperties in the linked dataset. This serves many purposes such as\nidentification of completely new relations, elimination of irrelevant\nrelations, and generation of prospective property axioms. We have empirically\nevaluated our approach on several pairs of classes and found that the system\ncan indeed be used for enriching the linked datasets with new object properties\nand their instances. We compared DART with newOntExt system which is an\noffshoot of the NELL (Never-Ending Language Learning) effort. Our experiments\nreveal that DART gives better results than newOntExt with respect to both the\ncorrectness, as well as the number of relations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 06:00:42 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 04:49:49 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 11:12:42 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["S", "Subhashree", ""], ["Kumar", "P Sreenivasa", ""]]}, {"id": "1606.07601", "submitter": "Kebin Peng", "authors": "KeBin Peng", "title": "Evaluation method of word embedding by roots and affixes", "comments": "7 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding has been shown to be remarkably effective in a lot of Natural\nLanguage Processing tasks. However, existing models still have a couple of\nlimitations in interpreting the dimensions of word vector. In this paper, we\nprovide a new approach---roots and affixes model(RAAM)---to interpret it from\nthe intrinsic structures of natural language. Also it can be used as an\nevaluation measure of the quality of word embedding. We introduce the\ninformation entropy into our model and divide the dimensions into two\ncategories, just like roots and affixes in lexical semantics. Then considering\neach category as a whole rather than individually. We experimented with English\nWikipedia corpus. Our result show that there is a negative linear relation\nbetween the two attributes and a high positive correlation between our model\nand downstream semantic evaluation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 08:35:01 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Peng", "KeBin", ""]]}, {"id": "1606.07711", "submitter": "Rocco Tripodi", "authors": "Rocco Tripodi and Marcello Pelillo", "title": "A Game-Theoretic Approach to Word Sense Disambiguation", "comments": "To be published in Computational Linguistics", "journal-ref": null, "doi": "10.1162/COLI_a_00274", "report-no": null, "categories": "cs.AI cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model for word sense disambiguation formulated in\nterms of evolutionary game theory, where each word to be disambiguated is\nrepresented as a node on a graph whose edges represent word relations and\nsenses are represented as classes. The words simultaneously update their class\nmembership preferences according to the senses that neighboring words are\nlikely to choose. We use distributional information to weigh the influence that\neach word has on the decisions of the others and semantic similarity\ninformation to measure the strength of compatibility among the choices. With\nthis information we can formulate the word sense disambiguation problem as a\nconstraint satisfaction problem and solve it using tools derived from game\ntheory, maintaining the textual coherence. The model is based on two ideas:\nsimilar words should be assigned to similar classes and the meaning of a word\ndoes not depend on all the words in a text but just on some of them. The paper\nprovides an in-depth motivation of the idea of modeling the word sense\ndisambiguation problem in terms of game theory, which is illustrated by an\nexample. The conclusion presents an extensive analysis on the combination of\nsimilarity measures to use in the framework and a comparison with\nstate-of-the-art systems. The results show that our model outperforms\nstate-of-the-art algorithms and can be applied to different tasks and in\ndifferent scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 14:45:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 05:31:31 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 12:19:13 GMT"}, {"version": "v4", "created": "Mon, 4 Jul 2016 13:19:29 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Tripodi", "Rocco", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1606.07736", "submitter": "Tal Linzen", "authors": "Tal Linzen", "title": "Issues in evaluating semantic spaces using word analogies", "comments": "6 pages; The First Workshop on Evaluating Vector Space\n  Representations for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The offset method for solving word analogies has become a standard evaluation\ntool for vector-space semantic models: it is considered desirable for a space\nto represent semantic relations as consistent vector offsets. We show that the\nmethod's reliance on cosine similarity conflates offset consistency with\nlargely irrelevant neighborhood structure, and propose simple baselines that\nshould be used to improve the utility of the method in vector space evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 15:59:14 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Linzen", "Tal", ""]]}, {"id": "1606.07770", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond\n  Mooney, Trevor Darrell, Kate Saenko", "title": "Captioning Images with Diverse Objects", "comments": "CVPR 2017 Camera ready version. 17 pages (8 + 9 supplement), 12\n  figures, 8 tables. Includes project page\n  http://vsubhashini.github.io/noc.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent captioning models are limited in their ability to scale and describe\nconcepts unseen in paired image-text corpora. We propose the Novel Object\nCaptioner (NOC), a deep visual semantic captioning model that can describe a\nlarge number of object categories not present in existing image-caption\ndatasets. Our model takes advantage of external sources -- labeled images from\nobject recognition datasets, and semantic knowledge extracted from unannotated\ntext. We propose minimizing a joint objective which can learn from these\ndiverse data sources and leverage distributional semantic embeddings, enabling\nthe model to generalize and describe novel objects outside of image-caption\ndatasets. We demonstrate that our model exploits semantic information to\ngenerate captions for hundreds of object categories in the ImageNet object\nrecognition dataset that are not observed in MSCOCO image-caption training\ndata, as well as many categories that are observed very rarely. Both automatic\nevaluations and human judgements show that our model considerably outperforms\nprior work in being able to describe many more categories of objects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 17:53:45 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 20:54:17 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 18:06:27 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Hendricks", "Lisa Anne", ""], ["Rohrbach", "Marcus", ""], ["Mooney", "Raymond", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1606.07772", "submitter": "Andrew Reagan", "authors": "Andrew J. Reagan and Lewis Mitchell and Dilan Kiley and Christopher M.\n  Danforth and Peter Sheridan Dodds", "title": "The emotional arcs of stories are dominated by six basic shapes", "comments": "Manuscript: 10 pages, 7 figures. Supplementary: 81 pages, 29 figures", "journal-ref": null, "doi": "10.1140/epjds/s13688-016-0093-1", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in computing power, natural language processing, and digitization of\ntext now make it possible to study a culture's evolution through its texts\nusing a \"big data\" lens. Our ability to communicate relies in part upon a\nshared emotional experience, with stories often following distinct emotional\ntrajectories and forming patterns that are meaningful to us. Here, by\nclassifying the emotional arcs for a filtered subset of 1,327 stories from\nProject Gutenberg's fiction collection, we find a set of six core emotional\narcs which form the essential building blocks of complex emotional\ntrajectories. We strengthen our findings by separately applying Matrix\ndecomposition, supervised learning, and unsupervised learning. For each of\nthese six core emotional arcs, we examine the closest characteristic stories in\npublication today and find that particular emotional arcs enjoy greater\nsuccess, as measured by downloads.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 18:00:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 22:45:15 GMT"}, {"version": "v3", "created": "Mon, 26 Sep 2016 03:17:21 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Reagan", "Andrew J.", ""], ["Mitchell", "Lewis", ""], ["Kiley", "Dilan", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1606.07783", "submitter": "Ngoc Thang Vu", "authors": "Ngoc Thang Vu", "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken\n  Language Understanding", "comments": "Accepted at Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the usage of convolutional neural networks (CNNs) for the slot\nfilling task in spoken language understanding. We propose a novel CNN\narchitecture for sequence labeling which takes into account the previous\ncontext words with preserved order information and pays special attention to\nthe current word with its surrounding context. Moreover, it combines the\ninformation from the past and the future words for classification. Our proposed\nCNN architecture outperforms even the previously best ensembling recurrent\nneural network model and achieves state-of-the-art results with an F1-score of\n95.61% on the ATIS benchmark dataset without using any additional linguistic\nknowledge and resources.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 18:35:56 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Vu", "Ngoc Thang", ""]]}, {"id": "1606.07822", "submitter": "Jeroen Vuurens", "authors": "Jeroen B.P. Vuurens, Carsten Eickhoff, Arjen P. de Vries", "title": "Efficient Parallel Learning of Word2Vec", "comments": "ICML 2016 Machine Learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction, Word2Vec and its variants are widely used to learn\nsemantics-preserving representations of words or entities in an embedding\nspace, which can be used to produce state-of-art results for various Natural\nLanguage Processing tasks. Existing implementations aim to learn efficiently by\nrunning multiple threads in parallel while operating on a single model in\nshared memory, ignoring incidental memory update collisions. We show that these\ncollisions can degrade the efficiency of parallel learning, and propose a\nstraightforward caching strategy that improves the efficiency by a factor of 4.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 20:05:53 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Vuurens", "Jeroen B. P.", ""], ["Eickhoff", "Carsten", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1606.07829", "submitter": "Lu Wang", "authors": "Lu Wang and Claire Cardie", "title": "Unsupervised Topic Modeling Approaches to Decision Summarization in\n  Spoken Meetings", "comments": "SIGDIAL 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a token-level decision summarization framework that utilizes the\nlatent topic structures of utterances to identify \"summary-worthy\" words.\nConcretely, a series of unsupervised topic models is explored and experimental\nresults show that fine-grained topic models, which discover topics at the\nutterance-level rather than the document-level, can better identify the gist of\nthe decision-making process. Moreover, our proposed token-level summarization\napproach, which is able to remove redundancies within utterances, outperforms\nexisting utterance ranking based summarization methods. Finally, context\ninformation is also investigated to add additional relevant information to the\nsummary.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 20:17:44 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Wang", "Lu", ""], ["Cardie", "Claire", ""]]}, {"id": "1606.07839", "submitter": "Stefan Lee", "authors": "Stefan Lee, Senthil Purushwalkam, Michael Cogswell, Viresh Ranjan,\n  David Crandall, and Dhruv Batra", "title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical perception systems exist within larger processes that include\ninteractions with users or additional components capable of evaluating the\nquality of predicted solutions. In these contexts, it is beneficial to provide\nthese oracle mechanisms with multiple highly likely hypotheses rather than a\nsingle prediction. In this work, we pose the task of producing multiple outputs\nas a learning problem over an ensemble of deep networks -- introducing a novel\nstochastic gradient descent based approach to minimize the loss with respect to\nan oracle. Our method is simple to implement, agnostic to both architecture and\nloss function, and parameter-free. Our approach achieves lower oracle error\ncompared to existing methods on a wide range of tasks and deep architectures.\nWe also show qualitatively that the diverse solutions produced often provide\ninterpretable representations of task ambiguity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 21:48:55 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 20:44:55 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 17:12:00 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Lee", "Stefan", ""], ["Purushwalkam", "Senthil", ""], ["Cogswell", "Michael", ""], ["Ranjan", "Viresh", ""], ["Crandall", "David", ""], ["Batra", "Dhruv", ""]]}, {"id": "1606.07849", "submitter": "Lu Wang", "authors": "Lu Wang and Claire Cardie", "title": "Focused Meeting Summarization via Unsupervised Relation Extraction", "comments": "SIGDIAL 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised framework for focused meeting summarization\nthat views the problem as an instance of relation extraction. We adapt an\nexisting in-domain relation learner (Chen et al., 2011) by exploiting a set of\ntask-specific constraints and features. We evaluate the approach on a decision\nsummarization task and show that it outperforms unsupervised utterance-level\nextractive summarization baselines as well as an existing generic\nrelation-extraction-based summarization method. Moreover, our approach produces\nsummaries competitive with those generated by supervised methods in terms of\nthe standard ROUGE score.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 22:49:56 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Wang", "Lu", ""], ["Cardie", "Claire", ""]]}, {"id": "1606.07901", "submitter": "Yadollah Yaghoobzadeh", "authors": "Yadollah Yaghoobzadeh, Hinrich Sch\\\"utze", "title": "Corpus-level Fine-grained Entity Typing Using Contextual Information", "comments": "Accepted at EMNLP2015, Proceedings of the 2015 Conference on\n  Empirical Methods in Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of corpus-level entity typing, i.e.,\ninferring from a large corpus that an entity is a member of a class such as\n\"food\" or \"artist\". The application of entity typing we are interested in is\nknowledge base completion, specifically, to learn which classes an entity is a\nmember of. We propose FIGMENT to tackle this problem. FIGMENT is\nembedding-based and combines (i) a global model that scores based on aggregated\ncontextual information of an entity and (ii) a context model that first scores\nthe individual occurrences of an entity and then aggregates the scores. In our\nevaluation, FIGMENT strongly outperforms an approach to entity typing that\nrelies on relations obtained by an open information extraction system.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 12:22:05 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Yaghoobzadeh", "Yadollah", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1606.07902", "submitter": "Yadollah Yaghoobzadeh", "authors": "Yadollah Yaghoobzadeh, Hinrich Sch\\\"utze", "title": "Intrinsic Subspace Evaluation of Word Embedding Representations", "comments": "Long paper accepted in ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new methodology for intrinsic evaluation of word\nrepresentations. Specifically, we identify four fundamental criteria based on\nthe characteristics of natural language that pose difficulties to NLP systems;\nand develop tests that directly show whether or not representations contain the\nsubspaces necessary to satisfy these criteria. Current intrinsic evaluations\nare mostly based on the overall similarity or full-space similarity of words\nand thus view vector representations as points. We show the limits of these\npoint-based intrinsic evaluations. We apply our evaluation methodology to the\ncomparison of a count vector model and several neural network models and\ndemonstrate important properties of these models.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 12:27:17 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Yaghoobzadeh", "Yadollah", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1606.07947", "submitter": "Yoon Kim", "authors": "Yoon Kim, Alexander M. Rush", "title": "Sequence-Level Knowledge Distillation", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 18:16:39 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 17:24:18 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 15:02:54 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 01:17:12 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Kim", "Yoon", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1606.07950", "submitter": "Diego Amancio Dr.", "authors": "Edilson A. Correa Jr. and Alneu de Andrade Lopes and Diego R. Amancio", "title": "Word sense disambiguation: a complex network approach", "comments": null, "journal-ref": "Information Sciences 442-443, pages 103-113 (2018)", "doi": "10.1016/j.ins.2018.02.047", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, concepts and methods of complex networks have been employed\nto tackle the word sense disambiguation (WSD) task by representing words as\nnodes, which are connected if they are semantically similar. Despite the\nincreasingly number of studies carried out with such models, most of them use\nnetworks just to represent the data, while the pattern recognition performed on\nthe attribute space is performed using traditional learning techniques. In\nother words, the structural relationship between words have not been explicitly\nused in the pattern recognition process. In addition, only a few investigations\nhave probed the suitability of representations based on bipartite networks and\ngraphs (bigraphs) for the problem, as many approaches consider all possible\nlinks between words. In this context, we assess the relevance of a bipartite\nnetwork model representing both feature words (i.e. the words characterizing\nthe context) and target (ambiguous) words to solve ambiguities in written\ntexts. Here, we focus on the semantical relationships between these two type of\nwords, disregarding the relationships between feature words. In special, the\nproposed method not only serves to represent texts as graphs, but also\nconstructs a structure on which the discrimination of senses is accomplished.\nOur results revealed that the proposed learning algorithm in such bipartite\nnetworks provides excellent results mostly when topical features are employed\nto characterize the context. Surprisingly, our method even outperformed the\nsupport vector machine algorithm in particular cases, with the advantage of\nbeing robust even if a small training dataset is available. Taken together, the\nresults obtained here show that the proposed representation/classification\nmethod might be useful to improve the semantical characterization of written\ntexts.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 19:08:19 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 14:48:30 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Correa", "Edilson A.", "Jr."], ["Lopes", "Alneu de Andrade", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1606.07953", "submitter": "Abhyuday Jagannatha", "authors": "Abhyuday Jagannatha, Hong Yu", "title": "Bidirectional Recurrent Neural Networks for Medical Event Detection in\n  Electronic Health Records", "comments": "In proceedings of NAACL HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence labeling for extraction of medical events and their attributes from\nunstructured text in Electronic Health Record (EHR) notes is a key step towards\nsemantic understanding of EHRs. It has important applications in health\ninformatics including pharmacovigilance and drug surveillance. The state of the\nart supervised machine learning models in this domain are based on Conditional\nRandom Fields (CRFs) with features calculated from fixed context windows. In\nthis application, we explored various recurrent neural network frameworks and\nshow that they significantly outperformed the CRF models.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 19:46:28 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:10:38 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Jagannatha", "Abhyuday", ""], ["Yu", "Hong", ""]]}, {"id": "1606.07955", "submitter": "Joseph Corneli", "authors": "Daniel Winterstein and Joseph Corneli", "title": "X575: writing rengas with web services", "comments": "4 pages; submitted to CC-NLG - Computational Creativity in Natural\n  Language Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our software system simulates the classical collaborative Japanese poetry\nform, renga, made of linked haikus. We used NLP methods wrapped up as web\nservices. Our experiments were only a partial success, since results fail to\nsatisfy classical constraints. To gather ideas for future work, we examine\nrelated research in semiotics, linguistics, and computing.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 20:04:42 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Winterstein", "Daniel", ""], ["Corneli", "Joseph", ""]]}, {"id": "1606.07965", "submitter": "Lu Wang", "authors": "Lu Wang and Claire Cardie", "title": "Summarizing Decisions in Spoken Meetings", "comments": "ACL Workshop on Automatic Summarization for Different Genres, Media,\n  and Languages, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of summarizing decisions in spoken meetings:\nour goal is to produce a concise {\\it decision abstract} for each meeting\ndecision. We explore and compare token-level and dialogue act-level automatic\nsummarization methods using both unsupervised and supervised learning\nframeworks. In the supervised summarization setting, and given true clusterings\nof decision-related utterances, we find that token-level summaries that employ\ndiscourse context can approach an upper bound for decision abstracts derived\ndirectly from dialogue acts. In the unsupervised summarization setting,we find\nthat summaries based on unsupervised partitioning of decision-related\nutterances perform comparably to those based on partitions generated using\nsupervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23\nusing SVMs).\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 20:45:14 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Wang", "Lu", ""], ["Cardie", "Claire", ""]]}, {"id": "1606.07967", "submitter": "Lu Wang", "authors": "Lu Wang and Larry Heck and Dilek Hakkani-Tur", "title": "Leveraging Semantic Web Search and Browse Sessions for Multi-Turn Spoken\n  Dialog Systems", "comments": "ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training statistical dialog models in spoken dialog systems (SDS) requires\nlarge amounts of annotated data. The lack of scalable methods for data mining\nand annotation poses a significant hurdle for state-of-the-art statistical\ndialog managers. This paper presents an approach that directly leverage\nbillions of web search and browse sessions to overcome this hurdle. The key\ninsight is that task completion through web search and browse sessions is (a)\npredictable and (b) generalizes to spoken dialog task completion. The new\nmethod automatically mines behavioral search and browse patterns from web logs\nand translates them into spoken dialog models. We experiment with naturally\noccurring spoken dialogs and large scale web logs. Our session-based models\noutperform the state-of-the-art method for entity extraction task in SDS. We\nalso achieve better performance for both entity and relation extraction on web\nsearch queries when compared with nontrivial baselines.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 20:54:49 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Wang", "Lu", ""], ["Heck", "Larry", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "1606.07993", "submitter": "Feifan Liu", "authors": "Feifan Liu, Jinying Chen, Abhyuday Jagannatha, Hong Yu", "title": "Learning for Biomedical Information Extraction: Methodological Review of\n  Recent Advances", "comments": "The version under review of Briefings in Bioinformatics(submitted on\n  06/16/2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical information extraction (BioIE) is important to many applications,\nincluding clinical decision support, integrative biology, and\npharmacovigilance, and therefore it has been an active research. Unlike\nexisting reviews covering a holistic view on BioIE, this review focuses on\nmainly recent advances in learning based approaches, by systematically\nsummarizing them into different aspects of methodological development. In\naddition, we dive into open information extraction and deep learning, two\nemerging and influential techniques and envision next generation of BioIE.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 04:39:39 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Liu", "Feifan", ""], ["Chen", "Jinying", ""], ["Jagannatha", "Abhyuday", ""], ["Yu", "Hong", ""]]}, {"id": "1606.08003", "submitter": "Guy Edward Toh Emerson", "authors": "Guy Emerson, Ann Copestake", "title": "Functional Distributional Semantics", "comments": "Published at Representation Learning for NLP workshop at ACL 2016,\n  https://sites.google.com/site/repl4nlp2016/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector space models have become popular in distributional semantics, despite\nthe challenges they face in capturing various semantic phenomena. We propose a\nnovel probabilistic framework which draws on both formal semantics and recent\nadvances in machine learning. In particular, we separate predicates from the\nentities they refer to, allowing us to perform Bayesian inference based on\nlogical forms. We describe an implementation of this framework using a\ncombination of Restricted Boltzmann Machines and feedforward neural networks.\nFinally, we demonstrate the feasibility of this approach by training it on a\nparsed corpus and evaluating it on established similarity datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 07:44:08 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Emerson", "Guy", ""], ["Copestake", "Ann", ""]]}, {"id": "1606.08089", "submitter": "Gustave Hahn-Powell", "authors": "Gus Hahn-Powell, Dane Bell, Marco A. Valenzuela-Esc\\'arcega, and Mihai\n  Surdeanu", "title": "This before That: Causal Precedence in the Biomedical Domain", "comments": "To appear in the proceedings of the 2016 Workshop on Biomedical\n  Natural Language Processing (BioNLP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal precedence between biochemical interactions is crucial in the\nbiomedical domain, because it transforms collections of individual\ninteractions, e.g., bindings and phosphorylations, into the causal mechanisms\nneeded to inform meaningful search and inference. Here, we analyze causal\nprecedence in the biomedical domain as distinct from open-domain, temporal\nprecedence. First, we describe a novel, hand-annotated text corpus of causal\nprecedence in the biomedical domain. Second, we use this corpus to investigate\na battery of models of precedence, covering rule-based, feature-based, and\nlatent representation models. The highest-performing individual model achieved\na micro F1 of 43 points, approaching the best performers on the simpler\ntemporal-only precedence tasks. Feature-based and latent representation models\neach outperform the rule-based models, but their performance is complementary\nto one another. We apply a sieve-based architecture to capitalize on this lack\nof overlap, achieving a micro F1 score of 46 points.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2016 21:55:40 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Hahn-Powell", "Gus", ""], ["Bell", "Dane", ""], ["Valenzuela-Esc\u00e1rcega", "Marco A.", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1606.08140", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu and Mark Johnson", "title": "STransE: a novel embedding model of entities and relationships in\n  knowledge bases", "comments": "V1: In Proceedings of the 2016 Conference of the North American\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies, NAACL HLT 2016. V2: Corrected citation to (Krompa{\\ss} et al.,\n  2015). V3: A revised version of our NAACL-HLT 2016 paper with additional\n  experimental results and latest related work", "journal-ref": null, "doi": "10.18653/v1/N16-1054", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases of real-world facts about entities and their relationships\nare useful resources for a variety of natural language processing tasks.\nHowever, because knowledge bases are typically incomplete, it is useful to be\nable to perform link prediction or knowledge base completion, i.e., predict\nwhether a relationship not in the knowledge base is likely to be true. This\npaper combines insights from several previous link prediction models into a new\nembedding model STransE that represents each entity as a low-dimensional\nvector, and each relation by two matrices and a translation vector. STransE is\na simple combination of the SE and TransE models, but it obtains better link\nprediction performance on two benchmark datasets than previous embedding\nmodels. Thus, STransE can serve as a new baseline for the more complex models\nin the link prediction task.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 06:50:10 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 16:24:49 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 16:57:40 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Sirts", "Kairit", ""], ["Qu", "Lizhen", ""], ["Johnson", "Mark", ""]]}, {"id": "1606.08207", "submitter": "Sanja \\v{S}\\'cepanovi\\'c", "authors": "Sanja \\v{S}\\'cepanovi\\'c, Igor Mishkovski, Bruno Gon\\c{c}alves, Nguyen\n  Trung Hieu, Pan Hui", "title": "Semantic homophily in online communication: evidence from Twitter", "comments": "19 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are observed to assortatively connect on a set of traits. This\nphenomenon, termed assortative mixing or sometimes homophily, can be quantified\nthrough assortativity coefficient in social networks. Uncovering the exact\ncauses of strong assortative mixing found in social networks has been a\nresearch challenge. Among the main suggested causes from sociology are the\ntendency of similar individuals to connect (often itself referred as homophily)\nand the social influence among already connected individuals. An important\nquestion to researchers and in practice can be tackled, as we present here:\nunderstanding the exact mechanisms of interplay between these tendencies and\nthe underlying social network structure. Namely, in addition to the mentioned\nassortativity coefficient, there are several other static and temporal network\nproperties and substructures that can be linked to the tendencies of homophily\nand social influence in the social network and we herein investigate those.\nConcretely, we tackle a computer-mediated \\textit{communication network} (based\non Twitter mentions) and a particular type of assortative mixing that can be\ninferred from the semantic features of communication content that we term\n\\textit{semantic homophily}. Our work, to the best of our knowledge, is the\nfirst to offer an in-depth analysis on semantic homophily in a communication\nnetwork and the interplay between them. We quantify diverse levels of semantic\nhomophily, identify the semantic aspects that are the drivers of observed\nhomophily, show insights in its temporal evolution and finally, we present its\nintricate interplay with the communication network on Twitter. By analyzing\nthese mechanisms we increase understanding on what are the semantic aspects\nthat shape and how they shape the human computer-mediated communication.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 11:04:41 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 16:47:22 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 16:22:29 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["\u0160\u0107epanovi\u0107", "Sanja", ""], ["Mishkovski", "Igor", ""], ["Gon\u00e7alves", "Bruno", ""], ["Hieu", "Nguyen Trung", ""], ["Hui", "Pan", ""]]}, {"id": "1606.08270", "submitter": "Naomi Saphra", "authors": "Naomi Saphra and Adam Lopez", "title": "Evaluating Informal-Domain Word Representations With UrbanDictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Existing corpora for intrinsic evaluation are not targeted towards tasks in\ninformal domains such as Twitter or news comment forums. We want to test\nwhether a representation of informal words fulfills the promise of eliding\nexplicit text normalization as a preprocessing step. One possible evaluation\nmetric for such domains is the proximity of spelling variants. We propose how\nsuch a metric might be computed and how a spelling variant dataset can be\ncollected using UrbanDictionary.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 13:39:54 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Saphra", "Naomi", ""], ["Lopez", "Adam", ""]]}, {"id": "1606.08340", "submitter": "Chen Xing", "authors": "Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, Wei-Ying Ma", "title": "Topic Aware Neural Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider incorporating topic information into the sequence-to-sequence\nframework to generate informative and interesting responses for chatbots. To\nthis end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model. The\nmodel utilizes topics to simulate prior knowledge of human that guides them to\nform informative and interesting responses in conversation, and leverages the\ntopic information in generation by a joint attention mechanism and a biased\ngeneration probability. The joint attention mechanism summarizes the hidden\nvectors of an input message as context vectors by message attention,\nsynthesizes topic vectors by topic attention from the topic words of the\nmessage obtained from a pre-trained LDA model, and let these vectors jointly\naffect the generation of words in decoding. To increase the possibility of\ntopic words appearing in responses, the model modifies the generation\nprobability of topic words by adding an extra probability item to bias the\noverall distribution. Empirical study on both automatic evaluation metrics and\nhuman annotations shows that TA-Seq2Seq can generate more informative and\ninteresting responses, and significantly outperform the-state-of-the-art\nresponse generation models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 05:47:59 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 02:09:13 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Xing", "Chen", ""], ["Wu", "Wei", ""], ["Wu", "Yu", ""], ["Liu", "Jie", ""], ["Huang", "Yalou", ""], ["Zhou", "Ming", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1606.08359", "submitter": "Thomas Demeester", "authors": "Thomas Demeester and Tim Rockt\\\"aschel and Sebastian Riedel", "title": "Lifted Rule Injection for Relation Embeddings", "comments": "Camera-ready version for EMNLP 2016 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on representation learning currently hold the state-of-the-art\nin many natural language processing and knowledge base inference tasks. Yet, a\nmajor challenge is how to efficiently incorporate commonsense knowledge into\nsuch models. A recent approach regularizes relation and entity representations\nby propositionalization of first-order logic rules. However,\npropositionalization does not scale beyond domains with only few entities and\nrules. In this paper we present a highly efficient method for incorporating\nimplication rules into distributed representations for automated knowledge base\nconstruction. We map entity-tuple embeddings into an approximately Boolean\nspace and encourage a partial ordering over relation embeddings based on\nimplication rules mined from WordNet. Surprisingly, we find that the strong\nrestriction of the entity-tuple embedding space does not hurt the\nexpressiveness of the model and even acts as a regularizer that improves\ngeneralization. By incorporating few commonsense rules, we achieve an increase\nof 2 percentage points mean average precision over a matrix factorization\nbaseline, while observing a negligible increase in runtime.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 16:39:23 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 20:40:25 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Demeester", "Thomas", ""], ["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1606.08425", "submitter": "Elliot Schumacher", "authors": "Elliot Schumacher, Maxine Eskenazi, Gwen Frishkoff, Kevyn\n  Collins-Thompson", "title": "Predicting the Relative Difficulty of Single Sentences With and Without\n  Surrounding Context", "comments": "EMNLP 2016 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of accurately predicting relative reading difficulty across a set\nof sentences arises in a number of important natural language applications,\nsuch as finding and curating effective usage examples for intelligent language\ntutoring systems. Yet while significant research has explored document- and\npassage-level reading difficulty, the special challenges involved in assessing\naspects of readability for single sentences have received much less attention,\nparticularly when considering the role of surrounding passages. We introduce\nand evaluate a novel approach for estimating the relative reading difficulty of\na set of sentences, with and without surrounding context. Using different sets\nof lexical and grammatical features, we explore models for predicting pairwise\nrelative difficulty using logistic regression, and examine rankings generated\nby aggregating pairwise difficulty labels using a Bayesian rating system to\nform a final ranking. We also compare rankings derived for sentences assessed\nwith and without context, and find that contextual features can help predict\ndifferences in relative difficulty judgments across these two conditions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 19:48:40 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 14:54:57 GMT"}, {"version": "v3", "created": "Tue, 25 Oct 2016 00:11:06 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Schumacher", "Elliot", ""], ["Eskenazi", "Maxine", ""], ["Frishkoff", "Gwen", ""], ["Collins-Thompson", "Kevyn", ""]]}, {"id": "1606.08495", "submitter": "Mihajlo Grbovic", "authors": "Erik Ordentlich, Lee Yang, Andy Feng, Peter Cnudde, Mihajlo Grbovic,\n  Nemanja Djuric, Vladan Radosavljevic, Gavin Owens", "title": "Network-Efficient Distributed Word2vec Training System for Large\n  Vocabularies", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec is a popular family of algorithms for unsupervised training of dense\nvector representations of words on large text corpuses. The resulting vectors\nhave been shown to capture semantic relationships among their corresponding\nwords, and have shown promise in reducing a number of natural language\nprocessing (NLP) tasks to mathematical operations on these vectors. While\nheretofore applications of word2vec have centered around vocabularies with a\nfew million words, wherein the vocabulary is the set of words for which vectors\nare simultaneously trained, novel applications are emerging in areas outside of\nNLP with vocabularies comprising several 100 million words. Existing word2vec\ntraining systems are impractical for training such large vocabularies as they\neither require that the vectors of all vocabulary words be stored in the memory\nof a single server or suffer unacceptable training latency due to massive\nnetwork data transfer. In this paper, we present a novel distributed, parallel\ntraining system that enables unprecedented practical training of vectors for\nvocabularies with several 100 million words on a shared cluster of commodity\nservers, using far less network traffic than the existing solutions. We\nevaluate the proposed system on a benchmark dataset, showing that the quality\nof vectors does not degrade relative to non-distributed training. Finally, for\nseveral quarters, the system has been deployed for the purpose of matching\nqueries to ads in Gemini, the sponsored search advertising platform at Yahoo,\nresulting in significant improvement of business metrics.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 22:00:21 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Ordentlich", "Erik", ""], ["Yang", "Lee", ""], ["Feng", "Andy", ""], ["Cnudde", "Peter", ""], ["Grbovic", "Mihajlo", ""], ["Djuric", "Nemanja", ""], ["Radosavljevic", "Vladan", ""], ["Owens", "Gavin", ""]]}, {"id": "1606.08513", "submitter": "Tomasz Jurczyk", "authors": "Tomasz Jurczyk, Michael Zhai, Jinho D. Choi", "title": "SelQA: A New Benchmark for Selection-based Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new selection-based question answering dataset, SelQA.\nThe dataset consists of questions generated through crowdsourcing and sentence\nlength answers that are drawn from the ten most prevalent topics in the English\nWikipedia. We introduce a corpus annotation scheme that enhances the generation\nof large, diverse, and challenging datasets by explicitly aiming to reduce word\nco-occurrences between the question and answers. Our annotation scheme is\ncomposed of a series of crowdsourcing tasks with a view to more effectively\nutilize crowdsourcing in the creation of question answering datasets in various\ndomains. Several systems are compared on the tasks of answer sentence selection\nand answer triggering, providing strong baseline results for future work to\nimprove upon.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 23:48:16 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 16:36:02 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 01:20:19 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Jurczyk", "Tomasz", ""], ["Zhai", "Michael", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1606.08689", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Hao Wu, Vladan Radosavljevic, Mihajlo Grbovic, Narayan\n  Bhamidipati", "title": "Hierarchical Neural Language Models for Joint Representation of\n  Streaming Documents and their Content", "comments": "24th International World Wide Web Conference", "journal-ref": null, "doi": "10.1145/2736277.2741643", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning distributed representations for documents\nin data streams. The documents are represented as low-dimensional vectors and\nare jointly learned with distributed vector representations of word tokens\nusing a hierarchical framework with two embedded neural language models. In\nparticular, we exploit the context of documents in streams and use one of the\nlanguage models to model the document sequences, and the other to model word\nsequences within them. The models learn continuous vector representations for\nboth word tokens and documents such that semantically similar documents and\nwords are close in a common vector space. We discuss extensions to our model,\nwhich can be applied to personalized recommendation and social relationship\nmining by adding further user layers to the hierarchy, thus learning\nuser-specific vectors to represent individual preferences. We validated the\nlearned representations on a public movie rating data set from MovieLens, as\nwell as on a large-scale Yahoo News data comprising three months of user\nactivity logs collected on Yahoo servers. The results indicate that the\nproposed model can learn useful representations of both documents and word\ntokens, outperforming the current state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:32:08 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Djuric", "Nemanja", ""], ["Wu", "Hao", ""], ["Radosavljevic", "Vladan", ""], ["Grbovic", "Mihajlo", ""], ["Bhamidipati", "Narayan", ""]]}, {"id": "1606.08733", "submitter": "Ond\\v{r}ej Pl\\'atek", "authors": "Ond\\v{r}ej Pl\\'atek and Petr B\\v{e}lohl\\'avek and Vojt\\v{e}ch\n  Hude\\v{c}ek and Filip Jur\\v{c}\\'i\\v{c}ek", "title": "Recurrent Neural Networks for Dialogue State Tracking", "comments": "Accepted to slo-nlp 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper discusses models for dialogue state tracking using recurrent\nneural networks (RNN). We present experiments on the standard dialogue state\ntracking (DST) dataset, DSTC2. On the one hand, RNN models became the state of\nthe art models in DST, on the other hand, most state-of-the-art models are only\nturn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in\norder to achieve such results. We implemented two architectures which can be\nused in incremental settings and require almost no preprocessing. We compare\ntheir performance to the benchmarks on DSTC2 and discuss their properties. With\nonly trivial preprocessing, the performance of our models is close to the\nstate-of- the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 14:33:29 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 21:42:58 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Pl\u00e1tek", "Ond\u0159ej", ""], ["B\u011blohl\u00e1vek", "Petr", ""], ["Hude\u010dek", "Vojt\u011bch", ""], ["Jur\u010d\u00ed\u010dek", "Filip", ""]]}, {"id": "1606.08777", "submitter": "Gemma Boleda", "authors": "Gemma Boleda and Sebastian Pad\\'o and Marco Baroni", "title": "\"Show me the cup\": Reference with Continuous Representations", "comments": null, "journal-ref": "In: Gelbukh A. (eds) Computational Linguistics and Intelligent\n  Text Processing. CICLing 2017. Lecture Notes in Computer Science, vol 10761.\n  Springer, Cham", "doi": "10.1007/978-3-319-77113-7_17", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most basic functions of language is to refer to objects in a\nshared scene. Modeling reference with continuous representations is challenging\nbecause it requires individuation, i.e., tracking and distinguishing an\narbitrary number of referents. We introduce a neural network model that, given\na definite description and a set of objects represented by natural images,\npoints to the intended object if the expression has a unique referent, or\nindicates a failure, if it does not. The model, directly trained on reference\nacts, is competitive with a pipeline manually engineered to perform the same\ntask, both when referents are purely visual, and when they are characterized by\na combination of visual and linguistic properties.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 16:31:50 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Boleda", "Gemma", ""], ["Pad\u00f3", "Sebastian", ""], ["Baroni", "Marco", ""]]}, {"id": "1606.08821", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Aravind Ganapathiraju, Ananth N. Iyer, Scott A. Randal and\n  Felix I. Wyss", "title": "Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy", "comments": "Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition, especially name recognition, is widely used in phone\nservices such as company directory dialers, stock quote providers or location\nfinders. It is usually challenging due to pronunciation variations. This paper\nproposes an efficient and robust data-driven technique which automatically\nlearns acceptable word pronunciations and updates the pronunciation dictionary\nto build a better lexicon without affecting recognition of other words similar\nto the target word. It generalizes well on datasets with various sizes, and\nreduces the error rate on a database with 13000+ human names by 42%, compared\nto a baseline with regular dictionaries already covering canonical\npronunciations of 97%+ words in names, plus a well-trained\nspelling-to-pronunciation (STP) engine.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 18:44:38 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Ge", "Zhenhao", ""], ["Ganapathiraju", "Aravind", ""], ["Iyer", "Ananth N.", ""], ["Randal", "Scott A.", ""], ["Wyss", "Felix I.", ""]]}, {"id": "1606.08954", "submitter": "Swabha Swayamdipta", "authors": "Swabha Swayamdipta and Miguel Ballesteros and Chris Dyer and Noah A.\n  Smith", "title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs", "comments": "Proceedings of CoNLL 2016; 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transition-based parser that jointly produces syntactic and\nsemantic dependencies. It learns a representation of the entire algorithm\nstate, using stack long short-term memories. Our greedy inference algorithm has\nlinear time, including feature extraction. On the CoNLL 2008--9 English shared\ntasks, we obtain the best published parsing performance among models that\njointly learn syntax and semantics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 05:01:56 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 00:43:14 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Swayamdipta", "Swabha", ""], ["Ballesteros", "Miguel", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1606.09058", "submitter": "Dimitrios Alikaniotis", "authors": "Dimitrios Alikaniotis and John N. Williams", "title": "A Distributional Semantics Approach to Implicit Language Learning", "comments": "5 pages, 7 figures, NetWords 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we show that distributional information is particularly\nimportant when considering concept availability under implicit language\nlearning conditions. Based on results from different behavioural experiments we\nargue that the implicit learnability of semantic regularities depends on the\ndegree to which the relevant concept is reflected in language use. In our\nsimulations, we train a Vector-Space model on either an English or a Chinese\ncorpus and then feed the resulting representations to a feed-forward neural\nnetwork. The task of the neural network was to find a mapping between the word\nrepresentations and the novel words. Using datasets from four behavioural\nexperiments, which used different semantic manipulations, we were able to\nobtain learning patterns very similar to those obtained by humans.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 12:08:51 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Alikaniotis", "Dimitrios", ""], ["Williams", "John N.", ""]]}, {"id": "1606.09163", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka and Giampiero Salvi", "title": "Optimising The Input Window Alignment in CD-DNN Based Phoneme\n  Recognition for Low Latency Processing", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a systematic analysis on the performance of a phonetic recogniser\nwhen the window of input features is not symmetric with respect to the current\nframe. The recogniser is based on Context Dependent Deep Neural Networks\n(CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the\nlatency of the system by reducing the number of future feature frames required\nto estimate the current output. Our tests performed on the TIMIT database show\nthat the performance does not degrade when the input window is shifted up to 5\nframes in the past compared to common practice (no future frame). This\ncorresponds to improving the latency by 50 ms in our settings. Our tests also\nshow that the best results are not obtained with the symmetric window commonly\nemployed, but with an asymmetric window with eight past and two future context\nframes, although this observation should be confirmed on other data sets. The\nreduction in latency suggested by our results is critical for specific\napplications such as real-time lip synchronisation for tele-presence, but may\nalso be beneficial in general applications to improve the lag in human-machine\nspoken interaction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 15:51:44 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1606.09222", "submitter": "Ary Setijadi Prihatmanto", "authors": "Salita Ulitia Prini, Ary Setijadi Prihatmanto", "title": "Penambahan emosi menggunakan metode manipulasi prosodi untuk sistem text\n  to speech bahasa Indonesia", "comments": "Keywords: Text To Speech; eSpeak; MBROLA; Human Speech Corpus;\n  emotion; intonation; prosody manipulation; emosi; intonasi; manipulasi\n  prosodi", "journal-ref": null, "doi": "10.13140/RG.2.1.4140.6165", "report-no": null, "categories": "cs.SD cs.CL cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adding an emotions using prosody manipulation method for Indonesian text to\nspeech system. Text To Speech (TTS) is a system that can convert text in one\nlanguage into speech, accordance with the reading of the text in the language\nused. The focus of this research is a natural sounding concept, the make\n\"humanize\" for the pronunciation of voice synthesis system Text To Speech.\nHumans have emotions / intonation that may affect the sound produced. The main\nrequirement for the system used Text To Speech in this research is eSpeak, the\ndatabase MBROLA using id1, Human Speech Corpus database from a website that\nsummarizes the words with the highest frequency (Most Common Words) used in a\ncountry. And there are 3 types of emotional / intonation designed base. There\nis a happy, angry and sad emotion. Method for develop the emotional filter is\nmanipulate the relevant features of prosody (especially pitch and duration\nvalue) using a predetermined rate factor that has been established by analyzing\nthe differences between the standard output Text To Speech and voice recording\nwith emotional prosody / a particular intonation. The test results for the\nperception tests of Human Speech Corpus for happy emotion is 95 %, 96.25 % for\nangry emotion and 98.75 % for sad emotions. For perception test system carried\nby intelligibility and naturalness test. Intelligibility test for the accuracy\nof sound with the original sentence is 93.3%, and for clarity rate for each\nsentence is 62.8%. For naturalness, accuracy emotional election amounted to\n75.6 % for happy emotion, 73.3 % for angry emotion, and 60 % for sad emotions.\n  -----\n  Text To Speech (TTS) merupakan suatu sistem yang dapat mengonversi teks dalam\nformat suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa\nyang digunakan.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 19:06:48 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Prini", "Salita Ulitia", ""], ["Prihatmanto", "Ary Setijadi", ""]]}, {"id": "1606.09239", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan,\n  Eric P. Xing", "title": "Learning Concept Taxonomies from Multi-modal Data", "comments": "To appear in ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of automatically building hypernym taxonomies from\ntextual and visual data. Previous works in taxonomy induction generally ignore\nthe increasingly prominent visual data, which encode important perceptual\nsemantics. Instead, we propose a probabilistic model for taxonomy induction by\njointly leveraging text and images. To avoid hand-crafted feature engineering,\nwe design end-to-end features based on distributed representations of images\nand words. The model is discriminatively trained given a small set of existing\nontologies and is capable of building full taxonomies from scratch for a\ncollection of unseen conceptual label items with associated images. We evaluate\nour model and features on the WordNet hierarchies, where our system outperforms\nprevious approaches by a large gap.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 19:52:53 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Zhang", "Hao", ""], ["Hu", "Zhiting", ""], ["Deng", "Yuntian", ""], ["Sachan", "Mrinmaya", ""], ["Yan", "Zhicheng", ""], ["Xing", "Eric P.", ""]]}, {"id": "1606.09274", "submitter": "Abigail See", "authors": "Abigail See, Minh-Thang Luong, Christopher D. Manning", "title": "Compression of Neural Machine Translation Models via Pruning", "comments": "Accepted to CoNLL 2016. 9 pages plus references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT), like many other deep learning domains,\ntypically suffers from over-parameterization, resulting in large storage sizes.\nThis paper examines three simple magnitude-based pruning schemes to compress\nNMT models, namely class-blind, class-uniform, and class-distribution, which\ndiffer in terms of how pruning thresholds are computed for the different\nclasses of weights in the NMT architecture. We demonstrate the efficacy of\nweight pruning as a compression technique for a state-of-the-art NMT system. We\nshow that an NMT model with over 200 million parameters can be pruned by 40%\nwith very little performance loss as measured on the WMT'14 English-German\ntranslation task. This sheds light on the distribution of redundancy in the NMT\narchitecture. Our main result is that with retraining, we can recover and even\nsurpass the original performance with an 80%-pruned model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 20:36:23 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["See", "Abigail", ""], ["Luong", "Minh-Thang", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1606.09370", "submitter": "Sunil Sahu", "authors": "Sunil Kumar Sahu, Ashish Anand, Krishnadev Oruganty, Mahanandeeshwar\n  Gattu", "title": "Relation extraction from clinical texts using domain invariant\n  convolutional neural network", "comments": "This paper has been accepted in ACL BioNLP 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years extracting relevant information from biomedical and clinical\ntexts such as research articles, discharge summaries, or electronic health\nrecords have been a subject of many research efforts and shared challenges.\nRelation extraction is the process of detecting and classifying the semantic\nrelation among entities in a given piece of texts. Existing models for this\ntask in biomedical domain use either manually engineered features or kernel\nmethods to create feature vector. These features are then fed to classifier for\nthe prediction of the correct class. It turns out that the results of these\nmethods are highly dependent on quality of user designed features and also\nsuffer from curse of dimensionality. In this work we focus on extracting\nrelations from clinical discharge summaries. Our main objective is to exploit\nthe power of convolution neural network (CNN) to learn features automatically\nand thus reduce the dependency on manual feature engineering. We evaluate\nperformance of the proposed model on i2b2-2010 clinical relation extraction\nchallenge dataset. Our results indicate that convolution neural network can be\na good model for relation exaction in clinical text without being dependent on\nexpert's knowledge on defining quality features.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 07:10:07 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Sahu", "Sunil Kumar", ""], ["Anand", "Ashish", ""], ["Oruganty", "Krishnadev", ""], ["Gattu", "Mahanandeeshwar", ""]]}, {"id": "1606.09371", "submitter": "Sunil Sahu", "authors": "Sunil Kumar Sahu, Ashish Anand", "title": "Recurrent neural network models for disease name recognition using\n  domain invariant features", "comments": "This work has been accepted in ACL-2016 as long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Hand-crafted features based on linguistic and domain-knowledge play crucial\nrole in determining the performance of disease name recognition systems. Such\nmethods are further limited by the scope of these features or in other words,\ntheir ability to cover the contexts or word dependencies within a sentence. In\nthis work, we focus on reducing such dependencies and propose a\ndomain-invariant framework for the disease name recognition task. In\nparticular, we propose various end-to-end recurrent neural network (RNN) models\nfor the tasks of disease name recognition and their classification into four\npre-defined categories. We also utilize convolution neural network (CNN) in\ncascade of RNN to get character-based embedded features and employ it with\nword-embedded features in our model. We compare our models with the\nstate-of-the-art results for the two tasks on NCBI disease dataset. Our results\nfor the disease mention recognition task indicate that state-of-the-art\nperformance can be obtained without relying on feature engineering. Further the\nproposed models obtained improved performance on the classification task of\ndisease names.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 07:15:56 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Sahu", "Sunil Kumar", ""], ["Anand", "Ashish", ""]]}, {"id": "1606.09403", "submitter": "Long Duong", "authors": "Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, Trevor Cohn", "title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Crosslingual word embeddings represent lexical items from different languages\nin the same vector space, enabling transfer of NLP tools. However, previous\nattempts had expensive resource requirements, difficulty incorporating\nmonolingual data or were unable to handle polysemy. We address these drawbacks\nin our method which takes advantage of a high coverage dictionary in an EM\nstyle training algorithm over monolingual corpora in two languages. Our model\nachieves state-of-the-art performance on bilingual lexicon induction task\nexceeding models using large bilingual corpora, and competitive results on the\nmonolingual word similarity and cross-lingual document classification task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 09:18:53 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Duong", "Long", ""], ["Kanayama", "Hiroshi", ""], ["Ma", "Tengfei", ""], ["Bird", "Steven", ""], ["Cohn", "Trevor", ""]]}, {"id": "1606.09560", "submitter": "Michael Auli", "authors": "Joel Legrand, Michael Auli, Ronan Collobert", "title": "Neural Network-based Word Alignment through Score Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple neural network for word alignment that builds source and\ntarget word window representations to compute alignment scores for sentence\npairs. To enable unsupervised training, we use an aggregation operation that\nsummarizes the alignment scores for a given target word. A soft-margin\nobjective increases scores for true target words while decreasing scores for\ntarget words that are not present. Compared to the popular Fast Align model,\nour approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on\nRomanian-English and by 1.7 AER on English-French alignment.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 16:32:00 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Legrand", "Joel", ""], ["Auli", "Michael", ""], ["Collobert", "Ronan", ""]]}, {"id": "1606.09600", "submitter": "Daniel Beck", "authors": "Daniel Beck, Lucia Specia, Trevor Cohn", "title": "Exploring Prediction Uncertainty in Machine Translation Quality\n  Estimation", "comments": "Proceedings of CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation Quality Estimation is a notoriously difficult task, which\nlessens its usefulness in real-world translation environments. Such scenarios\ncan be improved if quality predictions are accompanied by a measure of\nuncertainty. However, models in this task are traditionally evaluated only in\nterms of point estimate metrics, which do not take prediction uncertainty into\naccount. We investigate probabilistic methods for Quality Estimation that can\nprovide well-calibrated uncertainty estimates and evaluate them in terms of\ntheir full posterior predictive distributions. We also show how this posterior\ninformation can be useful in an asymmetric risk scenario, which aims to capture\ntypical situations in translation workflows.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 18:10:46 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Beck", "Daniel", ""], ["Specia", "Lucia", ""], ["Cohn", "Trevor", ""]]}, {"id": "1606.09604", "submitter": "Marco Antonio Valenzuela Esc\\'arcega", "authors": "Marco A. Valenzuela-Escarcega, Gus Hahn-Powell, Dane Bell, Mihai\n  Surdeanu", "title": "SnapToGrid: From Statistical to Interpretable Models for Biomedical\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for biomedical information extraction that marries the\nadvantages of machine learning models, e.g., learning directly from data, with\nthe benefits of rule-based approaches, e.g., interpretability. Our approach\nstarts by training a feature-based statistical model, then converts this model\nto a rule-based variant by converting its features to rules, and \"snapping to\ngrid\" the feature weights to discrete votes. In doing so, our proposal takes\nadvantage of the large body of work in machine learning, but it produces an\ninterpretable model, which can be directly edited by experts. We evaluate our\napproach on the BioNLP 2009 event extraction task. Our results show that there\nis a small performance penalty when converting the statistical model to rules,\nbut the gain in interpretability compensates for that: with minimal effort,\nhuman experts improve this model to have similar performance to the statistical\nmodel that served as starting point.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 18:23:47 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Valenzuela-Escarcega", "Marco A.", ""], ["Hahn-Powell", "Gus", ""], ["Bell", "Dane", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1606.09636", "submitter": "Diego Amancio Dr.", "authors": "Henrique F. de Arruda, Filipi N. Silva, Vanessa Q. Marinho, Diego R.\n  Amancio, Luciano da F. Costa", "title": "Representation of texts as complex networks: a mesoscopic approach", "comments": null, "journal-ref": "Journal of Complex Networks 6(1), 125-144, 2018", "doi": "10.1093/comnet/cnx023", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical techniques that analyze texts, referred to as text analytics,\nhave departed from the use of simple word count statistics towards a new\nparadigm. Text mining now hinges on a more sophisticated set of methods,\nincluding the representations in terms of complex networks. While\nwell-established word-adjacency (co-occurrence) methods successfully grasp\nsyntactical features of written texts, they are unable to represent important\naspects of textual data, such as its topical structure, i.e. the sequence of\nsubjects developing at a mesoscopic level along the text. Such aspects are\noften overlooked by current methodologies. In order to grasp the mesoscopic\ncharacteristics of semantical content in written texts, we devised a network\nmodel which is able to analyze documents in a multi-scale fashion. In the\nproposed model, a limited amount of adjacent paragraphs are represented as\nnodes, which are connected whenever they share a minimum semantical content. To\nillustrate the capabilities of our model, we present, as a case example, a\nqualitative analysis of \"Alice's Adventures in Wonderland\". We show that the\nmesoscopic structure of a document, modeled as a network, reveals many semantic\ntraits of texts. Such an approach paves the way to a myriad of semantic-based\napplications. In addition, our approach is illustrated in a machine learning\ncontext, in which texts are classified among real texts and randomized\ninstances.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 19:47:17 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 00:06:48 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["de Arruda", "Henrique F.", ""], ["Silva", "Filipi N.", ""], ["Marinho", "Vanessa Q.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}]