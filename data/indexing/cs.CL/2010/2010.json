[{"id": "2010.00038", "submitter": "Mohit Chandra", "authors": "Mohit Chandra, Ashwin Pathak, Eesha Dutta, Paryul Jain, Manish Gupta,\n  Manish Shrivastava, Ponnurangam Kumaraguru", "title": "AbuseAnalyzer: Abuse Detection, Severity and Target Prediction for Gab\n  Posts", "comments": "Extended version for our paper accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While extensive popularity of online social media platforms has made\ninformation dissemination faster, it has also resulted in widespread online\nabuse of different types like hate speech, offensive language, sexist and\nracist opinions, etc. Detection and curtailment of such abusive content is\ncritical for avoiding its psychological impact on victim communities, and\nthereby preventing hate crimes. Previous works have focused on classifying user\nposts into various forms of abusive behavior. But there has hardly been any\nfocus on estimating the severity of abuse and the target. In this paper, we\npresent a first of the kind dataset with 7601 posts from Gab which looks at\nonline abuse from the perspective of presence of abuse, severity and target of\nabusive behavior. We also propose a system to address these tasks, obtaining an\naccuracy of ~80% for abuse presence, ~82% for abuse target prediction, and ~65%\nfor abuse severity prediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:12:50 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 17:42:33 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chandra", "Mohit", ""], ["Pathak", "Ashwin", ""], ["Dutta", "Eesha", ""], ["Jain", "Paryul", ""], ["Gupta", "Manish", ""], ["Shrivastava", "Manish", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2010.00117", "submitter": "Yuning Mao", "authors": "Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, Jiawei Han", "title": "Multi-document Summarization with Maximal Marginal Relevance-guided\n  Reinforcement Learning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural sequence learning methods have made significant progress in\nsingle-document summarization (SDS), they produce unsatisfactory results on\nmulti-document summarization (MDS). We observe two major challenges when\nadapting SDS advances to MDS: (1) MDS involves larger search space and yet more\nlimited training data, setting obstacles for neural methods to learn adequate\nrepresentations; (2) MDS needs to resolve higher information redundancy among\nthe source documents, which SDS methods are less effective to handle. To close\nthe gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement\nLearning for MDS, which unifies advanced neural SDS methods and statistical\nmeasures used in classical MDS. RL-MMR casts MMR guidance on fewer promising\ncandidates, which restrains the search space and thus leads to better\nrepresentation learning. Additionally, the explicit redundancy measure in MMR\nhelps the neural representation of the summary to better capture redundancy.\nExtensive experiments demonstrate that RL-MMR achieves state-of-the-art\nperformance on benchmark MDS datasets. In particular, we show the benefits of\nincorporating MMR into end-to-end learning when adapting SDS to MDS in terms of\nboth learning effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 21:50:46 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Mao", "Yuning", ""], ["Qu", "Yanru", ""], ["Xie", "Yiqing", ""], ["Ren", "Xiang", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.00121", "submitter": "James Powell", "authors": "James Powell, Kari Sentz", "title": "Interactive Re-Fitting as a Technique for Improving Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are a fixed, distributional representation of the context of\nwords in a corpus learned from word co-occurrences. While word embeddings have\nproven to have many practical uses in natural language processing tasks, they\nreflect the attributes of the corpus upon which they are trained. Recent work\nhas demonstrated that post-processing of word embeddings to apply information\nfound in lexical dictionaries can improve their quality. We build on this\npost-processing technique by making it interactive. Our approach makes it\npossible for humans to adjust portions of a word embedding space by moving sets\nof words closer to one another. One motivating use case for this capability is\nto enable users to identify and reduce the presence of bias in word embeddings.\nOur approach allows users to trigger selective post-processing as they interact\nwith and assess potential bias in word embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 21:54:22 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Powell", "James", ""], ["Sentz", "Kari", ""]]}, {"id": "2010.00133", "submitter": "Nikita Nangia", "authors": "Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R. Bowman", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked\n  Language Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models, especially masked language models (MLMs) have\nseen success across many NLP tasks. However, there is ample evidence that they\nuse the cultural biases that are undoubtedly present in the corpora they are\ntrained on, implicitly creating harm with biased representations. To measure\nsome forms of social bias in language models against protected demographic\ngroups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark\n(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing\nwith nine types of bias, like race, religion, and age. In CrowS-Pairs a model\nis presented with two sentences: one that is more stereotyping and another that\nis less stereotyping. The data focuses on stereotypes about historically\ndisadvantaged groups and contrasts them with advantaged groups. We find that\nall three of the widely-used MLMs we evaluate substantially favor sentences\nthat express stereotypes in every category in CrowS-Pairs. As work on building\nless biased models advances, this dataset can be used as a benchmark to\nevaluate progress.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 22:38:40 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Nangia", "Nikita", ""], ["Vania", "Clara", ""], ["Bhalerao", "Rasika", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2010.00150", "submitter": "Lena Reed", "authors": "Lena Reed, Vrindavan Harrison, Shereen Oraby, Dilek Hakkani-Tur and\n  Marilyn Walker", "title": "Learning from Mistakes: Combining Ontologies via Self-Training for\n  Dialogue Generation", "comments": "main paper 9 pages, 3 pages references, 2 pages supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language generators (NLGs) for task-oriented dialogue typically take\na meaning representation (MR) as input. They are trained end-to-end with a\ncorpus of MR/utterance pairs, where the MRs cover a specific set of dialogue\nacts and domain attributes. Creation of such datasets is labor-intensive and\ntime-consuming. Therefore, dialogue systems for new domain ontologies would\nbenefit from using data for pre-existing ontologies. Here we explore, for the\nfirst time, whether it is possible to train an NLG for a new larger ontology\nusing existing training sets for the restaurant domain, where each set is based\non a different ontology. We create a new, larger combined ontology, and then\ntrain an NLG to produce utterances covering it. For example, if one dataset has\nattributes for family-friendly and rating information, and the other has\nattributes for decor and service, our aim is an NLG for the combined ontology\nthat can produce utterances that realize values for family-friendly, rating,\ndecor and service. Initial experiments with a baseline neural\nsequence-to-sequence model show that this task is surprisingly challenging. We\nthen develop a novel self-training method that identifies (errorful) model\noutputs, automatically constructs a corrected MR input to form a new (MR,\nutterance) training pair, and then repeatedly adds these new instances back\ninto the training data. We then test the resulting model on a new test set. The\nresult is a self-trained model whose performance is an absolute 75.4%\nimprovement over the baseline model. We also report a human qualitative\nevaluation of the final model showing that it achieves high naturalness,\nsemantic coherence and grammaticality\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 23:54:38 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Reed", "Lena", ""], ["Harrison", "Vrindavan", ""], ["Oraby", "Shereen", ""], ["Hakkani-Tur", "Dilek", ""], ["Walker", "Marilyn", ""]]}, {"id": "2010.00153", "submitter": "Zining Zhu", "authors": "Zining Zhu, Chuer Pan, Mohamed Abdalla, Frank Rudzicz", "title": "Examining the rhetorical capacities of neural language models", "comments": "EMNLP 2020 BlackboxNLP Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural language models (LMs) have demonstrated impressive abilities\nin generating high-quality discourse. While many recent papers have analyzed\nthe syntactic aspects encoded in LMs, there has been no analysis to date of the\ninter-sentential, rhetorical knowledge. In this paper, we propose a method that\nquantitatively evaluates the rhetorical capacities of neural LMs. We examine\nthe capacities of neural LMs understanding the rhetoric of discourse by\nevaluating their abilities to encode a set of linguistic features derived from\nRhetorical Structure Theory (RST). Our experiments show that BERT-based LMs\noutperform other Transformer LMs, revealing the richer discourse knowledge in\ntheir intermediate layer representations. In addition, GPT-2 and XLNet\napparently encode less rhetorical knowledge, and we suggest an explanation\ndrawing from linguistic philosophy. Our method shows an avenue towards\nquantifying the rhetorical capacities of neural LMs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 00:18:43 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 22:16:11 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhu", "Zining", ""], ["Pan", "Chuer", ""], ["Abdalla", "Mohamed", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2010.00170", "submitter": "Samiul Alam", "authors": "Samiul Alam, Tahsin Reasat, Asif Shahriyar Sushmit, Sadi Mohammad\n  Siddiquee, Fuad Rahman, Mahady Hasan, Ahmed Imtiaz Humayun", "title": "A Large Multi-Target Dataset of Common Bengali Handwritten Graphemes", "comments": "15 pages, 12 figures, 6 Tables, Submitted to CVPR-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Latin has historically led the state-of-the-art in handwritten optical\ncharacter recognition (OCR) research. Adapting existing systems from Latin to\nalpha-syllabary languages is particularly challenging due to a sharp contrast\nbetween their orthographies. The segmentation of graphical constituents\ncorresponding to characters becomes significantly hard due to a cursive writing\nsystem and frequent use of diacritics in the alpha-syllabary family of\nlanguages. We propose a labeling scheme based on graphemes (linguistic segments\nof word formation) that makes segmentation in-side alpha-syllabary words linear\nand present the first dataset of Bengali handwritten graphemes that are\ncommonly used in an everyday context. The dataset contains 411k curated samples\nof 1295 unique commonly used Bengali graphemes. Additionally, the test set\ncontains 900 uncommon Bengali graphemes for out of dictionary performance\nevaluation. The dataset is open-sourced as a part of a public Handwritten\nGrapheme Classification Challenge on Kaggle to benchmark vision algorithms for\nmulti-target grapheme classification. The unique graphemes present in this\ndataset are selected based on commonality in the Google Bengali ASR corpus.\nFrom competition proceedings, we see that deep-learning methods can generalize\nto a large span of out of dictionary graphemes which are absent during\ntraining. Dataset and starter codes at www.kaggle.com/c/bengaliai-cv19.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 01:51:45 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 23:18:35 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:19:52 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Alam", "Samiul", ""], ["Reasat", "Tahsin", ""], ["Sushmit", "Asif Shahriyar", ""], ["Siddiquee", "Sadi Mohammad", ""], ["Rahman", "Fuad", ""], ["Hasan", "Mahady", ""], ["Humayun", "Ahmed Imtiaz", ""]]}, {"id": "2010.00182", "submitter": "Yang Zhang", "authors": "Yang Zhang, Qiang Ma", "title": "Dual Attention Model for Citation Recommendation", "comments": null, "journal-ref": "Proceedings of the 28th International Conference on Computational\n  Linguistics (COLING2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on an exponentially increasing number of academic articles, discovering\nand citing comprehensive and appropriate resources has become a non-trivial\ntask. Conventional citation recommender methods suffer from severe information\nloss. For example, they do not consider the section of the paper that the user\nis writing and for which they need to find a citation, the relatedness between\nthe words in the local context (the text span that describes a citation), or\nthe importance on each word from the local context. These shortcomings make\nsuch methods insufficient for recommending adequate citations to academic\nmanuscripts. In this study, we propose a novel embedding-based neural network\ncalled \"dual attention model for citation recommendation (DACR)\" to recommend\ncitations during manuscript preparation. Our method adapts embedding of three\ndimensions of semantic information: words in the local context, structural\ncontexts, and the section on which a user is working. A neural network is\ndesigned to maximize the similarity between the embedding of the three input\n(local context words, section and structural contexts) and the target citation\nappearing in the context. The core of the neural network is composed of\nself-attention and additive attention, where the former aims to capture the\nrelatedness between the contextual words and structural context, and the latter\naims to learn the importance of them. The experiments on real-world datasets\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 02:41:47 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 11:27:20 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 12:57:58 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 12:31:26 GMT"}, {"version": "v5", "created": "Thu, 3 Dec 2020 05:16:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhang", "Yang", ""], ["Ma", "Qiang", ""]]}, {"id": "2010.00190", "submitter": "Long Xuan Ma", "authors": "Longxuan Ma and Weinan Zhang and Runxin Sun and Ting Liu", "title": "A Compare Aggregate Transformer for Understanding Document-grounded\n  Dialogue", "comments": "7pages, 3 figures, 6 tables, Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured documents serving as external knowledge of the dialogues help to\ngenerate more informative responses. Previous research focused on knowledge\nselection (KS) in the document with dialogue. However, dialogue history that is\nnot related to the current dialogue may introduce noise in the KS processing.\nIn this paper, we propose a Compare Aggregate Transformer (CAT) to jointly\ndenoise the dialogue context and aggregate the document information for\nresponse generation. We designed two different comparison mechanisms to reduce\nnoise (before and during decoding). In addition, we propose two metrics for\nevaluating document utilization efficiency based on word overlap. Experimental\nresults on the CMUDoG dataset show that the proposed CAT model outperforms the\nstate-of-the-art approach and strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 03:44:44 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ma", "Longxuan", ""], ["Zhang", "Weinan", ""], ["Sun", "Runxin", ""], ["Liu", "Ting", ""]]}, {"id": "2010.00198", "submitter": "Binh Nguyen", "authors": "Thai Binh Nguyen, Quang Minh Nguyen, Thi Thu Hien Nguyen, Quoc Truong\n  Do, Chi Mai Luong", "title": "Improving Vietnamese Named Entity Recognition from Speech Using Word\n  Capitalization and Punctuation Recovery Models", "comments": "Accepted in Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on the Named Entity Recognition (NER) task have shown outstanding\nresults that reach human parity on input texts with correct text formattings,\nsuch as with proper punctuation and capitalization. However, such conditions\nare not available in applications where the input is speech, because the text\nis generated from a speech recognition system (ASR), and that the system does\nnot consider the text formatting. In this paper, we (1) presented the first\nVietnamese speech dataset for NER task, and (2) the first pre-trained public\nlarge-scale monolingual language model for Vietnamese that achieved the new\nstate-of-the-art for the Vietnamese NER task by 1.3% absolute F1 score\ncomparing to the latest study. And finally, (3) we proposed a new pipeline for\nNER task from speech that overcomes the text formatting problem by introducing\na text capitalization and punctuation recovery model (CaPu) into the pipeline.\nThe model takes input text from an ASR system and performs two tasks at the\nsame time, producing proper text formatting that helps to improve NER\nperformance. Experimental results indicated that the CaPu model helps to\nimprove by nearly 4% of F1-score.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 05:21:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Nguyen", "Thai Binh", ""], ["Nguyen", "Quang Minh", ""], ["Nguyen", "Thi Thu Hien", ""], ["Do", "Quoc Truong", ""], ["Luong", "Chi Mai", ""]]}, {"id": "2010.00200", "submitter": "Michael Bendersky", "authors": "Michael Bendersky and Honglei Zhuang and Ji Ma and Shuguang Han and\n  Keith Hall and Ryan McDonald", "title": "RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report the results of our participation in the TREC-COVID\nchallenge. To meet the challenge of building a search engine for rapidly\nevolving biomedical collection, we propose a simple yet effective weighted\nhierarchical rank fusion approach, that ensembles together 102 runs from (a)\nlexical and semantic retrieval systems, (b) pre-trained and fine-tuned BERT\nrankers, and (c) relevance feedback runs. Our ablation studies demonstrate the\ncontributions of each of these systems to the overall ensemble. The submitted\nensemble runs achieved state-of-the-art performance in rounds 4 and 5 of the\nTREC-COVID challenge.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 05:27:51 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bendersky", "Michael", ""], ["Zhuang", "Honglei", ""], ["Ma", "Ji", ""], ["Han", "Shuguang", ""], ["Hall", "Keith", ""], ["McDonald", "Ryan", ""]]}, {"id": "2010.00247", "submitter": "Fandong Meng", "authors": "Fandong Meng, Jianhao Yan, Yijin Liu, Yuan Gao, Xianfeng Zeng, Qinsong\n  Zeng, Peng Li, Ming Chen, Jie Zhou, Sifan Liu and Hao Zhou", "title": "WeChat Neural Machine Translation Systems for WMT20", "comments": "Accepted at WMT 2020. Our Chinese to English system achieved the\n  highest case-sensitive BLEU score among all submissions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participate in the WMT 2020 shared news translation task on Chinese to\nEnglish. Our system is based on the Transformer (Vaswani et al., 2017a) with\neffective variants and the DTMT (Meng and Zhang, 2019) architecture. In our\nexperiments, we employ data selection, several synthetic data generation\napproaches (i.e., back-translation, knowledge distillation, and iterative\nin-domain knowledge transfer), advanced finetuning approaches and self-bleu\nbased model ensemble. Our constrained Chinese to English system achieves 36.9\ncase-sensitive BLEU score, which is the highest among all submissions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 08:15:09 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:01:01 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Meng", "Fandong", ""], ["Yan", "Jianhao", ""], ["Liu", "Yijin", ""], ["Gao", "Yuan", ""], ["Zeng", "Xianfeng", ""], ["Zeng", "Qinsong", ""], ["Li", "Peng", ""], ["Chen", "Ming", ""], ["Zhou", "Jie", ""], ["Liu", "Sifan", ""], ["Zhou", "Hao", ""]]}, {"id": "2010.00287", "submitter": "Ehsan Doostmohammadi", "authors": "Ehsan Doostmohammadi, Minoo Nassajian, Adel Rahimi", "title": "Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner\n  Recognition Using BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words are properly segmented in the Persian writing system; in practice,\nhowever, these writing rules are often neglected, resulting in single words\nbeing written disjointedly and multiple words written without any white spaces\nbetween them. This paper addresses the problems of word segmentation and\nzero-width non-joiner (ZWNJ) recognition in Persian, which we approach jointly\nas a sequence labeling problem. We achieved a macro-averaged F1-score of 92.40%\non a carefully collected corpus of 500 sentences with a high level of\ndifficulty.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:32:17 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 09:40:18 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Doostmohammadi", "Ehsan", ""], ["Nassajian", "Minoo", ""], ["Rahimi", "Adel", ""]]}, {"id": "2010.00294", "submitter": "Anshul Wadhawan", "authors": "Anshul Wadhawan", "title": "Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID\n  Twitter BERT and Bagging Ensemble Technique based on Plurality Voting", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2020.wnut-1.47", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the approach that we employed to tackle the EMNLP\nWNUT-2020 Shared Task 2 : Identification of informative COVID-19 English\nTweets. The task is to develop a system that automatically identifies whether\nan English Tweet related to the novel coronavirus (COVID-19) is informative or\nnot. We solve the task in three stages. The first stage involves pre-processing\nthe dataset by filtering only relevant information. This is followed by\nexperimenting with multiple deep learning models like CNNs, RNNs and\nTransformer based models. In the last stage, we propose an ensemble of the best\nmodel trained on different subsets of the provided dataset. Our final approach\nachieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score\nas the evaluation criteria.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:54:54 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 09:43:34 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 08:35:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wadhawan", "Anshul", ""]]}, {"id": "2010.00309", "submitter": "Tianxiang Sun", "authors": "Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing\n  Huang, Zheng Zhang", "title": "CoLAKE: Contextualized Language and Knowledge Embedding", "comments": "Accepted by COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emerging branch of incorporating factual knowledge into pre-trained\nlanguage models such as BERT, most existing models consider shallow, static,\nand separately pre-trained entity embeddings, which limits the performance\ngains of these models. Few works explore the potential of deep contextualized\nknowledge representation when injecting knowledge. In this paper, we propose\nthe Contextualized Language and Knowledge Embedding (CoLAKE), which jointly\nlearns contextualized representation for both language and knowledge with the\nextended MLM objective. Instead of injecting only entity embeddings, CoLAKE\nextracts the knowledge context of an entity from large-scale knowledge bases.\nTo handle the heterogeneity of knowledge context and language context, we\nintegrate them in a unified data structure, word-knowledge graph (WK graph).\nCoLAKE is pre-trained on large-scale WK graphs with the modified Transformer\nencoder. We conduct experiments on knowledge-driven tasks, knowledge probing\ntasks, and language understanding tasks. Experimental results show that CoLAKE\noutperforms previous counterparts on most of the tasks. Besides, CoLAKE\nachieves surprisingly high performance on our synthetic task called\nword-knowledge graph completion, which shows the superiority of simultaneously\ncontextualizing language and knowledge representation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:39:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sun", "Tianxiang", ""], ["Shao", "Yunfan", ""], ["Qiu", "Xipeng", ""], ["Guo", "Qipeng", ""], ["Hu", "Yaru", ""], ["Huang", "Xuanjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "2010.00310", "submitter": "Anshul Wadhawan", "authors": "Akshita Aggarwal, Anshul Wadhawan, Anshima Chaudhary, Kavita Maurya", "title": "\"Did you really mean what you said?\" : Sarcasm Detection in\n  Hindi-English Code-Mixed Data using Bilingual Word Embeddings", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2020.wnut-1.2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased use of social media platforms by people across the world,\nmany new interesting NLP problems have come into existence. One such being the\ndetection of sarcasm in the social media texts. We present a corpus of tweets\nfor training custom word embeddings and a Hinglish dataset labelled for sarcasm\ndetection. We propose a deep learning based approach to address the issue of\nsarcasm detection in Hindi-English code mixed tweets using bilingual word\nembeddings derived from FastText and Word2Vec approaches. We experimented with\nvarious deep learning models, including CNNs, LSTMs, Bi-directional LSTMs (with\nand without attention). We were able to outperform all state-of-the-art\nperformances with our deep learning models, with attention based Bi-directional\nLSTMs giving the best performance exhibiting an accuracy of 78.49%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:41:44 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 08:32:33 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 08:32:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Aggarwal", "Akshita", ""], ["Wadhawan", "Anshul", ""], ["Chaudhary", "Anshima", ""], ["Maurya", "Kavita", ""]]}, {"id": "2010.00357", "submitter": "Hind Alatwi", "authors": "Hind Saleh Alatawi, Areej Maatog Alhothali and Kawthar Mustafa Moria", "title": "Detecting White Supremacist Hate Speech using Domain Specific Word\n  Embedding with Deep Learning and BERT", "comments": "32 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White supremacists embrace a radical ideology that considers white people\nsuperior to people of other races. The critical influence of these groups is no\nlonger limited to social media; they also have a significant effect on society\nin many ways by promoting racial hatred and violence. White supremacist hate\nspeech is one of the most recently observed harmful content on social\nmedia.Traditional channels of reporting hate speech have proved inadequate due\nto the tremendous explosion of information, and therefore, it is necessary to\nfind an automatic way to detect such speech in a timely manner. This research\ninvestigates the viability of automatically detecting white supremacist hate\nspeech on Twitter by using deep learning and natural language processing\ntechniques. Through our experiments, we used two approaches, the first approach\nis by using domain-specific embeddings which are extracted from white\nsupremacist corpus in order to catch the meaning of this white supremacist\nslang with bidirectional Long Short-Term Memory (LSTM) deep learning model,\nthis approach reached a 0.74890 F1-score. The second approach is by using the\none of the most recent language model which is BERT, BERT model provides the\nstate of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both\napproaches are tested on a balanced dataset given that our experiments were\nbased on textual data only. The dataset was combined from dataset created from\nTwitter and a Stormfront dataset compiled from that white supremacist forum.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:44:24 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Alatawi", "Hind Saleh", ""], ["Alhothali", "Areej Maatog", ""], ["Moria", "Kawthar Mustafa", ""]]}, {"id": "2010.00363", "submitter": "Daichi Mochihashi", "authors": "Chihiro Shibata, Kei Uchiumi, Daichi Mochihashi", "title": "How LSTM Encodes Syntax: Exploring Context Vectors and Semi-Quantization\n  on Natural Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory recurrent neural network (LSTM) is widely used and\nknown to capture informative long-term syntactic dependencies. However, how\nsuch information are reflected in its internal vectors for natural text has not\nyet been sufficiently investigated. We analyze them by learning a language\nmodel where syntactic structures are implicitly given. We empirically show that\nthe context update vectors, i.e. outputs of internal gates, are approximately\nquantized to binary or ternary values to help the language model to count the\ndepth of nesting accurately, as Suzgun et al. (2019) recently show for\nsynthetic Dyck languages. For some dimensions in the context vector, we show\nthat their activations are highly correlated with the depth of phrase\nstructures, such as VP and NP. Moreover, with an $L_1$ regularization, we also\nfound that it can accurately predict whether a word is inside a phrase\nstructure or not from a small number of components of the context vector. Even\nfor the case of learning from raw text, context vectors are shown to still\ncorrelate well with the phrase structures. Finally, we show that natural\nclusters of the functional words and the part of speeches that trigger phrases\nare represented in a small but principal subspace of the context-update vector\nof LSTM.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:49:01 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Shibata", "Chihiro", ""], ["Uchiumi", "Kei", ""], ["Mochihashi", "Daichi", ""]]}, {"id": "2010.00372", "submitter": "Haixia Liu", "authors": "Haixia Liu", "title": "Citation Sentiment Changes Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics for measuring the citation sentiment changes were introduced.\nCitation sentiment changes can be observed from global citation sentiment\nsequences (GCSSs). With respect to a cited paper, the citation sentiment\nsequences were analysed across a collection of citing papers ordered by the\npublished time. For analysing GCSSs, Eddy Dissipation Rate (EDR) was adopted,\nwith the hypothesis that the GCSSs pattern differences can be spotted by EDR\nbased method. Preliminary evidence showed that EDR based method holds the\npotential for analysing a publication's impact in a time series fashion.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:10:03 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Liu", "Haixia", ""]]}, {"id": "2010.00389", "submitter": "Mokanarangan Thayaparan", "authors": "Mokanarangan Thayaparan, Marco Valentino, Andr\\'e Freitas", "title": "A Survey on Explainability in Machine Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a systematic review of benchmarks and approaches for\nexplainability in Machine Reading Comprehension (MRC). We present how the\nrepresentation and inference challenges evolved and the steps which were taken\nto tackle these challenges. We also present the evaluation methodologies to\nassess the performance of explainable systems. In addition, we identify\npersisting open research questions and highlight critical directions for future\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:26:58 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2010.00454", "submitter": "Claudia Kittask", "authors": "Claudia Kittask, Kirill Milintsevich, Kairit Sirts", "title": "Evaluating Multilingual BERT for Estonian", "comments": "V1: Baltic HLT 2020 V2: Changed NER baseline results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, large pre-trained language models, such as BERT, have reached\nstate-of-the-art performance in many natural language processing tasks, but for\nmany languages, including Estonian, BERT models are not yet available. However,\nthere exist several multilingual BERT models that can handle multiple languages\nsimultaneously and that have been trained also on Estonian data. In this paper,\nwe evaluate four multilingual models -- multilingual BERT, multilingual\ndistilled BERT, XLM and XLM-RoBERTa -- on several NLP tasks including POS and\nmorphological tagging, NER and text classification. Our aim is to establish a\ncomparison between these multilingual BERT models and the existing baseline\nneural models for these tasks. Our results show that multilingual BERT models\ncan generalise well on different Estonian NLP tasks outperforming all baselines\nmodels for POS and morphological tagging and text classification, and reaching\nthe comparable level with the best baseline for NER, with XLM-RoBERTa achieving\nthe highest results compared with other multilingual models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:48:31 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 10:00:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kittask", "Claudia", ""], ["Milintsevich", "Kirill", ""], ["Sirts", "Kairit", ""]]}, {"id": "2010.00462", "submitter": "Ly Antoine PhD", "authors": "Antoine Ly, Benno Uthayasooriyar, Tingting Wang", "title": "A survey on natural language processing (nlp) and applications in\n  insurance", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text is the most widely used means of communication today. This data is\nabundant but nevertheless complex to exploit within algorithms. For years,\nscientists have been trying to implement different techniques that enable\ncomputers to replicate some mechanisms of human reading. During the past five\nyears, research disrupted the capacity of the algorithms to unleash the value\nof text data. It brings today, many opportunities for the insurance\nindustry.Understanding those methods and, above all, knowing how to apply them\nis a major challenge and key to unleash the value of text data that have been\nstored for many years. Processing language with computer brings many new\nopportunities especially in the insurance sector where reports are central in\nthe information used by insurers. SCOR's Data Analytics team has been working\non the implementation of innovative tools or products that enable the use of\nthe latest research on text analysis. Understanding text mining techniques in\ninsurance enhances the monitoring of the underwritten risks and many processes\nthat finally benefit policyholders.This article proposes to explain\nopportunities that Natural Language Processing (NLP) are providing to\ninsurance. It details different methods used today in practice traces back the\nstory of them. We also illustrate the implementation of certain methods using\nopen source libraries and python codes that we have developed to facilitate the\nuse of these techniques.After giving a general overview on the evolution of\ntext mining during the past few years,we share about how to conduct a full\nstudy with text mining and share some examples to serve those models into\ninsurance products or services. Finally, we explained in more details every\nstep that composes a Natural Language Processing study to ensure the reader can\nhave a deep understanding on the implementation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:56:18 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ly", "Antoine", ""], ["Uthayasooriyar", "Benno", ""], ["Wang", "Tingting", ""]]}, {"id": "2010.00490", "submitter": "Daniel Deutsch", "authors": "Daniel Deutsch, Tania Bedrax-Weiss, Dan Roth", "title": "Towards Question-Answering as an Automatic Metric for Evaluating the\n  Content Quality of a Summary", "comments": "This is a pre-MIT Press publication version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A desirable property of a reference-based evaluation metric that measures the\ncontent quality of a summary is that it should estimate how much information\nthat summary has in common with a reference. Traditional text overlap based\nmetrics such as ROUGE fail to achieve this because they are limited to matching\ntokens, either lexically or via embeddings. In this work, we propose a metric\nto evaluate the content quality of a summary using question-answering (QA).\nQA-based methods directly measure a summary's information overlap with a\nreference, making them fundamentally different than text overlap metrics. We\ndemonstrate the experimental benefits of QA-based metrics through an analysis\nof our proposed metric, QAEval. QAEval out-performs current state-of-the-art\nmetrics on most evaluations using benchmark datasets, while being competitive\non others due to limitations of state-of-the-art models. Through a careful\nanalysis of each component of QAEval, we identify its performance bottlenecks\nand estimate that its potential upper-bound performance surpasses all other\nautomatic metrics, approaching that of the gold-standard Pyramid Method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:33:09 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 16:47:46 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 18:47:26 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Deutsch", "Daniel", ""], ["Bedrax-Weiss", "Tania", ""], ["Roth", "Dan", ""]]}, {"id": "2010.00502", "submitter": "Gautam Kishore Shahi", "authors": "Gautam Kishore Shahi", "title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:50:41 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Shahi", "Gautam Kishore", ""]]}, {"id": "2010.00514", "submitter": "Shaofei Huang", "authors": "Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong\n  Han, Luoqi Liu, Bo Li", "title": "Referring Image Segmentation via Cross-Modal Progressive Comprehension", "comments": "Accepted by CVPR 2020. Code is available at\n  https://github.com/spyflying/CMPC-Refseg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring image segmentation aims at segmenting the foreground masks of the\nentities that can well match the description given in the natural language\nexpression. Previous approaches tackle this problem using implicit feature\ninteraction and fusion between visual and linguistic modalities, but usually\nfail to explore informative words of the expression to well align features from\nthe two modalities for accurately identifying the referred entity. In this\npaper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a\nText-Guided Feature Exchange (TGFE) module to effectively address the\nchallenging task. Concretely, the CMPC module first employs entity and\nattribute words to perceive all the related entities that might be considered\nby the expression. Then, the relational words are adopted to highlight the\ncorrect entity as well as suppress other irrelevant ones by multimodal graph\nreasoning. In addition to the CMPC module, we further leverage a simple yet\neffective TGFE module to integrate the reasoned multimodal features from\ndifferent levels with the guidance of textual information. In this way,\nfeatures from multi-levels could communicate with each other and be refined\nbased on the textual context. We conduct extensive experiments on four popular\nreferring segmentation benchmarks and achieve new state-of-the-art\nperformances.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:02:30 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Huang", "Shaofei", ""], ["Hui", "Tianrui", ""], ["Liu", "Si", ""], ["Li", "Guanbin", ""], ["Wei", "Yunchao", ""], ["Han", "Jizhong", ""], ["Liu", "Luoqi", ""], ["Li", "Bo", ""]]}, {"id": "2010.00515", "submitter": "Shaofei Huang", "authors": "Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang,\n  Jizhong Han", "title": "Linguistic Structure Guided Context Modeling for Referring Image\n  Segmentation", "comments": "Accepted by ECCV 2020. Code is available at\n  https://github.com/spyflying/LSCM-Refseg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring image segmentation aims to predict the foreground mask of the\nobject referred by a natural language sentence. Multimodal context of the\nsentence is crucial to distinguish the referent from the background. Existing\nmethods either insufficiently or redundantly model the multimodal context. To\ntackle this problem, we propose a \"gather-propagate-distribute\" scheme to model\nmultimodal context by cross-modal interaction and implement this scheme as a\nnovel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM\nmodule builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which\nguides all the words to include valid multimodal context of the sentence while\nexcluding disturbing ones through three steps over the multimodal feature,\ni.e., gathering, constrained propagation and distributing. Extensive\nexperiments on four benchmarks demonstrate that our method outperforms all the\nprevious state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:03:51 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 03:19:48 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 08:49:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hui", "Tianrui", ""], ["Liu", "Si", ""], ["Huang", "Shaofei", ""], ["Li", "Guanbin", ""], ["Yu", "Sansi", ""], ["Zhang", "Faxi", ""], ["Han", "Jizhong", ""]]}, {"id": "2010.00526", "submitter": "Qianying Liu", "authors": "Qianying Liu, Sicong Jiang, Yizhong Wang and Sujian Li", "title": "LiveQA: A Question Answering Dataset over Sports Live", "comments": "CCL 2020 Oral paper, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce LiveQA, a new question answering dataset\nconstructed from play-by-play live broadcast. It contains 117k multiple-choice\nquestions written by human commentators for over 1,670 NBA games, which are\ncollected from the Chinese Hupu (https://nba.hupu.com/games) website. Derived\nfrom the characteristics of sports games, LiveQA can potentially test the\nreasoning ability across timeline-based live broadcasts, which is challenging\ncompared to the existing datasets. In LiveQA, the questions require\nunderstanding the timeline, tracking events or doing mathematical computations.\nOur preliminary experiments show that the dataset introduces a challenging\nproblem for question answering models, and a strong baseline model only\nachieves the accuracy of 53.1\\% and cannot beat the dominant option rule. We\nrelease the code and data of this paper for future research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:18:51 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Liu", "Qianying", ""], ["Jiang", "Sicong", ""], ["Wang", "Yizhong", ""], ["Li", "Sujian", ""]]}, {"id": "2010.00562", "submitter": "Jose Manuel Gomez-Perez", "authors": "Jose Manuel Gomez-Perez, Raul Ortega", "title": "ISAAQ -- Mastering Textbook Questions with Pre-trained Transformers and\n  Bottom-Up and Top-Down Attention", "comments": "Accepted for publication as a long paper in EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textbook Question Answering is a complex task in the intersection of Machine\nComprehension and Visual Question Answering that requires reasoning with\nmultimodal information from text and diagrams. For the first time, this paper\ntaps on the potential of transformer language models and bottom-up and top-down\nattention to tackle the language and visual understanding challenges this task\nentails. Rather than training a language-visual transformer from scratch we\nrely on pre-trained transformers, fine-tuning and ensembling. We add bottom-up\nand top-down attention to identify regions of interest corresponding to diagram\nconstituents and their relationships, improving the selection of relevant\nvisual information for each question and answer options. Our system ISAAQ\nreports unprecedented success in all TQA question types, with accuracies of\n81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice\nquestions. ISAAQ also demonstrates its broad applicability, obtaining\nstate-of-the-art results in other demanding datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:28:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Gomez-Perez", "Jose Manuel", ""], ["Ortega", "Raul", ""]]}, {"id": "2010.00571", "submitter": "Julian Eisenschlos", "authors": "Julian Martin Eisenschlos, Syrine Krichene, Thomas M\\\"uller", "title": "Understanding tables with intermediate pre-training", "comments": "Accepted to EMNLP Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table entailment, the binary classification task of finding if a sentence is\nsupported or refuted by the content of a table, requires parsing language and\ntable structure as well as numerical and discrete reasoning. While there is\nextensive work on textual entailment, table entailment is less well studied. We\nadapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize\nentailment. Motivated by the benefits of data augmentation, we create a\nbalanced dataset of millions of automatically created training examples which\nare learned in an intermediate step prior to fine-tuning. This new data is not\nonly useful for table entailment, but also for SQA (Iyyer et al., 2017), a\nsequential table QA task. To be able to use long examples as input of BERT\nmodels, we evaluate table pruning techniques as a pre-processing step to\ndrastically improve the training and prediction efficiency at a moderate drop\nin accuracy. The different methods set the new state-of-the-art on the TabFact\n(Chen et al., 2020) and SQA datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:43:27 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 12:26:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Eisenschlos", "Julian Martin", ""], ["Krichene", "Syrine", ""], ["M\u00fcller", "Thomas", ""]]}, {"id": "2010.00577", "submitter": "Michael Sejr Schlichtkrull", "authors": "Michael Sejr Schlichtkrull, Nicola De Cao, Ivan Titov", "title": "Interpreting Graph Neural Networks for NLP With Differentiable Edge\n  Masking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have become a popular approach to integrating\nstructural inductive biases into NLP models. However, there has been little\nwork on interpreting them, and specifically on understanding which parts of the\ngraphs (e.g. syntactic trees or co-reference structures) contribute to a\nprediction. In this work, we introduce a post-hoc method for interpreting the\npredictions of GNNs which identifies unnecessary edges. Given a trained GNN\nmodel, we learn a simple classifier that, for every edge in every layer,\npredicts if that edge can be dropped. We demonstrate that such a classifier can\nbe trained in a fully differentiable fashion, employing stochastic gates and\nencouraging sparsity through the expected $L_0$ norm. We use our technique as\nan attribution method to analyze GNN models for two tasks -- question answering\nand semantic role labeling -- providing insights into the information flow in\nthese models. We show that we can drop a large proportion of edges without\ndeteriorating the performance of the model, while we can analyse the remaining\nedges for interpreting model predictions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:51:19 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 10:17:42 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Schlichtkrull", "Michael Sejr", ""], ["De Cao", "Nicola", ""], ["Titov", "Ivan", ""]]}, {"id": "2010.00633", "submitter": "David Vilares", "authors": "David Vilares and Carlos G\\'omez-Rodr\\'iguez", "title": "Discontinuous Constituent Parsing as Sequence Labeling", "comments": "To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reduces discontinuous parsing to sequence labeling. It first shows\nthat existing reductions for constituent parsing as labeling do not support\ndiscontinuities. Second, it fills this gap and proposes to encode tree\ndiscontinuities as nearly ordered permutations of the input sequence. Third, it\nstudies whether such discontinuous representations are learnable. The\nexperiments show that despite the architectural simplicity, under the right\nrepresentation, the models are fast and accurate.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:17:58 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Vilares", "David", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2010.00656", "submitter": "Rui Meng", "authors": "Rui Meng, Zhen Yue, Alyssa Glass", "title": "Predicting User Engagement Status for Online Evaluation of Intelligent\n  Assistants", "comments": "Paper has been accepted by ECIR 2021 (43rd edition of the annual\n  European Conference on Information Retrieval)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of intelligent assistants in large-scale and online settings\nremains an open challenge. User behavior-based online evaluation metrics have\ndemonstrated great effectiveness for monitoring large-scale web search and\nrecommender systems. Therefore, we consider predicting user engagement status\nas the very first and critical step to online evaluation for intelligent\nassistants. In this work, we first proposed a novel framework for classifying\nuser engagement status into four categories -- fulfillment, continuation,\nreformulation and abandonment. We then demonstrated how to design simple but\nindicative metrics based on the framework to quantify user engagement levels.\nWe also aim for automating user engagement prediction with machine learning\nmethods. We compare various models and features for predicting engagement\nstatus using four real-world datasets. We conducted detailed analyses on\nfeatures and failure cases to discuss the performance of current models as well\nas challenges.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 19:33:27 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 22:34:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Meng", "Rui", ""], ["Yue", "Zhen", ""], ["Glass", "Alyssa", ""]]}, {"id": "2010.00667", "submitter": "Hanjie Chen", "authors": "Hanjie Chen, Yangfeng Ji", "title": "Learning Variational Word Masks to Improve the Interpretability of\n  Neural Text Classifiers", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build an interpretable neural text classifier, most of the prior work has\nfocused on designing inherently interpretable models or finding faithful\nexplanations. A new line of work on improving model interpretability has just\nstarted, and many existing methods require either prior information or human\nannotations as additional inputs in training. To address this limitation, we\npropose the variational word mask (VMASK) method to automatically learn\ntask-specific important words and reduce irrelevant information on\nclassification, which ultimately improves the interpretability of model\npredictions. The proposed method is evaluated with three neural text\nclassifiers (CNN, LSTM, and BERT) on seven benchmark text classification\ndatasets. Experiments show the effectiveness of VMASK in improving both model\nprediction accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 20:02:43 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 03:48:07 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 04:16:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chen", "Hanjie", ""], ["Ji", "Yangfeng", ""]]}, {"id": "2010.00677", "submitter": "Jiaming Shen", "authors": "Jiaming Shen and Heng Ji and Jiawei Han", "title": "Near-imperceptible Neural Linguistic Steganography via Self-Adjusting\n  Arithmetic Coding", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistic steganography studies how to hide secret messages in natural\nlanguage cover texts. Traditional methods aim to transform a secret message\ninto an innocent text via lexical substitution or syntactical modification.\nRecently, advances in neural language models (LMs) enable us to directly\ngenerate cover text conditioned on the secret message. In this study, we\npresent a new linguistic steganography method which encodes secret messages\nusing self-adjusting arithmetic coding based on a neural language model. We\nformally analyze the statistical imperceptibility of this method and\nempirically show it outperforms the previous state-of-the-art methods on four\ndatasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively.\nFinally, human evaluations show that 51% of generated cover texts can indeed\nfool eavesdroppers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 20:40:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Shen", "Jiaming", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.00678", "submitter": "Yan Shvartzshnaider", "authors": "Yan Shvartzshnaider, Ananth Balashankar, Vikas Patidar, Thomas Wies,\n  Lakshminarayanan Subramanian", "title": "Beyond The Text: Analysis of Privacy Statements through Syntactic and\n  Semantic Role Labeling", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates a new task of extracting privacy parameters from a\nprivacy policy, through the lens of Contextual Integrity, an established social\ntheory framework for reasoning about privacy norms. Privacy policies, written\nby lawyers, are lengthy and often comprise incomplete and vague statements. In\nthis paper, we show that traditional NLP tasks, including the recently proposed\nQuestion-Answering based solutions, are insufficient to address the privacy\nparameter extraction problem and provide poor precision and recall. We describe\n4 different types of conventional methods that can be partially adapted to\naddress the parameter extraction task with varying degrees of success: Hidden\nMarkov Models, BERT fine-tuned models, Dependency Type Parsing (DP) and\nSemantic Role Labeling (SRL). Based on a detailed evaluation across 36\nreal-world privacy policies of major enterprises, we demonstrate that a\nsolution combining syntactic DP coupled with type-specific SRL tasks provides\nthe highest accuracy for retrieving contextual privacy parameters from privacy\nstatements. We also observe that incorporating domain-specific knowledge is\ncritical to achieving high precision and recall, thus inspiring new NLP\nresearch to address this important problem in the privacy domain.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 20:48:37 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Shvartzshnaider", "Yan", ""], ["Balashankar", "Ananth", ""], ["Patidar", "Vikas", ""], ["Wies", "Thomas", ""], ["Subramanian", "Lakshminarayanan", ""]]}, {"id": "2010.00685", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim\n  Rockt\\\"aschel, Jason Weston", "title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and\n  Act in Fantasy Worlds", "comments": "In NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to create agents that both act and communicate with other agents in\npursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019) -- a\nlarge-scale crowd-sourced fantasy text-game -- with a dataset of quests. These\ncontain natural language motivations paired with in-game goals and human\ndemonstrations; completing a quest might require dialogue or actions (or both).\nWe introduce a reinforcement learning system that (1) incorporates large-scale\nlanguage modeling-based and commonsense reasoning-based pre-training to imbue\nthe agent with relevant priors; and (2) leverages a factorized action space of\naction commands and dialogue, balancing between the two. We conduct zero-shot\nevaluations using held-out human expert demonstrations, showing that our agents\nare able to act consistently and talk naturally with respect to their\nmotivations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:06:21 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 05:27:18 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 15:26:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Urbanek", "Jack", ""], ["Li", "Margaret", ""], ["Szlam", "Arthur", ""], ["Rockt\u00e4schel", "Tim", ""], ["Weston", "Jason", ""]]}, {"id": "2010.00710", "submitter": "Urvashi Khandelwal", "authors": "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike\n  Lewis", "title": "Nearest Neighbor Machine Translation", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results -- without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, $k$NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 22:24:46 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 14:44:51 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Khandelwal", "Urvashi", ""], ["Fan", "Angela", ""], ["Jurafsky", "Dan", ""], ["Zettlemoyer", "Luke", ""], ["Lewis", "Mike", ""]]}, {"id": "2010.00711", "submitter": "Marina Danilevsky", "authors": "Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas,\n  Prithviraj Sen", "title": "A Survey of the State of Explainable AI for Natural Language Processing", "comments": "To appear in AACL-IJCNLP 2020", "journal-ref": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of\n  the Association for Computational Linguistics and the 10th International\n  Joint Conference on Natural Language Processing 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen important advances in the quality of state-of-the-art\nmodels, but this has come at the expense of models becoming less interpretable.\nThis survey presents an overview of the current state of Explainable AI (XAI),\nconsidered within the domain of Natural Language Processing (NLP). We discuss\nthe main categorization of explanations, as well as the various ways\nexplanations can be arrived at and visualized. We detail the operations and\nexplainability techniques currently available for generating explanations for\nNLP model predictions, to serve as a resource for model developers in the\ncommunity. Finally, we point out the current gaps and encourage directions for\nfuture work in this important research area.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 22:33:21 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Danilevsky", "Marina", ""], ["Qian", "Kun", ""], ["Aharonov", "Ranit", ""], ["Katsis", "Yannis", ""], ["Kawas", "Ban", ""], ["Sen", "Prithviraj", ""]]}, {"id": "2010.00722", "submitter": "Ameet Deshpande", "authors": "Ameet Deshpande and Mitesh M. Khapra", "title": "Evaluating a Generative Adversarial Framework for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have resulted in\nits widespread applications to multiple domains. A recent model, IRGAN, applies\nthis framework to Information Retrieval (IR) and has gained significant\nattention over the last few years. In this focused work, we critically analyze\nmultiple components of IRGAN, while providing experimental and theoretical\nevidence of some of its shortcomings. Specifically, we identify issues with the\nconstant baseline term in the policy gradients optimization and show that the\ngenerator harms IRGAN's performance. Motivated by our findings, we propose two\nmodels influenced by self-contrastive estimation and co-training which\noutperform IRGAN on two out of the three tasks considered.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:11:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Deshpande", "Ameet", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "2010.00735", "submitter": "Wentao Zhu", "authors": "Yufang Huang, Wentao Zhu, Deyi Xiong, Yiye Zhang, Changjian Hu, Feiyu\n  Xu", "title": "Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style\n  Transfer", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised text style transfer is full of challenges due to the lack of\nparallel data and difficulties in content preservation. In this paper, we\npropose a novel neural approach to unsupervised text style transfer, which we\nrefer to as Cycle-consistent Adversarial autoEncoders (CAE) trained from\nnon-parallel data. CAE consists of three essential components: (1) LSTM\nautoencoders that encode a text in one style into its latent representation and\ndecode an encoded representation into its original text or a transferred\nrepresentation into a style-transferred text, (2) adversarial style transfer\nnetworks that use an adversarially trained generator to transform a latent\nrepresentation in one style into a representation in another style, and (3) a\ncycle-consistent constraint that enhances the capacity of the adversarial style\ntransfer networks in content preservation. The entire CAE with these three\ncomponents can be trained end-to-end. Extensive experiments and in-depth\nanalyses on two widely-used public datasets consistently validate the\neffectiveness of proposed CAE in both style transfer and content preservation\nagainst several strong baselines in terms of four automatic evaluation metrics\nand human evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 00:43:39 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Huang", "Yufang", ""], ["Zhu", "Wentao", ""], ["Xiong", "Deyi", ""], ["Zhang", "Yiye", ""], ["Hu", "Changjian", ""], ["Xu", "Feiyu", ""]]}, {"id": "2010.00747", "submitter": "Yuhao Zhang", "authors": "Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning,\n  Curtis P. Langlotz", "title": "Contrastive Learning of Medical Visual Representations from Paired\n  Images and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning visual representations of medical images is core to medical image\nunderstanding but its progress has been held back by the small size of\nhand-labeled datasets. Existing work commonly relies on transferring weights\nfrom ImageNet pretraining, which is suboptimal due to drastically different\nimage characteristics, or rule-based label extraction from the textual report\ndata paired with medical images, which is inaccurate and hard to generalize. We\npropose an alternative unsupervised strategy to learn medical visual\nrepresentations directly from the naturally occurring pairing of images and\ntextual data. Our method of pretraining medical image encoders with the paired\ntext data via a bidirectional contrastive objective between the two modalities\nis domain-agnostic, and requires no additional expert input. We test our method\nby transferring our pretrained weights to 4 medical image classification tasks\nand 2 zero-shot retrieval tasks, and show that our method leads to image\nrepresentations that considerably outperform strong baselines in most settings.\nNotably, in all 4 classification tasks, our method requires only 10% as much\nlabeled training data as an ImageNet initialized counterpart to achieve better\nor comparable performance, demonstrating superior data efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 02:10:18 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Yuhao", ""], ["Jiang", "Hang", ""], ["Miura", "Yasuhide", ""], ["Manning", "Christopher D.", ""], ["Langlotz", "Curtis P.", ""]]}, {"id": "2010.00760", "submitter": "Jack FitzGerald", "authors": "Jack G. M. FitzGerald", "title": "STIL -- Simultaneous Slot Filling, Translation, Intent Classification,\n  and Language Identification: Initial Results using mBART on MultiATIS++", "comments": "4 pages; To be published at AACL 2020; For code, see:\n  https://github.com/jgmfitz/stil-mbart-multiatispp-aacl2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slot-filling, Translation, Intent classification, and Language\nidentification, or STIL, is a newly-proposed task for multilingual Natural\nLanguage Understanding (NLU). By performing simultaneous slot filling and\ntranslation into a single output language (English in this case), some portion\nof downstream system components can be monolingual, reducing development and\nmaintenance cost. Results are given using the multilingual BART model (Liu et\nal., 2020) fine-tuned on 7 languages using the MultiATIS++ dataset. When no\ntranslation is performed, mBART's performance is comparable to the current\nstate of the art system (Cross-Lingual BERT by Xu et al. (2020)) for the\nlanguages tested, with better average intent classification accuracy (96.07%\nversus 95.50%) but worse average slot F1 (89.87% versus 90.81%). When\nsimultaneous translation is performed, average intent classification accuracy\ndegrades by only 1.7% relative and average slot F1 degrades by only 1.2%\nrelative.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 03:09:26 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["FitzGerald", "Jack G. M.", ""]]}, {"id": "2010.00761", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Suma Bhat, Pramod Viswanath", "title": "Enriching Word Embeddings with Temporal and Spatial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The meaning of a word is closely linked to sociocultural factors that can\nchange over time and location, resulting in corresponding meaning changes.\nTaking a global view of words and their meanings in a widely used language,\nsuch as English, may require us to capture more refined semantics for use in\ntime-specific or location-aware situations, such as the study of cultural\ntrends or language use. However, popular vector representations for words do\nnot adequately include temporal or spatial information. In this work, we\npresent a model for learning word representation conditioned on time and\nlocation. In addition to capturing meaning changes over time and location, we\nrequire that the resulting word embeddings retain salient semantic and\ngeometric properties. We train our model on time- and location-stamped corpora,\nand show using both quantitative and qualitative evaluations that it can\ncapture semantics across time and locations. We note that our model compares\nfavorably with the state-of-the-art for time-specific embedding, and serves as\na new benchmark for location-specific embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 03:15:03 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gong", "Hongyu", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "2010.00767", "submitter": "Heng Yang", "authors": "Heng Yang, Biqing Zeng", "title": "Enhancing Fine-grained Sentiment Classification Exploiting Local Context\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target-oriented sentiment classification is a fine-grained task of natural\nlanguage processing to analyze the sentiment polarity of the targets. To\nimprove the performance of sentiment classification, many approaches proposed\nvarious attention mechanisms to capture the important context words of a\ntarget. However, previous approaches ignored the significant relatedness of a\ntarget's sentiment and its local context. This paper proposes a local\ncontext-aware network (LCA-Net), equipped with the local context embedding and\nlocal context prediction loss, to strengthen the model by emphasizing the\nsentiment information of the local context. The experimental results on three\ncommon datasets show that local context-aware network performs superior to\nexisting approaches in extracting local context features. Besides, the local\ncontext-aware framework is easy to adapt to many models, with the potential to\nimprove other target-level tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 03:54:37 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 02:21:01 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 10:58:36 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yang", "Heng", ""], ["Zeng", "Biqing", ""]]}, {"id": "2010.00784", "submitter": "Kristjan Arumae", "authors": "Kristjan Arumae, Qing Sun, and Parminder Bhatia", "title": "An Empirical Investigation Towards Efficient Multi-Domain Language Model\n  Pre-training", "comments": "arXiv admin note: text overlap with arXiv:2004.03794", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training large language models has become a standard in the natural\nlanguage processing community. Such models are pre-trained on generic data\n(e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the\nsame domain. However, in order to achieve state-of-the-art performance on out\nof domain tasks such as clinical named entity recognition and relation\nextraction, additional in domain pre-training is required. In practice, staged\nmulti-domain pre-training presents performance deterioration in the form of\ncatastrophic forgetting (CF) when evaluated on a generic benchmark such as\nGLUE. In this paper we conduct an empirical investigation into known methods to\nmitigate CF. We find that elastic weight consolidation provides best overall\nscores yielding only a 0.33% drop in performance across seven generic tasks\nwhile remaining competitive in bio-medical tasks. Furthermore, we explore\ngradient and latent clustering based data selection techniques to improve\ncoverage when using elastic weight consolidation and experience replay methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 09:20:18 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Arumae", "Kristjan", ""], ["Sun", "Qing", ""], ["Bhatia", "Parminder", ""]]}, {"id": "2010.00796", "submitter": "Chenguang Zhu", "authors": "Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng", "title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) contain rich information about world knowledge,\nentities and relations. Thus, they can be great supplements to existing\npre-trained language models. However, it remains a challenge to efficiently\nintegrate information from KG into language modeling. And the understanding of\na knowledge graph requires related context. We propose a novel joint\npre-training framework, JAKET, to model both the knowledge graph and language.\nThe knowledge module and language module provide essential information to\nmutually assist each other: the knowledge module produces embeddings for\nentities in text while the language module generates context-aware initial\nembeddings for entities and relations in the graph. Our design enables the\npre-trained model to easily adapt to unseen knowledge graphs in new domains.\nExperimental results on several knowledge-aware NLP tasks show that our\nproposed framework achieves superior performance by effectively leveraging\nknowledge in language understanding.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:53:36 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yu", "Donghan", ""], ["Zhu", "Chenguang", ""], ["Yang", "Yiming", ""], ["Zeng", "Michael", ""]]}, {"id": "2010.00840", "submitter": "Peng Xu", "authors": "Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung,\n  Anima Anandkumar and Bryan Catanzaro", "title": "MEGATRON-CNTRL: Controllable Story Generation with External Knowledge\n  Using Large-Scale Language Models", "comments": "Accepted in EMNLP 2020 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing pre-trained large language models have shown unparalleled generative\ncapabilities. However, they are not controllable. In this paper, we propose\nMEGATRON-CNTRL, a novel framework that uses large-scale language models and\nadds control to text generation by incorporating an external knowledge base.\nOur framework consists of a keyword predictor, a knowledge retriever, a\ncontextual knowledge ranker, and a conditional text generator. As we do not\nhave access to ground-truth supervision for the knowledge ranker, we make use\nof weak supervision from sentence embedding. The empirical results show that\nour model generates more fluent, consistent, and coherent stories with less\nrepetition and higher diversity compared to prior work on the ROC story\ndataset. We showcase the controllability of our model by replacing the keywords\nused to generate stories and re-running the generation process. Human\nevaluation results show that 77.5% of these stories are successfully controlled\nby the new keywords. Furthermore, by scaling our model from 124 million to 8.3\nbillion parameters we demonstrate that larger models improve both the quality\nof generation (from 74.5% to 93.0% for consistency) and controllability (from\n77.5% to 91.5%).\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:07:12 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Xu", "Peng", ""], ["Patwary", "Mostofa", ""], ["Shoeybi", "Mohammad", ""], ["Puri", "Raul", ""], ["Fung", "Pascale", ""], ["Anandkumar", "Anima", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2010.00854", "submitter": "Patrick Xia", "authors": "Patrick Xia, Shijie Wu, Benjamin Van Durme", "title": "Which *BERT? A Survey Organizing Contextualized Encoders", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained contextualized text encoders are now a staple of the NLP\ncommunity. We present a survey on language representation learning with the aim\nof consolidating a series of shared lessons learned across a variety of recent\nefforts. While significant advancements continue at a rapid pace, we find that\nenough has now been discovered, in different directions, that we can begin to\norganize advances according to common themes. Through this organization, we\nhighlight important considerations when interpreting recent contributions and\nchoosing which model to use.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:34:34 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Xia", "Patrick", ""], ["Wu", "Shijie", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2010.00857", "submitter": "Vani Kanjirangat", "authors": "K Vani, Sandra Mitrovic, Alessandro Antonucci, Fabio Rinaldi", "title": "SST-BERT at SemEval-2020 Task 1: Semantic Shift Tracing by Clustering in\n  BERT-based Embedding Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical semantic change detection (also known as semantic shift tracing) is a\ntask of identifying words that have changed their meaning over time.\nUnsupervised semantic shift tracing, focal point of SemEval2020, is\nparticularly challenging. Given the unsupervised setup, in this work, we\npropose to identify clusters among different occurrences of each target word,\nconsidering these as representatives of different word meanings. As such,\ndisagreements in obtained clusters naturally allow to quantify the level of\nsemantic shift per each target word in four target languages. To leverage this\nidea, clustering is performed on contextualized (BERT-based) embeddings of word\noccurrences. The obtained results show that our approach performs well both\nmeasured separately (per language) and overall, where we surpass all provided\nSemEval baselines.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:38:40 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Vani", "K", ""], ["Mitrovic", "Sandra", ""], ["Antonucci", "Alessandro", ""], ["Rinaldi", "Fabio", ""]]}, {"id": "2010.00860", "submitter": "Claire N\\'edellec", "authors": "Claire N\\'edellec, Wiktoria Golik, Sophie Aubin, Robert Bossy", "title": "Building Large Lexicalized Ontologies from Text: a Use Case in Automatic\n  Indexing of Biotechnology Patents", "comments": null, "journal-ref": "International Conference on Knowledge Engineering and Knowledge\n  Management. EKAW 2010. Lecture Notes in Computer Science, vol 6317. (pp.\n  514-523) Springer, Berlin, Heidelberg", "doi": "10.1007/978-3-642-16438-5_41", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a tool, TyDI, and methods experimented in the building of\na termino-ontology, i.e. a lexicalized ontology aimed at fine-grained\nindexation for semantic search applications. TyDI provides facilities for\nknowledge engineers and domain experts to efficiently collaborate to validate,\norganize and conceptualize corpus extracted terms. A use case on biotechnology\npatent search demonstrates TyDI's potential.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:42:56 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["N\u00e9dellec", "Claire", ""], ["Golik", "Wiktoria", ""], ["Aubin", "Sophie", ""], ["Bossy", "Robert", ""]]}, {"id": "2010.00904", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni", "title": "Autoregressive Entity Retrieval", "comments": "Accepted (spotlight) at International Conference on Learning\n  Representations (ICLR) 2021. Code at\n  https://github.com/facebookresearch/GENRE. 20 pages, 9 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entities are at the center of how we represent and aggregate knowledge. For\ninstance, Encyclopedias such as Wikipedia are structured by entities (e.g., one\nper Wikipedia article). The ability to retrieve such entities given a query is\nfundamental for knowledge-intensive tasks such as entity linking and\nopen-domain question answering. Current approaches can be understood as\nclassifiers among atomic labels, one for each entity. Their weight vectors are\ndense entity representations produced by encoding entity meta information such\nas their descriptions. This approach has several shortcomings: (i) context and\nentity affinity is mainly captured through a vector dot product, potentially\nmissing fine-grained interactions; (ii) a large memory footprint is needed to\nstore dense representations when considering large entity sets; (iii) an\nappropriately hard set of negative data has to be subsampled at training time.\nIn this work, we propose GENRE, the first system that retrieves entities by\ngenerating their unique names, left to right, token-by-token in an\nautoregressive fashion. This mitigates the aforementioned technical issues\nsince: (i) the autoregressive formulation directly captures relations between\ncontext and entity name, effectively cross encoding both; (ii) the memory\nfootprint is greatly reduced because the parameters of our encoder-decoder\narchitecture scale with vocabulary size, not entity count; (iii) the softmax\nloss is computed without subsampling negative data. We experiment with more\nthan 20 datasets on entity disambiguation, end-to-end entity linking and\ndocument retrieval tasks, achieving new state-of-the-art or very competitive\nresults while using a tiny fraction of the memory footprint of competing\nsystems. Finally, we demonstrate that new entities can be added by simply\nspecifying their names. Code and pre-trained models at\nhttps://github.com/facebookresearch/GENRE.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:13:31 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 15:20:52 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 07:21:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["De Cao", "Nicola", ""], ["Izacard", "Gautier", ""], ["Riedel", "Sebastian", ""], ["Petroni", "Fabio", ""]]}, {"id": "2010.00910", "submitter": "Fei Mi", "authors": "Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang and Boi Faltings", "title": "Continual Learning for Natural Language Generation in Task-oriented\n  Dialog Systems", "comments": "Accepted as Long Paper at \"Findgins of EMNLP, 2020\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language generation (NLG) is an essential component of task-oriented\ndialog systems. Despite the recent success of neural approaches for NLG, they\nare typically developed in an offline manner for particular domains. To better\nfit real-life applications where new data come in a stream, we study NLG in a\n\"continual learning\" setting to expand its knowledge to new domains or\nfunctionalities incrementally. The major challenge towards this goal is\ncatastrophic forgetting, meaning that a continually trained model tends to\nforget the knowledge it has learned before. To this end, we propose a method\ncalled ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying\nprioritized historical exemplars, together with an adaptive regularization\ntechnique based on ElasticWeight Consolidation. Extensive experiments to\ncontinually learn new domains and intents are conducted on MultiWoZ-2.0 to\nbenchmark ARPER with a wide range of techniques. Empirical results demonstrate\nthat ARPER significantly outperforms other methods by effectively mitigating\nthe detrimental catastrophic forgetting issue.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:32:29 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Mi", "Fei", ""], ["Chen", "Liangwei", ""], ["Zhao", "Mengjie", ""], ["Huang", "Minlie", ""], ["Faltings", "Boi", ""]]}, {"id": "2010.00980", "submitter": "Andreas R\\\"uckl\\'e", "authors": "Andreas R\\\"uckl\\'e, Jonas Pfeiffer, Iryna Gurevych", "title": "MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on\n  a Massive Scale", "comments": "EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the zero-shot transfer capabilities of text matching models on a\nmassive scale, by self-supervised training on 140 source domains from community\nquestion answering forums in English. We investigate the model performances on\nnine benchmarks of answer selection and question similarity tasks, and show\nthat all 140 models transfer surprisingly well, where the large majority of\nmodels substantially outperforms common IR baselines. We also demonstrate that\nconsidering a broad selection of source domains is crucial for obtaining the\nbest zero-shot transfer performances, which contrasts the standard procedure\nthat merely relies on the largest and most similar domains. In addition, we\nextensively study how to best combine multiple source domains. We propose to\nincorporate self-supervised with supervised multi-task learning on all\navailable source domains. Our best zero-shot transfer model considerably\noutperforms in-domain BERT and the previous state of the art on six benchmarks.\nFine-tuning of our model with in-domain data results in additional large gains\nand achieves the new state of the art on all nine benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:22:12 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["R\u00fcckl\u00e9", "Andreas", ""], ["Pfeiffer", "Jonas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.01029", "submitter": "Chengjin Xu", "authors": "Chengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi,\n  Jens Lehmann", "title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation", "comments": "This paper is accepted by COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, there has been a surge of interest in learning\nrepresentations of entitiesand relations in knowledge graph (KG). However, the\nrecent availability of temporal knowledgegraphs (TKGs) that contain time\ninformation for each fact created the need for reasoning overtime in such TKGs.\nIn this regard, we present a new approach of TKG embedding, TeRo, which defines\nthe temporal evolution of entity embedding as a rotation from the initial time\nto the currenttime in the complex vector space. Specially, for facts involving\ntime intervals, each relation isrepresented as a pair of dual complex\nembeddings to handle the beginning and the end of therelation, respectively. We\nshow our proposed model overcomes the limitations of the existing KG embedding\nmodels and TKG embedding models and has the ability of learning and\ninferringvarious relation patterns over time. Experimental results on four\ndifferent TKGs show that TeRo significantly outperforms existing\nstate-of-the-art models for link prediction. In addition, we analyze the effect\nof time granularity on link prediction over TKGs, which as far as we know\nhasnot been investigated in previous literature.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:35:27 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 22:42:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xu", "Chengjin", ""], ["Nayyeri", "Mojtaba", ""], ["Alkhoury", "Fouad", ""], ["Yazdi", "Hamed Shariat", ""], ["Lehmann", "Jens", ""]]}, {"id": "2010.01054", "submitter": "Eric Malmi", "authors": "Eric Malmi, Aliaksei Severyn, Sascha Rothe", "title": "Unsupervised Text Style Transfer with Padded Masked Language Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Masker, an unsupervised text-editing method for style transfer. To\ntackle cases when no parallel source-target pairs are available, we train\nmasked language models (MLMs) for both the source and the target domain. Then\nwe find the text spans where the two models disagree the most in terms of\nlikelihood. This allows us to identify the source tokens to delete to transform\nthe source text to match the style of the target domain. The deleted tokens are\nreplaced with the target MLM, and by using a padded MLM variant, we avoid\nhaving to predetermine the number of inserted tokens. Our experiments on\nsentence fusion and sentiment transfer demonstrate that Masker performs\ncompetitively in a fully unsupervised setting. Moreover, in low-resource\nsettings, it improves supervised methods' accuracy by over 10 percentage points\nwhen pre-training them on silver training data generated by Masker.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:33:42 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Malmi", "Eric", ""], ["Severyn", "Aliaksei", ""], ["Rothe", "Sascha", ""]]}, {"id": "2010.01057", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji\n  Matsumoto", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware\n  Self-attention", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:38:03 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yamada", "Ikuya", ""], ["Asai", "Akari", ""], ["Shindo", "Hiroyuki", ""], ["Takeda", "Hideaki", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "2010.01061", "submitter": "Nils Rethmeier", "authors": "Nils Rethmeier and Isabelle Augenstein", "title": "Data-Efficient Pretraining via Contrastive Self-Supervision", "comments": "Majorly reworked version. Comparison to a large-scale RoBERTa model\n  added. Focus on learning efficiency comparison to self and RoBERTa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For natural language processing `text-to-text' tasks, the prevailing\napproaches heavily rely on pretraining large self-supervised models on\nincreasingly larger `task-external' data. Transfer learning from high-resource\npretraining works well, but research has focused on settings with very large\ndata and compute requirements, while the potential of efficient low-resource\nlearning, without large `task-external' pretraining, remains under-explored. In\nthis work, we evaluate against three core challenges for resource efficient\nlearning. Namely, we analyze: (1) pretraining data ($X$) efficiency; (2) zero\nto few-shot label ($Y$) efficiency; and (3) long-tail generalization, since\nlong-tail preservation has been linked to algorithmic fairness and because data\nin the tail is limited by definition. To address these challenges, we propose a\ndata and compute efficient self-supervised, contrastive text encoder,\npretrained on 60MB of `task-internal' text data, and compare it to RoBERTa,\nwhich was pretrained on 160GB of `task-external' text. We find our method\noutperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa's\nfine-tuning time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:41:57 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 18:24:13 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 18:28:21 GMT"}, {"version": "v4", "created": "Thu, 15 Apr 2021 15:16:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rethmeier", "Nils", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2010.01063", "submitter": "Tomasz Limisiewicz", "authors": "Tomasz Limisiewicz and David Mare\\v{c}ek", "title": "Syntax Representation in Word Embeddings and Neural Networks -- A Survey", "comments": null, "journal-ref": "Proceedings of the 20th Conference ITAT 2020: Automata, Formal and\n  Natural Languages Workshop", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks trained on natural language processing tasks capture syntax\neven though it is not provided as a supervision signal. This indicates that\nsyntactic analysis is essential to the understating of language in artificial\nintelligence systems. This overview paper covers approaches of evaluating the\namount of syntactic information included in the representations of words for\ndifferent neural network architectures. We mainly summarize re-search on\nEnglish monolingual data on language modeling tasks and multilingual data for\nneural machine translation systems and multilingual language models. We\ndescribe which pre-trained models and representations of language are best\nsuited for transfer to syntactic tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:44:58 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Limisiewicz", "Tomasz", ""], ["Mare\u010dek", "David", ""]]}, {"id": "2010.01080", "submitter": "Moritz Wolf", "authors": "Moritz Wolf, Dana Ruiter, Ashwin Geet D'Sa, Liane Reiners, Jan\n  Alexandersson, Dietrich Klakow", "title": "HUMAN: Hierarchical Universal Modular ANnotator", "comments": "7 pages, 4 figures, EMNLP - Demonstrations 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of real-world phenomena are complex and cannot be captured by single\ntask annotations. This causes a need for subsequent annotations, with\ninterdependent questions and answers describing the nature of the subject at\nhand. Even in the case a phenomenon is easily captured by a single task, the\nhigh specialisation of most annotation tools can result in having to switch to\nanother tool if the task only slightly changes.\n  We introduce HUMAN, a novel web-based annotation tool that addresses the\nabove problems by a) covering a variety of annotation tasks on both textual and\nimage data, and b) the usage of an internal deterministic state machine,\nallowing the researcher to chain different annotation tasks in an\ninterdependent manner. Further, the modular nature of the tool makes it easy to\ndefine new annotation tasks and integrate machine learning algorithms e.g., for\nactive learning. HUMAN comes with an easy-to-use graphical user interface that\nsimplifies the annotation task and management.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:20:30 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Wolf", "Moritz", ""], ["Ruiter", "Dana", ""], ["D'Sa", "Ashwin Geet", ""], ["Reiners", "Liane", ""], ["Alexandersson", "Jan", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.01082", "submitter": "Kurt Shuster", "authors": "Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston", "title": "Multi-Modal Open-Domain Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in open-domain conversational agents has demonstrated that\nsignificant improvements in model engagingness and humanness metrics can be\nachieved via massive scaling in both pre-training data and model size\n(Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build\nagents with human-like abilities, we must expand beyond handling just text. A\nparticularly important topic is the ability to see images and communicate about\nwhat is perceived. With the goal of engaging humans in multi-modal dialogue, we\ninvestigate combining components from state-of-the-art open-domain dialogue\nagents with those from state-of-the-art vision models. We study incorporating\ndifferent image fusion schemes and domain-adaptive pre-training and fine-tuning\nstrategies, and show that our best resulting model outperforms strong existing\nmodels in multi-modal dialogue while simultaneously performing as well as its\npredecessor (text-only) BlenderBot (Roller et al., 2020) in text-based\nconversation. We additionally investigate and incorporate safety components in\nour final model, and show that such efforts do not diminish model performance\nwith respect to engagingness metrics.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:20:39 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Shuster", "Kurt", ""], ["Smith", "Eric Michael", ""], ["Ju", "Da", ""], ["Weston", "Jason", ""]]}, {"id": "2010.01108", "submitter": "Dumitru-Clementin Cercel", "authors": "George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu", "title": "Cross-Lingual Transfer Learning for Complex Word Identification", "comments": "accepted at ICTAI 2020, 7 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex Word Identification (CWI) is a task centered on detecting\nhard-to-understand words, or groups of words, in texts from different areas of\nexpertise. The purpose of CWI is to highlight problematic structures that\nnon-native speakers would usually find difficult to understand. Our approach\nuses zero-shot, one-shot, and few-shot learning techniques, alongside\nstate-of-the-art solutions for Natural Language Processing (NLP) tasks (i.e.,\nTransformers). Our aim is to provide evidence that the proposed models can\nlearn the characteristics of complex words in a multilingual environment by\nrelying on the CWI shared task 2018 dataset available for four different\nlanguages (i.e., English, German, Spanish, and also French). Our approach\nsurpasses state-of-the-art cross-lingual results in terms of macro F1-score on\nEnglish (0.774), German (0.782), and Spanish (0.734) languages, for the\nzero-shot learning scenario. At the same time, our model also outperforms the\nstate-of-the-art monolingual result for German (0.795 macro F1-score).\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 17:09:47 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zaharia", "George-Eduard", ""], ["Cercel", "Dumitru-Clementin", ""], ["Dascalu", "Mihai", ""]]}, {"id": "2010.01150", "submitter": "Xiang Dai", "authors": "Xiang Dai and Sarvnaz Karimi and Ben Hachey and Cecile Paris", "title": "Cost-effective Selection of Pretraining Data: A Case Study of\n  Pretraining BERT on Social Media", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies on domain-specific BERT models show that effectiveness on\ndownstream tasks can be improved when models are pretrained on in-domain data.\nOften, the pretraining data used in these models are selected based on their\nsubject matter, e.g., biology or computer science. Given the range of\napplications using social media text, and its unique language variety, we\npretrain two models on tweets and forum text respectively, and empirically\ndemonstrate the effectiveness of these two resources. In addition, we\ninvestigate how similarity measures can be used to nominate in-domain\npretraining data. We publicly release our pretrained models at\nhttps://bit.ly/35RpTf0.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:06:31 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Dai", "Xiang", ""], ["Karimi", "Sarvnaz", ""], ["Hachey", "Ben", ""], ["Paris", "Cecile", ""]]}, {"id": "2010.01160", "submitter": "Aditi Chaudhary", "authors": "Aditi Chaudhary, Antonios Anastasopoulos, Adithya Pratapa, David R.\n  Mortensen, Zaid Sheikh, Yulia Tsvetkov, Graham Neubig", "title": "Automatic Extraction of Rules Governing Morphological Agreement", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating a descriptive grammar of a language is an indispensable step for\nlanguage documentation and preservation. However, at the same time it is a\ntedious, time-consuming task. In this paper, we take steps towards automating\nthis process by devising an automated framework for extracting a first-pass\ngrammatical specification from raw text in a concise, human- and\nmachine-readable format. We focus on extracting rules describing agreement, a\nmorphosyntactic phenomenon at the core of the grammars of many of the world's\nlanguages. We apply our framework to all languages included in the Universal\nDependencies project, with promising results. Using cross-lingual transfer,\neven with no expert annotations in the language of interest, our framework\nextracts a grammatical specification which is nearly equivalent to those\ncreated with large amounts of gold-standard annotated data. We confirm this\nfinding with human expert evaluations of the rules that our framework produces,\nwhich have an average accuracy of 78%. We release an interface demonstrating\nthe extracted rules at https://neulab.github.io/lase/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:31:45 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 03:30:27 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chaudhary", "Aditi", ""], ["Anastasopoulos", "Antonios", ""], ["Pratapa", "Adithya", ""], ["Mortensen", "David R.", ""], ["Sheikh", "Zaid", ""], ["Tsvetkov", "Yulia", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.01165", "submitter": "Zeljko Kraljevic", "authors": "Zeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz Roguski, Kawsar\n  Noor, Daniel Bean, Aurelie Mascio, Leilei Zhu, Amos A Folarin, Angus Roberts,\n  Rebecca Bendayan, Mark P Richardson, Robert Stewart, Anoop D Shah, Wai Keong\n  Wong, Zina Ibrahim, James T Teo, Richard JB Dobson", "title": "Multi-domain Clinical Natural Language Processing with MedCAT: the\n  Medical Concept Annotation Toolkit", "comments": "Preprint: 27 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) contain large volumes of unstructured text,\nrequiring the application of Information Extraction (IE) technologies to enable\nclinical analysis. We present the open-source Medical Concept Annotation\nToolkit (MedCAT) that provides: a) a novel self-supervised machine learning\nalgorithm for extracting concepts using any concept vocabulary including\nUMLS/SNOMED-CT; b) a feature-rich annotation interface for customising and\ntraining IE models; and c) integrations to the broader CogStack ecosystem for\nvendor-agnostic health system deployment. We show improved performance in\nextracting UMLS concepts from open datasets (F1:0.448-0.738 vs 0.429-0.650).\nFurther real-world validation demonstrates SNOMED-CT extraction at 3 large\nLondon hospitals with self-supervised training over ~8.8B words from ~17M\nclinical records and further fine-tuning with ~6K clinician annotated examples.\nWe show strong transferability (F1 > 0.94) between hospitals, datasets, and\nconcept types indicating cross-domain EHR-agnostic utility for accelerated\nclinical and research use cases.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 19:01:02 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 13:21:50 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kraljevic", "Zeljko", ""], ["Searle", "Thomas", ""], ["Shek", "Anthony", ""], ["Roguski", "Lukasz", ""], ["Noor", "Kawsar", ""], ["Bean", "Daniel", ""], ["Mascio", "Aurelie", ""], ["Zhu", "Leilei", ""], ["Folarin", "Amos A", ""], ["Roberts", "Angus", ""], ["Bendayan", "Rebecca", ""], ["Richardson", "Mark P", ""], ["Stewart", "Robert", ""], ["Shah", "Anoop D", ""], ["Wong", "Wai Keong", ""], ["Ibrahim", "Zina", ""], ["Teo", "James T", ""], ["Dobson", "Richard JB", ""]]}, {"id": "2010.01169", "submitter": "Vineeth Ravi", "authors": "Vineeth Ravi, Selim Amrouni, Andrea Stefanucci, Armineh Nourbakhsh,\n  Prashant Reddy, Manuela Veloso", "title": "DocuBot : Generating financial reports using natural language\n  interactions", "comments": "Accepted at :- AAAI 2021 Workshop on Content Authoring and Design\n  (CAD21) and NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The financial services industry perpetually processes an overwhelming amount\nof complex data. Digital reports are often created based on tedious manual\nanalysis as well as visualization of the underlying trends and characteristics\nof data. Often, the accruing costs of human computation errors in creating\nthese reports are very high. We present DocuBot, a novel AI-powered virtual\nassistant for creating and modifying content in digital documents by modeling\nnatural language interactions as \"skills\" and using them to transform\nunderlying data. DocuBot has the ability to agglomerate saved skills for reuse,\nenabling humans to automatically generate recurrent reports. DocuBot also has\nthe capability to continuously learn domain-specific and user-specific\nvocabulary by interacting with the user. We present evidence that DocuBot adds\nvalue to the financial industry and demonstrate its impact with experiments\ninvolving real and simulated users tasked with creating PowerPoint\npresentations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 19:06:29 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 22:32:51 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ravi", "Vineeth", ""], ["Amrouni", "Selim", ""], ["Stefanucci", "Andrea", ""], ["Nourbakhsh", "Armineh", ""], ["Reddy", "Prashant", ""], ["Veloso", "Manuela", ""]]}, {"id": "2010.01239", "submitter": "Mingda Chen", "authors": "Mingda Chen, Zewei Chu, Karl Stratos, Kevin Gimpel", "title": "Mining Knowledge for Natural Language Inference from Wikipedia\n  Categories", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate lexical entailment (LE) and natural language inference (NLI) often\nrequire large quantities of costly annotations. To alleviate the need for\nlabeled data, we introduce WikiNLI: a resource for improving model performance\non NLI and LE tasks. It contains 428,899 pairs of phrases constructed from\nnaturally annotated category hierarchies in Wikipedia. We show that we can\nimprove strong baselines such as BERT and RoBERTa by pretraining them on\nWikiNLI and transferring the models on downstream tasks. We conduct systematic\ncomparisons with phrases extracted from other knowledge bases such as WordNet\nand Wikidata to find that pretraining on WikiNLI gives the best performance. In\naddition, we construct WikiNLI in other languages, and show that pretraining on\nthem improves performance on NLI tasks of corresponding languages.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 00:45:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Mingda", ""], ["Chu", "Zewei", ""], ["Stratos", "Karl", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2010.01263", "submitter": "Nikolaos Pappas", "authors": "Xuhui Zhou, Nikolaos Pappas, Noah A. Smith", "title": "Multilevel Text Alignment with Cross-Document Attention", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text alignment finds application in tasks such as citation recommendation and\nplagiarism detection. Existing alignment methods operate at a single,\npredefined level and cannot learn to align texts at, for example, sentence and\ndocument levels. We propose a new learning approach that equips previously\nestablished hierarchical attention encoders for representing documents with a\ncross-document attention component, enabling structural comparisons across\ndifferent levels (document-to-document and sentence-to-document). Our component\nis weakly supervised from document pairs and can align at multiple levels. Our\nevaluation on predicting document-to-document relationships and\nsentence-to-document relationships on the tasks of citation recommendation and\nplagiarism detection shows that our approach outperforms previously established\nhierarchical, attention encoders based on recurrent and transformer\ncontextualization that are unaware of structural correspondence between\ndocuments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 02:52:28 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhou", "Xuhui", ""], ["Pappas", "Nikolaos", ""], ["Smith", "Noah A.", ""]]}, {"id": "2010.01268", "submitter": "Zihao Fu", "authors": "Zihao Fu, Bei Shi, Wai Lam, Lidong Bing, Zhiyuan Liu", "title": "Partially-Aligned Data-to-Text Generation with Distant Supervision", "comments": "To appear EMNLP 2020. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Data-to-Text task aims to generate human-readable text for describing\nsome given structured data enabling more interpretability. However, the typical\ngeneration task is confined to a few particular domains since it requires\nwell-aligned data which is difficult and expensive to obtain. Using\npartially-aligned data is an alternative way of solving the dataset scarcity\nproblem. This kind of data is much easier to obtain since it can be produced\nautomatically. However, using this kind of data induces the over-generation\nproblem posing difficulties for existing models, which tends to add unrelated\nexcerpts during the generation procedure. In order to effectively utilize\nautomatically annotated partially-aligned datasets, we extend the traditional\ngeneration task to a refined task called Partially-Aligned Data-to-Text\nGeneration (PADTG) which is more practical since it utilizes automatically\nannotated data for training and thus considerably expands the application\ndomains. To tackle this new task, we propose a novel distant supervision\ngeneration framework. It firstly estimates the input data's supportiveness for\neach target word with an estimator and then applies a supportiveness adaptor\nand a rebalanced beam search to harness the over-generation problem in the\ntraining and generation phases respectively. We also contribute a\npartially-aligned dataset (The data and source code of this paper can be\nobtained from https://github.com/fuzihaofzh/distant_supervision_nlg by sampling\nsentences from Wikipedia and automatically extracting corresponding KB triples\nfor each sentence from Wikidata. The experimental results show that our\nframework outperforms all baseline models as well as verify the feasibility of\nutilizing partially-aligned data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 03:18:52 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Fu", "Zihao", ""], ["Shi", "Bei", ""], ["Lam", "Wai", ""], ["Bing", "Lidong", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2010.01272", "submitter": "Mucheng Ren", "authors": "Mucheng Ren, Xiubo Geng, Tao Qin, Heyan Huang, Daxin Jiang", "title": "Towards Interpretable Reasoning over Paragraph Effects in Situation", "comments": "14 pages. Accepted as EMNLP2020 Long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the task of reasoning over paragraph effects in situation, which\nrequires a model to understand the cause and effect described in a background\nparagraph, and apply the knowledge to a novel situation. Existing works ignore\nthe complicated reasoning process and solve it with a one-step \"black box\"\nmodel. Inspired by human cognitive processes, in this paper we propose a\nsequential approach for this task which explicitly models each step of the\nreasoning process with neural network modules. In particular, five reasoning\nmodules are designed and learned in an end-to-end manner, which leads to a more\ninterpretable model. Experimental results on the ROPES dataset demonstrate the\neffectiveness and explainability of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 04:03:52 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ren", "Mucheng", ""], ["Geng", "Xiubo", ""], ["Qin", "Tao", ""], ["Huang", "Heyan", ""], ["Jiang", "Daxin", ""]]}, {"id": "2010.01288", "submitter": "Jiahui Gao", "authors": "Jiahui Gao, Yi Zhou, Philip L. H. Yu, Shafiq Joty and Jiuxiang Gu", "title": "Unsupervised Cross-lingual Image Captioning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in image captioning has mostly focused on English because of the\navailability of image-caption paired datasets in this language. However,\nbuilding vision-language systems only for English deprives a large part of the\nworld population of AI technologies' benefit. On the other hand, creating\nimage-caption paired datasets for every target language is expensive. In this\nwork, we present a novel unsupervised cross-lingual method to generate image\ncaptions in a target language without using any image-caption corpus in the\nsource or target languages. Our method relies on (i) a cross-lingual scene\ngraph to sentence translation process, which learns to decode sentences in the\ntarget language from a cross-lingual encoding space of scene graphs using a\nsentence parallel (bitext) corpus, and (ii) an unsupervised cross-modal feature\nmapping which seeks to map an encoded scene graph features from image modality\nto language modality. We verify the effectiveness of our proposed method on the\nChinese image caption generation task. The comparisons against several existing\nmethods demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 06:14:06 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:11:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gao", "Jiahui", ""], ["Zhou", "Yi", ""], ["Yu", "Philip L. H.", ""], ["Joty", "Shafiq", ""], ["Gu", "Jiuxiang", ""]]}, {"id": "2010.01309", "submitter": "Amirmohammad Kazemeini", "authors": "Amirmohammad Kazameini, Samin Fatehi, Yash Mehta, Sauleh Eetemadi,\n  Erik Cambria", "title": "Personality Trait Detection Using Bagged SVM over BERT Word Embedding\n  Ensembles", "comments": null, "journal-ref": "Proceedings of the The Fourth Widening Natural Language Processing\n  Workshop (2020)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the automatic prediction of personality traits has received\nincreasing attention and has emerged as a hot topic within the field of\naffective computing. In this work, we present a novel deep learning-based\napproach for automated personality detection from text. We leverage state of\nthe art advances in natural language understanding, namely the BERT language\nmodel to extract contextualized word embeddings from textual data for automated\nauthor personality detection. Our primary goal is to develop a computationally\nefficient, high-performance personality prediction model which can be easily\nused by a large number of people without access to huge computation resources.\nOur extensive experiments with this ideology in mind, led us to develop a novel\nmodel which feeds contextualized embeddings along with psycholinguistic\nfeatures toa Bagged-SVM classifier for personality trait prediction. Our model\noutperforms the previous state of the art by 1.04% and, at the same time is\nsignificantly more computationally efficient to train. We report our results on\nthe famous gold standard Essays dataset for personality detection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 09:25:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kazameini", "Amirmohammad", ""], ["Fatehi", "Samin", ""], ["Mehta", "Yash", ""], ["Eetemadi", "Sauleh", ""], ["Cambria", "Erik", ""]]}, {"id": "2010.01345", "submitter": "Zhao Meng", "authors": "Zhao Meng, Roger Wattenhofer", "title": "A Geometry-Inspired Attack for Generating Natural Language Adversarial\n  Examples", "comments": "COLING 2020 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating adversarial examples for natural language is hard, as natural\nlanguage consists of discrete symbols, and examples are often of variable\nlengths. In this paper, we propose a geometry-inspired attack for generating\nnatural language adversarial examples. Our attack generates adversarial\nexamples by iteratively approximating the decision boundary of Deep Neural\nNetworks (DNNs). Experiments on two datasets with two different models show\nthat our attack fools natural language models with high success rates, while\nonly replacing a few words. Human evaluation shows that adversarial examples\ngenerated by our attack are hard for humans to recognize. Further experiments\nshow that adversarial training can improve model robustness against our attack.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 12:58:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Meng", "Zhao", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2010.01410", "submitter": "Hariharan Sezhiyan", "authors": "David Gros, Hariharan Sezhiyan, Prem Devanbu, Zhou Yu", "title": "Code to Comment \"Translation\": Data, Metrics, Baselining & Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship of comments to code, and in particular, the task of\ngenerating useful comments given the code, has long been of interest. The\nearliest approaches have been based on strong syntactic theories of\ncomment-structures, and relied on textual templates. More recently, researchers\nhave applied deep learning methods to this task, and specifically, trainable\ngenerative translation models which are known to work very well for Natural\nLanguage translation (e.g., from German to English). We carefully examine the\nunderlying assumption here: that the task of generating comments sufficiently\nresembles the task of translating between natural languages, and so similar\nmodels and evaluation metrics could be used. We analyze several recent\ncode-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We\ncompare them with WMT19, a standard dataset frequently used to train state of\nthe art natural language translators. We found some interesting differences\nbetween the code-comment data and the WMT19 natural language data. Next, we\ndescribe and conduct some studies to calibrate BLEU (which is commonly used as\na measure of comment quality). using \"affinity pairs\" of methods, from\ndifferent projects, in the same project, in the same class, etc; Our study\nsuggests that the current performance on some datasets might need to be\nimproved substantially. We also argue that fairly naive information retrieval\n(IR) methods do well enough at this task to be considered a reasonable\nbaseline. Finally, we make some suggestions on how our findings might be used\nin future research in this area.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 18:57:26 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gros", "David", ""], ["Sezhiyan", "Hariharan", ""], ["Devanbu", "Prem", ""], ["Yu", "Zhou", ""]]}, {"id": "2010.01417", "submitter": "Kun Xu", "authors": "Kun Xu and Haochen Tan and Linfeng Song and Han Wu and Haisong Zhang\n  and Linqi Song and Dong Yu", "title": "Semantic Role Labeling Guided Multi-turn Dialogue ReWriter", "comments": "To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For multi-turn dialogue rewriting, the capacity of effectively modeling the\nlinguistic knowledge in dialog context and getting rid of the noises is\nessential to improve its performance. Existing attentive models attend to all\nwords without prior focus, which results in inaccurate concentration on some\ndispensable words. In this paper, we propose to use semantic role labeling\n(SRL), which highlights the core semantic information of who did what to whom,\nto provide additional guidance for the rewriter model. Experiments show that\nthis information significantly improves a RoBERTa-based model that already\noutperforms previous state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 19:50:04 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Xu", "Kun", ""], ["Tan", "Haochen", ""], ["Song", "Linfeng", ""], ["Wu", "Han", ""], ["Zhang", "Haisong", ""], ["Song", "Linqi", ""], ["Yu", "Dong", ""]]}, {"id": "2010.01429", "submitter": "Krenare Pireva Nuci", "authors": "Rinor Hajrizi and Krenare Pireva Nu\\c{c}i", "title": "Aspect-Based Sentiment Analysis in Education Domain", "comments": "Sentiment Analysis, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of a large amount of data has always brought value to institutions\nand organizations. Lately, people's opinions expressed through text have become\na very important aspect of this analysis. In response to this challenge, a\nnatural language processing technique known as Aspect-Based Sentiment Analysis\n(ABSA) has emerged. Having the ability to extract the polarity for each aspect\nof opinions separately, ABSA has found itself useful in a wide range of\ndomains. Education is one of the domains in which ABSA can be successfully\nutilized. Being able to understand and find out what students like and don't\nlike most about a course, professor, or teaching methodology can be of great\nimportance for the respective institutions. While this task represents a unique\nNLP challenge, many studies have proposed different approaches to tackle the\nproblem. In this work, we present a comprehensive review of the existing work\nin ABSA with a focus in the education domain. A wide range of methodologies are\ndiscussed and conclusions are drawn.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 21:51:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hajrizi", "Rinor", ""], ["Nu\u00e7i", "Krenare Pireva", ""]]}, {"id": "2010.01447", "submitter": "Shiquan Yang", "authors": "Shiquan Yang, Rui Zhang, Sarah Erfani", "title": "GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented\n  Dialogue Systems", "comments": "11 pages, 5 figures, Accepted as an EMNLP 2020 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end task-oriented dialogue systems aim to generate system responses\ndirectly from plain text inputs. There are two challenges for such systems: one\nis how to effectively incorporate external knowledge bases (KBs) into the\nlearning framework; the other is how to accurately capture the semantics of\ndialogue history. In this paper, we address these two challenges by exploiting\nthe graph structural information in the knowledge base and in the dependency\nparsing tree of the dialogue. To effectively leverage the structural\ninformation in dialogue history, we propose a new recurrent cell architecture\nwhich allows representation learning on graphs. To exploit the relations\nbetween entities in KBs, the model combines multi-hop reasoning ability based\non the graph structure. Experimental results show that the proposed model\nachieves consistent improvement over state-of-the-art models on two different\ntask-oriented dialogue datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 00:04:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yang", "Shiquan", ""], ["Zhang", "Rui", ""], ["Erfani", "Sarah", ""]]}, {"id": "2010.01450", "submitter": "Kexin Huang", "authors": "Yue Yu, Kexin Huang, Chao Zhang, Lucas M. Glass, Jimeng Sun, and Cao\n  Xiao", "title": "SumGNN: Multi-typed Drug Interaction Prediction via Efficient Knowledge\n  Graph Summarization", "comments": "Published in Bioinformatics 2021", "journal-ref": null, "doi": "10.1093/bioinformatics/btab207", "report-no": null, "categories": "cs.LG cs.CL cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the increasing availability of drug-drug interactions (DDI)\ndatasets and large biomedical knowledge graphs (KGs), accurate detection of\nadverse DDI using machine learning models becomes possible. However, it remains\nlargely an open problem how to effectively utilize large and noisy biomedical\nKG for DDI detection. Due to its sheer size and amount of noise in KGs, it is\noften less beneficial to directly integrate KGs with other smaller but higher\nquality data (e.g., experimental data). Most of the existing approaches ignore\nKGs altogether. Some try to directly integrate KGs with other data via graph\nneural networks with limited success. Furthermore, most previous works focus on\nbinary DDI prediction whereas the multi-typed DDI pharmacological effect\nprediction is a more meaningful but harder task. To fill the gaps, we propose a\nnew method SumGNN: knowledge summarization graph neural network, which is\nenabled by a subgraph extraction module that can efficiently anchor on relevant\nsubgraphs from a KG, a self-attention based subgraph summarization scheme to\ngenerate a reasoning path within the subgraph, and a multi-channel knowledge\nand data integration module that utilizes massive external biomedical knowledge\nfor significantly improved multi-typed DDI predictions. SumGNN outperforms the\nbest baseline by up to 5.54\\%, and the performance gain is particularly\nsignificant in low data relation types. In addition, SumGNN provides\ninterpretable prediction via the generated reasoning paths for each prediction.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 00:14:57 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 21:07:42 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yu", "Yue", ""], ["Huang", "Kexin", ""], ["Zhang", "Chao", ""], ["Glass", "Lucas M.", ""], ["Sun", "Jimeng", ""], ["Xiao", "Cao", ""]]}, {"id": "2010.01454", "submitter": "Soujanya Poria", "authors": "Navonil Majumder, Pengfei Hong, Shanshan Peng, Jiankun Lu, Deepanway\n  Ghosal, Alexander Gelbukh, Rada Mihalcea, Soujanya Poria", "title": "MIME: MIMicking Emotions for Empathetic Response Generation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current approaches to empathetic response generation view the set of emotions\nexpressed in the input text as a flat structure, where all the emotions are\ntreated uniformly. We argue that empathetic responses often mimic the emotion\nof the user to a varying degree, depending on its positivity or negativity and\ncontent. We show that the consideration of this polarity-based emotion clusters\nand emotional mimicry results in improved empathy and contextual relevance of\nthe response as compared to the state-of-the-art. Also, we introduce\nstochasticity into the emotion mixture that yields emotionally more varied\nempathetic responses than the previous work. We demonstrate the importance of\nthese factors to empathetic response generation using both automatic- and\nhuman-based evaluations. The implementation of MIME is publicly available at\nhttps://github.com/declare-lab/MIME.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 00:35:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Majumder", "Navonil", ""], ["Hong", "Pengfei", ""], ["Peng", "Shanshan", ""], ["Lu", "Jiankun", ""], ["Ghosal", "Deepanway", ""], ["Gelbukh", "Alexander", ""], ["Mihalcea", "Rada", ""], ["Poria", "Soujanya", ""]]}, {"id": "2010.01461", "submitter": "Yuncong Li", "authors": "Yuncong Li, Cunxiang Yin and Sheng-hua Zhong", "title": "Sentence Constituent-Aware Aspect-Category Sentiment Analysis with Graph\n  Attention Networks", "comments": "Long paper accepted by NLPCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect category sentiment analysis (ACSA) aims to predict the sentiment\npolarities of the aspect categories discussed in sentences. Since a sentence\nusually discusses one or more aspect categories and expresses different\nsentiments toward them, various attention-based methods have been developed to\nallocate the appropriate sentiment words for the given aspect category and\nobtain promising results. However, most of these methods directly use the given\naspect category to find the aspect category-related sentiment words, which may\ncause mismatching between the sentiment words and the aspect categories when an\nunrelated sentiment word is semantically meaningful for the given aspect\ncategory. To mitigate this problem, we propose a Sentence Constituent-Aware\nNetwork (SCAN) for aspect-category sentiment analysis. SCAN contains two graph\nattention modules and an interactive loss function. The graph attention modules\ngenerate representations of the nodes in sentence constituency parse trees for\nthe aspect category detection (ACD) task and the ACSA task, respectively. ACD\naims to detect aspect categories discussed in sentences and is a auxiliary\ntask. For a given aspect category, the interactive loss function helps the ACD\ntask to find the nodes which can predict the aspect category but can't predict\nother aspect categories. The sentiment words in the nodes then are used to\npredict the sentiment polarity of the aspect category by the ACSA task. The\nexperimental results on five public datasets demonstrate the effectiveness of\nSCAN.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 01:23:17 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Yuncong", ""], ["Yin", "Cunxiang", ""], ["Zhong", "Sheng-hua", ""]]}, {"id": "2010.01475", "submitter": "Dayiheng Liu", "authors": "Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Jiancheng Lv,\n  Nan Duan and Ming Zhou", "title": "Tell Me How to Ask Again: Question Data Augmentation with Controllable\n  Rewriting in Continuous Space", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data augmentation method, referred to as\nControllable Rewriting based Question Data Augmentation (CRQDA), for machine\nreading comprehension (MRC), question generation, and question-answering\nnatural language inference tasks. We treat the question data augmentation task\nas a constrained question rewriting problem to generate context-relevant,\nhigh-quality, and diverse question data samples. CRQDA utilizes a Transformer\nautoencoder to map the original discrete question into a continuous embedding\nspace. It then uses a pre-trained MRC model to revise the question\nrepresentation iteratively with gradient-based optimization. Finally, the\nrevised question representations are mapped back into the discrete space, which\nserve as additional question data. Comprehensive experiments on SQuAD 2.0,\nSQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of\nCRQDA\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:13:46 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Dayiheng", ""], ["Gong", "Yeyun", ""], ["Fu", "Jie", ""], ["Yan", "Yu", ""], ["Chen", "Jiusheng", ""], ["Lv", "Jiancheng", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""]]}, {"id": "2010.01480", "submitter": "Junyi Li", "authors": "Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas\n  Jing Yuan and Ji-Rong Wen", "title": "Knowledge-Enhanced Personalized Review Generation with Capsule Graph\n  Neural Network", "comments": "Accepted by CIKM 2020 (Long Paper)", "journal-ref": null, "doi": "10.1145/3340531.3411893", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized review generation (PRG) aims to automatically produce review\ntext reflecting user preference, which is a challenging natural language\ngeneration task. Most of previous studies do not explicitly model factual\ndescription of products, tending to generate uninformative content. Moreover,\nthey mainly focus on word-level generation, but cannot accurately reflect more\nabstractive user preference in multiple aspects. To address the above issues,\nwe propose a novel knowledge-enhanced PRG model based on capsule graph neural\nnetwork~(Caps-GNN). We first construct a heterogeneous knowledge graph (HKG)\nfor utilizing rich item attributes. We adopt Caps-GNN to learn graph capsules\nfor encoding underlying characteristics from the HKG. Our generation process\ncontains two major steps, namely aspect sequence generation and sentence\ngeneration. First, based on graph capsules, we adaptively learn aspect capsules\nfor inferring the aspect sequence. Then, conditioned on the inferred aspect\nlabel, we design a graph-based copy mechanism to generate sentences by\nincorporating related entities or words from HKG. To our knowledge, we are the\nfirst to utilize knowledge graph for the PRG task. The incorporated KG\ninformation is able to enhance user preference at both aspect and word levels.\nExtensive experiments on three real-world datasets have demonstrated the\neffectiveness of our model on the PRG task.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:54:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Junyi", ""], ["Li", "Siqing", ""], ["Zhao", "Wayne Xin", ""], ["He", "Gaole", ""], ["Wei", "Zhicheng", ""], ["Yuan", "Nicholas Jing", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.01482", "submitter": "HaiYing Wang", "authors": "Haim Bar and HaiYing Wang", "title": "Reproducible Science with LaTeX", "comments": null, "journal-ref": "J. data sci. 19(2021), no. 1, (2021) 111-125", "doi": "10.6339/21-JDS998", "report-no": null, "categories": "cs.SE cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a procedure to execute external source codes from a LaTeX\ndocument and include the calculation outputs in the resulting Portable Document\nFormat (pdf) file automatically. It integrates programming tools into the LaTeX\nwriting tool to facilitate the production of reproducible research. In our\nproposed approach to a LaTeX-based scientific notebook the user can easily\ninvoke any programming language or a command-line program when compiling the\nLaTeX document, while using their favorite LaTeX editor in the writing process.\nThe required LaTeX setup, a new Python package, and the defined preamble are\ndiscussed in detail, and working examples using R, Julia, and MatLab to\nreproduce existing research are provided to illustrate the proposed procedure.\nWe also demonstrate how to include system setting information in a paper by\ninvoking shell scripts when compiling the document.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 04:04:07 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 01:54:58 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bar", "Haim", ""], ["Wang", "HaiYing", ""]]}, {"id": "2010.01486", "submitter": "Saadia Gabriel", "authors": "Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz, Ronan Le Bras,\n  Maxwell Forbes, Yejin Choi", "title": "Paragraph-level Commonsense Transformers with Recurrent Memory", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human understanding of narrative texts requires making commonsense inferences\nbeyond what is stated explicitly in the text. A recent model, COMET, can\ngenerate such implicit commonsense inferences along several dimensions such as\npre- and post-conditions, motivations, and mental states of the participants.\nHowever, COMET was trained on commonsense inferences of short phrases, and is\ntherefore discourse-agnostic. When presented with each sentence of a\nmulti-sentence narrative, it might generate inferences that are inconsistent\nwith the rest of the narrative.\n  We present the task of discourse-aware commonsense inference. Given a\nsentence within a narrative, the goal is to generate commonsense inferences\nalong predefined dimensions, while maintaining coherence with the rest of the\nnarrative. Such large-scale paragraph-level annotation is hard to get and\ncostly, so we use available sentence-level annotations to efficiently and\nautomatically construct a distantly supervised corpus.\n  Using this corpus, we train PARA-COMET, a discourse-aware model that\nincorporates paragraph-level information to generate coherent commonsense\ninferences from narratives. PARA-COMET captures both semantic knowledge\npertaining to prior world knowledge, and episodic knowledge involving how\ncurrent events relate to prior and future events in a narrative. Our results\nshow that PARA-COMET outperforms the sentence-level baselines, particularly in\ngenerating inferences that are both coherent and novel.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 05:24:12 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 05:17:31 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gabriel", "Saadia", ""], ["Bhagavatula", "Chandra", ""], ["Shwartz", "Vered", ""], ["Bras", "Ronan Le", ""], ["Forbes", "Maxwell", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.01495", "submitter": "Yifan Gao", "authors": "Yifan Gao, Piji Li, Wei Bi, Xiaojiang Liu, Michael R. Lyu, Irwin King", "title": "Dialogue Generation on Infrequent Sentence Functions via Structured\n  Meta-Learning", "comments": "EMNLP 2020, Findings, 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sentence function is an important linguistic feature indicating the\ncommunicative purpose in uttering a sentence. Incorporating sentence functions\ninto conversations has shown improvements in the quality of generated\nresponses. However, the number of utterances for different types of\nfine-grained sentence functions is extremely imbalanced. Besides a small number\nof high-resource sentence functions, a large portion of sentence functions is\ninfrequent. Consequently, dialogue generation conditioned on these infrequent\nsentence functions suffers from data deficiency. In this paper, we investigate\na structured meta-learning (SML) approach for dialogue generation on infrequent\nsentence functions. We treat dialogue generation conditioned on different\nsentence functions as separate tasks, and apply model-agnostic meta-learning to\nhigh-resource sentence functions data. Furthermore, SML enhances meta-learning\neffectiveness by promoting knowledge customization among different sentence\nfunctions but simultaneously preserving knowledge generalization for similar\nsentence functions. Experimental results demonstrate that SML not only improves\nthe informativeness and relevance of generated responses, but also can generate\nresponses consistent with the target sentence functions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 07:13:36 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gao", "Yifan", ""], ["Li", "Piji", ""], ["Bi", "Wei", ""], ["Liu", "Xiaojiang", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2010.01496", "submitter": "Oana-Maria Camburu", "authors": "Oana-Maria Camburu", "title": "Explaining Deep Neural Networks", "comments": "PhD Thesis, University of Oxford", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are becoming more and more popular due to their\nrevolutionary success in diverse areas, such as computer vision, natural\nlanguage processing, and speech recognition. However, the decision-making\nprocesses of these models are generally not interpretable to users. In various\ndomains, such as healthcare, finance, or law, it is critical to know the\nreasons behind a decision made by an artificial intelligence system. Therefore,\nseveral directions for explaining neural models have recently been explored.\n  In this thesis, I investigate two major directions for explaining deep neural\nnetworks. The first direction consists of feature-based post-hoc explanatory\nmethods, that is, methods that aim to explain an already trained and fixed\nmodel (post-hoc), and that provide explanations in terms of input features,\nsuch as tokens for text and superpixels for images (feature-based). The second\ndirection consists of self-explanatory neural models that generate natural\nlanguage explanations, that is, models that have a built-in module that\ngenerates explanations for the predictions of the model.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 07:23:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Camburu", "Oana-Maria", ""]]}, {"id": "2010.01502", "submitter": "Qi Jia", "authors": "Qi Jia, Yizhu Liu, Siyu Ren, Kenny Q. Zhu, Haifeng Tang", "title": "Multi-turn Response Selection using Dialogue Dependency Relations", "comments": "Accepted for publication as a long paper in EMNLP2020", "journal-ref": "Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-turn response selection is a task designed for developing dialogue\nagents. The performance on this task has a remarkable improvement with\npre-trained language models. However, these models simply concatenate the turns\nin dialogue history as the input and largely ignore the dependencies between\nthe turns. In this paper, we propose a dialogue extraction algorithm to\ntransform a dialogue history into threads based on their dependency relations.\nEach thread can be regarded as a self-contained sub-dialogue. We also propose\nThread-Encoder model to encode threads and candidates into compact\nrepresentations by pre-trained Transformers and finally get the matching score\nthrough an attention layer. The experiments show that dependency relations are\nhelpful for dialogue context understanding, and our model outperforms the\nstate-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results\non UbuntuV2.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:00:19 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jia", "Qi", ""], ["Liu", "Yizhu", ""], ["Ren", "Siyu", ""], ["Zhu", "Kenny Q.", ""], ["Tang", "Haifeng", ""]]}, {"id": "2010.01512", "submitter": "Chen Zhang", "authors": "Chen Zhang, Qiuchi Li, Dawei Song, Benyou Wang", "title": "A Multi-task Learning Framework for Opinion Triplet Extraction", "comments": "10 pages, 4 figures, 3 tables, accepted to EMNLP 2020 Findings. Repo:\n  https://github.com/GeneZC/OTE-MTL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art Aspect-based Sentiment Analysis (ABSA) approaches are\nmainly based on either detecting aspect terms and their corresponding sentiment\npolarities, or co-extracting aspect and opinion terms. However, the extraction\nof aspect-sentiment pairs lacks opinion terms as a reference, while\nco-extraction of aspect and opinion terms would not lead to meaningful pairs\nwithout determining their sentiment dependencies. To address the issue, we\npresent a novel view of ABSA as an opinion triplet extraction task, and propose\na multi-task learning framework to jointly extract aspect terms and opinion\nterms, and simultaneously parses sentiment dependencies between them with a\nbiaffine scorer. At inference phase, the extraction of triplets is facilitated\nby a triplet decoding method based on the above outputs. We evaluate the\nproposed framework on four SemEval benchmarks for ASBA. The results demonstrate\nthat our approach significantly outperforms a range of strong baselines and\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:31:54 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 02:55:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Chen", ""], ["Li", "Qiuchi", ""], ["Song", "Dawei", ""], ["Wang", "Benyou", ""]]}, {"id": "2010.01526", "submitter": "Sahil Shah", "authors": "Sahil Shah, Vihari Piratla, Soumen Chakrabarti, Sunita Sarawagi", "title": "NLP Service APIs and Models for Efficient Registration of New Clients", "comments": "Accepted to Findings of EMNLP, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art NLP inference uses enormous neural architectures and models\ntrained for GPU-months, well beyond the reach of most consumers of NLP. This\nhas led to one-size-fits-all public API-based NLP service models by major AI\ncompanies, serving large numbers of clients. Neither (hardware deficient)\nclients nor (heavily subscribed) servers can afford traditional fine tuning.\nMany clients own little or no labeled data. We initiate a study of adaptation\nof centralized NLP services to clients, and present one practical and\nlightweight approach. Each client uses an unsupervised, corpus-based sketch to\nregister to the service. The server uses an auxiliary network to map the sketch\nto an abstract vector representation, which then informs the main labeling\nnetwork. When a new client registers with its sketch, it gets immediate\naccuracy benefits. We demonstrate the success of the proposed architecture\nusing sentiment labeling, NER, and predictive language modeling\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 09:47:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shah", "Sahil", ""], ["Piratla", "Vihari", ""], ["Chakrabarti", "Soumen", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "2010.01535", "submitter": "Wenjuan Han", "authors": "Wenjuan Han, Yong Jiang, Hwee Tou Ng, Kewei Tu", "title": "A Survey of Unsupervised Dependency Parsing", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic dependency parsing is an important task in natural language\nprocessing. Unsupervised dependency parsing aims to learn a dependency parser\nfrom sentences that have no annotation of their correct parse trees. Despite\nits difficulty, unsupervised parsing is an interesting research direction\nbecause of its capability of utilizing almost unlimited unannotated text data.\nIt also serves as the basis for other research in low-resource parsing. In this\npaper, we survey existing approaches to unsupervised dependency parsing,\nidentify two major classes of approaches, and discuss recent trends. We hope\nthat our survey can provide insights for researchers and facilitate future\nresearch on this topic.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 10:51:22 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Han", "Wenjuan", ""], ["Jiang", "Yong", ""], ["Ng", "Hwee Tou", ""], ["Tu", "Kewei", ""]]}, {"id": "2010.01549", "submitter": "Faria Huq", "authors": "Faria Huq, Nafees Ahmed, Anindya Iqbal", "title": "Static and Animated 3D Scene Generation from Free-form Text Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating coherent and useful image/video scenes from a free-form textual\ndescription is technically a very difficult problem to handle. Textual\ndescription of the same scene can vary greatly from person to person, or\nsometimes even for the same person from time to time. As the choice of words\nand syntax vary while preparing a textual description, it is challenging for\nthe system to reliably produce a consistently desirable output from different\nforms of language input. The prior works of scene generation have been mostly\nconfined to rigorous sentence structures of text input which restrict the\nfreedom of users to write description. In our work, we study a new pipeline\nthat aims to generate static as well as animated 3D scenes from different types\nof free-form textual scene description without any major restriction. In\nparticular, to keep our study practical and tractable, we focus on a small\nsubspace of all possible 3D scenes, containing various combinations of cube,\ncylinder and sphere. We design a two-stage pipeline. In the first stage, we\nencode the free-form text using an encoder-decoder neural architecture. In the\nsecond stage, we generate a 3D scene based on the generated encoding. Our\nneural architecture exploits state-of-the-art language model as encoder to\nleverage rich contextual encoding and a new multi-head decoder to predict\nmultiple features of an object in the scene simultaneously. For our\nexperiments, we generate a large synthetic data-set which contains 13,00,000\nand 14,00,000 samples of unique static and animated scene descriptions,\nrespectively. We achieve 98.427% accuracy on test data set in detecting the 3D\nobjects features successfully. Our work shows a proof of concept of one\napproach towards solving the problem, and we believe with enough training data,\nthe same pipeline can be expanded to handle even broader set of 3D scene\ngeneration problems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:31:21 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 19:28:30 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Huq", "Faria", ""], ["Ahmed", "Nafees", ""], ["Iqbal", "Anindya", ""]]}, {"id": "2010.01554", "submitter": "Sina Ahmadi", "authors": "Sina Ahmadi, Hossein Hassani, Daban Q. Jaff", "title": "Leveraging Multilingual News Websites for Building a Kurdish Parallel\n  Corpus", "comments": "11 pages, under review in the ACM Transactions on Asian and\n  Low-Resource Language Information Processing (TALLIP) Corpus available at\n  https://github.com/KurdishBLARK/InterdialectCorpus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine translation has been a major motivation of development in natural\nlanguage processing. Despite the burgeoning achievements in creating more\nefficient machine translation systems thanks to deep learning methods, parallel\ncorpora have remained indispensable for progress in the field. In an attempt to\ncreate parallel corpora for the Kurdish language, in this paper, we describe\nour approach in retrieving potentially-alignable news articles from\nmulti-language websites and manually align them across dialects and languages\nbased on lexical similarity and transliteration of scripts. We present a corpus\ncontaining 12,327 translation pairs in the two major dialects of Kurdish,\nSorani and Kurmanji. We also provide 1,797 and 650 translation pairs in\nEnglish-Kurmanji and English-Sorani. The corpus is publicly available under the\nCC BY-NC-SA 4.0 license.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:52:50 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ahmadi", "Sina", ""], ["Hassani", "Hossein", ""], ["Jaff", "Daban Q.", ""]]}, {"id": "2010.01556", "submitter": "Qianying Liu", "authors": "Qianying Liu, Wenyu Guan, Sujian Li, Fei Cheng, Daisuke Kawahara and\n  Sadao Kurohashi", "title": "Reverse Operation based Data Augmentation for Solving Math Word Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically solving math word problems is a critical task in the field of\nnatural language processing. Recent models have reached their performance\nbottleneck and require more high-quality data for training. Inspired by human\ndouble-checking mechanism, we propose a reverse operation based data\naugmentation method that makes use of mathematical logic to produce new\nhigh-quality math problems and introduce new knowledge points that can give\nsupervision for new mathematical reasoning logic. We apply the augmented data\non two SOTA math word problem solving models. Experimental results show the\neffectiveness of our approach\\footnote{We will release our code and data after\nthe paper is accepted.}.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:59:59 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Qianying", ""], ["Guan", "Wenyu", ""], ["Li", "Sujian", ""], ["Cheng", "Fei", ""], ["Kawahara", "Daisuke", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "2010.01610", "submitter": "Wenjuan Han", "authors": "Wenjuan Han, Liwen Zhang, Yong Jiang, Kewei Tu", "title": "Adversarial Attack and Defense of Structured Prediction Models", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building an effective adversarial attacker and elaborating on countermeasures\nfor adversarial attacks for natural language processing (NLP) have attracted a\nlot of research in recent years. However, most of the existing approaches focus\non classification problems. In this paper, we investigate attacks and defenses\nfor structured prediction tasks in NLP. Besides the difficulty of perturbing\ndiscrete words and the sentence fluency problem faced by attackers in any NLP\ntasks, there is a specific challenge to attackers of structured prediction\nmodels: the structured output of structured prediction models is sensitive to\nsmall perturbations in the input. To address these problems, we propose a novel\nand unified framework that learns to attack a structured prediction model using\na sequence-to-sequence model with feedbacks from multiple reference models of\nthe same structured prediction task. Based on the proposed attack, we further\nreinforce the victim model with adversarial training, making its prediction\nmore robust and accurate. We evaluate the proposed framework in dependency\nparsing and part-of-speech tagging. Automatic and human evaluations show that\nour proposed framework succeeds in both attacking state-of-the-art structured\nprediction models and boosting them with adversarial training.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 15:54:03 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 09:39:58 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Han", "Wenjuan", ""], ["Zhang", "Liwen", ""], ["Jiang", "Yong", ""], ["Tu", "Kewei", ""]]}, {"id": "2010.01611", "submitter": "Pouya Rezazadeh Kalehbasti", "authors": "Liubov Nikolenko, Pouya Rezazadeh Kalehbasti", "title": "When in Doubt, Ask: Generating Answerable and Unanswerable Questions,\n  Unsupervised", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Question Answering (QA) is key for making possible a robust communication\nbetween human and machine. Modern language models used for QA have surpassed\nthe human-performance in several essential tasks; however, these models require\nlarge amounts of human-generated training data which are costly and\ntime-consuming to create. This paper studies augmenting human-made datasets\nwith synthetic data as a way of surmounting this problem. A state-of-the-art\nmodel based on deep transformers is used to inspect the impact of using\nsynthetic answerable and unanswerable questions to complement a well-known\nhuman-made dataset. The results indicate a tangible improvement in the\nperformance of the language model (measured in terms of F1 and EM scores)\ntrained on the mixed dataset. Specifically, unanswerable question-answers prove\nmore effective in boosting the model: the F1 score gain from adding to the\noriginal dataset the answerable, unanswerable, and combined question-answers\nwere 1.3%, 5.0%, and 6.7%, respectively. [Link to the Github repository:\nhttps://github.com/lnikolenko/EQA]\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 15:56:44 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 02:29:13 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nikolenko", "Liubov", ""], ["Kalehbasti", "Pouya Rezazadeh", ""]]}, {"id": "2010.01620", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Jie Wang", "title": "Meta Sequence Learning and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a meta-sequence representation of sentences and demonstrate how to\nuse meta sequence learning to generate adequate question-answer pairs (QAPs)\nover a given article. A meta sequence is a sequence of vectors of semantic and\nsyntactic tags. On a given declarative sentence, a trained model converts it to\na meta sequence, finds a matched meta sequence in its learned database, and\nuses the corresponding meta sequence for interrogative sentence to generate\nQAPs. We show that, trained on a small dataset, our method generates\nefficiently, on the official SAT practice reading tests, a large number of\nsyntactically and semantically correct QAPs with high accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 16:28:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhang", "Cheng", ""], ["Wang", "Jie", ""]]}, {"id": "2010.01625", "submitter": "Sheena Panthaplackel", "authors": "Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, Raymond J.\n  Mooney", "title": "Deep Just-In-Time Inconsistency Detection Between Comments and Source\n  Code", "comments": "Accepted in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language comments convey key aspects of source code such as\nimplementation, usage, and pre- and post-conditions. Failure to update comments\naccordingly when the corresponding code is modified introduces inconsistencies,\nwhich is known to lead to confusion and software bugs. In this paper, we aim to\ndetect whether a comment becomes inconsistent as a result of changes to the\ncorresponding body of code, in order to catch potential inconsistencies\njust-in-time, i.e., before they are committed to a code base. To achieve this,\nwe develop a deep-learning approach that learns to correlate a comment with\ncode changes. By evaluating on a large corpus of comment/code pairs spanning\nvarious comment types, we show that our model outperforms multiple baselines by\nsignificant margins. For extrinsic evaluation, we show the usefulness of our\napproach by combining it with a comment update model to build a more\ncomprehensive automatic comment maintenance system which can both detect and\nresolve inconsistent comments based on code changes.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 16:49:28 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 22:55:17 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Panthaplackel", "Sheena", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "2010.01653", "submitter": "Ilias Chalkidis", "authors": "Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos\n  Malakasiotis, Nikolaos Aletras and Ion Androutsopoulos", "title": "An Empirical Study on Large-Scale Multi-Label Text Classification\n  Including Few and Zero-Shot Labels", "comments": "9 pages, long paper at EMNLP 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale Multi-label Text Classification (LMTC) has a wide range of\nNatural Language Processing (NLP) applications and presents interesting\nchallenges. First, not all labels are well represented in the training set, due\nto the very large label set and the skewed label distributions of LMTC\ndatasets. Also, label hierarchies and differences in human labelling guidelines\nmay affect graph-aware annotation proximity. Finally, the label hierarchies are\nperiodically updated, requiring LMTC models capable of zero-shot\ngeneralization. Current state-of-the-art LMTC models employ Label-Wise\nAttention Networks (LWANs), which (1) typically treat LMTC as flat multi-label\nclassification; (2) may use the label hierarchy to improve zero-shot learning,\nalthough this practice is vastly understudied; and (3) have not been combined\nwith pre-trained Transformers (e.g. BERT), which have led to state-of-the-art\nresults in several NLP benchmarks. Here, for the first time, we empirically\nevaluate a battery of LMTC methods from vanilla LWANs to hierarchical\nclassification approaches and transfer learning, on frequent, few, and\nzero-shot learning on three datasets from different domains. We show that\nhierarchical methods based on Probabilistic Label Trees (PLTs) outperform\nLWANs. Furthermore, we show that Transformer-based approaches outperform the\nstate-of-the-art in two of the datasets, and we propose a new state-of-the-art\nmethod which combines BERT with LWANs. Finally, we propose new models that\nleverage the label hierarchy to improve few and zero-shot learning, considering\non each dataset a graph-aware annotation proximity measure that we introduce.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 18:55:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chalkidis", "Ilias", ""], ["Fergadiotis", "Manos", ""], ["Kotitsas", "Sotiris", ""], ["Malakasiotis", "Prodromos", ""], ["Aletras", "Nikolaos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "2010.01657", "submitter": "Wei-Jen Ko", "authors": "Wei-Jen Ko and Te-Yuan Chen and Yiyan Huang and Greg Durrett and Junyi\n  Jessy Li", "title": "Inquisitive Question Generation for High Level Text Comprehension", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inquisitive probing questions come naturally to humans in a variety of\nsettings, but is a challenging task for automatic systems. One natural type of\nquestion to ask tries to fill a gap in knowledge during text comprehension,\nlike reading a news article: we might ask about background information, deeper\nreasons behind things occurring, or more. Despite recent progress with\ndata-driven approaches, generating such questions is beyond the range of models\ntrained on existing datasets.\n  We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while\na person is reading through a document. Compared to existing datasets,\nINQUISITIVE questions target more towards high-level (semantic and discourse)\ncomprehension of text. We show that readers engage in a series of pragmatic\nstrategies to seek information. Finally, we evaluate question generation models\nbased on GPT-2 and show that our model is able to generate reasonable questions\nalthough the task is challenging, and highlight the importance of context to\ngenerate INQUISITIVE questions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:03:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ko", "Wei-Jen", ""], ["Chen", "Te-Yuan", ""], ["Huang", "Yiyan", ""], ["Durrett", "Greg", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2010.01658", "submitter": "Wei-Jen Ko", "authors": "Wei-Jen Ko and Avik Ray and Yilin Shen and Hongxia Jin", "title": "Generating Dialogue Responses from a Semantic Latent Space", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing open-domain dialogue generation models are usually trained to mimic\nthe gold response in the training set using cross-entropy loss on the\nvocabulary. However, a good response does not need to resemble the gold\nresponse, since there are multiple possible responses to a given prompt. In\nthis work, we hypothesize that the current models are unable to integrate\ninformation from multiple semantically similar valid responses of a prompt,\nresulting in the generation of generic and uninformative responses. To address\nthis issue, we propose an alternative to the end-to-end classification on\nvocabulary. We learn the pair relationship between the prompts and responses as\na regression task on a latent space instead. In our novel dialog generation\nmodel, the representations of semantically related sentences are close to each\nother on the latent space. Human evaluation showed that learning the task on a\ncontinuous space can generate responses that are both relevant and informative.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:06:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ko", "Wei-Jen", ""], ["Ray", "Avik", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""]]}, {"id": "2010.01667", "submitter": "Luyu Gao", "authors": "Luyu Gao, Xinyi Wang, Graham Neubig", "title": "Improving Target-side Lexical Transfer in Multilingual Neural Machine\n  Translation", "comments": "Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the performance of Neural Machine Translation~(NMT) for\nlow-resource languages~(LRL), one effective strategy is to leverage parallel\ndata from a related high-resource language~(HRL). However, multilingual data\nhas been found more beneficial for NMT models that translate from the LRL to a\ntarget language than the ones that translate into the LRLs. In this paper, we\naim to improve the effectiveness of multilingual transfer for NMT models that\ntranslate \\emph{into} the LRL, by designing a better decoder word embedding.\nExtending upon a general-purpose multilingual encoding method Soft Decoupled\nEncoding~\\citep{SDE}, we propose DecSDE, an efficient character n-gram based\nembedding specifically designed for the NMT decoder. Our experiments show that\nDecSDE leads to consistent gains of up to 1.8 BLEU on translation from English\nto four different languages.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:42:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gao", "Luyu", ""], ["Wang", "Xinyi", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.01672", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Diyi Yang", "title": "Multi-View Sequence-to-Sequence Models with Conversational Structure for\n  Abstractive Dialogue Summarization", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization is one of the most challenging and interesting problems in\nNLP. Although much attention has been paid to summarizing structured text like\nnews reports or encyclopedia articles, summarizing conversations---an essential\npart of human-human/machine interaction where most important pieces of\ninformation are scattered across various utterances of different\nspeakers---remains relatively under-investigated. This work proposes a\nmulti-view sequence-to-sequence model by first extracting conversational\nstructures of unstructured daily chats from different views to represent\nconversations and then utilizing a multi-view decoder to incorporate different\nviews to generate dialogue summaries. Experiments on a large-scale dialogue\nsummarization corpus demonstrated that our methods significantly outperformed\nprevious state-of-the-art models via both automatic evaluations and human\njudgment. We also discussed specific challenges that current approaches faced\nwith this task. We have publicly released our code at\nhttps://github.com/GT-SALT/Multi-View-Seq2Seq.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:12:44 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Jiaao", ""], ["Yang", "Diyi", ""]]}, {"id": "2010.01677", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, Diyi Yang", "title": "Local Additivity Based Data Augmentation for Semi-supervised NER", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is one of the first stages in deep language\nunderstanding yet current NER models heavily rely on human-annotated data. In\nthis work, to alleviate the dependence on labeled data, we propose a Local\nAdditivity based Data Augmentation (LADA) method for semi-supervised NER, in\nwhich we create virtual samples by interpolating sequences close to each other.\nOur approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA\nperforms interpolations among tokens within one sentence, and Inter-LADA\nsamples different sentences to interpolate. Through linear additions between\nsampled training data, LADA creates an infinite amount of labeled data and\nimproves both entity and context learning. We further extend LADA to the\nsemi-supervised setting by designing a novel consistency loss for unlabeled\ndata. Experiments conducted on two NER benchmarks demonstrate the effectiveness\nof our methods over several strong baselines. We have publicly released our\ncode at https://github.com/GT-SALT/LADA.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:46:26 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Jiaao", ""], ["Wang", "Zhenghui", ""], ["Tian", "Ran", ""], ["Yang", "Zichao", ""], ["Yang", "Diyi", ""]]}, {"id": "2010.01678", "submitter": "Xi Ye", "authors": "Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett", "title": "Optimal Neural Program Synthesis from Multimodal Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal program synthesis, which leverages different types of user input\nto synthesize a desired program, is an attractive way to scale program\nsynthesis to challenging settings; however, it requires integrating noisy\nsignals from the user (like natural language) with hard constraints on the\nprogram's behavior. This paper proposes an optimal neural synthesis approach\nwhere the goal is to find a program that satisfies user-provided constraints\nwhile also maximizing the program's score with respect to a neural model.\nSpecifically, we focus on multimodal synthesis tasks in which the user intent\nis expressed using combination of natural language (NL) and input-output\nexamples. At the core of our method is a top-down recurrent neural model that\nplaces distributions over abstract syntax trees conditioned on the NL input.\nThis model not only allows for efficient search over the space of syntactically\nvalid programs, but it allows us to leverage automated program analysis\ntechniques for pruning the search space based on infeasibility of partial\nprograms with respect to the user's constraints. The experimental results on a\nmultimodal synthesis dataset (StructuredRegex) show that our method\nsubstantially outperforms prior state-of-the-art techniques in terms of\naccuracy %, finds model-optimal programs more frequently, and explores fewer\nstates during search.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:51:21 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ye", "Xi", ""], ["Chen", "Qiaochu", ""], ["Dillig", "Isil", ""], ["Durrett", "Greg", ""]]}, {"id": "2010.01683", "submitter": "Wenlin Yao", "authors": "Wenlin Yao, Cheng Zhang, Shiva Saravanan, Ruihong Huang, Ali Mostafavi", "title": "Weakly-supervised Fine-grained Event Recognition on Social Media Texts\n  for Disaster Management", "comments": "In Proceedings of the AAAI 2020 (AI for Social Impact Track). Link:\n  https://aaai.org/ojs/index.php/AAAI/article/view/5391", "journal-ref": null, "doi": "10.1609/aaai.v34i01.5391", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People increasingly use social media to report emergencies, seek help or\nshare information during disasters, which makes social networks an important\ntool for disaster management. To meet these time-critical needs, we present a\nweakly supervised approach for rapidly building high-quality classifiers that\nlabel each individual Twitter message with fine-grained event categories. Most\nimportantly, we propose a novel method to create high-quality labeled data in a\ntimely manner that automatically clusters tweets containing an event keyword\nand asks a domain expert to disambiguate event word senses and label clusters\nquickly. In addition, to process extremely noisy and often rather short\nuser-generated messages, we enrich tweet representations using preceding\ncontext tweets and reply tweets in building event recognition classifiers. The\nevaluation on two hurricanes, Harvey and Florence, shows that using only 1-2\nperson-hours of human supervision, the rapidly trained weakly supervised\nclassifiers outperform supervised classifiers trained using more than ten\nthousand annotated tweets created in over 50 person-hours.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 21:06:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yao", "Wenlin", ""], ["Zhang", "Cheng", ""], ["Saravanan", "Shiva", ""], ["Huang", "Ruihong", ""], ["Mostafavi", "Ali", ""]]}, {"id": "2010.01693", "submitter": "Oluwatobi Olabiyi", "authors": "Oluwatobi O. Olabiyi, Prarthana Bhattarai, C. Bayan Bruss, Zachary\n  Kulis", "title": "DLGNet-Task: An End-to-end Neural Network Framework for Modeling\n  Multi-turn Multi-domain Task-Oriented Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Task oriented dialogue (TOD) requires the complex interleaving of a number of\nindividually controllable components with strong guarantees for explainability\nand verifiability. This has made it difficult to adopt the multi-turn\nmulti-domain dialogue generation capabilities of streamlined end-to-end\nopen-domain dialogue systems. In this paper, we present a new framework,\nDLGNet-Task, a unified task-oriented dialogue system which employs\nautoregressive transformer networks such as DLGNet and GPT-2/3 to complete user\ntasks in multi-turn multi-domain conversations. Our framework enjoys the\ncontrollable, verifiable, and explainable outputs of modular approaches, and\nthe low development, deployment and maintenance cost of end-to-end systems.\nTreating open-domain system components as additional TOD system modules allows\nDLGNet-Task to learn the joint distribution of the inputs and outputs of all\nthe functional blocks of existing modular approaches such as, natural language\nunderstanding (NLU), state tracking, action policy, as well as natural language\ngeneration (NLG). Rather than training the modules individually, as is common\nin real-world systems, we trained them jointly with appropriate module\nseparations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows\ncomparable performance to the existing state-of-the-art approaches.\nFurthermore, using DLGNet-Task in conversational AI systems reduces the level\nof effort required for developing, deploying, and maintaining intelligent\nassistants at scale.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 21:43:17 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:31:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Olabiyi", "Oluwatobi O.", ""], ["Bhattarai", "Prarthana", ""], ["Bruss", "C. Bayan", ""], ["Kulis", "Zachary", ""]]}, {"id": "2010.01694", "submitter": "St\\'ephane Aroca-Ouellette", "authors": "Stephane Aroca-Ouellette, Frank Rudzicz", "title": "On Losses for Modern Language Models", "comments": "Accepted to EMNLP 2020. 9 Pages + 3 Pages of References and\n  Appendices (12 Pages total)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT set many state-of-the-art results over varied NLU benchmarks by\npre-training over two tasks: masked language modelling (MLM) and next sentence\nprediction (NSP), the latter of which has been highly criticized. In this\npaper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen\npossible auxiliary pre-training tasks, of which seven are novel to modern\nlanguage models, and 3) investigate different ways to include multiple tasks\ninto pre-training. We show that NSP is detrimental to training due to its\ncontext splitting and shallow semantic signal. We also identify six auxiliary\npre-training tasks -- sentence ordering, adjacent sentence prediction, TF\nprediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant\n-- that outperform a pure MLM baseline. Finally, we demonstrate that using\nmultiple tasks in a multi-task pre-training framework provides better results\nthan using any single auxiliary task. Using these methods, we outperform BERT\nBase on the GLUE benchmark using fewer than a quarter of the training tokens.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 21:44:15 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Aroca-Ouellette", "Stephane", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2010.01703", "submitter": "Zhong-Qiu Wang", "authors": "Zhong-Qiu Wang and Peidong Wang and DeLiang Wang", "title": "Multi-microphone Complex Spectral Mapping for Utterance-wise and\n  Continuous Speech Separation", "comments": "14 pages, 6 figures. To appear in IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing. Sound demo\n  https://zqwang7.github.io/demos/SMSWSJ_demo/taslp20_SMSWSJ_demo.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose multi-microphone complex spectral mapping, a simple way of\napplying deep learning for time-varying non-linear beamforming, for speaker\nseparation in reverberant conditions. We aim at both speaker separation and\ndereverberation. Our study first investigates offline utterance-wise speaker\nseparation and then extends to block-online continuous speech separation (CSS).\nAssuming a fixed array geometry between training and testing, we train deep\nneural networks (DNN) to predict the real and imaginary (RI) components of\ntarget speech at a reference microphone from the RI components of multiple\nmicrophones. We then integrate multi-microphone complex spectral mapping with\nminimum variance distortionless response (MVDR) beamforming and post-filtering\nto further improve separation, and combine it with frame-level speaker counting\nfor block-online CSS. Although our system is trained on simulated room impulse\nresponses (RIR) based on a fixed number of microphones arranged in a given\ngeometry, it generalizes well to a real array with the same geometry.\nState-of-the-art separation performance is obtained on the simulated two-talker\nSMS-WSJ corpus and the real-recorded LibriCSS dataset.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:13:13 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 15:00:30 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Zhong-Qiu", ""], ["Wang", "Peidong", ""], ["Wang", "DeLiang", ""]]}, {"id": "2010.01713", "submitter": "Anshuman Mishra", "authors": "Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Li, Pavan\n  Kapanipathi, Kartik Talamadupula", "title": "Reading Comprehension as Natural Language Inference: A Semantic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent past, Natural language Inference (NLI) has gained significant\nattention, particularly given its promise for downstream NLP tasks. However,\nits true impact is limited and has not been well studied. Therefore, in this\npaper, we explore the utility of NLI for one of the most prominent downstream\ntasks, viz. Question Answering (QA). We transform the one of the largest\navailable MRC dataset (RACE) to an NLI form, and compare the performances of a\nstate-of-the-art model (RoBERTa) on both these forms. We propose new\ncharacterizations of questions, and evaluate the performance of QA and NLI\nmodels on these categories. We highlight clear categories for which the model\nis able to perform better when the data is presented in a coherent entailment\nform, and a structured question-answer concatenation form, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:50:59 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Mishra", "Anshuman", ""], ["Patel", "Dhruvesh", ""], ["Vijayakumar", "Aparna", ""], ["Li", "Xiang", ""], ["Kapanipathi", "Pavan", ""], ["Talamadupula", "Kartik", ""]]}, {"id": "2010.01717", "submitter": "Nader Akoury", "authors": "Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng,\n  Mohit Iyyer", "title": "STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story\n  Generation", "comments": "Accepted as a long paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for story generation are asked to produce plausible and enjoyable\nstories given an input context. This task is underspecified, as a vast number\nof diverse stories can originate from a single input. The large output space\nmakes it difficult to build and evaluate story generation models, as (1)\nexisting datasets lack rich enough contexts to meaningfully guide models, and\n(2) existing evaluations (both crowdsourced and automatic) are unreliable for\nassessing long-form creative text. To address these issues, we introduce a\ndataset and evaluation platform built from STORIUM, an online collaborative\nstorytelling community. Our author-generated dataset contains 6K lengthy\nstories (125M tokens) with fine-grained natural language annotations (e.g.,\ncharacter goals and attributes) interspersed throughout each narrative, forming\na robust source for guiding models. We evaluate language models fine-tuned on\nour dataset by integrating them onto STORIUM, where real authors can query a\nmodel for suggested story continuations and then edit them. Automatic metrics\ncomputed over these edits correlate well with both user ratings of generated\nstories and qualitative feedback from semi-structured user interviews. We\nrelease both the STORIUM dataset and evaluation platform to spur more\nprincipled research into story generation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 23:26:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Akoury", "Nader", ""], ["Wang", "Shufan", ""], ["Whiting", "Josh", ""], ["Hood", "Stephen", ""], ["Peng", "Nanyun", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2010.01735", "submitter": "Lu Zhang", "authors": "Lu Zhang, Mo Yu, Tian Gao, Yue Yu", "title": "MCMH: Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning", "comments": "Accepted Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop reasoning approaches over knowledge graphs infer a missing\nrelationship between entities with a multi-hop rule, which corresponds to a\nchain of relationships. We extend existing works to consider a generalized form\nof multi-hop rules, where each rule is a set of relation chains. To learn such\ngeneralized rules efficiently, we propose a two-step approach that first\nselects a small set of relation chains as a rule and then evaluates the\nconfidence of the target relationship by jointly scoring the selected chains. A\ngame-theoretical framework is proposed to this end to simultaneously optimize\nthe rule selection and prediction steps. Empirical results show that our\nmulti-chain multi-hop (MCMH) rules result in superior results compared to the\nstandard single-chain approaches, justifying both our formulation of\ngeneralized rules and the effectiveness of the proposed learning framework.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 01:32:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhang", "Lu", ""], ["Yu", "Mo", ""], ["Gao", "Tian", ""], ["Yu", "Yue", ""]]}, {"id": "2010.01737", "submitter": "Yinghao Li", "authors": "Yinghao Li (Georgia Institute of Technology), Rui Feng (Georgia\n  Institute of Technology), Isaac Rehg (Georgia Institute of Technology), Chao\n  Zhang (Georgia Institute of Technology)", "title": "Transformer-Based Neural Text Generation with Syntactic Guidance", "comments": "11 pages, 4 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of using (partial) constituency parse trees as syntactic\nguidance for controlled text generation. Existing approaches to this problem\nuse recurrent structures, which not only suffer from the long-term dependency\nproblem but also falls short in modeling the tree structure of the syntactic\nguidance. We propose to leverage the parallelism of Transformer to better\nincorporate parse trees. Our method first expands a partial template\nconstituency parse tree to a full-fledged parse tree tailored for the input\nsource text, and then uses the expanded tree to guide text generation. The\neffectiveness of our model in this process hinges upon two new attention\nmechanisms: 1) a path attention mechanism that forces one node to attend to\nonly other nodes located in its path in the syntax tree to better incorporate\nsyntax guidance; 2) a multi-encoder attention mechanism that allows the decoder\nto dynamically attend to information from multiple encoders. Our experiments in\nthe controlled paraphrasing task show that our method outperforms SOTA models\nboth semantically and syntactically, improving the best baseline's BLEU score\nfrom 11.83 to 26.27.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 01:33:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Yinghao", "", "Georgia Institute of Technology"], ["Feng", "Rui", "", "Georgia\n  Institute of Technology"], ["Rehg", "Isaac", "", "Georgia Institute of Technology"], ["Zhang", "Chao", "", "Georgia Institute of Technology"]]}, {"id": "2010.01739", "submitter": "Thuy-Trang Vu", "authors": "Thuy-Trang Vu, Dinh Phung and Gholamreza Haffari", "title": "Effective Unsupervised Domain Adaptation with Adversarially Trained\n  Language Models", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown the importance of adaptation of broad-coverage\ncontextualised embedding models on the domain of the target task of interest.\nCurrent self-supervised adaptation methods are simplistic, as the training\nsignal comes from a small percentage of \\emph{randomly} masked-out tokens. In\nthis paper, we show that careful masking strategies can bridge the knowledge\ngap of masked language models (MLMs) about the domains more effectively by\nallocating self-supervision where it is needed. Furthermore, we propose an\neffective training strategy by adversarially masking out those tokens which are\nharder to reconstruct by the underlying MLM. The adversarial objective leads to\na challenging combinatorial optimisation problem over \\emph{subsets} of tokens,\nwhich we tackle efficiently through relaxation to a variational lowerbound and\ndynamic programming. On six unsupervised domain adaptation tasks involving\nnamed entity recognition, our method strongly outperforms the random masking\nstrategy and achieves up to +1.64 F1 score improvements.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 01:49:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Vu", "Thuy-Trang", ""], ["Phung", "Dinh", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2010.01745", "submitter": "Diego Ramirez-Echavarria", "authors": "Diego Ramirez-Echavarria, Antonis Bikakis, Luke Dickens, Rob Miller,\n  Andreas Vlachidis", "title": "On the Effects of Knowledge-Augmented Data in Word Embeddings", "comments": "10 pages, 5 figures, submitted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates techniques for knowledge injection into word\nembeddings learned from large corpora of unannotated data. These\nrepresentations are trained with word cooccurrence statistics and do not\ncommonly exploit syntactic and semantic information from linguistic knowledge\nbases, which potentially limits their transferability to domains with differing\nlanguage distributions or usages. We propose a novel approach for linguistic\nknowledge injection through data augmentation to learn word embeddings that\nenforce semantic relationships from the data, and systematically evaluate the\nimpact it has on the resulting representations. We show our knowledge\naugmentation approach improves the intrinsic characteristics of the learned\nembeddings while not significantly altering their results on a downstream text\nclassification task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 02:14:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ramirez-Echavarria", "Diego", ""], ["Bikakis", "Antonis", ""], ["Dickens", "Luke", ""], ["Miller", "Rob", ""], ["Vlachidis", "Andreas", ""]]}, {"id": "2010.01770", "submitter": "John Morris", "authors": "John X. Morris", "title": "Second-Order NLP Adversarial Examples", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial example generation methods in NLP rely on models like language\nmodels or sentence encoders to determine if potential adversarial examples are\nvalid. In these methods, a valid adversarial example fools the model being\nattacked, and is determined to be semantically or syntactically valid by a\nsecond model. Research to date has counted all such examples as errors by the\nattacked model. We contend that these adversarial examples may not be flaws in\nthe attacked model, but flaws in the model that determines validity. We term\nsuch invalid inputs second-order adversarial examples. We propose the\nconstraint robustness curve and associated metric ACCS as tools for evaluating\nthe robustness of a constraint to second-order adversarial examples. To\ngenerate this curve, we design an adversarial attack to run directly on the\nsemantic similarity models. We test on two constraints, the Universal Sentence\nEncoder (USE) and BERTScore. Our findings indicate that such second-order\nexamples exist, but are typically less common than first-order adversarial\nexamples in state-of-the-art models. They also indicate that USE is effective\nas constraint on NLP adversarial examples, while BERTScore is nearly\nineffectual. Code for running the experiments in this paper is available at\nhttps://github.com/jxmorris12/second-order-adversarial-examples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:32:38 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 01:20:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Morris", "John X.", ""]]}, {"id": "2010.01771", "submitter": "Dongqin Xu", "authors": "Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, Guodong Zhou", "title": "Improving AMR Parsing with Sequence-to-Sequence Pre-training", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, the research on abstract meaning representation (AMR)\nparsing is much restricted by the size of human-curated dataset which is\ncritical to build an AMR parser with good performance. To alleviate such data\nsize restriction, pre-trained models have been drawing more and more attention\nin AMR parsing. However, previous pre-trained models, like BERT, are\nimplemented for general purpose which may not work as expected for the specific\ntask of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq)\nAMR parsing and propose a seq2seq pre-training approach to build pre-trained\nmodels in both single and joint way on three relevant tasks, i.e., machine\ntranslation, syntactic parsing, and AMR parsing itself. Moreover, we extend the\nvanilla fine-tuning method to a multi-task learning fine-tuning method that\noptimizes for the performance of AMR parsing while endeavors to preserve the\nresponse of pre-trained models. Extensive experimental results on two English\nbenchmark datasets show that both the single and joint pre-trained models\nsignificantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0),\nwhich reaches the state of the art. The result is very encouraging since we\nachieve this with seq2seq models rather than complex models. We make our code\nand model available at https://github.com/xdqkid/S2S-AMR-Parser.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:32:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Xu", "Dongqin", ""], ["Li", "Junhui", ""], ["Zhu", "Muhua", ""], ["Zhang", "Min", ""], ["Zhou", "Guodong", ""]]}, {"id": "2010.01781", "submitter": "Tengfei Ma", "authors": "Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa and Shouling Ji", "title": "Unsupervised Reference-Free Summary Quality Evaluation via Contrastive\n  Learning", "comments": "Long Paper in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of a document summarization system has been a critical factor to\nimpact the success of the summarization task. Previous approaches, such as\nROUGE, mainly consider the informativeness of the assessed summary and require\nhuman-generated references for each test summary. In this work, we propose to\nevaluate the summary qualities without reference summaries by unsupervised\ncontrastive learning. Specifically, we design a new metric which covers both\nlinguistic qualities and semantic informativeness based on BERT. To learn the\nmetric, for each summary, we construct different types of negative samples with\nrespect to different aspects of the summary qualities, and train our model with\na ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our\nnew evaluation method outperforms other metrics even without reference\nsummaries. Furthermore, we show that our method is general and transferable\nacross datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 05:04:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wu", "Hanlu", ""], ["Ma", "Tengfei", ""], ["Wu", "Lingfei", ""], ["Manyumwa", "Tariro", ""], ["Ji", "Shouling", ""]]}, {"id": "2010.01786", "submitter": "Tanmoy Chakraborty", "authors": "Alvin Dey, Tanya Chowdhury, Yash Kumar Atri, Tanmoy Chakraborty", "title": "Corpora Evaluation and System Bias Detection in Multi-document\n  Summarization", "comments": "11 pages, 3 tables, 5 figures, Accepted in the Findings of EMNLP,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-document summarization (MDS) is the task of reflecting key points from\nany set of documents into a concise text paragraph. In the past, it has been\nused to aggregate news, tweets, product reviews, etc. from various sources.\nOwing to no standard definition of the task, we encounter a plethora of\ndatasets with varying levels of overlap and conflict between participating\ndocuments. There is also no standard regarding what constitutes summary\ninformation in MDS. Adding to the challenge is the fact that new systems report\nresults on a set of chosen datasets, which might not correlate with their\nperformance on the other datasets. In this paper, we study this heterogeneous\ntask with the help of a few widely used MDS corpora and a suite of\nstate-of-the-art models. We make an attempt to quantify the quality of\nsummarization corpus and prescribe a list of points to consider while proposing\na new MDS corpus. Next, we analyze the reason behind the absence of an MDS\nsystem which achieves superior performance across all corpora. We then observe\nthe extent to which system metrics are influenced, and bias is propagated due\nto corpus properties. The scripts to reproduce the experiments in this work are\navailable at https://github.com/LCS2-IIITD/summarization_bias.git.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 05:25:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Dey", "Alvin", ""], ["Chowdhury", "Tanya", ""], ["Atri", "Yash Kumar", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2010.01791", "submitter": "Zi Lin", "authors": "Zi Lin, Jeremiah Zhe Liu, Zi Yang, Nan Hua, Dan Roth", "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized\n  Identity Prior", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional (unstructured) pruning methods for a Transformer model focus on\nregularizing the individual weights by penalizing them toward zero. In this\nwork, we explore spectral-normalized identity priors (SNIP), a structured\npruning approach that penalizes an entire residual module in a Transformer\nmodel toward an identity mapping. Our method identifies and discards\nunimportant non-linear mappings in the residual connections by applying a\nthresholding operator on the function norm. It is applicable to any structured\nmodule, including a single attention head, an entire attention block, or a\nfeed-forward subnetwork. Furthermore, we introduce spectral normalization to\nstabilize the distribution of the post-activation values of the Transformer\nlayers, further improving the pruning effectiveness of the proposed\nmethodology. We conduct experiments with BERT on 5 GLUE benchmark tasks to\ndemonstrate that SNIP achieves effective pruning results while maintaining\ncomparable performance. Specifically, we improve the performance over the\nstate-of-the-art by 0.5 to 1.0% on average at 50% compression ratio.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 05:40:56 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lin", "Zi", ""], ["Liu", "Jeremiah Zhe", ""], ["Yang", "Zi", ""], ["Hua", "Nan", ""], ["Roth", "Dan", ""]]}, {"id": "2010.01794", "submitter": "Steven Y. Feng", "authors": "Steven Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, Eduard\n  Hovy", "title": "GenAug: Data Augmentation for Finetuning Text Generators", "comments": "EMNLP 2020 Deep Learning Inside Out (DeeLIO) Workshop; Code available\n  at https://github.com/styfeng/GenAug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate data augmentation for text generation, which we\ncall GenAug. Text generation and language modeling are important tasks within\nnatural language processing, and are especially challenging for low-data\nregimes. We propose and evaluate various augmentation methods, including some\nthat incorporate external knowledge, for finetuning GPT-2 on a subset of Yelp\nReviews. We also examine the relationship between the amount of augmentation\nand the quality of the generated text. We utilize several metrics that evaluate\nimportant aspects of the generated text including its diversity and fluency.\nOur experiments demonstrate that insertion of character-level synthetic noise\nand keyword replacement with hypernyms are effective augmentation methods, and\nthat the quality of generations improves to a peak at approximately three times\nthe amount of original data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 05:46:39 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 06:00:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Feng", "Steven Y.", ""], ["Gangal", "Varun", ""], ["Kang", "Dongyeop", ""], ["Mitamura", "Teruko", ""], ["Hovy", "Eduard", ""]]}, {"id": "2010.01825", "submitter": "Yoav Levine", "authors": "Yoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown,\n  Moshe Tennenholtz, Yoav Shoham", "title": "PMI-Masking: Principled masking of correlated spans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Masking tokens uniformly at random constitutes a common flaw in the\npretraining of Masked Language Models (MLMs) such as BERT. We show that such\nuniform masking allows an MLM to minimize its training objective by latching\nonto shallow local signals, leading to pretraining inefficiency and suboptimal\ndownstream performance. To address this flaw, we propose PMI-Masking, a\nprincipled masking strategy based on the concept of Pointwise Mutual\nInformation (PMI), which jointly masks a token n-gram if it exhibits high\ncollocation over the corpus. PMI-Masking motivates, unifies, and improves upon\nprior more heuristic approaches that attempt to address the drawback of random\nuniform token masking, such as whole-word masking, entity/phrase masking, and\nrandom-span masking. Specifically, we show experimentally that PMI-Masking\nreaches the performance of prior masking approaches in half the training time,\nand consistently improves performance at the end of training.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:19:52 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Levine", "Yoav", ""], ["Lenz", "Barak", ""], ["Lieber", "Opher", ""], ["Abend", "Omri", ""], ["Leyton-Brown", "Kevin", ""], ["Tennenholtz", "Moshe", ""], ["Shoham", "Yoav", ""]]}, {"id": "2010.01830", "submitter": "Mark Anderson", "authors": "Mark Anderson and Carlos G\\'omez-Rodr\\'iguez", "title": "On the Frailty of Universal POS Tags for Neural UD Parsers", "comments": "To be published in proceedings of the 24th SIGNLL Conference on\n  Computational Natural Language Learning (CoNLL). Be aware of long appendix:\n  please don't print all 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis on the effect UPOS accuracy has on parsing\nperformance. Results suggest that leveraging UPOS tags as features for neural\nparsers requires a prohibitively high tagging accuracy and that the use of gold\ntags offers a non-linear increase in performance, suggesting some sort of\nexceptionality. We also investigate what aspects of predicted UPOS tags impact\nparsing accuracy the most, highlighting some potentially meaningful linguistic\nfacets of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:40:35 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 07:42:54 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 05:47:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Anderson", "Mark", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2010.01838", "submitter": "Yifan Gao", "authors": "Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq Joty, Steven C.H. Hoi,\n  Caiming Xiong, Irwin King, Michael R. Lyu", "title": "Discern: Discourse-Aware Entailment Reasoning Network for Conversational\n  Machine Reading", "comments": "EMNLP 2020 main conference, 11 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Document interpretation and dialog understanding are the two major challenges\nfor conversational machine reading. In this work, we propose Discern, a\ndiscourse-aware entailment reasoning network to strengthen the connection and\nenhance the understanding for both document and dialog. Specifically, we split\nthe document into clause-like elementary discourse units (EDU) using a\npre-trained discourse segmentation model, and we train our model in a\nweakly-supervised manner to predict whether each EDU is entailed by the user\nfeedback in a conversation. Based on the learned EDU and entailment\nrepresentations, we either reply to the user our final decision\n\"yes/no/irrelevant\" of the initial question, or generate a follow-up question\nto inquiry more information. Our experiments on the ShARC benchmark (blind,\nheld-out test set) show that Discern achieves state-of-the-art results of 78.3%\nmacro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question\ngeneration. Code and models are released at\nhttps://github.com/Yifan-Gao/Discern.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:49:51 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:16:32 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 10:06:46 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gao", "Yifan", ""], ["Wu", "Chien-Sheng", ""], ["Li", "Jingjing", ""], ["Joty", "Shafiq", ""], ["Hoi", "Steven C. H.", ""], ["Xiong", "Caiming", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""]]}, {"id": "2010.01869", "submitter": "Alessio Miaschi", "authors": "Alessio Miaschi, Dominique Brunato, Felice Dell'Orletta, Giulia\n  Venturi", "title": "Linguistic Profiling of a Neural Language Model", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the linguistic knowledge learned by a Neural\nLanguage Model (NLM) before and after a fine-tuning process and how this\nknowledge affects its predictions during several classification problems. We\nuse a wide set of probing tasks, each of which corresponds to a distinct\nsentence-level feature extracted from different levels of linguistic\nannotation. We show that BERT is able to encode a wide range of linguistic\ncharacteristics, but it tends to lose this information when trained on specific\ndownstream tasks. We also find that BERT's capacity to encode different kind of\nlinguistic properties has a positive influence on its predictions: the more it\nstores readable linguistic information of a sentence, the higher will be its\ncapacity of predicting the expected label assigned to that sentence.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:09:01 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 11:26:26 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 17:43:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Miaschi", "Alessio", ""], ["Brunato", "Dominique", ""], ["Dell'Orletta", "Felice", ""], ["Venturi", "Giulia", ""]]}, {"id": "2010.01878", "submitter": "Mathieu Rita", "authors": "Mathieu Rita, Rahma Chaabouni, Emmanuel Dupoux", "title": "\"LazImpa\": Lazy and Impatient neural agents learn to communicate\n  efficiently", "comments": "Accepted to CoNLL 2020", "journal-ref": "Proceedings of CoNLL 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that artificial neural agents naturally develop\nsurprisingly non-efficient codes. This is illustrated by the fact that in a\nreferential game involving a speaker and a listener neural networks optimizing\naccurate transmission over a discrete channel, the emergent messages fail to\nachieve an optimal length. Furthermore, frequent messages tend to be longer\nthan infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA)\nobserved in all natural languages. Here, we show that near-optimal and\nZLA-compatible messages can emerge, but only if both the speaker and the\nlistener are modified. We hence introduce a new communication system,\n\"LazImpa\", where the speaker is made increasingly lazy, i.e. avoids long\nmessages, and the listener impatient, i.e.,~seeks to guess the intended content\nas soon as possible.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:25:53 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Rita", "Mathieu", ""], ["Chaabouni", "Rahma", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2010.01891", "submitter": "Dat Quoc Nguyen", "authors": "Anh Tuan Nguyen, Mai Hoang Dao and Dat Quoc Nguyen", "title": "A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese", "comments": "EMNLP 2020 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing is an important NLP task. However, Vietnamese is a\nlow-resource language in this research area. In this paper, we present the\nfirst public large-scale Text-to-SQL semantic parsing dataset for Vietnamese.\nWe extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et\nal., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two\nbaselines with key configurations and find that: automatic Vietnamese word\nsegmentation improves the parsing results of both baselines; the normalized\npointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema\nlinking; latent syntactic features extracted from a neural dependency parser\nfor Vietnamese also improve the results; and the monolingual language model\nPhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher\nperformances than the recent best multilingual language model XLM-R (Conneau et\nal., 2020).\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 09:54:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nguyen", "Anh Tuan", ""], ["Dao", "Mai Hoang", ""], ["Nguyen", "Dat Quoc", ""]]}, {"id": "2010.01893", "submitter": "Shaoxiong Feng", "authors": "Shaoxiong Feng, Xuancheng Ren, Hongshen Chen, Bin Sun, Kan Li, Xu Sun", "title": "Regularizing Dialogue Generation by Imitating Implicit Scenarios", "comments": "Accepted by EMNLP 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human dialogues are scenario-based and appropriate responses generally relate\nto the latent context knowledge entailed by the specific scenario. To enable\nresponses that are more meaningful and context-specific, we propose to improve\ngenerative dialogue systems from the scenario perspective, where both dialogue\nhistory and future conversation are taken into account to implicitly\nreconstruct the scenario knowledge. More importantly, the conversation\nscenarios are further internalized using imitation learning framework, where\nthe conventional dialogue model that has no access to future conversations is\neffectively regularized by transferring the scenario knowledge contained in\nhierarchical supervising signals from the scenario-based dialogue model, so\nthat the future conversation is not required in actual inference. Extensive\nevaluations show that our approach significantly outperforms state-of-the-art\nbaselines on diversity and relevance, and expresses scenario-specific\nknowledge.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:10:19 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 05:51:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Feng", "Shaoxiong", ""], ["Ren", "Xuancheng", ""], ["Chen", "Hongshen", ""], ["Sun", "Bin", ""], ["Li", "Kan", ""], ["Sun", "Xu", ""]]}, {"id": "2010.01897", "submitter": "Urszula Wali\\'nska", "authors": "Piotr Janiszewski, Mateusz Skiba, Urszula Wali\\'nska", "title": "PUM at SemEval-2020 Task 12: Aggregation of Transformer-based models'\n  features for offensive language recognition", "comments": "7 pages, 0 figures. Proceedings of the International Workshop on\n  Semantic Evaluation (SemEval-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe the PUM team's entry to the SemEval-2020 Task 12.\nCreating our solution involved leveraging two well-known pretrained models used\nin natural language processing: BERT and XLNet, which achieve state-of-the-art\nresults in multiple NLP tasks. The models were fine-tuned for each subtask\nseparately and features taken from their hidden layers were combined and fed\ninto a fully connected neural network. The model using aggregated Transformer\nfeatures can serve as a powerful tool for offensive language identification\nproblem. Our team was ranked 7th out of 40 in Sub-task C - Offense target\nidentification with 64.727% macro F1-score and 64th out of 85 in Sub-task A -\nOffensive language identification (89.726% F1-score).\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:25:29 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Janiszewski", "Piotr", ""], ["Skiba", "Mateusz", ""], ["Wali\u0144ska", "Urszula", ""]]}, {"id": "2010.01898", "submitter": "Jie Huang", "authors": "Jie Huang, Zilong Wang, Kevin Chen-Chuan Chang, Wen-mei Hwu, Jinjun\n  Xiong", "title": "Exploring Semantic Capacity of Terms", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study semantic capacity of terms. For example, the semantic\ncapacity of artificial intelligence is higher than that of linear regression\nsince artificial intelligence possesses a broader meaning scope. Understanding\nsemantic capacity of terms will help many downstream tasks in natural language\nprocessing. For this purpose, we propose a two-step model to investigate\nsemantic capacity of terms, which takes a large text corpus as input and can\nevaluate semantic capacity of terms if the text corpus can provide enough\nco-occurrence information of terms. Extensive experiments in three fields\ndemonstrate the effectiveness and rationality of our model compared with\nwell-designed baselines and human-level evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:26:36 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Huang", "Jie", ""], ["Wang", "Zilong", ""], ["Chang", "Kevin Chen-Chuan", ""], ["Hwu", "Wen-mei", ""], ["Xiong", "Jinjun", ""]]}, {"id": "2010.01899", "submitter": "Xin Lv", "authors": "Xin Lv, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Wei Zhang, Yichi\n  Zhang, Hao Kong, Suhui Wu", "title": "Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse\n  Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop reasoning has been widely studied in recent years to seek an\neffective and interpretable method for knowledge graph (KG) completion. Most\nprevious reasoning methods are designed for dense KGs with enough paths between\nentities, but cannot work well on those sparse KGs that only contain sparse\npaths for reasoning. On the one hand, sparse KGs contain less information,\nwhich makes it difficult for the model to choose correct paths. On the other\nhand, the lack of evidential paths to target entities also makes the reasoning\nprocess difficult. To solve these problems, we propose a multi-hop reasoning\nmodel named DacKGR over sparse KGs, by applying novel dynamic anticipation and\ncompletion strategies: (1) The anticipation strategy utilizes the latent\nprediction of embedding-based models to make our model perform more potential\npath search over sparse KGs. (2) Based on the anticipation information, the\ncompletion strategy dynamically adds edges as additional actions during the\npath search, which further alleviates the sparseness problem of KGs. The\nexperimental results on five datasets sampled from Freebase, NELL and Wikidata\nshow that our method outperforms state-of-the-art baselines. Our codes and\ndatasets can be obtained from https://github.com/THU-KEG/DacKGR\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:28:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lv", "Xin", ""], ["Han", "Xu", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""], ["Zhang", "Wei", ""], ["Zhang", "Yichi", ""], ["Kong", "Hao", ""], ["Wu", "Suhui", ""]]}, {"id": "2010.01908", "submitter": "Wenxiang Jiao", "authors": "Wenxiang Jiao, Michael R. Lyu, Irwin King", "title": "Exploiting Unsupervised Data for Emotion Recognition in Conversations", "comments": "Accepted to the Findings of EMNLP 2020, 8 pages (published version of\n  arXiv:1910.08916)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion Recognition in Conversations (ERC) aims to predict the emotional\nstate of speakers in conversations, which is essentially a text classification\ntask. Unlike the sentence-level text classification problem, the available\nsupervised data for the ERC task is limited, which potentially prevents the\nmodels from playing their maximum effect. In this paper, we propose a novel\napproach to leverage unsupervised conversation data, which is more accessible.\nSpecifically, we propose the Conversation Completion (ConvCom) task, which\nattempts to select the correct answer from candidate answers to fill a masked\nutterance in a conversation. Then, we Pre-train a basic COntext- Dependent\nEncoder (Pre-CODE) on the ConvCom task. Finally, we fine-tune the Pre-CODE on\nthe datasets of ERC. Experimental results demonstrate that pre-training on\nunsupervised data achieves significant improvement of performance on the ERC\ndatasets, particularly on the minority emotion classes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:28:47 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:32:31 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Jiao", "Wenxiang", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2010.01923", "submitter": "Tianyu Gao", "authors": "Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu,\n  Maosong Sun, Jie Zhou", "title": "Learning from Context or Names? An Empirical Study on Neural Relation\n  Extraction", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have achieved remarkable success on relation extraction (RE)\nbenchmarks. However, there is no clear understanding which type of information\naffects existing RE models to make decisions and how to further improve the\nperformance of these models. To this end, we empirically study the effect of\ntwo main information sources in text: textual context and entity mentions\n(names). We find that (i) while context is the main source to support the\npredictions, RE models also heavily rely on the information from entity\nmentions, most of which is type information, and (ii) existing datasets may\nleak shallow heuristics via entity mentions and thus contribute to the high\nperformance on RE benchmarks. Based on the analyses, we propose an\nentity-masked contrastive pre-training framework for RE to gain a deeper\nunderstanding on both textual context and type information while avoiding rote\nmemorization of entities or use of superficial cues in mentions. We carry out\nextensive experiments to support our views, and show that our framework can\nimprove the effectiveness and robustness of neural models in different RE\nscenarios. All the code and datasets are released at\nhttps://github.com/thunlp/RE-Context-or-Names.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 11:21:59 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 04:10:37 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Peng", "Hao", ""], ["Gao", "Tianyu", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Li", "Peng", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""], ["Zhou", "Jie", ""]]}, {"id": "2010.01949", "submitter": "Kellen Gillespie", "authors": "Kellen Gillespie, Ioannis C. Konstantakopoulos, Xingzhi Guo, Vishal\n  Thanvantri Vasudevan, Abhinav Sethy", "title": "Improving Device Directedness Classification of Utterances with Semantic\n  Lexical Features", "comments": "Accepted and Published at ICASSP 2020", "journal-ref": "2020 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Barcelona, Spain, 2020, pp. 7859-7863", "doi": "10.1109/ICASSP40776.2020.9054304", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User interactions with personal assistants like Alexa, Google Home and Siri\nare typically initiated by a wake term or wakeword. Several personal assistants\nfeature \"follow-up\" modes that allow users to make additional interactions\nwithout the need of a wakeword. For the system to only respond when\nappropriate, and to ignore speech not intended for it, utterances must be\nclassified as device-directed or non-device-directed. State-of-the-art systems\nhave largely used acoustic features for this task, while others have used only\nlexical features or have added LM-based lexical features. We propose a\ndirectedness classifier that combines semantic lexical features with a\nlightweight acoustic feature and show it is effective in classifying\ndirectedness. The mixed-domain lexical and acoustic feature model is able to\nachieve 14% relative reduction of EER over a state-of-the-art acoustic-only\nbaseline model. Finally, we successfully apply transfer learning and\nsemi-supervised learning to the model to improve accuracy even further.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 20:13:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gillespie", "Kellen", ""], ["Konstantakopoulos", "Ioannis C.", ""], ["Guo", "Xingzhi", ""], ["Vasudevan", "Vishal Thanvantri", ""], ["Sethy", "Abhinav", ""]]}, {"id": "2010.01986", "submitter": "Wanzheng Zhu", "authors": "Wanzheng Zhu, Chao Zhang, Shuochao Yao, Xiaobin Gao, Jiawei Han", "title": "A Spherical Hidden Markov Model for Semantics-Rich Human Mobility\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of modeling human mobility from semantic trace data,\nwherein each GPS record in a trace is associated with a text message that\ndescribes the user's activity. Existing methods fall short in unveiling human\nmovement regularities, because they either do not model the text data at all or\nsuffer from text sparsity severely. We propose SHMM, a multi-modal spherical\nhidden Markov model for semantics-rich human mobility modeling. Under the\nhidden Markov assumption, SHMM models the generation process of a given trace\nby jointly considering the observed location, time, and text at each step of\nthe trace. The distinguishing characteristic of SHMM is the text modeling part.\nWe use fixed-size vector representations to encode the semantics of the text\nmessages, and model the generation of the l2-normalized text embeddings on a\nunit sphere with the von Mises-Fisher (vMF) distribution. Compared with other\nalternatives like multi-variate Gaussian, our choice of the vMF distribution\nnot only incurs much fewer parameters, but also better leverages the\ndiscriminative power of text embeddings in a directional metric space. The\nparameter inference for the vMF distribution is non-trivial since it involves\nfunctional inversion of ratios of Bessel functions. We theoretically prove\nthat: 1) the classical Expectation-Maximization algorithm can work with vMF\ndistributions; and 2) while closed-form solutions are hard to be obtained for\nthe M-step, Newton's method is guaranteed to converge to the optimal solution\nwith quadratic convergence rate. We have performed extensive experiments on\nboth synthetic and real-life data. The results on synthetic data verify our\ntheoretical analysis; while the results on real-life data demonstrate that SHMM\nlearns meaningful semantics-rich mobility models, outperforms state-of-the-art\nmobility models for next location prediction, and incurs lower training cost.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:18:38 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhu", "Wanzheng", ""], ["Zhang", "Chao", ""], ["Yao", "Shuochao", ""], ["Gao", "Xiaobin", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.01998", "submitter": "Angel Daza", "authors": "Angel Daza and Anette Frank", "title": "X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset", "comments": "To be presented at the EMNLP 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though SRL is researched for many languages, major improvements have\nmostly been obtained for English, for which more resources are available. In\nfact, existing multilingual SRL datasets contain disparate annotation styles or\ncome from different domains, hampering generalization in multilingual learning.\nIn this work, we propose a method to automatically construct an SRL corpus that\nis parallel in four languages: English, French, German, Spanish, with unified\npredicate and role annotations that are fully comparable across languages. We\napply high-quality machine translation to the English CoNLL-09 dataset and use\nmultilingual BERT to project its high-quality annotations to the target\nlanguages. We include human-validated test sets that we use to measure the\nprojection quality, and show that projection is denser and more precise than a\nstrong baseline. Finally, we train different SOTA models on our novel corpus\nfor mono- and multilingual SRL, showing that the multilingual annotations\nimprove performance especially for the weaker languages.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:34:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Daza", "Angel", ""], ["Frank", "Anette", ""]]}, {"id": "2010.02004", "submitter": "Emanuele La Malfa", "authors": "Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony\n  Hartshorn, Marta Kwiatkowska", "title": "Assessing Robustness of Text Classification through Maximal Safe Radius\n  Computation", "comments": "12 pages + appendix", "journal-ref": "EMNLP-Findings2020", "doi": "10.18653/v1/2020.findings-emnlp.266", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network NLP models are vulnerable to small modifications of the input\nthat maintain the original meaning but result in a different prediction. In\nthis paper, we focus on robustness of text classification against word\nsubstitutions, aiming to provide guarantees that the model prediction does not\nchange if a word is replaced with a plausible alternative, such as a synonym.\nAs a measure of robustness, we adopt the notion of the maximal safe radius for\na given input text, which is the minimum distance in the embedding space to the\ndecision boundary. Since computing the exact maximal safe radius is not\nfeasible in practice, we instead approximate it by computing a lower and upper\nbound. For the upper bound computation, we employ Monte Carlo Tree Search in\nconjunction with syntactic filtering to analyse the effect of single and\nmultiple word substitutions. The lower bound computation is achieved through an\nadaptation of the linear bounding techniques implemented in tools CNN-Cert and\nPOPQORN, respectively for convolutional and recurrent network models. We\nevaluate the methods on sentiment analysis and news classification models for\nfour datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and\nprovide an analysis of robustness trends. We also apply our framework to\ninterpretability analysis and compare it with LIME.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 09:46:32 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 08:50:10 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["La Malfa", "Emanuele", ""], ["Wu", "Min", ""], ["Laurenti", "Luca", ""], ["Wang", "Benjie", ""], ["Hartshorn", "Anthony", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "2010.02005", "submitter": "Maaike de Boer", "authors": "Maaike Burghoorn and Maaike H.T. de Boer and Stephan Raaijmakers", "title": "Gender prediction using limited Twitter Data", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have shown impressive performance on a variety of NLP\ntasks. Off-the-shelf, pre-trained models can be fine-tuned for specific NLP\nclassification tasks, reducing the need for large amounts of additional\ntraining data. However, little research has addressed how much data is required\nto accurately fine-tune such pre-trained transformer models, and how much data\nis needed for accurate prediction. This paper explores the usability of BERT (a\nTransformer model for word embedding) for gender prediction on social media.\nForensic applications include detecting gender obfuscation, e.g. males posing\nas females in chat rooms. A Dutch BERT model is fine-tuned on different samples\nof a Dutch Twitter dataset labeled for gender, varying in the number of tweets\nused per person. The results show that finetuning BERT contributes to good\ngender classification performance (80% F1) when finetuned on only 200 tweets\nper person. But when using just 20 tweets per person, the performance of our\nclassifier deteriorates non-steeply (to 70% F1). These results show that even\nwith relatively small amounts of data, BERT can be fine-tuned to accurately\nhelp predict the gender of Twitter users, and, consequently, that it is\npossible to determine gender on the basis of just a low volume of tweets. This\nopens up an operational perspective on the swift detection of gender.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 11:46:07 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Burghoorn", "Maaike", ""], ["de Boer", "Maaike H. T.", ""], ["Raaijmakers", "Stephan", ""]]}, {"id": "2010.02010", "submitter": "Griffin Adams", "authors": "Griffin Adams, Mert Ketenci, Shreyas Bhave, Adler Perotte, No\\'emie\n  Elhadad", "title": "Zero-Shot Clinical Acronym Expansion via Latent Meaning Cells", "comments": "To appear in Proceedings Track for Machine Learning for Health (ML4H)\n  Workshop at NeurIPS (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Latent Meaning Cells, a deep latent variable model which learns\ncontextualized representations of words by combining local lexical context and\nmetadata. Metadata can refer to granular context, such as section type, or to\nmore global context, such as unique document ids. Reliance on metadata for\ncontextualized representation learning is apropos in the clinical domain where\ntext is semi-structured and expresses high variation in topics. We evaluate the\nLMC model on the task of zero-shot clinical acronym expansion across three\ndatasets. The LMC significantly outperforms a diverse set of baselines at a\nfraction of the pre-training cost and learns clinically coherent\nrepresentations. We demonstrate that not only is metadata itself very helpful\nfor the task, but that the LMC inference algorithm provides an additional large\nbenefit.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 00:28:30 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 19:25:32 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Adams", "Griffin", ""], ["Ketenci", "Mert", ""], ["Bhave", "Shreyas", ""], ["Perotte", "Adler", ""], ["Elhadad", "No\u00e9mie", ""]]}, {"id": "2010.02053", "submitter": "Federico L\\'opez", "authors": "Federico L\\'opez, Michael Strube", "title": "A Fully Hyperbolic Neural Model for Hierarchical Multi-Class\n  Classification", "comments": "16 pages, accepted at Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label inventories for fine-grained entity typing have grown in size and\ncomplexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic\nspaces offer a mathematically appealing approach for learning hierarchical\nrepresentations of symbolic data. However, it is not clear how to integrate\nhyperbolic components into downstream tasks. This is the first work that\nproposes a fully hyperbolic model for multi-class multi-label classification,\nwhich performs all operations in hyperbolic space. We evaluate the proposed\nmodel on two challenging datasets and compare to different baselines that\noperate under Euclidean assumptions. Our hyperbolic model infers the latent\nhierarchy from the class distribution, captures implicit hyponymic relations in\nthe inventory, and shows performance on par with state-of-the-art methods on\nfine-grained classification with remarkable reduction of the parameter size. A\nthorough analysis sheds light on the impact of each component in the final\nprediction and showcases its ease of integration with Euclidean layers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:42:56 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["L\u00f3pez", "Federico", ""], ["Strube", "Michael", ""]]}, {"id": "2010.02057", "submitter": "No\\'e Tits", "authors": "Jean-Benoit Delbrouck and No\\'e Tits and St\\'ephane Dupont", "title": "Modulated Fusion using Transformer for Linguistic-Acoustic Emotion\n  Recognition", "comments": "EMNLP 2020 workshop: NLP Beyond Text (NLPBT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to bring a new lightweight yet powerful solution for the task\nof Emotion Recognition and Sentiment Analysis. Our motivation is to propose two\narchitectures based on Transformers and modulation that combine the linguistic\nand acoustic inputs from a wide range of datasets to challenge, and sometimes\nsurpass, the state-of-the-art in the field. To demonstrate the efficiency of\nour models, we carefully evaluate their performances on the IEMOCAP, MOSI,\nMOSEI and MELD dataset. The experiments can be directly replicated and the code\nis fully open for future researches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:46:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Tits", "No\u00e9", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "2010.02069", "submitter": "Oskar van der Wal MSc", "authors": "Oskar van der Wal, Silvan de Boer, Elia Bruni and Dieuwke Hupkes", "title": "The Grammar of Emergent Languages", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the syntactic properties of languages emerged in\nreferential games, using unsupervised grammar induction (UGI) techniques\noriginally designed to analyse natural language. We show that the considered\nUGI techniques are appropriate to analyse emergent languages and we then study\nif the languages that emerge in a typical referential game setup exhibit\nsyntactic structure, and to what extent this depends on the maximum message\nlength and number of symbols that the agents are allowed to use. Our\nexperiments demonstrate that a certain message length and vocabulary size are\nrequired for structure to emerge, but they also illustrate that more\nsophisticated game scenarios are required to obtain syntactic properties more\nakin to those observed in human language. We argue that UGI techniques should\nbe part of the standard toolkit for analysing emergent languages and release a\ncomprehensive library to facilitate such analysis for future researchers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:06:27 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 17:52:45 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["van der Wal", "Oskar", ""], ["de Boer", "Silvan", ""], ["Bruni", "Elia", ""], ["Hupkes", "Dieuwke", ""]]}, {"id": "2010.02094", "submitter": "Gaurav Arora", "authors": "Gaurav Arora", "title": "Gauravarora@HASOC-Dravidian-CodeMix-FIRE2020: Pre-training ULMFiT on\n  Synthetically Generated Code-Mixed Data for Hate Speech Detection", "comments": "System description paper for 2nd ranked system in Sub-task B accepted\n  at Dravidian-Codemix-HASOC2020@FIRE2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the system submitted to Dravidian-Codemix-HASOC2020:\nHate Speech and Offensive Content Identification in Dravidian languages\n(Tamil-English and Malayalam-English). The task aims to identify offensive\nlanguage in code-mixed dataset of comments/posts in Dravidian languages\ncollected from social media. We participated in both Sub-task A, which aims to\nidentify offensive content in mixed-script (mixture of Native and Roman script)\nand Sub-task B, which aims to identify offensive content in Roman script, for\nDravidian languages. In order to address these tasks, we proposed pre-training\nULMFiT on synthetically generated code-mixed data, generated by modelling\ncode-mixed data generation as a Markov process using Markov chains. Our model\nachieved 0.88 weighted F1-score for code-mixed Tamil-English language in\nSub-task B and got 2nd rank on the leader-board. Additionally, our model\nachieved 0.91 weighted F1-score (4th Rank) for mixed-script Malayalam-English\nin Sub-task A and 0.74 weighted F1-score (5th Rank) for code-mixed\nMalayalam-English language in Sub-task B.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:25:47 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 18:11:41 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Arora", "Gaurav", ""]]}, {"id": "2010.02114", "submitter": "Divyansh Kaushik", "authors": "Divyansh Kaushik, Amrith Setlur, Eduard Hovy, Zachary C. Lipton", "title": "Explaining The Efficacy of Counterfactually Augmented Data", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In attempts to produce ML models less reliant on spurious patterns in NLP\ndatasets, researchers have recently proposed curating counterfactually\naugmented data (CAD) via a human-in-the-loop process in which given some\ndocuments and their (initial) labels, humans must revise the text to make a\ncounterfactual label applicable. Importantly, edits that are not necessary to\nflip the applicable label are prohibited. Models trained on the augmented data\nappear, empirically, to rely less on semantically irrelevant words and to\ngeneralize better out of domain. While this work draws loosely on causal\nthinking, the underlying causal model (even at an abstract level) and the\nprinciples underlying the observed out-of-domain improvements remain unclear.\nIn this paper, we introduce a toy analog based on linear Gaussian models,\nobserving interesting relationships between causal models, measurement noise,\nout-of-domain generalization, and reliance on spurious signals. Our analysis\nprovides some insights that help to explain the efficacy of CAD. Moreover, we\ndevelop the hypothesis that while adding noise to causal features should\ndegrade both in-domain and out-of-domain performance, adding noise to\nnon-causal features should lead to relative improvements in out-of-domain\nperformance. This idea inspires a speculative test for determining whether a\nfeature attribution technique has identified the causal spans. If adding noise\n(e.g., by random word flips) to the highlighted spans degrades both in-domain\nand out-of-domain performance on a battery of challenge datasets, but adding\nnoise to the complement gives improvements out-of-domain, it suggests we have\nidentified causal spans. We present a large-scale empirical study comparing\nspans edited to create CAD to those selected by attention and saliency maps.\nAcross numerous domains and models, we find that the hypothesized phenomenon is\npronounced for CAD.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:57:07 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:21:13 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 02:02:40 GMT"}, {"version": "v4", "created": "Wed, 24 Mar 2021 01:46:15 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kaushik", "Divyansh", ""], ["Setlur", "Amrith", ""], ["Hovy", "Eduard", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "2010.02123", "submitter": "Yung-Sung Chuang", "authors": "Yung-Sung Chuang, Shang-Yu Su, Yun-Nung Chen", "title": "Lifelong Language Knowledge Distillation", "comments": "EMNLP 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to perform lifelong language learning (LLL) on a stream of\ndifferent tasks without any performance degradation comparing to the multi-task\ncounterparts. To address this issue, we present Lifelong Language Knowledge\nDistillation (L2KD), a simple but efficient method that can be easily applied\nto existing LLL architectures in order to mitigate the degradation.\nSpecifically, when the LLL model is trained on a new task, we assign a teacher\nmodel to first learn the new task, and pass the knowledge to the LLL model via\nknowledge distillation. Therefore, the LLL model can better adapt to the new\ntask while keeping the previously learned knowledge. Experiments show that the\nproposed L2KD consistently improves previous state-of-the-art models, and the\ndegradation comparing to multi-task models in LLL tasks is well mitigated for\nboth sequence generation and text classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:10:11 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chuang", "Yung-Sung", ""], ["Su", "Shang-Yu", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2010.02140", "submitter": "Jan Deriu", "authors": "Jan Deriu and Don Tuggener and Pius von D\\\"aniken and Jon Ander Campos\n  and Alvaro Rodrigo and Thiziri Belkacem and Aitor Soroa and Eneko Agirre and\n  Mark Cieliebak", "title": "Spot The Bot: A Robust and Efficient Framework for the Evaluation of\n  Conversational Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of time-efficient and reliable evaluation methods hamper the\ndevelopment of conversational dialogue systems (chatbots). Evaluations\nrequiring humans to converse with chatbots are time and cost-intensive, put\nhigh cognitive demands on the human judges, and yield low-quality results. In\nthis work, we introduce \\emph{Spot The Bot}, a cost-efficient and robust\nevaluation framework that replaces human-bot conversations with conversations\nbetween bots. Human judges then only annotate for each entity in a conversation\nwhether they think it is human or not (assuming there are humans participants\nin these conversations). These annotations then allow us to rank chatbots\nregarding their ability to mimic the conversational behavior of humans. Since\nwe expect that all bots are eventually recognized as such, we incorporate a\nmetric that measures which chatbot can uphold human-like behavior the longest,\ni.e., \\emph{Survival Analysis}. This metric has the ability to correlate a\nbot's performance to certain of its characteristics (e.g., \\ fluency or\nsensibleness), yielding interpretable results. The comparably low cost of our\nframework allows for frequent evaluations of chatbots during their evaluation\ncycle. We empirically validate our claims by applying \\emph{Spot The Bot} to\nthree domains, evaluating several state-of-the-art chatbots, and drawing\ncomparisons to related work. The framework is released as a ready-to-use tool.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:37:52 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Deriu", "Jan", ""], ["Tuggener", "Don", ""], ["von D\u00e4niken", "Pius", ""], ["Campos", "Jon Ander", ""], ["Rodrigo", "Alvaro", ""], ["Belkacem", "Thiziri", ""], ["Soroa", "Aitor", ""], ["Agirre", "Eneko", ""], ["Cieliebak", "Mark", ""]]}, {"id": "2010.02142", "submitter": "Anshul Wadhawan", "authors": "Janvijay Singh, Anshul Wadhawan", "title": "PublishInCovid19 at WNUT 2020 Shared Task-1: Entity Recognition in Wet\n  Lab Protocols using Structured Learning Ensemble and Contextualised\n  Embeddings", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2020.wnut-1.35", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the approach that we employed to address the task\nof Entity Recognition over Wet Lab Protocols -- a shared task in EMNLP\nWNUT-2020 Workshop. Our approach is composed of two phases. In the first phase,\nwe experiment with various contextualised word embeddings (like Flair,\nBERT-based) and a BiLSTM-CRF model to arrive at the best-performing\narchitecture. In the second phase, we create an ensemble composed of eleven\nBiLSTM-CRF models. The individual models are trained on random train-validation\nsplits of the complete dataset. Here, we also experiment with different output\nmerging schemes, including Majority Voting and Structured Learning Ensembling\n(SLE). Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for\nthe partial and exact match of the entity spans, respectively. We were ranked\nfirst and second, in terms of partial and exact match, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:45:30 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 08:33:45 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Singh", "Janvijay", ""], ["Wadhawan", "Anshul", ""]]}, {"id": "2010.02150", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Huy H. Nguyen, Junichi Yamagishi and Isao Echizen", "title": "Viable Threat on News Reading: Generating Biased News Using Natural\n  Language Models", "comments": "11 pages, 4 figures, 6 tables, Accepted at NLP+CSS Workshop at EMNLP\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in natural language generation has raised serious\nconcerns. High-performance language models are widely used for language\ngeneration tasks because they are able to produce fluent and meaningful\nsentences. These models are already being used to create fake news. They can\nalso be exploited to generate biased news, which can then be used to attack\nnews aggregators to change their reader's behavior and influence their bias. In\nthis paper, we use a threat model to demonstrate that the publicly available\nlanguage models can reliably generate biased news content based on an input\noriginal news. We also show that a large number of high-quality biased news\narticles can be generated using controllable text generation. A subjective\nevaluation with 80 participants demonstrated that the generated biased news is\ngenerally fluent, and a bias evaluation with 24 participants demonstrated that\nthe bias (left or right) is usually evident in the generated articles and can\nbe easily identified.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:55:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gupta", "Saurabh", ""], ["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "2010.02162", "submitter": "Zequn Sun", "authors": "Zequn Sun, Muhao Chen, Wei Hu, Chengming Wang, Jian Dai, Wei Zhang", "title": "Knowledge Association with Hyperbolic Knowledge Graph Embeddings", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing associations for knowledge graphs (KGs) through entity alignment,\nentity type inference and other related tasks benefits NLP applications with\ncomprehensive knowledge representations. Recent related methods built on\nEuclidean embeddings are challenged by the hierarchical structures and\ndifferent scales of KGs. They also depend on high embedding dimensions to\nrealize enough expressiveness. Differently, we explore with low-dimensional\nhyperbolic embeddings for knowledge association. We propose a hyperbolic\nrelational graph neural network for KG embedding and capture knowledge\nassociations with a hyperbolic transformation. Extensive experiments on entity\nalignment and type inference demonstrate the effectiveness and efficiency of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:11:35 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Sun", "Zequn", ""], ["Chen", "Muhao", ""], ["Hu", "Wei", ""], ["Wang", "Chengming", ""], ["Dai", "Jian", ""], ["Zhang", "Wei", ""]]}, {"id": "2010.02164", "submitter": "Kevin Yang", "authors": "Kevin Yang, Violet Yao, John DeNero, Dan Klein", "title": "A Streaming Approach For Efficient Batched Beam Search", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient batching strategy for variable-length decoding on GPU\narchitectures. During decoding, when candidates terminate or are pruned\naccording to heuristics, our streaming approach periodically \"refills\" the\nbatch before proceeding with a selected subset of candidates. We apply our\nmethod to variable-width beam search on a state-of-the-art machine translation\nmodel. Our method decreases runtime by up to 71% compared to a fixed-width beam\nsearch baseline and 17% compared to a variable-width baseline, while matching\nbaselines' BLEU. Finally, experiments show that our method can speed up\ndecoding in other domains, such as semantic and syntactic parsing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:13:34 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 22:22:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Kevin", ""], ["Yao", "Violet", ""], ["DeNero", "John", ""], ["Klein", "Dan", ""]]}, {"id": "2010.02172", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Rowan Hall Maudslay, Dami\\'an Blasi, Ryan Cotterell", "title": "Speakers Fill Lexical Semantic Gaps with Context", "comments": "Camera ready version of EMNLP 2020 publication. Code is available in\n  https://github.com/tpimentelms/lexical-ambiguity-in-context", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical ambiguity is widespread in language, allowing for the reuse of\neconomical word forms and therefore making language more efficient. If\nambiguous words cannot be disambiguated from context, however, this gain in\nefficiency might make language less clear -- resulting in frequent\nmiscommunication. For a language to be clear and efficiently encoded, we posit\nthat the lexical ambiguity of a word type should correlate with how much\ninformation context provides about it, on average. To investigate whether this\nis the case, we operationalise the lexical ambiguity of a word as the entropy\nof meanings it can take, and provide two ways to estimate this -- one which\nrequires human annotation (using WordNet), and one which does not (using BERT),\nmaking it readily applicable to a large number of languages. We validate these\nmeasures by showing that, on six high-resource languages, there are significant\nPearson correlations between our BERT-based estimate of ambiguity and the\nnumber of synonyms a word has in WordNet (e.g. $\\rho = 0.40$ in English). We\nthen test our main hypothesis -- that a word's lexical ambiguity should\nnegatively correlate with its contextual uncertainty -- and find significant\ncorrelations on all 18 typologically diverse languages we analyse. This\nsuggests that, in the presence of ambiguity, speakers compensate by making\ncontexts more informative.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:19:10 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 23:23:38 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 15:13:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Pimentel", "Tiago", ""], ["Maudslay", "Rowan Hall", ""], ["Blasi", "Dami\u00e1n", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2010.02179", "submitter": "Chieh-Yang Huang", "authors": "Yun-Hsuan Jen, Chieh-Yang Huang, Mei-Hua Chen, Ting-Hao 'Kenneth'\n  Huang, Lun-Wei Ku", "title": "Assessing the Helpfulness of Learning Materials with Inference-Based\n  Learner-Like Agent", "comments": "9 pages, to appear in EMNLP 2020 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many English-as-a-second language learners have trouble using near-synonym\nwords (e.g., small vs.little; briefly vs.shortly) correctly, and often look for\nexample sentences to learn how two nearly synonymous terms differ. Prior work\nuses hand-crafted scores to recommend sentences but has difficulty in adopting\nsuch scores to all the near-synonyms as near-synonyms differ in various ways.\nWe notice that the helpfulness of the learning material would reflect on the\nlearners' performance. Thus, we propose the inference-based learner-like agent\nto mimic learner behavior and identify good learning materials by examining the\nagent's performance. To enable the agent to behave like a learner, we leverage\nentailment modeling's capability of inferring answers from the provided\nmaterials. Experimental results show that the proposed agent is equipped with\ngood learner-like behavior to achieve the best performance in both\nfill-in-the-blank (FITB) and good example sentence selection tasks. We further\nconduct a classroom user study with college ESL learners. The results of the\nuser study show that the proposed agent can find out example sentences that\nhelp students learn more easily and efficiently. Compared to other models, the\nproposed agent improves the score of more than 17% of students after learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:24:57 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jen", "Yun-Hsuan", ""], ["Huang", "Chieh-Yang", ""], ["Chen", "Mei-Hua", ""], ["Huang", "Ting-Hao 'Kenneth'", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2010.02180", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Naomi Saphra, Adina Williams, Ryan Cotterell", "title": "Pareto Probing: Trading Off Accuracy for Complexity", "comments": "Tiago Pimentel and Naomi Saphra contributed equally to this work.\n  Camera ready version of EMNLP 2020 publication. Code available in\n  https://github.com/rycolab/pareto-probing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how to probe contextual word representations for linguistic\nstructure in a way that is both principled and useful has seen significant\nattention recently in the NLP literature. In our contribution to this\ndiscussion, we argue for a probe metric that reflects the fundamental trade-off\nbetween probe complexity and performance: the Pareto hypervolume. To measure\ncomplexity, we present a number of parametric and non-parametric metrics. Our\nexperiments using Pareto hypervolume as an evaluation metric show that probes\noften do not conform to our expectations---e.g., why should the non-contextual\nfastText representations encode more morpho-syntactic information than the\ncontextual BERT representations? These results suggest that common, simplistic\nprobing tasks, such as part-of-speech labeling and dependency arc labeling, are\ninadequate to evaluate the linguistic structure encoded in contextual word\nrepresentations. This leads us to propose full dependency parsing as a probing\ntask. In support of our suggestion that harder probing tasks are necessary, our\nexperiments with dependency parsing reveal a wide gap in syntactic knowledge\nbetween contextual and non-contextual representations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:27:31 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 23:02:57 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Pimentel", "Tiago", ""], ["Saphra", "Naomi", ""], ["Williams", "Adina", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2010.02194", "submitter": "Alexis Conneau", "authors": "Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur\n  Celebi, Michael Auli, Ves Stoyanov, Alexis Conneau", "title": "Self-training Improves Pre-training for Natural Language Understanding", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pre-training has led to much recent progress in natural language\nunderstanding. In this paper, we study self-training as another way to leverage\nunlabeled data through semi-supervised learning. To obtain additional data for\na specific task, we introduce SentAugment, a data augmentation method which\ncomputes task-specific query embeddings from labeled data to retrieve sentences\nfrom a bank of billions of unlabeled sentences crawled from the web. Unlike\nprevious semi-supervised methods, our approach does not require in-domain\nunlabeled data and is therefore more generally applicable. Experiments show\nthat self-training is complementary to strong RoBERTa baselines on a variety of\ntasks. Our augmentation approach leads to scalable and effective self-training\nwith improvements of up to 2.6% on standard text classification benchmarks.\nFinally, we also show strong gains on knowledge-distillation and few-shot\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:52:25 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Du", "Jingfei", ""], ["Grave", "Edouard", ""], ["Gunel", "Beliz", ""], ["Chaudhary", "Vishrav", ""], ["Celebi", "Onur", ""], ["Auli", "Michael", ""], ["Stoyanov", "Ves", ""], ["Conneau", "Alexis", ""]]}, {"id": "2010.02229", "submitter": "Xusen Yin", "authors": "Xusen Yin, Ralph Weischedel, Jonathan May", "title": "Learning to Generalize for Sequential Decision Making", "comments": "Findings of EMNLP2020, 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems of making sequences of decisions to accomplish tasks,\ninteracting via the medium of language. These problems are often tackled with\nreinforcement learning approaches. We find that these models do not generalize\nwell when applied to novel task domains. However, the large amount of\ncomputation necessary to adequately train and explore the search space of\nsequential decision making, under a reinforcement learning paradigm, precludes\nthe inclusion of large contextualized language models, which might otherwise\nenable the desired generalization ability. We introduce a teacher-student\nimitation learning methodology and a means of converting a reinforcement\nlearning model into a natural language understanding model. Together, these\nmethodologies enable the introduction of contextualized language models into\nthe sequential decision making problem space. We show that models can learn\nfaster and generalize more, leveraging both the imitation learning and the\nreformulation. Our models exceed teacher performance on various held-out\ndecision problems, by up to 7% on in-domain problems and 24% on out-of-domain\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:00:03 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yin", "Xusen", ""], ["Weischedel", "Ralph", ""], ["May", "Jonathan", ""]]}, {"id": "2010.02239", "submitter": "Katharina Kann", "authors": "Rajat Agarwal and Katharina Kann", "title": "Acrostic Poem Generation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task in the area of computational creativity: acrostic poem\ngeneration in English. Acrostic poems are poems that contain a hidden message;\ntypically, the first letter of each line spells out a word or short phrase. We\ndefine the task as a generation task with multiple constraints: given an input\nword, 1) the initial letters of each line should spell out the provided word,\n2) the poem's semantics should also relate to it, and 3) the poem should\nconform to a rhyming scheme. We further provide a baseline model for the task,\nwhich consists of a conditional neural language model in combination with a\nneural rhyming model. Since no dedicated datasets for acrostic poem generation\nexist, we create training data for our task by first training a separate topic\nprediction model on a small set of topic-annotated poems and then predicting\ntopics for additional poems. Our experiments show that the acrostic poems\ngenerated by our baseline are received well by humans and do not lose much\nquality due to the additional constraints. Last, we confirm that poems\ngenerated by our model are indeed closely related to the provided prompts, and\nthat pretraining on Wikipedia can boost performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:00:15 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Agarwal", "Rajat", ""], ["Kann", "Katharina", ""]]}, {"id": "2010.02246", "submitter": "Sopan Khosla", "authors": "Sopan Khosla, Shikhar Vashishth, Jill Fain Lehman, Carolyn Rose", "title": "MedFilter: Improving Extraction of Task-relevant Utterances through\n  Integration of Discourse Structure and Ontological Knowledge", "comments": "Accepted as Long Paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information extraction from conversational data is particularly challenging\nbecause the task-centric nature of conversation allows for effective\ncommunication of implicit information by humans, but is challenging for\nmachines. The challenges may differ between utterances depending on the role of\nthe speaker within the conversation, especially when relevant expertise is\ndistributed asymmetrically across roles. Further, the challenges may also\nincrease over the conversation as more shared context is built up through\ninformation communicated implicitly earlier in the dialogue. In this paper, we\npropose the novel modeling approach MedFilter, which addresses these insights\nin order to increase performance at identifying and categorizing task-relevant\nutterances, and in so doing, positively impacts performance at a downstream\ninformation extraction task. We evaluate this approach on a corpus of nearly\n7,000 doctor-patient conversations where MedFilter is used to identify\nmedically relevant contributions to the discussion (achieving a 10% improvement\nover SOTA baselines in terms of area under the PR curve). Identifying\ntask-relevant utterances benefits downstream medical processing, achieving\nimprovements of 15%, 105%, and 23% respectively for the extraction of symptoms,\nmedications, and complaints.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:01:38 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 18:28:49 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Khosla", "Sopan", ""], ["Vashishth", "Shikhar", ""], ["Lehman", "Jill Fain", ""], ["Rose", "Carolyn", ""]]}, {"id": "2010.02256", "submitter": "Morteza Pourreza Shahri", "authors": "Morteza Pourreza Shahri, Amir Tahmasebi, Bingyang Ye, Henghui Zhu,\n  Javed Aslam, Timothy Ferris", "title": "An Ensemble Approach for Automatic Structuring of Radiology Reports", "comments": "Accepted by the 3rd Clinical NLP Workshop at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic structuring of electronic medical records is of high demand for\nclinical workflow solutions to facilitate extraction, storage, and querying of\npatient care information. However, developing a scalable solution is extremely\nchallenging, specifically for radiology reports, as most healthcare institutes\nuse either no template or department/institute specific templates. Moreover,\nradiologists' reporting style varies from one to another as sentences are\ntelegraphic and do not follow general English grammar rules. We present an\nensemble method that consolidates the predictions of three models, capturing\nvarious attributes of textual information for automatic labeling of sentences\nwith section labels. These three models are: 1) Focus Sentence model, capturing\ncontext of the target sentence; 2) Surrounding Context model, capturing the\nneighboring context of the target sentence; and finally, 3) Formatting/Layout\nmodel, aimed at learning report formatting cues. We utilize Bi-directional\nLSTMs, followed by sentence encoders, to acquire the context. Furthermore, we\ndefine several features that incorporate the structure of reports. We compare\nour proposed approach against multiple baselines and state-of-the-art\napproaches on a proprietary dataset as well as 100 manually annotated radiology\nnotes from the MIMIC-III dataset, which we are making publicly available. Our\nproposed approach significantly outperforms other approaches by achieving 97.1%\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:11:23 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 00:06:51 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Shahri", "Morteza Pourreza", ""], ["Tahmasebi", "Amir", ""], ["Ye", "Bingyang", ""], ["Zhu", "Henghui", ""], ["Aslam", "Javed", ""], ["Ferris", "Timothy", ""]]}, {"id": "2010.02260", "submitter": "Jatin Ganhotra", "authors": "Jatin Ganhotra, Robert Moore, Sachindra Joshi and Kahini Wadhawan", "title": "Effects of Naturalistic Variation in Goal-Oriented Dialog", "comments": "Findings of EMNLP 2020. The updated datasets are available at:\n  https://github.com/IBM/naturalistic-variation-goal-oriented-dialog-datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing benchmarks used to evaluate the performance of end-to-end neural\ndialog systems lack a key component: natural variation present in human\nconversations. Most datasets are constructed through crowdsourcing, where the\ncrowd workers follow a fixed template of instructions while enacting the role\nof a user/agent. This results in straight-forward, somewhat routine, and mostly\ntrouble-free conversations, as crowd workers do not think to represent the full\nrange of actions that occur naturally with real users. In this work, we\ninvestigate the impact of naturalistic variation on two goal-oriented datasets:\nbAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new\nand more effective testbeds for both datasets, by introducing naturalistic\nvariation by the user. We observe that there is a significant drop in\nperformance (more than 60% in Ent. F1 on SMD and 85% in per-dialog accuracy on\nbAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet\nand GLMP on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:13:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ganhotra", "Jatin", ""], ["Moore", "Robert", ""], ["Joshi", "Sachindra", ""], ["Wadhawan", "Kahini", ""]]}, {"id": "2010.02295", "submitter": "Chenguang Zhu", "authors": "Yu-An Chung, Chenguang Zhu, Michael Zeng", "title": "SPLAT: Speech-Language Joint Pre-Training for Spoken Language\n  Understanding", "comments": null, "journal-ref": "North American Chapter of the Association for Computational\n  Linguistics (NAACL), Mexico City, Mexico, 2021. North American Chapter of the\n  Association for Computational Linguistics (NAACL), Mexico City, Mexico, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU) requires a model to analyze input\nacoustic signal to understand its linguistic content and make predictions. To\nboost the models' performance, various pre-training methods have been proposed\nto learn rich representations from large-scale unannotated speech and text.\nHowever, the inherent disparities between the two modalities necessitate a\nmutual analysis. In this paper, we propose a novel semi-supervised learning\nframework, SPLAT, to jointly pre-train the speech and language modules. Besides\nconducting a self-supervised masked language modeling task on the two\nindividual modules using unpaired speech and text, SPLAT aligns representations\nfrom the two modules in a shared latent space using a small amount of paired\nspeech and text. Thus, during fine-tuning, the speech module alone can produce\nrepresentations carrying both acoustic information and contextual semantic\nknowledge of an input acoustic signal. Experimental results verify the\neffectiveness of our approach on various SLU tasks. For example, SPLAT improves\nthe previous state-of-the-art performance on the Spoken SQuAD dataset by more\nthan 10%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:29:49 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 01:38:07 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 00:55:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chung", "Yu-An", ""], ["Zhu", "Chenguang", ""], ["Zeng", "Michael", ""]]}, {"id": "2010.02301", "submitter": "Xinyu Hua", "authors": "Xinyu Hua and Lu Wang", "title": "PAIR: Planning and Iterative Refinement in Pre-trained Transformers for\n  Long Text Generation", "comments": "Accepted at EMNLP 2020 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained Transformers have enabled impressive breakthroughs in generating\nlong and fluent text, yet their outputs are often \"rambling\" without coherently\narranged content. In this work, we present a novel content-controlled text\ngeneration framework, PAIR, with planning and iterative refinement, which is\nbuilt upon a large model, BART. We first adapt the BERT model to automatically\nconstruct the content plans, consisting of keyphrase assignments and their\ncorresponding sentence-level positions. The BART model is employed for\ngeneration without modifying its structure. We then propose a refinement\nalgorithm to gradually enhance the generation quality within the\nsequence-to-sequence framework. Evaluation with automatic metrics shows that\nadding planning consistently improves the generation quality on three distinct\ndomains, with an average of 20 BLEU points and 12 METEOR points improvements.\nIn addition, human judges rate our system outputs to be more relevant and\ncoherent than comparisons without planning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:45:03 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hua", "Xinyu", ""], ["Wang", "Lu", ""]]}, {"id": "2010.02305", "submitter": "Jatin Ganhotra", "authors": "Jatin Ganhotra, Haggai Roitman, Doron Cohen, Nathaniel Mills, Chulaka\n  Gunasekara, Yosi Mass, Sachindra Joshi, Luis Lastras and David Konopnicki", "title": "Conversational Document Prediction to Assist Customer Care Agents", "comments": "EMNLP 2020. The released Twitter dataset is available at:\n  https://github.com/IBM/twitter-customer-care-document-prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A frequent pattern in customer care conversations is the agents responding\nwith appropriate webpage URLs that address users' needs. We study the task of\npredicting the documents that customer care agents can use to facilitate users'\nneeds. We also introduce a new public dataset which supports the aforementioned\nproblem. Using this dataset and two others, we investigate state-of-the art\ndeep learning (DL) and information retrieval (IR) models for the task.\nAdditionally, we analyze the practicality of such systems in terms of inference\ntime complexity. Our show that an hybrid IR+DL approach provides the best of\nboth worlds.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:53:41 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ganhotra", "Jatin", ""], ["Roitman", "Haggai", ""], ["Cohen", "Doron", ""], ["Mills", "Nathaniel", ""], ["Gunasekara", "Chulaka", ""], ["Mass", "Yosi", ""], ["Joshi", "Sachindra", ""], ["Lastras", "Luis", ""], ["Konopnicki", "David", ""]]}, {"id": "2010.02307", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang", "title": "KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation", "comments": "Accepted to Main Conference of EMNLP 2020 as Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-to-text generation has recently attracted substantial interests due to\nits wide applications. Existing methods have shown impressive performance on an\narray of tasks. However, they rely on a significant amount of labeled data for\neach task, which is costly to acquire and thus limits their application to new\ntasks and domains. In this paper, we propose to leverage pre-training and\ntransfer learning to address this issue. We propose a knowledge-grounded\npre-training (KGPT), which consists of two parts, 1) a general\nknowledge-grounded generation model to generate knowledge-enriched text. 2) a\npre-training paradigm on a massive knowledge-grounded text corpus crawled from\nthe web. The pre-trained model can be fine-tuned on various data-to-text\ngeneration tasks to generate task-specific text. We adopt three settings,\nnamely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.\nUnder the fully-supervised setting, our model can achieve remarkable gains over\nthe known baselines. Under zero-shot setting, our model without seeing any\nexamples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.\nUnder the few-shot setting, our model only needs about one-fifteenth as many\nlabeled examples to achieve the same level of performance as baseline models.\nThese experiments consistently prove the strong generalization ability of our\nproposed framework https://github.com/wenhuchen/KGPT.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:59:05 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 18:09:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Wenhu", ""], ["Su", "Yu", ""], ["Yan", "Xifeng", ""], ["Wang", "William Yang", ""]]}, {"id": "2010.02316", "submitter": "Ameet Deshpande", "authors": "Ameet Deshpande, Eve Fleisig", "title": "Sentiment Analysis for Reinforcement Learning", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning (RL) has been successful in natural language\nprocessing (NLP) domains such as dialogue generation and text-based games, it\ntypically faces the problem of sparse rewards that leads to slow or no\nconvergence. Traditional methods that use text descriptions to extract only a\nstate representation ignore the feedback inherently present in them. In\ntext-based games, for example, descriptions like \"Good Job! You ate the food}\"\nindicate progress, and descriptions like \"You entered a new room\" indicate\nexploration. Positive and negative cues like these can be converted to rewards\nthrough sentiment analysis. This technique converts the sparse reward problem\ninto a dense one, which is easier to solve. Furthermore, this can enable\nreinforcement learning without rewards, in which the agent learns entirely from\nthese intrinsic sentiment rewards. This framework is similar to intrinsic\nmotivation, where the environment does not necessarily provide the rewards, but\nthe agent analyzes and realizes them by itself. We find that providing dense\nrewards in text-based games using sentiment analysis improves performance under\nsome conditions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:15:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Deshpande", "Ameet", ""], ["Fleisig", "Eve", ""]]}, {"id": "2010.02322", "submitter": "Yue Yu", "authors": "Rongzhi Zhang, Yue Yu and Chao Zhang", "title": "SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup", "comments": "EMNLP 2020 Long Paper", "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Active learning is an important technique for low-resource sequence labeling\ntasks. However, current active sequence labeling methods use the queried\nsamples alone in each iteration, which is an inefficient way of leveraging\nhuman annotations. We propose a simple but effective data augmentation method\nto improve the label efficiency of active sequence labeling. Our method,\nSeqMix, simply augments the queried samples by generating extra labeled\nsequences in each iteration. The key difficulty is to generate plausible\nsequences along with token-level labels. In SeqMix, we address this challenge\nby performing mixup for both sequences and token-level labels of the queried\nsamples. Furthermore, we design a discriminator during sequence mixup, which\njudges whether the generated sequences are plausible or not. Our experiments on\nNamed Entity Recognition and Event Detection tasks show that SeqMix can improve\nthe standard active sequence labeling method by $2.27\\%$--$3.75\\%$ in terms of\n$F_1$ scores. The code and data for SeqMix can be found at\nhttps://github.com/rz-zhang/SeqMix\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:27:14 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhang", "Rongzhi", ""], ["Yu", "Yue", ""], ["Zhang", "Chao", ""]]}, {"id": "2010.02329", "submitter": "Boxin Wang", "authors": "Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li,\n  Jingjing Liu", "title": "InfoBERT: Improving Robustness of Language Models from An Information\n  Theoretic Perspective", "comments": "Accepted to ICLR 2021. 23 pages, 9 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale language models such as BERT have achieved state-of-the-art\nperformance across a wide range of NLP tasks. Recent studies, however, show\nthat such BERT-based models are vulnerable facing the threats of textual\nadversarial attacks. We aim to address this problem from an\ninformation-theoretic perspective, and propose InfoBERT, a novel learning\nframework for robust fine-tuning of pre-trained language models. InfoBERT\ncontains two mutual-information-based regularizers for model training: (i) an\nInformation Bottleneck regularizer, which suppresses noisy mutual information\nbetween the input and the feature representation; and (ii) a Robust Feature\nregularizer, which increases the mutual information between local robust\nfeatures and global features. We provide a principled way to theoretically\nanalyze and improve the robustness of representation learning for language\nmodels in both standard and adversarial training. Extensive experiments\ndemonstrate that InfoBERT achieves state-of-the-art robust accuracy over\nseveral adversarial datasets on Natural Language Inference (NLI) and Question\nAnswering (QA) tasks. Our code is available at\nhttps://github.com/AI-secure/InfoBERT.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:49:26 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 13:24:03 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 03:58:19 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 11:44:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Boxin", ""], ["Wang", "Shuohang", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Jia", "Ruoxi", ""], ["Li", "Bo", ""], ["Liu", "Jingjing", ""]]}, {"id": "2010.02338", "submitter": "Tianlu Wang", "authors": "Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen,\n  Alex Beutel, Ed Chi", "title": "CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial\n  Text Generation", "comments": "6 pages, accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP models are shown to suffer from robustness issues, i.e., a model's\nprediction can be easily changed under small perturbations to the input. In\nthis work, we present a Controlled Adversarial Text Generation (CAT-Gen) model\nthat, given an input text, generates adversarial texts through controllable\nattributes that are known to be invariant to task labels. For example, in order\nto attack a model for sentiment classification over product reviews, we can use\nthe product categories as the controllable attribute which would not change the\nsentiment of the reviews. Experiments on real-world NLP datasets demonstrate\nthat our method can generate more diverse and fluent adversarial texts,\ncompared to many existing adversarial text generation approaches. We further\nuse our generated adversarial examples to improve models through adversarial\ntraining, and we demonstrate that our generated attacks are more robust against\nmodel re-training and different model architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:07:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Tianlu", ""], ["Wang", "Xuezhi", ""], ["Qin", "Yao", ""], ["Packer", "Ben", ""], ["Li", "Kang", ""], ["Chen", "Jilin", ""], ["Beutel", "Alex", ""], ["Chi", "Ed", ""]]}, {"id": "2010.02339", "submitter": "Ashiqur KhudaBukhsh Ashiqur Rahman KhudaBukhsh", "authors": "Ashiqur R. KhudaBukhsh, Rupak Sarkar, Mark S. Kamlet, Tom M. Mitchell", "title": "We Don't Speak the Same Language: Interpreting Polarization through\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polarization among US political parties, media and elites is a widely studied\ntopic. Prominent lines of prior research across multiple disciplines have\nobserved and analyzed growing polarization in social media. In this paper, we\npresent a new methodology that offers a fresh perspective on interpreting\npolarization through the lens of machine translation. With a novel proposition\nthat two sub-communities are speaking in two different \\emph{languages}, we\ndemonstrate that modern machine translation methods can provide a simple yet\npowerful and interpretable framework to understand the differences between two\n(or more) large-scale social media discussion data sets at the granularity of\nwords. Via a substantial corpus of 86.6 million comments by 6.5 million users\non over 200,000 news videos hosted by YouTube channels of four prominent US\nnews networks, we demonstrate that simple word-level and phrase-level\ntranslation pairs can reveal deep insights into the current political divide --\nwhat is \\emph{black lives matter} to one can be \\emph{all lives matter} to the\nother.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:16:30 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 07:34:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["KhudaBukhsh", "Ashiqur R.", ""], ["Sarkar", "Rupak", ""], ["Kamlet", "Mark S.", ""], ["Mitchell", "Tom M.", ""]]}, {"id": "2010.02352", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, George Foster, Colin Cherry", "title": "Inference Strategies for Machine Translation with Conditional Masking", "comments": "EMNLP 2020, updated Fig 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional masked language model (CMLM) training has proven successful for\nnon-autoregressive and semi-autoregressive sequence generation tasks, such as\nmachine translation. Given a trained CMLM, however, it is not clear what the\nbest inference strategy is. We formulate masked inference as a factorization of\nconditional probabilities of partial sequences, show that this does not harm\nperformance, and investigate a number of simple heuristics motivated by this\nperspective. We identify a thresholding strategy that has advantages over the\nstandard \"mask-predict\" algorithm, and provide analyses of its behavior on\nmachine translation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:50:09 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 15:14:40 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Kreutzer", "Julia", ""], ["Foster", "George", ""], ["Cherry", "Colin", ""]]}, {"id": "2010.02353", "submitter": "Julia Kreutzer", "authors": "Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa,\n  Tajudeen Kolawole, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen\n  Hassan Muhammad, Salomon Kabongo, Salomey Osei, Sackey Freshia, Rubungo Andre\n  Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa, Mofe\n  Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Jane Martinus,\n  Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia\n  Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Abbott, Iroro Orife, Ignatius\n  Ezeani, Idris Abdulkabir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru,\n  Ghollah Kioko, Espoir Murhabazi, Elan van Biljon, Daniel Whitenack,\n  Christopher Onyefuluchi, Chris Emezue, Bonaventure Dossou, Blessing Sibanda,\n  Blessing Itoro Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp \\\"Oktem,\n  Adewale Akinfaderin, Abdallah Bashir", "title": "Participatory Research for Low-resourced Machine Translation: A Case\n  Study in African Languages", "comments": "Findings of EMNLP 2020; updated benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in NLP lacks geographic diversity, and the question of how NLP can\nbe scaled to low-resourced languages has not yet been adequately solved.\n\"Low-resourced\"-ness is a complex problem going beyond data availability and\nreflects systemic problems in society. In this paper, we focus on the task of\nMachine Translation (MT), that plays a crucial role for information\naccessibility and communication worldwide. Despite immense improvements in MT\nover the past decade, MT is centered around a few high-resourced languages. As\nMT researchers cannot solve the problem of low-resourcedness alone, we propose\nparticipatory research as a means to involve all necessary agents required in\nthe MT development process. We demonstrate the feasibility and scalability of\nparticipatory research with a case study on MT for African languages. Its\nimplementation leads to a collection of novel translation datasets, MT\nbenchmarks for over 30 languages, with human evaluations for a third of them,\nand enables participants without formal training to make a unique scientific\ncontribution. Benchmarks, models, data, code, and evaluation results are\nreleased under https://github.com/masakhane-io/masakhane-mt.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:50:38 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 23:30:45 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Nekoto", "Wilhelmina", ""], ["Marivate", "Vukosi", ""], ["Matsila", "Tshinondiwa", ""], ["Fasubaa", "Timi", ""], ["Kolawole", "Tajudeen", ""], ["Fagbohungbe", "Taiwo", ""], ["Akinola", "Solomon Oluwole", ""], ["Muhammad", "Shamsuddeen Hassan", ""], ["Kabongo", "Salomon", ""], ["Osei", "Salomey", ""], ["Freshia", "Sackey", ""], ["Niyongabo", "Rubungo Andre", ""], ["Macharm", "Ricky", ""], ["Ogayo", "Perez", ""], ["Ahia", "Orevaoghene", ""], ["Meressa", "Musie", ""], ["Adeyemi", "Mofe", ""], ["Mokgesi-Selinga", "Masabata", ""], ["Okegbemi", "Lawrence", ""], ["Martinus", "Laura Jane", ""], ["Tajudeen", "Kolawole", ""], ["Degila", "Kevin", ""], ["Ogueji", "Kelechi", ""], ["Siminyu", "Kathleen", ""], ["Kreutzer", "Julia", ""], ["Webster", "Jason", ""], ["Ali", "Jamiil Toure", ""], ["Abbott", "Jade", ""], ["Orife", "Iroro", ""], ["Ezeani", "Ignatius", ""], ["Dangana", "Idris Abdulkabir", ""], ["Kamper", "Herman", ""], ["Elsahar", "Hady", ""], ["Duru", "Goodness", ""], ["Kioko", "Ghollah", ""], ["Murhabazi", "Espoir", ""], ["van Biljon", "Elan", ""], ["Whitenack", "Daniel", ""], ["Onyefuluchi", "Christopher", ""], ["Emezue", "Chris", ""], ["Dossou", "Bonaventure", ""], ["Sibanda", "Blessing", ""], ["Bassey", "Blessing Itoro", ""], ["Olabiyi", "Ayodele", ""], ["Ramkilowan", "Arshath", ""], ["\u00d6ktem", "Alp", ""], ["Akinfaderin", "Adewale", ""], ["Bashir", "Abdallah", ""]]}, {"id": "2010.02357", "submitter": "Tsvetomila Mihaylova", "authors": "Tsvetomila Mihaylova, Vlad Niculae, Andr\\'e F. T. Martins", "title": "Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent\n  Structure Learning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent structure models are a powerful tool for modeling language data: they\ncan mitigate the error propagation and annotation bottleneck in pipeline\nsystems, while simultaneously uncovering linguistic insights about the data.\nOne challenge with end-to-end training of these models is the argmax operation,\nwhich has null gradient. In this paper, we focus on surrogate gradients, a\npopular strategy to deal with this problem. We explore latent structure\nlearning through the angle of pulling back the downstream learning objective.\nIn this paradigm, we discover a principled motivation for both the\nstraight-through estimator (STE) as well as the recently-proposed SPIGOT - a\nvariant of STE for structured models. Our perspective leads to new algorithms\nin the same family. We empirically compare the known and the novel pulled-back\nestimators against the popular alternatives, yielding new insight for\npractitioners and revealing intriguing failure cases.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:56:00 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Mihaylova", "Tsvetomila", ""], ["Niculae", "Vlad", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2010.02375", "submitter": "Robert Hawkins", "authors": "Robert D. Hawkins, Takateru Yamakoshi, Thomas L. Griffiths, Adele E.\n  Goldberg", "title": "Investigating representations of verb bias in neural language models", "comments": "Accepted to EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages typically provide more than one grammatical construction to express\ncertain types of messages. A speaker's choice of construction is known to\ndepend on multiple factors, including the choice of main verb -- a phenomenon\nknown as \\emph{verb bias}. Here we introduce DAIS, a large benchmark dataset\ncontaining 50K human judgments for 5K distinct sentence pairs in the English\ndative alternation. This dataset includes 200 unique verbs and systematically\nvaries the definiteness and length of arguments. We use this dataset, as well\nas an existing corpus of naturally occurring data, to evaluate how well recent\nneural language models capture human preferences. Results show that larger\nmodels perform better than smaller models, and transformer architectures (e.g.\nGPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under\ncomparable parameter and training settings. Additional analyses of internal\nfeature representations suggest that transformers may better integrate specific\nlexical information with grammatical constructions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:39:08 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 19:37:48 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hawkins", "Robert D.", ""], ["Yamakoshi", "Takateru", ""], ["Griffiths", "Thomas L.", ""], ["Goldberg", "Adele E.", ""]]}, {"id": "2010.02377", "submitter": "Alexander Hoyle", "authors": "Alexander Hoyle, Pranav Goel, Philip Resnik", "title": "Improving Neural Topic Models using Knowledge Distillation", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are often used to identify human-interpretable topics to help\nmake sense of large document collections. We use knowledge distillation to\ncombine the best attributes of probabilistic topic models and pretrained\ntransformers. Our modular method can be straightforwardly applied with any\nneural topic model to improve topic quality, which we demonstrate using two\nmodels having disparate architectures, obtaining state-of-the-art topic\ncoherence. We show that our adaptable framework not only improves performance\nin the aggregate over all estimated topics, as is commonly reported, but also\nin head-to-head comparisons of aligned topics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:49:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hoyle", "Alexander", ""], ["Goel", "Pranav", ""], ["Resnik", "Philip", ""]]}, {"id": "2010.02384", "submitter": "Tejas Srinivasan", "authors": "Tejas Srinivasan, Ramon Sanabria, Florian Metze and Desmond Elliott", "title": "Fine-Grained Grounding for Multimodal Speech Recognition", "comments": "Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal automatic speech recognition systems integrate information from\nimages to improve speech recognition quality, by grounding the speech in the\nvisual context. While visual signals have been shown to be useful for\nrecovering entities that have been masked in the audio, these models should be\ncapable of recovering a broader range of word types. Existing systems rely on\nglobal visual features that represent the entire image, but localizing the\nrelevant regions of the image will make it possible to recover a larger set of\nwords, such as adjectives and verbs. In this paper, we propose a model that\nuses finer-grained visual information from different parts of the image, using\nautomatic object proposals. In experiments on the Flickr8K Audio Captions\nCorpus, we find that our model improves over approaches that use global visual\nfeatures, that the proposals enable the model to recover entities and other\nrelated words, such as adjectives, and that improvements are due to the model's\nability to localize the correct proposals.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:06:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Srinivasan", "Tejas", ""], ["Sanabria", "Ramon", ""], ["Metze", "Florian", ""], ["Elliott", "Desmond", ""]]}, {"id": "2010.02386", "submitter": "Mo Yu", "authors": "Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu\n  Chang", "title": "Interactive Fiction Game Playing as Multi-Paragraph Reading\n  Comprehension with Reinforcement Learning", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive Fiction (IF) games with real human-written natural language texts\nprovide a new natural evaluation for language understanding techniques. In\ncontrast to previous text games with mostly synthetic texts, IF games pose\nlanguage understanding challenges on the human-written textual descriptions of\ndiverse and sophisticated game worlds and language generation challenges on the\naction command generation from less restricted combinatorial space. We take a\nnovel perspective of IF game solving and re-formulate it as Multi-Passage\nReading Comprehension (MPRC) tasks. Our approaches utilize the context-query\nattention mechanisms and the structured prediction in MPRC to efficiently\ngenerate and evaluate action outputs and apply an object-centric historical\nobservation retrieval strategy to mitigate the partial observability of the\ntextual observations. Extensive experiments on the recent IF benchmark\n(Jericho) demonstrate clear advantages of our approaches achieving high winning\nrates and low data requirements compared to all previous approaches. Our source\ncode is available at: https://github.com/XiaoxiaoGuo/rcdqn.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:09:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Guo", "Xiaoxiao", ""], ["Yu", "Mo", ""], ["Gao", "Yupeng", ""], ["Gan", "Chuang", ""], ["Campbell", "Murray", ""], ["Chang", "Shiyu", ""]]}, {"id": "2010.02394", "submitter": "Lichao Sun", "authors": "Lichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Philip S. Yu,\n  Lifang He", "title": "Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks", "comments": "Accepted by COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixup is the latest data augmentation technique that linearly interpolates\ninput examples and the corresponding labels. It has shown strong effectiveness\nin image classification by interpolating images at the pixel level. Inspired by\nthis line of research, in this paper, we explore i) how to apply mixup to\nnatural language processing tasks since text data can hardly be mixed in the\nraw format; ii) if mixup is still effective in transformer-based learning\nmodels, e.g., BERT. To achieve the goal, we incorporate mixup to\ntransformer-based pre-trained architecture, named \"mixup-transformer\", for a\nwide range of NLP tasks while keeping the whole end-to-end training system. We\nevaluate the proposed framework by running extensive experiments on the GLUE\nbenchmark. Furthermore, we also examine the performance of mixup-transformer in\nlow-resource scenarios by reducing the training data with a certain ratio. Our\nstudies show that mixup is a domain-independent data augmentation technique to\npre-trained language models, resulting in significant performance improvement\nfor transformer-based models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:37:30 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 23:51:24 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Sun", "Lichao", ""], ["Xia", "Congying", ""], ["Yin", "Wenpeng", ""], ["Liang", "Tingting", ""], ["Yu", "Philip S.", ""], ["He", "Lifang", ""]]}, {"id": "2010.02395", "submitter": "Arbi Haza Nasution", "authors": "Arbi Haza Nasution, Yohei Murakami, Toru Ishida", "title": "A Generalized Constraint Approach to Bilingual Dictionary Induction for\n  Low-Resource Language Families", "comments": "30 pages, 13 figures, 14 tables, published in ACM TALLIP", "journal-ref": "ACM Trans. Asian Low-Resour. Lang. Inf. Process. 17, 2, Article 9\n  (November 2017), 29 pages", "doi": "10.1145/3138815", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack or absence of parallel and comparable corpora makes bilingual\nlexicon extraction a difficult task for low-resource languages. The pivot\nlanguage and cognate recognition approaches have been proven useful for\ninducing bilingual lexicons for such languages. We propose constraint-based\nbilingual lexicon induction for closely-related languages by extending\nconstraints from the recent pivot-based induction technique and further\nenabling multiple symmetry assumption cycles to reach many more cognates in the\ntransgraph. We further identify cognate synonyms to obtain many-to-many\ntranslation pairs. This paper utilizes four datasets: one Austronesian\nlow-resource language and three Indo-European high-resource languages. We use\nthree constraint-based methods from our previous work, the Inverse Consultation\nmethod and translation pairs generated from the Cartesian product of input\ndictionaries as baselines. We evaluate our result using the metrics of\nprecision, recall and F-score. Our customizable approach allows the user to\nconduct cross-validation to predict the optimal hyperparameters (cognate\nthreshold and cognate synonym threshold) with various combinations of\nheuristics and the number of symmetry assumption cycles to gain the highest\nF-score. Our proposed methods have statistically significant improvement of\nprecision and F-score compared to our previous constraint-based methods. The\nresults show that our method demonstrates the potential to complement other\nbilingual dictionary creation methods like word alignment models using parallel\ncorpora for high-resource languages while well handling low-resource languages.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:41:04 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nasution", "Arbi Haza", ""], ["Murakami", "Yohei", ""], ["Ishida", "Toru", ""]]}, {"id": "2010.02396", "submitter": "Arbi Haza Nasution", "authors": "Arbi Haza Nasution, Yohei Murakami, Toru Ishida", "title": "Plan Optimization to Bilingual Dictionary Induction for Low-Resource\n  Language Families", "comments": "29 pages, 16 figures, 9 tables, accepted for publication in ACM\n  TALLIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating bilingual dictionary is the first crucial step in enriching\nlow-resource languages. Especially for the closely-related ones, it has been\nshown that the constraint-based approach is useful for inducing bilingual\nlexicons from two bilingual dictionaries via the pivot language. However, if\nthere are no available machine-readable dictionaries as input, we need to\nconsider manual creation by bilingual native speakers. To reach a goal of\ncomprehensively create multiple bilingual dictionaries, even if we already have\nseveral existing machine-readable bilingual dictionaries, it is still difficult\nto determine the execution order of the constraint-based approach to reducing\nthe total cost. Plan optimization is crucial in composing the order of\nbilingual dictionaries creation with the consideration of the methods and their\ncosts. We formalize the plan optimization for creating bilingual dictionaries\nby utilizing Markov Decision Process (MDP) with the goal to get a more accurate\nestimation of the most feasible optimal plan with the least total cost before\nfully implementing the constraint-based bilingual lexicon induction. We model a\nprior beta distribution of bilingual lexicon induction precision with language\nsimilarity and polysemy of the topology as $\\alpha$ and $\\beta$ parameters. It\nis further used to model cost function and state transition probability. We\nestimated the cost of all investment plan as a baseline for evaluating the\nproposed MDP-based approach with total cost as an evaluation metric. After\nutilizing the posterior beta distribution in the first batch of experiments to\nconstruct the prior beta distribution in the second batch of experiments, the\nresult shows 61.5\\% of cost reduction compared to the estimated all investment\nplan and 39.4\\% of cost reduction compared to the estimated MDP optimal plan.\nThe MDP-based proposal outperformed the baseline on the total cost.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:43:40 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nasution", "Arbi Haza", ""], ["Murakami", "Yohei", ""], ["Ishida", "Toru", ""]]}, {"id": "2010.02399", "submitter": "Ameet Deshpande", "authors": "Ameet Deshpande, Karthik Narasimhan", "title": "Guiding Attention for Self-Supervised Learning with Transformers", "comments": "Accepted to Findings of EMNLP, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple and effective technique to allow for\nefficient self-supervised learning with bi-directional Transformers. Our\napproach is motivated by recent studies demonstrating that self-attention\npatterns in trained models contain a majority of non-linguistic regularities.\nWe propose a computationally efficient auxiliary loss function to guide\nattention heads to conform to such patterns. Our method is agnostic to the\nactual pre-training objective and results in faster convergence of models as\nwell as better performance on downstream tasks compared to the baselines,\nachieving state of the art results in low-resource settings. Surprisingly, we\nalso find that linguistic properties of attention heads are not necessarily\ncorrelated with language modeling performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 00:04:08 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Deshpande", "Ameet", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2010.02405", "submitter": "Yi Yang", "authors": "Yi Yang and Arzoo Katiyar", "title": "Simple and Effective Few-Shot Named Entity Recognition with Structured\n  Nearest Neighbor Learning", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple few-shot named entity recognition (NER) system based on\nnearest neighbor learning and structured inference. Our system uses a\nsupervised NER model trained on the source domain, as a feature extractor.\nAcross several test domains, we show that a nearest neighbor classifier in this\nfeature-space is far more effective than the standard meta-learning approaches.\nWe further propose a cheap but effective method to capture the label\ndependencies between entity tags without expensive CRF training. We show that\nour method of combining structured decoding with nearest neighbor learning\nachieves state-of-the-art performance on standard few-shot NER evaluation\ntasks, improving F1 scores by $6\\%$ to $16\\%$ absolute points over prior\nmeta-learning based systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 00:25:50 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yang", "Yi", ""], ["Katiyar", "Arzoo", ""]]}, {"id": "2010.02407", "submitter": "Vipul Raheja", "authors": "Vipul Raheja and Dimitrios Alikaniotis", "title": "Adversarial Grammatical Error Correction", "comments": "13 Pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works in Grammatical Error Correction (GEC) have leveraged the\nprogress in Neural Machine Translation (NMT), to learn rewrites from parallel\ncorpora of grammatically incorrect and corrected sentences, achieving\nstate-of-the-art results. At the same time, Generative Adversarial Networks\n(GANs) have been successful in generating realistic texts across many different\ntasks by learning to directly minimize the difference between human-generated\nand synthetic text. In this work, we present an adversarial learning approach\nto GEC, using the generator-discriminator framework. The generator is a\nTransformer model, trained to produce grammatically correct sentences given\ngrammatically incorrect ones. The discriminator is a sentence-pair\nclassification model, trained to judge a given pair of grammatically\nincorrect-correct sentences on the quality of grammatical correction. We\npre-train both the discriminator and the generator on parallel texts and then\nfine-tune them further using a policy gradient method that assigns high rewards\nto sentences which could be true corrections of the grammatically incorrect\ntext. Experimental results on FCE, CoNLL-14, and BEA-19 datasets show that\nAdversarial-GEC can achieve competitive GEC quality compared to NMT-based\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 00:31:33 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Raheja", "Vipul", ""], ["Alikaniotis", "Dimitrios", ""]]}, {"id": "2010.02411", "submitter": "Abd AlRahman AlMomani", "authors": "Abd AlRahman AlMomani and Erik Bollt", "title": "ERFit: Entropic Regression Fit Matlab Package, for Data-Driven System\n  Identification of Underlying Dynamic Equations", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven sparse system identification becomes the general framework for a\nwide range of problems in science and engineering. It is a problem of growing\nimportance in applied machine learning and artificial intelligence algorithms.\nIn this work, we developed the Entropic Regression Software Package (ERFit), a\nMATLAB package for sparse system identification using the entropic regression\nmethod. The code requires minimal supervision, with a wide range of options\nthat make it adapt easily to different problems in science and engineering. The\nERFit is available at https://github.com/almomaa/ERFit-Package\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:07:15 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["AlMomani", "Abd AlRahman", ""], ["Bollt", "Erik", ""]]}, {"id": "2010.02413", "submitter": "Belinda Z. Li", "authors": "Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad and Wen-tau\n  Yih", "title": "Efficient One-Pass End-to-End Entity Linking for Questions", "comments": "9 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ELQ, a fast end-to-end entity linking model for questions, which\nuses a biencoder to jointly perform mention detection and linking in one pass.\nEvaluated on WebQSP and GraphQuestions with extended annotations that cover\nmultiple entities per question, ELQ outperforms the previous state of the art\nby a large margin of +12.7% and +19.6% F1, respectively. With a very fast\ninference time (1.57 examples/s on a single CPU), ELQ can be useful for\ndownstream question answering systems. In a proof-of-concept experiment, we\ndemonstrate that using ELQ significantly improves the downstream QA performance\nof GraphRetriever (arXiv:1911.03868). Code and data available at\nhttps://github.com/facebookresearch/BLINK/tree/master/elq\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:14:10 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Belinda Z.", ""], ["Min", "Sewon", ""], ["Iyer", "Srinivasan", ""], ["Mehdad", "Yashar", ""], ["Yih", "Wen-tau", ""]]}, {"id": "2010.02416", "submitter": "Yi-Te Hsu", "authors": "Yi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, Ilya Chatsviorkin", "title": "Efficient Inference For Neural Machine Translation", "comments": "Accepted SustaiNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Transformer models have achieved state-of-the-art results in neural\nmachine translation and have become standard in the field. In this work, we\nlook for the optimal combination of known techniques to optimize inference\nspeed without sacrificing translation quality. We conduct an empirical study\nthat stacks various approaches and demonstrates that combination of replacing\ndecoder self-attention with simplified recurrent units, adopting a deep encoder\nand a shallow decoder architecture and multi-head attention pruning can achieve\nup to 109% and 84% speedup on CPU and GPU respectively and reduce the number of\nparameters by 25% while maintaining the same translation quality in terms of\nBLEU.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:21:11 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 13:48:02 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Hsu", "Yi-Te", ""], ["Garg", "Sarthak", ""], ["Liao", "Yi-Hsiu", ""], ["Chatsviorkin", "Ilya", ""]]}, {"id": "2010.02423", "submitter": "Haoyue Shi", "authors": "Haoyue Shi, Karen Livescu, Kevin Gimpel", "title": "On the Role of Supervision in Unsupervised Constituency Parsing", "comments": "EMNLP 2020. Project page:\n  https://ttic.uchicago.edu/~freda/project/rsucp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze several recent unsupervised constituency parsing models, which are\ntuned with respect to the parsing $F_1$ score on the Wall Street Journal (WSJ)\ndevelopment set (1,700 sentences). We introduce strong baselines for them, by\ntraining an existing supervised parsing model (Kitaev and Klein, 2018) on the\nsame labeled examples they access. When training on the 1,700 examples, or even\nwhen using only 50 examples for training and 5 for development, such a few-shot\nparsing approach can outperform all the unsupervised parsing methods by a\nsignificant margin. Few-shot parsing can be further improved by a simple data\naugmentation method and self-training. This suggests that, in order to arrive\nat fair conclusions, we should carefully consider the amount of labeled data\nused for model development. We propose two protocols for future work on\nunsupervised parsing: (i) use fully unsupervised criteria for hyperparameter\ntuning and model selection; (ii) use as few labeled examples as possible for\nmodel development, and compare to few-shot parsing trained on the same labeled\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:34:58 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 01:38:38 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Shi", "Haoyue", ""], ["Livescu", "Karen", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2010.02428", "submitter": "Tao Li", "authors": "Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, Vivek Srikumar", "title": "UnQovering Stereotyping Biases via Underspecified Questions", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While language embeddings have been shown to have stereotyping biases, how\nthese biases affect downstream question answering (QA) models remains\nunexplored. We present UNQOVER, a general framework to probe and quantify\nbiases through underspecified questions. We show that a naive use of model\nscores can lead to incorrect bias estimates due to two forms of reasoning\nerrors: positional dependence and question independence. We design a formalism\nthat isolates the aforementioned errors. As case studies, we use this metric to\nanalyze four important classes of stereotypes: gender, nationality, ethnicity,\nand religion. We probe five transformer-based QA models trained on two QA\ndatasets, along with their underlying language models. Our broad study reveals\nthat (1) all these models, with and without fine-tuning, have notable\nstereotyping biases in these classes; (2) larger models often have higher bias;\nand (3) the effect of fine-tuning on bias varies strongly with the dataset and\nthe model size.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:49:52 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 04:51:22 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 01:48:31 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Tao", ""], ["Khot", "Tushar", ""], ["Khashabi", "Daniel", ""], ["Sabharwal", "Ashish", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2010.02429", "submitter": "Heeyoung Kwon", "authors": "Heeyoung Kwon, Mahnaz Koupaee, Pratyush Singh, Gargi Sawhney, Anmol\n  Shukla, Keerthi Kumar Kallur, Nathanael Chambers and Niranjan Balasubramanian", "title": "Modeling Preconditions in Text with a Crowd-sourced Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preconditions provide a form of logical connection between events that\nexplains why some events occur together and information that is complementary\nto the more widely studied relations such as causation, temporal ordering,\nentailment, and discourse relations. Modeling preconditions in text has been\nhampered in part due to the lack of large scale labeled data grounded in text.\nThis paper introduces PeKo, a crowd-sourced annotation of preconditions between\nevent pairs in newswire, an order of magnitude larger than prior text\nannotations. To complement this new corpus, we also introduce two challenge\ntasks aimed at modeling preconditions: (i) Precondition Identification -- a\nstandard classification task defined over pairs of event mentions, and (ii)\nPrecondition Generation -- a generative task aimed at testing a more general\nability to reason about a given event. Evaluation on both tasks shows that\nmodeling preconditions is challenging even for today's large language models\n(LM). This suggests that precondition knowledge is not easily accessible in\nLM-derived representations alone. Our generation results show that fine-tuning\nan LM on PeKo yields better conditional relations than when trained on raw text\nor temporally-ordered corpora.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:52:34 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 02:20:14 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 17:56:03 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kwon", "Heeyoung", ""], ["Koupaee", "Mahnaz", ""], ["Singh", "Pratyush", ""], ["Sawhney", "Gargi", ""], ["Shukla", "Anmol", ""], ["Kallur", "Keerthi Kumar", ""], ["Chambers", "Nathanael", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2010.02434", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Tomoki Hayashi, Shinji Watanabe, Tomoki Toda", "title": "The Sequence-to-Sequence Baseline for the Voice Conversion Challenge\n  2020: Cascading ASR and TTS", "comments": "Accepted to the ISCA Joint Workshop for the Blizzard Challenge and\n  Voice Conversion Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the sequence-to-sequence (seq2seq) baseline system for\nthe voice conversion challenge (VCC) 2020. We consider a naive approach for\nvoice conversion (VC), which is to first transcribe the input speech with an\nautomatic speech recognition (ASR) model, followed using the transcriptions to\ngenerate the voice of the target with a text-to-speech (TTS) model. We revisit\nthis method under a sequence-to-sequence (seq2seq) framework by utilizing\nESPnet, an open-source end-to-end speech processing toolkit, and the many\nwell-configured pretrained models provided by the community. Official\nevaluation results show that our system comes out top among the participating\nsystems in terms of conversion similarity, demonstrating the promising ability\nof seq2seq models to convert speaker identity. The implementation is made\nopen-source at: https://github.com/espnet/espnet/tree/master/egs/vcc20.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 02:27:38 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Hayashi", "Tomoki", ""], ["Watanabe", "Shinji", ""], ["Toda", "Tomoki", ""]]}, {"id": "2010.02443", "submitter": "Yue Dong", "authors": "Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung and\n  Jingjing Liu", "title": "Multi-Fact Correction in Abstractive Text Summarization", "comments": "12 pages, accepted at EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained neural abstractive summarization systems have dominated\nextractive strategies on news summarization performance, at least in terms of\nROUGE. However, system-generated abstractive summaries often face the pitfall\nof factual inconsistency: generating incorrect facts with respect to the source\ntext. To address this challenge, we propose Span-Fact, a suite of two factual\ncorrection models that leverages knowledge learned from question answering\nmodels to make corrections in system-generated summaries via span selection.\nOur models employ single or multi-masking strategies to either iteratively or\nauto-regressively replace entities in order to ensure semantic consistency\nw.r.t. the source text, while retaining the syntactic structure of summaries\ngenerated by abstractive summarization models. Experiments show that our models\nsignificantly boost the factual consistency of system-generated summaries\nwithout sacrificing summary quality in terms of both automatic metrics and\nhuman evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 02:51:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Dong", "Yue", ""], ["Wang", "Shuohang", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Cheung", "Jackie Chi Kit", ""], ["Liu", "Jingjing", ""]]}, {"id": "2010.02448", "submitter": "Huayang Li", "authors": "Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi", "title": "On the Branching Bias of Syntax Extracted from Pre-trained Language\n  Models", "comments": "EMNLP 2020 findings", "journal-ref": "EMNLP 2020 Findings", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efforts have been devoted to extracting constituency trees from\npre-trained language models, often proceeding in two stages: feature definition\nand parsing. However, this kind of methods may suffer from the branching bias\nissue, which will inflate the performances on languages with the same branch it\nbiases to. In this work, we propose quantitatively measuring the branching bias\nby comparing the performance gap on a language and its reversed language, which\nis agnostic to both language models and extracting methods. Furthermore, we\nanalyze the impacts of three factors on the branching bias, namely parsing\nalgorithms, feature definitions, and language models. Experiments show that\nseveral existing works exhibit branching biases, and some implementations of\nthese three factors can introduce the branching bias.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:09:14 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Li", "Huayang", ""], ["Liu", "Lemao", ""], ["Huang", "Guoping", ""], ["Shi", "Shuming", ""]]}, {"id": "2010.02458", "submitter": "Zhao Wang", "authors": "Zhao Wang and Aron Culotta", "title": "Identifying Spurious Correlations for Robust Text Classification", "comments": "Findings of EMNLP-2020", "journal-ref": "Findings of EMNLP-2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictions of text classifiers are often driven by spurious correlations\n-- e.g., the term `Spielberg' correlates with positively reviewed movies, even\nthough the term itself does not semantically convey a positive sentiment. In\nthis paper, we propose a method to distinguish spurious and genuine\ncorrelations in text classification. We treat this as a supervised\nclassification problem, using features derived from treatment effect estimators\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\nnature of these features and their small dimensionality, we find that the\napproach works well even with limited training examples, and that it is\npossible to transport the word classifier to new domains. Experiments on four\ndatasets (sentiment classification and toxicity detection) suggest that using\nthis approach to inform feature selection also leads to more robust\nclassification, as measured by improved worst-case accuracy on the samples\naffected by spurious correlations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:49:22 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Zhao", ""], ["Culotta", "Aron", ""]]}, {"id": "2010.02466", "submitter": "Zhao Wang", "authors": "Zhao Wang, Jennifer Cutler, Aron Culotta", "title": "Are Words Commensurate with Actions? Quantifying Commitment to a Cause\n  from Online Public Messaging", "comments": "In IEEE International Conference on Data Mining (ICDM) Workshop on\n  Data science for Human Performance in Social Networks, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public entities such as companies and politicians increasingly use online\nsocial networks to communicate directly with their constituencies. Often, this\npublic messaging is aimed at aligning the entity with a particular cause or\nissue, such as the environment or public health. However, as a consumer or\nvoter, it can be difficult to assess an entity's true commitment to a cause\nbased on public messaging. In this paper, we present a text classification\napproach to categorize a message according to its commitment level toward a\ncause. We then compare the volume of such messages with external ratings based\non entities' actions (e.g., a politician's voting record with respect to the\nenvironment or a company's rating from environmental non-profits). We find that\nby distinguishing between low- and high- level commitment messages, we can more\nreliably identify truly committed entities. Furthermore, by measuring the\ndiscrepancy between classified messages and external ratings, we can identify\nentities whose public messaging does not align with their actions, thereby\nproviding a methodology to identify potentially \"inauthentic\" messaging\ncampaigns.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:12:28 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Zhao", ""], ["Cutler", "Jennifer", ""], ["Culotta", "Aron", ""]]}, {"id": "2010.02467", "submitter": "Jianmo Ni", "authors": "Jianmo Ni, Chun-Nan Hsu, Amilcare Gentili, Julian McAuley", "title": "Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on\n  Chest X-rays", "comments": "7 pages, 2 figures, to be published in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical image report generation has drawn growing attention due to\nits potential to alleviate radiologists' workload. Existing work on report\ngeneration often trains encoder-decoder networks to generate complete reports.\nHowever, such models are affected by data bias (e.g.~label imbalance) and face\ncommon issues inherent in text generation models (e.g.~repetition). In this\nwork, we focus on reporting abnormal findings on radiology images; instead of\ntraining on complete radiology reports, we propose a method to identify\nabnormal findings from the reports in addition to grouping them with\nunsupervised clustering and minimal rules. We formulate the task as cross-modal\nretrieval and propose Conditional Visual-Semantic Embeddings to align images\nand fine-grained abnormal findings in a joint embedding space. We demonstrate\nthat our method is able to retrieve abnormal findings and outperforms existing\ngeneration models on both clinical correctness and text generation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:18:18 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ni", "Jianmo", ""], ["Hsu", "Chun-Nan", ""], ["Gentili", "Amilcare", ""], ["McAuley", "Julian", ""]]}, {"id": "2010.02473", "submitter": "Zhirui Zhang", "authors": "Hao-Ran Wei, Zhirui Zhang, Boxing Chen, Weihua Luo", "title": "Iterative Domain-Repaired Back-Translation", "comments": "EMNLP 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the domain-specific translation with low\nresources, where in-domain parallel corpora are scarce or nonexistent. One\ncommon and effective strategy for this case is exploiting in-domain monolingual\ndata with the back-translation method. However, the synthetic parallel data is\nvery noisy because they are generated by imperfect out-of-domain systems,\nresulting in the poor performance of domain adaptation. To address this issue,\nwe propose a novel iterative domain-repaired back-translation framework, which\nintroduces the Domain-Repair (DR) model to refine translations in synthetic\nbilingual data. To this end, we construct corresponding data for the DR model\ntraining by round-trip translating the monolingual sentences, and then design\nthe unified training framework to optimize paired DR and NMT models jointly.\nExperiments on adapting NMT models between specific domains and from the\ngeneral domain to specific domains demonstrate the effectiveness of our\nproposed approach, achieving 15.79 and 4.47 BLEU improvements on average over\nunadapted models and back-translation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:38:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wei", "Hao-Ran", ""], ["Zhang", "Zhirui", ""], ["Chen", "Boxing", ""], ["Luo", "Weihua", ""]]}, {"id": "2010.02477", "submitter": "Youngmoon Jung", "authors": "Youngmoon Jung, Yeunju Choi, Hyungjun Lim, Hoirin Kim", "title": "A Unified Deep Learning Framework for Short-Duration Speaker\n  Verification in Adverse Environments", "comments": "19 pages, 10 figures, 13 tables", "journal-ref": "in IEEE Access, vol. 8, pp. 175448-175466, 2020", "doi": "10.1109/ACCESS.2020.3025941", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker verification (SV) has recently attracted considerable research\ninterest due to the growing popularity of virtual assistants. At the same time,\nthere is an increasing requirement for an SV system: it should be robust to\nshort speech segments, especially in noisy and reverberant environments. In\nthis paper, we consider one more important requirement for practical\napplications: the system should be robust to an audio stream containing long\nnon-speech segments, where a voice activity detection (VAD) is not applied. To\nmeet these two requirements, we introduce feature pyramid module (FPM)-based\nmulti-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present\nthe FPM-based MSA to deal with short speech segments in noisy and reverberant\nenvironments. Also, we use the SAS-VAD to increase the robustness to long\nnon-speech segments. To further improve the robustness to acoustic distortions\n(i.e., noise and reverberation), we apply a masking-based speech enhancement\n(SE) method. We combine SV, VAD, and SE models in a unified deep learning\nframework and jointly train the entire network in an end-to-end manner. To the\nbest of our knowledge, this is the first work combining these three models in a\ndeep learning framework. We conduct experiments on Korean indoor (KID) and\nVoxCeleb datasets, which are corrupted by noise and reverberation. The results\nshow that the proposed method is effective for SV in the challenging conditions\nand performs better than the baseline i-vector and deep speaker embedding\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:51:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Jung", "Youngmoon", ""], ["Choi", "Yeunju", ""], ["Lim", "Hyungjun", ""], ["Kim", "Hoirin", ""]]}, {"id": "2010.02480", "submitter": "Cheng-Han Chiang", "authors": "Cheng-Han Chiang, Sung-Feng Huang and Hung-yi Lee", "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "comments": "Accepted to EMNLP 2020, short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While behaviors of pretrained language models (LMs) have been thoroughly\nexamined, what happened during pretraining is rarely studied. We thus\ninvestigate the developmental process from a set of randomly initialized\nparameters to a totipotent language model, which we refer to as the embryology\nof a pretrained language model. Our results show that ALBERT learns to\nreconstruct and predict tokens of different parts of speech (POS) in different\nlearning speeds during pretraining. We also find that linguistic knowledge and\nworld knowledge do not generally improve as pretraining proceeds, nor do\ndownstream tasks' performance. These findings suggest that knowledge of a\npretrained model varies during pretraining, and having more pretrain steps does\nnot necessarily provide a model with more comprehensive knowledge. We will\nprovide source codes and pretrained models to reproduce our results at\nhttps://github.com/d223302/albert-embryology.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:15:39 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 00:07:43 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chiang", "Cheng-Han", ""], ["Huang", "Sung-Feng", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2010.02481", "submitter": "Hoang Nguyen", "authors": "Hoang Nguyen, Chenwei Zhang, Congying Xia, Philip S. Yu", "title": "Dynamic Semantic Matching and Aggregation Network for Few-shot Intent\n  Detection", "comments": "10 pages, 3 figures. To appear in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot Intent Detection is challenging due to the scarcity of available\nannotated utterances. Although recent works demonstrate that multi-level\nmatching plays an important role in transferring learned knowledge from seen\ntraining classes to novel testing classes, they rely on a static similarity\nmeasure and overly fine-grained matching components. These limitations inhibit\ngeneralizing capability towards Generalized Few-shot Learning settings where\nboth seen and novel classes are co-existent. In this paper, we propose a novel\nSemantic Matching and Aggregation Network where semantic components are\ndistilled from utterances via multi-head self-attention with additional dynamic\nregularization constraints. These semantic components capture high-level\ninformation, resulting in more effective matching between instances. Our\nmulti-perspective matching method provides a comprehensive matching measure to\nenhance representations of both labeled and unlabeled instances. We also\npropose a more challenging evaluation setting that considers classification on\nthe joint all-class label space. Extensive experimental results demonstrate the\neffectiveness of our method. Our code and data are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:16:38 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 06:07:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Nguyen", "Hoang", ""], ["Zhang", "Chenwei", ""], ["Xia", "Congying", ""], ["Yu", "Philip S.", ""]]}, {"id": "2010.02494", "submitter": "Venkata Subrahmanyan Govindarajan", "authors": "Venkata Subrahmanyan Govindarajan, Benjamin T Chen, Rebecca Warholic,\n  Katrin Erk, Junyi Jessy Li", "title": "Help! Need Advice on Identifying Advice", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans use language to accomplish a wide variety of tasks - asking for and\ngiving advice being one of them. In online advice forums, advice is mixed in\nwith non-advice, like emotional support, and is sometimes stated explicitly,\nsometimes implicitly. Understanding the language of advice would equip systems\nwith a better grasp of language pragmatics; practically, the ability to\nidentify advice would drastically increase the efficiency of advice-seeking\nonline, as well as advice-giving in natural language generation systems.\n  We present a dataset in English from two Reddit advice forums - r/AskParents\nand r/needadvice - annotated for whether sentences in posts contain advice or\nnot. Our analysis reveals rich linguistic phenomena in advice discourse. We\npresent preliminary models showing that while pre-trained language models are\nable to capture advice better than rule-based systems, advice identification is\nchallenging, and we identify directions for future research.\n  Comments: To be presented at EMNLP 2020.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:49:03 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Govindarajan", "Venkata Subrahmanyan", ""], ["Chen", "Benjamin T", ""], ["Warholic", "Rebecca", ""], ["Erk", "Katrin", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2010.02495", "submitter": "Praveen Kumar Bodigutla", "authors": "Praveen Kumar Bodigutla, Aditya Tiwari, Josep Valls Vargas, Lazaros\n  Polymenakos, Spyros Matsoukas", "title": "Joint Turn and Dialogue level User Satisfaction Estimation on\n  Multi-Domain Conversations", "comments": "Findings of EMNLP, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue level quality estimation is vital for optimizing data driven\ndialogue management. Current automated methods to estimate turn and dialogue\nlevel user satisfaction employ hand-crafted features and rely on complex\nannotation schemes, which reduce the generalizability of the trained models. We\npropose a novel user satisfaction estimation approach which minimizes an\nadaptive multi-task loss function in order to jointly predict turn-level\nResponse Quality labels provided by experts and explicit dialogue-level ratings\nprovided by end users. The proposed BiLSTM based deep neural net model\nautomatically weighs each turn's contribution towards the estimated\ndialogue-level rating, implicitly encodes temporal dependencies, and removes\nthe need to hand-craft features.\n  On dialogues sampled from 28 Alexa domains, two dialogue systems and three\nuser groups, the joint dialogue-level satisfaction estimation model achieved up\nto an absolute 27% (0.43->0.70) and 7% (0.63->0.70) improvement in linear\ncorrelation performance over baseline deep neural net and benchmark Gradient\nboosting regression models, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:53:13 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 21:10:47 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bodigutla", "Praveen Kumar", ""], ["Tiwari", "Aditya", ""], ["Vargas", "Josep Valls", ""], ["Polymenakos", "Lazaros", ""], ["Matsoukas", "Spyros", ""]]}, {"id": "2010.02498", "submitter": "Wanzheng Zhu", "authors": "Wanzheng Zhu, Suma Bhat", "title": "GRUEN for Evaluating Linguistic Quality of Generated Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluation metrics are indispensable for evaluating generated text.\nTo date, these metrics have focused almost exclusively on the content selection\naspect of the system output, ignoring the linguistic quality aspect altogether.\nWe bridge this gap by proposing GRUEN for evaluating Grammaticality,\nnon-Redundancy, focUs, structure and coherENce of generated text. GRUEN\nutilizes a BERT-based model and a class of syntactic, semantic, and contextual\nfeatures to examine the system output. Unlike most existing evaluation metrics\nwhich require human references as an input, GRUEN is reference-less and\nrequires only the system output. Besides, it has the advantage of being\nunsupervised, deterministic, and adaptable to various tasks. Experiments on\nseven datasets over four language generation tasks show that the proposed\nmetric correlates highly with human judgments.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:59:25 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhu", "Wanzheng", ""], ["Bhat", "Suma", ""]]}, {"id": "2010.02500", "submitter": "Zirui Wang", "authors": "Zirui Wang, Sanket Vaibhav Mehta, Barnab\\'as P\\'oczos and Jaime\n  Carbonell", "title": "Efficient Meta Lifelong-Learning with Limited Memory", "comments": "Published as a main conference paper at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current natural language processing models work well on a single task, yet\nthey often fail to continuously learn new tasks without forgetting previous\nones as they are re-trained throughout their lifetime, a challenge known as\nlifelong learning. State-of-the-art lifelong language learning methods store\npast examples in episodic memory and replay them at both training and inference\ntime. However, as we show later in our experiments, there are three significant\nimpediments: (1) needing unrealistically large memory module to achieve good\nperformance, (2) suffering from negative transfer, (3) requiring multiple local\nadaptation steps for each test example that significantly slows down the\ninference speed. In this paper, we identify three common principles of lifelong\nlearning methods and propose an efficient meta-lifelong framework that combines\nthem in a synergistic fashion. To achieve sample efficiency, our method trains\nthe model in a manner that it learns a better initialization for local\nadaptation. Extensive experiments on text classification and question answering\nbenchmarks demonstrate the effectiveness of our framework by achieving\nstate-of-the-art performance using merely 1% memory size and narrowing the gap\nwith multi-task learning. We further show that our method alleviates both\ncatastrophic forgetting and negative transfer at the same time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 06:08:07 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Zirui", ""], ["Mehta", "Sanket Vaibhav", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Carbonell", "Jaime", ""]]}, {"id": "2010.02510", "submitter": "Lily Ou", "authors": "Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon\n  Levy, Diba Mirza, William Yang Wang", "title": "Investigating African-American Vernacular English in Transformer-Based\n  Text Generation", "comments": "7 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of social media has encouraged the written use of African American\nVernacular English (AAVE), which has traditionally been used only in oral\ncontexts. However, NLP models have historically been developed using dominant\nEnglish varieties, such as Standard American English (SAE), due to text corpora\navailability. We investigate the performance of GPT-2 on AAVE text by creating\na dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating\nsyntactic structure and AAVE- or SAE-specific language for each pair. We\nevaluate each sample and its GPT-2 generated text with pretrained sentiment\nclassifiers and find that while AAVE text results in more classifications of\nnegative sentiment than SAE, the use of GPT-2 generally increases occurrences\nof positive sentiment for both. Additionally, we conduct human evaluation of\nAAVE and SAE text generated with GPT-2 to compare contextual rigor and overall\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 06:27:02 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 04:00:46 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Groenwold", "Sophie", ""], ["Ou", "Lily", ""], ["Parekh", "Aesha", ""], ["Honnavalli", "Samhita", ""], ["Levy", "Sharon", ""], ["Mirza", "Diba", ""], ["Wang", "William Yang", ""]]}, {"id": "2010.02523", "submitter": "Yiren Wang", "authors": "Yiren Wang, ChengXiang Zhai, Hany Hassan Awadalla", "title": "Multi-task Learning for Multilingual Neural Machine Translation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While monolingual data has been shown to be useful in improving bilingual\nneural machine translation (NMT), effectively and efficiently leveraging\nmonolingual data for Multilingual NMT (MNMT) systems is a less explored area.\nIn this work, we propose a multi-task learning (MTL) framework that jointly\ntrains the model with the translation task on bitext data and two denoising\ntasks on the monolingual data. We conduct extensive empirical studies on MNMT\nsystems with 10 language pairs from WMT datasets. We show that the proposed\napproach can effectively improve the translation quality for both high-resource\nand low-resource languages with large margin, achieving significantly better\nresults than the individual bilingual models. We also demonstrate the efficacy\nof the proposed approach in the zero-shot setup for language pairs without\nbitext training data. Furthermore, we show the effectiveness of MTL over\npre-training approaches for both NMT and cross-lingual transfer learning NLU\ntasks; the proposed approach outperforms massive scale models trained on single\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 06:54:12 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Yiren", ""], ["Zhai", "ChengXiang", ""], ["Awadalla", "Hany Hassan", ""]]}, {"id": "2010.02534", "submitter": "Joohong Lee", "authors": "Kyubyong Park, Joohong Lee, Seongbo Jang, Dawoon Jung", "title": "An Empirical Study of Tokenization Strategies for Various Korean NLP\n  Tasks", "comments": "Accepted to AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, tokenization is the very first step in most text processing works.\nAs a token serves as an atomic unit that embeds the contextual information of\ntext, how to define a token plays a decisive role in the performance of a\nmodel.Even though Byte Pair Encoding (BPE) has been considered the de facto\nstandard tokenization method due to its simplicity and universality, it still\nremains unclear whether BPE works best across all languages and tasks. In this\npaper, we test several tokenization strategies in order to answer our primary\nresearch question, that is, \"What is the best tokenization strategy for Korean\nNLP tasks?\" Experimental results demonstrate that a hybrid approach of\nmorphological segmentation followed by BPE works best in Korean to/from English\nmachine translation and natural language understanding tasks such as KorNLI,\nKorSTS, NSMC, and PAWS-X. As an exception, for KorQuAD, the Korean extension of\nSQuAD, BPE segmentation turns out to be the most effective.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 07:20:41 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Park", "Kyubyong", ""], ["Lee", "Joohong", ""], ["Jang", "Seongbo", ""], ["Jung", "Dawoon", ""]]}, {"id": "2010.02537", "submitter": "Shijie Wu", "authors": "Shijie Wu, Mark Dredze", "title": "Do Explicit Alignments Robustly Improve Multilingual Encoders?", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised\nmultilingual encoders can effectively learn cross-lingual representation.\nExplicit alignment objectives based on bitexts like Europarl or MultiUN have\nbeen shown to further improve these representations. However, word-level\nalignments are often suboptimal and such bitexts are unavailable for many\nlanguages. In this paper, we propose a new contrastive alignment objective that\ncan better utilize such signal, and examine whether these previous alignment\nmethods can be adapted to noisier sources of aligned data: a randomly sampled 1\nmillion pair subset of the OPUS collection. Additionally, rather than report\nresults on a single dataset with a single model run, we report the mean and\nstandard derivation of multiple runs with different seeds, on four datasets and\ntasks. Our more extensive analysis finds that, while our new objective\noutperforms previous work, overall these methods do not improve performance\nwith a more robust evaluation framework. Furthermore, the gains from using a\nbetter underlying model eclipse any benefits from alignment training. These\nnegative results dictate more care in evaluating these methods and suggest\nlimitations in applying explicit alignment objectives.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 07:43:17 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wu", "Shijie", ""], ["Dredze", "Mark", ""]]}, {"id": "2010.02550", "submitter": "Ran Zmigrod", "authors": "Ran Zmigrod, Tim Vieira, Ryan Cotterell", "title": "Please Mind the Root: Decoding Arborescences for Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection between dependency trees and spanning trees is exploited by\nthe NLP community to train and to decode graph-based dependency parsers.\nHowever, the NLP literature has missed an important difference between the two\nstructures: only one edge may emanate from the root in a dependency tree. We\nanalyzed the output of state-of-the-art parsers on many languages from the\nUniversal Dependency Treebank: although these parsers are often able to learn\nthat trees which violate the constraint should be assigned lower probabilities,\ntheir ability to do so unsurprisingly de-grades as the size of the training set\ndecreases. In fact, the worst constraint-violation rate we observe is 24%.\nPrior work has proposed an inefficient algorithm to enforce the constraint,\nwhich adds a factor of n to the decoding runtime. We adapt an algorithm due to\nGabow and Tarjan (1984) to dependency parsing, which satisfies the constraint\nwithout compromising the original runtime.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 08:31:14 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 08:12:11 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zmigrod", "Ran", ""], ["Vieira", "Tim", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2010.02552", "submitter": "Wenxiang Jiao", "authors": "Wenxiang Jiao, Xing Wang, Shilin He, Irwin King, Michael R. Lyu,\n  Zhaopeng Tu", "title": "Data Rejuvenation: Exploiting Inactive Training Examples for Neural\n  Machine Translation", "comments": "Accepted to EMNLP 2020 main conference, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale training datasets lie at the core of the recent success of neural\nmachine translation (NMT) models. However, the complex patterns and potential\nnoises in the large-scale data make training NMT models difficult. In this\nwork, we explore to identify the inactive training examples which contribute\nless to the model performance, and show that the existence of inactive examples\ndepends on the data distribution. We further introduce data rejuvenation to\nimprove the training of NMT models on large-scale datasets by exploiting\ninactive examples. The proposed framework consists of three phases. First, we\ntrain an identification model on the original training data, and use it to\ndistinguish inactive examples and active examples by their sentence-level\noutput probabilities. Then, we train a rejuvenation model on the active\nexamples, which is used to re-label the inactive examples with\nforward-translation. Finally, the rejuvenated examples and the active examples\nare combined to train the final NMT model. Experimental results on WMT14\nEnglish-German and English-French datasets show that the proposed data\nrejuvenation consistently and significantly improves performance for several\nstrong NMT models. Extensive analyses reveal that our approach stabilizes and\naccelerates the training process of NMT models, resulting in final models with\nbetter generalization capability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 08:57:31 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Jiao", "Wenxiang", ""], ["Wang", "Xing", ""], ["He", "Shilin", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2010.02556", "submitter": "Sumegh Roychowdhury", "authors": "Sumegh Roychowdhury, Sumedh A. Sontakke, Nikaash Puri, Mausoom Sarkar,\n  Milan Aggarwal, Pinkesh Badjatiya, Balaji Krishnamurthy, Laurent Itti", "title": "Unsupervised Hierarchical Concept Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering concepts (or temporal abstractions) in an unsupervised manner\nfrom demonstration data in the absence of an environment is an important\nproblem. Organizing these discovered concepts hierarchically at different\nlevels of abstraction is useful in discovering patterns, building ontologies,\nand generating tutorials from demonstration data. However, recent work to\ndiscover such concepts without access to any environment does not discover\nrelationships (or a hierarchy) between these discovered concepts. In this\npaper, we present a Transformer-based concept abstraction architecture UNHCLE\n(pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way\nfrom demonstration data. We empirically demonstrate how UNHCLE discovers\nmeaningful hierarchies using datasets from Chess and Cooking domains. Finally,\nwe show how UNHCLE learns meaningful language labels for concepts by using\ndemonstration data augmented with natural language for cooking and chess. All\nof our code is available at https://github.com/UNHCLE/UNHCLE\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:04:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Roychowdhury", "Sumegh", ""], ["Sontakke", "Sumedh A.", ""], ["Puri", "Nikaash", ""], ["Sarkar", "Mausoom", ""], ["Aggarwal", "Milan", ""], ["Badjatiya", "Pinkesh", ""], ["Krishnamurthy", "Balaji", ""], ["Itti", "Laurent", ""]]}, {"id": "2010.02557", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad and Jianfeng Chi and Yuan Tian and Kai-Wei Chang", "title": "PolicyQA: A Reading Comprehension Dataset for Privacy Policies", "comments": "EMNLP Findings 2020 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy policy documents are long and verbose. A question answering (QA)\nsystem can assist users in finding the information that is relevant and\nimportant to them. Prior studies in this domain frame the QA task as retrieving\nthe most relevant text segment or a list of sentences from the policy document\ngiven a question. On the contrary, we argue that providing users with a short\ntext span from policy documents reduces the burden of searching the target\ninformation from a lengthy text segment. In this paper, we present PolicyQA, a\ndataset that contains 25,017 reading comprehension style examples curated from\nan existing corpus of 115 website privacy policies. PolicyQA provides 714\nhuman-annotated questions written for a wide range of privacy practices. We\nevaluate two existing neural QA models and perform rigorous analysis to reveal\nthe advantages and challenges offered by PolicyQA.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:04:58 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Chi", "Jianfeng", ""], ["Tian", "Yuan", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2010.02559", "submitter": "Ilias Chalkidis", "authors": "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos\n  Aletras and Ion Androutsopoulos", "title": "LEGAL-BERT: The Muppets straight out of Law School", "comments": "5 pages, short paper in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT has achieved impressive performance in several NLP tasks. However, there\nhas been limited investigation on its adaptation guidelines in specialised\ndomains. Here we focus on the legal domain, where we explore several approaches\nfor applying BERT models to downstream legal tasks, evaluating on multiple\ndatasets. Our findings indicate that the previous guidelines for pre-training\nand fine-tuning, often blindly followed, do not always generalize well in the\nlegal domain. Thus we propose a systematic investigation of the available\nstrategies when applying BERT in specialised domains. These are: (a) use the\noriginal BERT out of the box, (b) adapt BERT by additional pre-training on\ndomain-specific corpora, and (c) pre-train BERT from scratch on domain-specific\ncorpora. We also propose a broader hyper-parameter search space when\nfine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT\nmodels intended to assist legal NLP research, computational law, and legal\ntechnology applications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:06:07 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chalkidis", "Ilias", ""], ["Fergadiotis", "Manos", ""], ["Malakasiotis", "Prodromos", ""], ["Aletras", "Nikolaos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "2010.02562", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Daniel Hsu, Luis Gravano", "title": "Cross-Lingual Text Classification with Minimal Resources by Transferring\n  a Sparse Teacher", "comments": "Accepted to Findings of EMNLP 2020 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual text classification alleviates the need for manually labeled\ndocuments in a target language by leveraging labeled documents from other\nlanguages. Existing approaches for transferring supervision across languages\nrequire expensive cross-lingual resources, such as parallel corpora, while less\nexpensive cross-lingual representation learning approaches train classifiers\nwithout target labeled documents. In this work, we propose a cross-lingual\nteacher-student method, CLTS, that generates \"weak\" supervision in the target\nlanguage using minimal cross-lingual resources, in the form of a small number\nof word translations. Given a limited translation budget, CLTS extracts and\ntransfers only the most important task-specific seed words across languages and\ninitializes a teacher classifier based on the translated seed words. Then, CLTS\niteratively trains a more powerful student that also exploits the context of\nthe seed words in unlabeled target documents and outperforms the teacher. CLTS\nis simple and surprisingly effective in 18 diverse languages: by transferring\njust 20 seed words, even a bag-of-words logistic regression student outperforms\nstate-of-the-art cross-lingual methods (e.g., based on multilingual BERT).\nMoreover, CLTS can accommodate any type of student classifier: leveraging a\nmonolingual BERT student leads to further improvements and outperforms even\nmore expensive approaches by up to 12% in accuracy. Finally, CLTS addresses\nemerging tasks in low-resource languages using just a small number of word\ntranslations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:11:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Hsu", "Daniel", ""], ["Gravano", "Luis", ""]]}, {"id": "2010.02568", "submitter": "Umanga Bista", "authors": "Umanga Bista, Alexander Patrick Mathews, Aditya Krishna Menon, Lexing\n  Xie", "title": "SupMMD: A Sentence Importance Model for Extractive Summarization using\n  Maximum Mean Discrepancy", "comments": "15 pages", "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on multi-document summarization has focused on generic\nsummarization of information present in each individual document set. However,\nthe under-explored setting of update summarization, where the goal is to\nidentify the new information present in each set, is of equal practical\ninterest (e.g., presenting readers with updates on an evolving news topic). In\nthis work, we present SupMMD, a novel technique for generic and update\nsummarization based on the maximum mean discrepancy from kernel two-sample\ntesting. SupMMD combines both supervised learning for salience and unsupervised\nlearning for coverage and diversity. Further, we adapt multiple kernel learning\nto make use of similarity across multiple information sources (e.g., text\nfeatures and knowledge based concepts). We show the efficacy of SupMMD in both\ngeneric and update summarization tasks by meeting or exceeding the current\nstate-of-the-art on the DUC-2004 and TAC-2009 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:26:55 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bista", "Umanga", ""], ["Mathews", "Alexander Patrick", ""], ["Menon", "Aditya Krishna", ""], ["Xie", "Lexing", ""]]}, {"id": "2010.02569", "submitter": "Ze Yang", "authors": "Ze Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi Bai, Liran Wang, Wei\n  Wang, Zhoujun Li", "title": "StyleDGPT: Stylized Response Generation with Pre-trained Language Models", "comments": "Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating responses following a desired style has great potentials to extend\napplications of open-domain dialogue systems, yet is refrained by lacking of\nparallel data for training. In this work, we explore the challenging task with\npre-trained language models that have brought breakthrough to various natural\nlanguage tasks. To this end, we introduce a KL loss and a style classifier to\nthe fine-tuning step in order to steer response generation towards the target\nstyle in both a word-level and a sentence-level. Comprehensive empirical\nstudies with two public datasets indicate that our model can significantly\noutperform state-of-the-art methods in terms of both style consistency and\ncontextual coherence.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:29:50 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yang", "Ze", ""], ["Wu", "Wei", ""], ["Xu", "Can", ""], ["Liang", "Xinnian", ""], ["Bai", "Jiaqi", ""], ["Wang", "Liran", ""], ["Wang", "Wei", ""], ["Li", "Zhoujun", ""]]}, {"id": "2010.02570", "submitter": "Yordan Yordanov", "authors": "Yordan Yordanov, Oana-Maria Camburu, Vid Kocijan, Thomas Lukasiewicz", "title": "Does the Objective Matter? Comparing Training Objectives for Pronoun\n  Resolution", "comments": "Accepted to the EMNLP 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hard cases of pronoun resolution have been used as a long-standing benchmark\nfor commonsense reasoning. In the recent literature, pre-trained language\nmodels have been used to obtain state-of-the-art results on pronoun resolution.\nOverall, four categories of training and evaluation objectives have been\nintroduced. The variety of training datasets and pre-trained language models\nused in these works makes it unclear whether the choice of training objective\nis critical. In this work, we make a fair comparison of the performance and\nseed-wise stability of four models that represent the four categories of\nobjectives. Our experiments show that the objective of sequence ranking\nperforms the best in-domain, while the objective of semantic similarity between\ncandidates and pronoun performs the best out-of-domain. We also observe a\nseed-wise instability of the model using sequence ranking, which is not the\ncase when the other objectives are used.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:29:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yordanov", "Yordan", ""], ["Camburu", "Oana-Maria", ""], ["Kocijan", "Vid", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2010.02573", "submitter": "Phillip Keung", "authors": "Phillip Keung, Yichao Lu, Gy\\\"orgy Szarvas, Noah A. Smith", "title": "The Multilingual Amazon Reviews Corpus", "comments": "To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale\ncollection of Amazon reviews for multilingual text classification. The corpus\ncontains reviews in English, Japanese, German, French, Spanish, and Chinese,\nwhich were collected between 2015 and 2019. Each record in the dataset contains\nthe review text, the review title, the star rating, an anonymized reviewer ID,\nan anonymized product ID, and the coarse-grained product category (e.g.,\n'books', 'appliances', etc.) The corpus is balanced across the 5 possible star\nratings, so each rating constitutes 20% of the reviews in each language. For\neach language, there are 200,000, 5,000, and 5,000 reviews in the training,\ndevelopment, and test sets, respectively. We report baseline results for\nsupervised text classification and zero-shot cross-lingual transfer learning by\nfine-tuning a multilingual BERT model on reviews data. We propose the use of\nmean absolute error (MAE) instead of classification accuracy for this task,\nsince MAE accounts for the ordinal nature of the ratings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:34:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Keung", "Phillip", ""], ["Lu", "Yichao", ""], ["Szarvas", "Gy\u00f6rgy", ""], ["Smith", "Noah A.", ""]]}, {"id": "2010.02582", "submitter": "Wei Han", "authors": "Wei Han and Hantao Huang and Tao Han", "title": "Finding the Evidence: Localization-aware Answer Prediction for Text\n  Visual Question Answering", "comments": "Accepted in COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image text carries essential information to understand the scene and perform\nreasoning. Text-based visual question answering (text VQA) task focuses on\nvisual questions that require reading text in images. Existing text VQA systems\ngenerate an answer by selecting from optical character recognition (OCR) texts\nor a fixed vocabulary. Positional information of text is underused and there is\na lack of evidence for the generated answer. As such, this paper proposes a\nlocalization-aware answer prediction network (LaAP-Net) to address this\nchallenge. Our LaAP-Net not only generates the answer to the question but also\npredicts a bounding box as evidence of the generated answer. Moreover, a\ncontext-enriched OCR representation (COR) for multimodal fusion is proposed to\nfacilitate the localization task. Our proposed LaAP-Net outperforms existing\napproaches on three benchmark datasets for the text VQA task by a noticeable\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:46:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Han", "Wei", ""], ["Huang", "Hantao", ""], ["Han", "Tao", ""]]}, {"id": "2010.02584", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Nazneen Fatema Rajani, Dragomir Radev, Richard Socher,\n  Caiming Xiong", "title": "Universal Natural Language Processing with Limited Annotations: Try\n  Few-shot Textual Entailment as a Start", "comments": "EMNLP2020 Long, camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard way to address different NLP problems is by first constructing a\nproblem-specific dataset, then building a model to fit this dataset. To build\nthe ultimate artificial intelligence, we desire a single machine that can\nhandle diverse new problems, for which task-specific annotations are limited.\nWe bring up textual entailment as a unified solver for such NLP problems.\nHowever, current research of textual entailment has not spilled much ink on the\nfollowing questions: (i) How well does a pretrained textual entailment system\ngeneralize across domains with only a handful of domain-specific examples? and\n(ii) When is it worth transforming an NLP task into textual entailment? We\nargue that the transforming is unnecessary if we can obtain rich annotations\nfor this task. Textual entailment really matters particularly when the target\nNLP task has insufficient annotations.\n  Universal NLP can be probably achieved through different routines. In this\nwork, we introduce Universal Few-shot textual Entailment (UFO-Entail). We\ndemonstrate that this framework enables a pretrained entailment model to work\nwell on new entailment domains in a few-shot setting, and show its\neffectiveness as a unified solver for several downstream NLP tasks such as\nquestion answering and coreference resolution when the end-task annotations are\nlimited. Code: https://github.com/salesforce/UniversalFewShotNLP\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:50:25 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yin", "Wenpeng", ""], ["Rajani", "Nazneen Fatema", ""], ["Radev", "Dragomir", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.02586", "submitter": "Carel van Niekerk", "authors": "Carel van Niekerk, Michael Heck, Christian Geishauser, Hsien-Chin Lin,\n  Nurul Lubis, Marco Moresi, Milica Ga\\v{s}i\\'c", "title": "Knowing What You Know: Calibrating Dialogue Belief State Distributions\n  via Ensembles", "comments": "7 pages, 9 figures, to be published in Findings of EMNLP 2020, code\n  available at:\n  https://gitlab.cs.uni-duesseldorf.de/general/dsml/calibrating-dialogue-belief-state-distributions", "journal-ref": "Proceedings of the Findings of the Association for Computational\n  Linguistics: EMNLP 2020, Pages 3096-3102; Association for Computational\n  Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately track what happens during a conversation is\nessential for the performance of a dialogue system. Current state-of-the-art\nmulti-domain dialogue state trackers achieve just over 55% accuracy on the\ncurrent go-to benchmark, which means that in almost every second dialogue turn\nthey place full confidence in an incorrect dialogue state. Belief trackers, on\nthe other hand, maintain a distribution over possible dialogue states. However,\nthey lack in performance compared to dialogue state trackers, and do not\nproduce well calibrated distributions. In this work we present state-of-the-art\nperformance in calibration for multi-domain dialogue belief trackers using a\ncalibrated ensemble of models. Our resulting dialogue belief tracker also\noutperforms previous dialogue belief tracking models in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:51:04 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 10:56:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["van Niekerk", "Carel", ""], ["Heck", "Michael", ""], ["Geishauser", "Christian", ""], ["Lin", "Hsien-Chin", ""], ["Lubis", "Nurul", ""], ["Moresi", "Marco", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "2010.02587", "submitter": "Roman Klinger", "authors": "Sean Papay and Roman Klinger and Sebastian Pad\\'o", "title": "Dissecting Span Identification Tasks with Performance Prediction", "comments": "accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Span identification (in short, span ID) tasks such as chunking, NER, or\ncode-switching detection, ask models to identify and classify relevant spans in\na text. Despite being a staple of NLP, and sharing a common structure, there is\nlittle insight on how these tasks' properties influence their difficulty, and\nthus little guidance on what model families work well on span ID tasks, and\nwhy. We analyze span ID tasks via performance prediction, estimating how well\nneural architectures do on different tasks. Our contributions are: (a) we\nidentify key properties of span ID tasks that can inform performance\nprediction; (b) we carry out a large-scale experiment on English data, building\na model to predict performance for unseen span ID tasks that can support\narchitecture choices; (c), we investigate the parameters of the meta model,\nyielding new insights on how model and task properties interact to affect span\nID performance. We find, e.g., that span frequency is especially important for\nLSTMs, and that CRFs help when spans are infrequent and boundaries\nnon-distinctive.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:55:00 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Papay", "Sean", ""], ["Klinger", "Roman", ""], ["Pad\u00f3", "Sebastian", ""]]}, {"id": "2010.02588", "submitter": "Arie Cattan", "authors": "Aaron Bornstein, Arie Cattan, Ido Dagan", "title": "CoRefi: A Crowd Sourcing Suite for Coreference Annotation", "comments": "EMNLP 2020 system demonstration paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference annotation is an important, yet expensive and time consuming,\ntask, which often involved expert annotators trained on complex decision\nguidelines. To enable cheaper and more efficient annotation, we present CoRefi,\na web-based coreference annotation suite, oriented for crowdsourcing. Beyond\nthe core coreference annotation tool, CoRefi provides guided onboarding for the\ntask as well as a novel algorithm for a reviewing phase. CoRefi is open source\nand directly embeds into any website, including popular crowdsourcing\nplatforms.\n  CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo:\nhttps://github.com/aribornstein/corefi\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:55:36 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bornstein", "Aaron", ""], ["Cattan", "Arie", ""], ["Dagan", "Ido", ""]]}, {"id": "2010.02591", "submitter": "Xuanli He", "authors": "Xuanli He, Quan Hung Tran, Gholamreza Haffari, Walter Chang, Trung\n  Bui, Zhe Lin, Franck Dernoncourt, Nhan Dam", "title": "Scene Graph Modification Based on Natural Language Commands", "comments": "Accepted to the Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured representations like graphs and parse trees play a crucial role in\nmany Natural Language Processing systems. In recent years, the advancements in\nmulti-turn user interfaces necessitate the need for controlling and updating\nthese structured representations given new sources of information. Although\nthere have been many efforts focusing on improving the performance of the\nparsers that map text to graphs or parse trees, very few have explored the\nproblem of directly manipulating these representations. In this paper, we\nexplore the novel problem of graph modification, where the systems need to\nlearn how to update an existing scene graph given a new user's command. Our\nnovel models based on graph-based sparse transformer and cross attention\ninformation fusion outperform previous systems adapted from the machine\ntranslation and graph generation literature. We further contribute our large\ngraph modification datasets to the research community to encourage future\nresearch for this new problem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:01:19 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["He", "Xuanli", ""], ["Tran", "Quan Hung", ""], ["Haffari", "Gholamreza", ""], ["Chang", "Walter", ""], ["Bui", "Trung", ""], ["Lin", "Zhe", ""], ["Dernoncourt", "Franck", ""], ["Dam", "Nhan", ""]]}, {"id": "2010.02592", "submitter": "Eyal Ben-David", "authors": "Eyal Ben-David, Orgad Keller, Eric Malmi, Idan Szpektor, Roi Reichart", "title": "Semantically Driven Sentence Fusion: Modeling and Evaluation", "comments": "This paper was accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentence fusion is the task of joining related sentences into coherent text.\nCurrent training and evaluation schemes for this task are based on single\nreference ground-truths and do not account for valid fusion variants. We show\nthat this hinders models from robustly capturing the semantic relationship\nbetween input sentences. To alleviate this, we present an approach in which\nground-truth solutions are automatically expanded into multiple references via\ncurated equivalence classes of connective phrases. We apply this method to a\nlarge-scale dataset and use the augmented dataset for both model training and\nevaluation. To improve the learning of semantic representation using multiple\nreferences, we enrich the model with auxiliary discourse classification tasks\nunder a multi-tasking framework. Our experiments highlight the improvements of\nour approach over state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:06:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ben-David", "Eyal", ""], ["Keller", "Orgad", ""], ["Malmi", "Eric", ""], ["Szpektor", "Idan", ""], ["Reichart", "Roi", ""]]}, {"id": "2010.02598", "submitter": "Maksim Riabinin", "authors": "Max Ryabinin, Sergei Popov, Liudmila Prokhorenkova, Elena Voita", "title": "Embedding Words in Non-Vector Space with Unsupervised Graph Learning", "comments": "Accepted as a long paper for EMNLP 2020. 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become a de-facto standard to represent words as elements of a vector\nspace (word2vec, GloVe). While this approach is convenient, it is unnatural for\nlanguage: words form a graph with a latent hierarchical structure, and this\nstructure has to be revealed and encoded by word embeddings. We introduce\nGraphGlove: unsupervised graph word representations which are learned\nend-to-end. In our setting, each word is a node in a weighted graph and the\ndistance between words is the shortest path distance between the corresponding\nnodes. We adopt a recent method learning a representation of data in the form\nof a differentiable weighted graph and use it to modify the GloVe training\nalgorithm. We show that our graph-based representations substantially\noutperform vector-based methods on word similarity and analogy tasks. Our\nanalysis reveals that the structure of the learned graphs is hierarchical and\nsimilar to that of WordNet, the geometry is highly non-trivial and contains\nsubgraphs with different local topology.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:17:49 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ryabinin", "Max", ""], ["Popov", "Sergei", ""], ["Prokhorenkova", "Liudmila", ""], ["Voita", "Elena", ""]]}, {"id": "2010.02600", "submitter": "Isabelle G. Lee", "authors": "Isabelle G. Lee, Vera Zu, Sai Srujana Buddi, Dennis Liang, Purva\n  Kulkarni, Jack G.M. Fitzgerald", "title": "Converting the Point of View of Messages Spoken to Virtual Assistants", "comments": "10 pages, 11 figures, Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Assistants can be quite literal at times. If the user says \"tell Bob\nI love him,\" most virtual assistants will extract the message \"I love him\" and\nsend it to the user's contact named Bob, rather than properly converting the\nmessage to \"I love you.\" We designed a system to allow virtual assistants to\ntake a voice message from one user, convert the point of view of the message,\nand then deliver the result to its target user. We developed a rule-based\nmodel, which integrates a linear text classification model, part-of-speech\ntagging, and constituency parsing with rule-based transformation methods. We\nalso investigated Neural Machine Translation (NMT) approaches, including LSTMs,\nCopyNet, and T5. We explored 5 metrics to gauge both naturalness and\nfaithfulness automatically, and we chose to use BLEU plus METEOR for\nfaithfulness and relative perplexity using a separately trained language model\n(GPT) for naturalness. Transformer-Copynet and T5 performed similarly on\nfaithfulness metrics, with T5 achieving slight edge, a BLEU score of 63.8 and a\nMETEOR score of 83.0. CopyNet was the most natural, with a relative perplexity\nof 1.59. CopyNet also has 37 times fewer parameters than T5. We have publicly\nreleased our dataset, which is composed of 46,565 crowd-sourced samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:19:39 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 01:31:27 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Lee", "Isabelle G.", ""], ["Zu", "Vera", ""], ["Buddi", "Sai Srujana", ""], ["Liang", "Dennis", ""], ["Kulkarni", "Purva", ""], ["Fitzgerald", "Jack G. M.", ""]]}, {"id": "2010.02602", "submitter": "Guanglin Niu", "authors": "Guanglin Niu, Bo Li, Yongfei Zhang, Yongpan Sheng, Chuan Shi, Jingyang\n  Li, Shiliang Pu", "title": "Joint Semantics and Data-Driven Path Representation for Knowledge Graph\n  Inference", "comments": "12 pages, 6 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference on a large-scale knowledge graph (KG) is of great importance for KG\napplications like question answering. The path-based reasoning models can\nleverage much information over paths other than pure triples in the KG, which\nface several challenges: all the existing path-based methods are data-driven,\nlacking explainability for path representation. Besides, some methods either\nconsider only relational paths or ignore the heterogeneity between entities and\nrelations both contained in paths, which cannot capture the rich semantics of\npaths well. To address the above challenges, in this work, we propose a novel\njoint semantics and data-driven path representation that balances\nexplainability and generalization in the framework of KG embedding. More\nspecifically, we inject horn rules to obtain the condensed paths by the\ntransparent and explainable path composition procedure. The entity converter is\ndesigned to transform the entities along paths into the representations in the\nsemantic level similar to relations for reducing the heterogeneity between\nentities and relations, in which the KGs both with and without type information\nare considered. Our proposed model is evaluated on two classes of tasks: link\nprediction and path query answering task. The experimental results show that it\nhas a significant performance gain over several different state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:24:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Niu", "Guanglin", ""], ["Li", "Bo", ""], ["Zhang", "Yongfei", ""], ["Sheng", "Yongpan", ""], ["Shi", "Chuan", ""], ["Li", "Jingyang", ""], ["Pu", "Shiliang", ""]]}, {"id": "2010.02605", "submitter": "Ekaterina Artemova", "authors": "Taisia Glushkova and Alexey Machnev and Alena Fenogenova and Tatiana\n  Shavrina and Ekaterina Artemova and Dmitry I. Ignatov", "title": "DaNetQA: a yes/no Question Answering Dataset for the Russian Language", "comments": "Analysis of Images, Social Networks and Texts - 9 th International\n  Conference, AIST 2020, Skolkovo, Russia, October 15-16, 2020, Revised\n  Selected Papers. Lecture Notes in Computer Science\n  (https://dblp.org/db/series/lncs/index.html), Springer 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DaNetQA, a new question-answering corpus, follows (Clark et. al, 2019)\ndesign: it comprises natural yes/no questions. Each question is paired with a\nparagraph from Wikipedia and an answer, derived from the paragraph. The task is\nto take both the question and a paragraph as input and come up with a yes/no\nanswer, i.e. to produce a binary output. In this paper, we present a\nreproducible approach to DaNetQA creation and investigate transfer learning\nmethods for task and language transferring. For task transferring we leverage\nthree similar sentence modelling tasks: 1) a corpus of paraphrases,\nParaphraser, 2) an NLI task, for which we use the Russian part of XNLI, 3)\nanother question answering task, SberQUAD. For language transferring we use\nEnglish to Russian translation together with multilingual language fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:30:48 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 10:36:06 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Glushkova", "Taisia", ""], ["Machnev", "Alexey", ""], ["Fenogenova", "Alena", ""], ["Shavrina", "Tatiana", ""], ["Artemova", "Ekaterina", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "2010.02609", "submitter": "Lu Xu", "authors": "Lu Xu, Hao Li, Wei Lu, and Lidong Bing", "title": "Position-Aware Tagging for Aspect Sentiment Triplet Extraction", "comments": "15 pages, 10 figures, accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the\ntriplets of target entities, their associated sentiment, and opinion spans\nexplaining the reason for the sentiment. Existing research efforts mostly solve\nthis problem using pipeline approaches, which break the triplet extraction\nprocess into several stages. Our observation is that the three elements within\na triplet are highly related to each other, and this motivates us to build a\njoint model to extract such triplets using a sequence tagging approach.\nHowever, how to effectively design a tagging approach to extract the triplets\nthat can capture the rich interactions among the elements is a challenging\nresearch question. In this work, we propose the first end-to-end model with a\nnovel position-aware tagging scheme that is capable of jointly extracting the\ntriplets. Our experimental results on several existing datasets show that\njointly capturing elements in the triplet using our approach leads to improved\nperformance over the existing approaches. We also conducted extensive\nexperiments to investigate the model effectiveness and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:40:34 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 03:43:07 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 15:38:12 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Xu", "Lu", ""], ["Li", "Hao", ""], ["Lu", "Wei", ""], ["Bing", "Lidong", ""]]}, {"id": "2010.02616", "submitter": "Marius Mosbach", "authors": "Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, Dietrich Klakow", "title": "On the Interplay Between Fine-tuning and Sentence-level Probing for\n  Linguistic Knowledge in Pre-trained Transformers", "comments": "Accepted at Findings of EMNLP 2020 and BlackboxNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning pre-trained contextualized embedding models has become an\nintegral part of the NLP pipeline. At the same time, probing has emerged as a\nway to investigate the linguistic knowledge captured by pre-trained models.\nVery little is, however, understood about how fine-tuning affects the\nrepresentations of pre-trained models and thereby the linguistic knowledge they\nencode. This paper contributes towards closing this gap. We study three\ndifferent pre-trained models: BERT, RoBERTa, and ALBERT, and investigate\nthrough sentence-level probing how fine-tuning affects their representations.\nWe find that for some probing tasks fine-tuning leads to substantial changes in\naccuracy, possibly suggesting that fine-tuning introduces or even removes\nlinguistic knowledge from a pre-trained model. These changes, however, vary\ngreatly across different models, fine-tuning and probing tasks. Our analysis\nreveals that while fine-tuning indeed changes the representations of a\npre-trained model and these changes are typically larger for higher layers,\nonly in very few cases, fine-tuning has a positive effect on probing accuracy\nthat is larger than just using the pre-trained model with a strong pooling\nmethod. Based on our findings, we argue that both positive and negative effects\nof fine-tuning on probing require a careful interpretation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:54:00 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Mosbach", "Marius", ""], ["Khokhlova", "Anna", ""], ["Hedderich", "Michael A.", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.02636", "submitter": "Mark Fishel", "authors": "Liisa R\\\"atsep, Liisi Piits, Hille Pajupuu, Indrek Hein, Mark\n  Fi\\v{s}el", "title": "Neural Speech Synthesis for Estonian", "comments": "9 pages in Estonian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report describes the results of a collaboration between the\nNLP research group at the University of Tartu and the Institute of Estonian\nLanguage on improving neural speech synthesis for Estonian. The report (written\nin Estonian) describes the project results, the summary of which is: (1) Speech\nsynthesis data from 6 speakers for a total of 92.4 hours is collected and\nopenly released (CC-BY-4.0). Data available at https://konekorpus.tartunlp.ai\nand https://www.eki.ee/litsents/. (2) software and models for neural speech\nsynthesis is released open-source (MIT license). Available at\nhttps://koodivaramu.eesti.ee/tartunlp/text-to-speech . (3) We ran evaluations\nof the new models and compared them to other existing solutions (HMM-based HTS\nmodels from EKI, http://www.eki.ee/heli/, and Google's speech synthesis for\nEstonian, accessed via https://translate.google.com). Evaluation includes voice\nacceptability MOS scores for sentence-level and longer excerpts, detailed error\nanalysis and evaluation of the pre-processing module.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:37:46 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["R\u00e4tsep", "Liisa", ""], ["Piits", "Liisi", ""], ["Pajupuu", "Hille", ""], ["Hein", "Indrek", ""], ["Fi\u0161el", "Mark", ""]]}, {"id": "2010.02646", "submitter": "Yong Wang", "authors": "Yong Wang, Longyue Wang, Victor O.K. Li, Zhaopeng Tu", "title": "On the Sparsity of Neural Machine Translation Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural machine translation (NMT) models employ a large number of\nparameters, which leads to serious over-parameterization and typically causes\nthe underutilization of computational resources. In response to this problem,\nwe empirically investigate whether the redundant parameters can be reused to\nachieve better performance. Experiments and analyses are systematically\nconducted on different datasets and NMT architectures. We show that: 1) the\npruned parameters can be rejuvenated to improve the baseline model by up to\n+0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the\nability of modeling low-level lexical information.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:47:20 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Yong", ""], ["Wang", "Longyue", ""], ["Li", "Victor O. K.", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2010.02648", "submitter": "Yilin Yang", "authors": "Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee\n  and Zhaopeng Tu", "title": "On the Sub-Layer Functionalities of Transformer Decoder", "comments": "Findings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (Long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been significant efforts to interpret the encoder of\nTransformer-based encoder-decoder architectures for neural machine translation\n(NMT); meanwhile, the decoder remains largely unexamined despite its critical\nrole. During translation, the decoder must predict output tokens by considering\nboth the source-language text from the encoder and the target-language prefix\nproduced in previous steps. In this work, we study how Transformer-based\ndecoders leverage information from the source and target languages --\ndeveloping a universal probe task to assess how information is propagated\nthrough each module of each decoder layer. We perform extensive experiments on\nthree major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis\nprovides insight on when and where decoders leverage different sources. Based\non these insights, we demonstrate that the residual feed-forward module in each\nTransformer decoder layer can be dropped with minimal loss of performance -- a\nsignificant reduction in computation and number of parameters, and consequently\na significant boost to both training and inference speed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:50:54 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yang", "Yilin", ""], ["Wang", "Longyue", ""], ["Shi", "Shuming", ""], ["Tadepalli", "Prasad", ""], ["Lee", "Stefan", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2010.02649", "submitter": "Hao Zhang", "authors": "Sicheng Yu, Hao Zhang, Wei Jing, Jing Jiang", "title": "Context Modeling with Evidence Filter for Multiple Choice Question\n  Answering", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-Choice Question Answering (MCQA) is a challenging task in machine\nreading comprehension. The main challenge in MCQA is to extract \"evidence\" from\nthe given context that supports the correct answer. In the OpenbookQA dataset,\nthe requirement of extracting \"evidence\" is particularly important due to the\nmutual independence of sentences in the context. Existing work tackles this\nproblem by annotated evidence or distant supervision with rules which overly\nrely on human efforts. To address the challenge, we propose a simple yet\neffective approach termed evidence filtering to model the relationships between\nthe encoded contexts with respect to different options collectively and to\npotentially highlight the evidence sentences and filter out unrelated\nsentences. In addition to the effective reduction of human efforts of our\napproach compared, through extensive experiments on OpenbookQA, we show that\nthe proposed approach outperforms the models that use the same backbone and\nmore training data; and our parameter analysis also demonstrates the\ninterpretability of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:53:23 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yu", "Sicheng", ""], ["Zhang", "Hao", ""], ["Jing", "Wei", ""], ["Jiang", "Jing", ""]]}, {"id": "2010.02650", "submitter": "Clara Meister", "authors": "Clara Meister, Tim Vieira, Ryan Cotterell", "title": "If beam search is the answer, what was the question?", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural\nlanguage generators frequently leads to low-quality results. Rather, most\nstate-of-the-art results on language generation tasks are attained using beam\nsearch despite its overwhelmingly high search error rate. This implies that the\nMAP objective alone does not express the properties we desire in text, which\nmerits the question: if beam search is the answer, what was the question? We\nframe beam search as the exact solution to a different decoding objective in\norder to gain insights into why high probability under a model alone may not\nindicate adequacy. We find that beam search enforces uniform information\ndensity in text, a property motivated by cognitive science. We suggest a set of\ndecoding objectives that explicitly enforce this property and find that exact\ndecoding with these objectives alleviates the problems encountered when\ndecoding poorly calibrated language generation models. Additionally, we analyze\nthe text produced using various decoding strategies and see that, in our neural\nmachine translation experiments, the extent to which this property is adhered\nto strongly correlates with BLEU.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:57:03 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 09:39:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Meister", "Clara", ""], ["Vieira", "Tim", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2010.02654", "submitter": "Yohan Jo", "authors": "Yohan Jo, Jacky Visser, Chris Reed, Eduard Hovy", "title": "Extracting Implicitly Asserted Propositions in Argumentation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation accommodates various rhetorical devices, such as questions,\nreported speech, and imperatives. These rhetorical tools usually assert\nargumentatively relevant propositions rather implicitly, so understanding their\ntrue meaning is key to understanding certain arguments properly. However, most\nargument mining systems and computational linguistics research have paid little\nattention to implicitly asserted propositions in argumentation. In this paper,\nwe examine a wide range of computational methods for extracting propositions\nthat are implicitly asserted in questions, reported speech, and imperatives in\nargumentation. By evaluating the models on a corpus of 2016 U.S. presidential\ndebates and online commentary, we demonstrate the effectiveness and limitations\nof the computational models. Our study may inform future research on argument\nmining and the semantics of these rhetorical devices in argumentation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:03:47 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Jo", "Yohan", ""], ["Visser", "Jacky", ""], ["Reed", "Chris", ""], ["Hovy", "Eduard", ""]]}, {"id": "2010.02656", "submitter": "Yuncong Li", "authors": "Yuncong Li, Cunxiang Yin, Sheng-hua Zhong and Xu Pan", "title": "Multi-Instance Multi-Label Learning Networks for Aspect-Category\n  Sentiment Analysis", "comments": "Long paper accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-category sentiment analysis (ACSA) aims to predict sentiment\npolarities of sentences with respect to given aspect categories. To detect the\nsentiment toward a particular aspect category in a sentence, most previous\nmethods first generate an aspect category-specific sentence representation for\nthe aspect category, then predict the sentiment polarity based on the\nrepresentation. These methods ignore the fact that the sentiment of an aspect\ncategory mentioned in a sentence is an aggregation of the sentiments of the\nwords indicating the aspect category in the sentence, which leads to suboptimal\nperformance. In this paper, we propose a Multi-Instance Multi-Label Learning\nNetwork for Aspect-Category sentiment analysis (AC-MIMLLN), which treats\nsentences as bags, words as instances, and the words indicating an aspect\ncategory as the key instances of the aspect category. Given a sentence and the\naspect categories mentioned in the sentence, AC-MIMLLN first predicts the\nsentiments of the instances, then finds the key instances for the aspect\ncategories, finally obtains the sentiments of the sentence toward the aspect\ncategories by aggregating the key instance sentiments. Experimental results on\nthree public datasets demonstrate the effectiveness of AC-MIMLLN.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:07:54 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Yuncong", ""], ["Yin", "Cunxiang", ""], ["Zhong", "Sheng-hua", ""], ["Pan", "Xu", ""]]}, {"id": "2010.02660", "submitter": "Yohan Jo", "authors": "Yohan Jo, Seojin Bang, Emaad Manzoor, Eduard Hovy, Chris Reed", "title": "Detecting Attackable Sentences in Arguments", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding attackable sentences in an argument is the first step toward\nsuccessful refutation in argumentation. We present a first large-scale analysis\nof sentence attackability in online arguments. We analyze driving reasons for\nattacks in argumentation and identify relevant characteristics of sentences. We\ndemonstrate that a sentence's attackability is associated with many of these\ncharacteristics regarding the sentence's content, proposition types, and tone,\nand that an external knowledge source can provide useful information about\nattackability. Building on these findings, we demonstrate that machine learning\nmodels can automatically detect attackable sentences in arguments,\nsignificantly better than several baselines and comparably well to laypeople.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:13:00 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Jo", "Yohan", ""], ["Bang", "Seojin", ""], ["Manzoor", "Emaad", ""], ["Hovy", "Eduard", ""], ["Reed", "Chris", ""]]}, {"id": "2010.02665", "submitter": "Nachum Dershowitz", "authors": "Kfir Bar, Nachum Dershowitz, Lena Dankin", "title": "Automatic Metaphor Interpretation Using Word Embeddings", "comments": "Presented at 19th International Conference on Computational\n  Linguistics and Intelligent Text Processing (CICLing), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a model for metaphor interpretation using word embeddings trained\nover a relatively large corpus. Our system handles nominal metaphors, like\n\"time is money\". It generates a ranked list of potential interpretations of\ngiven metaphors. Candidate meanings are drawn from collocations of the topic\n(\"time\") and vehicle (\"money\") components, automatically extracted from a\ndependency-parsed corpus. We explore adding candidates derived from word\nassociation norms (common human responses to cues). Our ranking procedure\nconsiders similarity between candidate interpretations and metaphor components,\nmeasured in a semantic vector space. Lastly, a clustering algorithm removes\nsemantically related duplicates, thereby allowing other candidate\ninterpretations to attain higher rank. We evaluate using a set of annotated\nmetaphors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:35:13 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bar", "Kfir", ""], ["Dershowitz", "Nachum", ""], ["Dankin", "Lena", ""]]}, {"id": "2010.02667", "submitter": "Ruey-Cheng Chen", "authors": "Ruey-Cheng Chen, Chia-Jung Lee", "title": "Incorporating Behavioral Hypotheses for Query Generation", "comments": "EMNLP 2020 short paper, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural networks have been shown effective on query suggestion.\nCommonly posed as a conditional generation problem, the task aims to leverage\nearlier inputs from users in a search session to predict queries that they will\nlikely issue at a later time. User inputs come in various forms such as\nquerying and clicking, each of which can imply different semantic signals\nchanneled through the corresponding behavioral patterns. This paper induces\nthese behavioral biases as hypotheses for query generation, where a generic\nencoder-decoder Transformer framework is presented to aggregate arbitrary\nhypotheses of choice. Our experimental results show that the proposed approach\nleads to significant improvements on top-$k$ word error rate and Bert F1 Score\ncompared to a recent BART model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:38:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chen", "Ruey-Cheng", ""], ["Lee", "Chia-Jung", ""]]}, {"id": "2010.02684", "submitter": "Alvin Chan", "authors": "Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang", "title": "Poison Attacks against Text Datasets with Conditional Adversarially\n  Regularized Autoencoder", "comments": "Accepted in EMNLP-Findings 2020, Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates a fatal vulnerability in natural language inference\n(NLI) and text classification systems. More concretely, we present a 'backdoor\npoisoning' attack on NLP models. Our poisoning attack utilizes conditional\nadversarially regularized autoencoder (CARA) to generate poisoned training\nsamples by poison injection in latent space. Just by adding 1% poisoned data,\nour experiments show that a victim BERT finetuned classifier's predictions can\nbe steered to the poison target class with success rates of >80% when the input\nhypothesis is injected with the poison signature, demonstrating that NLI and\ntext classification systems face a huge security risk.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:03:49 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chan", "Alvin", ""], ["Tay", "Yi", ""], ["Ong", "Yew-Soon", ""], ["Zhang", "Aston", ""]]}, {"id": "2010.02686", "submitter": "Aina Gar\\'i Soler", "authors": "Aina Gar\\'i Soler, Marianna Apidianaki", "title": "BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking\n  Scalar Adjectives with Contextualised Representations", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adjectives like pretty, beautiful and gorgeous describe positive properties\nof the nouns they modify but with different intensity. These differences are\nimportant for natural language understanding and reasoning. We propose a novel\nBERT-based approach to intensity detection for scalar adjectives. We model\nintensity by vectors directly derived from contextualised representations and\nshow they can successfully rank scalar adjectives. We evaluate our models both\nintrinsically, on gold standard datasets, and on an Indirect Question Answering\ntask. Our results demonstrate that BERT encodes rich knowledge about the\nsemantics of scalar adjectives, and is able to provide better quality intensity\nrankings than static embeddings and previous models with access to dedicated\nresources.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:05:47 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Soler", "Aina Gar\u00ed", ""], ["Apidianaki", "Marianna", ""]]}, {"id": "2010.02693", "submitter": "Liang Ding", "authors": "Di Wu, Liang Ding, Fan Lu and Jian Xie", "title": "SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection\n  and Slot Filling", "comments": "To appear in the the main conference of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slot filling and intent detection are two main tasks in spoken language\nunderstanding (SLU) system. In this paper, we propose a novel\nnon-autoregressive model named SlotRefine for joint intent detection and slot\nfilling. Besides, we design a novel two-pass iteration mechanism to handle the\nuncoordinated slots problem caused by conditional independence of\nnon-autoregressive model. Experiments demonstrate that our model significantly\noutperforms previous models in slot filling task, while considerably speeding\nup the decoding (up to X 10.77). In-depth analyses show that 1) pretraining\nschemes could further enhance our model; 2) two-pass mechanism indeed remedy\nthe uncoordinated slots.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:16:53 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 12:29:45 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wu", "Di", ""], ["Ding", "Liang", ""], ["Lu", "Fan", ""], ["Xie", "Jian", ""]]}, {"id": "2010.02695", "submitter": "Nadir Durrani Dr", "authors": "Nadir Durrani and Hassan Sajjad and Fahim Dalvi and Yonatan Belinkov", "title": "Analyzing Individual Neurons in Pre-trained Language Models", "comments": "Accepted in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a lot of analysis has been carried to demonstrate linguistic knowledge\ncaptured by the representations learned within deep NLP models, very little\nattention has been paid towards individual neurons.We carry outa neuron-level\nanalysis using core linguistic tasks of predicting morphology, syntax and\nsemantics, on pre-trained language models, with questions like: i) do\nindividual neurons in pre-trained models capture linguistic information? ii)\nwhich parts of the network learn more about certain linguistic phenomena? iii)\nhow distributed or focused is the information? and iv) how do various\narchitectures differ in learning these properties? We found small subsets of\nneurons to predict linguistic tasks, with lower level tasks (such as\nmorphology) localized in fewer neurons, compared to higher level task of\npredicting syntax. Our study also reveals interesting cross architectural\ncomparisons. For example, we found neurons in XLNet to be more localized and\ndisjoint when predicting properties compared to BERT and others, where they are\nmore distributed and coupled.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:17:38 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Durrani", "Nadir", ""], ["Sajjad", "Hassan", ""], ["Dalvi", "Fahim", ""], ["Belinkov", "Yonatan", ""]]}, {"id": "2010.02696", "submitter": "Lu Xu", "authors": "Lu Xu, Lidong Bing, Wei Lu and Fei Huang", "title": "Aspect Based Sentiment Analysis with Aspect-Specific Opinion Spans", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Aspect based sentiment analysis, predicting sentiment polarity of given\naspects, has drawn extensive attention. Previous attention-based models\nemphasize using aspect semantics to help extract opinion features for\nclassification. However, these works are either not able to capture opinion\nspans as a whole, or not able to capture variable-length opinion spans. In this\npaper, we present a neat and effective structured attention model by\naggregating multiple linear-chain CRFs. Such a design allows the model to\nextract aspect-specific opinion spans and then evaluate sentiment polarity by\nexploiting the extracted opinion features. The experimental results on four\ndatasets demonstrate the effectiveness of the proposed model, and our analysis\ndemonstrates that our model can capture aspect-specific opinion spans.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:18:35 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 10:26:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Lu", ""], ["Bing", "Lidong", ""], ["Lu", "Wei", ""], ["Huang", "Fei", ""]]}, {"id": "2010.02705", "submitter": "Minki Kang", "authors": "Minki Kang, Moonsu Han, Sung Ju Hwang", "title": "Neural Mask Generator: Learning to Generate Adaptive Word Maskings for\n  Language Model Adaptation", "comments": "19 pages, 9 figures, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to automatically generate a domain- and task-adaptive\nmaskings of the given text for self-supervised pre-training, such that we can\neffectively adapt the language model to a particular target task (e.g. question\nanswering). Specifically, we present a novel reinforcement learning-based\nframework which learns the masking policy, such that using the generated masks\nfor further pre-training of the target language model helps improve task\nperformance on unseen texts. We use off-policy actor-critic with entropy\nregularization and experience replay for reinforcement learning, and propose a\nTransformer-based policy network that can consider the relative importance of\nwords in a given text. We validate our Neural Mask Generator (NMG) on several\nquestion answering and text classification datasets using BERT and DistilBERT\nas the language models, on which it outperforms rule-based masking strategies,\nby automatically learning optimal adaptive maskings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:27:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Kang", "Minki", ""], ["Han", "Moonsu", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2010.02733", "submitter": "Jo\\\"el Doat", "authors": "Jo\\\"el A. Doat", "title": "Towards Coalgebras in Stylometry", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The syntactic behaviour of texts can highly vary depending on their contexts\n(e.g. author, genre, etc.). From the standpoint of stylometry, it can be\nhelpful to objectively measure this behaviour. In this paper, we discuss how\ncoalgebras are used to formalise the notion of behaviour by embedding syntactic\nfeatures of a given text into probabilistic transition systems. By introducing\nthe behavioural distance, we are then able to quantitatively measure\ndifferences between points in these systems and thus, comparing features of\ndifferent texts. Furthermore, the behavioural distance of points can be\napproximated by a polynomial-time algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:55:11 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Doat", "Jo\u00ebl A.", ""]]}, {"id": "2010.02744", "submitter": "Shashi Narayan", "authors": "Shashi Narayan and Joshua Maynez and Jakub Adamek and Daniele Pighin\n  and Bla\\v{z} Bratani\\v{c} and Ryan McDonald", "title": "Stepwise Extractive Summarization and Planning with Structured\n  Transformers", "comments": "17 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose encoder-centric stepwise models for extractive summarization using\nstructured transformers -- HiBERT and Extended Transformers. We enable stepwise\nsummarization by injecting the previously generated summary into the structured\ntransformer as an auxiliary sub-structure. Our models are not only efficient in\nmodeling the structure of long inputs, but they also do not rely on\ntask-specific redundancy-aware modeling, making them a general purpose\nextractive content planner for different tasks. When evaluated on CNN/DailyMail\nextractive summarization, stepwise models achieve state-of-the-art performance\nin terms of Rouge without any redundancy aware modeling or sentence filtering.\nThis also holds true for Rotowire table-to-text generation, where our models\nsurpass previously reported metrics for content selection, planning and\nordering, highlighting the strength of stepwise modeling. Amongst the two\nstructured transformers we test, stepwise Extended Transformers provides the\nbest performance across both datasets and sets a new standard for these\nchallenges.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:12:58 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Narayan", "Shashi", ""], ["Maynez", "Joshua", ""], ["Adamek", "Jakub", ""], ["Pighin", "Daniele", ""], ["Bratani\u010d", "Bla\u017e", ""], ["McDonald", "Ryan", ""]]}, {"id": "2010.02784", "submitter": "Zehui Dai", "authors": "Zehui Dai, Cheng Peng, Huajie Chen, and Yadong Ding", "title": "A Multi-Task Incremental Learning Framework with Category Name Embedding\n  for Aspect-Category Sentiment Analysis", "comments": "EMNLP 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and\ntargeted aspect-category sentiment analysis (TACSA), aims at identifying\nsentiment polarity on predefined categories. Incremental learning on new\ncategories is necessary for (T)ACSA real applications. Though current\nmulti-task learning models achieve good performance in (T)ACSA tasks, they\nsuffer from catastrophic forgetting problems in (T)ACSA incremental learning\ntasks. In this paper, to make multi-task learning feasible for incremental\nlearning, we proposed Category Name Embedding network (CNE-net). We set both\nencoder and decoder shared among all categories to weaken the catastrophic\nforgetting problem. Besides the origin input sentence, we applied another input\nfeature, i.e., category name, for task discrimination. Our model achieved\nstate-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a\ndataset for (T)ACSA incremental learning and achieved the best performance\ncompared with other strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:52:54 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Dai", "Zehui", ""], ["Peng", "Cheng", ""], ["Chen", "Huajie", ""], ["Ding", "Yadong", ""]]}, {"id": "2010.02789", "submitter": "Lifu Tu", "authors": "Lifu Tu, Tianyu Liu, Kevin Gimpel", "title": "An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based\n  Inference Networks", "comments": "EMNLP 2020. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many tasks in natural language processing involve predicting structured\noutputs, e.g., sequence labeling, semantic role labeling, parsing, and machine\ntranslation. Researchers are increasingly applying deep representation learning\nto these problems, but the structured component of these approaches is usually\nquite simplistic. In this work, we propose several high-order energy terms to\ncapture complex dependencies among labels in sequence labeling, including\nseveral that consider the entire label sequence. We use neural\nparameterizations for these energy terms, drawing from convolutional,\nrecurrent, and self-attention networks. We use the framework of learning\nenergy-based inference networks (Tu and Gimpel, 2018) for dealing with the\ndifficulties of training and inference with such models. We empirically\ndemonstrate that this approach achieves substantial improvement using a variety\nof high-order energy terms on four sequence labeling tasks, while having the\nsame decoding speed as simple, local classifiers. We also find high-order\nenergies to help in noisy data conditions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 14:59:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Tu", "Lifu", ""], ["Liu", "Tianyu", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2010.02795", "submitter": "Soujanya Poria", "authors": "Deepanway Ghosal, Navonil Majumder, Alexander Gelbukh, Rada Mihalcea,\n  Soujanya Poria", "title": "COSMIC: COmmonSense knowledge for eMotion Identification in\n  Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we address the task of utterance level emotion recognition in\nconversations using commonsense knowledge. We propose COSMIC, a new framework\nthat incorporates different elements of commonsense such as mental states,\nevents, and causal relations, and build upon them to learn interactions between\ninterlocutors participating in a conversation. Current state-of-the-art methods\noften encounter difficulties in context propagation, emotion shift detection,\nand differentiating between related emotion classes. By learning distinct\ncommonsense representations, COSMIC addresses these challenges and achieves new\nstate-of-the-art results for emotion recognition on four different benchmark\nconversational datasets. Our code is available at\nhttps://github.com/declare-lab/conv-emotion.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:09:38 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ghosal", "Deepanway", ""], ["Majumder", "Navonil", ""], ["Gelbukh", "Alexander", ""], ["Mihalcea", "Rada", ""], ["Poria", "Soujanya", ""]]}, {"id": "2010.02804", "submitter": "Manuel Mager", "authors": "Manuel Mager, \\\"Ozlem \\c{C}etino\\u{g}lu and Katharina Kann", "title": "Tackling the Low-resource Challenge for Canonical Segmentation", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Canonical morphological segmentation consists of dividing words into their\nstandardized morphemes. Here, we are interested in approaches for the task when\ntraining data is limited. We compare model performance in a simulated\nlow-resource setting for the high-resource languages German, English, and\nIndonesian to experiments on new datasets for the truly low-resource languages\nPopoluca and Tepehua. We explore two new models for the task, borrowing from\nthe closely related area of morphological generation: an LSTM pointer-generator\nand a sequence-to-sequence model with hard monotonic attention trained with\nimitation learning. We find that, in the low-resource setting, the novel\napproaches outperform existing ones on all languages by up to 11.4% accuracy.\nHowever, while accuracy in emulated low-resource scenarios is over 50% for all\nlanguages, for the truly low-resource languages Popoluca and Tepehua, our best\nmodel only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude\nthat canonical segmentation is still a challenging task for low-resource\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:15:05 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Mager", "Manuel", ""], ["\u00c7etino\u011flu", "\u00d6zlem", ""], ["Kann", "Katharina", ""]]}, {"id": "2010.02806", "submitter": "Bertrand Higy", "authors": "Bertrand Higy, Desmond Elliott, Grzegorz Chrupa{\\l}a", "title": "Textual Supervision for Visually Grounded Spoken Language Understanding", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visually-grounded models of spoken language understanding extract semantic\ninformation directly from speech, without relying on transcriptions. This is\nuseful for low-resource languages, where transcriptions can be expensive or\nimpossible to obtain. Recent work showed that these models can be improved if\ntranscriptions are available at training time. However, it is not clear how an\nend-to-end approach compares to a traditional pipeline-based approach when one\nhas access to transcriptions. Comparing different strategies, we find that the\npipeline approach works better when enough text is available. With low-resource\nlanguages in mind, we also show that translations can be effectively used in\nplace of transcriptions but more data is needed to obtain similar results.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:16:23 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 07:48:12 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Higy", "Bertrand", ""], ["Elliott", "Desmond", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "2010.02807", "submitter": "Shubham Toshniwal", "authors": "Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin\n  Gimpel", "title": "Learning to Ignore: Long Document Coreference with Bounded Memory Neural\n  Networks", "comments": "Post EMNLP 2020 camera ready updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long document coreference resolution remains a challenging task due to the\nlarge memory and runtime requirements of current models. Recent work doing\nincremental coreference resolution using just the global representation of\nentities shows practical benefits but requires keeping all entities in memory,\nwhich can be impractical for long documents. We argue that keeping all entities\nin memory is unnecessary, and we propose a memory-augmented neural network that\ntracks only a small bounded number of entities at a time, thus guaranteeing a\nlinear runtime in length of document. We show that (a) the model remains\ncompetitive with models with high memory and computational requirements on\nOntoNotes and LitBank, and (b) the model learns an efficient memory management\nstrategy easily outperforming a rule-based strategy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:16:31 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 09:40:48 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:31:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Toshniwal", "Shubham", ""], ["Wiseman", "Sam", ""], ["Ettinger", "Allyson", ""], ["Livescu", "Karen", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2010.02810", "submitter": "Michel Pl\\\"uss", "authors": "Michel Pl\\\"uss and Lukas Neukom and Christian Scheller and Manfred\n  Vogel", "title": "Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech\n  to Standard German Text Corpus", "comments": "8 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Swiss Parliaments Corpus (SPC), an automatically aligned Swiss\nGerman speech to Standard German text corpus. This first version of the corpus\nis based on publicly available data of the Bernese cantonal parliament and\nconsists of 293 hours of data. It was created using a novel forced sentence\nalignment procedure and an alignment quality estimator, which can be used to\ntrade off corpus size and quality. We trained Automatic Speech Recognition\n(ASR) models as baselines on different subsets of the data and achieved a Word\nError Rate (WER) of 0.278 and a BLEU score of 0.586 on the SPC test set. The\ncorpus is freely available for download.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:18:21 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 11:47:40 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pl\u00fcss", "Michel", ""], ["Neukom", "Lukas", ""], ["Scheller", "Christian", ""], ["Vogel", "Manfred", ""]]}, {"id": "2010.02812", "submitter": "Lucas Torroba Hennigen", "authors": "Lucas Torroba Hennigen, Adina Williams, Ryan Cotterell", "title": "Intrinsic Probing through Dimension Selection", "comments": "To appear EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern NLP systems make use of pre-trained contextual representations\nthat attain astonishingly high performance on a variety of tasks. Such high\nperformance should not be possible unless some form of linguistic structure\ninheres in these representations, and a wealth of research has sprung up on\nprobing for it. In this paper, we draw a distinction between intrinsic probing,\nwhich examines how linguistic information is structured within a\nrepresentation, and the extrinsic probing popular in prior work, which only\nargues for the presence of such information by showing that it can be\nsuccessfully extracted. To enable intrinsic probing, we propose a novel\nframework based on a decomposable multivariate Gaussian probe that allows us to\ndetermine whether the linguistic information in word embeddings is dispersed or\nfocal. We then probe fastText and BERT for various morphosyntactic attributes\nacross 36 languages. We find that most attributes are reliably encoded by only\na few neurons, with fastText concentrating its linguistic structure more than\nBERT.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:21:08 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hennigen", "Lucas Torroba", ""], ["Williams", "Adina", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2010.02815", "submitter": "Valentina Pyatkin", "authors": "Valentina Pyatkin, Ayal Klein, Reut Tsarfaty, Ido Dagan", "title": "QADiscourse -- Discourse Relations as QA Pairs: Representation,\n  Crowdsourcing and Baselines", "comments": "To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relations describe how two propositions relate to one another, and\nidentifying them automatically is an integral part of natural language\nunderstanding. However, annotating discourse relations typically requires\nexpert annotators. Recently, different semantic aspects of a sentence have been\nrepresented and crowd-sourced via question-and-answer (QA) pairs. This paper\nproposes a novel representation of discourse relations as QA pairs, which in\nturn allows us to crowd-source wide-coverage data annotated with discourse\nrelations, via an intuitively appealing interface for composing such questions\nand answers. Based on our proposed representation, we collect a novel and\nwide-coverage QADiscourse dataset, and present baseline algorithms for\npredicting QADiscourse relations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:25:15 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pyatkin", "Valentina", ""], ["Klein", "Ayal", ""], ["Tsarfaty", "Reut", ""], ["Dagan", "Ido", ""]]}, {"id": "2010.02830", "submitter": "Swarnadeep Saha", "authors": "Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal", "title": "PRover: Proof Generation for Interpretable Reasoning over Rules", "comments": "EMNLP 2020 (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work by Clark et al. (2020) shows that transformers can act as 'soft\ntheorem provers' by answering questions over explicitly provided knowledge in\nnatural language. In our work, we take a step closer to emulating formal\ntheorem provers, by proposing PROVER, an interpretable transformer-based model\nthat jointly answers binary questions over rule-bases and generates the\ncorresponding proofs. Our model learns to predict nodes and edges corresponding\nto proof graphs in an efficient constrained training paradigm. During\ninference, a valid proof, satisfying a set of global constraints is generated.\nWe conduct experiments on synthetic, hand-authored, and human-paraphrased\nrule-bases to show promising results for QA and proof generation, with strong\ngeneralization performance. First, PROVER generates proofs with an accuracy of\n87%, while retaining or improving performance on the QA task, compared to\nRuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained\non questions requiring lower depths of reasoning, it generalizes significantly\nbetter to higher depths (up to 15% improvement). Third, PROVER obtains near\nperfect QA accuracy of 98% using only 40% of the training data. However,\ngenerating proofs for questions requiring higher depths of reasoning becomes\nchallenging, and the accuracy drops to 65% for 'depth 5', indicating\nsignificant scope for future work. Our code and models are publicly available\nat https://github.com/swarnaHub/PRover\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:47:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Saha", "Swarnadeep", ""], ["Ghosh", "Sayan", ""], ["Srivastava", "Shashank", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.02840", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Tao Yu, Dan Klein", "title": "Semantic Evaluation for Text-to-SQL with Distilled Test Suites", "comments": "EMNLP 2020 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose test suite accuracy to approximate semantic accuracy for\nText-to-SQL models. Our method distills a small test suite of databases that\nachieves high code coverage for the gold query from a large number of randomly\ngenerated databases. At evaluation time, it computes the denotation accuracy of\nthe predicted queries on the distilled test suite, hence calculating a tight\nupper-bound for semantic accuracy efficiently. We use our proposed method to\nevaluate 21 models submitted to the Spider leader board and manually verify\nthat our method is always correct on 100 examples. In contrast, the current\nSpider metric leads to a 2.5% false negative rate on average and 8.1% in the\nworst case, indicating that test suite accuracy is needed. Our implementation,\nalong with distilled test suites for eleven Text-to-SQL datasets, is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:04:12 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Yu", "Tao", ""], ["Klein", "Dan", ""]]}, {"id": "2010.02847", "submitter": "Haiyang Zhang", "authors": "Haiyang Zhang, Alison Sneyd and Mark Stevenson", "title": "Robustness and Reliability of Gender Bias Assessment in Word Embeddings:\n  The Role of Base Pairs", "comments": "Accepted at AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that word embeddings can exhibit gender bias, and various\nmethods have been proposed to quantify this. However, the extent to which the\nmethods are capturing social stereotypes inherited from the data has been\ndebated. Bias is a complex concept and there exist multiple ways to define it.\nPrevious work has leveraged gender word pairs to measure bias and extract\nbiased analogies. We show that the reliance on these gendered pairs has strong\nlimitations: bias measures based off of them are not robust and cannot identify\ncommon types of real-world bias, whilst analogies utilising them are unsuitable\nindicators of bias. In particular, the well-known analogy \"man is to\ncomputer-programmer as woman is to homemaker\" is due to word similarity rather\nthan societal bias. This has important implications for work on measuring bias\nin embeddings and related work debiasing embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:09:05 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 21:24:16 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhang", "Haiyang", ""], ["Sneyd", "Alison", ""], ["Stevenson", "Mark", ""]]}, {"id": "2010.02864", "submitter": "Avi Shmidman", "authors": "Avi Shmidman, Joshua Guedalia, Shaltiel Shmidman, Moshe Koppel, Reut\n  Tsarfaty", "title": "A Novel Challenge Set for Hebrew Morphological Disambiguation and\n  Diacritics Restoration", "comments": null, "journal-ref": "Findings of EMNLP, 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary tasks of morphological parsers is the disambiguation of\nhomographs. Particularly difficult are cases of unbalanced ambiguity, where one\nof the possible analyses is far more frequent than the others. In such cases,\nthere may not exist sufficient examples of the minority analyses in order to\nproperly evaluate performance, nor to train effective classifiers. In this\npaper we address the issue of unbalanced morphological ambiguities in Hebrew.\nWe offer a challenge set for Hebrew homographs -- the first of its kind --\ncontaining substantial attestation of each analysis of 21 Hebrew homographs. We\nshow that the current SOTA of Hebrew disambiguation performs poorly on cases of\nunbalanced ambiguity. Leveraging our new dataset, we achieve a new\nstate-of-the-art for all 21 words, improving the overall average F1 score from\n0.67 to 0.95. Our resulting annotated datasets are made publicly available for\nfurther research.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:34:03 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Shmidman", "Avi", ""], ["Guedalia", "Joshua", ""], ["Shmidman", "Shaltiel", ""], ["Koppel", "Moshe", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2010.02867", "submitter": "Jieyu Zhao", "authors": "Jieyu Zhao and Kai-Wei Chang", "title": "LOGAN: Local Group Bias Detection by Clustering", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have been widely used in natural language\nprocessing (NLP). However, as revealed by many recent studies, machine learning\nmodels often inherit and amplify the societal biases in data. Various metrics\nhave been proposed to quantify biases in model predictions. In particular,\nseveral of them evaluate disparity in model performance between protected\ngroups and advantaged groups in the test corpus. However, we argue that\nevaluating bias at the corpus level is not enough for understanding how biases\nare embedded in a model. In fact, a model with similar aggregated performance\nbetween different groups on the entire data may behave differently on instances\nin a local region. To analyze and detect such local bias, we propose LOGAN, a\nnew bias detection technique based on clustering. Experiments on toxicity\nclassification and object classification tasks show that LOGAN identifies bias\nin a local region and allows us to better analyze the biases in model\npredictions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:42:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhao", "Jieyu", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2010.02882", "submitter": "Nathaniel Weir", "authors": "Nathaniel Weir, Jo\\~ao Sedoc, and Benjamin Van Durme", "title": "COD3S: Diverse Generation with Discrete Semantic Signatures", "comments": "EMNLP2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COD3S, a novel method for generating semantically diverse\nsentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an\ninput, seq2seq models typically produce semantically and syntactically\nhomogeneous sets of sentences and thus perform poorly on one-to-many sequence\ngeneration tasks. Our two-stage approach improves output diversity by\nconditioning generation on locality-sensitive hash (LSH)-based semantic\nsentence codes whose Hamming distances highly correlate with human judgments of\nsemantic textual similarity. Though it is generally applicable, we apply COD3S\nto causal generation, the task of predicting a proposition's plausible causes\nor effects. We demonstrate through automatic and human evaluation that\nresponses produced using our method exhibit improved diversity without\ndegrading task performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:06:50 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Weir", "Nathaniel", ""], ["Sedoc", "Jo\u00e3o", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2010.02903", "submitter": "Shunyu Yao", "authors": "Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan", "title": "Keep CALM and Explore: Language Models for Action Generation in\n  Text-based Games", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based games present a unique challenge for autonomous agents to operate\nin natural language and handle enormous action spaces. In this paper, we\npropose the Contextual Action Language Model (CALM) to generate a compact set\nof action candidates at each game state. Our key insight is to train language\nmodels on human gameplay, where people demonstrate linguistic priors and a\ngeneral game sense for promising actions conditioned on game history. We\ncombine CALM with a reinforcement learning agent which re-ranks the generated\naction candidates to maximize in-game rewards. We evaluate our approach using\nthe Jericho benchmark, on games unseen by CALM during training. Our method\nobtains a 69% relative improvement in average game score over the previous\nstate-of-the-art model. Surprisingly, on half of these games, CALM is\ncompetitive with or better than other models that have access to ground truth\nadmissible actions. Code and data are available at\nhttps://github.com/princeton-nlp/calm-textgame.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:36:29 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yao", "Shunyu", ""], ["Rao", "Rohan", ""], ["Hausknecht", "Matthew", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2010.02949", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Hexiang Hu, Vihan Jain, Eugene Ie, Fei Sha", "title": "Learning to Represent Image and Text with Denotation Graph", "comments": "to appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to fuse vision and language information and representing them is an\nimportant research problem with many applications. Recent progresses have\nleveraged the ideas of pre-training (from language modeling) and attention\nlayers in Transformers to learn representation from datasets containing images\naligned with linguistic expressions that describe the images. In this paper, we\npropose learning representations from a set of implied, visually grounded\nexpressions between image and text, automatically mined from those datasets. In\nparticular, we use denotation graphs to represent how specific concepts (such\nas sentences describing images) can be linked to abstract and generic concepts\n(such as short phrases) that are also visually grounded. This type of\ngeneric-to-specific relations can be discovered using linguistic analysis\ntools. We propose methods to incorporate such relations into learning\nrepresentation. We show that state-of-the-art multimodal learning models can be\nfurther improved by leveraging automatically harvested structural relations.\nThe representations lead to stronger empirical results on downstream tasks of\ncross-modal image retrieval, referring expression, and compositional\nattribute-object recognition. Both our codes and the extracted denotation\ngraphs on the Flickr30K and the COCO datasets are publically available on\nhttps://sha-lab.github.io/DG.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:00:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhang", "Bowen", ""], ["Hu", "Hexiang", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Sha", "Fei", ""]]}, {"id": "2010.02960", "submitter": "David Gaddy", "authors": "David Gaddy and Dan Klein", "title": "Digital Voicing of Silent Speech", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the task of digitally voicing silent speech, where\nsilently mouthed words are converted to audible speech based on\nelectromyography (EMG) sensor measurements that capture muscle impulses. While\nprior work has focused on training speech synthesis models from EMG collected\nduring vocalized speech, we are the first to train from EMG collected during\nsilently articulated speech. We introduce a method of training on silent EMG by\ntransferring audio targets from vocalized to silent signals. Our method greatly\nimproves intelligibility of audio generated from silent EMG compared to a\nbaseline that only trains with vocalized data, decreasing transcription word\nerror rate from 64% to 4% in one data condition and 88% to 68% in another. To\nspur further development on this task, we share our new dataset of silent and\nvocalized facial EMG measurements.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:23:35 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Gaddy", "David", ""], ["Klein", "Dan", ""]]}, {"id": "2010.02975", "submitter": "Yuchen Lu", "authors": "Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, Aaron\n  Courville", "title": "Supervised Seeded Iterated Learning for Interactive Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language drift has been one of the major obstacles to train language models\nthrough interaction. When word-based conversational agents are trained towards\ncompleting a task, they tend to invent their language rather than leveraging\nnatural language. In recent literature, two general methods partially counter\nthis phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL).\nWhile S2P jointly trains interactive and supervised losses to counter the\ndrift, SIL changes the training dynamics to prevent language drift from\noccurring. In this paper, we first highlight their respective weaknesses, i.e.,\nlate-stage training collapses and higher negative likelihood when evaluated on\nhuman corpus. Given these observations, we introduce Supervised Seeded Iterated\nLearning to combine both methods to minimize their respective weaknesses. We\nthen show the effectiveness of \\algo in the language-drift translation game.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:09:02 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lu", "Yuchen", ""], ["Singhal", "Soumye", ""], ["Strub", "Florian", ""], ["Pietquin", "Olivier", ""], ["Courville", "Aaron", ""]]}, {"id": "2010.02976", "submitter": "Albert Webson", "authors": "Albert Webson, Zhizhong Chen, Carsten Eickhoff, Ellie Pavlick", "title": "Are \"Undocumented Workers\" the Same as \"Illegal Aliens\"? Disentangling\n  Denotation and Connotation in Vector Spaces", "comments": "Published at EMNLP 2020. Recorded talk available at\n  https://youtu.be/V2pdS6Y_8n0 . Code and data available at\n  https://github.com/awebson/congressional_adversary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In politics, neologisms are frequently invented for partisan objectives. For\nexample, \"undocumented workers\" and \"illegal aliens\" refer to the same group of\npeople (i.e., they have the same denotation), but they carry clearly different\nconnotations. Examples like these have traditionally posed a challenge to\nreference-based semantic theories and led to increasing acceptance of\nalternative theories (e.g., Two-Factor Semantics) among philosophers and\ncognitive scientists. In NLP, however, popular pretrained models encode both\ndenotation and connotation as one entangled representation. In this study, we\npropose an adversarial neural network that decomposes a pretrained\nrepresentation as independent denotation and connotation representations. For\nintrinsic interpretability, we show that words with the same denotation but\ndifferent connotations (e.g., \"immigrants\" vs. \"aliens\", \"estate tax\" vs.\n\"death tax\") move closer to each other in denotation space while moving further\napart in connotation space. For extrinsic application, we train an information\nretrieval system with our disentangled representations and show that the\ndenotation vectors improve the viewpoint diversity of document rankings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:09:03 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 15:43:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Webson", "Albert", ""], ["Chen", "Zhizhong", ""], ["Eickhoff", "Carsten", ""], ["Pavlick", "Ellie", ""]]}, {"id": "2010.02983", "submitter": "Florian Mai", "authors": "Florian Mai (1 and 2), Nikolaos Pappas (3), Ivan Montero (3), Noah A.\n  Smith (3 and 4), James Henderson (1) ((1) Idiap Research Institute, (2) EPFL,\n  (3) University of Washington, (4) Allen Institute for Artificial\n  Intelligence)", "title": "Plug and Play Autoencoders for Conditional Text Generation", "comments": "To be published in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text autoencoders are commonly used for conditional generation tasks such as\nstyle transfer. We propose methods which are plug and play, where any\npretrained autoencoder can be used, and only require learning a mapping within\nthe autoencoder's embedding space, training embedding-to-embedding (Emb2Emb).\nThis reduces the need for labeled training data for the task and makes the\ntraining procedure more efficient. Crucial to the success of this method is a\nloss term for keeping the mapped embedding on the manifold of the autoencoder\nand a mapping which is trained to navigate the manifold by learning offset\nvectors. Evaluations on style transfer tasks both with and without\nsequence-to-sequence supervision show that our method performs better than or\ncomparable to strong baselines while being up to four times faster.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:18:06 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 08:20:59 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Mai", "Florian", "", "1 and 2"], ["Pappas", "Nikolaos", "", "3 and 4"], ["Montero", "Ivan", "", "3 and 4"], ["Smith", "Noah A.", "", "3 and 4"], ["Henderson", "James", ""]]}, {"id": "2010.02986", "submitter": "Charlie Welch", "authors": "Charles Welch, Jonathan K. Kummerfeld, Ver\\'onica P\\'erez-Rosas, Rada\n  Mihalcea", "title": "Compositional Demographic Word Embeddings", "comments": "To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are usually derived from corpora containing text from many\nindividuals, thus leading to general purpose representations rather than\nindividually personalized representations. While personalized embeddings can be\nuseful to improve language model performance and other language processing\ntasks, they can only be computed for people with a large amount of longitudinal\ndata, which is not the case for new users. We propose a new form of\npersonalized word embeddings that use demographic-specific word representations\nderived compositionally from full or partial demographic information for a user\n(i.e., gender, age, location, religion). We show that the resulting\ndemographic-aware word representations outperform generic word representations\non two tasks for English: language modeling and word associations. We further\nexplore the trade-off between the number of available attributes and their\nrelative effectiveness and discuss the ethical implications of using them.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:23:46 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:54:53 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Welch", "Charles", ""], ["Kummerfeld", "Jonathan K.", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2010.03001", "submitter": "Giannis Bekoulis", "authors": "Giannis Bekoulis, Christina Papagiannopoulou, Nikos Deligiannis", "title": "A Review on Fact Extraction and Verification", "comments": "author preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fact checking problem, which aims to identify the veracity of a\ngiven claim. Specifically, we focus on the task of Fact Extraction and\nVERification (FEVER) and its accompanied dataset. The task consists of the\nsubtasks of retrieving the relevant documents (and sentences) from Wikipedia\nand validating whether the information in the documents supports or refutes a\ngiven claim. This task is essential and can be the building block of\napplications such as fake news detection and medical claim verification. In\nthis paper, we aim at a better understanding of the challenges of the task by\npresenting the literature in a structured and comprehensive way. Moreover, we\ndescribe the proposed methods by analyzing the technical perspectives of the\ndifferent approaches and discussing the performance results on the FEVER\ndataset, which is the most well-studied and formally structured dataset on the\nfact extraction and verification task. We also conduct the largest experimental\nstudy to date on identifying beneficial loss functions for the sentence\nretrieval component. Our analysis indicates that sampling negative sentences is\nimportant for improving the performance and decreasing the computational\ncomplexity. Finally, we describe open issues and future challenges, and we\nmotivate future research in the task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:05:43 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 20:35:52 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 09:46:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bekoulis", "Giannis", ""], ["Papagiannopoulou", "Christina", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2010.03009", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad and Nanyun Peng and Kai-Wei Chang", "title": "GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and\n  Event Extraction", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in cross-lingual relation and event extraction use graph\nconvolutional networks (GCNs) with universal dependency parses to learn\nlanguage-agnostic sentence representations such that models trained on one\nlanguage can be applied to other languages. However, GCNs struggle to model\nwords with long-range dependencies or are not directly connected in the\ndependency tree. To address these challenges, we propose to utilize the\nself-attention mechanism where we explicitly fuse structural information to\nlearn the dependencies between words with different syntactic distances. We\nintroduce GATE, a {\\bf G}raph {\\bf A}ttention {\\bf T}ransformer {\\bf E}ncoder,\nand test its cross-lingual transferability on relation and event extraction\ntasks. We perform experiments on the ACE05 dataset that includes three\ntypologically different languages: English, Chinese, and Arabic. The evaluation\nresults show that GATE outperforms three recently proposed methods by a large\nmargin. Our detailed analysis reveals that due to the reliance on syntactic\ndependencies, GATE produces robust representations that facilitate transfer\nacross languages.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:30:35 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 20:11:41 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Peng", "Nanyun", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2010.03010", "submitter": "Kanishka Misra", "authors": "Kanishka Misra, Allyson Ettinger, Julia Taylor Rayz", "title": "Exploring BERT's Sensitivity to Lexical Cues using Tests from Semantic\n  Priming", "comments": "Accepted for publication in Findings of ACL: EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.415", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models trained to estimate word probabilities in context have become\nubiquitous in natural language processing. How do these models use lexical cues\nin context to inform their word probabilities? To answer this question, we\npresent a case study analyzing the pre-trained BERT model with tests informed\nby semantic priming. Using English lexical stimuli that show priming in humans,\nwe find that BERT too shows \"priming,\" predicting a word with greater\nprobability when the context includes a related word versus an unrelated one.\nThis effect decreases as the amount of information provided by the context\nincreases. Follow-up analysis shows BERT to be increasingly distracted by\nrelated prime words as context becomes more informative, assigning lower\nprobabilities to related words. Our findings highlight the importance of\nconsidering contextual constraint effects when studying word prediction in\nthese models, and highlight possible parallels with human processing.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:30:59 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Misra", "Kanishka", ""], ["Ettinger", "Allyson", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2010.03017", "submitter": "Zirui Wang", "authors": "Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov", "title": "On Negative Interference in Multilingual Models: Findings and A\n  Meta-Learning Treatment", "comments": "Published as a main conference paper at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multilingual models are trained on concatenated text from multiple\nlanguages in hopes of conferring benefits to each (positive transfer), with the\nmost pronounced benefits accruing to low-resource languages. However, recent\nwork has shown that this approach can degrade performance on high-resource\nlanguages, a phenomenon known as negative interference. In this paper, we\npresent the first systematic study of negative interference. We show that,\ncontrary to previous belief, negative interference also impacts low-resource\nlanguages. While parameters are maximally shared to learn language-universal\nstructures, we demonstrate that language-specific parameters do exist in\nmultilingual models and they are a potential cause of negative interference.\nMotivated by these observations, we also present a meta-learning algorithm that\nobtains better cross-lingual transferability and alleviates negative\ninterference, by adding language-specific layers as meta-parameters and\ntraining them in a manner that explicitly improves shared layers'\ngeneralization on all languages. Overall, our results show that negative\ninterference is more common than previously known, suggesting new directions\nfor improving multilingual representations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 20:48:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wang", "Zirui", ""], ["Lipton", "Zachary C.", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2010.03022", "submitter": "Jie Ma", "authors": "Jie Ma, Shuai Wang, Rishita Anubhai, Miguel Ballesteros, Yaser\n  Al-Onaizan", "title": "Resource-Enhanced Neural Model for Event Argument Extraction", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event argument extraction (EAE) aims to identify the arguments of an event\nand classify the roles that those arguments play. Despite great efforts made in\nprior work, there remain many challenges: (1) Data scarcity. (2) Capturing the\nlong-range dependency, specifically, the connection between an event trigger\nand a distant event argument. (3) Integrating event trigger information into\ncandidate argument representation. For (1), we explore using unlabeled data in\ndifferent ways. For (2), we propose to use a syntax-attending Transformer that\ncan utilize dependency parses to guide the attention mechanism. For (3), we\npropose a trigger-aware sequence encoder with several types of\ntrigger-dependent sequence representations. We also support argument extraction\neither from text annotated with gold entities or from plain text. Experiments\non the English ACE2005 benchmark show that our approach achieves a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:00:54 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ma", "Jie", ""], ["Wang", "Shuai", ""], ["Anubhai", "Rishita", ""], ["Ballesteros", "Miguel", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "2010.03034", "submitter": "Peyman Passban", "authors": "Yimeng Wu, Peyman Passban, Mehdi Rezagholizade, Qun Liu", "title": "Why Skip If You Can Combine: A Simple Knowledge Distillation Technique\n  for Intermediate Layers", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of computing power neural machine translation (NMT) models\nalso grow accordingly and become better. However, they also become harder to\ndeploy on edge devices due to memory constraints. To cope with this problem, a\ncommon practice is to distill knowledge from a large and accurately-trained\nteacher network (T) into a compact student network (S). Although knowledge\ndistillation (KD) is useful in most cases, our study shows that existing KD\ntechniques might not be suitable enough for deep NMT engines, so we propose a\nnovel alternative. In our model, besides matching T and S predictions we have a\ncombinatorial mechanism to inject layer-level supervision from T to S. In this\npaper, we target low-resource settings and evaluate our translation engines for\nPortuguese--English, Turkish--English, and English--German directions. Students\ntrained using our technique have 50% fewer parameters and can still deliver\ncomparable results to those of 12-layer teachers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:08:16 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wu", "Yimeng", ""], ["Passban", "Peyman", ""], ["Rezagholizade", "Mehdi", ""], ["Liu", "Qun", ""]]}, {"id": "2010.03060", "submitter": "Gongbo Liang", "authors": "Gongbo Liang, Connor Greenwell, Yu Zhang, Xiaoqin Wang, Ramakanth\n  Kavuluru, Nathan Jacobs", "title": "Contrastive Cross-Modal Pre-Training: A General Strategy for Small\n  Sample Medical Imaging", "comments": "This work is under review with the IEEE Journal of Biomedical and\n  Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:20:29 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 15:18:04 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 17:10:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liang", "Gongbo", ""], ["Greenwell", "Connor", ""], ["Zhang", "Yu", ""], ["Wang", "Xiaoqin", ""], ["Kavuluru", "Ramakanth", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2010.03061", "submitter": "Adam Poliak", "authors": "Adam Poliak", "title": "A Survey on Recognizing Textual Entailment as an NLP Evaluation", "comments": "1st Workshop on Evaluation and Comparison for NLP systems (Eval4NLP)\n  at EMNLP 2020; 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Textual Entailment (RTE) was proposed as a unified evaluation\nframework to compare semantic understanding of different NLP systems. In this\nsurvey paper, we provide an overview of different approaches for evaluating and\nunderstanding the reasoning capabilities of NLP systems. We then focus our\ndiscussion on RTE by highlighting prominent RTE datasets as well as advances in\nRTE dataset that focus on specific linguistic phenomena that can be used to\nevaluate NLP systems on a fine-grained level. We conclude by arguing that when\nevaluating NLP systems, the community should utilize newly introduced RTE\ndatasets that focus on specific linguistic phenomena.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:23:00 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Poliak", "Adam", ""]]}, {"id": "2010.03065", "submitter": "Aditya Pal", "authors": "Aditya Pal and Bhaskar Karn", "title": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:33:58 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Pal", "Aditya", ""], ["Karn", "Bhaskar", ""]]}, {"id": "2010.03070", "submitter": "Liam Dugan", "authors": "Liam Dugan, Daphne Ippolito, Arun Kirubarajan and Chris Callison-Burch", "title": "RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text", "comments": "To be published in Annual Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large neural networks for natural language generation (NLG)\nhave made leaps and bounds in their ability to generate fluent text. However,\nthe tasks of evaluating quality differences between NLG systems and\nunderstanding how humans perceive the generated text remain both crucial and\ndifficult. In this system demonstration, we present Real or Fake Text (RoFT), a\nwebsite that tackles both of these challenges by inviting users to try their\nhand at detecting machine-generated text in a variety of domains. We introduce\na novel evaluation task based on detecting the boundary at which a text passage\nthat starts off human-written transitions to being machine-generated. We show\npreliminary results of using RoFT to evaluate detection of machine-generated\nnews articles.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:47:43 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Dugan", "Liam", ""], ["Ippolito", "Daphne", ""], ["Kirubarajan", "Arun", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2010.03073", "submitter": "Cicero Nogueira dos Santos", "authors": "Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng\n  Huang, Bing Xiang", "title": "Beyond [CLS] through Ranking by Generation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for Information Retrieval, where ranking of documents is\nviewed as the task of generating a query from a document's language model, were\nvery successful in various IR tasks in the past. However, with the advent of\nmodern deep neural networks, attention has shifted to discriminative ranking\nfunctions that model the semantic similarity of documents and queries instead.\nRecently, deep generative models such as GPT2 and BART have been shown to be\nexcellent text generators, but their effectiveness as rankers have not been\ndemonstrated yet. In this work, we revisit the generative framework for\ninformation retrieval and show that our generative approaches are as effective\nas state-of-the-art semantic similarity-based discriminative models for the\nanswer selection task. Additionally, we demonstrate the effectiveness of\nunlikelihood losses for IR.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:56:31 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Ma", "Xiaofei", ""], ["Nallapati", "Ramesh", ""], ["Huang", "Zhiheng", ""], ["Xiang", "Bing", ""]]}, {"id": "2010.03084", "submitter": "Xiaoyu Yang", "authors": "Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, Xiaodan Zhu", "title": "Program Enhanced Fact Verification with Verbalization and Graph\n  Attention Network", "comments": "16 pages (Accepted by EMNLP 2020 as a long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performing fact verification based on structured data is important for many\nreal-life applications and is a challenging research problem, particularly when\nit involves both symbolic operations and informal inference based on language\nunderstanding. In this paper, we present a Program-enhanced Verbalization and\nGraph Attention Network (ProgVGAT) to integrate programs and execution into\ntextual inference models. Specifically, a verbalization with program execution\nmodel is proposed to accumulate evidences that are embedded in operations over\nthe tables. Built on that, we construct the graph attention verification\nnetworks, which are designed to fuse different sources of evidences from\nverbalized program execution, program structures, and the original statements\nand tables, to make the final verification decision. To support the above\nframework, we propose a program selection module optimized with a new training\nstrategy based on margin loss, to produce more accurate programs, which is\nshown to be effective in enhancing the final verification results. Experimental\nresults show that the proposed framework achieves the new state-of-the-art\nperformance, a 74.4% accuracy, on the benchmark dataset TABFACT.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 23:29:08 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 16:43:38 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 00:49:15 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 16:35:19 GMT"}, {"version": "v5", "created": "Thu, 26 Nov 2020 20:27:59 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yang", "Xiaoyu", ""], ["Nie", "Feng", ""], ["Feng", "Yufei", ""], ["Liu", "Quan", ""], ["Chen", "Zhigang", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2010.03088", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Kyle Gorman", "title": "Is the Best Better? Bayesian Statistical Model Comparison for Natural\n  Language Processing", "comments": "Accepted to EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work raises concerns about the use of standard splits to compare\nnatural language processing models. We propose a Bayesian statistical model\ncomparison technique which uses k-fold cross-validation across multiple data\nsets to estimate the likelihood that one model will outperform the other, or\nthat the two will produce practically equivalent results. We use this technique\nto rank six English part-of-speech taggers across two data sets and three\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 23:37:28 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Gorman", "Kyle", ""]]}, {"id": "2010.03093", "submitter": "Faisal Ladhak", "authors": "Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen McKeown", "title": "WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive\n  Summarization", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce WikiLingua, a large-scale, multilingual dataset for the\nevaluation of crosslingual abstractive summarization systems. We extract\narticle and summary pairs in 18 languages from WikiHow, a high quality,\ncollaborative resource of how-to guides on a diverse set of topics written by\nhuman authors. We create gold-standard article-summary alignments across\nlanguages by aligning the images that are used to describe each how-to step in\nan article. As a set of baselines for further studies, we evaluate the\nperformance of existing cross-lingual abstractive summarization methods on our\ndataset. We further propose a method for direct crosslingual summarization\n(i.e., without requiring translation at inference time) by leveraging synthetic\ndata and Neural Machine Translation as a pre-training step. Our method\nsignificantly outperforms the baseline approaches, while being more cost\nefficient during inference.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 00:28:05 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ladhak", "Faisal", ""], ["Durmus", "Esin", ""], ["Cardie", "Claire", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2010.03096", "submitter": "Sheng Bi", "authors": "Xiya Cheng and Sheng Bi and Guilin Qi and Yongzhen Wang", "title": "Knowledge-aware Method for Confusing Charge Prediction", "comments": "Accepted by NLPCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic charge prediction task aims to determine the final charges based on\nfact descriptions of criminal cases, which is a vital application of legal\nassistant systems. Conventional works usually depend on fact descriptions to\npredict charges while ignoring the legal schematic knowledge, which makes it\ndifficult to distinguish confusing charges. In this paper, we propose a\nknowledge-attentive neural network model, which introduces legal schematic\nknowledge about charges and exploit the knowledge hierarchical representation\nas the discriminative features to differentiate confusing charges. Our model\ntakes the textual fact description as the input and learns fact representation\nthrough a graph convolutional network. A legal schematic knowledge transformer\nis utilized to generate crucial knowledge representations oriented to the legal\nschematic knowledge at both the schema and charge levels. We apply a knowledge\nmatching network for effectively incorporating charge information into the fact\nto learn knowledge-aware fact representation. Finally, we use the\nknowledge-aware fact representation for charge prediction. We create two\nreal-world datasets and experimental results show that our proposed model can\noutperform other state-of-the-art baselines on accuracy and F1 score,\nespecially on dealing with confusing charges.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 00:58:10 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cheng", "Xiya", ""], ["Bi", "Sheng", ""], ["Qi", "Guilin", ""], ["Wang", "Yongzhen", ""]]}, {"id": "2010.03099", "submitter": "Jiecao Chen", "authors": "Jiecao Chen, Liu Yang, Karthik Raman, Michael Bendersky, Jung-Jung\n  Yeh, Yun Zhou, Marc Najork, Danyang Cai, Ehsan Emadzadeh", "title": "DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching\n  and Pair Modeling", "comments": "13 pages. Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.264", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained models like BERT (Devlin et al., 2018) have dominated NLP / IR\napplications such as single sentence classification, text pair classification,\nand question answering. However, deploying these models in real systems is\nhighly non-trivial due to their exorbitant computational costs. A common remedy\nto this is knowledge distillation (Hinton et al., 2015), leading to faster\ninference. However -- as we show here -- existing works are not optimized for\ndealing with pairs (or tuples) of texts. Consequently, they are either not\nscalable or demonstrate subpar performance. In this work, we propose DiPair --\na novel framework for distilling fast and accurate models on text pair tasks.\nCoupled with an end-to-end training strategy, DiPair is both highly scalable\nand offers improved quality-speed tradeoffs. Empirical studies conducted on\nboth academic and real-world e-commerce benchmarks demonstrate the efficacy of\nthe proposed approach with speedups of over 350x and minimal quality drop\nrelative to the cross-attention teacher BERT model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 01:19:23 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Jiecao", ""], ["Yang", "Liu", ""], ["Raman", "Karthik", ""], ["Bendersky", "Michael", ""], ["Yeh", "Jung-Jung", ""], ["Zhou", "Yun", ""], ["Najork", "Marc", ""], ["Cai", "Danyang", ""], ["Emadzadeh", "Ehsan", ""]]}, {"id": "2010.03124", "submitter": "Machel Reid", "authors": "Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo", "title": "VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word\n  Representations for Improved Definition Modeling", "comments": "EMNLP 2020, 10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the task of definition modeling, where the goal is\nto learn to generate definitions of words and phrases. Existing approaches for\nthis task are discriminative, combining distributional and lexical semantics in\nan implicit rather than direct way. To tackle this issue we propose a\ngenerative model for the task, introducing a continuous latent variable to\nexplicitly model the underlying relationship between a phrase used within a\ncontext and its definition. We rely on variational inference for estimation and\nleverage contextualized word embeddings for improved performance. Our approach\nis evaluated on four existing challenging benchmarks with the addition of two\nnew datasets, \"Cambridge\" and the first non-English corpus \"Robert\", which we\nrelease to complement our empirical study. Our Variational Contextual\nDefinition Modeler (VCDM) achieves state-of-the-art performance in terms of\nautomatic and human evaluation metrics, demonstrating the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:48:44 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Reid", "Machel", ""], ["Marrese-Taylor", "Edison", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2010.03127", "submitter": "Takuma Udagawa", "authors": "Takuma Udagawa, Takato Yamazaki, Akiko Aizawa", "title": "A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial\n  Expressions", "comments": "16 pages, Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent models achieve promising results in visually grounded dialogues.\nHowever, existing datasets often contain undesirable biases and lack\nsophisticated linguistic analyses, which make it difficult to understand how\nwell current models recognize their precise linguistic structures. To address\nthis problem, we make two design choices: first, we focus on OneCommon Corpus\n\\citep{udagawa2019natural,udagawa2020annotated}, a simple yet challenging\ncommon grounding dataset which contains minimal bias by design. Second, we\nanalyze their linguistic structures based on \\textit{spatial expressions} and\nprovide comprehensive and reliable annotation for 600 dialogues. We show that\nour annotation captures important linguistic structures including\npredicate-argument structure, modification and ellipsis. In our experiments, we\nassess the model's understanding of these structures through reference\nresolution. We demonstrate that our annotation can reveal both the strengths\nand weaknesses of baseline models in essential levels of detail. Overall, we\npropose a novel framework and resource for investigating fine-grained language\nunderstanding in visually grounded dialogues.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:50:38 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Udagawa", "Takuma", ""], ["Yamazaki", "Takato", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2010.03138", "submitter": "Linzi Xing", "authors": "Linzi Xing, Brad Hackinen, Giuseppe Carenini, Francesco Trebbi", "title": "Improving Context Modeling in Neural Topic Segmentation", "comments": "Accepted at AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic segmentation is critical in key NLP tasks and recent works favor highly\neffective neural supervised approaches. However, current neural solutions are\narguably limited in how they model context. In this paper, we enhance a\nsegmenter based on a hierarchical attention BiLSTM network to better model\ncontext, by adding a coherence-related auxiliary task and restricted\nself-attention. Our optimized segmenter outperforms SOTA approaches when\ntrained and tested on three datasets. We also the robustness of our proposed\nmodel in domain transfer setting by training a model on a large-scale dataset\nand testing it on four challenging real-world benchmarks. Furthermore, we apply\nour proposed strategy to two other languages (German and Chinese), and show its\neffectiveness in multilingual scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:40:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Xing", "Linzi", ""], ["Hackinen", "Brad", ""], ["Carenini", "Giuseppe", ""], ["Trebbi", "Francesco", ""]]}, {"id": "2010.03142", "submitter": "Zehui Lin", "authors": "Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao\n  Zhou and Lei Li", "title": "Pre-training Multilingual Neural Machine Translation by Leveraging\n  Alignment Information", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the following question for machine translation (MT): can we\ndevelop a single universal MT model to serve as the common seed and obtain\nderivative and improved models on arbitrary language pairs? We propose mRASP,\nan approach to pre-train a universal multilingual neural machine translation\nmodel. Our key idea in mRASP is its novel technique of random aligned\nsubstitution, which brings words and phrases with similar meanings across\nmultiple languages closer in the representation space. We pre-train a mRASP\nmodel on 32 language pairs jointly with only public datasets. The model is then\nfine-tuned on downstream language pairs to obtain specialized MT models. We\ncarry out extensive experiments on 42 translation directions across a diverse\nsettings, including low, medium, rich resource, and as well as transferring to\nexotic language pairs. Experimental results demonstrate that mRASP achieves\nsignificant performance improvement compared to directly training on those\ntarget pairs. It is the first time to verify that multiple low-resource\nlanguage pairs can be utilized to improve rich resource MT. Surprisingly, mRASP\nis even able to improve the translation quality on exotic languages that never\noccur in the pre-training corpus. Code, data, and pre-trained models are\navailable at https://github.com/linzehui/mRASP.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:57:54 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 12:00:07 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 06:35:55 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lin", "Zehui", ""], ["Pan", "Xiao", ""], ["Wang", "Mingxuan", ""], ["Qiu", "Xipeng", ""], ["Feng", "Jiangtao", ""], ["Zhou", "Hao", ""], ["Li", "Lei", ""]]}, {"id": "2010.03146", "submitter": "Steven Cao", "authors": "Steven Cao, Nikita Kitaev, Dan Klein", "title": "Unsupervised Parsing via Constituency Tests", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for unsupervised parsing based on the linguistic notion\nof a constituency test. One type of constituency test involves modifying the\nsentence via some transformation (e.g. replacing the span with a pronoun) and\nthen judging the result (e.g. checking if it is grammatical). Motivated by this\nidea, we design an unsupervised parser by specifying a set of transformations\nand using an unsupervised neural acceptability model to make grammaticality\ndecisions. To produce a tree given a sentence, we score each span by\naggregating its constituency test judgments, and we choose the binary tree with\nthe highest total score. While this approach already achieves performance in\nthe range of current methods, we further improve accuracy by fine-tuning the\ngrammaticality model through a refinement procedure, where we alternate between\nimproving the estimated trees and improving the grammaticality model. The\nrefined model achieves 62.8 F1 on the Penn Treebank test set, an absolute\nimprovement of 7.6 points over the previous best published result.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:05:01 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cao", "Steven", ""], ["Kitaev", "Nikita", ""], ["Klein", "Dan", ""]]}, {"id": "2010.03147", "submitter": "Keshav Kolluru", "authors": "Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, and Soumen\n  Chakrabarti", "title": "OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open\n  Information Extraction", "comments": "EMNLP 2020 (Long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent state-of-the-art neural open information extraction (OpenIE) system\ngenerates extractions iteratively, requiring repeated encoding of partial\noutputs. This comes at a significant computational cost. On the other hand,\nsequence labeling approaches for OpenIE are much faster, but worse in\nextraction quality. In this paper, we bridge this trade-off by presenting an\niterative labeling-based system that establishes a new state of the art for\nOpenIE, while extracting 10x faster. This is achieved through a novel Iterative\nGrid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling\ntask. We improve its performance further by applying coverage (soft)\nconstraints on the grid at training time.\n  Moreover, on observing that the best OpenIE systems falter at handling\ncoordination structures, our OpenIE system also incorporates a new coordination\nanalyzer built with the same IGL architecture. This IGL based coordination\nanalyzer helps our OpenIE system handle complicated coordination structures,\nwhile also establishing a new state of the art on the task of coordination\nanalysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE\nsystem, OpenIE6, beats the previous systems by as much as 4 pts in F1, while\nbeing much faster.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:05:37 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Kolluru", "Keshav", ""], ["Adlakha", "Vaibhav", ""], ["Aggarwal", "Samarth", ""], ["Mausam", "", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2010.03154", "submitter": "Xiaochuang Han", "authors": "Xiaochuang Han, Yulia Tsvetkov", "title": "Fortifying Toxic Speech Detectors Against Veiled Toxicity", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern toxic speech detectors are incompetent in recognizing disguised\noffensive language, such as adversarial attacks that deliberately avoid known\ntoxic lexicons, or manifestations of implicit bias. Building a large annotated\ndataset for such veiled toxicity can be very expensive. In this work, we\npropose a framework aimed at fortifying existing toxic speech detectors without\na large labeled corpus of veiled toxicity. Just a handful of probing examples\nare used to surface orders of magnitude more disguised offenses. We augment the\ntoxic speech detector's training data with these discovered offensive examples,\nthereby making it more robust to veiled toxicity while preserving its utility\nin detecting overt toxicity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:43:48 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Han", "Xiaochuang", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2010.03155", "submitter": "Masato Mita", "authors": "Masato Mita, Shun Kiyono, Masahiro Kaneko, Jun Suzuki and Kentaro Inui", "title": "A Self-Refinement Strategy for Noise Reduction in Grammatical Error\n  Correction", "comments": "accepted by EMNLP 2020 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for grammatical error correction (GEC) largely rely on\nsupervised learning with manually created GEC datasets. However, there has been\nlittle focus on verifying and ensuring the quality of the datasets, and on how\nlower-quality data might affect GEC performance. We indeed found that there is\na non-negligible amount of \"noise\" where errors were inappropriately edited or\nleft uncorrected. To address this, we designed a self-refinement method where\nthe key idea is to denoise these datasets by leveraging the prediction\nconsistency of existing models, and outperformed strong denoising baseline\nmethods. We further applied task-specific techniques and achieved\nstate-of-the-art performance on the CoNLL-2014, JFLEG, and BEA-2019 benchmarks.\nWe then analyzed the effect of the proposed denoising method, and found that\nour approach leads to improved coverage of corrections and facilitated fluency\nedits which are reflected in higher recall and overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:45:09 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Mita", "Masato", ""], ["Kiyono", "Shun", ""], ["Kaneko", "Masahiro", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2010.03157", "submitter": "Sheng Bi", "authors": "Sheng Bi and Xiya Cheng and Yuan-Fang Li and Yongzhen Wang and Guilin\n  Qi", "title": "Knowledge-enriched, Type-constrained and Grammar-guided Question\n  Generation over Knowledge Bases", "comments": "Accepted by COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question generation over knowledge bases (KBQG) aims at generating\nnatural-language questions about a subgraph, i.e. a set of (connected) triples.\nTwo main challenges still face the current crop of encoder-decoder-based\nmethods, especially on small subgraphs: (1) low diversity and poor fluency due\nto the limited information contained in the subgraphs, and (2) semantic drift\ndue to the decoder's oblivion of the semantics of the answer entity. We propose\nan innovative knowledge-enriched, type-constrained and grammar-guided KBQG\nmodel, named KTG, to addresses the above challenges. In our model, the encoder\nis equipped with auxiliary information from the KB, and the decoder is\nconstrained with word types during QG. Specifically, entity domain and\ndescription, as well as relation hierarchy information are considered to\nconstruct question contexts, while a conditional copy mechanism is incorporated\nto modulate question semantics according to current word types. Besides, a\nnovel reward function featuring grammatical similarity is designed to improve\nboth generative richness and syntactic correctness via reinforcement learning.\nExtensive experiments show that our proposed model outperforms existing methods\nby a significant margin on two widely-used benchmark datasets SimpleQuestion\nand PathQuestion.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:49:48 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 05:39:58 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 03:32:38 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bi", "Sheng", ""], ["Cheng", "Xiya", ""], ["Li", "Yuan-Fang", ""], ["Wang", "Yongzhen", ""], ["Qi", "Guilin", ""]]}, {"id": "2010.03158", "submitter": "Xuelu Chen", "authors": "Xuelu Chen, Muhao Chen, Changjun Fan, Ankith Uppunda, Yizhou Sun,\n  Carlo Zaniolo", "title": "Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting missing facts in a knowledge graph (KG) is a crucial task in\nknowledge base construction and reasoning, and it has been the subject of much\nresearch in recent works using KG embeddings. While existing KG embedding\napproaches mainly learn and predict facts within a single KG, a more plausible\nsolution would benefit from the knowledge in multiple language-specific KGs,\nconsidering that different KGs have their own strengths and limitations on data\nquality and coverage. This is quite challenging, since the transfer of\nknowledge among multiple independently maintained KGs is often hindered by the\ninsufficiency of alignment information and the inconsistency of described\nfacts. In this paper, we propose KEnS, a novel framework for embedding learning\nand ensemble knowledge transfer across a number of language-specific KGs. KEnS\nembeds all KGs in a shared embedding space, where the association of entities\nis captured based on self-learning. Then, KEnS performs ensemble inference to\ncombine prediction results from embeddings of multiple language-specific KGs,\nfor which multiple ensemble techniques are investigated. Experiments on five\nreal-world language-specific KGs show that KEnS consistently improves\nstate-of-the-art methods on KG completion, via effectively identifying and\nleveraging complementary knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:54:03 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 07:54:24 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chen", "Xuelu", ""], ["Chen", "Muhao", ""], ["Fan", "Changjun", ""], ["Uppunda", "Ankith", ""], ["Sun", "Yizhou", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "2010.03179", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, David Adelani, Dawei Zhu, Jesujoba Alabi, Udia\n  Markus, Dietrich Klakow", "title": "Transfer Learning and Distant Supervision for Multilingual Transformer\n  Models: A Study on African Languages", "comments": "Accepted at EMNLP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual transformer models like mBERT and XLM-RoBERTa have obtained\ngreat improvements for many NLP tasks on a variety of languages. However,\nrecent works also showed that results from high-resource languages could not be\neasily transferred to realistic, low-resource scenarios. In this work, we study\ntrends in performance for different amounts of available resources for the\nthree African languages Hausa, isiXhosa and Yor\\`ub\\'a on both NER and topic\nclassification. We show that in combination with transfer learning or distant\nsupervision, these models can achieve with as little as 10 or 100 labeled\nsentences the same performance as baselines with much more supervised training\ndata. However, we also find settings where this does not hold. Our discussions\nand additional experiments on assumptions such as time and hardware\nrestrictions highlight challenges and opportunities in low-resource learning.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 05:23:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Adelani", "David", ""], ["Zhu", "Dawei", ""], ["Alabi", "Jesujoba", ""], ["Markus", "Udia", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.03189", "submitter": "BalaSundaraRaman Lakshmanan", "authors": "BalaSundaraRaman Lakshmanan and Sanjeeth Kumar Ravindranath", "title": "Theedhum Nandrum@Dravidian-CodeMix-FIRE2020: A Sentiment Polarity\n  Classifier for YouTube Comments with Code-switching between Tamil, Malayalam\n  and English", "comments": "FIRE 2020, December 16-20, 2020, Hyderabad, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Theedhum Nandrum is a sentiment polarity detection system using two\napproaches--a Stochastic Gradient Descent (SGD) based classifier and a Long\nShort-term Memory (LSTM) based Classifier. Our approach utilises language\nfeatures like use of emoji, choice of scripts and code mixing which appeared\nquite marked in the datasets specified for the Dravidian Codemix - FIRE 2020\ntask. The hyperparameters for the SGD were tuned using GridSearchCV. Our system\nwas ranked 4th in Tamil-English with a weighted average F1 score of 0.62 and\n9th in Malayalam-English with a score of 0.65. We achieved a weighted average\nF1 score of 0.77 for Tamil-English using a Logistic Regression based model\nafter the task deadline. This performance betters the top ranked classifier on\nthis dataset by a wide margin. Our use of language-specific Soundex to\nharmonise the spelling variants in code-mixed data appears to be a novel\napplication of Soundex. Our complete code is published in github at\nhttps://github.com/oligoglot/theedhum-nandrum.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 05:40:25 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 09:27:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Lakshmanan", "BalaSundaraRaman", ""], ["Ravindranath", "Sanjeeth Kumar", ""]]}, {"id": "2010.03193", "submitter": "Urmish Thakker", "authors": "Urmish Thakker, Jesse Beu, Dibakar Gope, Ganesh Dasika, Matthew\n  Mattina", "title": "Rank and run-time aware compression of NLP Applications", "comments": "Published at SustaiNLP@EMNLP 2020. arXiv admin note: text overlap\n  with arXiv:1906.04886", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence model based NLP applications can be large. Yet, many applications\nthat benefit from them run on small devices with very limited compute and\nstorage capabilities, while still having run-time constraints. As a result,\nthere is a need for a compression technique that can achieve significant\ncompression without negatively impacting inference run-time and task accuracy.\nThis paper proposes a new compression technique called Hybrid Matrix\nFactorization that achieves this dual objective. HMF improves low-rank matrix\nfactorization (LMF) techniques by doubling the rank of the matrix using an\nintelligent hybrid-structure leading to better accuracy than LMF. Further, by\npreserving dense matrices, it leads to faster inference run-time than pruning\nor structure matrix based compression technique. We evaluate the impact of this\ntechnique on 5 NLP benchmarks across multiple tasks (Translation, Intent\nDetection, Language Modeling) and show that for similar accuracy values and\ncompression factors, HMF can achieve more than 2.32x faster inference run-time\nthan pruning and 16.77% better accuracy than LMF.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:03:15 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Thakker", "Urmish", ""], ["Beu", "Jesse", ""], ["Gope", "Dibakar", ""], ["Dasika", "Ganesh", ""], ["Mattina", "Matthew", ""]]}, {"id": "2010.03205", "submitter": "Bodhisattwa Prasad Majumder", "authors": "Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-Kirkpatrick,\n  Julian McAuley", "title": "Like hiking? You probably enjoy nature: Persona-grounded Dialog with\n  Commonsense Expansions", "comments": "Accepted in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing persona-grounded dialog models often fail to capture simple\nimplications of given persona descriptions, something which humans are able to\ndo seamlessly. For example, state-of-the-art models cannot infer that interest\nin hiking might imply love for nature or longing for a break. In this paper, we\npropose to expand available persona sentences using existing commonsense\nknowledge bases and paraphrasing resources to imbue dialog models with access\nto an expanded and richer set of persona descriptions. Additionally, we\nintroduce fine-grained grounding on personas by encouraging the model to make a\ndiscrete choice among persona sentences while synthesizing a dialog response.\nSince such a choice is not observed in the data, we model it using a discrete\nlatent random variable and use variational learning to sample from hundreds of\npersona expansions. Our model outperforms competitive baselines on the\nPersonaChat dataset in terms of dialog quality and diversity while achieving\npersona-consistent and controllable dialog generation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:25:39 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Majumder", "Bodhisattwa Prasad", ""], ["Jhamtani", "Harsh", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["McAuley", "Julian", ""]]}, {"id": "2010.03222", "submitter": "Lukas Muttenthaler", "authors": "Lukas Muttenthaler, Isabelle Augenstein, Johannes Bjerva", "title": "Unsupervised Evaluation for Question Answering with Transformers", "comments": "8 pages, to be published in the Proceedings of the 2020 EMNLP\n  Workshop BlackboxNLP: Analysing and Interpreting Neural Networks for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to automatically evaluate the answer of a QA model at\ninference time. Although many models provide confidence scores, and simple\nheuristics can go a long way towards indicating answer correctness, such\nmeasures are heavily dataset-dependent and are unlikely to generalize. In this\nwork, we begin by investigating the hidden representations of questions,\nanswers, and contexts in transformer-based QA architectures. We observe a\nconsistent pattern in the answer representations, which we show can be used to\nautomatically evaluate whether or not a predicted answer span is correct. Our\nmethod does not require any labeled data and outperforms strong heuristic\nbaselines, across 2 datasets and 7 domains. We are able to predict whether or\nnot a model's answer is correct with 91.37% accuracy on SQuAD, and 80.7%\naccuracy on SubjQA. We expect that this method will have broad applications,\ne.g., in the semi-automatic development of QA datasets\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:03:30 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Muttenthaler", "Lukas", ""], ["Augenstein", "Isabelle", ""], ["Bjerva", "Johannes", ""]]}, {"id": "2010.03224", "submitter": "Jingxuan Yang", "authors": "Jingxuan Yang, Kerui Xu, Jun Xu, Si Li, Sheng Gao, Jun Guo, Ji-Rong\n  Wen, Nianwen Xue", "title": "Transformer-GCRF: Recovering Chinese Dropped Pronouns with General\n  Conditional Random Fields", "comments": "Accept as EMNLP-findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pronouns are often dropped in Chinese conversations and recovering the\ndropped pronouns is important for NLP applications such as Machine Translation.\nExisting approaches usually formulate this as a sequence labeling task of\npredicting whether there is a dropped pronoun before each token and its type.\nEach utterance is considered to be a sequence and labeled independently.\nAlthough these approaches have shown promise, labeling each utterance\nindependently ignores the dependencies between pronouns in neighboring\nutterances. Modeling these dependencies is critical to improving the\nperformance of dropped pronoun recovery. In this paper, we present a novel\nframework that combines the strength of Transformer network with General\nConditional Random Fields (GCRF) to model the dependencies between pronouns in\nneighboring utterances. Results on three Chinese conversation datasets show\nthat the Transformer-GCRF model outperforms the state-of-the-art dropped\npronoun recovery models. Exploratory analysis also demonstrates that the GCRF\ndid help to capture the dependencies between pronouns in neighboring\nutterances, thus contributes to the performance improvements.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:06:09 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Jingxuan", ""], ["Xu", "Kerui", ""], ["Xu", "Jun", ""], ["Li", "Si", ""], ["Gao", "Sheng", ""], ["Guo", "Jun", ""], ["Wen", "Ji-Rong", ""], ["Xue", "Nianwen", ""]]}, {"id": "2010.03249", "submitter": "Zhiyuan Liu", "authors": "Zhiyuan Liu, Yixin Cao, Liangming Pan, Juanzi Li, Zhiyuan Liu,\n  Tat-Seng Chua", "title": "Exploring and Evaluating Attributes, Values, and Structures for Entity\n  Alignment", "comments": "10 pages, EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich\ncontent by linking the equivalent entities from various KGs. GNN-based EA\nmethods present promising performances by modeling the KG structure defined by\nrelation triples. However, attribute triples can also provide crucial alignment\nsignal but have not been well explored yet. In this paper, we propose to\nutilize an attributed value encoder and partition the KG into subgraphs to\nmodel the various types of attribute triples efficiently. Besides, the\nperformances of current EA methods are overestimated because of the name-bias\nof existing EA datasets. To make an objective evaluation, we propose a hard\nexperimental setting where we select equivalent entity pairs with very\ndifferent names as the test set. Under both the regular and hard settings, our\nmethod achieves significant improvements ($5.10\\%$ on average Hits@$1$ in\nDBP$15$k) over $12$ baselines in cross-lingual and monolingual datasets.\nAblation studies on different subgraphs and a case study about attribute types\nfurther demonstrate the effectiveness of our method. Source code and data can\nbe found at https://github.com/thunlp/explore-and-evaluate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:03:58 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 08:35:38 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Zhiyuan", ""], ["Cao", "Yixin", ""], ["Pan", "Liangming", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2010.03260", "submitter": "Tao Ge", "authors": "Mengyun Chen, Tao Ge, Xingxing Zhang, Furu Wei, Ming Zhou", "title": "Improving the Efficiency of Grammatical Error Correction with Erroneous\n  Span Detection and Correction", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel language-independent approach to improve the efficiency\nfor Grammatical Error Correction (GEC) by dividing the task into two subtasks:\nErroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD\nidentifies grammatically incorrect text spans with an efficient sequence\ntagging model. Then, ESC leverages a seq2seq model to take the sentence with\nannotated erroneous spans as input and only outputs the corrected text for\nthese spans. Experiments show our approach performs comparably to conventional\nseq2seq approaches in both English and Chinese GEC benchmarks with less than\n50% time cost for inference.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:29:11 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Chen", "Mengyun", ""], ["Ge", "Tao", ""], ["Zhang", "Xingxing", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "2010.03272", "submitter": "Harsh Jhamtani", "authors": "Harsh Jhamtani and Taylor Berg-Kirkpatrick", "title": "Narrative Text Generation with a Latent Discrete Plan", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past work on story generation has demonstrated the usefulness of conditioning\non a generation plan to generate coherent stories. However, these approaches\nhave used heuristics or off-the-shelf models to first tag training stories with\nthe desired type of plan, and then train generation models in a supervised\nfashion. In this paper, we propose a deep latent variable model that first\nsamples a sequence of anchor words, one per sentence in the story, as part of\nits generative process. During training, our model treats the sequence of\nanchor words as a latent variable and attempts to induce anchoring sequences\nthat help guide generation in an unsupervised fashion. We conduct experiments\nwith several types of sentence decoder distributions: left-to-right and\nnon-monotonic, with different degrees of restriction. Further, since we use\namortized variational inference to train our model, we introduce two\ncorresponding types of inference network for predicting the posterior on anchor\nwords. We conduct human evaluations which demonstrate that the stories produced\nby our model are rated better in comparison with baselines which do not\nconsider story plans, and are similar or better in quality relative to\nbaselines which use external supervision for plans. Additionally, the proposed\nmodel gets favorable scores when evaluated on perplexity, diversity, and\ncontrol of story via discrete plan.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:45:37 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Jhamtani", "Harsh", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2010.03274", "submitter": "Harsh Jhamtani", "authors": "Harsh Jhamtani and Peter Clark", "title": "Learning to Explain: Datasets and Models for Identifying Valid Reasoning\n  Chains in Multihop Question-Answering", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid progress in multihop question-answering (QA), models still\nhave trouble explaining why an answer is correct, with limited explanation\ntraining data available to learn from. To address this, we introduce three\nexplanation datasets in which explanations formed from corpus facts are\nannotated. Our first dataset, eQASC, contains over 98K explanation annotations\nfor the multihop question answering dataset QASC, and is the first that\nannotates multiple candidate explanations for each answer. The second dataset\neQASC-perturbed is constructed by crowd-sourcing perturbations (while\npreserving their validity) of a subset of explanations in QASC, to test\nconsistency and generalization of explanation prediction models. The third\ndataset eOBQA is constructed by adding explanation annotations to the OBQA\ndataset to test generalization of models trained on eQASC. We show that this\ndata can be used to significantly improve explanation quality (+14% absolute F1\nover a strong retrieval baseline) using a BERT-based classifier, but still\nbehind the upper bound, offering a new challenge for future research. We also\nexplore a delexicalized chain representation in which repeated noun phrases are\nreplaced by variables, thus turning them into generalized reasoning chains (for\nexample: \"X is a Y\" AND \"Y has Z\" IMPLIES \"X has Z\"). We find that generalized\nchains maintain performance while also being more robust to certain\nperturbations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:46:02 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Jhamtani", "Harsh", ""], ["Clark", "Peter", ""]]}, {"id": "2010.03276", "submitter": "Tzuf Paz-Argaman", "authors": "Tzuf Paz-Argaman, Yuval Atzmon, Gal Chechik, Reut Tsarfaty", "title": "ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity\n  and Visual Summarization", "comments": "11 pages, Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognizing visual entities from the textual\ndescriptions of their classes. Specifically, given birds' images with free-text\ndescriptions of their species, we learn to classify images of previously-unseen\nspecies based on specie descriptions. This setup has been studied in the vision\ncommunity under the name zero-shot learning from text, focusing on learning to\ntransfer knowledge about visual aspects of birds from seen classes to\npreviously-unseen ones. Here, we suggest focusing on the textual description\nand distilling from the description the most relevant information to\neffectively match visual features to the parts of the text that discuss them.\nSpecifically, (1) we propose to leverage the similarity between species,\nreflected in the similarity between text descriptions of the species. (2) we\nderive visual summaries of the texts, i.e., extractive summaries that focus on\nthe visual features that tend to be reflected in images. We propose a simple\nattention-based model augmented with the similarity and visual summaries\ncomponents. Our empirical results consistently and significantly outperform the\nstate-of-the-art on the largest benchmarks for text-based zero-shot learning,\nillustrating the critical importance of texts for zero-shot image-recognition.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:57:34 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Paz-Argaman", "Tzuf", ""], ["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2010.03295", "submitter": "Marco Basaldella", "authors": "Marco Basaldella, Fangyu Liu, Ehsan Shareghi and Nigel Collier", "title": "COMETA: A Corpus for Medical Entity Linking in the Social Media", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there has been growing progress in Entity Linking (EL) for general\nlanguage, existing datasets fail to address the complex nature of health\nterminology in layman's language. Meanwhile, there is a growing need for\napplications that can understand the public's voice in the health domain. To\naddress this we introduce a new corpus called COMETA, consisting of 20k English\nbiomedical entity mentions from Reddit expert-annotated with links to SNOMED\nCT, a widely-used medical knowledge graph. Our corpus satisfies a combination\nof desirable properties, from scale and coverage to diversity and quality, that\nto the best of our knowledge has not been met by any of the existing resources\nin the field. Through benchmark experiments on 20 EL baselines from string- to\nneural-based models we shed light on the ability of these systems to perform\ncomplex inference on entities and concepts under 2 challenging evaluation\nscenarios. Our experimental results on COMETA illustrate that no golden bullet\nexists and even the best mainstream techniques still have a significant\nperformance gap to fill, while the best solution relies on combining different\nviews of data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:16:45 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 12:01:55 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Basaldella", "Marco", ""], ["Liu", "Fangyu", ""], ["Shareghi", "Ehsan", ""], ["Collier", "Nigel", ""]]}, {"id": "2010.03338", "submitter": "Mingzhu Wu", "authors": "Mingzhu Wu, Nafise Sadat Moosavi, Andreas R\\\"uckl\\'e and Iryna\n  Gurevych", "title": "Improving QA Generalization by Concurrent Modeling of Multiple Biases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing NLP datasets contain various biases that models can easily exploit\nto achieve high performances on the corresponding evaluation sets. However,\nfocusing on dataset-specific biases limits their ability to learn more\ngeneralizable knowledge about the task from more general data patterns. In this\npaper, we investigate the impact of debiasing methods for improving\ngeneralization and propose a general framework for improving the performance on\nboth in-domain and out-of-domain datasets by concurrent modeling of multiple\nbiases in the training data. Our framework weights each example based on the\nbiases it contains and the strength of those biases in the training data. It\nthen uses these weights in the training objective so that the model relies less\non examples with high bias weights. We extensively evaluate our framework on\nextractive question answering with training data from various domains with\nmultiple biases of different strengths. We perform the evaluations in two\ndifferent settings, in which the model is trained on a single domain or\nmultiple domains simultaneously, and show its effectiveness in both settings\ncompared to state-of-the-art debiasing methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:18:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wu", "Mingzhu", ""], ["Moosavi", "Nafise Sadat", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.03343", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "Slice-Aware Neural Ranking", "comments": "Paper accepted to EMNLP workshop SCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding when and why neural ranking models fail for an IR task via\nerror analysis is an important part of the research cycle. Here we focus on the\nchallenges of (i) identifying categories of difficult instances (a pair of\nquestion and response candidates) for which a neural ranker is ineffective and\n(ii) improving neural ranking for such instances. To address both challenges we\nresort to slice-based learning for which the goal is to improve effectiveness\nof neural models for slices (subsets) of data. We address challenge (i) by\nproposing different slicing functions (SFs) that select slices of the\ndataset---based on prior work we heuristically capture different failures of\nneural rankers. Then, for challenge (ii) we adapt a neural ranking model to\nlearn slice-aware representations, i.e. the adapted model learns to represent\nthe question and responses differently based on the model's prediction of which\nslices they belong to. Our experimental results (the source code and data are\navailable at https://github.com/Guzpenha/slice_based_learning) across three\ndifferent ranking tasks and four corpora show that slice-based learning\nimproves the effectiveness by an average of 2% over a neural ranker that is not\nslice-aware.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:40:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2010.03369", "submitter": "Marco Guerini", "authors": "Thomas Scialom, Serra Sinem Tekiroglu, Jacopo Staiano, Marco Guerini", "title": "Toward Stance-based Personas for Opinionated Dialogues", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of chit-chat dialogues it has been shown that endowing systems\nwith a persona profile is important to produce more coherent and meaningful\nconversations. Still, the representation of such personas has thus far been\nlimited to a fact-based representation (e.g. \"I have two cats.\"). We argue that\nthese representations remain superficial w.r.t. the complexity of human\npersonality. In this work, we propose to make a step forward and investigate\nstance-based persona, trying to grasp more profound characteristics, such as\nopinions, values, and beliefs to drive language generation. To this end, we\nintroduce a novel dataset allowing to explore different stance-based persona\nrepresentations and their impact on claim generation, showing that they are\nable to grasp abstract and profound aspects of the author persona.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:30:30 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Scialom", "Thomas", ""], ["Tekiroglu", "Serra Sinem", ""], ["Staiano", "Jacopo", ""], ["Guerini", "Marco", ""]]}, {"id": "2010.03384", "submitter": "Max Glockner", "authors": "Max Glockner, Ivan Habernal, Iryna Gurevych", "title": "Why do you think that? Exploring Faithful Sentence-Level Rationales\n  Without Supervision", "comments": "EMNLP Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the trustworthiness of a model's prediction is essential for\ndifferentiating between `right for the right reasons' and `right for the wrong\nreasons'. Identifying textual spans that determine the target label, known as\nfaithful rationales, usually relies on pipeline approaches or reinforcement\nlearning. However, such methods either require supervision and thus costly\nannotation of the rationales or employ non-differentiable models. We propose a\ndifferentiable training-framework to create models which output faithful\nrationales on a sentence level, by solely applying supervision on the target\ntask. To achieve this, our model solves the task based on each rationale\nindividually and learns to assign high scores to those which solved the task\nbest. Our evaluation on three different datasets shows competitive results\ncompared to a standard BERT blackbox while exceeding a pipeline counterpart's\nperformance in two cases. We further exploit the transparent decision-making\nprocess of these models to prefer selecting the correct rationales by applying\ndirect supervision, thereby boosting the performance on the rationale-level.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:54:28 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Glockner", "Max", ""], ["Habernal", "Ivan", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.03412", "submitter": "Weijia Xu", "authors": "Weijia Xu, Xing Niu, Marine Carpuat", "title": "Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural\n  Machine Translation", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Iterative Back-Translation and Dual Learning effectively incorporate\nmonolingual training data in neural machine translation, they use different\nobjectives and heuristic gradient approximation strategies, and have not been\nextensively compared. We introduce a novel dual reconstruction objective that\nprovides a unified view of Iterative Back-Translation and Dual Learning. It\nmotivates a theoretical analysis and controlled empirical study on\nGerman-English and Turkish-English tasks, which both suggest that Iterative\nBack-Translation is more effective than Dual Learning despite its relative\nsimplicity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:40:32 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Xu", "Weijia", ""], ["Niu", "Xing", ""], ["Carpuat", "Marine", ""]]}, {"id": "2010.03424", "submitter": "Phuong Le-Hong", "authors": "The Viet Bui, Phuong Le-Hong", "title": "Cross-lingual Extended Named Entity Classification of Wikipedia Articles", "comments": "Accepted to NTCIR-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FPT.AI team participated in the SHINRA2020-ML subtask of the NTCIR-15\nSHINRA task. This paper describes our method to solving the problem and\ndiscusses the official results. Our method focuses on learning cross-lingual\nrepresentations, both on the word level and document level for page\nclassification. We propose a three-stage approach including multilingual model\npre-training, monolingual model fine-tuning and cross-lingual voting. Our\nsystem is able to achieve the best scores for 25 out of 30 languages; and its\naccuracy gaps to the best performing systems of the other five languages are\nrelatively small.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:06:09 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 09:06:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bui", "The Viet", ""], ["Le-Hong", "Phuong", ""]]}, {"id": "2010.03432", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Piotr \\.Zelasko, Mikolaj Morzy, Adrian Szymczak,\n  Marzena \\.Zy{\\l}a-Hoppe, Joanna Banaszczak, Lukasz Augustyniak, Jan Mizgajski\n  and Yishay Carmiel", "title": "WER we are and WER we think we are", "comments": "Accepted to EMNLP Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing of conversational speech requires the\navailability of high-quality transcripts. In this paper, we express our\nskepticism towards the recent reports of very low Word Error Rates (WERs)\nachieved by modern Automatic Speech Recognition (ASR) systems on benchmark\ndatasets. We outline several problems with popular benchmarks and compare three\nstate-of-the-art commercial ASR systems on an internal dataset of real-life\nspontaneous human conversations and HUB'05 public benchmark. We show that WERs\nare significantly higher than the best reported results. We formulate a set of\nguidelines which may aid in the creation of real-life, multi-domain datasets\nwith high quality annotations for training and testing of robust ASR systems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:20:31 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["\u017belasko", "Piotr", ""], ["Morzy", "Mikolaj", ""], ["Szymczak", "Adrian", ""], ["\u017by\u0142a-Hoppe", "Marzena", ""], ["Banaszczak", "Joanna", ""], ["Augustyniak", "Lukasz", ""], ["Mizgajski", "Jan", ""], ["Carmiel", "Yishay", ""]]}, {"id": "2010.03446", "submitter": "Ewan Dunbar", "authors": "Louis Fournier, Emmanuel Dupoux, Ewan Dunbar", "title": "Analogies minus analogy test: measuring regularities in word embeddings", "comments": null, "journal-ref": "Proceedings of CoNLL 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector space models of words have long been claimed to capture linguistic\nregularities as simple vector translations, but problems have been raised with\nthis claim. We decompose and empirically analyze the classic arithmetic word\nanalogy test, to motivate two new metrics that address the issues with the\nstandard test, and which distinguish between class-wise offset concentration\n(similar directions between pairs of words drawn from different broad classes,\nsuch as France--London, China--Ottawa, ...) and pairing consistency (the\nexistence of a regular transformation between correctly-matched pairs such as\nFrance:Paris::China:Beijing). We show that, while the standard analogy test is\nflawed, several popular word embeddings do nevertheless encode linguistic\nregularities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:38:35 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Fournier", "Louis", ""], ["Dupoux", "Emmanuel", ""], ["Dunbar", "Ewan", ""]]}, {"id": "2010.03450", "submitter": "Annie Louis", "authors": "Annie Louis, Dan Roth, and Filip Radlinski", "title": "\"I'd rather just go to bed\": Understanding Indirect Answers", "comments": "15 pages, 3 figures. To appear at the 2020 Conference on Empirical\n  Methods in Natural Language Processing (EMNLP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a pragmatic inference problem in dialog: understanding indirect\nresponses to questions. Humans can interpret 'I'm starving.' in response to\n'Hungry?', even without direct cue words such as 'yes' and 'no'. In dialog\nsystems, allowing natural responses rather than closed vocabularies would be\nsimilarly beneficial. However, today's systems are only as sensitive to these\npragmatic moves as their language model allows. We create and release the first\nlarge-scale English language corpus 'Circa' with 34,268 (polar question,\nindirect answer) pairs to enable progress on this task. The data was collected\nvia elaborate crowdsourcing, and contains utterances with yes/no meaning, as\nwell as uncertain, middle-ground, and conditional responses. We also present\nBERT-based neural models to predict such categories for a question-answer pair.\nWe find that while transfer learning from entailment works reasonably,\nperformance is not yet sufficient for robust dialog. Our models reach 82-88%\naccuracy for a 4-class distinction, and 74-85% for 6 classes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:41:40 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Louis", "Annie", ""], ["Roth", "Dan", ""], ["Radlinski", "Filip", ""]]}, {"id": "2010.03476", "submitter": "Bernhard Kratzwald", "authors": "Bernhard Kratzwald, Stefan Feuerriegel, Huan Sun", "title": "Learning a Cost-Effective Annotation Policy for Question Answering", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art question answering (QA) relies upon large amounts of\ntraining data for which labeling is time consuming and thus expensive. For this\nreason, customizing QA systems is challenging. As a remedy, we propose a novel\nframework for annotating QA datasets that entails learning a cost-effective\nannotation policy and a semi-supervised annotation scheme. The latter reduces\nthe human effort: it leverages the underlying QA system to suggest potential\ncandidate annotations. Human annotators then simply provide binary feedback on\nthese candidates. Our system is designed such that past annotations\ncontinuously improve the future performance and thus overall annotation cost.\nTo the best of our knowledge, this is the first paper to address the problem of\nannotating questions with minimal annotation cost. We compare our framework\nagainst traditional manual annotations in an extensive set of experiments. We\nfind that our approach can reduce up to 21.1% of the annotation cost.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:25:41 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 20:20:49 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kratzwald", "Bernhard", ""], ["Feuerriegel", "Stefan", ""], ["Sun", "Huan", ""]]}, {"id": "2010.03481", "submitter": "Andrey Kutuzov", "authors": "Julia Rodina, Yuliya Trofimova, Andrey Kutuzov, Ekaterina Artemova", "title": "ELMo and BERT in semantic change detection for Russian", "comments": "The 9th International Conference on Analysis of Images, Social\n  Networks and Texts (AIST 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effectiveness of contextualized embeddings for the task of\ndiachronic semantic change detection for Russian language data. Evaluation test\nsets consist of Russian nouns and adjectives annotated based on their\noccurrences in texts created in pre-Soviet, Soviet and post-Soviet time\nperiods. ELMo and BERT architectures are compared on the task of ranking\nRussian words according to the degree of their semantic change over time. We\nuse several methods for aggregation of contextualized embeddings from these\narchitectures and evaluate their performance. Finally, we compare unsupervised\nand supervised techniques in this task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:34:00 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Rodina", "Julia", ""], ["Trofimova", "Yuliya", ""], ["Kutuzov", "Andrey", ""], ["Artemova", "Ekaterina", ""]]}, {"id": "2010.03486", "submitter": "Valentin Barriere", "authors": "Valentin Barriere and Alexandra Balahur", "title": "Improving Sentiment Analysis over non-English Tweets using Multilingual\n  Transformers and Automatic Translation for Data-Augmentation", "comments": "Accepted to COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweets are specific text data when compared to general text. Although\nsentiment analysis over tweets has become very popular in the last decade for\nEnglish, it is still difficult to find huge annotated corpora for non-English\nlanguages. The recent rise of the transformer models in Natural Language\nProcessing allows to achieve unparalleled performances in many tasks, but these\nmodels need a consequent quantity of text to adapt to the tweet domain. We\npropose the use of a multilingual transformer model, that we pre-train over\nEnglish tweets and apply data-augmentation using automatic translation to adapt\nthe model to non-English languages. Our experiments in French, Spanish, German\nand Italian suggest that the proposed technique is an efficient way to improve\nthe results of the transformers over small corpora of tweets in a non-English\nlanguage.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:44:55 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Barriere", "Valentin", ""], ["Balahur", "Alexandra", ""]]}, {"id": "2010.03494", "submitter": "Sebastian Goodman", "authors": "Sebastian Goodman, Nan Ding, Radu Soricut", "title": "TeaForN: Teacher-Forcing with N-grams", "comments": "to be published in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence generation models trained with teacher-forcing suffer from issues\nrelated to exposure bias and lack of differentiability across timesteps. Our\nproposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these\nproblems directly, through the use of a stack of N decoders trained to decode\nalong a secondary time axis that allows model parameter updates based on N\nprediction steps. TeaForN can be used with a wide class of decoder\narchitectures and requires minimal modifications from a standard\nteacher-forcing setup. Empirically, we show that TeaForN boosts generation\nquality on one Machine Translation benchmark, WMT 2014 English-French, and two\nNews Summarization benchmarks, CNN/Dailymail and Gigaword.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:58:25 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 16:45:20 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Goodman", "Sebastian", ""], ["Ding", "Nan", ""], ["Soricut", "Radu", ""]]}, {"id": "2010.03496", "submitter": "Daniel Daza", "authors": "Daniel Daza, Michael Cochez, Paul Groth", "title": "Inductive Entity Representations from Text via Link Prediction", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3450141", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KG) are of vital importance for multiple applications on\nthe web, including information retrieval, recommender systems, and metadata\nannotation. Regardless of whether they are built manually by domain experts or\nwith automatic pipelines, KGs are often incomplete. Recent work has begun to\nexplore the use of textual descriptions available in knowledge graphs to learn\nvector representations of entities in order to preform link prediction.\nHowever, the extent to which these representations learned for link prediction\ngeneralize to other tasks is unclear. This is important given the cost of\nlearning such representations. Ideally, we would prefer representations that do\nnot need to be trained again when transferring to a different task, while\nretaining reasonable performance.\n  In this work, we propose a holistic evaluation protocol for entity\nrepresentations learned via a link prediction objective. We consider the\ninductive link prediction and entity classification tasks, which involve\nentities not seen during training. We also consider an information retrieval\ntask for entity-oriented search. We evaluate an architecture based on a\npretrained language model, that exhibits strong generalization to entities not\nobserved during training, and outperforms related state-of-the-art methods (22%\nMRR improvement in link prediction on average). We further provide evidence\nthat the learned representations transfer well to other tasks without\nfine-tuning. In the entity classification task we obtain an average improvement\nof 16% in accuracy compared with baselines that also employ pre-trained models.\nIn the information retrieval task, we obtain significant improvements of up to\n8.8% in NDCG@10 for natural language queries. We thus show that the learned\nrepresentations are not limited KG-specific tasks, and have greater\ngeneralization properties than evaluated in previous work.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:04:06 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 10:36:23 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 09:38:45 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Daza", "Daniel", ""], ["Cochez", "Michael", ""], ["Groth", "Paul", ""]]}, {"id": "2010.03526", "submitter": "Jiapeng Wu", "authors": "Jiapeng Wu, Meng Cao, Jackie Chi Kit Cheung and William L. Hamilton", "title": "TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion", "comments": "17 pages, 9 figures. EMNLP 2020 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental\nand challenging task. Previous works have approached this problem by augmenting\nmethods for static knowledge graphs to leverage time-dependent representations.\nHowever, these methods do not explicitly leverage multi-hop structural\ninformation and temporal facts from recent time steps to enhance their\npredictions. Additionally, prior work does not explicitly address the temporal\nsparsity and variability of entity distributions in TKGs. We propose the\nTemporal Message Passing (TeMP) framework to address these challenges by\ncombining graph neural networks, temporal dynamics models, data imputation and\nfrequency-based gating techniques. Experiments on standard TKG tasks show that\nour approach provides substantial gains compared to the previous state of the\nart, achieving a 10.7% average relative improvement in Hits@10 across three\nstandard benchmarks. Our analysis also reveals important sources of variability\nboth within and across TKG datasets, and we introduce several simple but strong\nbaselines that outperform the prior state of the art in certain settings.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:11:53 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wu", "Jiapeng", ""], ["Cao", "Meng", ""], ["Cheung", "Jackie Chi Kit", ""], ["Hamilton", "William L.", ""]]}, {"id": "2010.03532", "submitter": "Yixin Nie", "authors": "Yixin Nie, Xiang Zhou, Mohit Bansal", "title": "What Can We Learn from Collective Human Opinions on Natural Language\n  Inference Data?", "comments": "EMNLP 2020 (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the subjective nature of many NLP tasks, most NLU evaluations have\nfocused on using the majority label with presumably high agreement as the\nground truth. Less attention has been paid to the distribution of human\nopinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to\nstudy Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset\nis created by collecting 100 annotations per example for 3,113 examples in SNLI\nand MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high\nhuman disagreement exists in a noticeable amount of examples in these datasets;\n(2) the state-of-the-art models lack the ability to recover the distribution\nover human labels; (3) models achieve near-perfect accuracy on the subset of\ndata with a high level of human agreement, whereas they can barely beat a\nrandom guess on the data with low levels of human agreement, which compose most\nof the common errors made by state-of-the-art models on the evaluation sets.\nThis questions the validity of improving model performance on old metrics for\nthe low-agreement part of evaluation datasets. Hence, we argue for a detailed\nexamination of human agreement in future data collection efforts, and\nevaluating model outputs against the distribution over collective human\nopinions. The ChaosNLI dataset and experimental scripts are available at\nhttps://github.com/easonnie/ChaosNLI\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:26:06 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 19:32:45 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Nie", "Yixin", ""], ["Zhou", "Xiang", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.03538", "submitter": "Esin Durmus", "authors": "Jialu Li, Esin Durmus and Claire Cardie", "title": "Exploring the Role of Argument Structure in Online Debate Persuasion", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online debate forums provide users a platform to express their opinions on\ncontroversial topics while being exposed to opinions from diverse set of\nviewpoints. Existing work in Natural Language Processing (NLP) has shown that\nlinguistic features extracted from the debate text and features encoding the\ncharacteristics of the audience are both critical in persuasion studies. In\nthis paper, we aim to further investigate the role of discourse structure of\nthe arguments from online debates in their persuasiveness. In particular, we\nuse the factor graph model to obtain features for the argument structure of\ndebates from an online debating platform and incorporate these features to an\nLSTM-based model to predict the debater that makes the most convincing\narguments. We find that incorporating argument structure features play an\nessential role in achieving the better predictive performance in assessing the\npersuasiveness of the arguments in online debates.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:34:50 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Li", "Jialu", ""], ["Durmus", "Esin", ""], ["Cardie", "Claire", ""]]}, {"id": "2010.03542", "submitter": "Shuohuan Wang", "authors": "Shuohuan Wang, Jiaxiang Liu, Xuan Ouyang, Yu Sun", "title": "Galileo at SemEval-2020 Task 12: Multi-lingual Learning for Offensive\n  Language Identification using Pre-trained Language Models", "comments": "8 pages, 2 figures, 6 tables. Accepted at Proceedings of 14th\n  International Workshop on Semantic Evaluation (SemEval-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes Galileo's performance in SemEval-2020 Task 12 on\ndetecting and categorizing offensive language in social media. For Offensive\nLanguage Identification, we proposed a multi-lingual method using Pre-trained\nLanguage Models, ERNIE and XLM-R. For offensive language categorization, we\nproposed a knowledge distillation method trained on soft labels generated by\nseveral supervised models. Our team participated in all three sub-tasks. In\nSub-task A - Offensive Language Identification, we ranked first in terms of\naverage F1 scores in all languages. We are also the only team which ranked\namong the top three across all languages. We also took the first place in\nSub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence\nTarget Identification.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:40:19 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wang", "Shuohuan", ""], ["Liu", "Jiaxiang", ""], ["Ouyang", "Xuan", ""], ["Sun", "Yu", ""]]}, {"id": "2010.03544", "submitter": "Nima Ebadi", "authors": "Nima Ebadi, Peyman Najafirad", "title": "A Self-supervised Approach for Semantic Indexing in the Context of\n  COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic has accelerated the pace at which COVID-19 scientific papers are\npublished. In addition, the process of manually assigning semantic indexes to\nthese papers by experts is even more time-consuming and overwhelming in the\ncurrent health crisis. Therefore, there is an urgent need for automatic\nsemantic indexing models which can effectively scale-up to newly introduced\nconcepts and rapidly evolving distributions of the hyperfocused related\nliterature. In this research, we present a novel semantic indexing approach\nbased on the state-of-the-art self-supervised representation learning and\ntransformer encoding exclusively suitable for pandemic crises. We present a\ncase study on a novel dataset that is based on COVID-19 papers published and\nmanually indexed in PubMed. Our study shows that our self-supervised model\noutperforms the best performing models of BioASQ Task 8a by micro-F1 score of\n0.1 and LCA-F score of 0.08 on average. Our model also shows superior\nperformance on detecting the supplementary concepts which is quite important\nwhen the focus of the literature has drastically shifted towards specific\nconcepts related to the pandemic. Our study sheds light on the main challenges\nconfronting semantic indexing models during a pandemic, namely new domains and\ndrastic changes of their distributions, and as a superior alternative for such\nsituations, propose a model founded on approaches which have shown auspicious\nperformance in improving generalization and data efficiency in various NLP\ntasks. We also show the joint indexing of major Medical Subject Headings (MeSH)\nand supplementary concepts improves the overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:43:55 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ebadi", "Nima", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2010.03546", "submitter": "Xilun Chen", "authors": "Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer and Sonal\n  Gupta", "title": "Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic\n  Parsing", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented semantic parsing is a critical component of virtual assistants,\nwhich is responsible for understanding the user's intents (set reminder, play\nmusic, etc.). Recent advances in deep learning have enabled several approaches\nto successfully parse more complex queries (Gupta et al., 2018; Rongali et\nal.,2020), but these models require a large amount of annotated training data\nto parse queries on new domains (e.g. reminder, music).\n  In this paper, we focus on adapting task-oriented semantic parsers to\nlow-resource domains, and propose a novel method that outperforms a supervised\nneural model at a 10-fold data reduction. In particular, we identify two\nfundamental factors for low-resource domain adaptation: better representation\nlearning and better training techniques. Our representation learning uses BART\n(Lewis et al., 2019) to initialize our model which outperforms encoder-only\npre-trained representations used in previous work. Furthermore, we train with\noptimization-based meta-learning (Finn et al., 2017) to improve generalization\nto low-resource domains. This approach significantly outperforms all baseline\nmethods in the experiments on a newly collected multi-domain task-oriented\nsemantic parsing dataset (TOPv2), which we release to the public.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:47:53 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Chen", "Xilun", ""], ["Ghoshal", "Asish", ""], ["Mehdad", "Yashar", ""], ["Zettlemoyer", "Luke", ""], ["Gupta", "Sonal", ""]]}, {"id": "2010.03548", "submitter": "Rajarshi Das", "authors": "Rajarshi Das, Ameya Godbole, Nicholas Monath, Manzil Zaheer, Andrew\n  McCallum", "title": "Probabilistic Case-based Reasoning for Open-World Knowledge Graph\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A case-based reasoning (CBR) system solves a new problem by retrieving\n`cases' that are similar to the given problem. If such a system can achieve\nhigh accuracy, it is appealing owing to its simplicity, interpretability, and\nscalability. In this paper, we demonstrate that such a system is achievable for\nreasoning in knowledge-bases (KBs). Our approach predicts attributes for an\nentity by gathering reasoning paths from similar entities in the KB. Our\nprobabilistic model estimates the likelihood that a path is effective at\nanswering a query about the given entity. The parameters of our model can be\nefficiently computed using simple path statistics and require no iterative\noptimization. Our model is non-parametric, growing dynamically as new entities\nand relations are added to the KB. On several benchmark datasets our approach\nsignificantly outperforms other rule learning approaches and performs\ncomparably to state-of-the-art embedding-based approaches. Furthermore, we\ndemonstrate the effectiveness of our model in an \"open-world\" setting where new\nentities arrive in an online fashion, significantly outperforming\nstate-of-the-art approaches and nearly matching the best offline method. Code\navailable at https://github.com/ameyagodbole/Prob-CBR\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:48:12 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 14:44:18 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Das", "Rajarshi", ""], ["Godbole", "Ameya", ""], ["Monath", "Nicholas", ""], ["Zaheer", "Manzil", ""], ["McCallum", "Andrew", ""]]}, {"id": "2010.03550", "submitter": "Benjamin Nye", "authors": "Benjamin E. Nye, Jay DeYoung, Eric Lehman, Ani Nenkova, Iain J.\n  Marshall, Byron C. Wallace", "title": "Understanding Clinical Trial Reports: Extracting Medical Entities and\n  Their Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best evidence concerning comparative treatment effectiveness comes from\nclinical trials, the results of which are reported in unstructured articles.\nMedical experts must manually extract information from articles to inform\ndecision-making, which is time-consuming and expensive. Here we consider the\nend-to-end task of both (a) extracting treatments and outcomes from full-text\narticles describing clinical trials (entity identification) and, (b) inferring\nthe reported results for the former with respect to the latter (relation\nextraction). We introduce new data for this task, and evaluate models that have\nrecently achieved state-of-the-art results on similar tasks in Natural Language\nProcessing. We then propose a new method motivated by how trial results are\ntypically presented that outperforms these purely data-driven baselines.\nFinally, we run a fielded evaluation of the model with a non-profit seeking to\nidentify existing drugs that might be re-purposed for cancer, showing the\npotential utility of end-to-end evidence extraction systems.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:50:58 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 15:19:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Nye", "Benjamin E.", ""], ["DeYoung", "Jay", ""], ["Lehman", "Eric", ""], ["Nenkova", "Ani", ""], ["Marshall", "Iain J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2010.03574", "submitter": "Chao-Chun Hsu", "authors": "Chao-Chun Hsu, Shantanu Karnwal, Sendhil Mullainathan, Ziad Obermeyer,\n  Chenhao Tan", "title": "Characterizing the Value of Information in Medical Notes", "comments": "15 pages, 12 figures, Findings of EMNLP 2020, code is available at\n  https://github.com/BoulderDS/value-of-medical-notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models depend on the quality of input data. As electronic\nhealth records are widely adopted, the amount of data in health care is\ngrowing, along with complaints about the quality of medical notes. We use two\nprediction tasks, readmission prediction and in-hospital mortality prediction,\nto characterize the value of information in medical notes. We show that as a\nwhole, medical notes only provide additional predictive power over structured\ninformation in readmission prediction. We further propose a probing framework\nto select parts of notes that enable more accurate predictions than using all\nnotes, despite that the selected information leads to a distribution shift from\nthe training data (\"all notes\"). Finally, we demonstrate that models trained on\nthe selected valuable information achieve even better predictive performance,\nwith only 6.8% of all the tokens for readmission prediction.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:00:03 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 17:04:27 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Hsu", "Chao-Chun", ""], ["Karnwal", "Shantanu", ""], ["Mullainathan", "Sendhil", ""], ["Obermeyer", "Ziad", ""], ["Tan", "Chenhao", ""]]}, {"id": "2010.03604", "submitter": "Chen Zheng", "authors": "Chen Zheng, Parisa Kordjamshidi", "title": "SRLGRN: Semantic Role Labeling Graph Reasoning Network", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the challenge of learning and reasoning over multi-hop\nquestion answering (QA). We propose a graph reasoning network based on the\nsemantic structure of the sentences to learn cross paragraph reasoning paths\nand find the supporting facts and the answer jointly. The proposed graph is a\nheterogeneous document-level graph that contains nodes of type sentence\n(question, title, and other sentences), and semantic role labeling sub-graphs\nper sentence that contain arguments as nodes and predicates as edges.\nIncorporating the argument types, the argument phrases, and the semantics of\nthe edges originated from SRL predicates into the graph encoder helps in\nfinding and also the explainability of the reasoning paths. Our proposed\napproach shows competitive performance on the HotpotQA distractor setting\nbenchmark compared to the recent state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:51:17 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 15:30:08 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zheng", "Chen", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2010.03614", "submitter": "Radu Tudor Ionescu", "authors": "Mihaela Gaman, Radu Tudor Ionescu", "title": "Combining Deep Learning and String Kernels for the Localization of Swiss\n  German Tweets", "comments": "Accepted at VarDial 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the methods proposed by the UnibucKernel team in\nsolving the Social Media Variety Geolocation task featured in the 2020 VarDial\nEvaluation Campaign. We address only the second subtask, which targets a data\nset composed of nearly 30 thousand Swiss German Jodels. The dialect\nidentification task is about accurately predicting the latitude and longitude\nof test samples. We frame the task as a double regression problem, employing a\nvariety of machine learning approaches to predict both latitude and longitude.\nFrom simple models for regression, such as Support Vector Regression, to deep\nneural networks, such as Long Short-Term Memory networks and character-level\nconvolutional neural networks, and, finally, to ensemble models based on\nmeta-learners, such as XGBoost, our interest is focused on approaching the\nproblem from a few different perspectives, in an attempt to minimize the\nprediction error. With the same goal in mind, we also considered many types of\nfeatures, from high-level features, such as BERT embeddings, to low-level\nfeatures, such as characters n-grams, which are known to provide good results\nin dialect identification. Our empirical results indicate that the handcrafted\nmodel based on string kernels outperforms the deep learning approaches.\nNevertheless, our best performance is given by the ensemble model that combines\nboth handcrafted and deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 19:16:45 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Gaman", "Mihaela", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2010.03617", "submitter": "Markus Leippold", "authors": "Rahul Mishra and Piyush Yadav and Remi Calizzano and Markus Leippold", "title": "MuSeM: Detecting Incongruent News Headlines using Mutual Attentive\n  Semantic Matching", "comments": "Accepted paper; IEEE 2020 International Conference on Machine\n  Learning and Applications (ICMLA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the congruence between two texts has several useful applications,\nsuch as detecting the prevalent deceptive and misleading news headlines on the\nweb. Many works have proposed machine learning based solutions such as text\nsimilarity between the headline and body text to detect the incongruence. Text\nsimilarity based methods fail to perform well due to different inherent\nchallenges such as relative length mismatch between the news headline and its\nbody content and non-overlapping vocabulary. On the other hand, more recent\nworks that use headline guided attention to learn a headline derived contextual\nrepresentation of the news body also result in convoluting overall\nrepresentation due to the news body's lengthiness. This paper proposes a method\nthat uses inter-mutual attention-based semantic matching between the original\nand synthetically generated headlines, which utilizes the difference between\nall pairs of word embeddings of words involved. The paper also investigates two\nmore variations of our method, which use concatenation and dot-products of word\nembeddings of the words of original and synthetic headlines. We observe that\nthe proposed method outperforms prior arts significantly for two publicly\navailable datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 19:19:42 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Mishra", "Rahul", ""], ["Yadav", "Piyush", ""], ["Calizzano", "Remi", ""], ["Leippold", "Markus", ""]]}, {"id": "2010.03623", "submitter": "Dominika Woszczyk", "authors": "Dominika Woszczyk, Stavros Petridis, David Millard", "title": "Domain Adversarial Neural Networks for Dysarthric Speech Recognition", "comments": "5 pages, to be published in Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition systems have improved dramatically over the last few\nyears, however, their performance is significantly degraded for the cases of\naccented or impaired speech. This work explores domain adversarial neural\nnetworks (DANN) for speaker-independent speech recognition on the UAS dataset\nof dysarthric speech. The classification task on 10 spoken digits is performed\nusing an end-to-end CNN taking raw audio as input. The results are compared to\na speaker-adaptive (SA) model as well as speaker-dependent (SD) and multi-task\nlearning models (MTL). The experiments conducted in this paper show that DANN\nachieves an absolute recognition rate of 74.91% and outperforms the baseline by\n12.18%. Additionally, the DANN model achieves comparable results to the SA\nmodel's recognition rate of 77.65%. We also observe that when labelled\ndysarthric speech data is available DANN and MTL perform similarly, but when\nthey are not DANN performs better than MTL.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 19:51:41 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Woszczyk", "Dominika", ""], ["Petridis", "Stavros", ""], ["Millard", "David", ""]]}, {"id": "2010.03636", "submitter": "Anthony Chen", "authors": "Anthony Chen, Gabriel Stanovsky, Sameer Singh and Matt Gardner", "title": "MOCHA: A Dataset for Training and Evaluating Generative Reading\n  Comprehension Metrics", "comments": null, "journal-ref": "Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP)", "doi": "10.18653/v1/2020.emnlp-main.528", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posing reading comprehension as a generation problem provides a great deal of\nflexibility, allowing for open-ended questions with few restrictions on\npossible answers. However, progress is impeded by existing generation metrics,\nwhich rely on token overlap and are agnostic to the nuances of reading\ncomprehension. To address this, we introduce a benchmark for training and\nevaluating generative reading comprehension metrics: MOdeling Correctness with\nHuman Annotations. MOCHA contains 40K human judgement scores on model outputs\nfrom 6 diverse question answering datasets and an additional set of minimal\npairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for\nReading Comprehension, LERC, to mimic human judgement scores. LERC outperforms\nbaseline metrics by 10 to 36 absolute Pearson points on held-out annotations.\nWhen we evaluate robustness on minimal pairs, LERC achieves 80% accuracy,\noutperforming baselines by 14 to 26 absolute percentage points while leaving\nsignificant room for improvement. MOCHA presents a challenging problem for\ndeveloping accurate and robust generative reading comprehension metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:22:54 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 18:23:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Anthony", ""], ["Stanovsky", "Gabriel", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""]]}, {"id": "2010.03640", "submitter": "Emily Allaway", "authors": "Emily Allaway and Kathleen McKeown", "title": "Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic\n  Representations", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance detection is an important component of understanding hidden influences\nin everyday life. Since there are thousands of potential topics to take a\nstance on, most with little to no training data, we focus on zero-shot stance\ndetection: classifying stance from no training examples. In this paper, we\npresent a new dataset for zero-shot stance detection that captures a wider\nrange of topics and lexical variation than in previous datasets. Additionally,\nwe propose a new model for stance detection that implicitly captures\nrelationships between topics using generalized topic representations and show\nthat this model improves performance on a number of challenging linguistic\nphenomena.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:27:12 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Allaway", "Emily", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2010.03644", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Xin Eric Wang, Pradyumna Narayana, Kazoo Sone, Sugato\n  Basu, William Yang Wang", "title": "Towards Understanding Sample Variance in Visually Grounded Language\n  Generation: Evaluations and Observations", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in visually grounded language generation is to build robust\nbenchmark datasets and models that can generalize well in real-world settings.\nTo do this, it is critical to ensure that our evaluation protocols are correct,\nand benchmarks are reliable. In this work, we set forth to design a set of\nexperiments to understand an important but often ignored problem in visually\ngrounded language generation: given that humans have different utilities and\nvisual attention, how will the sample variance in multi-reference datasets\naffect the models' performance? Empirically, we study several multi-reference\ndatasets and corresponding vision-and-language tasks. We show that it is of\nparamount importance to report variance in experiments; that human-generated\nreferences could vary drastically in different datasets/tasks, revealing the\nnature of each task; that metric-wise, CIDEr has shown systematically larger\nvariances than others. Our evaluations on reference-per-instance shed light on\nthe design of reliable datasets in the future.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:45:14 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Zhu", "Wanrong", ""], ["Wang", "Xin Eric", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""], ["Wang", "William Yang", ""]]}, {"id": "2010.03648", "submitter": "Nikunj Saunshi", "authors": "Nikunj Saunshi, Sadhika Malladi, Sanjeev Arora", "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream\n  Tasks", "comments": "This version is the camera-ready version for ICLR 2021. Main changes\n  include a detailed discussion about natural tasks, more detailed proof sketch\n  and updated experimental evaluations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive language models, pretrained using large text corpora to do\nwell on next word prediction, have been successful at solving many downstream\ntasks, even with zero-shot usage. However, there is little theoretical\nunderstanding of this success. This paper initiates a mathematical study of\nthis phenomenon for the downstream task of text classification by considering\nthe following questions: (1) What is the intuitive connection between the\npretraining task of next word prediction and text classification? (2) How can\nwe mathematically formalize this connection and quantify the benefit of\nlanguage modeling? For (1), we hypothesize, and verify empirically, that\nclassification tasks of interest can be reformulated as sentence completion\ntasks, thus making language modeling a meaningful pretraining task. With a\nmathematical formalization of this hypothesis, we make progress towards (2) and\nshow that language models that are $\\epsilon$-optimal in cross-entropy\n(log-perplexity) learn features that can linearly solve such classification\ntasks with $\\mathcal{O}(\\sqrt{\\epsilon})$ error, thus demonstrating that doing\nwell on language modeling can be beneficial for downstream tasks. We\nexperimentally verify various assumptions and theoretical findings, and also\nuse insights from the analysis to design a new objective function that performs\nwell on some classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:56:40 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 17:59:14 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Saunshi", "Nikunj", ""], ["Malladi", "Sadhika", ""], ["Arora", "Sanjeev", ""]]}, {"id": "2010.03652", "submitter": "Shuohang Wang", "authors": "Shuohang Wang, Yuwei Fang, Siqi Sun, Zhe Gan, Yu Cheng, Jing Jiang,\n  Jingjing Liu", "title": "Cross-Thought for Sentence Encoder Pre-training", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Cross-Thought, a novel approach to pre-training\nsequence encoder, which is instrumental in building reusable sequence\nembeddings for large-scale NLP tasks such as question answering. Instead of\nusing the original signals of full sentences, we train a Transformer-based\nsequence encoder over a large set of short sequences, which allows the model to\nautomatically select the most useful information for predicting masked words.\nExperiments on question answering and textual entailment tasks demonstrate that\nour pre-trained encoder can outperform state-of-the-art encoders trained with\ncontinuous sentence signals as well as traditional masked language modeling\nbaselines. Our proposed approach also achieves new state of the art on HotpotQA\n(full-wiki setting) by improving intermediate information retrieval\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:02:41 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wang", "Shuohang", ""], ["Fang", "Yuwei", ""], ["Sun", "Siqi", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Jiang", "Jing", ""], ["Liu", "Jingjing", ""]]}, {"id": "2010.03656", "submitter": "Shachar Rosenman", "authors": "Shachar Rosenman, Alon Jacovi, Yoav Goldberg", "title": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge\n  Data", "comments": "Accepted as a short paper in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of collecting and annotating training data may introduce\ndistribution artifacts which may limit the ability of models to learn correct\ngeneralization behavior. We identify failure modes of SOTA relation extraction\n(RE) models trained on TACRED, which we attribute to limitations in the data\nannotation process. We collect and annotate a challenge-set we call Challenging\nRE (CRE), based on naturally occurring corpus examples, to benchmark this\nbehavior. Our experiments with four state-of-the-art RE models show that they\nhave indeed adopted shallow heuristics that do not generalize to the\nchallenge-set data. Further, we find that alternative question answering\nmodeling performs significantly better than the SOTA models on the\nchallenge-set, despite worse overall TACRED performance. By adding some of the\nchallenge data as training examples, the performance of the model improves.\nFinally, we provide concrete suggestion on how to improve RE data collection to\nalleviate this behavior.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:17:25 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Rosenman", "Shachar", ""], ["Jacovi", "Alon", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2010.03662", "submitter": "Eleftheria Briakou", "authors": "Eleftheria Briakou and Marine Carpuat", "title": "Detecting Fine-Grained Cross-Lingual Semantic Divergences without\n  Supervision by Learning to Rank", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting fine-grained differences in content conveyed in different languages\nmatters for cross-lingual NLP and multilingual corpora analysis, but it is a\nchallenging machine learning problem since annotation is expensive and hard to\nscale. This work improves the prediction and annotation of fine-grained\nsemantic divergences. We introduce a training strategy for multilingual BERT\nmodels by learning to rank synthetic divergent examples of varying granularity.\nWe evaluate our models on the Rationalized English-French Semantic Divergences,\na new dataset released with this work, consisting of English-French\nsentence-pairs annotated with semantic divergence classes and token-level\nrationales. Learning to rank helps detect fine-grained sentence-level\ndivergences more accurately than a strong sentence-level similarity model,\nwhile token-level predictions have the potential of further distinguishing\nbetween coarse and fine-grained divergences.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:26:20 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Briakou", "Eleftheria", ""], ["Carpuat", "Marine", ""]]}, {"id": "2010.03680", "submitter": "Yaqing Wang", "authors": "Yaqing Wang, Subhabrata Mukherjee, Haoda Chu, Yuancheng Tu, Ming Wu,\n  Jing Gao, Ahmed Hassan Awadallah", "title": "Adaptive Self-training for Few-shot Neural Sequence Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence labeling is an important technique employed for many Natural\nLanguage Processing (NLP) tasks, such as Named Entity Recognition (NER), slot\ntagging for dialog systems and semantic parsing. Large-scale pre-trained\nlanguage models obtain very good performance on these tasks when fine-tuned on\nlarge amounts of task-specific labeled data. However, such large-scale labeled\ndatasets are difficult to obtain for several tasks and domains due to the high\ncost of human annotation as well as privacy and data access constraints for\nsensitive user applications. This is exacerbated for sequence labeling tasks\nrequiring such annotations at token-level. In this work, we develop techniques\nto address the label scarcity challenge for neural sequence labeling models.\nSpecifically, we develop self-training and meta-learning techniques for\ntraining neural sequence taggers with few labels. While self-training serves as\nan effective mechanism to learn from large amounts of unlabeled data --\nmeta-learning helps in adaptive sample re-weighting to mitigate error\npropagation from noisy pseudo-labels. Extensive experiments on six benchmark\ndatasets including two for massive multilingual NER and four slot tagging\ndatasets for task-oriented dialog systems demonstrate the effectiveness of our\nmethod. With only 10 labeled examples for each class for each task, our method\nobtains 10% improvement over state-of-the-art systems demonstrating its\neffectiveness for the low-resource setting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 22:29:05 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 17:16:57 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wang", "Yaqing", ""], ["Mukherjee", "Subhabrata", ""], ["Chu", "Haoda", ""], ["Tu", "Yuancheng", ""], ["Wu", "Ming", ""], ["Gao", "Jing", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2010.03688", "submitter": "Amrit Nagarajan", "authors": "Amrit Nagarajan, Sanchari Sen, Jacob R. Stevens, Anand Raghunathan", "title": "Optimizing Transformers with Approximate Computing for Faster, Smaller\n  and more Accurate NLP Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have garnered a lot of interest in recent years by\ndelivering state-of-the-art performance in a range of Natural Language\nProcessing (NLP) tasks. However, these models can have over a hundred billion\nparameters, presenting very high computational and memory requirements. We\naddress this challenge through Approximate Computing, specifically targeting\nthe use of Transformers in NLP tasks. Transformers are typically pre-trained\nand subsequently specialized for specific tasks through transfer learning.\nBased on the observation that pre-trained Transformers are often\nover-parameterized for several downstream NLP tasks, we propose a framework to\ncreate smaller, faster and in some cases more accurate models. The key\ncornerstones of the framework are a Significance Analysis (SA) method that\nidentifies components in a pre-trained Transformer that are less significant\nfor a given task, and techniques to approximate the less significant\ncomponents. Our approximations include pruning of blocks, attention heads and\nweight groups, quantization of less significant weights and a low-complexity\nsign-matching based attention mechanism. Our framework can be adapted to\nproduce models that are faster, smaller and/or more accurate, depending on the\nuser's constraints. We apply our framework to seven Transformer models,\nincluding optimized models like DistilBERT and Q8BERT, and three downstream\ntasks. We demonstrate that our framework produces models that are up to 4x\nfaster and up to 14x smaller (with less than 0.5% relative accuracy\ndegradation), or up to 5.5% more accurate with simultaneous improvements of up\nto 9.83x in model size or 2.94x in speed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 23:29:34 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Nagarajan", "Amrit", ""], ["Sen", "Sanchari", ""], ["Stevens", "Jacob R.", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2010.03706", "submitter": "Ekin Aky\\\"urek", "authors": "Ekin Aky\\\"urek, Afra Feyza Aky\\\"urek, Jacob Andreas", "title": "Learning to Recombine and Resample Data for Compositional Generalization", "comments": "ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible neural sequence models outperform grammar- and automaton-based\ncounterparts on a variety of tasks. However, neural models perform poorly in\nsettings requiring compositional generalization beyond the training data --\nparticularly to rare or unseen subsequences. Past work has found symbolic\nscaffolding (e.g. grammars or automata) essential in these settings. We\ndescribe R&R, a learned data augmentation scheme that enables a large category\nof compositional generalizations without appeal to latent symbolic structure.\nR&R has two components: recombination of original training examples via a\nprototype-based generative model and resampling of generated examples to\nencourage extrapolation. Training an ordinary neural sequence model on a\ndataset augmented with recombined and resampled examples significantly improves\ngeneralization in two language processing problems -- instruction following\n(SCAN) and morphological analysis (SIGMORPHON 2018) -- where R&R enables\nlearning of new constructions and tenses from as few as eight initial examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 00:36:33 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:56:39 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 17:38:19 GMT"}, {"version": "v4", "created": "Sun, 21 Mar 2021 15:43:11 GMT"}, {"version": "v5", "created": "Tue, 27 Apr 2021 04:39:47 GMT"}, {"version": "v6", "created": "Tue, 8 Jun 2021 00:43:04 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Aky\u00fcrek", "Ekin", ""], ["Aky\u00fcrek", "Afra Feyza", ""], ["Andreas", "Jacob", ""]]}, {"id": "2010.03714", "submitter": "Haidar Khan", "authors": "Qile Zhu, Haidar Khan, Saleh Soltan, Stephen Rawls, Wael Hamza", "title": "Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based\n  Decoding", "comments": "Presented at CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing is one of the key components of natural language\nunderstanding systems. A successful parse transforms an input utterance to an\naction that is easily understood by the system. Many algorithms have been\nproposed to solve this problem, from conventional rulebased or statistical\nslot-filling systems to shiftreduce based neural parsers. For complex parsing\ntasks, the state-of-the-art method is based on autoregressive sequence to\nsequence models to generate the parse directly. This model is slow at inference\ntime, generating parses in O(n) decoding steps (n is the length of the target\nsequence). In addition, we demonstrate that this method performs poorly in\nzero-shot cross-lingual transfer learning settings. In this paper, we propose a\nnon-autoregressive parser which is based on the insertion transformer to\novercome these two issues. Our approach 1) speeds up decoding by 3x while\noutperforming the autoregressive model and 2) significantly improves\ncross-lingual transfer in the low-resource setting by 37% compared to\nautoregressive baseline. We test our approach on three well-known monolingual\ndatasets: ATIS, SNIPS and TOP. For cross lingual semantic parsing, we use the\nMultiATIS++ and the multilingual TOP datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 01:18:42 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Zhu", "Qile", ""], ["Khan", "Haidar", ""], ["Soltan", "Saleh", ""], ["Rawls", "Stephen", ""], ["Hamza", "Wael", ""]]}, {"id": "2010.03717", "submitter": "Hieu-Thi Luong", "authors": "Hieu-Thi Luong, Junichi Yamagishi", "title": "Latent linguistic embedding for cross-lingual text-to-speech and voice\n  conversion", "comments": "Accepted to Voice Conversion Challenge 2020 Online Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the recently proposed voice cloning system, NAUTILUS, is capable of\ncloning unseen voices using untranscribed speech, we investigate the\nfeasibility of using it to develop a unified cross-lingual TTS/VC system.\nCross-lingual speech generation is the scenario in which speech utterances are\ngenerated with the voices of target speakers in a language not spoken by them\noriginally. This type of system is not simply cloning the voice of the target\nspeaker, but essentially creating a new voice that can be considered better\nthan the original under a specific framing. By using a well-trained English\nlatent linguistic embedding to create a cross-lingual TTS and VC system for\nseveral German, Finnish, and Mandarin speakers included in the Voice Conversion\nChallenge 2020, we show that our method not only creates cross-lingual VC with\nhigh speaker similarity but also can be seamlessly used for cross-lingual TTS\nwithout having to perform any extra steps. However, the subjective evaluations\nof perceived naturalness seemed to vary between target speakers, which is one\naspect for future improvement.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 01:25:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Luong", "Hieu-Thi", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "2010.03722", "submitter": "Logan Lebanoff", "authors": "Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Walter Chang, Fei\n  Liu", "title": "A Cascade Approach to Neural Abstractive Summarization with Content\n  Selection and Fusion", "comments": "AACL-IJCNLP 2020 (Short Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an empirical study in favor of a cascade architecture to neural\ntext summarization. Summarization practices vary widely but few other than news\nsummarization can provide a sufficient amount of training data enough to meet\nthe requirement of end-to-end neural abstractive systems which perform content\nselection and surface realization jointly to generate abstracts. Such systems\nalso pose a challenge to summarization evaluation, as they force content\nselection to be evaluated along with text generation, yet evaluation of the\nlatter remains an unsolved problem. In this paper, we present empirical results\nshowing that the performance of a cascaded pipeline that separately identifies\nimportant content pieces and stitches them together into a coherent text is\ncomparable to or outranks that of end-to-end systems, whereas a pipeline\narchitecture allows for flexible content selection. We finally discuss how we\ncan take advantage of a cascaded pipeline in neural text summarization and shed\nlight on important directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 01:49:16 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lebanoff", "Logan", ""], ["Dernoncourt", "Franck", ""], ["Kim", "Doo Soon", ""], ["Chang", "Walter", ""], ["Liu", "Fei", ""]]}, {"id": "2010.03725", "submitter": "Yun He", "authors": "Yun He, Zhuoer Wang, Yin Zhang, Ruihong Huang and James Caverlee", "title": "PARADE: A New Dataset for Paraphrase Identification Requiring Computer\n  Science Domain Knowledge", "comments": "Accepted by EMNLP 2020 as a regular long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new benchmark dataset called PARADE for paraphrase\nidentification that requires specialized domain knowledge. PARADE contains\nparaphrases that overlap very little at the lexical and syntactic level but are\nsemantically equivalent based on computer science domain knowledge, as well as\nnon-paraphrases that overlap greatly at the lexical and syntactic level but are\nnot semantically equivalent based on this domain knowledge. Experiments show\nthat both state-of-the-art neural models and non-expert human annotators have\npoor performance on PARADE. For example, BERT after fine-tuning achieves an F1\nscore of 0.709, which is much lower than its performance on other paraphrase\nidentification datasets. PARADE can serve as a resource for researchers\ninterested in testing models that incorporate domain knowledge. We make our\ndata and code freely available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:01:31 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["He", "Yun", ""], ["Wang", "Zhuoer", ""], ["Zhang", "Yin", ""], ["Huang", "Ruihong", ""], ["Caverlee", "James", ""]]}, {"id": "2010.03726", "submitter": "Logan Lebanoff", "authors": "Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter\n  Chang, Fei Liu", "title": "Learning to Fuse Sentences with Transformers for Summarization", "comments": "EMNLP 2020 (Short Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to fuse sentences is highly attractive for summarization systems\nbecause it is an essential step to produce succinct abstracts. However, to\ndate, summarizers can fail on fusing sentences. They tend to produce few\nsummary sentences by fusion or generate incorrect fusions that lead the summary\nto fail to retain the original meaning. In this paper, we explore the ability\nof Transformers to fuse sentences and propose novel algorithms to enhance their\nability to perform sentence fusion by leveraging the knowledge of points of\ncorrespondence between sentences. Through extensive experiments, we investigate\nthe effects of different design choices on Transformer's performance. Our\nfindings highlight the importance of modeling points of correspondence between\nsentences for effective sentence fusion.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:01:35 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lebanoff", "Logan", ""], ["Dernoncourt", "Franck", ""], ["Kim", "Doo Soon", ""], ["Wang", "Lidan", ""], ["Chang", "Walter", ""], ["Liu", "Fei", ""]]}, {"id": "2010.03732", "submitter": "Inigo Jauregi Unanue", "authors": "Inigo Jauregi Unanue, Nazanin Esmaili, Gholamreza Haffari, Massimo\n  Piccardi", "title": "Leveraging Discourse Rewards for Document-Level Neural Machine\n  Translation", "comments": "Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level machine translation focuses on the translation of entire\ndocuments from a source to a target language. It is widely regarded as a\nchallenging task since the translation of the individual sentences in the\ndocument needs to retain aspects of the discourse at document level. However,\ndocument-level translation models are usually not trained to explicitly ensure\ndiscourse quality. Therefore, in this paper we propose a training approach that\nexplicitly optimizes two established discourse metrics, lexical cohesion (LC)\nand coherence (COH), by using a reinforcement learning objective. Experiments\nover four different language pairs and three translation domains have shown\nthat our training approach has been able to achieve more cohesive and coherent\ndocument translations than other competitive approaches, yet without\ncompromising the faithfulness to the reference translation. In the case of the\nZh-En language pair, our method has achieved an improvement of 2.46 percentage\npoints (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time\nimproving 0.63 pp in BLEU score and 0.47 pp in F_BERT.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:26:22 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 23:33:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Unanue", "Inigo Jauregi", ""], ["Esmaili", "Nazanin", ""], ["Haffari", "Gholamreza", ""], ["Piccardi", "Massimo", ""]]}, {"id": "2010.03737", "submitter": "Li Bei", "authors": "Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen\n  Wang and Jingbo Zhu", "title": "Shallow-to-Deep Training for Neural Machine Translation", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep encoders have been proven to be effective in improving neural machine\ntranslation (NMT) systems, but training an extremely deep encoder is time\nconsuming. Moreover, why deep models help NMT is an open question. In this\npaper, we investigate the behavior of a well-tuned deep Transformer system. We\nfind that stacking layers is helpful in improving the representation ability of\nNMT models and adjacent layers perform similarly. This inspires us to develop a\nshallow-to-deep training method that learns deep models by stacking shallow\nmodels. In this way, we successfully train a Transformer system with a 54-layer\nencoder. Experimental results on WMT'16 English-German and WMT'14\nEnglish-French translation tasks show that it is $1.4$ $\\times$ faster than\ntraining from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two\ntasks. The code is publicly available at\nhttps://github.com/libeineu/SDT-Training/.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:36:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Li", "Bei", ""], ["Wang", "Ziyang", ""], ["Liu", "Hui", ""], ["Jiang", "Yufan", ""], ["Du", "Quan", ""], ["Xiao", "Tong", ""], ["Wang", "Huizhen", ""], ["Zhu", "Jingbo", ""]]}, {"id": "2010.03738", "submitter": "Yang Deng", "authors": "Yang Deng, Wenxuan Zhang, Wai Lam", "title": "Multi-hop Inference for Question-driven Summarization", "comments": "Accepted by EMNLP 2020 (main conference, long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question-driven summarization has been recently studied as an effective\napproach to summarizing the source document to produce concise but informative\nanswers for non-factoid questions. In this work, we propose a novel\nquestion-driven abstractive summarization method, Multi-hop Selective Generator\n(MSG), to incorporate multi-hop reasoning into question-driven summarization\nand, meanwhile, provide justifications for the generated summaries.\nSpecifically, we jointly model the relevance to the question and the\ninterrelation among different sentences via a human-like multi-hop inference\nmodule, which captures important sentences for justifying the summarized\nanswer. A gated selective pointer generator network with a multi-view coverage\nmechanism is designed to integrate diverse information from different\nperspectives. Experimental results show that the proposed method consistently\noutperforms state-of-the-art methods on two non-factoid QA datasets, namely\nWikiHow and PubMedQA.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:36:39 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Deng", "Yang", ""], ["Zhang", "Wenxuan", ""], ["Lam", "Wai", ""]]}, {"id": "2010.03746", "submitter": "Yun He", "authors": "Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, James Caverlee", "title": "Infusing Disease Knowledge into BERT for Health Question Answering,\n  Medical Inference and Disease Name Recognition", "comments": "Accepted by EMNLP 2020 as a regular long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of a disease includes information of various aspects of the\ndisease, such as signs and symptoms, diagnosis and treatment. This disease\nknowledge is critical for many health-related and biomedical tasks, including\nconsumer health question answering, medical language inference and disease name\nrecognition. While pre-trained language models like BERT have shown success in\ncapturing syntactic, semantic, and world knowledge from text, we find they can\nbe further complemented by specific information like knowledge of symptoms,\ndiagnoses, treatments, and other disease aspects. Hence, we integrate BERT with\ndisease knowledge for improving these important tasks. Specifically, we propose\na new disease knowledge infusion training procedure and evaluate it on a suite\nof BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and\nALBERT. Experiments over the three tasks show that these models can be enhanced\nin nearly all cases, demonstrating the viability of disease knowledge infusion.\nFor example, accuracy of BioBERT on consumer health question answering is\nimproved from 68.29% to 72.09%, while new SOTA results are observed in two\ndatasets. We make our data and code freely available.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 03:14:38 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["He", "Yun", ""], ["Zhu", "Ziwei", ""], ["Zhang", "Yin", ""], ["Chen", "Qin", ""], ["Caverlee", "James", ""]]}, {"id": "2010.03755", "submitter": "Xinting Huang", "authors": "Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang", "title": "Generalizable and Explainable Dialogue Generation via Explicit Action\n  Learning", "comments": "Accepted to Proceedings of EMNLP 2020 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response generation for task-oriented dialogues implicitly optimizes two\nobjectives at the same time: task completion and language quality. Conditioned\nresponse generation serves as an effective approach to separately and better\noptimize these two objectives. Such an approach relies on system action\nannotations which are expensive to obtain. To alleviate the need of action\nannotations, latent action learning is introduced to map each utterance to a\nlatent representation. However, this approach is prone to over-dependence on\nthe training data, and the generalization capability is thus restricted. To\naddress this issue, we propose to learn natural language actions that represent\nutterances as a span of words. This explicit action representation promotes\ngeneralization via the compositional structure of language. It also enables an\nexplainable generation process. Our proposed unsupervised approach learns a\nmemory component to summarize system utterances into a short span of words. To\nfurther promote a compact action representation, we propose an auxiliary task\nthat restores state annotations as the summarized dialogue context using the\nmemory component. Our proposed approach outperforms latent action baselines on\nMultiWOZ, a benchmark multi-domain dataset.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 04:37:22 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Huang", "Xinting", ""], ["Qi", "Jianzhong", ""], ["Sun", "Yu", ""], ["Zhang", "Rui", ""]]}, {"id": "2010.03760", "submitter": "Xiaoan Ding", "authors": "Xiaoan Ding, Tianyu Liu, Baobao Chang, Zhifang Sui, Kevin Gimpel", "title": "Discriminatively-Tuned Generative Classifiers for Robust Natural\n  Language Inference", "comments": "14 pages, EMNLP 2020, the first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While discriminative neural network classifiers are generally preferred,\nrecent work has shown advantages of generative classifiers in term of data\nefficiency and robustness. In this paper, we focus on natural language\ninference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and\nempirically characterize its performance by comparing it to five baselines,\nincluding discriminative models and large-scale pretrained language\nrepresentation models like BERT. We explore training objectives for\ndiscriminative fine-tuning of our generative classifiers, showing improvements\nover log loss fine-tuning from prior work . In particular, we find strong\nresults with a simple unbounded modification to log loss, which we call the\n\"infinilog loss\". Our experiments show that GenNLI outperforms both\ndiscriminative and pretrained baselines across several challenging NLI\nexperimental settings, including small training sets, imbalanced label\ndistributions, and label noise.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 04:44:00 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ding", "Xiaoan", ""], ["Liu", "Tianyu", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2010.03763", "submitter": "Lang Yu", "authors": "Lang Yu and Allyson Ettinger", "title": "Assessing Phrasal Representation and Composition in Transformers", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep transformer models have pushed performance on NLP tasks to new limits,\nsuggesting sophisticated treatment of complex linguistic inputs, such as\nphrases. However, we have limited understanding of how these models handle\nrepresentation of phrases, and whether this reflects sophisticated composition\nof phrase meaning like that done by humans. In this paper, we present\nsystematic analysis of phrasal representations in state-of-the-art pre-trained\ntransformers. We use tests leveraging human judgments of phrase similarity and\nmeaning shift, and compare results before and after control of word overlap, to\ntease apart lexical effects versus composition effects. We find that phrase\nrepresentation in these models relies heavily on word content, with little\nevidence of nuanced composition. We also identify variations in phrase\nrepresentation quality across models, layers, and representation types, and\nmake corresponding recommendations for usage of representations from these\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 04:59:39 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:25:52 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Yu", "Lang", ""], ["Ettinger", "Allyson", ""]]}, {"id": "2010.03766", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang", "title": "Improving Attention Mechanism with Query-Value Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism has played critical roles in various state-of-the-art NLP\nmodels such as Transformer and BERT. It can be formulated as a ternary function\nthat maps the input queries, keys and values into an output by using a\nsummation of values weighted by the attention weights derived from the\ninteractions between queries and keys. Similar with query-key interactions,\nthere is also inherent relatedness between queries and values, and\nincorporating query-value interactions has the potential to enhance the output\nby learning customized values according to the characteristics of queries.\nHowever, the query-value interactions are ignored by existing attention\nmethods, which may be not optimal. In this paper, we propose to improve the\nexisting attention mechanism by incorporating query-value interactions. We\npropose a query-value interaction function which can learn query-aware\nattention values, and combine them with the original values and attention\nweights to form the final output. Extensive experiments on four datasets for\ndifferent tasks show that our approach can consistently improve the performance\nof many attention-based models by incorporating query-value interactions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:12:52 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2010.03768", "submitter": "Mohit Shridhar", "authors": "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\\^ot\\'e, Yonatan Bisk,\n  Adam Trischler, Matthew Hausknecht", "title": "ALFWorld: Aligning Text and Embodied Environments for Interactive\n  Learning", "comments": "ICLR 2021; Data, code, and videos are available at alfworld.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simple request like Put a washed apple in the kitchen fridge, humans\ncan reason in purely abstract terms by imagining action sequences and scoring\ntheir likelihood of success, prototypicality, and efficiency, all without\nmoving a muscle. Once we see the kitchen in question, we can update our\nabstract plans to fit the scene. Embodied agents require the same abilities,\nbut existing work does not yet provide the infrastructure necessary for both\nreasoning abstractly and executing concretely. We address this limitation by\nintroducing ALFWorld, a simulator that enables agents to learn abstract, text\nbased policies in TextWorld (C\\^ot\\'e et al., 2018) and then execute goals from\nthe ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment.\nALFWorld enables the creation of a new BUTLER agent whose abstract knowledge,\nlearned in TextWorld, corresponds directly to concrete, visually grounded\nactions. In turn, as we demonstrate empirically, this fosters better agent\ngeneralization than training only in the visually grounded environment.\nBUTLER's simple, modular design factors the problem to allow researchers to\nfocus on models for improving every piece of the pipeline (language\nunderstanding, planning, navigation, and visual scene understanding).\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:13:36 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 22:44:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shridhar", "Mohit", ""], ["Yuan", "Xingdi", ""], ["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["Bisk", "Yonatan", ""], ["Trischler", "Adam", ""], ["Hausknecht", "Matthew", ""]]}, {"id": "2010.03773", "submitter": "Yang Li", "authors": "Yang Li, Tao Shen, Guodong Long, Jing Jiang, Tianyi Zhou, Chengqi\n  Zhang", "title": "Improving Long-Tail Relation Extraction with Collaborating\n  Relation-Augmented Attention", "comments": "Accepted to appear at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wrong labeling problem and long-tail relations are two main challenges caused\nby distant supervision in relation extraction. Recent works alleviate the wrong\nlabeling by selective attention via multi-instance learning, but cannot well\nhandle long-tail relations even if hierarchies of the relations are introduced\nto share knowledge. In this work, we propose a novel neural network,\nCollaborating Relation-augmented Attention (CoRA), to handle both the wrong\nlabeling and long-tail relations. Particularly, we first propose\nrelation-augmented attention network as base model. It operates on sentence bag\nwith a sentence-to-relation attention to minimize the effect of wrong labeling.\nThen, facilitated by the proposed base model, we introduce collaborating\nrelation features shared among relations in the hierarchies to promote the\nrelation-augmenting process and balance the training data for long-tail\nrelations. Besides the main training objective to predict the relation of a\nsentence bag, an auxiliary objective is utilized to guide the\nrelation-augmenting process for a more accurate bag-level representation. In\nthe experiments on the popular benchmark dataset NYT, the proposed CoRA\nimproves the prior state-of-the-art performance by a large margin in terms of\nPrecision@N, AUC and Hits@K. Further analyses verify its superior capability in\nhandling long-tail relations in contrast to the competitors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:34:43 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 02:38:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Li", "Yang", ""], ["Shen", "Tao", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Zhou", "Tianyi", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2010.03776", "submitter": "Kunze Wang", "authors": "Kunze Wang, Dong Lu, Soyeon Caren Han, Siqu Long, Josiah Poon", "title": "Detect All Abuse! Toward Universal Abusive Language Detection Models", "comments": "11 pages, 3 figures. Accepted by COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online abusive language detection (ALD) has become a societal issue of\nincreasing importance in recent years. Several previous works in online ALD\nfocused on solving a single abusive language problem in a single domain, like\nTwitter, and have not been successfully transferable to the general ALD task or\ndomain. In this paper, we introduce a new generic ALD framework, MACAS, which\nis capable of addressing several types of ALD tasks across different domains.\nOur generic framework covers multi-aspect abusive language embeddings that\nrepresent the target and content aspects of abusive language and applies a\ntextual graph embedding that analyses the user's linguistic behaviour. Then, we\npropose and use the cross-attention gate flow mechanism to embrace multiple\naspects of abusive language. Quantitative and qualitative evaluation results\nshow that our ALD algorithm rivals or exceeds the six state-of-the-art ALD\nalgorithms across seven ALD datasets covering multiple aspects of abusive\nlanguage and different online community domains.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:39:00 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 10:29:36 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wang", "Kunze", ""], ["Lu", "Dong", ""], ["Han", "Soyeon Caren", ""], ["Long", "Siqu", ""], ["Poon", "Josiah", ""]]}, {"id": "2010.03777", "submitter": "Tianyu Liu", "authors": "Tianyu Liu, Xin Zheng, Xiaoan Ding, Baobao Chang and Zhifang Sui", "title": "An Empirical Study on Model-agnostic Debiasing Strategies for Robust\n  Natural Language Inference", "comments": "CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prior work on natural language inference (NLI) debiasing mainly targets\nat one or few known biases while not necessarily making the models more robust.\nIn this paper, we focus on the model-agnostic debiasing strategies and explore\nhow to (or is it possible to) make the NLI models robust to multiple distinct\nadversarial attacks while keeping or even strengthening the models'\ngeneralization power. We firstly benchmark prevailing neural NLI models\nincluding pretrained ones on various adversarial datasets. We then try to\ncombat distinct known biases by modifying a mixture of experts (MoE) ensemble\nmethod and show that it's nontrivial to mitigate multiple NLI biases at the\nsame time, and that model-level ensemble method outperforms MoE ensemble\nmethod. We also perform data augmentation including text swap, word\nsubstitution and paraphrase and prove its efficiency in combating various\n(though not all) adversarial attacks at the same time. Finally, we investigate\nseveral methods to merge heterogeneous training data (1.35M) and perform model\nensembling, which are straightforward but effective to strengthen NLI models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:40:45 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 14:57:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liu", "Tianyu", ""], ["Zheng", "Xin", ""], ["Ding", "Xiaoan", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "2010.03790", "submitter": "Keerthiram Murugesan", "authors": "Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar\n  Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya\n  Sachan, Murray Campbell", "title": "Text-based RL Agents with Commonsense Knowledge: New Challenges,\n  Environments and Baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based games have emerged as an important test-bed for Reinforcement\nLearning (RL) research, requiring RL agents to combine grounded language\nunderstanding with sequential decision making. In this paper, we examine the\nproblem of infusing RL agents with commonsense knowledge. Such knowledge would\nallow agents to efficiently act in the world by pruning out implausible\nactions, and to perform look-ahead planning to determine how current actions\nmight affect future world states. We design a new text-based gaming environment\ncalled TextWorld Commonsense (TWC) for training and evaluating RL agents with a\nspecific kind of commonsense knowledge about objects, their attributes, and\naffordances. We also introduce several baseline RL agents which track the\nsequential context and dynamically retrieve the relevant commonsense knowledge\nfrom ConceptNet. We show that agents which incorporate commonsense knowledge in\nTWC perform better, while acting more efficiently. We conduct user-studies to\nestimate human performance on TWC and show that there is ample room for future\nimprovement.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 06:20:00 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Murugesan", "Keerthiram", ""], ["Atzeni", "Mattia", ""], ["Kapanipathi", "Pavan", ""], ["Shukla", "Pushkar", ""], ["Kumaravel", "Sadhana", ""], ["Tesauro", "Gerald", ""], ["Talamadupula", "Kartik", ""], ["Sachan", "Mrinmaya", ""], ["Campbell", "Murray", ""]]}, {"id": "2010.03802", "submitter": "Noah Constant", "authors": "Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David Uthus,\n  Zarana Parekh", "title": "TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the problem of text style transfer. Unlike\nprevious approaches requiring style-labeled training data, our method makes use\nof readily-available unlabeled text by relying on the implicit connection in\nstyle between adjacent sentences, and uses labeled data only at inference time.\nWe adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to\nextract a style vector from text and use it to condition the decoder to perform\nstyle transfer. As our label-free training results in a style vector space\nencoding many facets of style, we recast transfers as \"targeted restyling\"\nvector operations that adjust specific attributes of the input while preserving\nothers. We demonstrate that training on unlabeled Amazon reviews data results\nin a model that is competitive on sentiment transfer, even compared to models\ntrained fully on labeled data. Furthermore, applying our novel method to a\ndiverse corpus of unlabeled web text results in a single model capable of\ntransferring along multiple dimensions of style (dialect, emotiveness,\nformality, politeness, sentiment) despite no additional training and using only\na handful of exemplars at inference time.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:06:38 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 03:34:08 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 06:16:15 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Riley", "Parker", ""], ["Constant", "Noah", ""], ["Guo", "Mandy", ""], ["Kumar", "Girish", ""], ["Uthus", "David", ""], ["Parekh", "Zarana", ""]]}, {"id": "2010.03813", "submitter": "Vincent Micheli", "authors": "Vincent Micheli, Martin d'Hoffschmidt, Fran\\c{c}ois Fleuret", "title": "On the importance of pre-training data volume for compact language\n  models", "comments": "EMNLP 2020; typo corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in language modeling have led to computationally intensive\nand resource-demanding state-of-the-art models. In an effort towards\nsustainable practices, we study the impact of pre-training data volume on\ncompact language models. Multiple BERT-based models are trained on gradually\nincreasing amounts of French text. Through fine-tuning on the French Question\nAnswering Dataset (FQuAD), we observe that well-performing models are obtained\nwith as little as 100 MB of text. In addition, we show that past critically low\namounts of pre-training data, an intermediate pre-training step on the\ntask-specific corpus does not yield substantial improvements.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:40:21 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 14:36:43 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Micheli", "Vincent", ""], ["d'Hoffschmidt", "Martin", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "2010.03824", "submitter": "Tom Hope", "authors": "Tom Hope, Aida Amini, David Wadden, Madeleine van Zuylen, Sravanthi\n  Parasa, Eric Horvitz, Daniel Weld, Roy Schwartz, Hannaneh Hajishirzi", "title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers", "comments": "Accepted to NAACL 2021 (long paper). Tom Hope and Aida Amini made an\n  equal contribution. Data and code: https://git.io/JUhv7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has spawned a diverse body of scientific literature\nthat is challenging to navigate, stimulating interest in automated tools to\nhelp find useful knowledge. We pursue the construction of a knowledge base (KB)\nof mechanisms -- a fundamental concept across the sciences encompassing\nactivities, functions and causal relations, ranging from cellular processes to\neconomic impacts. We extract this information from the natural language of\nscientific papers by developing a broad, unified schema that strikes a balance\nbetween relevance and breadth. We annotate a dataset of mechanisms with our\nschema and train a model to extract mechanism relations from papers. Our\nexperiments demonstrate the utility of our KB in supporting interdisciplinary\nscientific search over COVID-19 literature, outperforming the prominent PubMed\nsearch in a study with clinical experts.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:54:14 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 19:40:19 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 10:59:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hope", "Tom", ""], ["Amini", "Aida", ""], ["Wadden", "David", ""], ["van Zuylen", "Madeleine", ""], ["Parasa", "Sravanthi", ""], ["Horvitz", "Eric", ""], ["Weld", "Daniel", ""], ["Schwartz", "Roy", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2010.03851", "submitter": "Jue Wang", "authors": "Jue Wang and Wei Lu", "title": "Two are Better than One: Joint Entity and Relation Extraction with\n  Table-Sequence Encoders", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition and relation extraction are two important\nfundamental problems. Joint learning algorithms have been proposed to solve\nboth tasks simultaneously, and many of them cast the joint task as a\ntable-filling problem. However, they typically focused on learning a single\nencoder (usually learning representation in the form of a table) to capture\ninformation required for both tasks within the same space. We argue that it can\nbe beneficial to design two distinct encoders to capture such two different\ntypes of information in the learning process. In this work, we propose the\nnovel {\\em table-sequence encoders} where two different encoders -- a table\nencoder and a sequence encoder are designed to help each other in the\nrepresentation learning process. Our experiments confirm the advantages of\nhaving {\\em two} encoders over {\\em one} encoder. On several standard datasets,\nour model shows significant improvements over existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:10:55 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Wang", "Jue", ""], ["Lu", "Wei", ""]]}, {"id": "2010.03855", "submitter": "Dong-Jin Kim", "authors": "Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon", "title": "Dense Relational Image Captioning via Multi-task Triple-Stream Networks", "comments": "Journal extension of our CVPR 2019 paper ( arXiv:1903.05942 ). Source\n  code : https://github.com/Dong-JinKim/DenseRelationalCaptioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce dense relational captioning, a novel image captioning task which\naims to generate multiple captions with respect to relational information\nbetween objects in a visual scene. Relational captioning provides explicit\ndescriptions of each relationship between object combinations. This framework\nis advantageous in both diversity and amount of information, leading to a\ncomprehensive image understanding based on relationships, e.g., relational\nproposal generation. For relational understanding between objects, the\npart-of-speech (POS, i.e., subject-object-predicate categories) can be a\nvaluable prior information to guide the causal sequence of words in a caption.\nWe enforce our framework to not only learn to generate captions but also\npredict the POS of each word. To this end, we propose the multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units\nresponsible for each POS which is trained by jointly predicting the correct\ncaptions and POS for each word. In addition, we found that the performance of\nMTTSNet can be improved by modulating the object embeddings with an explicit\nrelational module. We demonstrate that our proposed model can generate more\ndiverse and richer captions, via extensive experimental analysis on large scale\ndatasets and several metrics. We additionally extend analysis to an ablation\nstudy, applications on holistic image captioning, scene graph generation, and\nretrieval tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:17:55 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 18:14:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kim", "Dong-Jin", ""], ["Oh", "Tae-Hyun", ""], ["Choi", "Jinsoo", ""], ["Kweon", "In So", ""]]}, {"id": "2010.03863", "submitter": "Anna Rogers", "authors": "Anna Rogers, Isabelle Augenstein", "title": "What Can We Do to Improve Peer Review in NLP?", "comments": "To appear at Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review is our best tool for judging the quality of conference\nsubmissions, but it is becoming increasingly spurious. We argue that a part of\nthe problem is that the reviewers and area chairs face a poorly defined task\nforcing apples-to-oranges comparisons. There are several potential ways\nforward, but the key difficulty is creating the incentives and mechanisms for\ntheir consistent implementation in the NLP community.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:32:21 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Rogers", "Anna", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2010.03880", "submitter": "Libo Qin", "authors": "Libo Qin, Tailu Liu, Wanxiang Che, Bingbing Kang, Sendong Zhao, Ting\n  Liu", "title": "A Co-Interactive Transformer for Joint Slot Filling and Intent Detection", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent detection and slot filling are two main tasks for building a spoken\nlanguage understanding (SLU) system. The two tasks are closely related and the\ninformation of one task can be utilized in the other task. Previous studies\neither model the two tasks separately or only consider the single information\nflow from intent to slot. None of the prior approaches model the bidirectional\nconnection between the two tasks simultaneously. In this paper, we propose a\nCo-Interactive Transformer to consider the cross-impact between the two tasks.\nInstead of adopting the self-attention mechanism in vanilla Transformer, we\npropose a co-interactive module to consider the cross-impact by building a\nbidirectional connection between the two related tasks. In addition, the\nproposed co-interactive module can be stacked to incrementally enhance each\nother with mutual features. The experimental results on two public datasets\n(SNIPS and ATIS) show that our model achieves the state-of-the-art performance\nwith considerable improvements (+3.4% and +0.9% on overall acc). Extensive\nexperiments empirically verify that our model successfully captures the mutual\ninteraction knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 10:16:52 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 12:29:17 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 05:44:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Qin", "Libo", ""], ["Liu", "Tailu", ""], ["Che", "Wanxiang", ""], ["Kang", "Bingbing", ""], ["Zhao", "Sendong", ""], ["Liu", "Ting", ""]]}, {"id": "2010.03881", "submitter": "Gyuwan Kim", "authors": "Gyuwan Kim and Tae-Hwan Jung", "title": "Large Product Key Memory for Pretrained Language Models", "comments": "Accepted to Findings of EMNLP 2020; 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product key memory (PKM) proposed by Lample et al. (2019) enables to improve\nprediction accuracy by increasing model capacity efficiently with insignificant\ncomputational overhead. However, their empirical application is only limited to\ncausal language modeling. Motivated by the recent success of pretrained\nlanguage models (PLMs), we investigate how to incorporate large PKM into PLMs\nthat can be finetuned for a wide variety of downstream NLP tasks. We define a\nnew memory usage metric, and careful observation using this metric reveals that\nmost memory slots remain outdated during the training of PKM-augmented models.\nTo train better PLMs by tackling this issue, we propose simple but effective\nsolutions: (1) initialization from the model weights pretrained without memory\nand (2) augmenting PKM by addition rather than replacing a feed-forward\nnetwork. We verify that both of them are crucial for the pretraining of\nPKM-augmented PLMs, enhancing memory utilization and downstream performance.\nCode and pretrained weights are available at\nhttps://github.com/clovaai/pkm-transformers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 10:19:50 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Kim", "Gyuwan", ""], ["Jung", "Tae-Hwan", ""]]}, {"id": "2010.03899", "submitter": "Gabriel Synnaeve", "authors": "Daniel Haziza, J\\'er\\'emy Rapin, Gabriel Synnaeve", "title": "Population Based Training for Data Augmentation and Regularization in\n  Speech Recognition", "comments": "tech report from Dec. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying data augmentation policies and regularization over the course of\noptimization has led to performance improvements over using fixed values. We\nshow that population based training is a useful tool to continuously search\nthose hyperparameters, within a fixed budget. This greatly simplifies the\nexperimental burden and computational cost of finding such optimal schedules.\nWe experiment in speech recognition by optimizing SpecAugment this way, as well\nas dropout. It compares favorably to a baseline that does not change those\nhyperparameters over the course of training, with an 8% relative WER\nimprovement. We obtain 5.18% word error rate on LibriSpeech's test-other.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 11:00:18 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Haziza", "Daniel", ""], ["Rapin", "J\u00e9r\u00e9my", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "2010.03903", "submitter": "Dechuan Teng", "authors": "Dechuang Teng, Libo Qin, Wanxiang Che, Sendong Zhao, Ting Liu", "title": "Injecting Word Information with Multi-Level Word Adapter for Chinese\n  Spoken Language Understanding", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we improve Chinese spoken language understanding (SLU) by\ninjecting word information. Previous studies on Chinese SLU do not consider the\nword information, failing to detect word boundaries that are beneficial for\nintent detection and slot filling. To address this issue, we propose a\nmulti-level word adapter to inject word information for Chinese SLU, which\nconsists of (1) sentence-level word adapter, which directly fuses the sentence\nrepresentations of the word information and character information to perform\nintent detection and (2) character-level word adapter, which is applied at each\ncharacter for selectively controlling weights on word information as well as\ncharacter information. Experimental results on two Chinese SLU datasets show\nthat our model can capture useful word information and achieve state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 11:11:05 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 01:40:15 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Teng", "Dechuang", ""], ["Qin", "Libo", ""], ["Che", "Wanxiang", ""], ["Zhao", "Sendong", ""], ["Liu", "Ting", ""]]}, {"id": "2010.03920", "submitter": "Daniel Zeman", "authors": "Martin Vastl, Daniel Zeman, Rudolf Rosa", "title": "Predicting Typological Features in WALS using Language Embeddings and\n  Conditional Probabilities: \\'UFAL Submission to the SIGTYP 2020 Shared Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present our submission to the SIGTYP 2020 Shared Task on the prediction of\ntypological features. We submit a constrained system, predicting typological\nfeatures only based on the WALS database. We investigate two approaches. The\nsimpler of the two is a system based on estimating correlation of feature\nvalues within languages by computing conditional probabilities and mutual\ninformation. The second approach is to train a neural predictor operating on\nprecomputed language embeddings based on WALS features. Our submitted system\ncombines the two approaches based on their self-estimated confidence scores. We\nreach the accuracy of 70.7% on the test data and rank first in the shared task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 12:05:48 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Vastl", "Martin", ""], ["Zeman", "Daniel", ""], ["Rosa", "Rudolf", ""]]}, {"id": "2010.03982", "submitter": "Arne K\\\"ohn", "authors": "Arne K\\\"ohn and Julia Wichlacz and \\'Alvaro Torralba and Daniel\n  H\\\"oller and J\\\"org Hoffmann and Alexander Koller", "title": "Generating Instructions at Different Levels of Abstraction", "comments": "Accepted COLING 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When generating technical instructions, it is often convenient to describe\ncomplex objects in the world at different levels of abstraction. A novice user\nmight need an object explained piece by piece, while for an expert, talking\nabout the complex object (e.g. a wall or railing) directly may be more succinct\nand efficient. We show how to generate building instructions at different\nlevels of abstraction in Minecraft. We introduce the use of hierarchical\nplanning to this end, a method from AI planning which can capture the structure\nof complex objects neatly. A crowdsourcing evaluation shows that the choice of\nabstraction level matters to users, and that an abstraction strategy which\nbalances low-level and high-level object descriptions compares favorably to\nones which don't.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 13:56:09 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["K\u00f6hn", "Arne", ""], ["Wichlacz", "Julia", ""], ["Torralba", "\u00c1lvaro", ""], ["H\u00f6ller", "Daniel", ""], ["Hoffmann", "J\u00f6rg", ""], ["Koller", "Alexander", ""]]}, {"id": "2010.03994", "submitter": "Zheng Ye", "authors": "Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, Xiaodan Liang", "title": "GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating\n  Open-Domain Dialogue Systems", "comments": "Long paper; EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically evaluating dialogue coherence is a challenging but high-demand\nability for developing high-quality open-domain dialogue systems. However,\ncurrent evaluation metrics consider only surface features or utterance-level\nsemantics, without explicitly considering the fine-grained topic transition\ndynamics of dialogue flows. Here, we first consider that the graph structure\nconstituted with topics in a dialogue can accurately depict the underlying\ncommunication logic, which is a more natural way to produce persuasive metrics.\nCapitalized on the topic-level dialogue graph, we propose a new evaluation\nmetric GRADE, which stands for Graph-enhanced Representations for Automatic\nDialogue Evaluation. Specifically, GRADE incorporates both coarse-grained\nutterance-level contextualized representations and fine-grained topic-level\ngraph representations to evaluate dialogue coherence. The graph representations\nare obtained by reasoning over topic-level dialogue graphs enhanced with the\nevidence from a commonsense graph, including k-hop neighboring representations\nand hop-attention weights. Experimental results show that our GRADE\nsignificantly outperforms other state-of-the-art metrics on measuring diverse\ndialogue models in terms of the Pearson and Spearman correlations with human\njudgements. Besides, we release a new large-scale human evaluation benchmark to\nfacilitate future research on automatic metrics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:07:32 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Huang", "Lishan", ""], ["Ye", "Zheng", ""], ["Qin", "Jinghui", ""], ["Lin", "Liang", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2010.04043", "submitter": "Haokun Liu", "authors": "Haokun Liu, William Huang, Dhara A. Mungra, Samuel R. Bowman", "title": "Precise Task Formalization Matters in Winograd Schema Evaluations", "comments": "Accepted to the EMNLP 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance on the Winograd Schema Challenge (WSC), a respected English\ncommonsense reasoning benchmark, recently rocketed from chance accuracy to 89%\non the SuperGLUE leaderboard, with relatively little corroborating evidence of\na correspondingly large improvement in reasoning ability. We hypothesize that\nmuch of this improvement comes from recent changes in task formalization---the\ncombination of input specification, loss function, and reuse of pretrained\nparameters---by users of the dataset, rather than improvements in the\npretrained model's reasoning ability. We perform an ablation on two Winograd\nSchema datasets that interpolates between the formalizations used before and\nafter this surge, and find (i) framing the task as multiple choice improves\nperformance by 2-6 points and (ii) several additional techniques, including the\nreuse of a pretrained language modeling head, can mitigate the model's extreme\nsensitivity to hyperparameters. We urge future benchmark creators to impose\nadditional structure to minimize the impact of formalization decisions on\nreported results.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:10:47 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Liu", "Haokun", ""], ["Huang", "William", ""], ["Mungra", "Dhara A.", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2010.04098", "submitter": "Varun Gangal", "authors": "Varun Gangal, Eduard Hovy", "title": "BERTering RAMS: What and How Much does BERT Already Know About Event\n  Arguments? -- A Study on the RAMS Dataset", "comments": "Accepted for the BlackBoxNLP 2020 Workshop @EMNLP 2020;\n  Pre-camera-ready copy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the attention map based probing frame-work from (Clark et al., 2019),\nwe observe that, on the RAMS dataset (Ebner et al., 2020), BERT's attention\nheads have modest but well above-chance ability to spot event arguments sans\nany training or domain finetuning, vary-ing from a low of 17.77% for Place to a\nhigh of 51.61% for Artifact. Next, we find that linear combinations of these\nheads, estimated with approx 11% of available total event argument detection\nsupervision, can push performance well-higher for some roles - highest two\nbeing Victim (68.29% Accuracy) and Artifact(58.82% Accuracy). Furthermore, we\ninvestigate how well our methods do for cross-sentence event arguments. We\npropose a procedure to isolate \"best heads\" for cross-sentence argument\ndetection separately of those for intra-sentence arguments. The heads thus\nestimated have superior cross-sentence performance compared to their jointly\nestimated equivalents, albeit only under the unrealistic assumption that we\nalready know the argument is present in an-other sentence. Lastly, we seek to\nisolate to what extent our numbers stem from lexical frequency based\nassociations between gold arguments and roles. We propose NONCE, a scheme to\ncreate adversarial test examples by replacing gold arguments with randomly\ngenerated \"nonce\" words. We find that learnt linear combinations are robust to\nNONCE, though individual best heads can be more sensitive.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 16:27:03 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 19:02:14 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gangal", "Varun", ""], ["Hovy", "Eduard", ""]]}, {"id": "2010.04119", "submitter": "Peter Hase", "authors": "Peter Hase, Shiyue Zhang, Harry Xie, Mohit Bansal", "title": "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial\n  Explanations of Their Behavior in Natural Language?", "comments": "EMNLP 2020 Findings (17 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection for natural language (NL) understanding tasks has\nincreasingly included human explanations alongside data points, allowing past\nworks to introduce models that both perform a task and generate NL explanations\nfor their outputs. Yet to date, model-generated explanations have been\nevaluated on the basis of surface-level similarities to human explanations,\nboth through automatic metrics like BLEU and human evaluations. We argue that\nthese evaluations are insufficient, since they fail to indicate whether\nexplanations support actual model behavior (faithfulness), rather than simply\nmatch what a human would say (plausibility). In this work, we address the\nproblem of evaluating explanations from the model simulatability perspective.\nOur contributions are as follows: (1) We introduce a leakage-adjusted\nsimulatability (LAS) metric for evaluating NL explanations, which measures how\nwell explanations help an observer predict a model's output, while controlling\nfor how explanations can directly leak the output. We use a model as a proxy\nfor a human observer, and validate this choice with two human subject\nexperiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing\ngenerative graphical models and two new approaches; one rationalizing method we\nintroduce achieves roughly human-level LAS scores. (3) Lastly, we frame\nexplanation generation as a multi-agent game and optimize explanations for\nsimulatability while penalizing label leakage, which can improve LAS scores. We\nprovide code for the experiments in this paper at\nhttps://github.com/peterbhase/LAS-NL-Explanations\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 16:59:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Hase", "Peter", ""], ["Zhang", "Shiyue", ""], ["Xie", "Harry", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.04125", "submitter": "Kun Zhou", "authors": "Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang and Ji-Rong Wen", "title": "Towards Topic-Guided Conversational Recommender System", "comments": "12 pages, Accepted by Coling2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational recommender systems (CRS) aim to recommend high-quality items\nto users through interactive conversations. To develop an effective CRS, the\nsupport of high-quality datasets is essential. Existing CRS datasets mainly\nfocus on immediate requests from users, while lack proactive guidance to the\nrecommendation scenario. In this paper, we contribute a new CRS dataset named\n\\textbf{TG-ReDial} (\\textbf{Re}commendation through\n\\textbf{T}opic-\\textbf{G}uided \\textbf{Dial}og). Our dataset has two major\nfeatures. First, it incorporates topic threads to enforce natural semantic\ntransitions towards the recommendation scenario. Second, it is created in a\nsemi-automatic way, hence human annotation is more reasonable and controllable.\nBased on TG-ReDial, we present the task of topic-guided conversational\nrecommendation, and propose an effective approach to this task. Extensive\nexperiments have demonstrated the effectiveness of our approach on three\nsub-tasks, namely topic prediction, item recommendation and response\ngeneration. TG-ReDial is available at https://github.com/RUCAIBox/TG-ReDial.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:04:30 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 14:25:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Kun", ""], ["Zhou", "Yuanhang", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Xiaoke", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.04141", "submitter": "Ernie Chang", "authors": "Ernie Chang, Jeriah Caplinger, Alex Marin, Xiaoyu Shen, Vera Demberg", "title": "DART: A Lightweight Quality-Suggestive Data-to-Text Annotation Tool", "comments": "Accepted to COLING 2020 (selected as outstanding paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a lightweight annotation tool, the Data AnnotatoR Tool (DART), for\nthe general task of labeling structured data with textual descriptions. The\ntool is implemented as an interactive application that reduces human efforts in\nannotating large quantities of structured data, e.g. in the format of a table\nor tree structure. By using a backend sequence-to-sequence model, our system\niteratively analyzes the annotated labels in order to better sample unlabeled\ndata. In a simulation experiment performed on annotating large quantities of\nstructured data, DART has been shown to reduce the total number of annotations\nneeded with active learning and automatically suggesting relevant labels.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:36:34 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:58:57 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chang", "Ernie", ""], ["Caplinger", "Jeriah", ""], ["Marin", "Alex", ""], ["Shen", "Xiaoyu", ""], ["Demberg", "Vera", ""]]}, {"id": "2010.04191", "submitter": "Abhishek Singh", "authors": "Abhishek Singh", "title": "PoinT-5: Pointer Network and T-5 based Financial NarrativeSummarisation", "comments": "Accepted in FNS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Companies provide annual reports to their shareholders at the end of the\nfinancial year that describes their operations and financial conditions. The\naverage length of these reports is 80, and it may extend up to 250 pages long.\nIn this paper, we propose our methodology PoinT-5 (the combination of Pointer\nNetwork and T-5 (Test-to-text transfer Transformer) algorithms) that we used in\nthe Financial Narrative Summarisation (FNS) 2020 task. The proposed method uses\npointer networks to extract important narrative sentences from the report, and\nthen T-5 is used to paraphrase extracted sentences into a concise yet\ninformative sentence. We evaluate our method using ROUGE-N (1,2), L, and SU4.\nThe proposed method achieves the highest precision scores in all the metrics\nand highest F1 scores in ROUGE1, and LCS and the only solution to cross the\nMUSE solution baseline in ROUGE-LCS metrics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 18:09:45 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 03:31:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Singh", "Abhishek", ""]]}, {"id": "2010.04245", "submitter": "Prudhvi Raj Dachapally", "authors": "Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, Yuxuan Chen", "title": "Query-Key Normalization for Transformers", "comments": "8 pages, 2 figures, accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resource language translation is a challenging but socially valuable NLP\ntask. Building on recent work adapting the Transformer's normalization to this\nsetting, we propose QKNorm, a normalization technique that modifies the\nattention mechanism to make the softmax function less prone to arbitrary\nsaturation without sacrificing expressivity. Specifically, we apply $\\ell_2$\nnormalization along the head dimension of each query and key matrix prior to\nmultiplying them and then scale up by a learnable parameter instead of dividing\nby the square root of the embedding dimension. We show improvements averaging\n0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource\ntranslation pairs from the TED Talks corpus and IWSLT'15.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 20:12:35 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Henry", "Alex", ""], ["Dachapally", "Prudhvi Raj", ""], ["Pawar", "Shubham", ""], ["Chen", "Yuxuan", ""]]}, {"id": "2010.04246", "submitter": "Shang-Yu Su", "authors": "Shang-Yu Su, Yung-Sung Chuang, Yun-Nung Chen", "title": "Dual Inference for Improving Language Understanding and Generation", "comments": "Published in Findings of EMNLP 2020. The first two authors\n  contributed to this paper equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language understanding (NLU) and Natural language generation (NLG)\ntasks hold a strong dual relationship, where NLU aims at predicting semantic\nlabels based on natural language utterances and NLG does the opposite. The\nprior work mainly focused on exploiting the duality in model training in order\nto obtain the models with better performance. However, regarding the\nfast-growing scale of models in the current NLP area, sometimes we may have\ndifficulty retraining whole NLU and NLG models. To better address the issue,\nthis paper proposes to leverage the duality in the inference stage without the\nneed of retraining. The experiments on three benchmark datasets demonstrate the\neffectiveness of the proposed method in both NLU and NLG, providing the great\npotential of practical usage.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 20:14:41 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 02:10:48 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Su", "Shang-Yu", ""], ["Chuang", "Yung-Sung", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2010.04249", "submitter": "Ansel MacLaughlin", "authors": "Ansel MacLaughlin, Jwala Dhamala, Anoop Kumar, Sriram Venkatapathy,\n  Ragav Venkatesan, Rahul Gupta", "title": "Evaluating the Effectiveness of Efficient Neural Architecture Search for\n  Sentence-Pair Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) methods, which automatically learn entire\nneural model or individual neural cell architectures, have recently achieved\ncompetitive or state-of-the-art (SOTA) performance on variety of natural\nlanguage processing and computer vision tasks, including language modeling,\nnatural language inference, and image classification. In this work, we explore\nthe applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search\n(ENAS) (Pham et al., 2018) to two sentence pair tasks, paraphrase detection and\nsemantic textual similarity. We use ENAS to perform a micro-level search and\nlearn a task-optimized RNN cell architecture as a drop-in replacement for an\nLSTM. We explore the effectiveness of ENAS through experiments on three\ndatasets (MRPC, SICK, STS-B), with two different models (ESIM, BiLSTM-Max), and\ntwo sets of embeddings (Glove, BERT). In contrast to prior work applying ENAS\nto NLP tasks, our results are mixed -- we find that ENAS architectures\nsometimes, but not always, outperform LSTMs and perform similarly to random\narchitecture search.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 20:26:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["MacLaughlin", "Ansel", ""], ["Dhamala", "Jwala", ""], ["Kumar", "Anoop", ""], ["Venkatapathy", "Sriram", ""], ["Venkatesan", "Ragav", ""], ["Gupta", "Rahul", ""]]}, {"id": "2010.04260", "submitter": "Akbar Siami Namin", "authors": "Faranak Abri, Luis Felipe Gutierrez, Akbar Siami Namin, Keith S.\n  Jones, David R. W. Sears", "title": "Fake Reviews Detection through Analysis of Linguistic Features", "comments": "The pre-print of a paper to appear in the proceedings of the IEEE\n  International Conference on Machine Learning Applications (ICMLA 2020), 11\n  pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews play an integral part for success or failure of businesses.\nPrior to purchasing services or goods, customers first review the online\ncomments submitted by previous customers. However, it is possible to\nsuperficially boost or hinder some businesses through posting counterfeit and\nfake reviews. This paper explores a natural language processing approach to\nidentify fake reviews. We present a detailed analysis of linguistic features\nfor distinguishing fake and trustworthy online reviews. We study 15 linguistic\nfeatures and measure their significance and importance towards the\nclassification schemes employed in this study. Our results indicate that fake\nreviews tend to include more redundant terms and pauses, and generally contain\nlonger sentences. The application of several machine learning classification\nalgorithms revealed that we were able to discriminate fake from real reviews\nwith high accuracy using these linguistic features.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:16:30 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Abri", "Faranak", ""], ["Gutierrez", "Luis Felipe", ""], ["Namin", "Akbar Siami", ""], ["Jones", "Keith S.", ""], ["Sears", "David R. W.", ""]]}, {"id": "2010.04284", "submitter": "Hong-Kwang Kuo", "authors": "Yinghui Huang, Hong-Kwang Kuo, Samuel Thomas, Zvi Kons, Kartik\n  Audhkhasi, Brian Kingsbury, Ron Hoory, Michael Picheny", "title": "Leveraging Unpaired Text Data for Training End-to-End Speech-to-Intent\n  Systems", "comments": "5 pages, published in ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training an end-to-end (E2E) neural network speech-to-intent (S2I) system\nthat directly extracts intents from speech requires large amounts of\nintent-labeled speech data, which is time consuming and expensive to collect.\nInitializing the S2I model with an ASR model trained on copious speech data can\nalleviate data sparsity. In this paper, we attempt to leverage NLU text\nresources. We implemented a CTC-based S2I system that matches the performance\nof a state-of-the-art, traditional cascaded SLU system. We performed controlled\nexperiments with varying amounts of speech and text training data. When only a\ntenth of the original data is available, intent classification accuracy\ndegrades by 7.6% absolute. Assuming we have additional text-to-intent data\n(without speech) available, we investigated two techniques to improve the S2I\nsystem: (1) transfer learning, in which acoustic embeddings for intent\nclassification are tied to fine-tuned BERT text embeddings; and (2) data\naugmentation, in which the text-to-intent data is converted into\nspeech-to-intent data using a multi-speaker text-to-speech system. The proposed\napproaches recover 80% of performance lost due to using limited intent-labeled\nspeech.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:16:26 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Huang", "Yinghui", ""], ["Kuo", "Hong-Kwang", ""], ["Thomas", "Samuel", ""], ["Kons", "Zvi", ""], ["Audhkhasi", "Kartik", ""], ["Kingsbury", "Brian", ""], ["Hoory", "Ron", ""], ["Picheny", "Michael", ""]]}, {"id": "2010.04288", "submitter": "Trang Tran", "authors": "Trang Tran, Jiahong Yuan, Yang Liu, Mari Ostendorf", "title": "On the Role of Style in Parsing Speech with Neural Models", "comments": "Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The differences in written text and conversational speech are substantial;\nprevious parsers trained on treebanked text have given very poor results on\nspontaneous speech. For spoken language, the mismatch in style also extends to\nprosodic cues, though it is less well understood. This paper re-examines the\nuse of written text in parsing speech in the context of recent advances in\nneural language processing. We show that neural approaches facilitate using\nwritten text to improve parsing of spontaneous speech, and that prosody further\nimproves over this state-of-the-art result. Further, we find an asymmetric\ndegradation from read vs. spontaneous mismatch, with spontaneous speech more\ngenerally useful for training parsers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:44:19 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Tran", "Trang", ""], ["Yuan", "Jiahong", ""], ["Liu", "Yang", ""], ["Ostendorf", "Mari", ""]]}, {"id": "2010.04292", "submitter": "Douglas Guilbeault R", "authors": "Bhargav Srinivasa Desikan, Tasker Hull, Ethan O. Nadler, Douglas\n  Guilbeault, Aabir Abubaker Kar, Mark Chu and Donald Ruggiero Lo Sardo", "title": "comp-syn: Perceptually Grounded Word Embeddings with Color", "comments": "9 pages, 3 figures, all code and data available at\n  https://github.com/comp-syn/comp-syn. Forthcoming in the Proceedings of the\n  28th International Conference on Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Popular approaches to natural language processing create word embeddings\nbased on textual co-occurrence patterns, but often ignore embodied, sensory\naspects of language. Here, we introduce the Python package comp-syn, which\nprovides grounded word embeddings based on the perceptually uniform color\ndistributions of Google Image search results. We demonstrate that comp-syn\nsignificantly enriches models of distributional semantics. In particular, we\nshow that (1) comp-syn predicts human judgments of word concreteness with\ngreater accuracy and in a more interpretable fashion than word2vec using\nlow-dimensional word-color embeddings, and (2) comp-syn performs comparably to\nword2vec on a metaphorical vs. literal word-pair classification task. comp-syn\nis open-source on PyPi and is compatible with mainstream machine-learning\nPython packages. Our package release includes word-color embeddings for over\n40,000 English words, each associated with crowd-sourced word concreteness\njudgments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:50:06 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 05:22:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Desikan", "Bhargav Srinivasa", ""], ["Hull", "Tasker", ""], ["Nadler", "Ethan O.", ""], ["Guilbeault", "Douglas", ""], ["Kar", "Aabir Abubaker", ""], ["Chu", "Mark", ""], ["Sardo", "Donald Ruggiero Lo", ""]]}, {"id": "2010.04293", "submitter": "Trang Tran", "authors": "Trang Tran, Morgan Tinkler, Gary Yeung, Abeer Alwan, Mari Ostendorf", "title": "Analysis of Disfluency in Children's Speech", "comments": "Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disfluencies are prevalent in spontaneous speech, as shown in many studies of\nadult speech. Less is understood about children's speech, especially in\npre-school children who are still developing their language skills. We present\na novel dataset with annotated disfluencies of spontaneous explanations from 26\nchildren (ages 5--8), interviewed twice over a year-long period. Our\npreliminary analysis reveals significant differences between children's speech\nin our corpus and adult spontaneous speech from two corpora (Switchboard and\nCallHome). Children have higher disfluency and filler rates, tend to use nasal\nfilled pauses more frequently, and on average exhibit longer reparandums than\nrepairs, in contrast to adult speakers. Despite the differences, an automatic\ndisfluency detection system trained on adult (Switchboard) speech transcripts\nperforms reasonably well on children's speech, achieving an F1 score that is\n10\\% higher than the score on an adult out-of-domain dataset (CallHome).\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:51:25 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Tran", "Trang", ""], ["Tinkler", "Morgan", ""], ["Yeung", "Gary", ""], ["Alwan", "Abeer", ""], ["Ostendorf", "Mari", ""]]}, {"id": "2010.04295", "submitter": "Yang Li", "authors": "Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, Zhiwei Guan", "title": "Widget Captioning: Generating Natural Language Description for Mobile\n  User Interface Elements", "comments": "16 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language descriptions of user interface (UI) elements such as\nalternative text are crucial for accessibility and language-based interaction\nin general. Yet, these descriptions are constantly missing in mobile UIs. We\npropose widget captioning, a novel task for automatically generating language\ndescriptions for UI elements from multimodal input including both the image and\nthe structural representations of user interfaces. We collected a large-scale\ndataset for widget captioning with crowdsourcing. Our dataset contains 162,859\nlanguage phrases created by human workers for annotating 61,285 UI elements\nacross 21,750 unique UI screens. We thoroughly analyze the dataset, and train\nand evaluate a set of deep model configurations to investigate how each feature\nmodality as well as the choice of learning strategies impact the quality of\npredicted captions. The task formulation and the dataset as well as our\nbenchmark models contribute a solid basis for this novel multimodal captioning\ntask that connects language and user interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:56:03 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Yang", ""], ["Li", "Gang", ""], ["He", "Luheng", ""], ["Zheng", "Jingjie", ""], ["Li", "Hong", ""], ["Guan", "Zhiwei", ""]]}, {"id": "2010.04297", "submitter": "Thibault Sellam", "authors": "Thibault Sellam, Amy Pu, Hyung Won Chung, Sebastian Gehrmann, Qijun\n  Tan, Markus Freitag, Dipanjan Das, Ankur P. Parikh", "title": "Learning to Evaluate Translation Beyond English: BLEURT Submissions to\n  the WMT Metrics 2020 Shared Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of machine translation systems has dramatically improved over the\nlast decade, and as a result, evaluation has become an increasingly challenging\nproblem. This paper describes our contribution to the WMT 2020 Metrics Shared\nTask, the main benchmark for automatic evaluation of translation. We make\nseveral submissions based on BLEURT, a previously published metric based on\ntransfer learning. We extend the metric beyond English and evaluate it on 14\nlanguage pairs for which fine-tuning data is available, as well as 4\n\"zero-shot\" language pairs, for which we have no labelled examples.\nAdditionally, we focus on English to German and demonstrate how to combine\nBLEURT's predictions with those of YiSi and use alternative reference\ntranslations to enhance the performance. Empirical results show that the models\nachieve competitive results on the WMT Metrics 2019 Shared Task, indicating\ntheir promise for the 2020 edition.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 23:16:26 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 21:45:11 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 22:40:08 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Sellam", "Thibault", ""], ["Pu", "Amy", ""], ["Chung", "Hyung Won", ""], ["Gehrmann", "Sebastian", ""], ["Tan", "Qijun", ""], ["Freitag", "Markus", ""], ["Das", "Dipanjan", ""], ["Parikh", "Ankur P.", ""]]}, {"id": "2010.04301", "submitter": "Jonathan Shen", "authors": "Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga\n  Zen, Yonghui Wu", "title": "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis\n  Including Unsupervised Duration Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents Non-Attentive Tacotron based on the Tacotron 2\ntext-to-speech model, replacing the attention mechanism with an explicit\nduration predictor. This improves robustness significantly as measured by\nunaligned duration ratio and word deletion rate, two metrics introduced in this\npaper for large-scale robustness evaluation using a pre-trained speech\nrecognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron\nachieves a 5-scale mean opinion score for naturalness of 4.41, slightly\noutperforming Tacotron 2. The duration predictor enables both utterance-wide\nand per-phoneme control of duration at inference time. When accurate target\ndurations are scarce or unavailable in the training data, we propose a method\nusing a fine-grained variational auto-encoder to train the duration predictor\nin a semi-supervised or unsupervised manner, with results almost as good as\nsupervised training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 23:41:39 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 03:05:23 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 01:19:03 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 04:12:14 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Shen", "Jonathan", ""], ["Jia", "Ye", ""], ["Chrzanowski", "Mike", ""], ["Zhang", "Yu", ""], ["Elias", "Isaac", ""], ["Zen", "Heiga", ""], ["Wu", "Yonghui", ""]]}, {"id": "2010.04302", "submitter": "Gregory Senay", "authors": "Gregory Senay and Emmanuelle Salin", "title": "Masked ELMo: An evolution of ELMo towards fully contextual RNN language\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Masked ELMo, a new RNN-based model for language model\npre-training, evolved from the ELMo language model. Contrary to ELMo which only\nuses independent left-to-right and right-to-left contexts, Masked ELMo learns\nfully bidirectional word representations. To achieve this, we use the same\nMasked language model objective as BERT. Additionally, thanks to optimizations\non the LSTM neuron, the integration of mask accumulation and bidirectional\ntruncated backpropagation through time, we have increased the training speed of\nthe model substantially. All these improvements make it possible to pre-train a\nbetter language model than ELMo while maintaining a low computational cost. We\nevaluate Masked ELMo by comparing it to ELMo within the same protocol on the\nGLUE benchmark, where our model outperforms significantly ELMo and is\ncompetitive with transformer approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 23:58:57 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Senay", "Gregory", ""], ["Salin", "Emmanuelle", ""]]}, {"id": "2010.04303", "submitter": "Javid Ebrahimi", "authors": "Javid Ebrahimi, Dhruv Gelda, Wei Zhang", "title": "How Can Self-Attention Networks Recognize Dyck-n Languages?", "comments": null, "journal-ref": "Findings of EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the recognition of Dyck-n ($\\mathcal{D}_n$) languages with\nself-attention (SA) networks, which has been deemed to be a difficult task for\nthese networks. We compare the performance of two variants of SA, one with a\nstarting symbol (SA$^+$) and one without (SA$^-$). Our results show that SA$^+$\nis able to generalize to longer sequences and deeper dependencies. For\n$\\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences\nwhereas the accuracy of SA$^+$ is 58.82$\\%$. We find attention maps learned by\n$\\text{SA}{^+}$ to be amenable to interpretation and compatible with a\nstack-based language recognizer. Surprisingly, the performance of SA networks\nis at par with LSTMs, which provides evidence on the ability of SA to learn\nhierarchies without recursion.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 00:03:17 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ebrahimi", "Javid", ""], ["Gelda", "Dhruv", ""], ["Zhang", "Wei", ""]]}, {"id": "2010.04314", "submitter": "Xiaomian Kang", "authors": "Xiaomian Kang, Yang Zhao, Jiajun Zhang, Chengqing Zong", "title": "Dynamic Context Selection for Document-level Neural Machine Translation\n  via Reinforcement Learning", "comments": "Accepted to EMNLP 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level neural machine translation has yielded attractive\nimprovements. However, majority of existing methods roughly use all context\nsentences in a fixed scope. They neglect the fact that different source\nsentences need different sizes of context. To address this problem, we propose\nan effective approach to select dynamic context so that the document-level\ntranslation model can utilize the more useful selected context sentences to\nproduce better translations. Specifically, we introduce a selection module that\nis independent of the translation module to score each candidate context\nsentence. Then, we propose two strategies to explicitly select a variable\nnumber of context sentences and feed them into the translation module. We train\nthe two modules end-to-end via reinforcement learning. A novel reward is\nproposed to encourage the selection and utilization of dynamic context\nsentences. Experiments demonstrate that our approach can select adaptive\ncontext sentences for different source sentences, and significantly improves\nthe performance of document-level translation methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 01:05:32 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kang", "Xiaomian", ""], ["Zhao", "Yang", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "2010.04332", "submitter": "Takumi Ito", "authors": "Takumi Ito, Tatsuki Kuribayashi, Masatoshi Hidaka, Jun Suzuki, Kentaro\n  Inui", "title": "Langsmith: An Interactive Academic Text Revision System", "comments": "Accepted at EMNLP 2020 (system demonstrations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the current diversity and inclusion initiatives in the academic\ncommunity, researchers with a non-native command of English still face\nsignificant obstacles when writing papers in English. This paper presents the\nLangsmith editor, which assists inexperienced, non-native researchers to write\nEnglish papers, especially in the natural language processing (NLP) field. Our\nsystem can suggest fluent, academic-style sentences to writers based on their\nrough, incomplete phrases or sentences. The system also encourages interaction\nbetween human writers and the computerized revision system. The experimental\nresults demonstrated that Langsmith helps non-native English-speaker students\nwrite papers in English. The system is available at https://emnlp-demo.editor.\nlangsmith.co.jp/.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 02:35:14 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ito", "Takumi", ""], ["Kuribayashi", "Tatsuki", ""], ["Hidaka", "Masatoshi", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2010.04335", "submitter": "Priyanshu Kumar", "authors": "Priyanshu Kumar and Aadarsh Singh", "title": "NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative\n  COVID-19 Tweets using Ensembling and Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We experiment with COVID-Twitter-BERT and RoBERTa models to identify\ninformative COVID-19 tweets. We further experiment with adversarial training to\nmake our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains\na F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task\n2 and ranks 1st on the leaderboard. The ensemble of the models trained using\nadversarial training also produces similar result.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 02:46:51 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kumar", "Priyanshu", ""], ["Singh", "Aadarsh", ""]]}, {"id": "2010.04344", "submitter": "Andrea Madotto Mr", "authors": "Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth Dathathri,\n  Pascale Fung", "title": "Plug-and-Play Conversational Models", "comments": "Accepted in EMNLP findings, and code available at\n  https://github.com/andreamad8/PPCM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable progress made towards conversational models that\ngenerate coherent and fluent responses; however, this often involves training\nlarge language models on large dialogue datasets, such as Reddit. These large\nconversational models provide little control over the generated responses, and\nthis control is further limited in the absence of annotated conversational\ndatasets for attribute specific generation that can be used for fine-tuning the\nmodel. In this paper, we first propose and evaluate plug-and-play methods for\ncontrollable response generation, which does not require dialogue specific\ndatasets and does not rely on fine-tuning a large model. While effective, the\ndecoding procedure induces considerable computational overhead, rendering the\nconversational model unsuitable for interactive usage. To overcome this, we\nintroduce an approach that does not require further computation at decoding\ntime, while also does not require any fine-tuning of a large language model. We\ndemonstrate, through extensive automatic and human evaluation, a high degree of\ncontrol over the generated conversational responses with regard to multiple\ndesired attributes, while being fluent.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:17:51 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Madotto", "Andrea", ""], ["Ishii", "Etsuko", ""], ["Lin", "Zhaojiang", ""], ["Dathathri", "Sumanth", ""], ["Fung", "Pascale", ""]]}, {"id": "2010.04355", "submitter": "Shang-Wen Li", "authors": "Jin Cao, Jun Wang, Wael Hamza, Kelly Vanee, Shang-Wen Li", "title": "Style Attuned Pre-training and Parameter Efficient Fine-tuning for\n  Spoken Language Understanding", "comments": "Accepted at INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have yielded state-of-the-art results in deciphering spoken\nlanguage understanding (SLU) problems; however, these models require a\nsignificant amount of domain-specific labeled examples for training, which is\nprohibitively expensive. While pre-trained language models like BERT have been\nshown to capture a massive amount of knowledge by learning from unlabeled\ncorpora and solve SLU using fewer labeled examples for adaption, the encoding\nof knowledge is implicit and agnostic to downstream tasks. Such encoding\nresults in model inefficiencies in parameter usage: an entirely new model is\nrequired for every domain. To address these challenges, we introduce a novel\nSLU framework, comprising a conversational language modeling (CLM) pre-training\ntask and a light encoder architecture. The CLM pre-training enables networks to\ncapture the representation of the language in conversation style with the\npresence of ASR errors. The light encoder architecture separates the shared\npre-trained networks from the mappings of generally encoded knowledge to\nspecific domains of SLU, allowing for the domain adaptation to be performed\nsolely at the light encoder and thus increasing efficiency. With the framework,\nwe match the performance of state-of-the-art SLU results on Alexa internal\ndatasets and on two public ones (ATIS, SNIPS), adding only 4.4% parameters per\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:53:37 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Cao", "Jin", ""], ["Wang", "Jun", ""], ["Hamza", "Wael", ""], ["Vanee", "Kelly", ""], ["Li", "Shang-Wen", ""]]}, {"id": "2010.04361", "submitter": "Mehdi Rezaee", "authors": "Mehdi Rezaee and Francis Ferraro", "title": "Event Representation with Sequential, Semi-Supervised Discrete Variables", "comments": "In Proceedings of the 2021 Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics (NAACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of event modeling and understanding, we propose a new\nmethod for neural sequence modeling that takes partially-observed sequences of\ndiscrete, external knowledge into account. We construct a sequential neural\nvariational autoencoder, which uses Gumbel-Softmax reparametrization within a\ncarefully defined encoder, to allow for successful backpropagation during\ntraining. The core idea is to allow semi-supervised external discrete knowledge\nto guide, but not restrict, the variational latent parameters during training.\nOur experiments indicate that our approach not only outperforms multiple\nbaselines and the state-of-the-art in narrative script induction, but also\nconverges more quickly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:05:49 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 18:54:51 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 00:44:03 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Rezaee", "Mehdi", ""], ["Ferraro", "Francis", ""]]}, {"id": "2010.04362", "submitter": "Brian Lester", "authors": "Brian Lester, Daniel Pressel, Amy Hemmeter, Sagnik Ray Choudhury,\n  Srinivas Bangalore", "title": "Constrained Decoding for Computationally Efficient Named Entity\n  Recognition Taggers", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art models for named entity recognition (NER) are neural\nmodels with a conditional random field (CRF) as the final layer. Entities are\nrepresented as per-token labels with a special structure in order to decode\nthem into spans. Current work eschews prior knowledge of how the span encoding\nscheme works and relies on the CRF learning which transitions are illegal and\nwhich are not to facilitate global coherence. We find that by constraining the\noutput to suppress illegal transitions we can train a tagger with a\ncross-entropy loss twice as fast as a CRF with differences in F1 that are\nstatistically insignificant, effectively eliminating the need for a CRF. We\nanalyze the dynamics of tag co-occurrence to explain when these constraints are\nmost effective and provide open source implementations of our tagger in both\nPyTorch and TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:07:52 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Lester", "Brian", ""], ["Pressel", "Daniel", ""], ["Hemmeter", "Amy", ""], ["Choudhury", "Sagnik Ray", ""], ["Bangalore", "Srinivas", ""]]}, {"id": "2010.04372", "submitter": "Zhengxuan Wu", "authors": "Zhengxuan Wu, Desmond C. Ong", "title": "Pragmatically Informative Color Generation by Grounding Contextual\n  Modifiers", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding language in contextual information is crucial for fine-grained\nnatural language understanding. One important task that involves grounding\ncontextual modifiers is color generation. Given a reference color \"green\", and\na modifier \"bluey\", how does one generate a color that could represent \"bluey\ngreen\"? We propose a computational pragmatics model that formulates this color\ngeneration task as a recursive game between speakers and listeners. In our\nmodel, a pragmatic speaker reasons about the inferences that a listener would\nmake, and thus generates a modified color that is maximally informative to help\nthe listener recover the original referents. In this paper, we show that\nincorporating pragmatic information provides significant improvements in\nperformance compared with other state-of-the-art deep learning models where\npragmatic inference and flexibility in representing colors from a large\ncontinuous space are lacking. Our model has an absolute 98% increase in\nperformance for the test cases where the reference colors are unseen during\ntraining, and an absolute 40% increase in performance for the test cases where\nboth the reference colors and the modifiers are unseen during training.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 04:54:54 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Ong", "Desmond C.", ""]]}, {"id": "2010.04373", "submitter": "Brian Lester", "authors": "Brian Lester", "title": "iobes: A Library for Span-Level Processing", "comments": "NLP-OSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in natural language processing, such as named entity recognition\nand slot-filling, involve identifying and labeling specific spans of text. In\norder to leverage common models, these tasks are often recast as sequence\nlabeling tasks. Each token is given a label and these labels are prefixed with\nspecial tokens such as B- or I-. After a model assigns labels to each token,\nthese prefixes are used to group the tokens into spans.\n  Properly parsing these annotations is critical for producing fair and\ncomparable metrics; however, despite its importance, there is not an\neasy-to-use, standardized, programmatically integratable library to help work\nwith span labeling. To remedy this, we introduce our open-source library,\niobes. iobes is used for parsing, converting, and processing spans represented\nas token level decisions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 05:03:48 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Lester", "Brian", ""]]}, {"id": "2010.04377", "submitter": "Tanmoy Chakraborty", "authors": "Sarah Masud, Subhabrata Dutta, Sakshi Makkar, Chhavi Jain, Vikram\n  Goyal, Amitava Das, Tanmoy Chakraborty", "title": "Hate is the New Infodemic: A Topic-aware Modeling of Hate Speech\n  Diffusion on Twitter", "comments": "6 table, 9 figures, Full paper in 37th International Conference on\n  Data Engineering (ICDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online hate speech, particularly over microblogging platforms like Twitter,\nhas emerged as arguably the most severe issue of the past decade. Several\ncountries have reported a steep rise in hate crimes infuriated by malicious\nhate campaigns. While the detection of hate speech is one of the emerging\nresearch areas, the generation and spread of topic-dependent hate in the\ninformation network remain under-explored. In this work, we focus on exploring\nuser behaviour, which triggers the genesis of hate speech on Twitter and how it\ndiffuses via retweets. We crawl a large-scale dataset of tweets, retweets, user\nactivity history, and follower networks, comprising over 161 million tweets\nfrom more than $41$ million unique users. We also collect over 600k\ncontemporary news articles published online. We characterize different signals\nof information that govern these dynamics. Our analyses differentiate the\ndiffusion dynamics in the presence of hate from usual information diffusion.\nThis motivates us to formulate the modelling problem in a topic-aware setting\nwith real-world knowledge. For predicting the initiation of hate speech for any\ngiven hashtag, we propose multiple feature-rich models, with the best\nperforming one achieving a macro F1 score of 0.65. Meanwhile, to predict the\nretweet dynamics on Twitter, we propose RETINA, a novel neural architecture\nthat incorporates exogenous influence using scaled dot-product attention.\nRETINA achieves a macro F1-score of 0.85, outperforming multiple\nstate-of-the-art models. Our analysis reveals the superlative power of RETINA\nto predict the retweet dynamics of hateful content compared to the existing\ndiffusion models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 05:43:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Masud", "Sarah", ""], ["Dutta", "Subhabrata", ""], ["Makkar", "Sakshi", ""], ["Jain", "Chhavi", ""], ["Goyal", "Vikram", ""], ["Das", "Amitava", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2010.04379", "submitter": "Ryosuke Kohita", "authors": "Ryosuke Kohita, Akifumi Wachi, Yang Zhao, Ryuki Tachibana", "title": "Q-learning with Language Model for Edit-based Unsupervised Summarization", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised methods are promising for abstractive text summarization in that\nthe parallel corpora is not required. However, their performance is still far\nfrom being satisfied, therefore research on promising solutions is on-going. In\nthis paper, we propose a new approach based on Q-learning with an edit-based\nsummarization. The method combines two key modules to form an Editorial Agent\nand Language Model converter (EALM). The agent predicts edit actions (e.t.,\ndelete, keep, and replace), and then the LM converter deterministically\ngenerates a summary on the basis of the action signals. Q-learning is leveraged\nto train the agent to produce proper edit actions. Experimental results show\nthat EALM delivered competitive performance compared with the previous\nencoder-decoder-based methods, even with truly zero paired data (i.e., no\nvalidation set). Defining the task as Q-learning enables us not only to develop\na competitive method but also to make the latest techniques in reinforcement\nlearning available for unsupervised summarization. We also conduct qualitative\nanalysis, providing insights into future study on unsupervised summarizers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 05:47:00 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kohita", "Ryosuke", ""], ["Wachi", "Akifumi", ""], ["Zhao", "Yang", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "2010.04380", "submitter": "Shuhao Gu", "authors": "Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie\n  Zhou, Dong Yu", "title": "Token-level Adaptive Training for Neural Machine Translation", "comments": "12 pages; Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a token imbalance phenomenon in natural language as different\ntokens appear with different frequencies, which leads to different learning\ndifficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT\nmodel usually adopts trivial equal-weighted objectives for target tokens with\ndifferent frequencies and tends to generate more high-frequency tokens and less\nlow-frequency tokens compared with the golden token distribution. However,\nlow-frequency tokens may carry critical semantic information that will affect\nthe translation quality once they are neglected. In this paper, we explored\ntarget token-level adaptive objectives based on token frequencies to assign\nappropriate weights for each target token during training. We aimed that those\nmeaningful but relatively low-frequency words could be assigned with larger\nweights in objectives to encourage the model to pay more attention to these\ntokens. Our method yields consistent improvements in translation quality on\nZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain\nmore low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases\ncompared with baseline, respectively. Further analyses show that our method can\nalso improve the lexical diversity of translation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 05:55:05 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Gu", "Shuhao", ""], ["Zhang", "Jinchao", ""], ["Meng", "Fandong", ""], ["Feng", "Yang", ""], ["Xie", "Wanying", ""], ["Zhou", "Jie", ""], ["Yu", "Dong", ""]]}, {"id": "2010.04383", "submitter": "Yan Zhang", "authors": "Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu\n  Liu, Lidong Bing", "title": "Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text\n  Generation", "comments": "Accepted to EMNLP 2020, long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AMR-to-text generation is used to transduce Abstract Meaning Representation\nstructures (AMR) into text. A key challenge in this task is to efficiently\nlearn effective graph representations. Previously, Graph Convolution Networks\n(GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to\ncapture non-local information and additionally, they follow a local\n(first-order) information aggregation scheme. To account for these issues,\nlarger and deeper GCN models are required to capture more complex interactions.\nIn this paper, we introduce a dynamic fusion mechanism, proposing Lightweight\nDynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local\ninteractions by synthesizing higher order information from the input graphs. We\nfurther develop two novel parameter saving strategies based on the group graph\nconvolutions and weight tied convolutions to reduce memory usage and model\ncomplexity. With the help of these strategies, we are able to train a model\nwith fewer parameters while maintaining the model capacity. Experiments\ndemonstrate that LDGCNs outperform state-of-the-art models on two benchmark\ndatasets for AMR-to-text generation with significantly fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:03:46 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhang", "Yan", ""], ["Guo", "Zhijiang", ""], ["Teng", "Zhiyang", ""], ["Lu", "Wei", ""], ["Cohen", "Shay B.", ""], ["Liu", "Zuozhu", ""], ["Bing", "Lidong", ""]]}, {"id": "2010.04388", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, S\\\"oren Auer", "title": "Sentence, Phrase, and Triple Annotations to Build a Knowledge Graph of\n  Natural Language Processing Contributions -- A Trial Dataset", "comments": "22 pages, 9 figures, 4 tables", "journal-ref": "Journal of Data and Information Science, 6(3) (2021)", "doi": "10.2478/jdis-2021-0023", "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Purpose: The aim of this work is to normalize the NLPCONTRIBUTIONS scheme\n(henceforward, NLPCONTRIBUTIONGRAPH) to structure, directly from article\nsentences, the contributions information in Natural Language Processing (NLP)\nscholarly articles via a two-stage annotation methodology: 1) pilot stage - to\ndefine the scheme (described in prior work); and 2) adjudication stage - to\nnormalize the graphing model (the focus of this paper).\n  Design/methodology/approach: We re-annotate, a second time, the\ncontributions-pertinent information across 50 prior-annotated NLP scholarly\narticles in terms of a data pipeline comprising: contribution-centered\nsentences, phrases, and triple statements. To this end, specifically, care was\ntaken in the adjudication annotation stage to reduce annotation noise while\nformulating the guidelines for our proposed novel NLP contributions structuring\nand graphing scheme.\n  Findings: The application of NLPCONTRIBUTIONGRAPH on the 50 articles resulted\nfinally in a dataset of 900 contribution-focused sentences, 4,702\ncontribution-information-centered phrases, and 2,980 surface-structured\ntriples. The intra-annotation agreement between the first and second stages, in\nterms of F1, was 67.92% for sentences, 41.82% for phrases, and 22.31% for\ntriple statements indicating that with increased granularity of the\ninformation, the annotation decision variance is greater.\n  Practical Implications: We demonstrate NLPCONTRIBUTIONGRAPH data integrated\ninto the Open Research Knowledge Graph (ORKG), a next-generation KG-based\ndigital library with intelligent computations enabled over structured scholarly\nknowledge, as a viable aid to assist researchers in their day-to-day tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:45:35 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:24:45 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 06:08:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2010.04389", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng\n  Ji, Meng Jiang", "title": "A Survey of Knowledge-Enhanced Text Generation", "comments": "42 pages, 12 tables, 8 figures; Under review at ACM CSUR (revised\n  manuscript)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:46:46 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 07:09:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yu", "Wenhao", ""], ["Zhu", "Chenguang", ""], ["Li", "Zaitang", ""], ["Hu", "Zhiting", ""], ["Wang", "Qingyun", ""], ["Ji", "Heng", ""], ["Jiang", "Meng", ""]]}, {"id": "2010.04395", "submitter": "Sunil Gundapu", "authors": "Sunil Gundapu, Radhika Mamidi", "title": "gundapusunil at SemEval-2020 Task 9: Syntactic Semantic LSTM\n  Architecture for SENTIment Analysis of Code-MIXed Data", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The phenomenon of mixing the vocabulary and syntax of multiple languages\nwithin the same utterance is called Code-Mixing. This is more evident in\nmultilingual societies. In this paper, we have developed a system for SemEval\n2020: Task 9 on Sentiment Analysis for Code-Mixed Social Media Text. Our system\nfirst generates two types of embeddings for the social media text. In those,\nthe first one is character level embeddings to encode the character level\ninformation and to handle the out-of-vocabulary entries and the second one is\nFastText word embeddings for capturing morphology and semantics. These two\nembeddings were passed to the LSTM network and the system outperformed the\nbaseline model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:07:04 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Gundapu", "Sunil", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2010.04411", "submitter": "Xiangpeng Wei", "authors": "Xiangpeng Wei and Heng Yu and Yue Hu and Rongxiang Weng and Luxi Xing\n  and Weihua Luo", "title": "Uncertainty-Aware Semantic Augmentation for Neural Machine Translation", "comments": "Accepted to EMNLP 2020, 12 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a sequence-to-sequence generation task, neural machine translation (NMT)\nnaturally contains intrinsic uncertainty, where a single sentence in one\nlanguage has multiple valid counterparts in the other. However, the dominant\nmethods for NMT only observe one of them from the parallel corpora for the\nmodel training but have to deal with adequate variations under the same meaning\nat inference. This leads to a discrepancy of the data distribution between the\ntraining and the inference phases. To address this problem, we propose\nuncertainty-aware semantic augmentation, which explicitly captures the\nuniversal semantic information among multiple semantically-equivalent source\nsentences and enhances the hidden representations with this information for\nbetter translations. Extensive experiments on various translation tasks reveal\nthat our approach significantly outperforms the strong baselines and the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:48:09 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wei", "Xiangpeng", ""], ["Yu", "Heng", ""], ["Hu", "Yue", ""], ["Weng", "Rongxiang", ""], ["Xing", "Luxi", ""], ["Luo", "Weihua", ""]]}, {"id": "2010.04429", "submitter": "Patrick Lumban Tobing", "authors": "Patrick Lumban Tobing, Yi-Chiao Wu, Tomoki Toda", "title": "Baseline System of Voice Conversion Challenge 2020 with Cyclic\n  Variational Autoencoder and Parallel WaveGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a description of the baseline system of Voice\nConversion Challenge (VCC) 2020 with a cyclic variational autoencoder\n(CycleVAE) and Parallel WaveGAN (PWG), i.e., CycleVAEPWG. CycleVAE is a\nnonparallel VAE-based voice conversion that utilizes converted acoustic\nfeatures to consider cyclically reconstructed spectra during optimization. On\nthe other hand, PWG is a non-autoregressive neural vocoder that is based on a\ngenerative adversarial network for a high-quality and fast waveform generator.\nIn practice, the CycleVAEPWG system can be straightforwardly developed with the\nVCC 2020 dataset using a unified model for both Task 1 (intralingual) and Task\n2 (cross-lingual), where our open-source implementation is available at\nhttps://github.com/bigpon/vcc20_baseline_cyclevae. The results of VCC 2020 have\ndemonstrated that the CycleVAEPWG baseline achieves the following: 1) a mean\nopinion score (MOS) of 2.87 in naturalness and a speaker similarity percentage\n(Sim) of 75.37% for Task 1, and 2) a MOS of 2.56 and a Sim of 56.46% for Task\n2, showing an approximately or nearly average score for naturalness and an\nabove average score for speaker similarity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:25:38 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Tobing", "Patrick Lumban", ""], ["Wu", "Yi-Chiao", ""], ["Toda", "Tomoki", ""]]}, {"id": "2010.04438", "submitter": "Harris Chan", "authors": "Harris Chan, Jamie Kiros, William Chan", "title": "Multichannel Generative Language Model: Learning All Possible\n  Factorizations Within and Across Channels", "comments": "10 pages (+3 appendix), 11 figures, 5 tables. Accepted to Findings of\n  EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A channel corresponds to a viewpoint or transformation of an underlying\nmeaning. A pair of parallel sentences in English and French express the same\nunderlying meaning, but through two separate channels corresponding to their\nlanguages. In this work, we present the Multichannel Generative Language Model\n(MGLM). MGLM is a generative joint distribution model over channels. MGLM\nmarginalizes over all possible factorizations within and across all channels.\nMGLM endows flexible inference, including unconditional generation, conditional\ngeneration (where 1 channel is observed and other channels are generated), and\npartially observed generation (where incomplete observations are spread across\nall the channels). We experiment with the Multi30K dataset containing English,\nFrench, Czech, and German. We demonstrate experiments with unconditional,\nconditional, and partially conditional generation. We provide qualitative\nsamples sampled unconditionally from the generative joint distribution. We also\nquantitatively analyze the quality-diversity trade-offs and find MGLM\noutperforms traditional bilingual discriminative models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:52:24 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chan", "Harris", ""], ["Kiros", "Jamie", ""], ["Chan", "William", ""]]}, {"id": "2010.04446", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Patrick Lumban Tobing, Yi-Chiao Wu, Kazuhiro\n  Kobayashi, Tomoki Toda", "title": "The NU Voice Conversion System for the Voice Conversion Challenge 2020:\n  On the Effectiveness of Sequence-to-sequence Models and Autoregressive Neural\n  Vocoders", "comments": "Accepted to the ISCA Joint Workshop for the Blizzard Challenge and\n  Voice Conversion Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the voice conversion (VC) systems developed at\nNagoya University (NU) for the Voice Conversion Challenge 2020 (VCC2020). We\naim to determine the effectiveness of two recent significant technologies in\nVC: sequence-to-sequence (seq2seq) models and autoregressive (AR) neural\nvocoders. Two respective systems were developed for the two tasks in the\nchallenge: for task 1, we adopted the Voice Transformer Network, a\nTransformer-based seq2seq VC model, and extended it with synthetic parallel\ndata to tackle nonparallel data; for task 2, we used the frame-based cyclic\nvariational autoencoder (CycleVAE) to model the spectral features of a speech\nwaveform and the AR WaveNet vocoder with additional fine-tuning. By comparing\nwith the baseline systems, we confirmed that the seq2seq modeling can improve\nthe conversion similarity and that the use of AR vocoders can improve the\nnaturalness of the converted speech.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:19:37 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Tobing", "Patrick Lumban", ""], ["Wu", "Yi-Chiao", ""], ["Kobayashi", "Kazuhiro", ""], ["Toda", "Tomoki", ""]]}, {"id": "2010.04480", "submitter": "Marina Fomicheva", "authors": "Marina Fomicheva, Shuo Sun, Erick Fonseca, Fr\\'ed\\'eric Blain, Vishrav\n  Chaudhary, Francisco Guzm\\'an, Nina Lopatina, Lucia Specia and Andr\\'e F. T.\n  Martins", "title": "MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MLQE-PE, a new dataset for Machine Translation (MT) Quality\nEstimation (QE) and Automatic Post-Editing (APE). The dataset contains seven\nlanguage pairs, with human labels for 9,000 translations per language pair in\nthe following formats: sentence-level direct assessments and post-editing\neffort, and word-level good/bad labels. It also contains the post-edited\nsentences, as well as titles of the articles where the sentences were extracted\nfrom, and the neural MT models used to translate the text.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:12:02 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Fomicheva", "Marina", ""], ["Sun", "Shuo", ""], ["Fonseca", "Erick", ""], ["Blain", "Fr\u00e9d\u00e9ric", ""], ["Chaudhary", "Vishrav", ""], ["Guzm\u00e1n", "Francisco", ""], ["Lopatina", "Nina", ""], ["Specia", "Lucia", ""], ["Martins", "Andr\u00e9 F. T.", ""]]}, {"id": "2010.04482", "submitter": "Sunil Gundapu", "authors": "Sunil Gundapu, Radhika Mamidi", "title": "Word Level Language Identification in English Telugu Code Mixed Data", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a multilingual or sociolingual configuration Intra-sentential Code\nSwitching (ICS) or Code Mixing (CM) is frequently observed nowadays. In the\nworld, most of the people know more than one language. CM usage is especially\napparent in social media platforms. Moreover, ICS is particularly significant\nin the context of technology, health, and law where conveying the upcoming\ndevelopments are difficult in one's native language. In applications like\ndialog systems, machine translation, semantic parsing, shallow parsing, etc. CM\nand Code Switching pose serious challenges. To do any further advancement in\ncode-mixed data, the necessary step is Language Identification. In this paper,\nwe present a study of various models - Nave Bayes Classifier, Random Forest\nClassifier, Conditional Random Field (CRF), and Hidden Markov Model (HMM) for\nLanguage Identification in English - Telugu Code Mixed Data. Considering the\npaucity of resources in code mixed languages, we proposed the CRF model and HMM\nmodel for word level language identification. Our best performing system is\nCRF-based with an f1-score of 0.91.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:15:06 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Gundapu", "Sunil", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2010.04486", "submitter": "Pierangelo Lombardo", "authors": "Pierangelo Lombardo, Alessio Boiardi, Luca Colombo, Angelo Schiavone,\n  Nicol\\`o Tamagnone", "title": "Top-Rank-Focused Adaptive Vote Collection for the Evaluation of\n  Domain-Specific Semantic Models", "comments": "This is a pre-print of an article published in the proceedings of the\n  2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "journal-ref": "Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) (pp. 3081-3093)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of domain-specific applications of semantic models, boosted by the\nrecent achievements of unsupervised embedding learning algorithms, demands\ndomain-specific evaluation datasets. In many cases, content-based recommenders\nbeing a prime example, these models are required to rank words or texts\naccording to their semantic relatedness to a given concept, with particular\nfocus on top ranks. In this work, we give a threefold contribution to address\nthese requirements: (i) we define a protocol for the construction, based on\nadaptive pairwise comparisons, of a relatedness-based evaluation dataset\ntailored on the available resources and optimized to be particularly accurate\nin top-rank evaluation; (ii) we define appropriate metrics, extensions of\nwell-known ranking correlation coefficients, to evaluate a semantic model via\nthe aforementioned dataset by taking into account the greater significance of\ntop ranks. Finally, (iii) we define a stochastic transitivity model to simulate\nsemantic-driven pairwise comparisons, which confirms the effectiveness of the\nproposed dataset construction protocol.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:20:58 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lombardo", "Pierangelo", ""], ["Boiardi", "Alessio", ""], ["Colombo", "Luca", ""], ["Schiavone", "Angelo", ""], ["Tamagnone", "Nicol\u00f2", ""]]}, {"id": "2010.04505", "submitter": "Yu Wan", "authors": "Yu Wan, Baosong Yang, Derek F. Wong, Yikai Zhou, Lidia S. Chao, Haibo\n  Zhang, Boxing Chen", "title": "Self-Paced Learning for Neural Machine Translation", "comments": "Accepted by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have proven that the training of neural machine translation\n(NMT) can be facilitated by mimicking the learning process of humans.\nNevertheless, achievements of such kind of curriculum learning rely on the\nquality of artificial schedule drawn up with the handcrafted features, e.g.\nsentence length or word rarity. We ameliorate this procedure with a more\nflexible manner by proposing self-paced learning, where NMT model is allowed to\n1) automatically quantify the learning confidence over training examples; and\n2) flexibly govern its learning via regulating the loss in each iteration step.\nExperimental results over multiple translation tasks demonstrate that the\nproposed model yields better performance than strong baselines and those models\ntrained with human-designed curricula on both translation quality and\nconvergence speed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:33:16 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 09:02:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wan", "Yu", ""], ["Yang", "Baosong", ""], ["Wong", "Derek F.", ""], ["Zhou", "Yikai", ""], ["Chao", "Lidia S.", ""], ["Zhang", "Haibo", ""], ["Chen", "Boxing", ""]]}, {"id": "2010.04520", "submitter": "Xuefeng Bai", "authors": "Xuefeng Bai, Linfeng Song and Yue Zhang", "title": "Online Back-Parsing for AMR-to-Text Generation", "comments": "To appear in EMNLP2020 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  AMR-to-text generation aims to recover a text containing the same meaning as\nan input AMR graph. Current research develops increasingly powerful graph\nencoders to better represent AMR graphs, with decoders based on standard\nlanguage modeling being used to generate outputs. We propose a decoder that\nback predicts projected AMR graphs on the target sentence during text\ngeneration. As the result, our outputs can better preserve the input meaning\nthan standard decoders. Experiments on two AMR benchmarks show the superiority\nof our model over the previous state-of-the-art system based on graph\nTransformer.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:08:14 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bai", "Xuefeng", ""], ["Song", "Linfeng", ""], ["Zhang", "Yue", ""]]}, {"id": "2010.04529", "submitter": "Leyang Cui", "authors": "Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie,\n  Yue Zhang", "title": "What Have We Achieved on Text Summarization?", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has led to significant improvement in text summarization with\nvarious methods investigated and improved ROUGE scores reported over the years.\nHowever, gaps still exist between summaries produced by automatic summarizers\nand human professionals. Aiming to gain more understanding of summarization\nsystems with respect to their strengths and limits on a fine-grained syntactic\nand semantic level, we consult the Multidimensional Quality Metric(MQM) and\nquantify 8 major sources of errors on 10 representative summarization models\nmanually. Primarily, we find that 1) under similar settings, extractive\nsummarizers are in general better than their abstractive counterparts thanks to\nstrength in faithfulness and factual-consistency; 2) milestone techniques such\nas copy, coverage and hybrid extractive/abstractive methods do bring specific\nimprovements but also demonstrate limitations; 3) pre-training techniques, and\nin particular sequence-to-sequence pre-training, are highly effective for\nimproving text summarization, with BART giving the best results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:39:33 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Huang", "Dandan", ""], ["Cui", "Leyang", ""], ["Yang", "Sen", ""], ["Bao", "Guangsheng", ""], ["Wang", "Kun", ""], ["Xie", "Jun", ""], ["Zhang", "Yue", ""]]}, {"id": "2010.04532", "submitter": "Carolina Scarton", "authors": "Carolina Scarton and Diego F. Silva and Kalina Bontcheva", "title": "Measuring What Counts: The case of Rumour Stance Classification", "comments": "Accepted to AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stance classification can be a powerful tool for understanding whether and\nwhich users believe in online rumours. The task aims to automatically predict\nthe stance of replies towards a given rumour, namely support, deny, question,\nor comment. Numerous methods have been proposed and their performance compared\nin the RumourEval shared tasks in 2017 and 2019. Results demonstrated that this\nis a challenging problem since naturally occurring rumour stance data is highly\nimbalanced. This paper specifically questions the evaluation metrics used in\nthese shared tasks. We re-evaluate the systems submitted to the two RumourEval\ntasks and show that the two widely adopted metrics -- accuracy and macro-F1 --\nare not robust for the four-class imbalanced task of rumour stance\nclassification, as they wrongly favour systems with highly skewed accuracy\ntowards the majority class. To overcome this problem, we propose new evaluation\nmetrics for rumour stance detection. These are not only robust to imbalanced\ndata but also score higher systems that are capable of recognising the two most\ninformative minority classes (support and deny).\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 12:45:27 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Scarton", "Carolina", ""], ["Silva", "Diego F.", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "2010.04543", "submitter": "Carolina Scarton", "authors": "Jo\\~ao A. Leite and Diego F. Silva and Kalina Bontcheva and Carolina\n  Scarton", "title": "Toxic Language Detection in Social Media for Brazilian Portuguese: New\n  Dataset and Multilingual Analysis", "comments": "Accepted to AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech and toxic comments are a common concern of social media platform\nusers. Although these comments are, fortunately, the minority in these\nplatforms, they are still capable of causing harm. Therefore, identifying these\ncomments is an important task for studying and preventing the proliferation of\ntoxicity in social media. Previous work in automatically detecting toxic\ncomments focus mainly in English, with very few work in languages like\nBrazilian Portuguese. In this paper, we propose a new large-scale dataset for\nBrazilian Portuguese with tweets annotated as either toxic or non-toxic or in\ndifferent types of toxicity. We present our dataset collection and annotation\nprocess, where we aimed to select candidates covering multiple demographic\ngroups. State-of-the-art BERT models were able to achieve 76% macro-F1 score\nusing monolingual data in the binary case. We also show that large-scale\nmonolingual data is still needed to create more accurate models, despite recent\nadvances in multilingual approaches. An error analysis and experiments with\nmulti-label classification show the difficulty of classifying certain types of\ntoxic comments that appear less frequently in our data and highlights the need\nto develop models that are aware of different categories of toxicity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:05:19 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Leite", "Jo\u00e3o A.", ""], ["Silva", "Diego F.", ""], ["Bontcheva", "Kalina", ""], ["Scarton", "Carolina", ""]]}, {"id": "2010.04576", "submitter": "Cheng-Te Li", "authors": "Hsin-Yu Chen, Cheng-Te Li", "title": "HENIN: Learning Heterogeneous Neural Interaction Networks for\n  Explainable Cyberbullying Detection on Social Media", "comments": "EMNLP 2020 long paper. Code is available at\n  https://github.com/HsinYu7330/HENIN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the computational detection of cyberbullying, existing work largely\nfocused on building generic classifiers that rely exclusively on text analysis\nof social media sessions. Despite their empirical success, we argue that a\ncritical missing piece is the model explainability, i.e., why a particular\npiece of media session is detected as cyberbullying. In this paper, therefore,\nwe propose a novel deep model, HEterogeneous Neural Interaction Networks\n(HENIN), for explainable cyberbullying detection. HENIN contains the following\ncomponents: a comment encoder, a post-comment co-attention sub-network, and\nsession-session and post-post interaction extractors. Extensive experiments\nconducted on real datasets exhibit not only the promising performance of HENIN,\nbut also highlight evidential comments so that one can understand why a media\nsession is identified as cyberbullying.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:44:34 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chen", "Hsin-Yu", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2010.04582", "submitter": "Wendi Ren", "authors": "Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell,\n  Chao Zhang", "title": "Denoising Multi-Source Weak Supervision for Neural Text Classification", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.334", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning neural text classifiers without using any\nlabeled data, but only easy-to-provide rules as multiple weak supervision\nsources. This problem is challenging because rule-induced weak labels are often\nnoisy and incomplete. To address these two challenges, we design a label\ndenoiser, which estimates the source reliability using a conditional soft\nattention mechanism and then reduces label noise by aggregating rule-annotated\nweak labels. The denoised pseudo labels then supervise a neural classifier to\npredicts soft labels for unmatched samples, which address the rule coverage\nissue. We evaluate our model on five benchmarks for sentiment, topic, and\nrelation classifications. The results show that our model outperforms\nstate-of-the-art weakly-supervised and semi-supervised methods consistently,\nand achieves comparable performance with fully-supervised methods even without\nany labeled data. Our code can be found at\nhttps://github.com/weakrules/Denoise-multi-weak-sources.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:57:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ren", "Wendi", ""], ["Li", "Yinghao", ""], ["Su", "Hanting", ""], ["Kartchner", "David", ""], ["Mitchell", "Cassie", ""], ["Zhang", "Chao", ""]]}, {"id": "2010.04606", "submitter": "Gon\\c{c}alo Mordido", "authors": "Gon\\c{c}alo Mordido and Christoph Meinel", "title": "Mark-Evaluate: Assessing Language Generation using Population Estimation\n  Methods", "comments": "Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of metrics to assess language generation derived from\npopulation estimation methods widely used in ecology. More specifically, we use\nmark-recapture and maximum-likelihood methods that have been applied over the\npast several decades to estimate the size of closed populations in the wild. We\npropose three novel metrics: ME$_\\text{Petersen}$ and ME$_\\text{CAPTURE}$,\nwhich retrieve a single-valued assessment, and ME$_\\text{Schnabel}$ which\nreturns a double-valued metric to assess the evaluation set in terms of quality\nand diversity, separately. In synthetic experiments, our family of methods is\nsensitive to drops in quality and diversity. Moreover, our methods show a\nhigher correlation to human evaluation than existing metrics on several\nchallenging tasks, namely unconditional language generation, machine\ntranslation, and text summarization.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:31:53 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mordido", "Gon\u00e7alo", ""], ["Meinel", "Christoph", ""]]}, {"id": "2010.04609", "submitter": "Guohou Shan", "authors": "Guohou Shan, James Foulds, Shimei Pan", "title": "Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification", "comments": "11 pages, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text features that are correlated with class labels, but do not directly\ncause them, are sometimesuseful for prediction, but they may not be insightful.\nAs an alternative to traditional correlation-basedfeature selection, causal\ninference could reveal more principled, meaningful relationships betweentext\nfeatures and labels. To help researchers gain insight into text data, e.g. for\nsocial scienceapplications, in this paper we investigate a class of\nmatching-based causal inference methods fortext feature selection. Features\nused in document classification are often high dimensional, howeverexisting\ncausal feature selection methods use Propensity Score Matching (PSM) which is\nknown to beless effective in high-dimensional spaces. We propose a new causal\nfeature selection framework thatcombines dimension reduction with causal\ninference to improve text feature selection. Experiments onboth synthetic and\nreal-world data demonstrate the promise of our methods in improving\nclassificationand enhancing interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:36:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Shan", "Guohou", ""], ["Foulds", "James", ""], ["Pan", "Shimei", ""]]}, {"id": "2010.04625", "submitter": "Omar Shaikh", "authors": "Omar Shaikh, Jiaao Chen, Jon Saad-Falcon, Duen Horng Chau and Diyi\n  Yang", "title": "Examining the Ordering of Rhetorical Strategies in Persuasive Requests", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting how persuasive language influences audiences has implications\nacross many domains like advertising, argumentation, and propaganda. Persuasion\nrelies on more than a message's content. Arranging the order of the message\nitself (i.e., ordering specific rhetorical strategies) also plays an important\nrole. To examine how strategy orderings contribute to persuasiveness, we first\nutilize a Variational Autoencoder model to disentangle content and rhetorical\nstrategies in textual requests from a large-scale loan request corpus. We then\nvisualize interplay between content and strategy through an attentional LSTM\nthat predicts the success of textual requests. We find that specific (orderings\nof) strategies interact uniquely with a request's content to impact success\nrate, and thus the persuasiveness of a request.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:10:44 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 00:37:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Shaikh", "Omar", ""], ["Chen", "Jiaao", ""], ["Saad-Falcon", "Jon", ""], ["Chau", "Duen Horng", ""], ["Yang", "Diyi", ""]]}, {"id": "2010.04637", "submitter": "Ludovica Pannitto", "authors": "Ludovica Pannitto and Aur\\'elie Herbelot", "title": "Recurrent babbling: evaluating the acquisition of grammar from limited\n  input data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been shown to capture various aspects\nof syntax from raw linguistic input. In most previous experiments, however,\nlearning happens over unrealistic corpora, which do not reflect the type and\namount of data a child would be exposed to. This paper remedies this state of\naffairs by training a Long Short-Term Memory network (LSTM) over a\nrealistically sized subset of child-directed input. The behaviour of the\nnetwork is analysed over time using a novel methodology which consists in\nquantifying the level of grammatical abstraction in the model's generated\noutput (its \"babbling\"), compared to the language it has been exposed to. We\nshow that the LSTM indeed abstracts new structuresas learning proceeds.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:30:05 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Pannitto", "Ludovica", ""], ["Herbelot", "Aur\u00e9lie", ""]]}, {"id": "2010.04640", "submitter": "Zhen Wu", "authors": "Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, Rui Xia", "title": "Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction", "comments": "Accepted by Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting\naspect terms and opinion terms from review in the form of opinion pairs or\nadditionally extracting sentiment polarity of aspect term to form opinion\ntriplet. Because of containing several opinion factors, the complete AFOE task\nis usually divided into multiple subtasks and achieved in the pipeline.\nHowever, pipeline approaches easily suffer from error propagation and\ninconvenience in real-world scenarios. To this end, we propose a novel tagging\nscheme, Grid Tagging Scheme (GTS), to address the AFOE task in an end-to-end\nfashion only with one unified grid tagging task. Additionally, we design an\neffective inference strategy on GTS to exploit mutual indication between\ndifferent opinion factors for more accurate extractions. To validate the\nfeasibility and compatibility of GTS, we implement three different GTS models\nrespectively based on CNN, BiLSTM, and BERT, and conduct experiments on the\naspect-oriented opinion pair extraction and opinion triplet extraction\ndatasets. Extensive experimental results indicate that GTS models outperform\nstrong baselines significantly and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:33:50 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 15:55:08 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wu", "Zhen", ""], ["Ying", "Chengcan", ""], ["Zhao", "Fei", ""], ["Fan", "Zhifang", ""], ["Dai", "Xinyu", ""], ["Xia", "Rui", ""]]}, {"id": "2010.04641", "submitter": "Zuchao Li", "authors": "Zuchao Li, Hai Zhao, Rui Wang, Kevin Parnow", "title": "High-order Semantic Role Labeling", "comments": "EMNLP 2020, ACL Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic role labeling is primarily used to identify predicates, arguments,\nand their semantic relationships. Due to the limitations of modeling methods\nand the conditions of pre-identified predicates, previous work has focused on\nthe relationships between predicates and arguments and the correlations between\narguments at most, while the correlations between predicates have been\nneglected for a long time. High-order features and structure learning were very\ncommon in modeling such correlations before the neural network era. In this\npaper, we introduce a high-order graph structure for the neural semantic role\nlabeling model, which enables the model to explicitly consider not only the\nisolated predicate-argument pairs but also the interaction between the\npredicate-argument pairs. Experimental results on 7 languages of the CoNLL-2009\nbenchmark show that the high-order structural learning techniques are\nbeneficial to the strong performing SRL models and further boost our baseline\nto achieve new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:33:54 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Zuchao", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""], ["Parnow", "Kevin", ""]]}, {"id": "2010.04650", "submitter": "Naomi Saphra", "authors": "Naomi Saphra and Adam Lopez", "title": "LSTMs Compose (and Learn) Bottom-Up", "comments": "Published in EMNLP Findings 2020. arXiv admin note: substantial text\n  overlap with arXiv:2004.13195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work in NLP shows that LSTM language models capture hierarchical\nstructure in language data. In contrast to existing work, we consider the\n\\textit{learning} process that leads to their compositional behavior. For a\ncloser look at how an LSTM's sequential representations are composed\nhierarchically, we present a related measure of Decompositional Interdependence\n(DI) between word meanings in an LSTM, based on their gate interactions. We\nconnect this measure to syntax with experiments on English language data, where\nDI is higher on pairs of words with lower syntactic distance. To explore the\ninductive biases that cause these compositional representations to arise during\ntraining, we conduct simple experiments on synthetic data. These synthetic\nexperiments support a specific hypothesis about how hierarchical structures are\ndiscovered over the course of training: that LSTM constituent representations\nare learned bottom-up, relying on effective representations of their shorter\nchildren, rather than learning the longer-range relations independently from\nchildren.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 13:00:32 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Saphra", "Naomi", ""], ["Lopez", "Adam", ""]]}, {"id": "2010.04658", "submitter": "Shrimai Prabhumoye", "authors": "Shrimai Prabhumoye, Brendon Boldt, Ruslan Salakhutdinov, Alan W Black", "title": "Case Study: Deontological Ethics in NLP", "comments": "Accepted at North American Chapter of the Association for\n  Computational Linguistics (NAACL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in natural language processing (NLP) has focused on ethical\nchallenges such as understanding and mitigating bias in data and algorithms;\nidentifying objectionable content like hate speech, stereotypes and offensive\nlanguage; and building frameworks for better system design and data handling\npractices. However, there has been little discussion about the ethical\nfoundations that underlie these efforts. In this work, we study one ethical\ntheory, namely deontological ethics, from the perspective of NLP. In\nparticular, we focus on the generalization principle and the respect for\nautonomy through informed consent. We provide four case studies to demonstrate\nhow these principles can be used with NLP systems. We also recommend directions\nto avoid the ethical issues in these systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:04:51 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 19:14:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Prabhumoye", "Shrimai", ""], ["Boldt", "Brendon", ""], ["Salakhutdinov", "Ruslan", ""], ["Black", "Alan W", ""]]}, {"id": "2010.04665", "submitter": "Seraphina Goldfarb-Tarrant", "authors": "Seraphina Goldfarb-Tarrant, Alexander Robertson, Jasmina Lazic,\n  Theodora Tsouloufi, Louise Donnison, Karen Smyth", "title": "Scaling Systematic Literature Reviews with Machine Learning Pipelines", "comments": "In EMNLP 2020 Scholarly Document Processing Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Systematic reviews, which entail the extraction of data from large numbers of\nscientific documents, are an ideal avenue for the application of machine\nlearning. They are vital to many fields of science and philanthropy, but are\nvery time-consuming and require experts. Yet the three main stages of a\nsystematic review are easily done automatically: searching for documents can be\ndone via APIs and scrapers, selection of relevant documents can be done via\nbinary classification, and extraction of data can be done via\nsequence-labelling classification. Despite the promise of automation for this\nfield, little research exists that examines the various ways to automate each\nof these tasks. We construct a pipeline that automates each of these aspects,\nand experiment with many human-time vs. system quality trade-offs. We test the\nability of classifiers to work well on small amounts of data and to generalise\nto data from countries not represented in the training data. We test different\ntypes of data extraction with varying difficulty in annotation, and five\ndifferent neural architectures to do the extraction. We find that we can get\nsurprising accuracy and generalisability of the whole pipeline system with only\n2 weeks of human-expert annotation, which is only 15% of the time it takes to\ndo the whole review manually and can be repeated and extended to new data with\nno additional effort.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:19:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Goldfarb-Tarrant", "Seraphina", ""], ["Robertson", "Alexander", ""], ["Lazic", "Jasmina", ""], ["Tsouloufi", "Theodora", ""], ["Donnison", "Louise", ""], ["Smyth", "Karen", ""]]}, {"id": "2010.04674", "submitter": "Brian DuSell", "authors": "Brian DuSell and David Chiang", "title": "Learning Context-Free Languages with Nondeterministic Stack RNNs", "comments": "13 pages, 5 figures, accepted for publication at CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentiable stack data structure that simultaneously and\ntractably encodes an exponential number of stack configurations, based on\nLang's algorithm for simulating nondeterministic pushdown automata. We call the\ncombination of this data structure with a recurrent neural network (RNN)\ncontroller a Nondeterministic Stack RNN. We compare our model against existing\nstack RNNs on various formal languages, demonstrating that our model converges\nmore reliably to algorithmic behavior on deterministic tasks, and achieves\nlower cross-entropy on inherently nondeterministic tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:48:41 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["DuSell", "Brian", ""], ["Chiang", "David", ""]]}, {"id": "2010.04704", "submitter": "Shawn Tan", "authors": "Shawn Tan and Yikang Shen and Timothy J. O'Donnell and Alessandro\n  Sordoni and Aaron Courville", "title": "Recursive Top-Down Production for Sentence Generation with Latent Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the recursive production property of context-free grammars for\nnatural and synthetic languages. To this end, we present a dynamic programming\nalgorithm that marginalises over latent binary tree structures with $N$ leaves,\nallowing us to compute the likelihood of a sequence of $N$ tokens under a\nlatent tree model, which we maximise to train a recursive neural function. We\ndemonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017),\nwhere it outperforms previous models on the LENGTH split, and English question\nformation (McCoy et al., 2020), where it performs comparably to decoders with\nthe ground-truth tree structure. We also present experimental results on\nGerman-English translation on the Multi30k dataset (Elliott et al., 2016), and\nqualitatively analyse the induced tree structures our model learns for the SCAN\ntasks and the German-English translation task.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:47:16 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Tan", "Shawn", ""], ["Shen", "Yikang", ""], ["O'Donnell", "Timothy J.", ""], ["Sordoni", "Alessandro", ""], ["Courville", "Aaron", ""]]}, {"id": "2010.04706", "submitter": "Katherine Keith", "authors": "Katherine A. Keith, Christoph Teichmann, Brendan O'Connor, Edgar Meij", "title": "Uncertainty over Uncertainty: Investigating the Assumptions,\n  Annotations, and Text Measurements of Economic Policy Uncertainty", "comments": "Accepted to the 2020 Natural Language Processing + Computational\n  Social Science Workshop (NLP+CSS) at EMNLP", "journal-ref": "2020 Natural Language Processing + Computational Social Science\n  Workshop (NLP+CSS) at EMNLP", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods and applications are inextricably linked in science, and in\nparticular in the domain of text-as-data. In this paper, we examine one such\ntext-as-data application, an established economic index that measures economic\npolicy uncertainty from keyword occurrences in news. This index, which is shown\nto correlate with firm investment, employment, and excess market returns, has\nhad substantive impact in both the private sector and academia. Yet, as we\nrevisit and extend the original authors' annotations and text measurements we\nfind interesting text-as-data methodological research questions: (1) Are\nannotator disagreements a reflection of ambiguity in language? (2) Do\nalternative text measurements correlate with one another and with measures of\nexternal predictive validity? We find for this application (1) some annotator\ndisagreements of economic policy uncertainty can be attributed to ambiguity in\nlanguage, and (2) switching measurements from keyword-matching to supervised\nmachine learning classifiers results in low correlation, a concerning\nimplication for the validity of the index.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:50:29 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Keith", "Katherine A.", ""], ["Teichmann", "Christoph", ""], ["O'Connor", "Brendan", ""], ["Meij", "Edgar", ""]]}, {"id": "2010.04736", "submitter": "Chenhao Tan", "authors": "Samuel Carton, Anirudh Rathore, Chenhao Tan", "title": "Evaluating and Characterizing Human Rationales", "comments": "14 pages, 15 figures, to appear in EMNLP 2020. Code is available at\n  https://github.com/BoulderDS/evaluating-human-rationales", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two main approaches for evaluating the quality of machine-generated\nrationales are: 1) using human rationales as a gold standard; and 2) automated\nmetrics based on how rationales affect model behavior. An open question,\nhowever, is how human rationales fare with these automatic metrics. Analyzing a\nvariety of datasets and models, we find that human rationales do not\nnecessarily perform well on these metrics. To unpack this finding, we propose\nimproved metrics to account for model-dependent baseline performance. We then\npropose two methods to further characterize rationale quality, one based on\nmodel retraining and one on using \"fidelity curves\" to reveal properties such\nas irrelevance and redundancy. Our work leads to actionable suggestions for\nevaluating and characterizing rationales.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:00:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Carton", "Samuel", ""], ["Rathore", "Anirudh", ""], ["Tan", "Chenhao", ""]]}, {"id": "2010.04744", "submitter": "Christopher Chu", "authors": "Christopher Chu, Scot Fang and Kevin Knight", "title": "Learning to Pronounce Chinese Without a Pronunciation Dictionary", "comments": "7 pages. To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate a program that learns to pronounce Chinese text in Mandarin,\nwithout a pronunciation dictionary. From non-parallel streams of Chinese\ncharacters and Chinese pinyin syllables, it establishes a many-to-many mapping\nbetween characters and pronunciations. Using unsupervised methods, the program\neffectively deciphers writing into speech. Its token-level\ncharacter-to-syllable accuracy is 89%, which significantly exceeds the 22%\naccuracy of prior work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:03:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chu", "Christopher", ""], ["Fang", "Scot", ""], ["Knight", "Kevin", ""]]}, {"id": "2010.04746", "submitter": "Christopher Chu", "authors": "Christopher Chu, Raphael Valenti, Kevin Knight", "title": "Solving Historical Dictionary Codes with a Neural Language Model", "comments": "10 pages, 6 figures. To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We solve difficult word-based substitution codes by constructing a decoding\nlattice and searching that lattice with a neural language model. We apply our\nmethod to a set of enciphered letters exchanged between US Army General James\nWilkinson and agents of the Spanish Crown in the late 1700s and early 1800s,\nobtained from the US Library of Congress. We are able to decipher 75.1% of the\ncipher-word tokens correctly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:04:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chu", "Christopher", ""], ["Valenti", "Raphael", ""], ["Knight", "Kevin", ""]]}, {"id": "2010.04747", "submitter": "Christopher Chu", "authors": "Arkady Arkhangorodsky, Amittai Axelrod, Christopher Chu, Scot Fang,\n  Yiqi Huang, Ajay Nagesh, Xing Shi, Boliang Zhang and Kevin Knight", "title": "MEEP: An Open-Source Platform for Human-Human Dialog Collection and\n  End-to-End Agent Training", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We create a new task-oriented dialog platform (MEEP) where agents are given\nconsiderable freedom in terms of utterances and API calls, but are constrained\nto work within a push-button environment. We include facilities for collecting\nhuman-human dialog corpora, and for training automatic agents in an end-to-end\nfashion. We demonstrate MEEP with a dialog assistant that lets users specify\ntrip destinations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:04:17 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Arkhangorodsky", "Arkady", ""], ["Axelrod", "Amittai", ""], ["Chu", "Christopher", ""], ["Fang", "Scot", ""], ["Huang", "Yiqi", ""], ["Nagesh", "Ajay", ""], ["Shi", "Xing", ""], ["Zhang", "Boliang", ""], ["Knight", "Kevin", ""]]}, {"id": "2010.04755", "submitter": "Jun Yen Leung", "authors": "Jun Yen Leung, Guy Emerson, Ryan Cotterell", "title": "Investigating Cross-Linguistic Adjective Ordering Tendencies with a\n  Latent-Variable Model", "comments": "13 pages, 7 tables, 1 figure. To be published in EMNLP 2020\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Across languages, multiple consecutive adjectives modifying a noun (e.g. \"the\nbig red dog\") follow certain unmarked ordering rules. While explanatory\naccounts have been put forward, much of the work done in this area has relied\nprimarily on the intuitive judgment of native speakers, rather than on corpus\ndata. We present the first purely corpus-driven model of multi-lingual\nadjective ordering in the form of a latent-variable model that can accurately\norder adjectives across 24 different languages, even when the training and\ntesting languages are different. We utilize this novel statistical model to\nprovide strong converging evidence for the existence of universal,\ncross-linguistic, hierarchical adjective ordering tendencies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:27:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Leung", "Jun Yen", ""], ["Emerson", "Guy", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2010.04762", "submitter": "William Huang", "authors": "William Huang, Haokun Liu, and Samuel R. Bowman", "title": "Counterfactually-Augmented SNLI Training Data Does Not Yield Better\n  Generalization Than Unaugmented Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of work shows that models exploit annotation artifacts to\nachieve state-of-the-art performance on standard crowdsourced\nbenchmarks---datasets collected from crowdworkers to create an evaluation\ntask---while still failing on out-of-domain examples for the same task. Recent\nwork has explored the use of counterfactually-augmented data---data built by\nminimally editing a set of seed examples to yield counterfactual labels---to\naugment training data associated with these benchmarks and build more robust\nclassifiers that generalize better. However, Khashabi et al. (2020) find that\nthis type of augmentation yields little benefit on reading comprehension tasks\nwhen controlling for dataset size and cost of collection. We build upon this\nwork by using English natural language inference data to test model\ngeneralization and robustness and find that models trained on a\ncounterfactually-augmented SNLI dataset do not generalize better than\nunaugmented datasets of similar size and that counterfactual augmentation can\nhurt performance, yielding models that are less robust to challenge examples.\nCounterfactual augmentation of natural language understanding data through\nstandard crowdsourcing techniques does not appear to be an effective way of\ncollecting training data and further innovation is required to make this\ngeneral line of work viable.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:44:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Huang", "William", ""], ["Liu", "Haokun", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2010.04791", "submitter": "Shiyue Zhang", "authors": "Shiyue Zhang, Benjamin Frey, Mohit Bansal", "title": "ChrEn: Cherokee-English Machine Translation for Endangered Language\n  Revitalization", "comments": "EMNLP 2020 (19 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cherokee is a highly endangered Native American language spoken by the\nCherokee people. The Cherokee culture is deeply embedded in its language.\nHowever, there are approximately only 2,000 fluent first language Cherokee\nspeakers remaining in the world, and the number is declining every year. To\nhelp save this endangered language, we introduce ChrEn, a Cherokee-English\nparallel dataset, to facilitate machine translation research between Cherokee\nand English. Compared to some popular machine translation language pairs, ChrEn\nis extremely low-resource, only containing 14k sentence pairs in total. We\nsplit our parallel data in ways that facilitate both in-domain and\nout-of-domain evaluation. We also collect 5k Cherokee monolingual data to\nenable semi-supervised learning. Besides these datasets, we propose several\nCherokee-English and English-Cherokee machine translation systems. We compare\nSMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems;\nsupervised versus semi-supervised (via language model, back-translation, and\nBERT/Multilingual-BERT) methods; as well as transfer learning versus\nmultilingual joint training with 4 other languages. Our best results are\n15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr\ntranslations, respectively, and we hope that our dataset and systems will\nencourage future work by the community for Cherokee language revitalization.\nOur data, code, and demo will be publicly available at\nhttps://github.com/ZhangShiyue/ChrEn\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 20:28:06 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Shiyue", ""], ["Frey", "Benjamin", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.04806", "submitter": "Silei Xu", "authors": "Silei Xu, Sina J. Semnani, Giovanni Campagna, Monica S. Lam", "title": "AutoQA: From Databases To QA Semantic Parsers With Only Synthetic\n  Training Data", "comments": "To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose AutoQA, a methodology and toolkit to generate semantic parsers\nthat answer questions on databases, with no manual effort. Given a database\nschema and its data, AutoQA automatically generates a large set of high-quality\nquestions for training that covers different database operations. It uses\nautomatic paraphrasing combined with template-based parsing to find alternative\nexpressions of an attribute in different parts of speech. It also uses a novel\nfiltered auto-paraphraser to generate correct paraphrases of entire sentences.\nWe apply AutoQA to the Schema2QA dataset and obtain an average logical form\naccuracy of 62.9% when tested on natural questions, which is only 6.4% lower\nthan a model trained with expert natural language annotations and paraphrase\ndata collected from crowdworkers. To demonstrate the generality of AutoQA, we\nalso apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy,\n16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower\nthan the same model trained with human data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 21:06:57 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 00:49:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xu", "Silei", ""], ["Semnani", "Sina J.", ""], ["Campagna", "Giovanni", ""], ["Lam", "Monica S.", ""]]}, {"id": "2010.04826", "submitter": "Prasanna Parthasarathi", "authors": "Prasanna Parthasarathi and Arvind Neelakantan and Sharan Narang", "title": "On Task-Level Dialogue Composition of Generative Transformer Model", "comments": "8 pages; Accepted at Workshop on Insights from Negative Results in\n  NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialogue systems help users accomplish tasks such as booking a\nmovie ticket and ordering food via conversation. Generative models\nparameterized by a deep neural network are widely used for next turn response\ngeneration in such systems. It is natural for users of the system to want to\naccomplish multiple tasks within the same conversation, but the ability of\ngenerative models to compose multiple tasks is not well studied. In this work,\nwe begin by studying the effect of training human-human task-oriented dialogues\ntowards improving the ability to compose multiple tasks on Transformer\ngenerative models. To that end, we propose and explore two solutions: (1)\ncreating synthetic multiple task dialogue data for training from human-human\nsingle task dialogue and (2) forcing the encoder representation to be invariant\nto single and multiple task dialogues using an auxiliary loss. The results from\nour experiments highlight the difficulty of even the sophisticated variant of\ntransformer model in learning to compose multiple tasks from single task\ndialogues.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:10:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Parthasarathi", "Prasanna", ""], ["Neelakantan", "Arvind", ""], ["Narang", "Sharan", ""]]}, {"id": "2010.04829", "submitter": "Amir Cohen", "authors": "Amir DN Cohen, Shachar Rosenman, Yoav Goldberg", "title": "Relation Classification as Two-way Span-Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current supervised relation classification (RC) task uses a single\nembedding to represent the relation between a pair of entities. We argue that a\nbetter approach is to treat the RC task as span-prediction (SP) problem,\nsimilar to Question answering (QA). We present a span-prediction based system\nfor RC and evaluate its performance compared to the embedding based system. We\ndemonstrate that the supervised SP objective works significantly better then\nthe standard classification based objective. We achieve state-of-the-art\nresults on the TACRED and SemEval task 8 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:23:21 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 01:24:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cohen", "Amir DN", ""], ["Rosenman", "Shachar", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2010.04842", "submitter": "Arun Tejasvi Chaganty", "authors": "Justin Dieter and Arun Tejasvi Chaganty", "title": "Conformal retrofitting via Riemannian manifolds: distilling\n  task-specific graphs into pretrained embeddings", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained (language) embeddings are versatile, task-agnostic feature\nrepresentations of entities, like words, that are central to many machine\nlearning applications. These representations can be enriched through\nretrofitting, a class of methods that incorporate task-specific domain\nknowledge encoded as a graph over a subset of these entities. However, existing\nretrofitting algorithms face two limitations: they overfit the observed graph\nby failing to represent relationships with missing entities; and they underfit\nthe observed graph by only learning embeddings in Euclidean manifolds, which\ncannot faithfully represent even simple tree-structured or cyclic graphs. We\naddress these problems with two key contributions: (i) we propose a novel\nregularizer, a conformality regularizer, that preserves local geometry from the\npretrained embeddings---enabling generalization to missing entities and (ii) a\nnew Riemannian feedforward layer that learns to map pre-trained embeddings onto\na non-Euclidean manifold that can better represent the entire graph. Through\nexperiments on WordNet, we demonstrate that the conformality regularizer\nprevents even existing (Euclidean-only) methods from overfitting on link\nprediction for missing entities, and---together with the Riemannian feedforward\nlayer---learns non-Euclidean embeddings that outperform them.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 23:06:57 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Dieter", "Justin", ""], ["Chaganty", "Arun Tejasvi", ""]]}, {"id": "2010.04844", "submitter": "James Michaelov", "authors": "James A. Michaelov and Benjamin K. Bergen", "title": "How well does surprisal explain N400 amplitude under different\n  experimental conditions?", "comments": "To be presented at CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT cs.LG math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the extent to which word surprisal can be used to predict a\nneural measure of human language processing difficulty - the N400. To do this,\nwe use recurrent neural networks to calculate the surprisal of stimuli from\npreviously published neurolinguistic studies of the N400. We find that\nsurprisal can predict N400 amplitude in a wide range of cases, and the cases\nwhere it cannot do so provide valuable insight into the neurocognitive\nprocesses underlying the response.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 23:18:23 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Michaelov", "James A.", ""], ["Bergen", "Benjamin K.", ""]]}, {"id": "2010.04863", "submitter": "Hao Huang", "authors": "Hao Huang, Guodong Long, Tao Shen, Jing Jiang, Chengqi Zhang", "title": "RatE: Relation-Adaptive Translating Embedding for Knowledge Graph\n  Completion", "comments": "Accepted to appear at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph embedding approaches have been proposed for knowledge graph\ncompletion via link prediction. Among those, translating embedding approaches\nenjoy the advantages of light-weight structure, high efficiency and great\ninterpretability. Especially when extended to complex vector space, they show\nthe capability in handling various relation patterns including symmetry,\nantisymmetry, inversion and composition. However, previous translating\nembedding approaches defined in complex vector space suffer from two main\nissues: 1) representing and modeling capacities of the model are limited by the\ntranslation function with rigorous multiplication of two complex numbers; and\n2) embedding ambiguity caused by one-to-many relations is not explicitly\nalleviated. In this paper, we propose a relation-adaptive translation function\nbuilt upon a novel weighted product in complex space, where the weights are\nlearnable, relation-specific and independent to embedding size. The translation\nfunction only requires eight more scalar parameters each relation, but improves\nexpressive power and alleviates embedding ambiguity problem. Based on the\nfunction, we then present our Relation-adaptive translating Embedding (RatE)\napproach to score each graph triple. Moreover, a novel negative sampling method\nis proposed to utilize both prior knowledge and self-adversarial learning for\neffective optimization. Experiments verify RatE achieves state-of-the-art\nperformance on four link prediction benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 01:30:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Huang", "Hao", ""], ["Long", "Guodong", ""], ["Shen", "Tao", ""], ["Jiang", "Jing", ""], ["Zhang", "Chengqi", ""]]}, {"id": "2010.04872", "submitter": "Charles Lovering J", "authors": "Charles Lovering and Ellie Pavlick", "title": "Self-play for Data Efficient Language Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When communicating, people behave consistently across conversational roles:\nPeople understand the words they say and are able to produce the words they\nhear. To date, artificial agents developed for language tasks have lacked such\nsymmetry, meaning agents trained to produce language are unable to understand\nit and vice-versa. In this work, we exploit the symmetric nature of\ncommunication in order to improve both the efficiency and quality of language\nacquisition in learning agents. Specifically, we consider the setting in which\nan agent must learn to both understand and generate words in an existing\nlanguage, but with the assumption that access to interaction with \"oracle\"\nspeakers of the language is very limited. We show that using self-play as a\nsubstitute for direct supervision enables the agent to transfer its knowledge\nacross roles (e.g. training as a listener but testing as a speaker) and make\nbetter inferences about the ground truth lexicon using only a handful of\ninteractions with the oracle.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:09:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lovering", "Charles", ""], ["Pavlick", "Ellie", ""]]}, {"id": "2010.04883", "submitter": "Xinyin Ma", "authors": "Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia,\n  Weiming Lu", "title": "Adversarial Self-Supervised Data-Free Distillation for Text\n  Classification", "comments": "11 pages, 5 figures, Accepted to EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained transformer-based language models have achieved impressive\nresults on a wide range of NLP tasks. In the past few years, Knowledge\nDistillation(KD) has become a popular paradigm to compress a computationally\nexpensive model to a resource-efficient lightweight model. However, most KD\nalgorithms, especially in NLP, rely on the accessibility of the original\ntraining dataset, which may be unavailable due to privacy issues. To tackle\nthis problem, we propose a novel two-stage data-free distillation method, named\nAdversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed\nfor compressing large-scale transformer-based models (e.g., BERT). To avoid\ntext generation in discrete space, we introduce a Plug & Play Embedding\nGuessing method to craft pseudo embeddings from the teacher's hidden knowledge.\nMeanwhile, with a self-supervised module to quantify the student's ability, we\nadapt the difficulty of pseudo embeddings in an adversarial training manner. To\nthe best of our knowledge, our framework is the first data-free distillation\nframework designed for NLP tasks. We verify the effectiveness of our method on\nseveral text classification datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:46:06 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ma", "Xinyin", ""], ["Shen", "Yongliang", ""], ["Fang", "Gongfan", ""], ["Chen", "Chen", ""], ["Jia", "Chenghao", ""], ["Lu", "Weiming", ""]]}, {"id": "2010.04887", "submitter": "Forrest Davis", "authors": "Forrest Davis and Marten van Schijndel", "title": "Discourse structure interacts with reference but not syntax in neural\n  language models", "comments": "Proceedings of the 2020 Conference on Computational Natural Language\n  Learning (CoNLL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) trained on large quantities of text have been claimed\nto acquire abstract linguistic representations. Our work tests the robustness\nof these abstractions by focusing on the ability of LMs to learn interactions\nbetween different linguistic representations. In particular, we utilized\nstimuli from psycholinguistic studies showing that humans can condition\nreference (i.e. coreference resolution) and syntactic processing on the same\ndiscourse structure (implicit causality). We compared both transformer and long\nshort-term memory LMs to find that, contrary to humans, implicit causality only\ninfluences LM behavior for reference, not syntax, despite model representations\nthat encode the necessary discourse information. Our results further suggest\nthat LM behavior can contradict not only learned representations of discourse\nbut also syntactic agreement, pointing to shortcomings of standard language\nmodeling.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 03:14:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Davis", "Forrest", ""], ["van Schijndel", "Marten", ""]]}, {"id": "2010.04897", "submitter": "Bo Wang", "authors": "John Pougue Biyong, Bo Wang, Terry Lyons and Alejo J Nevado-Holgado", "title": "Information Extraction from Swedish Medical Prescriptions with\n  Sig-Transformer Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relying on large pretrained language models such as Bidirectional Encoder\nRepresentations from Transformers (BERT) for encoding and adding a simple\nprediction layer has led to impressive performance in many clinical natural\nlanguage processing (NLP) tasks. In this work, we present a novel extension to\nthe Transformer architecture, by incorporating signature transform with the\nself-attention model. This architecture is added between embedding and\nprediction layers. Experiments on a new Swedish prescription data show the\nproposed architecture to be superior in two of the three information extraction\ntasks, comparing to baseline models. Finally, we evaluate two different\nembedding approaches between applying Multilingual BERT and translating the\nSwedish text to English then encode with a BERT model pretrained on clinical\nnotes.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:22:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Biyong", "John Pougue", ""], ["Wang", "Bo", ""], ["Lyons", "Terry", ""], ["Nevado-Holgado", "Alejo J", ""]]}, {"id": "2010.04898", "submitter": "Svitlana Vakulenko", "authors": "Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre,\n  Stephen Pulman, Srinivas Chappidi", "title": "Open-Domain Question Answering Goes Conversational via Question\n  Rewriting", "comments": "15 pages, 10 tables, 3 figures, accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset for Question Rewriting in Conversational Context\n(QReCC), which contains 14K conversations with 80K question-answer pairs. The\ntask in QReCC is to find answers to conversational questions within a\ncollection of 10M web pages (split into 54M passages). Answers to questions in\nthe same conversation may be distributed across several web pages. QReCC\nprovides annotations that allow us to train and evaluate individual subtasks of\nquestion rewriting, passage retrieval and reading comprehension required for\nthe end-to-end conversational question answering (QA) task. We report the\neffectiveness of a strong baseline approach that combines the state-of-the-art\nmodel for question rewriting, and competitive models for open-domain QA. Our\nresults set the first baseline for the QReCC dataset with F1 of 19.10, compared\nto the human upper bound of 75.45, indicating the difficulty of the setup and a\nlarge room for improvement.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:28:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:01:39 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 19:09:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Anantha", "Raviteja", ""], ["Vakulenko", "Svitlana", ""], ["Tu", "Zhucheng", ""], ["Longpre", "Shayne", ""], ["Pulman", "Stephen", ""], ["Chappidi", "Srinivas", ""]]}, {"id": "2010.04900", "submitter": "Chiyu Zhang", "authors": "Muhammad Abdul-Mageed and Chiyu Zhang and AbdelRahim Elmadany and Lyle\n  Ungar", "title": "Toward Micro-Dialect Identification in Diaglossic and Code-Switched\n  Environments", "comments": "Accepted in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the prediction of dialects is an important language processing task,\nwith a wide range of applications, existing work is largely limited to\ncoarse-grained varieties. Inspired by geolocation research, we propose the\nnovel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new\nlanguage model with striking abilities to predict a fine-grained variety (as\nsmall as that of a city) given a single, short message. For modeling, we offer\na range of novel spatially and linguistically-motivated multi-task learning\nmodels. To showcase the utility of our models, we introduce a new, large-scale\ndataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT\npredicts micro-dialects with 9.9% F1, ~76X better than a majority class\nbaseline. Our new language model also establishes new state-of-the-art on\nseveral external tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:35:16 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 07:55:03 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Zhang", "Chiyu", ""], ["Elmadany", "AbdelRahim", ""], ["Ungar", "Lyle", ""]]}, {"id": "2010.04903", "submitter": "Yu-An Wang", "authors": "Yu-An Wang, Yun-Nung Chen", "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained\n  Language Model Positional Encoding", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, pre-trained Transformers have dominated the majority of NLP\nbenchmark tasks. Many variants of pre-trained Transformers have kept breaking\nout, and most focus on designing different pre-training objectives or variants\nof self-attention. Embedding the position information in the self-attention\nmechanism is also an indispensable factor in Transformers however is often\ndiscussed at will. Therefore, this paper carries out an empirical study on\nposition embeddings of mainstream pre-trained Transformers, which mainly\nfocuses on two questions: 1) Do position embeddings really learn the meaning of\npositions? 2) How do these different learned position embeddings affect\nTransformers for NLP tasks? This paper focuses on providing a new insight of\npre-trained position embeddings through feature-level analysis and empirical\nexperiments on most of iconic NLP tasks. It is believed that our experimental\nresults can guide the future work to choose the suitable positional encoding\nfunction for specific tasks given the application property.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 05:03:14 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Yu-An", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2010.04922", "submitter": "Zhengxuan Wu", "authors": "Zhengxuan Wu, Thanh-Son Nguyen, Desmond C. Ong", "title": "Structured Self-Attention Weights Encode Semantics in Sentiment Analysis", "comments": "10 pages", "journal-ref": "BlackBoxNLP Workshop at EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural attention, especially the self-attention made popular by the\nTransformer, has become the workhorse of state-of-the-art natural language\nprocessing (NLP) models. Very recent work suggests that the self-attention in\nthe Transformer encodes syntactic information; Here, we show that\nself-attention scores encode semantics by considering sentiment analysis tasks.\nIn contrast to gradient-based feature attribution methods, we propose a simple\nand effective Layer-wise Attention Tracing (LAT) method to analyze structured\nattention weights. We apply our method to Transformer models trained on two\ntasks that have surface dissimilarities, but share common semantics---sentiment\nanalysis of movie reviews and time-series valence prediction in life story\nnarratives. Across both tasks, words with high aggregated attention weights\nwere rich in emotional semantics, as quantitatively validated by an emotion\nlexicon labeled by human annotators. Our results show that structured attention\nweights encode rich semantics in sentiment analysis, and match human\ninterpretations of semantics.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 06:49:25 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Nguyen", "Thanh-Son", ""], ["Ong", "Desmond C.", ""]]}, {"id": "2010.04924", "submitter": "Vikas Raunak", "authors": "Vikas Raunak, Siddharth Dalmia, Vivek Gupta and Florian Metze", "title": "On Long-Tailed Phenomena in Neural Machine Translation", "comments": "Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art Neural Machine Translation (NMT) models struggle with\ngenerating low-frequency tokens, tackling which remains a major challenge. The\nanalysis of long-tailed phenomena in the context of structured prediction tasks\nis further hindered by the added complexities of search during inference. In\nthis work, we quantitatively characterize such long-tailed phenomena at two\nlevels of abstraction, namely, token classification and sequence generation. We\npropose a new loss function, the Anti-Focal loss, to better adapt model\ntraining to the structural dependencies of conditional text generation by\nincorporating the inductive biases of beam search in the training process. We\nshow the efficacy of the proposed technique on a number of Machine Translation\n(MT) datasets, demonstrating that it leads to significant gains over\ncross-entropy across different language pairs, especially on the generation of\nlow-frequency words. We have released the code to reproduce our results.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 07:00:57 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Raunak", "Vikas", ""], ["Dalmia", "Siddharth", ""], ["Gupta", "Vivek", ""], ["Metze", "Florian", ""]]}, {"id": "2010.04926", "submitter": "Yian Zhang", "authors": "Yian Zhang", "title": "Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?", "comments": "6 pages, 3 figures, 3 tables, to appear in Proceedings of the 2020\n  EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\n  NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent latent tree learning models can learn constituency parsing without any\nexposure to human-annotated tree structures. One such model is ON-LSTM (Shen et\nal., 2019), which is trained on language modelling and has\nnear-state-of-the-art performance on unsupervised parsing. In order to better\nunderstand the performance and consistency of the model as well as how the\nparses it generates are different from gold-standard PTB parses, we replicate\nthe model with different restarts and examine their parses. We find that (1)\nthe model has reasonably consistent parsing behaviors across different\nrestarts, (2) the model struggles with the internal structures of complex noun\nphrases, (3) the model has a tendency to overestimate the height of the split\npoints right before verbs. We speculate that both problems could potentially be\nsolved by adopting a different training task other than unidirectional language\nmodelling.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 07:12:48 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Yian", ""]]}, {"id": "2010.04927", "submitter": "Qiansheng Wang", "authors": "Qiansheng Wang, Yuxin Liu, Chengguo Lv, Zhen Wang and Guohong Fu", "title": "Cue-word Driven Neural Response Generation with a Shrinking Vocabulary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain response generation is the task of generating sensible and\ninformative re-sponses to the source sentence. However, neural models tend to\ngenerate safe and mean-ingless responses. While cue-word introducing approaches\nencourage responses with concrete semantics and have shown tremendous\npotential, they still fail to explore di-verse responses during decoding. In\nthis paper, we propose a novel but natural approach that can produce multiple\ncue-words during decoding, and then uses the produced cue-words to drive\ndecoding and shrinks the decoding vocabulary. Thus the neural genera-tion model\ncan explore the full space of responses and discover informative ones with\nefficiency. Experimental results show that our approach significantly\noutperforms several strong baseline models with much lower decoding complexity.\nEspecially, our approach can converge to concrete semantics more efficiently\nduring decoding.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 07:13:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Qiansheng", ""], ["Liu", "Yuxin", ""], ["Lv", "Chengguo", ""], ["Wang", "Zhen", ""], ["Fu", "Guohong", ""]]}, {"id": "2010.04935", "submitter": "Jun Kong", "authors": "Jun Kong, Jin Wang and Xuejie Zhang", "title": "HPCC-YNU at SemEval-2020 Task 9: A Bilingual Vector Gating Mechanism for\n  Sentiment Analysis of Code-Mixed Text", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is fairly common to use code-mixing on a social media platform to express\nopinions and emotions in multilingual societies. The purpose of this task is to\ndetect the sentiment of code-mixed social media text. Code-mixed text poses a\ngreat challenge for the traditional NLP system, which currently uses\nmonolingual resources to deal with the problem of multilingual mixing. This\ntask has been solved in the past using lexicon lookup in respective sentiment\ndictionaries and using a long short-term memory (LSTM) neural network for\nmonolingual resources. In this paper, we (my codalab username is kongjun)\npresent a system that uses a bilingual vector gating mechanism for bilingual\nresources to complete the task. The model consists of two main parts: the\nvector gating mechanism, which combines the character and word levels, and the\nattention mechanism, which extracts the important emotional parts of the text.\nThe results show that the proposed system outperforms the baseline algorithm.\nWe achieved fifth place in Spanglish and 19th place in Hinglish.The code of\nthis paper is availabled at : https://github.com/JunKong5/Semveal2020-task9\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 08:02:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kong", "Jun", ""], ["Wang", "Jin", ""], ["Zhang", "Xuejie", ""]]}, {"id": "2010.04941", "submitter": "Changlong Yu", "authors": "Changlong Yu, Jialong Han, Peifeng Wang, Yangqiu Song, Hongming Zhang,\n  Wilfred Ng, Shuming Shi", "title": "When Hearst Is not Enough: Improving Hypernymy Detection from Corpus\n  with Distributional Models", "comments": "Accepted by EMNLP2020 Main Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address hypernymy detection, i.e., whether an is-a relationship exists\nbetween words (x, y), with the help of large textual corpora. Most conventional\napproaches to this task have been categorized to be either pattern-based or\ndistributional. Recent studies suggest that pattern-based ones are superior, if\nlarge-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x,\ny) pairs relieved. However, they become invalid in some specific sparsity\ncases, where x or y is not involved in any pattern. For the first time, this\npaper quantifies the non-negligible existence of those specific cases. We also\ndemonstrate that distributional methods are ideal to make up for pattern-based\nones in such cases. We devise a complementary framework, under which a\npattern-based and a distributional model collaborate seamlessly in cases which\nthey each prefer. On several benchmark datasets, our framework achieves\ncompetitive improvements and the case study shows its better interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 08:34:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yu", "Changlong", ""], ["Han", "Jialong", ""], ["Wang", "Peifeng", ""], ["Song", "Yangqiu", ""], ["Zhang", "Hongming", ""], ["Ng", "Wilfred", ""], ["Shi", "Shuming", ""]]}, {"id": "2010.04970", "submitter": "Yingxue Zhang", "authors": "Yingxue Zhang, Fandong Meng, Peng Li, Ping Jian, Jie Zhou", "title": "MS-Ranker: Accumulating Evidence from Potentially Correct Candidates for\n  Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As conventional answer selection (AS) methods generally match the question\nwith each candidate answer independently, they suffer from the lack of matching\ninformation between the question and the candidate. To address this problem, we\npropose a novel reinforcement learning (RL) based multi-step ranking model,\nnamed MS-Ranker, which accumulates information from potentially correct\ncandidate answers as extra evidence for matching the question with a candidate.\nIn specific, we explicitly consider the potential correctness of candidates and\nupdate the evidence with a gating mechanism. Moreover, as we use a listwise\nranking reward, our model learns to pay more attention to the overall\nperformance. Experiments on two benchmarks, namely WikiQA and SemEval-2016 CQA,\nshow that our model significantly outperforms existing methods that do not rely\non external resources.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 10:36:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Yingxue", ""], ["Meng", "Fandong", ""], ["Li", "Peng", ""], ["Jian", "Ping", ""], ["Zhou", "Jie", ""]]}, {"id": "2010.04971", "submitter": "Issa Annamoradnejad", "authors": "Navid Khezrian, Jafar Habibi, Issa Annamoradnejad", "title": "Tag Recommendation for Online Q&A Communities based on BERT Pre-Training\n  Technique", "comments": "5 pages, initial results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Q&A and open source communities use tags and keywords to index,\ncategorize, and search for specific content. The most obvious advantage of tag\nrecommendation is the correct classification of information. In this study, we\nused the BERT pre-training technique in tag recommendation task for online Q&A\nand open-source communities for the first time. Our evaluation on freecode\ndatasets show that the proposed method, called TagBERT, is more accurate\ncompared to deep learning and other baseline methods. Moreover, our model\nachieved a high stability by solving the problem of previous researches, where\nincreasing the number of tag recommendations significantly reduced model\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 10:52:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Khezrian", "Navid", ""], ["Habibi", "Jafar", ""], ["Annamoradnejad", "Issa", ""]]}, {"id": "2010.04976", "submitter": "Hritik Bansal", "authors": "Hritik Bansal, Gantavya Bhatt, Sumeet Agarwal", "title": "Can RNNs trained on harder subject-verb agreement instances still\n  perform well on easier ones?", "comments": "15 pages, 3 figures, 13 Tables (including Appendix); Non Archival\n  Extended Abstract Accepted in SciL 2021 -\n  https://scholarworks.umass.edu/scil/vol4/iss1/38/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work suggests that RNNs trained on natural language corpora can\ncapture number agreement well for simple sentences but perform less well when\nsentences contain agreement attractors: intervening nouns between the verb and\nthe main subject with grammatical number opposite to the latter. This suggests\nthese models may not learn the actual syntax of agreement, but rather infer\nshallower heuristics such as `agree with the recent noun'. In this work, we\ninvestigate RNN models with varying inductive biases trained on selectively\nchosen `hard' agreement instances, i.e., sentences with at least one agreement\nattractor. For these the verb number cannot be predicted using a simple linear\nheuristic, and hence they might help provide the model additional cues for\nhierarchical syntax. If RNNs can learn the underlying agreement rules when\ntrained on such hard instances, then they should generalize well to other\nsentences, including simpler ones. However, we observe that several RNN types,\nincluding the ONLSTM which has a soft structural inductive bias, surprisingly\nfail to perform well on sentences without attractors when trained solely on\nsentences with attractors. We analyze how these selectively trained RNNs\ncompare to the baseline (training on a natural distribution of agreement\nattractors) along the dimensions of number agreement accuracy, representational\nsimilarity, and performance across different syntactic constructions. Our\nfindings suggest that RNNs trained on our hard agreement instances still do not\ncapture the underlying syntax of agreement, but rather tend to overfit the\ntraining distribution in a way which leads them to perform poorly on `easy'\nout-of-distribution instances. Thus, while RNNs are powerful models which can\npick up non-trivial dependency patterns, inducing them to do so at the level of\nsyntax rather than surface remains a challenge.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 11:30:58 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 09:11:56 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Bansal", "Hritik", ""], ["Bhatt", "Gantavya", ""], ["Agarwal", "Sumeet", ""]]}, {"id": "2010.04980", "submitter": "Renato Negrinho", "authors": "Renato Negrinho, Matthew R. Gormley, Geoffrey J. Gordon", "title": "An Empirical Investigation of Beam-Aware Training in Supertagging", "comments": "EMNLP Findings 2020 camera-ready. Code can be found at\n  https://github.com/negrinho/beam_learn_supertagging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction is often approached by training a locally normalized\nmodel with maximum likelihood and decoding approximately with beam search. This\napproach leads to mismatches as, during training, the model is not exposed to\nits mistakes and does not use beam search. Beam-aware training aims to address\nthese problems, but unfortunately, it is not yet widely used due to a lack of\nunderstanding about how it impacts performance, when it is most useful, and\nwhether it is stable. Recently, Negrinho et al. (2018) proposed a\nmeta-algorithm that captures beam-aware training algorithms and suggests new\nones, but unfortunately did not provide empirical results. In this paper, we\nbegin an empirical investigation: we train the supertagging model of Vaswani et\nal. (2016) and a simpler model with instantiations of the meta-algorithm. We\nexplore the influence of various design choices and make recommendations for\nchoosing them. We observe that beam-aware training improves performance for\nboth models, with large improvements for the simpler model which must\neffectively manage uncertainty during decoding. Our results suggest that a\nmodel must be learned with search to maximize its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 12:25:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Negrinho", "Renato", ""], ["Gormley", "Matthew R.", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "2010.04987", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Lucia Specia, Francesca Toni", "title": "FIND: Human-in-the-Loop Debugging Deep Text Classifiers", "comments": "17 pages including appendices; To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since obtaining a perfect training dataset (i.e., a dataset which is\nconsiderably large, unbiased, and well-representative of unseen cases) is\nhardly possible, many real-world text classifiers are trained on the available,\nyet imperfect, datasets. These classifiers are thus likely to have undesirable\nproperties. For instance, they may have biases against some sub-populations or\nmay not work effectively in the wild due to overfitting. In this paper, we\npropose FIND -- a framework which enables humans to debug deep learning text\nclassifiers by disabling irrelevant hidden features. Experiments show that by\nusing FIND, humans can improve CNN text classifiers which were trained under\ndifferent types of imperfect datasets (including datasets with biases and\ndatasets with dissimilar train-test distributions).\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 12:52:53 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Specia", "Lucia", ""], ["Toni", "Francesca", ""]]}, {"id": "2010.04989", "submitter": "Liang Ding", "authors": "Lei Zhou, Liang Ding and Koichi Takeda", "title": "Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual\n  Patterns", "comments": "To appear in WMT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission of the WMT 2020 Shared Task on Sentence\nLevel Direct Assessment, Quality Estimation (QE). In this study, we empirically\nreveal the \\textit{mismatching issue} when directly adopting BERTScore to QE.\nSpecifically, there exist lots of mismatching errors between the source\nsentence and translated candidate sentence with token pairwise similarity. In\nresponse to this issue, we propose to expose explicit cross-lingual patterns,\n\\textit{e.g.} word alignments and generation score, to our proposed zero-shot\nmodels. Experiments show that our proposed QE model with explicit cross-lingual\npatterns could alleviate the mismatching issue, thereby improving the\nperformance. Encouragingly, our zero-shot QE method could achieve comparable\nperformance with supervised QE method, and even outperforms the supervised\ncounterpart on 2 out of 6 directions. We expect our work could shed light on\nthe zero-shot QE model improvement.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 13:11:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhou", "Lei", ""], ["Ding", "Liang", ""], ["Takeda", "Koichi", ""]]}, {"id": "2010.05001", "submitter": "Wanqing Cui", "authors": "Wanqing Cui, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng", "title": "Beyond Language: Learning Commonsense from Images for Reasoning", "comments": "Accepted to EMNLP'20 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to learn commonsense from images,\ninstead of limited raw texts or costly constructed knowledge bases, for the\ncommonsense reasoning problem in NLP. Our motivation comes from the fact that\nan image is worth a thousand words, where richer scene information could be\nleveraged to help distill the commonsense knowledge, which is often hidden in\nlanguages. Our approach, namely Loire, consists of two stages. In the first\nstage, a bi-modal sequence-to-sequence approach is utilized to conduct the\nscene layout generation task, based on a text representation model ViBERT. In\nthis way, the required visual scene knowledge, such as spatial relations, will\nbe encoded in ViBERT by the supervised learning process with some bi-modal data\nlike COCO. Then ViBERT is concatenated with a pre-trained language model to\nperform the downstream commonsense reasoning tasks. Experimental results on two\ncommonsense reasoning problems, i.e. commonsense question answering and pronoun\nresolution, demonstrate that Loire outperforms traditional language-based\nmethods. We also give some case studies to show what knowledge is learned from\nimages and explain how the generated scene layout helps the commonsense\nreasoning process.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 13:47:13 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Cui", "Wanqing", ""], ["Lan", "Yanyan", ""], ["Pang", "Liang", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2010.05002", "submitter": "Prafull Prakash", "authors": "Prafull Prakash, Saurabh Kumar Shashidhar, Wenlong Zhao, Subendhu\n  Rongali, Haidar Khan, Michael Kayser", "title": "Compressing Transformer-Based Semantic Parsing Models using\n  Compositional Code Embeddings", "comments": "Accepted at EMNLP 2020 (Findings); 7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art task-oriented semantic parsing models use BERT\nor RoBERTa as pretrained encoders; these models have huge memory footprints.\nThis poses a challenge to their deployment for voice assistants such as Amazon\nAlexa and Google Assistant on edge devices with limited memory budgets. We\npropose to learn compositional code embeddings to greatly reduce the sizes of\nBERT-base and RoBERTa-base. We also apply the technique to DistilBERT,\nALBERT-base, and ALBERT-large, three already compressed BERT variants which\nattain similar state-of-the-art performances on semantic parsing with much\nsmaller model sizes. We observe 95.15% ~ 98.46% embedding compression rates and\n20.47% ~ 34.22% encoder compression rates, while preserving greater than 97.5%\nsemantic parsing performances. We provide the recipe for training and analyze\nthe trade-off between code embedding sizes and downstream performances.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 13:47:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Prakash", "Prafull", ""], ["Shashidhar", "Saurabh Kumar", ""], ["Zhao", "Wenlong", ""], ["Rongali", "Subendhu", ""], ["Khan", "Haidar", ""], ["Kayser", "Michael", ""]]}, {"id": "2010.05003", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Kewei Tu", "title": "Second-Order Neural Dependency Parsing with Message Passing and\n  End-to-End Training", "comments": "Accepted to AACL 2020. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose second-order graph-based neural dependency parsing\nusing message passing and end-to-end neural networks. We empirically show that\nour approaches match the accuracy of very recent state-of-the-art second-order\ngraph-based neural dependency parsers and have significantly faster speed in\nboth training and testing. We also empirically show the advantage of\nsecond-order parsing over first-order parsing and observe that the usefulness\nof the head-selection structured constraint vanishes when using BERT embedding.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 13:49:46 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:06:52 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Xinyu", ""], ["Tu", "Kewei", ""]]}, {"id": "2010.05006", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei\n  Huang, Kewei Tu", "title": "Automated Concatenation of Embeddings for Structured Prediction", "comments": "Accepted to Proceedings of ACL-IJCNLP 2021. 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pretrained contextualized embeddings are powerful word representations for\nstructured prediction tasks. Recent work found that better word representations\ncan be obtained by concatenating different types of embeddings. However, the\nselection of embeddings to form the best concatenated representation usually\nvaries depending on the task and the collection of candidate embeddings, and\nthe ever-increasing number of embedding types makes it a more difficult\nproblem. In this paper, we propose Automated Concatenation of Embeddings (ACE)\nto automate the process of finding better concatenations of embeddings for\nstructured prediction tasks, based on a formulation inspired by recent progress\non neural architecture search. Specifically, a controller alternately samples a\nconcatenation of embeddings, according to its current belief of the\neffectiveness of individual embedding types in consideration for a task, and\nupdates the belief based on a reward. We follow strategies in reinforcement\nlearning to optimize the parameters of the controller and compute the reward\nbased on the accuracy of a task model, which is fed with the sampled\nconcatenation as input and trained on a task dataset. Empirical results on 6\ntasks and 21 datasets show that our approach outperforms strong baselines and\nachieves state-of-the-art performance with fine-tuned embeddings in all the\nevaluations.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 14:03:20 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 06:30:35 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 11:15:40 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 13:23:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Xinyu", ""], ["Jiang", "Yong", ""], ["Bach", "Nguyen", ""], ["Wang", "Tao", ""], ["Huang", "Zhongqiang", ""], ["Huang", "Fei", ""], ["Tu", "Kewei", ""]]}, {"id": "2010.05010", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang,\n  Zhongqiang Huang, Fei Huang, Kewei Tu", "title": "Structural Knowledge Distillation: Tractably Distilling Information for\n  Structured Predictor", "comments": "Accepted to Proceedings of ACL-IJCNLP 2021. 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge distillation is a critical technique to transfer knowledge between\nmodels, typically from a large model (the teacher) to a more fine-grained one\n(the student). The objective function of knowledge distillation is typically\nthe cross-entropy between the teacher and the student's output distributions.\nHowever, for structured prediction problems, the output space is exponential in\nsize; therefore, the cross-entropy objective becomes intractable to compute and\noptimize directly. In this paper, we derive a factorized form of the knowledge\ndistillation objective for structured prediction, which is tractable for many\ntypical choices of the teacher and student models. In particular, we show the\ntractability and empirical effectiveness of structural knowledge distillation\nbetween sequence labeling and dependency parsing models under four different\nscenarios: 1) the teacher and student share the same factorization form of the\noutput structure scoring function; 2) the student factorization produces more\nfine-grained substructures than the teacher factorization; 3) the teacher\nfactorization produces more fine-grained substructures than the student\nfactorization; 4) the factorization forms from the teacher and the student are\nincompatible.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 14:19:25 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 12:07:25 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 13:31:22 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 02:31:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Xinyu", ""], ["Jiang", "Yong", ""], ["Yan", "Zhaohui", ""], ["Jia", "Zixia", ""], ["Bach", "Nguyen", ""], ["Wang", "Tao", ""], ["Huang", "Zhongqiang", ""], ["Huang", "Fei", ""], ["Tu", "Kewei", ""]]}, {"id": "2010.05090", "submitter": "Kunal Chawla", "authors": "Kunal Chawla, Diyi Yang", "title": "Semi-supervised Formality Style Transfer using Language Model\n  Discriminator and Mutual Information Maximization", "comments": "EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formality style transfer is the task of converting informal sentences to\ngrammatically-correct formal sentences, which can be used to improve\nperformance of many downstream NLP tasks. In this work, we propose a\nsemi-supervised formality style transfer model that utilizes a language\nmodel-based discriminator to maximize the likelihood of the output sentence\nbeing formal, which allows us to use maximization of token-level conditional\nprobabilities for training. We further propose to maximize mutual information\nbetween source and target styles as our training objective instead of\nmaximizing the regular likelihood that often leads to repetitive and trivial\ngenerated responses. Experiments showed that our model outperformed previous\nstate-of-the-art baselines significantly in terms of both automated metrics and\nhuman judgement. We further generalized our model to unsupervised text style\ntransfer task, and achieved significant improvements on two benchmark sentiment\nstyle transfer datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 21:05:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chawla", "Kunal", ""], ["Yang", "Diyi", ""]]}, {"id": "2010.05096", "submitter": "Surabhi Datta", "authors": "Surabhi Datta and Shekhar Khanpara and Roy F. Riascos and Kirk Roberts", "title": "Leveraging Spatial Information in Radiology Reports for Ischemic Stroke\n  Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying fine-grained ischemic stroke phenotypes relies on identifying\nimportant clinical information. Radiology reports provide relevant information\nwith context to determine such phenotype information. We focus on stroke\nphenotypes with location-specific information: brain region affected,\nlaterality, stroke stage, and lacunarity. We use an existing fine-grained\nspatial information extraction system--Rad-SpatialNet--to identify clinically\nimportant information and apply simple domain rules on the extracted\ninformation to classify phenotypes. The performance of our proposed approach is\npromising (recall of 89.62% for classifying brain region and 74.11% for\nclassifying brain region, side, and stroke stage together). Our work\ndemonstrates that an information extraction system based on a fine-grained\nschema can be utilized to determine complex phenotypes with the inclusion of\nsimple domain rules. These phenotypes have the potential to facilitate stroke\nresearch focusing on post-stroke outcome and treatment planning based on the\nstroke location.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 21:35:42 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Datta", "Surabhi", ""], ["Khanpara", "Shekhar", ""], ["Riascos", "Roy F.", ""], ["Roberts", "Kirk", ""]]}, {"id": "2010.05103", "submitter": "Stephen Mussmann", "authors": "Stephen Mussmann, Robin Jia, Percy Liang", "title": "On the Importance of Adaptive Data Collection for Extremely Imbalanced\n  Pairwise Tasks", "comments": "In Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many pairwise classification tasks, such as paraphrase detection and\nopen-domain question answering, naturally have extreme label imbalance (e.g.,\n$99.99\\%$ of examples are negatives). In contrast, many recent datasets\nheuristically choose examples to ensure label balance. We show that these\nheuristics lead to trained models that generalize poorly: State-of-the art\nmodels trained on QQP and WikiQA each have only $2.4\\%$ average precision when\nevaluated on realistically imbalanced test data. We instead collect training\ndata with active learning, using a BERT-based embedding model to efficiently\nretrieve uncertain points from a very large pool of unlabeled utterance pairs.\nBy creating balanced training data with more informative negative examples,\nactive learning greatly improves average precision to $32.5\\%$ on QQP and\n$20.1\\%$ on WikiQA.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 21:56:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Mussmann", "Stephen", ""], ["Jia", "Robin", ""], ["Liang", "Percy", ""]]}, {"id": "2010.05106", "submitter": "Mehrad Moradshahi", "authors": "Mehrad Moradshahi, Giovanni Campagna, Sina J. Semnani, Silei Xu,\n  Monica S. Lam", "title": "Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine\n  Translation", "comments": "Published in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural\nMachine Translation (NMT) systems to localize a semantic parser for a new\nlanguage. Our methodology is to (1) generate training data automatically in the\ntarget language by augmenting machine-translated datasets with local entities\nscraped from public websites, (2) add a few-shot boost of human-translated\nsentences and train a novel XLMR-LSTM semantic parser, and (3) test the model\non natural utterances curated using human translators.\n  We assess the effectiveness of our approach by extending the current\ncapabilities of Schema2QA, a system for English Question Answering (QA) on the\nopen web, to 10 new languages for the restaurants and hotels domains. Our\nmodels achieve an overall test accuracy ranging between 61% and 69% for the\nhotels domain and between 64% and 78% for restaurants domain, which compares\nfavorably to 69% and 80% obtained for English parser trained on gold English\ndata and a few examples from validation set. We show our approach outperforms\nthe previous state-of-the-art methodology by more than 30% for hotels and 40%\nfor restaurants with localized ontologies for the subset of languages tested.\n  Our methodology enables any software developer to add a new language\ncapability to a QA system for a new domain, leveraging machine translation, in\nless than 24 hours.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 22:03:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Moradshahi", "Mehrad", ""], ["Campagna", "Giovanni", ""], ["Semnani", "Sina J.", ""], ["Xu", "Silei", ""], ["Lam", "Monica S.", ""]]}, {"id": "2010.05111", "submitter": "Shyam Subramanian", "authors": "Shyam Subramanian, Kyumin Lee", "title": "Hierarchical Evidence Set Modeling for Automated Fact Extraction and\n  Verification", "comments": "12 pages, 7 figures. Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated fact extraction and verification is a challenging task that\ninvolves finding relevant evidence sentences from a reliable corpus to verify\nthe truthfulness of a claim. Existing models either (i) concatenate all the\nevidence sentences, leading to the inclusion of redundant and noisy\ninformation; or (ii) process each claim-evidence sentence pair separately and\naggregate all of them later, missing the early combination of related sentences\nfor more accurate claim verification. Unlike the prior works, in this paper, we\npropose Hierarchical Evidence Set Modeling (HESM), a framework to extract\nevidence sets (each of which may contain multiple evidence sentences), and\nverify a claim to be supported, refuted or not enough info, by encoding and\nattending the claim and evidence sets at different levels of hierarchy. Our\nexperimental results show that HESM outperforms 7 state-of-the-art methods for\nfact extraction and claim verification. Our source code is available at\nhttps://github.com/ShyamSubramanian/HESM.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 22:27:17 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Subramanian", "Shyam", ""], ["Lee", "Kyumin", ""]]}, {"id": "2010.05122", "submitter": "Zuchao Li", "authors": "Zuchao Li, Hai Zhao, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro\n  Sumita", "title": "SJTU-NICT's Supervised and Unsupervised Neural Machine Translation\n  Systems for the WMT20 News Translation Task", "comments": "WMT20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduced our joint team SJTU-NICT 's participation in the\nWMT 2020 machine translation shared task. In this shared task, we participated\nin four translation directions of three language pairs: English-Chinese,\nEnglish-Polish on supervised machine translation track, German-Upper Sorbian on\nlow-resource and unsupervised machine translation tracks. Based on different\nconditions of language pairs, we have experimented with diverse neural machine\ntranslation (NMT) techniques: document-enhanced NMT, XLM pre-trained language\nmodel enhanced NMT, bidirectional translation as a pre-training, reference\nlanguage based UNMT, data-dependent gaussian prior objective, and BT-BLEU\ncollaborative filtering self-training. We also used the TF-IDF algorithm to\nfilter the training set to obtain a domain more similar set with the test set\nfor finetuning. In our submissions, the primary systems won the first place on\nEnglish to Chinese, Polish to English, and German to Upper Sorbian translation\ndirections.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 00:40:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Zuchao", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""], ["Chen", "Kehai", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "2010.05129", "submitter": "Dongyeop Kang", "authors": "Dongyeop Kang, Andrew Head, Risham Sidhu, Kyle Lo, Daniel S. Weld,\n  Marti A. Hearst", "title": "Document-Level Definition Detection in Scholarly Documents: Existing\n  Models, Error Analyses, and Future Directions", "comments": "Workshop on Scholarly Document Processing (SDP), EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of definition detection is important for scholarly papers, because\npapers often make use of technical terminology that may be unfamiliar to\nreaders. Despite prior work on definition detection, current approaches are far\nfrom being accurate enough to use in real-world applications. In this paper, we\nfirst perform in-depth error analysis of the current best performing definition\ndetection system and discover major causes of errors. Based on this analysis,\nwe develop a new definition detection system, HEDDEx, that utilizes syntactic\nfeatures, transformer encoders, and heuristic filters, and evaluate it on a\nstandard sentence-level benchmark. Because current benchmarks evaluate randomly\nsampled sentences, we propose an alternative evaluation that assesses every\nsentence within a document. This allows for evaluating recall in addition to\nprecision. HEDDEx outperforms the leading system on both the sentence-level and\nthe document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively.\nWe note that performance on the high-recall document-level task is much lower\nthan in the standard evaluation approach, due to the necessity of incorporation\nof document structure as features. We discuss remaining challenges in\ndocument-level definition detection, ideas for improvements, and potential\nissues for the development of reading aid applications.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 01:16:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kang", "Dongyeop", ""], ["Head", "Andrew", ""], ["Sidhu", "Risham", ""], ["Lo", "Kyle", ""], ["Weld", "Daniel S.", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2010.05139", "submitter": "Yiran Chen", "authors": "Yiran Chen, Pengfei Liu, Ming Zhong, Zi-Yi Dou, Danqing Wang, Xipeng\n  Qiu and Xuanjing Huang", "title": "CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural\n  Summarization Systems", "comments": "13 pages, Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based models augmented with unsupervised pre-trained knowledge\nhave achieved impressive performance on text summarization. However, most\nexisting evaluation methods are limited to an in-domain setting, where\nsummarizers are trained and evaluated on the same dataset. We argue that this\napproach can narrow our understanding of the generalization ability for\ndifferent summarization systems. In this paper, we perform an in-depth analysis\nof characteristics of different datasets and investigate the performance of\ndifferent summarization models under a cross-dataset setting, in which a\nsummarizer trained on one corpus will be evaluated on a range of out-of-domain\ncorpora. A comprehensive study of 11 representative summarization systems on 5\ndatasets from different domains reveals the effect of model architectures and\ngeneration ways (i.e. abstractive and extractive) on model generalization\nability. Further, experimental results shed light on the limitations of\nexisting summarizers. Brief introduction and supplementary code can be found in\nhttps://github.com/zide05/CDEvalSumm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 02:19:15 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 12:11:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Yiran", ""], ["Liu", "Pengfei", ""], ["Zhong", "Ming", ""], ["Dou", "Zi-Yi", ""], ["Wang", "Danqing", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2010.05141", "submitter": "Dongyeop Kang", "authors": "Dongyeop Kang, Eduard Hovy", "title": "Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of contextualized language models on various NLP\ntasks, language model itself cannot capture textual coherence of a long,\nmulti-sentence document (e.g., a paragraph). Humans often make structural\ndecisions on what and how to say about before making utterances. Guiding\nsurface realization with such high-level decisions and structuring text in a\ncoherent way is essentially called a planning process. Where can the model\nlearn such high-level coherence? A paragraph itself contains various forms of\ninductive coherence signals called self-supervision in this work, such as\nsentence orders, topical keywords, rhetorical structures, and so on. Motivated\nby that, this work proposes a new paragraph completion task PARCOM; predicting\nmasked sentences in a paragraph. However, the task suffers from predicting and\nselecting appropriate topical content with respect to the given context. To\naddress that, we propose a self-supervised text planner SSPlanner that predicts\nwhat to say first (content prediction), then guides the pretrained language\nmodel (surface realization) using the predicted content. SSPlanner outperforms\nthe baseline generation models on the paragraph completion task in both\nautomatic and human evaluation. We also find that a combination of noun and\nverb types of keywords is the most effective for content selection. As more\nnumber of content keywords are provided, overall generation quality also\nincreases.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 02:38:21 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kang", "Dongyeop", ""], ["Hovy", "Eduard", ""]]}, {"id": "2010.05143", "submitter": "Xiang Yue", "authors": "Xiang Yue and Shuang Zhou", "title": "PHICON: Improving Generalization of Clinical Text De-identification\n  Models via Data Augmentation", "comments": "Accepted by The 3rd ClinicalNLP Workshop at EMNLP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De-identification is the task of identifying protected health information\n(PHI) in the clinical text. Existing neural de-identification models often fail\nto generalize to a new dataset. We propose a simple yet effective data\naugmentation method PHICON to alleviate the generalization issue. PHICON\nconsists of PHI augmentation and Context augmentation, which creates augmented\ntraining corpora by replacing PHI entities with named-entities sampled from\nexternal sources, and by changing background context with synonym replacement\nor random word insertion, respectively. Experimental results on the i2b2 2006\nand 2014 de-identification challenge datasets show that PHICON can help three\nselected de-identification models boost F1-score (by at most 8.6%) on\ncross-dataset test setting. We also discuss how much augmentation to use and\nhow each augmentation method influences the performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 02:57:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yue", "Xiang", ""], ["Zhou", "Shuang", ""]]}, {"id": "2010.05150", "submitter": "Tsung-Yen Yang", "authors": "Tsung-Yen Yang and Michael Hu and Yinlam Chow and Peter J. Ramadge and\n  Karthik Narasimhan", "title": "Safe Reinforcement Learning with Natural Language Constraints", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of learning control policies for tasks\nwhen provided with constraints in natural language. In contrast to instruction\nfollowing, language here is used not to specify goals, but rather to describe\nsituations that an agent must avoid during its exploration of the environment.\nSpecifying constraints in natural language also differs from the predominant\nparadigm in safe reinforcement learning, where safety criteria are enforced by\nhand-defined cost functions. While natural language allows for easy and\nflexible specification of safety constraints and budget limitations, its\nambiguous nature presents a challenge when mapping these specifications into\nrepresentations that can be used by techniques for safe reinforcement learning.\nTo address this, we develop a model that contains two components: (1) a\nconstraint interpreter to encode natural language constraints into vector\nrepresentations capturing spatial and temporal information on forbidden states,\nand (2) a policy network that uses these representations to output a policy\nwith minimal constraint violations. Our model is end-to-end differentiable and\nwe train it using a recently proposed algorithm for constrained policy\noptimization. To empirically demonstrate the effectiveness of our approach, we\ncreate a new benchmark task for autonomous navigation with crowd-sourced\nfree-form text specifying three different types of constraints. Our method\noutperforms several baselines by achieving 6-7 times higher returns and 76%\nfewer constraint violations on average. Dataset and code to reproduce our\nexperiments are available at https://sites.google.com/view/polco-hazard-world/.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 03:41:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yang", "Tsung-Yen", ""], ["Hu", "Michael", ""], ["Chow", "Yinlam", ""], ["Ramadge", "Peter J.", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2010.05164", "submitter": "Laurence Clarfeld", "authors": "Laurence A. Clarfeld, Robert Gramling, Donna M. Rizzo, Margaret J.\n  Eppstein", "title": "A General Model of Conversational Dynamics and an Example Application in\n  Serious Illness Communication", "comments": "34 pages, 20 figures, submitted to PLOS One (in review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversation has been a primary means for the exchange of information since\nancient times. Understanding patterns of information flow in conversations is a\ncritical step in assessing and improving communication quality. In this paper,\nwe describe COnversational DYnamics Model (CODYM) analysis, a novel approach\nfor studying patterns of information flow in conversations. CODYMs are Markov\nModels that capture sequential dependencies in the lengths of speaker turns.\nThe proposed method is automated and scalable, and preserves the privacy of the\nconversational participants. The primary function of CODYM analysis is to\nquantify and visualize patterns of information flow, concisely summarized over\nsequential turns from one or more conversations. Our approach is general and\ncomplements existing methods, providing a new tool for use in the analysis of\nany type of conversation. As an important first application, we demonstrate the\nmodel on transcribed conversations between palliative care clinicians and\nseriously ill patients. These conversations are dynamic and complex, taking\nplace amidst heavy emotions, and include difficult topics such as end-of-life\npreferences and patient values. We perform a versatile set of CODYM analyses\nthat (a) establish the validity of the model by confirming known patterns of\nconversational turn-taking and word usage, (b) identify normative patterns of\ninformation flow in serious illness conversations, and (c) show how these\npatterns vary across narrative time and differ under expressions of anger, fear\nand sadness. Potential applications of CODYMs range from assessment and\ntraining of effective healthcare communication to comparing conversational\ndynamics across language and culture, with the prospect of identifying\nuniversal similarities and unique \"fingerprints\" of information flow.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 04:33:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Clarfeld", "Laurence A.", ""], ["Gramling", "Robert", ""], ["Rizzo", "Donna M.", ""], ["Eppstein", "Margaret J.", ""]]}, {"id": "2010.05171", "submitter": "Changhan Wang", "authors": "Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino", "title": "fairseq S2T: Fast Speech-to-Text Modeling with fairseq", "comments": "Accepted to AACL 2020 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)\nmodeling tasks such as end-to-end speech recognition and speech-to-text\ntranslation. It follows fairseq's careful design for scalability and\nextensibility. We provide end-to-end workflows from data pre-processing, model\ntraining to offline (online) inference. We implement state-of-the-art RNN-based\nas well as Transformer-based models and open-source detailed training recipes.\nFairseq's machine translation models and language models can be seamlessly\nintegrated into S2T workflows for multi-task learning or transfer learning.\nFairseq S2T documentation and examples are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 05:36:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Changhan", ""], ["Tang", "Yun", ""], ["Ma", "Xutai", ""], ["Wu", "Anne", ""], ["Okhonko", "Dmytro", ""], ["Pino", "Juan", ""]]}, {"id": "2010.05185", "submitter": "Chenhui Chu", "authors": "Chenhui Chu, Yuto Takebayashi, Mishra Vipul, Yuta Nakashima", "title": "Constructing a Visual Relationship Authenticity Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visual relationship denotes a relationship between two objects in an image,\nwhich can be represented as a triplet of (subject; predicate; object). Visual\nrelationship detection is crucial for scene understanding in images. Existing\nvisual relationship detection datasets only contain true relationships that\ncorrectly describe the content in an image. However, distinguishing false\nvisual relationships from true ones is also crucial for image understanding and\ngrounded natural language processing. In this paper, we construct a visual\nrelationship authenticity dataset, where both true and false relationships\namong all objects appeared in the captions in the Flickr30k entities image\ncaption dataset are annotated. The dataset is available at\nhttps://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this\ndataset can promote the study on both vision and language understanding.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 07:38:33 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chu", "Chenhui", ""], ["Takebayashi", "Yuto", ""], ["Vipul", "Mishra", ""], ["Nakashima", "Yuta", ""]]}, {"id": "2010.05190", "submitter": "Siddharth Karamcheti", "authors": "Siddharth Karamcheti, Dorsa Sadigh, Percy Liang", "title": "Learning Adaptive Language Interfaces through Decomposition", "comments": "Accepted at the 1st Workshop for Interactive and Executable Semantic\n  Parsing (IntEx-SemPar) @ EMNLP 2020. 11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to create an interactive natural language interface that\nefficiently and reliably learns from users to complete tasks in simulated\nrobotics settings. We introduce a neural semantic parsing system that learns\nnew high-level abstractions through decomposition: users interactively teach\nthe system by breaking down high-level utterances describing novel behavior\ninto low-level steps that it can understand. Unfortunately, existing methods\neither rely on grammars which parse sentences with limited flexibility, or\nneural sequence-to-sequence models that do not learn efficiently or reliably\nfrom individual examples. Our approach bridges this gap, demonstrating the\nflexibility of modern neural systems, as well as the one-shot reliable\ngeneralization of grammar-based methods. Our crowdsourced interactive\nexperiments suggest that over time, users complete complex tasks more\nefficiently while using our system by leveraging what they just taught. At the\nsame time, getting users to trust the system enough to be incentivized to teach\nhigh-level utterances is still an ongoing challenge. We end with a discussion\nof some of the obstacles we need to overcome to fully realize the potential of\nthe interactive paradigm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 08:27:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Karamcheti", "Siddharth", ""], ["Sadigh", "Dorsa", ""], ["Liang", "Percy", ""]]}, {"id": "2010.05193", "submitter": "Chenhui Chu", "authors": "Vipul Mishra, Chenhui Chu and Yuki Arase", "title": "Lexically Cohesive Neural Machine Translation with Copy Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexically cohesive translations preserve consistency in word choices in\ndocument-level translation. We employ a copy mechanism into a context-aware\nneural machine translation model to allow copying words from previous\ntranslation outputs. Different from previous context-aware neural machine\ntranslation models that handle all the discourse phenomena implicitly, our\nmodel explicitly addresses the lexical cohesion problem by boosting the\nprobabilities to output words consistently. We conduct experiments on Japanese\nto English translation using an evaluation dataset for discourse translation.\nThe results showed that the proposed model significantly improved lexical\ncohesion compared to previous context-aware models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 08:39:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Mishra", "Vipul", ""], ["Chu", "Chenhui", ""], ["Arase", "Yuki", ""]]}, {"id": "2010.05194", "submitter": "Giannis Karamanolakis", "authors": "Ziyi Liu, Giannis Karamanolakis, Daniel Hsu, Luis Gravano", "title": "Detecting Foodborne Illness Complaints in Multiple Languages Using\n  English Annotations Only", "comments": "Accepted for the 11th International Workshop on Health Text Mining\n  and Information Analysis (LOUHI@EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health departments have been deploying text classification systems for the\nearly detection of foodborne illness complaints in social media documents such\nas Yelp restaurant reviews. Current systems have been successfully applied for\ndocuments in English and, as a result, a promising direction is to increase\ncoverage and recall by considering documents in additional languages, such as\nSpanish or Chinese. Training previous systems for more languages, however,\nwould be expensive, as it would require the manual annotation of many documents\nfor each new target language. To address this challenge, we consider\ncross-lingual learning and train multilingual classifiers using only the\nannotations for English-language reviews. Recent zero-shot approaches based on\npre-trained multi-lingual BERT (mBERT) have been shown to effectively align\nlanguages for aspects such as sentiment. Interestingly, we show that those\napproaches are less effective for capturing the nuances of foodborne illness,\nour public health application of interest. To improve performance without extra\nannotations, we create artificial training documents in the target language\nthrough machine translation and train mBERT jointly for the source (English)\nand target language. Furthermore, we show that translating labeled documents to\nmultiple languages leads to additional performance improvements for some target\nlanguages. We demonstrate the benefits of our approach through extensive\nexperiments with Yelp restaurant reviews in seven languages. Our classifiers\nidentify foodborne illness complaints in multilingual reviews from the Yelp\nChallenge dataset, which highlights the potential of our general approach for\ndeployment in health departments.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 08:46:17 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Ziyi", ""], ["Karamanolakis", "Giannis", ""], ["Hsu", "Daniel", ""], ["Gravano", "Luis", ""]]}, {"id": "2010.05202", "submitter": "Wangchunshu Zhou", "authors": "Qifei Li and Wangchunshu Zhou", "title": "Connecting the Dots Between Fact Verification and Fake News Detection", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact verification models have enjoyed a fast advancement in the last two\nyears with the development of pre-trained language models like BERT and the\nrelease of large scale datasets such as FEVER. However, the challenging problem\nof fake news detection has not benefited from the improvement of fact\nverification models, which is closely related to fake news detection. In this\npaper, we propose a simple yet effective approach to connect the dots between\nfact verification and fake news detection. Our approach first employs a text\nsummarization model pre-trained on news corpora to summarize the long news\narticle into a short claim. Then we use a fact verification model pre-trained\non the FEVER dataset to detect whether the input news article is real or fake.\nOur approach makes use of the recent success of fact verification models and\nenables zero-shot fake news detection, alleviating the need of large-scale\ntraining data to train fake news detection models. Experimental results on\nFakenewsNet, a benchmark dataset for fake news detection, demonstrate the\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 09:28:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Qifei", ""], ["Zhou", "Wangchunshu", ""]]}, {"id": "2010.05223", "submitter": "Harshil Jain", "authors": "Harshil Jain, Akshat Agarwal, Kumar Shridhar, Denis Kleyko", "title": "End to End Binarized Neural Networks for Text Classification", "comments": "14 pages. Accepted at the SustaiNLP Workshop on Simple and Efficient\n  Natural Language Processing at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have demonstrated their superior performance in almost\nevery Natural Language Processing task, however, their increasing complexity\nraises concerns. In particular, these networks require high expenses on\ncomputational hardware, and training budget is a concern for many. Even for a\ntrained network, the inference phase can be too demanding for\nresource-constrained devices, thus limiting its applicability. The\nstate-of-the-art transformer models are a vivid example. Simplifying the\ncomputations performed by a network is one way of relaxing the complexity\nrequirements. In this paper, we propose an end to end binarized neural network\narchitecture for the intent classification task. In order to fully utilize the\npotential of end to end binarization, both input representations (vector\nembeddings of tokens statistics) and the classifier are binarized. We\ndemonstrate the efficiency of such architecture on the intent classification of\nshort texts over three datasets and for text classification with a larger\ndataset. The proposed architecture achieves comparable to the state-of-the-art\nresults on standard intent classification datasets while utilizing ~ 20-40%\nlesser memory and training time. Furthermore, the individual components of the\narchitecture, such as binarized vector embeddings of documents or binarized\nclassifiers, can be used separately with not necessarily fully binary\narchitectures.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 11:21:53 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Jain", "Harshil", ""], ["Agarwal", "Akshat", ""], ["Shridhar", "Kumar", ""], ["Kleyko", "Denis", ""]]}, {"id": "2010.05229", "submitter": "Tanya Schmah", "authors": "Aditya Ohri and Tanya Schmah", "title": "Machine Translation of Mathematical Text", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have implemented a machine translation system, the PolyMath Translator,\nfor LaTeX documents containing mathematical text. The current implementation\ntranslates English LaTeX to French LaTeX, attaining a BLEU score of 53.5 on a\nheld-out test corpus of mathematical sentences. It produces LaTeX documents\nthat can be compiled to PDF without further editing. The system first converts\nthe body of an input LaTeX document into English sentences containing math\ntokens, using the pandoc universal document converter to parse LaTeX input. We\nhave trained a Transformer-based translator model, using OpenNMT, on a combined\ncorpus containing a small proportion of domain-specific sentences. Our full\nsystem uses both this Transformer model and Google Translate, the latter being\nused as a backup to better handle linguistic features that do not appear in our\ntraining dataset. If the Transformer model does not have confidence in its\ntranslation, as determined by a high perplexity score, then we use Google\nTranslate with a custom glossary. This backup was used 26% of the time on our\ntest corpus of mathematical sentences. The PolyMath Translator is available as\na web service at www.polymathtrans.ai.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 11:59:40 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ohri", "Aditya", ""], ["Schmah", "Tanya", ""]]}, {"id": "2010.05230", "submitter": "Xinpeng Wang", "authors": "Feifei Xu, Xinpeng Wang, Yunpu Ma, Volker Tresp, Yuyi Wang, Shanlin\n  Zhou and Haizhou Du", "title": "Controllable Multi-Character Psychology-Oriented Story Generation", "comments": "Accepted by CIKM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story generation, which aims to generate a long and coherent story\nautomatically based on the title or an input sentence, is an important research\narea in the field of natural language generation. There is relatively little\nwork on story generation with appointed emotions. Most existing works focus on\nusing only one specific emotion to control the generation of a whole story and\nignore the emotional changes in the characters in the course of the story. In\nour work, we aim to design an emotional line for each character that considers\nmultiple emotions common in psychological theories, with the goal of generating\nstories with richer emotional changes in the characters. To the best of our\nknowledge, this work is first to focuses on characters' emotional lines in\nstory generation. We present a novel model-based attention mechanism that we\ncall SoCP (Storytelling of multi-Character Psychology). We show that the\nproposed model can generate stories considering the changes in the\npsychological state of different characters. To take into account the\nparticularity of the model, in addition to commonly used evaluation\nindicators(BLEU, ROUGE, etc.), we introduce the accuracy rate of psychological\nstate control as a novel evaluation metric. The new indicator reflects the\neffect of the model on the psychological state control of story characters.\nExperiments show that with SoCP, the generated stories follow the psychological\nstate for each character according to both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 12:05:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Feifei", ""], ["Wang", "Xinpeng", ""], ["Ma", "Yunpu", ""], ["Tresp", "Volker", ""], ["Wang", "Yuyi", ""], ["Zhou", "Shanlin", ""], ["Du", "Haizhou", ""]]}, {"id": "2010.05243", "submitter": "Debaditya Pal", "authors": "Debaditya Pal, Harsh Sharma, Kaustubh Chaudhari", "title": "Data Agnostic RoBERTa-based Natural Language to SQL Query Generation", "comments": "8 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational databases are among the most widely used architectures to store\nmassive amounts of data in the modern world. However, there is a barrier\nbetween these databases and the average user. The user often lacks the\nknowledge of a query language such as SQL required to interact with the\ndatabase. The NL2SQL task aims at finding deep learning approaches to solve\nthis problem by converting natural language questions into valid SQL queries.\nGiven the sensitive nature of some databases and the growing need for data\nprivacy, we have presented an approach with data privacy at its core. We have\npassed RoBERTa embeddings and data-agnostic knowledge vectors into LSTM based\nsubmodels to predict the final query. Although we have not achieved state of\nthe art results, we have eliminated the need for the table data, right from the\ntraining of the model, and have achieved a test set execution accuracy of\n76.7%. By eliminating the table data dependency while training we have created\na model capable of zero shot learning based on the natural language question\nand table schema alone.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 13:18:46 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 06:29:58 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 05:55:10 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Pal", "Debaditya", ""], ["Sharma", "Harsh", ""], ["Chaudhari", "Kaustubh", ""]]}, {"id": "2010.05248", "submitter": "Qingqing Cao", "authors": "Qingqing Cao, Aruna Balasubramanian, Niranjan Balasubramanian", "title": "Towards Accurate and Reliable Energy Measurement of NLP Models", "comments": "Accepted to SustaiNLP 2020 (co-located with EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate and reliable measurement of energy consumption is critical for\nmaking well-informed design choices when choosing and training large scale NLP\nmodels. In this work, we show that existing software-based energy measurements\nare not accurate because they do not take into account hardware differences and\nhow resource utilization affects energy consumption. We conduct energy\nmeasurement experiments with four different models for a question answering\ntask. We quantify the error of existing software-based energy measurements by\nusing a hardware power meter that provides highly accurate energy measurements.\nOur key takeaway is the need for a more accurate energy estimation model that\ntakes into account hardware variabilities and the non-linear relationship\nbetween resource utilization and energy consumption. We release the code and\ndata at https://github.com/csarron/sustainlp2020-energy.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 13:44:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Cao", "Qingqing", ""], ["Balasubramanian", "Aruna", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2010.05256", "submitter": "Yutai Hou", "authors": "Yutai Hou, Yongkui Lai, Yushan Wu, Wanxiang Che, Ting Liu", "title": "Few-shot Learning for Multi-label Intent Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the few-shot multi-label classification for user\nintent detection. For multi-label intent detection, state-of-the-art work\nestimates label-instance relevance scores and uses a threshold to select\nmultiple associated intent labels. To determine appropriate thresholds with\nonly a few examples, we first learn universal thresholding experience on\ndata-rich domains, and then adapt the thresholds to certain few-shot domains\nwith a calibration based on nonparametric learning. For better calculation of\nlabel-instance relevance score, we introduce label name embedding as anchor\npoints in representation space, which refines representations of different\nclasses to be well-separated from each other. Experiments on two datasets show\nthat the proposed model significantly outperforms strong baselines in both\none-shot and five-shot settings.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 14:42:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hou", "Yutai", ""], ["Lai", "Yongkui", ""], ["Wu", "Yushan", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2010.05265", "submitter": "Shauli Ravfogel", "authors": "Shauli Ravfogel, Yanai Elazar, Jacob Goldberger, Yoav Goldberg", "title": "Unsupervised Distillation of Syntactic Information from Contextualized\n  Word Representations", "comments": "Accepted in BlackboxNLP@EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized word representations, such as ELMo and BERT, were shown to\nperform well on various semantic and syntactic tasks. In this work, we tackle\nthe task of unsupervised disentanglement between semantics and structure in\nneural language representations: we aim to learn a transformation of the\ncontextualized vectors, that discards the lexical semantics, but keeps the\nstructural information. To this end, we automatically generate groups of\nsentences which are structurally similar but semantically different, and use\nmetric-learning approach to learn a transformation that emphasizes the\nstructural component that is encoded in the vectors. We demonstrate that our\ntransformation clusters vectors in space by structural properties, rather than\nby lexical semantics. Finally, we demonstrate the utility of our distilled\nrepresentations by showing that they outperform the original contextualized\nrepresentations in a few-shot parsing setting.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 15:13:18 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 20:41:09 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ravfogel", "Shauli", ""], ["Elazar", "Yanai", ""], ["Goldberger", "Jacob", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2010.05269", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Khalid Alnajjar, Mika H\\\"am\\\"al\\\"ainen, Niko Partanen, Jack Rueter", "title": "Automated Prediction of Medieval Arabic Diacritics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study uses a character level neural machine translation approach trained\non a long short-term memory-based bi-directional recurrent neural network\narchitecture for diacritization of Medieval Arabic. The results improve from\nthe online tool used as a baseline. A diacritization model have been published\nopenly through an easy to use Python package available on PyPi and Zenodo. We\nhave found that context size should be considered when optimizing a feasible\nprediction model.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 15:21:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Alnajjar", "Khalid", ""], ["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Partanen", "Niko", ""], ["Rueter", "Jack", ""]]}, {"id": "2010.05293", "submitter": "Jared Millson", "authors": "Jared Millson", "title": "A Defeasible Calculus for Zetetic Agents", "comments": null, "journal-ref": null, "doi": "10.12775/LLP.2020.019", "report-no": null, "categories": "cs.AI cs.CL math.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The study of defeasible reasoning unites epistemologists with those working\nin AI, in part, because both are interested in epistemic rationality. While it\nis traditionally thought to govern the formation and (with)holding of beliefs,\nepistemic rationality may also apply to the interrogative attitudes associated\nwith our core epistemic practice of inquiry, such as wondering, investigating,\nand curiosity. Since generally intelligent systems should be capable of\nrational inquiry, AI researchers have a natural interest in the norms that\ngovern interrogative attitudes. Following its recent coinage, we use the term\n\"zetetic\" to refer to the properties and norms associated with the capacity to\ninquire. In this paper, we argue that zetetic norms can be modeled via\ndefeasible inferences to and from questions---a.k.a erotetic inferences---in a\nmanner similar to the way norms of epistemic rationality are represented by\ndefeasible inference rules. We offer a sequent calculus that accommodates the\nunique features of \"erotetic defeat\" and that exhibits the computational\nproperties needed to inform the design of zetetic agents. The calculus\npresented here is an improved version of the one presented in Millson (2019),\nextended to cover a new class of defeasible erotetic inferences.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 17:39:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Millson", "Jared", ""]]}, {"id": "2010.05317", "submitter": "Sai Prabhakar Pandi Selvaraj", "authors": "Dhruvesh Patel, Sandeep Konam, Sai P. Selvaraj", "title": "Weakly Supervised Medication Regimen Extraction from Medical\n  Conversations", "comments": "To appear in the Proceedings of the Clinical Natural Language\n  Processing Workshop, EMNLP, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Medication Regimen (MR) extraction from medical conversations can\nnot only improve recall and help patients follow through with their care plan,\nbut also reduce the documentation burden for doctors. In this paper, we focus\non extracting spans for frequency, route and change, corresponding to\nmedications discussed in the conversation. We first describe a unique dataset\nof annotated doctor-patient conversations and then present a weakly supervised\nmodel architecture that can perform span extraction using noisy classification\ndata. The model utilizes an attention bottleneck inside a classification model\nto perform the extraction. We experiment with several variants of attention\nscoring and projection functions and propose a novel transformer-based\nattention scoring function (TAScore). The proposed combination of TAScore and\nFusedmax projection achieves a 10 point increase in Longest Common Substring F1\ncompared to the baseline of additive scoring plus softmax projection.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 18:53:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Patel", "Dhruvesh", ""], ["Konam", "Sandeep", ""], ["Selvaraj", "Sai P.", ""]]}, {"id": "2010.05318", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov", "title": "TransQuest at WMT2020: Sentence-Level Direct Assessment", "comments": "Accepted to WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the team TransQuest's participation in Sentence-Level\nDirect Assessment shared task in WMT 2020. We introduce a simple QE framework\nbased on cross-lingual transformers, and we use it to implement and evaluate\ntwo different neural architectures. The proposed methods achieve\nstate-of-the-art results surpassing the results obtained by OpenKiwi, the\nbaseline used in the shared task. We further fine tune the QE framework by\nperforming ensemble and data augmentation. Our approach is the winning solution\nin all of the language pairs according to the WMT 2020 official results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 18:53:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Orasan", "Constantin", ""], ["Mitkov", "Ruslan", ""]]}, {"id": "2010.05324", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Marcos Zampieri", "title": "Multilingual Offensive Language Identification with Cross-lingual\n  Embeddings", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of English\ndata available by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in languages with less resources. We\nproject predictions on comparable data in Bengali, Hindi, and Spanish and we\nreport results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and\n0.7513 F1 macro for Spanish. Finally, we show that our approach compares\nfavorably to the best systems submitted to recent shared tasks on these three\nlanguages, confirming the robustness of cross-lingual contextual embeddings and\ntransfer learning for this task.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:17:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2010.05327", "submitter": "Tharindu Ranasinghe Mr", "authors": "Hansi Hettiarachchi, Tharindu Ranasinghe", "title": "InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative\n  Tweet Extraction", "comments": "Accepted to the 6th Workshop on Noisy User-generated Text (W-NUT) at\n  EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying informative tweets is an important step when building information\nextraction systems based on social media. WNUT-2020 Task 2 was organised to\nrecognise informative tweets from noise tweets. In this paper, we present our\napproach to tackle the task objective using transformers. Overall, our approach\nachieves 10th place in the final rankings scoring 0.9004 F1 score for the test\nset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:31:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hettiarachchi", "Hansi", ""], ["Ranasinghe", "Tharindu", ""]]}, {"id": "2010.05330", "submitter": "Brielen Madureira", "authors": "Brielen Madureira and David Schlangen", "title": "Incremental Processing in the Age of Non-Incremental Encoders: An\n  Empirical Assessment of Bidirectional Models for Incremental NLU", "comments": "Accepted to the EMNLP 2020 conference (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While humans process language incrementally, the best language encoders\ncurrently used in NLP do not. Both bidirectional LSTMs and Transformers assume\nthat the sequence that is to be encoded is available in full, to be processed\neither forwards and backwards (BiLSTMs) or as a whole (Transformers). We\ninvestigate how they behave under incremental interfaces, when partial output\nmust be provided based on partial input seen up to a certain time step, which\nmay happen in interactive systems. We test five models on various NLU datasets\nand compare their performance using three incremental evaluation metrics. The\nresults support the possibility of using bidirectional encoders in incremental\nmode while retaining most of their non-incremental quality. The\n\"omni-directional\" BERT model, which achieves better non-incremental\nperformance, is impacted more by the incremental access. This can be alleviated\nby adapting the training regime (truncated training), or the testing procedure,\nby delaying the output until some right context is available or by\nincorporating hypothetical right contexts generated by a language model like\nGPT-2.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:51:21 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Madureira", "Brielen", ""], ["Schlangen", "David", ""]]}, {"id": "2010.05332", "submitter": "Danielle Saunders", "authors": "Danielle Saunders and Rosie Sallis and Bill Byrne", "title": "Neural Machine Translation Doesn't Translate Gender Coreference Right\n  Unless You Make It", "comments": "Workshop on Gender Bias in NLP (GeBNLP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) has been shown to struggle with grammatical\ngender that is dependent on the gender of human referents, which can cause\ngender bias effects. Many existing approaches to this problem seek to control\ngender inflection in the target language by explicitly or implicitly adding a\ngender feature to the source sentence, usually at the sentence level.\n  In this paper we propose schemes for incorporating explicit word-level gender\ninflection tags into NMT. We explore the potential of this gender-inflection\ncontrolled translation when the gender feature can be determined from a human\nreference, or when a test sentence can be automatically gender-tagged,\nassessing on English-to-Spanish and English-to-German translation.\n  We find that simple existing approaches can over-generalize a gender-feature\nto multiple entities in a sentence, and suggest effective alternatives in the\nform of tagged coreference adaptation data. We also propose an extension to\nassess translations of gender-neutral entities from English given a\ncorresponding linguistic convention, such as a non-binary inflection, in the\ntarget language.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 20:05:42 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 15:02:08 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Saunders", "Danielle", ""], ["Sallis", "Rosie", ""], ["Byrne", "Bill", ""]]}, {"id": "2010.05333", "submitter": "Danielle Saunders", "authors": "Danielle Saunders and Bill Byrne", "title": "Addressing Exposure Bias With Document Minimum Risk Training: Cambridge\n  at the WMT20 Biomedical Translation Task", "comments": "WMT20 biomedical task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2020 WMT Biomedical translation task evaluated Medline abstract\ntranslations. This is a small-domain translation task, meaning limited relevant\ntraining data with very distinct style and vocabulary. Models trained on such\ndata are susceptible to exposure bias effects, particularly when training\nsentence pairs are imperfect translations of each other. This can result in\npoor behaviour during inference if the model learns to neglect the source\nsentence.\n  The UNICAM entry addresses this problem during fine-tuning using a robust\nvariant on Minimum Risk Training. We contrast this approach with data-filtering\nto remove `problem' training examples. Under MRT fine-tuning we obtain good\nresults for both directions of English-German and English-Spanish biomedical\ntranslation. In particular we achieve the best English-to-Spanish translation\nresult and second-best Spanish-to-English result, despite using only single\nmodels with no ensembling.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 20:09:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Saunders", "Danielle", ""], ["Byrne", "Bill", ""]]}, {"id": "2010.05338", "submitter": "Ramy Baly", "authors": "Ramy Baly, Giovanni Da San Martino, James Glass and Preslav Nakov", "title": "We Can Detect Your Bias: Predicting the Political Ideology of News\n  Articles", "comments": "Political bias, bias in news, neural networks bias, adversarial\n  adaptation, triplet loss, transformers, recurrent neural networks", "journal-ref": "EMNLP-2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the task of predicting the leading political ideology or bias of\nnews articles. First, we collect and release a large dataset of 34,737 articles\nthat were manually annotated for political ideology -left, center, or right-,\nwhich is well-balanced across both topics and media. We further use a\nchallenging experimental setup where the test examples come from media that\nwere not seen during training, which prevents the model from learning to detect\nthe source of the target news article instead of predicting its political\nideology. From a modeling perspective, we propose an adversarial media\nadaptation, as well as a specially adapted triplet loss. We further add\nbackground information about the source, and we show that it is quite helpful\nfor improving article-level prediction. Our experimental results show very\nsizable improvements over using state-of-the-art pre-trained Transformers in\nthis challenging setup.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 20:27:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Baly", "Ramy", ""], ["Martino", "Giovanni Da San", ""], ["Glass", "James", ""], ["Nakov", "Preslav", ""]]}, {"id": "2010.05345", "submitter": "Xikun Zhang", "authors": "Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth", "title": "Do Language Embeddings Capture Scales?", "comments": "Accepted at EMNLP Findings 2020 and EMNLP BlackboxNLP workshop 2020;\n  8 pages, 2 figures; Minor changes to the acknowledgment section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained Language Models (LMs) have been shown to possess significant\nlinguistic, common sense, and factual knowledge. One form of knowledge that has\nnot been studied yet in this context is information about the scalar magnitudes\nof objects. We show that pretrained language models capture a significant\namount of this information but are short of the capability required for general\ncommon-sense reasoning. We identify contextual information in pre-training and\nnumeracy as two key factors affecting their performance and show that a simple\nmethod of canonicalizing numbers can have a significant effect on the results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:11:09 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 03:15:15 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 07:25:41 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zhang", "Xikun", ""], ["Ramachandran", "Deepak", ""], ["Tenney", "Ian", ""], ["Elazar", "Yanai", ""], ["Roth", "Dan", ""]]}, {"id": "2010.05349", "submitter": "Rahim Dehkharghani", "authors": "Ali Najafi, Araz Gholipour-Shilabin, Rahim Dehkharghani, Ali\n  Mohammadpur-Fard, Meysam Asgari-Chenaghlu", "title": "ComStreamClust: a communicative multi-agent approach to text clustering\n  in streaming data", "comments": "11 pages, 6 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic detection is the task of determining and tracking hot topics in social\nmedia. Twitter is arguably the most popular platform for people to share their\nideas with others about different issues. One such prevalent issue is the\nCOVID-19 pandemic. Detecting and tracking topics on these kinds of issues would\nhelp governments and healthcare companies deal with this phenomenon. In this\npaper, we propose a novel, multi-agent, communicative clustering approach,\nso-called ComStreamClust for clustering sub-topics inside a broader topic,\ne.g., COVID-19. The proposed approach is parallelizable, and can simultaneously\nhandle several data-point. The LaBSE sentence embedding is used to measure the\nsemantic similarity between two tweets. ComStreamClust has been evaluated on\ntwo datasets: the COVID-19 and the FA CUP. The results obtained from\nComStreamClust approve the effectiveness of the proposed approach when compared\nto existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:19:19 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 16:58:49 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Najafi", "Ali", ""], ["Gholipour-Shilabin", "Araz", ""], ["Dehkharghani", "Rahim", ""], ["Mohammadpur-Fard", "Ali", ""], ["Asgari-Chenaghlu", "Meysam", ""]]}, {"id": "2010.05357", "submitter": "Jiahua Chen", "authors": "Jiahua Chen and Shuai Wang and Sahisnu Mazumder and Bing Liu", "title": "A Knowledge-Driven Approach to Classifying Object and Attribute\n  Coreferences in Opinion Mining", "comments": "Accepted to Proceedings of EMNLP 2020 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying and resolving coreferences of objects (e.g., product names) and\nattributes (e.g., product aspects) in opinionated reviews is crucial for\nimproving the opinion mining performance. However, the task is challenging as\none often needs to consider domain-specific knowledge (e.g., iPad is a tablet\nand has aspect resolution) to identify coreferences in opinionated reviews.\nAlso, compiling a handcrafted and curated domain-specific knowledge base for\neach domain is very time consuming and arduous. This paper proposes an approach\nto automatically mine and leverage domain-specific knowledge for classifying\nobjects and attribute coreferences. The approach extracts domain-specific\nknowledge from unlabeled review data and trains a knowledgeaware neural\ncoreference classification model to leverage (useful) domain knowledge together\nwith general commonsense knowledge for the task. Experimental evaluation on\nrealworld datasets involving five domains (product types) shows the\neffectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 22:08:43 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 13:03:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Jiahua", ""], ["Wang", "Shuai", ""], ["Mazumder", "Sahisnu", ""], ["Liu", "Bing", ""]]}, {"id": "2010.05358", "submitter": "Alex Warstadt", "authors": "Alex Warstadt, Yian Zhang, Haau-Sing Li, Haokun Liu, Samuel R. Bowman", "title": "Learning Which Features Matter: RoBERTa Acquires a Preference for\n  Linguistic Generalizations (Eventually)", "comments": "accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One reason pretraining on self-supervised linguistic tasks is effective is\nthat it teaches models features that are helpful for language understanding.\nHowever, we want pretrained models to learn not only to represent linguistic\nfeatures, but also to use those features preferentially during fine-turning.\nWith this goal in mind, we introduce a new English-language diagnostic set\ncalled MSGS (the Mixed Signals Generalization Set), which consists of 20\nambiguous binary classification tasks that we use to test whether a pretrained\nmodel prefers linguistic or surface generalizations during fine-tuning. We\npretrain RoBERTa models from scratch on quantities of data ranging from 1M to\n1B words and compare their performance on MSGS to the publicly available\nRoBERTa-base. We find that models can learn to represent linguistic features\nwith little pretraining data, but require far more data to learn to prefer\nlinguistic generalizations over surface ones. Eventually, with about 30B words\nof pretraining data, RoBERTa-base does demonstrate a linguistic bias with some\nregularity. We conclude that while self-supervised pretraining is an effective\nway to learn helpful inductive biases, there is likely room to improve the rate\nat which models learn which features matter.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 22:09:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Warstadt", "Alex", ""], ["Zhang", "Yian", ""], ["Li", "Haau-Sing", ""], ["Liu", "Haokun", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2010.05369", "submitter": "Roy Bar-Haim", "authors": "Roy Bar-Haim, Yoav Kantor, Lilach Eden, Roni Friedman, Dan Lahav and\n  Noam Slonim", "title": "Quantitative Argument Summarization and Beyond: Cross-Domain Key Point\n  Analysis", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When summarizing a collection of views, arguments or opinions on some topic,\nit is often desirable not only to extract the most salient points, but also to\nquantify their prevalence. Work on multi-document summarization has\ntraditionally focused on creating textual summaries, which lack this\nquantitative aspect. Recent work has proposed to summarize arguments by mapping\nthem to a small set of expert-generated key points, where the salience of each\nkey point corresponds to the number of its matching arguments. The current work\nadvances key point analysis in two important respects: first, we develop a\nmethod for automatic extraction of key points, which enables fully automatic\nanalysis, and is shown to achieve performance comparable to a human expert.\nSecond, we demonstrate that the applicability of key point analysis goes well\nbeyond argumentation data. Using models trained on publicly available\nargumentation datasets, we achieve promising results in two additional domains:\nmunicipal surveys and user reviews. An additional contribution is an in-depth\nevaluation of argument-to-key point matching models, where we substantially\noutperform previous results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 23:01:51 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bar-Haim", "Roy", ""], ["Kantor", "Yoav", ""], ["Eden", "Lilach", ""], ["Friedman", "Roni", ""], ["Lahav", "Dan", ""], ["Slonim", "Noam", ""]]}, {"id": "2010.05379", "submitter": "Qinxin Wang", "authors": "Qinxin Wang, Hao Tan, Sheng Shen, Michael W. Mahoney, Zhewei Yao", "title": "MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase\n  Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phrase localization is a task that studies the mapping from textual phrases\nto regions of an image. Given difficulties in annotating phrase-to-object\ndatasets at scale, we develop a Multimodal Alignment Framework (MAF) to\nleverage more widely-available caption-image datasets, which can then be used\nas a form of weak supervision. We first present algorithms to model\nphrase-object relevance by leveraging fine-grained visual representations and\nvisually-aware language representations. By adopting a contrastive objective,\nour method uses information in caption-image pairs to boost the performance in\nweakly-supervised scenarios. Experiments conducted on the widely-adopted\nFlickr30k dataset show a significant improvement over existing\nweakly-supervised methods. With the help of the visually-aware language\nrepresentations, we can also improve the previous best unsupervised result by\n5.56%. We conduct ablation studies to show that both our novel model and our\nweakly-supervised strategies significantly contribute to our strong results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 00:43:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Qinxin", ""], ["Tan", "Hao", ""], ["Shen", "Sheng", ""], ["Mahoney", "Michael W.", ""], ["Yao", "Zhewei", ""]]}, {"id": "2010.05384", "submitter": "Yao-Chung Fan", "authors": "Ho-Lam Chung, Ying-Hong Chan, Yao-Chung Fan", "title": "A BERT-based Distractor Generation Scheme with Multi-tasking and\n  Negative Answer Training Strategies", "comments": "Accepted by EMNLP2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we investigate the following two limitations for the existing\ndistractor generation (DG) methods. First, the quality of the existing DG\nmethods are still far from practical use. There is still room for DG quality\nimprovement. Second, the existing DG designs are mainly for single distractor\ngeneration. However, for practical MCQ preparation, multiple distractors are\ndesired. Aiming at these goals, in this paper, we present a new distractor\ngeneration scheme with multi-tasking and negative answer training strategies\nfor effectively generating \\textit{multiple} distractors. The experimental\nresults show that (1) our model advances the state-of-the-art result from 28.65\nto 39.81 (BLEU 1 score) and (2) the generated multiple distractors are diverse\nand show strong distracting power for multiple choice question.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 01:22:29 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chung", "Ho-Lam", ""], ["Chan", "Ying-Hong", ""], ["Fan", "Yao-Chung", ""]]}, {"id": "2010.05406", "submitter": "Mingzhe Li", "authors": "Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao and\n  Rui Yan", "title": "VMSMO: Learning to Generate Multimodal Summary for Video-based News\n  Articles", "comments": "Accepted by The 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular multimedia news format nowadays is providing users with a lively\nvideo and a corresponding news article, which is employed by influential news\nmedia including CNN, BBC, and social media including Twitter and Weibo. In such\na case, automatically choosing a proper cover frame of the video and generating\nan appropriate textual summary of the article can help editors save time, and\nreaders make the decision more effectively. Hence, in this paper, we propose\nthe task of Video-based Multimodal Summarization with Multimodal Output (VMSMO)\nto tackle such a problem. The main challenge in this task is to jointly model\nthe temporal dependency of video with semantic meaning of article. To this end,\nwe propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of\na dual interaction module and multimodal generator. In the dual interaction\nmodule, we propose a conditional self-attention mechanism that captures local\nsemantic information within video and a global-attention mechanism that handles\nthe semantic relationship between news text and video from a high level.\nExtensive experiments conducted on a large-scale real-world VMSMO dataset show\nthat DIMS achieves the state-of-the-art performance in terms of both automatic\nmetrics and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 02:19:16 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Mingzhe", ""], ["Chen", "Xiuying", ""], ["Gao", "Shen", ""], ["Chan", "Zhangming", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2010.05419", "submitter": "Jens Tuyls", "authors": "Junlin Wang, Jens Tuyls, Eric Wallace, Sameer Singh", "title": "Gradient-based Analysis of NLP Models is Manipulable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based analysis methods, such as saliency map visualizations and\nadversarial input perturbations, have found widespread use in interpreting\nneural NLP models due to their simplicity, flexibility, and most importantly,\ntheir faithfulness. In this paper, however, we demonstrate that the gradients\nof a model are easily manipulable, and thus bring into question the reliability\nof gradient-based analyses. In particular, we merge the layers of a target\nmodel with a Facade that overwhelms the gradients without affecting the\npredictions. This Facade can be trained to have gradients that are misleading\nand irrelevant to the task, such as focusing only on the stop words in the\ninput. On a variety of NLP tasks (text classification, NLI, and QA), we show\nthat our method can manipulate numerous gradient-based analysis techniques:\nsaliency maps, input reduction, and adversarial perturbations all identify\nunimportant or targeted tokens as being highly important. The code and a\ntutorial of this paper is available at http://ucinlp.github.io/facade.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 02:54:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Junlin", ""], ["Tuyls", "Jens", ""], ["Wallace", "Eric", ""], ["Singh", "Sameer", ""]]}, {"id": "2010.05432", "submitter": "Md Mosharaf Hossain", "authors": "Md Mosharaf Hossain, Antonios Anastasopoulos, Eduardo Blanco, and\n  Alexis Palmer", "title": "It's not a Non-Issue: Negation as a Source of Error in Machine\n  Translation", "comments": "Accepted at the Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine translation (MT) systems progress at a rapid pace, questions of\ntheir adequacy linger. In this study we focus on negation, a universal, core\nproperty of human language that significantly affects the semantics of an\nutterance. We investigate whether translating negation is an issue for modern\nMT systems using 17 translation directions as test bed. Through thorough\nanalysis, we find that indeed the presence of negation can significantly impact\ndownstream quality, in some cases resulting in quality reductions of more than\n60%. We also provide a linguistically motivated analysis that directly explains\nthe majority of our findings. We release our annotations and code to replicate\nour analysis here: https://github.com/mosharafhossain/negation-mt.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 03:34:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hossain", "Md Mosharaf", ""], ["Anastasopoulos", "Antonios", ""], ["Blanco", "Eduardo", ""], ["Palmer", "Alexis", ""]]}, {"id": "2010.05444", "submitter": "Hai Hu", "authors": "Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kuebler, Lawrence S.\n  Moss", "title": "OCNLI: Original Chinese Natural Language Inference", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the tremendous recent progress on natural language inference (NLI),\ndriven largely by large-scale investment in new datasets (e.g., SNLI, MNLI) and\nadvances in modeling, most progress has been limited to English due to a lack\nof reliable datasets for most of the world's languages. In this paper, we\npresent the first large-scale NLI dataset (consisting of ~56,000 annotated\nsentence pairs) for Chinese called the Original Chinese Natural Language\nInference dataset (OCNLI). Unlike recent attempts at extending NLI to other\nlanguages, our dataset does not rely on any automatic translation or non-expert\nannotation. Instead, we elicit annotations from native speakers specializing in\nlinguistics. We follow closely the annotation protocol used for MNLI, but\ncreate new strategies for eliciting diverse hypotheses. We establish several\nbaseline results on our dataset using state-of-the-art pre-trained models for\nChinese, and find even the best performing models to be far outpaced by human\nperformance (~12% absolute performance gap), making it a challenging new\nresource that we hope will help to accelerate progress in Chinese NLU. To the\nbest of our knowledge, this is the first human-elicited MNLI-style corpus for a\nnon-English language.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 04:25:48 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hu", "Hai", ""], ["Richardson", "Kyle", ""], ["Xu", "Liang", ""], ["Li", "Lu", ""], ["Kuebler", "Sandra", ""], ["Moss", "Lawrence S.", ""]]}, {"id": "2010.05445", "submitter": "Fahimeh Saleh Miss", "authors": "Fahimeh Saleh, Wray Buntine, Gholamreza Haffari", "title": "Collective Wisdom: Improving Low-resource Neural Machine Translation\n  using Adaptive Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of parallel sentence-pairs poses a significant hurdle for training\nhigh-quality Neural Machine Translation (NMT) models in bilingually\nlow-resource scenarios. A standard approach is transfer learning, which\ninvolves taking a model trained on a high-resource language-pair and\nfine-tuning it on the data of the low-resource MT condition of interest.\nHowever, it is not clear generally which high-resource language-pair offers the\nbest transfer learning for the target MT setting. Furthermore, different\ntransferred models may have complementary semantic and/or syntactic strengths,\nhence using only one model may be sub-optimal. In this paper, we tackle this\nproblem using knowledge distillation, where we propose to distill the knowledge\nof ensemble of teacher models to a single student model. As the quality of\nthese teacher models varies, we propose an effective adaptive knowledge\ndistillation approach to dynamically adjust the contribution of the teacher\nmodels during the distillation process. Experiments on transferring from a\ncollection of six language pairs from IWSLT to five low-resource language-pairs\nfrom TED Talks demonstrate the effectiveness of our approach, achieving up to\n+0.9 BLEU score improvement compared to strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 04:26:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Saleh", "Fahimeh", ""], ["Buntine", "Wray", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2010.05465", "submitter": "Najoung Kim", "authors": "Najoung Kim and Tal Linzen", "title": "COGS: A Compositional Generalization Challenge Based on Semantic\n  Interpretation", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language is characterized by compositionality: the meaning of a\ncomplex expression is constructed from the meanings of its constituent parts.\nTo facilitate the evaluation of the compositional abilities of language\nprocessing architectures, we introduce COGS, a semantic parsing dataset based\non a fragment of English. The evaluation portion of COGS contains multiple\nsystematic gaps that can only be addressed by compositional generalization;\nthese include new combinations of familiar syntactic structures, or new\ncombinations of familiar words and familiar structures. In experiments with\nTransformers and LSTMs, we found that in-distribution accuracy on the COGS test\nset was near-perfect (96--99%), but generalization accuracy was substantially\nlower (16--35%) and showed high sensitivity to random seed ($\\pm$6--8%). These\nfindings indicate that contemporary standard NLP models are limited in their\ncompositional generalization capacity, and position COGS as a good way to\nmeasure progress.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:45:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kim", "Najoung", ""], ["Linzen", "Tal", ""]]}, {"id": "2010.05471", "submitter": "Qiansheng Wang", "authors": "Zhen Wang, Qiansheng Wang, Chengguo Lv, Xue Cao and Guohong Fu", "title": "Unseen Target Stance Detection with Adversarial Domain Generalization", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206635", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although stance detection has made great progress in the past few years, it\nis still facing the problem of unseen targets. In this study, we investigate\nthe domain difference between targets and thus incorporate attention-based\nconditional encoding with adversarial domain generalization to perform unseen\ntarget stance detection. Experimental results show that our approach achieves\nnew state-of-the-art performance on the SemEval-2016 dataset, demonstrating the\nimportance of domain difference between targets in unseen target stance\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 06:12:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Zhen", ""], ["Wang", "Qiansheng", ""], ["Lv", "Chengguo", ""], ["Cao", "Xue", ""], ["Fu", "Guohong", ""]]}, {"id": "2010.05478", "submitter": "Tanya Goyal", "authors": "Tanya Goyal, Greg Durrett", "title": "Evaluating Factuality in Generation with Dependency-level Entailment", "comments": "Findings of Emnlp 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in text generation models, a serious limitation\nis their tendency to produce text that is factually inconsistent with\ninformation in the input. Recent work has studied whether textual entailment\nsystems can be used to identify factual errors; however, these sentence-level\nentailment models are trained to solve a different problem than generation\nfiltering and they do not localize which part of a generation is non-factual.\nIn this paper, we propose a new formulation of entailment that decomposes it at\nthe level of dependency arcs. Rather than focusing on aggregate decisions, we\ninstead ask whether the semantic relationship manifested by individual\ndependency arcs in the generated output is supported by the input. Human\njudgments on this task are difficult to obtain; we therefore propose a method\nto automatically create data based on existing entailment or paraphrase\ncorpora. Experiments show that our dependency arc entailment model trained on\nthis data can identify factual inconsistencies in paraphrasing and\nsummarization better than sentence-level methods or those based on question\ngeneration, while additionally localizing the erroneous parts of the\ngeneration.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 06:43:10 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 06:35:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Goyal", "Tanya", ""], ["Durrett", "Greg", ""]]}, {"id": "2010.05496", "submitter": "HyeonJun Kim", "authors": "HyeonJun Kim", "title": "Feature Extraction of Text for Deep Learning Algorithms: Application on\n  Fake News Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Feature extraction is an important process of machine learning and deep\nlearning, as the process make algorithms function more efficiently, and also\naccurate. In natural language processing used in deception detection such as\nfake news detection, several ways of feature extraction in statistical aspect\nhad been introduced (e.g. N-gram). In this research, it will be shown that by\nusing deep learning algorithms and alphabet frequencies of the original text of\na news without any information about the sequence of the alphabet can actually\nbe used to classify fake news and trustworthy ones in high accuracy (85\\%). As\nthis pre-processing method makes the data notably compact but also include the\nfeature that is needed for the classifier, it seems that alphabet frequencies\ncontains some useful features for understanding complex context or meaning of\nthe original text.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:43:01 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 11:32:14 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Kim", "HyeonJun", ""]]}, {"id": "2010.05511", "submitter": "Jianhao Yan", "authors": "Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou", "title": "A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge\n  Graph", "comments": "Accepted as a regular paper in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a vivid, novel, and diverse essay with only several given topic\nwords is a challenging task of natural language generation. In previous work,\nthere are two problems left unsolved: neglect of sentiment beneath the text and\ninsufficient utilization of topic-related knowledge. Therefore, we propose a\nnovel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge\nGraph enhanced decoder, named SCTKG, which is based on the conditional\nvariational autoencoder (CVAE) framework. We firstly inject the sentiment\ninformation into the generator for controlling sentiment for each sentence,\nwhich leads to various generated essays. Then we design a Topic Knowledge Graph\nenhanced decoder. Unlike existing models that use knowledge entities\nseparately, our model treats the knowledge graph as a whole and encodes more\nstructured, connected semantic information in the graph to generate a more\nrelevant essay. Experimental results show that our SCTKG can generate sentiment\ncontrollable essays and outperform the state-of-the-art approach in terms of\ntopic relevance, fluency, and diversity on both automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:06:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Qiao", "Lin", ""], ["Yan", "Jianhao", ""], ["Meng", "Fandong", ""], ["Yang", "Zhendong", ""], ["Zhou", "Jie", ""]]}, {"id": "2010.05522", "submitter": "Guirong Bai", "authors": "Guirong Bai, Shizhu He, Kang Liu, Jun Zhao, Zaiqing Nie", "title": "Pre-trained Language Model Based Active Learning for Sentence Matching", "comments": "Accepted by the conference of coling 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is able to significantly reduce the annotation cost for\ndata-driven techniques. However, previous active learning approaches for\nnatural language processing mainly depend on the entropy-based uncertainty\ncriterion, and ignore the characteristics of natural language. In this paper,\nwe propose a pre-trained language model based active learning approach for\nsentence matching. Differing from previous active learning, it can provide\nlinguistic criteria to measure instances and help select more efficient\ninstances for annotation. Experiments demonstrate our approach can achieve\ngreater accuracy with fewer labeled training instances.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:24:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bai", "Guirong", ""], ["He", "Shizhu", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""], ["Nie", "Zaiqing", ""]]}, {"id": "2010.05523", "submitter": "Xiangru Tang", "authors": "Xiangru Tang, Alan Aw", "title": "FILM: A Fast, Interpretable, and Low-rank Metric Learning Approach for\n  Sentence Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of semantic similarity plays a vital role in sentence matching. It\nrequires to learn discriminative representations of natural language. Recently,\nowing to more and more sophisticated model architecture, impressive progress\nhas been made, along with a time-consuming training process and\nnot-interpretable inference. To alleviate this problem, we explore a metric\nlearning approach, named FILM (Fast, Interpretable, and Low-rank Metric\nlearning) to efficiently find a high discriminative projection of the\nhigh-dimensional data. We construct this metric learning problem as a manifold\noptimization problem and solve it with the Cayley transformation method with\nthe Barzilai-Borwein step size. In experiments, we apply FILM with triplet loss\nminimization objective to the Quora Challenge and Semantic Textual Similarity\n(STS) Task. The results demonstrate that the FILM method achieves superior\nperformance as well as the fastest computation speed, which is consistent with\nour theoretical analysis of time complexity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:24:41 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 01:14:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Tang", "Xiangru", ""], ["Aw", "Alan", ""]]}, {"id": "2010.05533", "submitter": "Cunliang Kong", "authors": "Cunliang Kong, Liner Yang, Tianzuo Zhang, Qinan Fan, Zhenghao Liu, Yun\n  Chen, Erhong Yang", "title": "Toward Cross-Lingual Definition Generation for Language Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating dictionary definitions automatically can prove useful for language\nlearners. However, it's still a challenging task of cross-lingual definition\ngeneration. In this work, we propose to generate definitions in English for\nwords in various languages. To achieve this, we present a simple yet effective\napproach based on publicly available pretrained language models. In this\napproach, models can be directly applied to other languages after trained on\nthe English dataset. We demonstrate the effectiveness of this approach on\nzero-shot definition generation. Experiments and manual analyses on newly\nconstructed datasets show that our models have a strong cross-lingual transfer\nability and can generate fluent English definitions for Chinese words. We\nfurther measure the lexical complexity of generated and reference definitions.\nThe results show that the generated definitions are much simpler, which is more\nsuitable for language learners.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:45:28 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kong", "Cunliang", ""], ["Yang", "Liner", ""], ["Zhang", "Tianzuo", ""], ["Fan", "Qinan", ""], ["Liu", "Zhenghao", ""], ["Chen", "Yun", ""], ["Yang", "Erhong", ""]]}, {"id": "2010.05542", "submitter": "Dawn Knight", "authors": "Dawn Knight, Steve Morris, Tess Fitzpatrick, Paul Rayson, Irena\n  Spasi\\'c, Enlli M\\^on Thomas", "title": "The National Corpus of Contemporary Welsh: Project Report | Y Corpws\n  Cenedlaethol Cymraeg Cyfoes: Adroddiad y Prosiect", "comments": "English-Welsh bilingual project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report provides an overview of the CorCenCC project and the online\ncorpus resource that was developed as a result of work on the project. The\nreport lays out the theoretical underpinnings of the research, demonstrating\nhow the project has built on and extended this theory. We also raise and\ndiscuss some of the key operational questions that arose during the course of\nthe project, outlining the ways in which they were answered, the impact of\nthese decisions on the resource that has been produced and the longer-term\ncontribution they will make to practices in corpus-building. Finally, we\ndiscuss some of the applications and the utility of the work, outlining the\nimpact that CorCenCC is set to have on a range of different individuals and\nuser groups.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:57:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Knight", "Dawn", ""], ["Morris", "Steve", ""], ["Fitzpatrick", "Tess", ""], ["Rayson", "Paul", ""], ["Spasi\u0107", "Irena", ""], ["Thomas", "Enlli M\u00f4n", ""]]}, {"id": "2010.05549", "submitter": "Yash Sharma", "authors": "Yash Sharma, Basil Abraham, Karan Taneja, Preethi Jyothi", "title": "Improving Low Resource Code-switched ASR using Augmented Code-switched\n  TTS", "comments": "Interspeech 2020, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building Automatic Speech Recognition (ASR) systems for code-switched speech\nhas recently gained renewed attention due to the widespread use of speech\ntechnologies in multilingual communities worldwide. End-to-end ASR systems are\na natural modeling choice due to their ease of use and superior performance in\nmonolingual settings. However, it is well known that end-to-end systems require\nlarge amounts of labeled speech. In this work, we investigate improving\ncode-switched ASR in low resource settings via data augmentation using\ncode-switched text-to-speech (TTS) synthesis. We propose two targeted\ntechniques to effectively leverage TTS speech samples: 1) Mixup, an existing\ntechnique to create new training samples via linear interpolation of existing\nsamples, applied to TTS and real speech samples, and 2) a new loss function,\nused in conjunction with TTS samples, to encourage code-switched predictions.\nWe report significant improvements in ASR performance achieving absolute word\nerror rate (WER) reductions of up to 5%, and measurable improvement in code\nswitching using our proposed techniques on a Hindi-English code-switched ASR\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:15:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sharma", "Yash", ""], ["Abraham", "Basil", ""], ["Taneja", "Karan", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2010.05567", "submitter": "Rahul Aralikatte", "authors": "Rahul Aralikatte, Mostafa Abdou, Heather Lent, Daniel Hershcovich,\n  Anders S{\\o}gaard", "title": "Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference resolution and semantic role labeling are NLP tasks that capture\ndifferent aspects of semantics, indicating respectively, which expressions\nrefer to the same entity, and what semantic roles expressions serve in the\nsentence. However, they are often closely interdependent, and both generally\nnecessitate natural language understanding. Do they form a coherent abstract\nrepresentation of documents? We present a neural network architecture for joint\ncoreference resolution and semantic role labeling for English, and train graph\nneural networks to model the 'coherence' of the combined shallow semantic\ngraph. Using the resulting coherence score as a reward for our joint semantic\nanalyzer, we use reinforcement learning to encourage global coherence over the\ndocument and between semantic annotations. This leads to improvements on both\ntasks in multiple datasets from different domains, and across a range of\nencoders of different expressivity, calling, we believe, for a more holistic\napproach to semantics in NLP.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:36:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Aralikatte", "Rahul", ""], ["Abdou", "Mostafa", ""], ["Lent", "Heather", ""], ["Hershcovich", "Daniel", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2010.05569", "submitter": "Suranjana Samanta", "authors": "Suranjana Samanta, Ajay Gupta, Prateeti Mohapatra, Amar Prakash Azad", "title": "Carbon to Diamond: An Incident Remediation Assistant System From Site\n  Reliability Engineers' Conversations in Hybrid Cloud Operations", "comments": "6 Pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational channels are changing the landscape of hybrid cloud service\nmanagement. These channels are becoming important avenues for Site Reliability\nEngineers (SREs) %Subject Matter Experts (SME) to collaboratively work together\nto resolve an incident or issue. Identifying segmented conversations and\nextracting key insights or artefacts from them can help engineers to improve\nthe efficiency of the incident remediation process by using information\nretrieval mechanisms for similar incidents. However, it has been empirically\nobserved that due to the semi-formal behavior of such conversations (human\nlanguage) they are very unique in nature and also contain lot of\ndomain-specific terms. This makes it difficult to use the standard natural\nlanguage processing frameworks directly, which are popularly used in standard\nNLP tasks. %It is important to identify the correct keywords and artefacts like\nsymptoms, issue etc., present in the conversation chats. In this paper, we\nbuild a framework that taps into the conversational channels and uses various\nlearning methods to (a) understand and extract key artefacts from conversations\nlike diagnostic steps and resolution actions taken, and (b) present an approach\nto identify past conversations about similar issues. Experimental results on\nour dataset show the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:43:35 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Samanta", "Suranjana", ""], ["Gupta", "Ajay", ""], ["Mohapatra", "Prateeti", ""], ["Azad", "Amar Prakash", ""]]}, {"id": "2010.05572", "submitter": "Suranjana Samanta", "authors": "Debanjana Kar, Suranjana Samanta, Amar Prakash Azad", "title": "Meta-Context Transformers for Domain-Specific Response Generation", "comments": "7+2 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the tremendous success of neural dialogue models in recent years, it\nsuffers a lack of relevance, diversity, and some times coherence in generated\nresponses. Lately, transformer-based models, such as GPT-2, have revolutionized\nthe landscape of dialogue generation by capturing the long-range structures\nthrough language modeling. Though these models have exhibited excellent\nlanguage coherence, they often lack relevance and terms when used for\ndomain-specific response generation. In this paper, we present DSRNet (Domain\nSpecific Response Network), a transformer-based model for dialogue response\ngeneration by reinforcing domain-specific attributes. In particular, we extract\nmeta attributes from context and infuse them with the context utterances for\nbetter attention over domain-specific key terms and relevance. We study the use\nof DSRNet in a multi-turn multi-interlocutor environment for domain-specific\nresponse generation. In our experiments, we evaluate DSRNet on Ubuntu dialogue\ndatasets, which are mainly composed of various technical domain related\ndialogues for IT domain issue resolutions and also on CamRest676 dataset, which\ncontains restaurant domain conversations. Trained with maximum likelihood\nobjective, our model shows significant improvement over the state-of-the-art\nfor multi-turn dialogue systems supported by better BLEU and semantic\nsimilarity (BertScore) scores. Besides, we also observe that the responses\nproduced by our model carry higher relevance due to the presence of\ndomain-specific key attributes that exhibit better overlap with the attributes\nof the context. Our analysis shows that the performance improvement is mostly\ndue to the infusion of key terms along with dialogues which result in better\nattention over domain-relevant terms. Other contributing factors include joint\nmodeling of dialogue context with the domain-specific meta attributes and\ntopics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:49:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kar", "Debanjana", ""], ["Samanta", "Suranjana", ""], ["Azad", "Amar Prakash", ""]]}, {"id": "2010.05581", "submitter": "Sicheng Yu", "authors": "Sicheng Yu, Yulei Niu, Shuohang Wang, Jing Jiang, Qianru Sun", "title": "Counterfactual Variable Control for Robust and Interpretable Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based question answering (QA) models are neither robust\nnor explainable in many cases. For example, a multiple-choice QA model, tested\nwithout any input of question, is surprisingly \"capable\" to predict the most of\ncorrect options. In this paper, we inspect such spurious \"capability\" of QA\nmodels using causal inference. We find the crux is the shortcut correlation,\ne.g., unrobust word alignment between passage and options learned by the\nmodels. We propose a novel approach called Counterfactual Variable Control\n(CVC) that explicitly mitigates any shortcut correlation and preserves the\ncomprehensive reasoning for robust QA. Specifically, we leverage multi-branch\narchitecture that allows us to disentangle robust and shortcut correlations in\nthe training process of QA. We then conduct two novel CVC inference methods (on\ntrained models) to capture the effect of comprehensive reasoning as the final\nprediction. For evaluation, we conduct extensive experiments using two BERT\nbackbones on both multi-choice and span-extraction QA benchmarks. The results\nshow that our CVC achieves high robustness against a variety of adversarial\nattacks in QA while maintaining good interpretation ability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:09:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yu", "Sicheng", ""], ["Niu", "Yulei", ""], ["Wang", "Shuohang", ""], ["Jiang", "Jing", ""], ["Sun", "Qianru", ""]]}, {"id": "2010.05587", "submitter": "Debjit Paul", "authors": "Debjit Paul and Anette Frank", "title": "Social Commonsense Reasoning with Multi-Head Knowledge Attention", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Commonsense Reasoning requires understanding of text, knowledge about\nsocial events and their pragmatic implications, as well as commonsense\nreasoning skills. In this work we propose a novel multi-head knowledge\nattention model that encodes semi-structured commonsense inference rules and\nlearns to incorporate them in a transformer-based reasoning cell. We assess the\nmodel's performance on two tasks that require different reasoning skills:\nAbductive Natural Language Inference and Counterfactual Invariance Prediction\nas a new task. We show that our proposed model improves performance over strong\nstate-of-the-art models (i.e., RoBERTa) across both reasoning tasks. Notably we\nare, to the best of our knowledge, the first to demonstrate that a model that\nlearns to perform counterfactual reasoning helps predicting the best\nexplanation in an abductive reasoning task. We validate the robustness of the\nmodel's reasoning capabilities by perturbing the knowledge and provide\nqualitative analysis on the model's knowledge incorporation capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:24:40 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Paul", "Debjit", ""], ["Frank", "Anette", ""]]}, {"id": "2010.05594", "submitter": "Wei Peng", "authors": "Ting Han, Ximing Liu, Ryuichi Takanobu, Yixin Lian, Chongxuan Huang,\n  Dazhen Wan, Wei Peng, Minlie Huang", "title": "MultiWOZ 2.3: A multi-domain task-oriented dialogue dataset enhanced\n  with annotation corrections and co-reference annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialogue systems have made unprecedented progress with multiple\nstate-of-the-art (SOTA) models underpinned by a number of publicly available\nMultiWOZ datasets. Dialogue state annotations are error-prone, leading to\nsub-optimal performance. Various efforts have been put in rectifying the\nannotation errors presented in the original MultiWOZ dataset. In this paper, we\nintroduce MultiWOZ 2.3, in which we differentiate incorrect annotations in\ndialogue acts from dialogue states, identifying a lack of co-reference when\npublishing the updated dataset. To ensure consistency between dialogue acts and\ndialogue states, we implement co-reference features and unify annotations of\ndialogue acts and dialogue states. We update the state of the art performance\nof natural language understanding and dialogue state tracking on MultiWOZ 2.3,\nwhere the results show significant improvements than on previous versions of\nMultiWOZ datasets (2.0-2.2).\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:53:19 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 06:42:47 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 11:25:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Han", "Ting", ""], ["Liu", "Ximing", ""], ["Takanobu", "Ryuichi", ""], ["Lian", "Yixin", ""], ["Huang", "Chongxuan", ""], ["Wan", "Dazhen", ""], ["Peng", "Wei", ""], ["Huang", "Minlie", ""]]}, {"id": "2010.05607", "submitter": "Jasmijn Bastings", "authors": "Jasmijn Bastings and Katja Filippova", "title": "The elephant in the interpretability room: Why use attention as\n  explanation when we have saliency methods?", "comments": "Accepted at BlackboxNLP 2020", "journal-ref": "Proceedings of the 2020 EMNLP Workshop BlackboxNLP: Analyzing and\n  Interpreting Neural Networks for NLP", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent surge of interest in using attention as explanation of\nmodel predictions, with mixed evidence on whether attention can be used as\nsuch. While attention conveniently gives us one weight per input token and is\neasily extracted, it is often unclear toward what goal it is used as\nexplanation. We find that often that goal, whether explicitly stated or not, is\nto find out what input tokens are the most relevant to a prediction, and that\nthe implied user for the explanation is a model developer. For this goal and\nuser, we argue that input saliency methods are better suited, and that there\nare no compelling reasons to use attention, despite the coincidence that it\nprovides a weight for each input. With this position paper, we hope to shift\nsome of the recent focus on attention to saliency methods, and for authors to\nclearly state the goal and user for their explanations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:27:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bastings", "Jasmijn", ""], ["Filippova", "Katja", ""]]}, {"id": "2010.05609", "submitter": "Amine Abdaoui", "authors": "Amine Abdaoui, Camille Pradel and Gr\\'egoire Sigel", "title": "Load What You Need: Smaller Versions of Multilingual BERT", "comments": null, "journal-ref": "SustaiNLP / EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained Transformer-based models are achieving state-of-the-art results\non a variety of Natural Language Processing data sets. However, the size of\nthese models is often a drawback for their deployment in real production\napplications. In the case of multilingual models, most of the parameters are\nlocated in the embeddings layer. Therefore, reducing the vocabulary size should\nhave an important impact on the total number of parameters. In this paper, we\npropose to generate smaller models that handle fewer number of languages\naccording to the targeted corpora. We present an evaluation of smaller versions\nof multilingual BERT on the XNLI data set, but we believe that this method may\nbe applied to other multilingual transformers. The obtained results confirm\nthat we can generate smaller models that keep comparable results, while\nreducing up to 45% of the total number of parameters. We compared our models\nwith DistilmBERT (a distilled version of multilingual BERT) and showed that\nunlike language reduction, distillation induced a 1.7% to 6% drop in the\noverall accuracy on the XNLI data set. The presented models and code are\npublicly available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:29:06 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Abdaoui", "Amine", ""], ["Pradel", "Camille", ""], ["Sigel", "Gr\u00e9goire", ""]]}, {"id": "2010.05633", "submitter": "Omnia Zayed", "authors": "Omnia Zayed, John P. McCrae, Paul Buitelaar", "title": "Contextual Modulation for Relation-Level Metaphor Identification", "comments": "accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying metaphors in text is very challenging and requires comprehending\nthe underlying comparison. The automation of this cognitive process has gained\nwide attention lately. However, the majority of existing approaches concentrate\non word-level identification by treating the task as either single-word\nclassification or sequential labelling without explicitly modelling the\ninteraction between the metaphor components. On the other hand, while existing\nrelation-level approaches implicitly model this interaction, they ignore the\ncontext where the metaphor occurs. In this work, we address these limitations\nby introducing a novel architecture for identifying relation-level metaphoric\nexpressions of certain grammatical relations based on contextual modulation. In\na methodology inspired by works in visual reasoning, our approach is based on\nconditioning the neural network computation on the deep contextualised features\nof the candidate expressions using feature-wise linear modulation. We\ndemonstrate that the proposed architecture achieves state-of-the-art results on\nbenchmark datasets. The proposed methodology is generic and could be applied to\nother textual classification problems that benefit from contextual interaction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:07:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zayed", "Omnia", ""], ["McCrae", "John P.", ""], ["Buitelaar", "Paul", ""]]}, {"id": "2010.05639", "submitter": "Qiao Jin", "authors": "Qiao Jin, Chuanqi Tan, Mosha Chen, Xiaozhong Liu, Songfang Huang", "title": "Predicting Clinical Trial Results by Implicit Evidence Integration", "comments": "EMNLP 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials provide essential guidance for practicing Evidence-Based\nMedicine, though often accompanying with unendurable costs and risks. To\noptimize the design of clinical trials, we introduce a novel Clinical Trial\nResult Prediction (CTRP) task. In the CTRP framework, a model takes a\nPICO-formatted clinical trial proposal with its background as input and\npredicts the result, i.e. how the Intervention group compares with the\nComparison group in terms of the measured Outcome in the studied Population.\nWhile structured clinical evidence is prohibitively expensive for manual\ncollection, we exploit large-scale unstructured sentences from medical\nliterature that implicitly contain PICOs and results as evidence. Specifically,\nwe pre-train a model to predict the disentangled results from such implicit\nevidence and fine-tune the model with limited data on the downstream datasets.\nExperiments on the benchmark Evidence Integration dataset show that the\nproposed model outperforms the baselines by large margins, e.g., with a 10.7%\nrelative gain over BioBERT in macro-F1. Moreover, the performance improvement\nis also validated on another dataset composed of clinical trials related to\nCOVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:25:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Jin", "Qiao", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Liu", "Xiaozhong", ""], ["Huang", "Songfang", ""]]}, {"id": "2010.05647", "submitter": "Inbar Oren", "authors": "Inbar Oren, Jonathan Herzig, Nitish Gupta, Matt Gardner, Jonathan\n  Berant", "title": "Improving Compositional Generalization in Semantic Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization of models to out-of-distribution (OOD) data has captured\ntremendous attention recently. Specifically, compositional generalization,\ni.e., whether a model generalizes to new structures built of components\nobserved during training, has sparked substantial interest. In this work, we\ninvestigate compositional generalization in semantic parsing, a natural\ntest-bed for compositional generalization, as output programs are constructed\nfrom sub-components. We analyze a wide variety of models and propose multiple\nextensions to the attention module of the semantic parser, aiming to improve\ncompositional generalization. We find that the following factors improve\ncompositional generalization: (a) using contextual representations, such as\nELMo and BERT, (b) informing the decoder what input tokens have previously been\nattended to, (c) training the decoder attention to agree with pre-computed\ntoken alignments, and (d) downsampling examples corresponding to frequent\nprogram templates. While we substantially reduce the gap between\nin-distribution and OOD generalization, performance on OOD compositions is\nstill substantially lower.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:34:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Oren", "Inbar", ""], ["Herzig", "Jonathan", ""], ["Gupta", "Nitish", ""], ["Gardner", "Matt", ""], ["Berant", "Jonathan", ""]]}, {"id": "2010.05648", "submitter": "Steffen Eger", "authors": "Steffen Eger and Yannik Benz", "title": "From Hero to Z\\'eroe: A Benchmark of Low-Level Adversarial Attacks", "comments": "Authors accidentally in wrong order; cannot be undone due to\n  conference constraints. Accepted for publication at AACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks are label-preserving modifications to inputs of machine\nlearning classifiers designed to fool machines but not humans. Natural Language\nProcessing (NLP) has mostly focused on high-level attack scenarios such as\nparaphrasing input texts. We argue that these are less realistic in typical\napplication scenarios such as in social media, and instead focus on low-level\nattacks on the character-level. Guided by human cognitive abilities and human\nrobustness, we propose the first large-scale catalogue and benchmark of\nlow-level adversarial attacks, which we dub Z\\'eroe, encompassing nine\ndifferent attack modes including visual and phonetic adversaries. We show that\nRoBERTa, NLP's current workhorse, fails on our attacks. Our dataset provides a\nbenchmark for testing robustness of future more human-like NLP models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:35:36 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 12:53:05 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Eger", "Steffen", ""], ["Benz", "Yannik", ""]]}, {"id": "2010.05670", "submitter": "Francois Meyer", "authors": "Francois Meyer and Martha Lewis", "title": "Modelling Lexical Ambiguity with Density Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words can have multiple senses. Compositional distributional models of\nmeaning have been argued to deal well with finer shades of meaning variation\nknown as polysemy, but are not so well equipped to handle word senses that are\netymologically unrelated, or homonymy. Moving from vectors to density matrices\nallows us to encode a probability distribution over different senses of a word,\nand can also be accommodated within a compositional distributional model of\nmeaning. In this paper we present three new neural models for learning density\nmatrices from a corpus, and test their ability to discriminate between word\nsenses on a range of compositional datasets. When paired with a particular\ncomposition method, our best model outperforms existing vector-based\ncompositional models as well as strong sentence encoders.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:08:45 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Meyer", "Francois", ""], ["Lewis", "Martha", ""]]}, {"id": "2010.05700", "submitter": "Kalpesh Krishna", "authors": "Kalpesh Krishna, John Wieting, Mohit Iyyer", "title": "Reformulating Unsupervised Style Transfer as Paraphrase Generation", "comments": "EMNLP 2020 camera-ready (26 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern NLP defines the task of style transfer as modifying the style of a\ngiven sentence without appreciably changing its semantics, which implies that\nthe outputs of style transfer systems should be paraphrases of their inputs.\nHowever, many existing systems purportedly designed for style transfer\ninherently warp the input's meaning through attribute transfer, which changes\nsemantic properties such as sentiment. In this paper, we reformulate\nunsupervised style transfer as a paraphrase generation problem, and present a\nsimple methodology based on fine-tuning pretrained language models on\nautomatically generated paraphrase data. Despite its simplicity, our method\nsignificantly outperforms state-of-the-art style transfer systems on both human\nand automatic evaluations. We also survey 23 style transfer papers and discover\nthat existing automatic metrics can be easily gamed and propose fixed variants.\nFinally, we pivot to a more real-world style transfer setting by collecting a\nlarge dataset of 15M sentences in 11 diverse styles, which we use for an\nin-depth analysis of our system.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:31:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Krishna", "Kalpesh", ""], ["Wieting", "John", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2010.05710", "submitter": "Ofir Arviv", "authors": "Ofir Arviv, Ruixiang Cui, Daniel Hershcovich", "title": "HUJI-KU at MRP~2020: Two Transition-based Neural Parsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the HUJI-KU system submission to the shared task on\nCross-Framework Meaning Representation Parsing (MRP) at the 2020 Conference for\nComputational Language Learning (CoNLL), employing TUPA and the HIT-SCIR\nparser, which were, respectively, the baseline system and winning system in the\n2019 MRP shared task. Both are transition-based parsers using BERT\ncontextualized embeddings. We generalized TUPA to support the newly-added MRP\nframeworks and languages, and experimented with multitask learning with the\nHIT-SCIR parser. We reached 4th place in both the cross-framework and\ncross-lingual tracks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 13:41:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Arviv", "Ofir", ""], ["Cui", "Ruixiang", ""], ["Hershcovich", "Daniel", ""]]}, {"id": "2010.05725", "submitter": "Ethan Wilcox", "authors": "Ethan Wilcox, Peng Qian, Richard Futrell, Ryosuke Kohita, Roger Levy\n  and Miguel Ballesteros", "title": "Structural Supervision Improves Few-Shot Learning and Syntactic\n  Generalization in Neural Language Models", "comments": "To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans can learn structural properties about a word from minimal experience,\nand deploy their learned syntactic representations uniformly in different\ngrammatical contexts. We assess the ability of modern neural language models to\nreproduce this behavior in English and evaluate the effect of structural\nsupervision on learning outcomes. First, we assess few-shot learning\ncapabilities by developing controlled experiments that probe models' syntactic\nnominal number and verbal argument structure generalizations for tokens seen as\nfew as two times during training. Second, we assess invariance properties of\nlearned representation: the ability of a model to transfer syntactic\ngeneralizations from a base context (e.g., a simple declarative active-voice\nsentence) to a transformed context (e.g., an interrogative sentence). We test\nfour models trained on the same dataset: an n-gram baseline, an LSTM, and two\nLSTM-variants trained with explicit structural supervision (Dyer et al.,2016;\nCharniak et al., 2016). We find that in most cases, the neural models are able\nto induce the proper syntactic generalizations after minimal exposure, often\nfrom just two examples during training, and that the two structurally\nsupervised models generalize more accurately than the LSTM model. All neural\nmodels are able to leverage information learned in base contexts to drive\nexpectations in transformed contexts, indicating that they have learned some\ninvariance properties of syntax.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:12:37 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wilcox", "Ethan", ""], ["Qian", "Peng", ""], ["Futrell", "Richard", ""], ["Kohita", "Ryosuke", ""], ["Levy", "Roger", ""], ["Ballesteros", "Miguel", ""]]}, {"id": "2010.05731", "submitter": "Ivan Vuli\\'c", "authors": "Ivan Vuli\\'c, Edoardo Maria Ponti, Robert Litschko, Goran Glava\\v{s},\n  Anna Korhonen", "title": "Probing Pretrained Language Models for Lexical Semantics", "comments": "EMNLP 2020: Long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of large pretrained language models (LMs) such as BERT and\nRoBERTa has sparked interest in probing their representations, in order to\nunveil what types of knowledge they implicitly capture. While prior research\nfocused on morphosyntactic, semantic, and world knowledge, it remains unclear\nto which extent LMs also derive lexical type-level knowledge from words in\ncontext. In this work, we present a systematic empirical analysis across six\ntypologically diverse languages and five different lexical tasks, addressing\nthe following questions: 1) How do different lexical knowledge extraction\nstrategies (monolingual versus multilingual source LM, out-of-context versus\nin-context encoding, inclusion of special tokens, and layer-wise averaging)\nimpact performance? How consistent are the observed effects across tasks and\nlanguages? 2) Is lexical knowledge stored in few parameters, or is it scattered\nthroughout the network? 3) How do these representations fare against\ntraditional static word vectors in lexical tasks? 4) Does the lexical\ninformation emerging from independently trained monolingual LMs display latent\nsimilarities? Our main results indicate patterns and best practices that hold\nuniversally, but also point to prominent variations across languages and tasks.\nMoreover, we validate the claim that lower Transformer layers carry more\ntype-level lexical knowledge, but also show that this knowledge is distributed\nacross multiple layers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:24:01 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Vuli\u0107", "Ivan", ""], ["Ponti", "Edoardo Maria", ""], ["Litschko", "Robert", ""], ["Glava\u0161", "Goran", ""], ["Korhonen", "Anna", ""]]}, {"id": "2010.05732", "submitter": "Francis Ferraro", "authors": "Rajat Patel and Francis Ferraro", "title": "On the Complementary Nature of Knowledge Graph Embedding, Fine Grain\n  Entity Types, and Language Modeling", "comments": "To appear at the EMNLP 2020 Workshop on Deep Learning Inside Out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the complementary natures of neural knowledge graph embedding,\nfine-grain entity type prediction, and neural language modeling. We show that a\nlanguage model-inspired knowledge graph embedding approach yields both improved\nknowledge graph embeddings and fine-grain entity type representations. Our work\nalso shows that jointly modeling both structured knowledge tuples and language\nimproves both.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:26:48 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Patel", "Rajat", ""], ["Ferraro", "Francis", ""]]}, {"id": "2010.05736", "submitter": "Marco Di Giovanni", "authors": "Marco Di Giovanni and Marco Brambilla", "title": "EFSG: Evolutionary Fooling Sentences Generator", "comments": "13 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language representation models (LMs) have recently\ncollected a huge number of successes in many NLP tasks.\n  In 2018 BERT, and later its successors (e.g. RoBERTa), obtained\nstate-of-the-art results in classical benchmark tasks, such as GLUE benchmark.\n  After that, works about adversarial attacks have been published to test their\ngeneralization proprieties and robustness.\n  In this work, we design Evolutionary Fooling Sentences Generator (EFSG), a\nmodel- and task-agnostic adversarial attack algorithm built using an\nevolutionary approach to generate false-positive sentences for binary\nclassification tasks.\n  We successfully apply EFSG to CoLA and MRPC tasks, on BERT and RoBERTa,\ncomparing performances. Results prove the presence of weak spots in\nstate-of-the-art LMs.\n  We finally test adversarial training as a data augmentation defence approach\nagainst EFSG, obtaining stronger improved models with no loss of accuracy when\ntested on the original datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:28:48 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Di Giovanni", "Marco", ""], ["Brambilla", "Marco", ""]]}, {"id": "2010.05738", "submitter": "Sopan Khosla", "authors": "Sopan Khosla, Carolyn Rose", "title": "Using Type Information to Improve Entity Coreference Resolution", "comments": "Accepted as Long Paper at CODI workshop EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coreference resolution (CR) is an essential part of discourse analysis. Most\nrecently, neural approaches have been proposed to improve over SOTA models from\nearlier paradigms. So far none of the published neural models leverage external\nsemantic knowledge such as type information. This paper offers the first such\nmodel and evaluation, demonstrating modest gains in accuracy by introducing\neither gold standard or predicted types. In the proposed approach, type\ninformation serves both to (1) improve mention representation and (2) create a\nsoft type consistency check between coreference candidate mentions. Our\nevaluation covers two different grain sizes of types over four different\nbenchmark corpora.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:32:39 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Khosla", "Sopan", ""], ["Rose", "Carolyn", ""]]}, {"id": "2010.05740", "submitter": "Yanjie Gou", "authors": "Yanjie Gou, Yinjie Lei, Lingqiao Liu", "title": "Contextualize Knowledge Bases with Transformer for End-to-end\n  Task-Oriented Dialogue Systems", "comments": "Third version of this work; Correct some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies try to build task-oriented dialogue systems in an end-to-end\nmanner and the existing works make great progress on this task. However, there\nis still an issue need to be further considered, i.e., how to effectively\nrepresent the knowledge bases and incorporate that into dialogue systems. To\nsolve this issue, we design a novel Transformer-based Context-aware Memory\nGenerator to model the entities in knowledge bases, which can produce entity\nrepresentations with perceiving all the relevant entities and dialogue history.\nFurthermore, we propose Context-aware Memory Enhanced Transformer (CMET), which\ncan effectively aggregate information from the dialogue history and knowledge\nbases to generate more accurate responses. Through extensive experiments, our\nmethod can achieve superior performance over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:34:07 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:37:22 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:37:40 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Gou", "Yanjie", ""], ["Lei", "Yinjie", ""], ["Liu", "Lingqiao", ""]]}, {"id": "2010.05757", "submitter": "Carsten Eickhoff", "authors": "Aaron S. Eisman, Nishant R. Shah, Carsten Eickhoff, George Zerveas,\n  Elizabeth S. Chen, Wen-Chih Wu, Indra Neil Sarkar", "title": "Extracting Angina Symptoms from Clinical Notes Using Pre-Trained\n  Transformer Architectures", "comments": null, "journal-ref": "AMIA Annual Symposium 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anginal symptoms can connote increased cardiac risk and a need for change in\ncardiovascular management. This study evaluated the potential to extract these\nsymptoms from physician notes using the Bidirectional Encoder from Transformers\nlanguage model fine-tuned on a domain-specific corpus. The history of present\nillness section of 459 expert annotated primary care physician notes from\nconsecutive patients referred for cardiac testing without known atherosclerotic\ncardiovascular disease were included. Notes were annotated for positive and\nnegative mentions of chest pain and shortness of breath characterization. The\nresults demonstrate high sensitivity and specificity for the detection of chest\npain or discomfort, substernal chest pain, shortness of breath, and dyspnea on\nexertion. Small sample size limited extracting factors related to provocation\nand palliation of chest pain. This study provides a promising starting point\nfor the natural language processing of physician notes to characterize\nclinically actionable anginal symptoms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:53:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Eisman", "Aaron S.", ""], ["Shah", "Nishant R.", ""], ["Eickhoff", "Carsten", ""], ["Zerveas", "George", ""], ["Chen", "Elizabeth S.", ""], ["Wu", "Wen-Chih", ""], ["Sarkar", "Indra Neil", ""]]}, {"id": "2010.05763", "submitter": "Nikolaos Manginas", "authors": "Nikolaos Manginas, Ilias Chalkidis and Prodromos Malakasiotis", "title": "Layer-wise Guided Training for BERT: Learning Incrementally Refined\n  Document Representations", "comments": "5 pages, short paper at SPNLP 2020 (EMNLP 2020 Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although BERT is widely used by the NLP community, little is known about its\ninner workings. Several attempts have been made to shed light on certain\naspects of BERT, often with contradicting conclusions. A much raised concern\nfocuses on BERT's over-parameterization and under-utilization issues. To this\nend, we propose o novel approach to fine-tune BERT in a structured manner.\nSpecifically, we focus on Large Scale Multilabel Text Classification (LMTC)\nwhere documents are assigned with one or more labels from a large predefined\nset of hierarchically organized labels. Our approach guides specific BERT\nlayers to predict labels from specific hierarchy levels. Experimenting with two\nLMTC datasets we show that this structured fine-tuning approach not only yields\nbetter classification results but also leads to better parameter utilization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:56:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Manginas", "Nikolaos", ""], ["Chalkidis", "Ilias", ""], ["Malakasiotis", "Prodromos", ""]]}, {"id": "2010.05774", "submitter": "Sagar Samtani", "authors": "Sagar Samtani, Hongyi Zhu, Balaji Padmanabhan, Yidong Chai, Hsinchun\n  Chen", "title": "Deep Learning for Information Systems Research", "comments": "56 pages total, 1 page title and authors, 42 pages main text, 13\n  pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has rapidly emerged as a key disruptive\ntechnology in the 21st century. At the heart of modern AI lies Deep Learning\n(DL), an emerging class of algorithms that has enabled today's platforms and\norganizations to operate at unprecedented efficiency, effectiveness, and scale.\nDespite significant interest, IS contributions in DL have been limited, which\nwe argue is in part due to issues with defining, positioning, and conducting DL\nresearch. Recognizing the tremendous opportunity here for the IS community,\nthis work clarifies, streamlines, and presents approaches for IS scholars to\nmake timely and high-impact contributions. Related to this broader goal, this\npaper makes five timely contributions. First, we systematically summarize the\nmajor components of DL in a novel Deep Learning for Information Systems\nResearch (DL-ISR) schematic that illustrates how technical DL processes are\ndriven by key factors from an application environment. Second, we present a\nnovel Knowledge Contribution Framework (KCF) to help IS scholars position their\nDL contributions for maximum impact. Third, we provide ten guidelines to help\nIS scholars generate rigorous and relevant DL-ISR in a systematic, high-quality\nfashion. Fourth, we present a review of prevailing journal and conference\nvenues to examine how IS scholars have leveraged DL for various research\ninquiries. Finally, we provide a unique perspective on how IS scholars can\nformulate DL-ISR inquiries by carefully considering the interplay of business\nfunction(s), application areas(s), and the KCF. This perspective intentionally\nemphasizes inter-disciplinary, intra-disciplinary, and cross-IS tradition\nperspectives. Taken together, these contributions provide IS scholars a timely\nframework to advance the scale, scope, and impact of deep learning research.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:23:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Samtani", "Sagar", ""], ["Zhu", "Hongyi", ""], ["Padmanabhan", "Balaji", ""], ["Chai", "Yidong", ""], ["Chen", "Hsinchun", ""]]}, {"id": "2010.05848", "submitter": "Judy Hanwen Shen", "authors": "Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson,\n  Agata Lapedriza, Noah Jones, Shixiang Shane Gu, and Rosalind Picard", "title": "Human-centric Dialog Training via Offline Reinforcement Learning", "comments": "To appear in EMNLP 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we train a dialog model to produce better conversations by learning\nfrom human feedback, without the risk of humans teaching it harmful chat\nbehaviors? We start by hosting models online, and gather human feedback from\nreal-time, open-ended conversations, which we then use to train and improve the\nmodels using offline reinforcement learning (RL). We identify implicit\nconversational cues including language similarity, elicitation of laughter,\nsentiment, and more, which indicate positive human feedback, and embed these in\nmultiple reward functions. A well-known challenge is that learning an RL policy\nin an offline setting usually fails due to the lack of ability to explore and\nthe tendency to make over-optimistic estimates of future reward. These problems\nbecome even harder when using RL for language models, which can easily have a\n20,000 action vocabulary and many possible reward functions. We solve the\nchallenge by developing a novel class of offline RL algorithms. These\nalgorithms use KL-control to penalize divergence from a pre-trained prior\nlanguage model, and use a new strategy to make the algorithm pessimistic,\ninstead of optimistic, in the face of uncertainty. We test the resulting dialog\nmodel with ratings from 80 users in an open-domain setting and find it achieves\nsignificant improvements over existing deep offline RL approaches. The novel\noffline RL method is viable for improving any existing generative dialog model\nusing a static dataset of human feedback.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:53:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Jaques", "Natasha", ""], ["Shen", "Judy Hanwen", ""], ["Ghandeharioun", "Asma", ""], ["Ferguson", "Craig", ""], ["Lapedriza", "Agata", ""], ["Jones", "Noah", ""], ["Gu", "Shixiang Shane", ""], ["Picard", "Rosalind", ""]]}, {"id": "2010.05856", "submitter": "Mingda Chen", "authors": "Mingda Chen, Sam Wiseman, Kevin Gimpel", "title": "Controllable Paraphrasing and Translation with a Syntactic Exemplar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior work on exemplar-based syntactically controlled paraphrase\ngeneration relies on automatically-constructed large-scale paraphrase datasets.\nWe sidestep this prerequisite by adapting models from prior work to be able to\nlearn solely from bilingual text (bitext). Despite only using bitext for\ntraining, and in near zero-shot conditions, our single proposed model can\nperform four tasks: controlled paraphrase generation in both languages and\ncontrolled machine translation in both language directions. To evaluate these\ntasks quantitatively, we create three novel evaluation datasets. Our\nexperimental results show that our models achieve competitive results on\ncontrolled paraphrase generation and strong performance on controlled machine\ntranslation. Analysis shows that our models learn to disentangle semantics and\nsyntax in their latent representations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:02:50 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Mingda", ""], ["Wiseman", "Sam", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2010.05873", "submitter": "Katja Filippova", "authors": "Katja Filippova", "title": "Controlled Hallucinations: Learning to Generate Faithfully from Noisy\n  Data", "comments": null, "journal-ref": "Findings of EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation (data- or text-to-text) demonstrates remarkable\nperformance when training data is abundant which for many applications is not\nthe case. To collect a large corpus of parallel data, heuristic rules are often\nused but they inevitably let noise into the data, such as phrases in the output\nwhich cannot be explained by the input. Consequently, models pick up on the\nnoise and may hallucinate--generate fluent but unsupported text. Our\ncontribution is a simple but powerful technique to treat such hallucinations as\na controllable aspect of the generated text, without dismissing any input and\nwithout modifying the model architecture. On the WikiBio corpus (Lebret et al.,\n2016), a particularly noisy dataset, we demonstrate the efficacy of the\ntechnique both in an automatic and in a human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:25:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Filippova", "Katja", ""]]}, {"id": "2010.05874", "submitter": "Zirui Wang", "authors": "Zirui Wang, Yulia Tsvetkov, Orhan Firat, Yuan Cao", "title": "Gradient Vaccine: Investigating and Improving Multi-task Optimization in\n  Massively Multilingual Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively multilingual models subsuming tens or even hundreds of languages\npose great challenges to multi-task optimization. While it is a common practice\nto apply a language-agnostic procedure optimizing a joint multilingual task\nobjective, how to properly characterize and take advantage of its underlying\nproblem structure for improving optimization efficiency remains under-explored.\nIn this paper, we attempt to peek into the black-box of multilingual\noptimization through the lens of loss function geometry. We find that gradient\nsimilarity measured along the optimization trajectory is an important signal,\nwhich correlates well with not only language proximity but also the overall\nmodel performance. Such observation helps us to identify a critical limitation\nof existing gradient-based multi-task learning methods, and thus we derive a\nsimple and scalable optimization procedure, named Gradient Vaccine, which\nencourages more geometrically aligned parameter updates for close tasks.\nEmpirically, our method obtains significant model performance gains on\nmultilingual machine translation and XTREME benchmark tasks for multilingual\nlanguage models. Our work reveals the importance of properly measuring and\nutilizing language proximity in multilingual optimization, and has broader\nimplications for multi-task learning beyond multilingual modeling.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:26:34 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Zirui", ""], ["Tsvetkov", "Yulia", ""], ["Firat", "Orhan", ""], ["Cao", "Yuan", ""]]}, {"id": "2010.05904", "submitter": "Revanth Reddy", "authors": "Rong Zhang, Revanth Gangi Reddy, Md Arafat Sultan, Vittorio Castelli,\n  Anthony Ferritto, Radu Florian, Efsun Sarioglu Kayi, Salim Roukos, Avirup\n  Sil, Todd Ward", "title": "Multi-Stage Pre-training for Low-Resource Domain Adaptation", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning techniques are particularly useful in NLP tasks where a\nsizable amount of high-quality annotated data is difficult to obtain. Current\napproaches directly adapt a pre-trained language model (LM) on in-domain text\nbefore fine-tuning to downstream tasks. We show that extending the vocabulary\nof the LM with domain-specific terms leads to further gains. To a bigger\neffect, we utilize structure in the unlabeled data to create auxiliary\nsynthetic tasks, which helps the LM transfer to downstream tasks. We apply\nthese approaches incrementally on a pre-trained Roberta-large LM and show\nconsiderable performance gain on three tasks in the IT domain: Extractive\nReading Comprehension, Document Ranking and Duplicate Question Detection.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:57:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhang", "Rong", ""], ["Reddy", "Revanth Gangi", ""], ["Sultan", "Md Arafat", ""], ["Castelli", "Vittorio", ""], ["Ferritto", "Anthony", ""], ["Florian", "Radu", ""], ["Kayi", "Efsun Sarioglu", ""], ["Roukos", "Salim", ""], ["Sil", "Avirup", ""], ["Ward", "Todd", ""]]}, {"id": "2010.05906", "submitter": "Lianhui Qin", "authors": "Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena\n  Hwang, Ronan Le Bras, Antoine Bosselut, Yejin Choi", "title": "Back to the Future: Unsupervised Backprop-based Decoding for\n  Counterfactual and Abductive Commonsense Reasoning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abductive and counterfactual reasoning, core abilities of everyday human\ncognition, require reasoning about what might have happened at time t, while\nconditioning on multiple contexts from the relative past and future. However,\nsimultaneous incorporation of past and future contexts using generative\nlanguage models (LMs) can be challenging, as they are trained either to\ncondition only on the past context or to perform narrowly scoped\ntext-infilling. In this paper, we propose DeLorean, a new unsupervised decoding\nalgorithm that can flexibly incorporate both the past and future contexts using\nonly off-the-shelf, left-to-right language models and no supervision. The key\nintuition of our algorithm is incorporating the future through\nback-propagation, during which, we only update the internal representation of\nthe output while fixing the model parameters. By alternating between forward\nand backward propagation, DeLorean can decode the output representation that\nreflects both the left and right contexts. We demonstrate that our approach is\ngeneral and applicable to two nonmonotonic reasoning tasks: abductive text\ngeneration and counterfactual story revision, where DeLorean outperforms a\nrange of unsupervised and some supervised methods, based on automatic and human\nevaluation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:58:43 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 05:39:07 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 07:59:45 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Qin", "Lianhui", ""], ["Shwartz", "Vered", ""], ["West", "Peter", ""], ["Bhagavatula", "Chandra", ""], ["Hwang", "Jena", ""], ["Bras", "Ronan Le", ""], ["Bosselut", "Antoine", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.05953", "submitter": "Jena Hwang", "authors": "Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke\n  Sakaguchi, Antoine Bosselut, Yejin Choi", "title": "COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have brought about a renewed interest in commonsense\nrepresentation and reasoning in the field of natural language understanding.\nThe development of new commonsense knowledge graphs (CSKG) has been central to\nthese advances as their diverse facts can be used and referenced by machine\nlearning models for tackling new and challenging tasks. At the same time, there\nremain questions about the quality and coverage of these resources due to the\nmassive scale required to comprehensively encompass general commonsense\nknowledge.\n  In this work, we posit that manually constructed CSKGs will never achieve the\ncoverage necessary to be applicable in all situations encountered by NLP\nagents. Therefore, we propose a new evaluation framework for testing the\nutility of KGs based on how effectively implicit knowledge representations can\nbe learned from them.\n  With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose\ncommonsense knowledge containing knowledge that is not readily available in\npretrained language models. We evaluate its properties in comparison with other\nleading CSKGs, performing the first large-scale pairwise study of commonsense\nknowledge resources. Next, we show that ATOMIC 2020 is better suited for\ntraining knowledge models that can generate accurate, representative knowledge\nfor new, unseen entities and events. Finally, through human evaluation, we show\nthat the few-shot performance of GPT-3 (175B parameters), while impressive,\nremains ~12 absolute points lower than a BART-based knowledge model trained on\nATOMIC 2020 despite using over 430x fewer parameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:27:05 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hwang", "Jena D.", ""], ["Bhagavatula", "Chandra", ""], ["Bras", "Ronan Le", ""], ["Da", "Jeff", ""], ["Sakaguchi", "Keisuke", ""], ["Bosselut", "Antoine", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.05959", "submitter": "Alexander Gutkin", "authors": "Alexander Gutkin and Martin Jansche and Lucy Skidmore", "title": "Towards Induction of Structured Phoneme Inventories", "comments": "To appear in the Second Workshop on Computational Research in\n  Linguistic Typology (SIGTYP 2020) at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This extended abstract surveying the work on phonological typology was\nprepared for \"SIGTYP 2020: The Second Workshop on Computational Research in\nLinguistic Typology\" to be held at EMNLP 2020.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:39:07 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gutkin", "Alexander", ""], ["Jansche", "Martin", ""], ["Skidmore", "Lucy", ""]]}, {"id": "2010.05961", "submitter": "Ewan Dunbar", "authors": "Juliette Millet and Ewan Dunbar", "title": "Perceptimatic: A human speech perception benchmark for unsupervised\n  subword modelling", "comments": null, "journal-ref": "Proceedings of Interspeech 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a data set and methods to compare speech processing\nmodels and human behaviour on a phone discrimination task. We provide\nPerceptimatic, an open data set which consists of French and English speech\nstimuli, as well as the results of 91 English- and 93 French-speaking\nlisteners. The stimuli test a wide range of French and English contrasts, and\nare extracted directly from corpora of natural running read speech, used for\nthe 2017 Zero Resource Speech Challenge. We provide a method to compare humans'\nperceptual space with models' representational space, and we apply it to models\npreviously submitted to the Challenge. We show that, unlike unsupervised models\nand supervised multilingual models, a standard supervised monolingual HMM-GMM\nphone recognition system, while good at discriminating phones, yields a\nrepresentational space very different from that of human native listeners.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:40:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Millet", "Juliette", ""], ["Dunbar", "Ewan", ""]]}, {"id": "2010.05967", "submitter": "Ewan Dunbar", "authors": "Ewan Dunbar and Julien Karadayi and Mathieu Bernard and Xuan-Nga Cao\n  and Robin Algayres and Lucas Ondel and Laurent Besacier and Sakriani Sakti\n  and Emmanuel Dupoux", "title": "The Zero Resource Speech Challenge 2020: Discovering discrete subword\n  and word units", "comments": null, "journal-ref": "Proceedings of Interspeech 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Zero Resource Speech Challenge 2020, which aims at learning\nspeech representations from raw audio signals without any labels. It combines\nthe data sets and metrics from two previous benchmarks (2017 and 2019) and\nfeatures two tasks which tap into two levels of speech representation. The\nfirst task is to discover low bit-rate subword representations that optimize\nthe quality of speech synthesis; the second one is to discover word-like units\nfrom unsegmented raw speech. We present the results of the twenty submitted\nmodels and discuss the implications of the main findings for unsupervised\nspeech learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 18:56:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dunbar", "Ewan", ""], ["Karadayi", "Julien", ""], ["Bernard", "Mathieu", ""], ["Cao", "Xuan-Nga", ""], ["Algayres", "Robin", ""], ["Ondel", "Lucas", ""], ["Besacier", "Laurent", ""], ["Sakti", "Sakriani", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2010.05971", "submitter": "Yanai Elazar", "authors": "Yanai Elazar, Victoria Basmov, Shauli Ravfogel, Yoav Goldberg, Reut\n  Tsarfaty", "title": "The Extraordinary Failure of Complement Coercion Crowdsourcing", "comments": "Workshop on Insights from Negative Results in NLP, co-located with\n  EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has eased and scaled up the collection of linguistic annotation\nin recent years. In this work, we follow known methodologies of collecting\nlabeled data for the complement coercion phenomenon. These are constructions\nwith an implied action -- e.g., \"I started a new book I bought last week\",\nwhere the implied action is reading. We aim to collect annotated data for this\nphenomenon by reducing it to either of two known tasks: Explicit Completion and\nNatural Language Inference. However, in both cases, crowdsourcing resulted in\nlow agreement scores, even though we followed the same methodologies as in\nprevious work. Why does the same process fail to yield high agreement scores?\nWe specify our modeling schemes, highlight the differences with previous work\nand provide some insights about the task and possible explanations for the\nfailure. We conclude that specific phenomena require tailored solutions, not\nonly in specialized algorithms, but also in data collection methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:04:04 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Elazar", "Yanai", ""], ["Basmov", "Victoria", ""], ["Ravfogel", "Shauli", ""], ["Goldberg", "Yoav", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2010.05985", "submitter": "Alexander Gutkin", "authors": "Alexander Gutkin and Richard Sproat", "title": "NEMO: Frequentist Inference Approach to Constrained Linguistic Typology\n  Feature Prediction in SIGTYP 2020 Shared Task", "comments": "To appear in Second Workshop on Computational Research in Linguistic\n  Typology (SIGTYP 2020) at 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the NEMO submission to SIGTYP 2020 shared task which\ndeals with prediction of linguistic typological features for multiple languages\nusing the data derived from World Atlas of Language Structures (WALS). We\nemploy frequentist inference to represent correlations between typological\nfeatures and use this representation to train simple multi-class estimators\nthat predict individual features. We describe two submitted ridge\nregression-based configurations which ranked second and third overall in the\nconstrained task. Our best configuration achieved the micro-averaged accuracy\nscore of 0.66 on 149 test languages.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:25:43 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gutkin", "Alexander", ""], ["Sproat", "Richard", ""]]}, {"id": "2010.05987", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Arman Cohan, Nazli Goharian", "title": "SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search", "comments": "EMNLP 2020. This article draws heavily from arXiv:2005.02365", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With worldwide concerns surrounding the Severe Acute Respiratory Syndrome\nCoronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific\nliterature on the virus. Clinicians, researchers, and policy-makers need to be\nable to search these articles effectively. In this work, we present a zero-shot\nranking algorithm that adapts to COVID-related scientific literature. Our\napproach filters training data from another collection down to medical-related\nqueries, uses a neural re-ranking model pre-trained on scientific text\n(SciBERT), and filters the target document collection. This approach ranks top\namong zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a\nP@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2\njudgments. Despite not relying on TREC-COVID data, our method outperforms\nmodels that do. As one of the first search methods to thoroughly evaluate\nCOVID-19 search, we hope that this serves as a strong baseline and helps in the\nglobal crisis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:28:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["MacAvaney", "Sean", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "2010.05990", "submitter": "Jordan J. Bird", "authors": "Jordan J. Bird, Anik\\'o Ek\\'art, Diego R. Faria", "title": "Chatbot Interaction with Artificial Intelligence: Human Data\n  Augmentation with T5 and Language Transformer Ensemble for Text\n  Classification", "comments": "18 pages, 10 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Chatbot Interaction with Artificial Intelligence\n(CI-AI) framework as an approach to the training of deep learning chatbots for\ntask classification. The intelligent system augments human-sourced data via\nartificial paraphrasing in order to generate a large set of training data for\nfurther classical, attention, and language transformation-based learning\napproaches for Natural Language Processing. Human beings are asked to\nparaphrase commands and questions for task identification for further execution\nof a machine. The commands and questions are split into training and validation\nsets. A total of 483 responses were recorded. Secondly, the training set is\nparaphrased by the T5 model in order to augment it with further data. Seven\nstate-of-the-art transformer-based text classification algorithms (BERT,\nDistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are\nbenchmarked for both sets after fine-tuning on the training data for two\nepochs. We find that all models are improved when training data is augmented by\nthe T5 model, with an average increase of classification accuracy by 4.01%. The\nbest result was the RoBERTa model trained on T5 augmented data which achieved\n98.96% classification accuracy. Finally, we found that an ensemble of the five\nbest-performing transformer models via Logistic Regression of output label\npredictions led to an accuracy of 99.59% on the dataset of human responses. A\nhighly-performing model allows the intelligent system to interpret human\ncommands at the social-interaction level through a chatbot-like interface (e.g.\n\"Robot, can we have a conversation?\") and allows for better accessibility to AI\nby non-technical users.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:37:18 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:33:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Bird", "Jordan J.", ""], ["Ek\u00e1rt", "Anik\u00f3", ""], ["Faria", "Diego R.", ""]]}, {"id": "2010.05993", "submitter": "Andrea Zugarini", "authors": "Andrea Zugarini and Matteo Tiezzi and Marco Maggini", "title": "Vulgaris: Analysis of a Corpus for Middle-Age Varieties of Italian\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Italian is a Romance language that has its roots in Vulgar Latin. The birth\nof the modern Italian started in Tuscany around the 14th century, and it is\nmainly attributed to the works of Dante Alighieri, Francesco Petrarca and\nGiovanni Boccaccio, who are among the most acclaimed authors of the medieval\nage in Tuscany. However, Italy has been characterized by a high variety of\ndialects, which are often loosely related to each other, due to the past\nfragmentation of the territory. Italian has absorbed influences from many of\nthese dialects, as also from other languages due to dominion of portions of the\ncountry by other nations, such as Spain and France. In this work we present\nVulgaris, a project aimed at studying a corpus of Italian textual resources\nfrom authors of different regions, ranging in a time period between 1200 and\n1600. Each composition is associated to its author, and authors are also\ngrouped in families, i.e. sharing similar stylistic/chronological\ncharacteristics. Hence, the dataset is not only a valuable resource for\nstudying the diachronic evolution of Italian and the differences between its\ndialects, but it is also useful to investigate stylistic aspects between single\nauthors. We provide a detailed statistical analysis of the data, and a\ncorpus-driven study in dialectology and diachronic varieties.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:42:22 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Zugarini", "Andrea", ""], ["Tiezzi", "Matteo", ""], ["Maggini", "Marco", ""]]}, {"id": "2010.05994", "submitter": "Jianqiao Li", "authors": "Guoyin Wang, Chunyuan Li, Jianqiao Li, Hao Fu, Yuh-Chen Lin, Liqun\n  Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Dinghan Shen, Qian\n  Yang and Lawrence Carin", "title": "Improving Text Generation with Student-Forcing Optimal Transport", "comments": "To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models are often trained with maximum likelihood estimation\n(MLE), where the next word is generated conditioned on the ground-truth word\ntokens. During testing, however, the model is instead conditioned on previously\ngenerated tokens, resulting in what is termed exposure bias. To reduce this gap\nbetween training and testing, we propose using optimal transport (OT) to match\nthe sequences generated in these two modes. An extension is further proposed to\nimprove the OT learning, based on the structural and contextual information of\nthe text sequences. The effectiveness of the proposed method is validated on\nmachine translation, text summarization, and text generation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:42:25 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wang", "Guoyin", ""], ["Li", "Chunyuan", ""], ["Li", "Jianqiao", ""], ["Fu", "Hao", ""], ["Lin", "Yuh-Chen", ""], ["Chen", "Liqun", ""], ["Zhang", "Yizhe", ""], ["Tao", "Chenyang", ""], ["Zhang", "Ruiyi", ""], ["Wang", "Wenlin", ""], ["Shen", "Dinghan", ""], ["Yang", "Qian", ""], ["Carin", "Lawrence", ""]]}, {"id": "2010.05997", "submitter": "Xing Jie Zhong", "authors": "Xing Jie Zhong, and David Chiang", "title": "Look It Up: Bilingual and Monolingual Dictionaries Improve Neural\n  Machine Translation", "comments": "Accepted for publication in Proceedings of WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advances in neural machine translation (NMT) quality, rare words\ncontinue to be problematic. For humans, the solution to the rare-word problem\nhas long been dictionaries, but dictionaries cannot be straightforwardly\nincorporated into NMT. In this paper, we describe a new method for \"attaching\"\ndictionary definitions to rare words so that the network can learn the best way\nto use them. We demonstrate improvements of up to 3.1 BLEU using bilingual\ndictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:53:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Zhong", "Xing Jie", ""], ["Chiang", "David", ""]]}, {"id": "2010.06000", "submitter": "Sanjay Subramanian", "authors": "Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine\n  van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi", "title": "MedICaT: A Dataset of Medical Images, Captions, and Textual References", "comments": "EMNLP-Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between figures and text is key to scientific\ndocument understanding. Medical figures in particular are quite complex, often\nconsisting of several subfigures (75% of figures in our dataset), with detailed\ntext describing their content. Previous work studying figures in scientific\npapers focused on classifying figure content rather than understanding how\nimages relate to the text. To address challenges in figure retrieval and\nfigure-to-text alignment, we introduce MedICaT, a dataset of medical images in\ncontext. MedICaT consists of 217K images from 131K open access biomedical\npapers, and includes captions, inline references for 74% of figures, and\nmanually annotated subfigures and subcaptions for a subset of figures. Using\nMedICaT, we introduce the task of subfigure to subcaption alignment in compound\nfigures and demonstrate the utility of inline references in image-text\nmatching. Our data and code can be accessed at\nhttps://github.com/allenai/medicat.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:56:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Subramanian", "Sanjay", ""], ["Wang", "Lucy Lu", ""], ["Mehta", "Sachin", ""], ["Bogin", "Ben", ""], ["van Zuylen", "Madeleine", ""], ["Parasa", "Sravanthi", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2010.06018", "submitter": "Tom Kocmi", "authors": "Tom Kocmi, Tomasz Limisiewicz, Gabriel Stanovsky", "title": "Gender Coreference and Bias Evaluation at WMT 2020", "comments": "Accepted WMT20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender bias in machine translation can manifest when choosing gender\ninflections based on spurious gender correlations. For example, always\ntranslating doctors as men and nurses as women. This can be particularly\nharmful as models become more popular and deployed within commercial systems.\nOur work presents the largest evidence for the phenomenon in more than 19\nsystems submitted to the WMT over four diverse target languages: Czech, German,\nPolish, and Russian. To achieve this, we use WinoMT, a recent automatic test\nsuite which examines gender coreference and bias when translating from English\nto languages with grammatical gender. We extend WinoMT to handle two new\nlanguages tested in WMT: Polish and Czech. We find that all systems\nconsistently use spurious correlations in the data rather than meaningful\ncontextual information.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 20:42:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kocmi", "Tom", ""], ["Limisiewicz", "Tomasz", ""], ["Stanovsky", "Gabriel", ""]]}, {"id": "2010.06028", "submitter": "Cicero Nogueira Dos Santos", "authors": "Siamak Shakeri, Cicero Nogueira dos Santos, Henry Zhu, Patrick Ng,\n  Feng Nan, Zhiguo Wang, Ramesh Nallapati, Bing Xiang", "title": "End-to-End Synthetic Data Generation for Domain Adaptation of Question\n  Answering Systems", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end approach for synthetic QA data generation. Our model\ncomprises a single transformer-based encoder-decoder network that is trained\nend-to-end to generate both answers and questions. In a nutshell, we feed a\npassage to the encoder and ask the decoder to generate a question and an answer\ntoken-by-token. The likelihood produced in the generation process is used as a\nfiltering score, which avoids the need for a separate filtering model. Our\ngenerator is trained by fine-tuning a pretrained LM using maximum likelihood\nestimation. The experimental results indicate significant improvements in the\ndomain adaptation of QA models outperforming current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:10:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Shakeri", "Siamak", ""], ["Santos", "Cicero Nogueira dos", ""], ["Zhu", "Henry", ""], ["Ng", "Patrick", ""], ["Nan", "Feng", ""], ["Wang", "Zhiguo", ""], ["Nallapati", "Ramesh", ""], ["Xiang", "Bing", ""]]}, {"id": "2010.06030", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Wei Han, Anmol Gulati, Chung-Cheng Chiu, Bo Li, Tara N.\n  Sainath, Yonghui Wu, Ruoming Pang", "title": "Dual-mode ASR: Unify and Improve Streaming ASR with Full-context\n  Modeling", "comments": "Accepted in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming automatic speech recognition (ASR) aims to emit each hypothesized\nword as quickly and accurately as possible, while full-context ASR waits for\nthe completion of a full speech utterance before emitting completed hypotheses.\nIn this work, we propose a unified framework, Dual-mode ASR, to train a single\nend-to-end ASR model with shared weights for both streaming and full-context\nspeech recognition. We show that the latency and accuracy of streaming ASR\nsignificantly benefit from weight sharing and joint training of full-context\nASR, especially with inplace knowledge distillation during the training. The\nDual-mode ASR framework can be applied to recent state-of-the-art\nconvolution-based and transformer-based ASR networks. We present extensive\nexperiments with two state-of-the-art ASR networks, ContextNet and Conformer,\non two datasets, a widely used public dataset LibriSpeech and a large-scale\ndataset MultiDomain. Experiments and ablation studies demonstrate that\nDual-mode ASR not only simplifies the workflow of training and deploying\nstreaming and full-context ASR models, but also significantly improves both\nemission latency and recognition accuracy of streaming ASR. With Dual-mode ASR,\nwe achieve new state-of-the-art streaming ASR results on both LibriSpeech and\nMultiDomain in terms of accuracy and latency.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:12:56 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 17:56:46 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yu", "Jiahui", ""], ["Han", "Wei", ""], ["Gulati", "Anmol", ""], ["Chiu", "Chung-Cheng", ""], ["Li", "Bo", ""], ["Sainath", "Tara N.", ""], ["Wu", "Yonghui", ""], ["Pang", "Ruoming", ""]]}, {"id": "2010.06032", "submitter": "Kellie Webster", "authors": "Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and\n  Emily Pitler and Ellie Pavlick and Jilin Chen and Ed Chi and Slav Petrov", "title": "Measuring and Reducing Gendered Correlations in Pre-trained Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained models have revolutionized natural language understanding.\nHowever, researchers have found they can encode artifacts undesired in many\napplications, such as professions correlating with one gender more than\nanother. We explore such gendered correlations as a case study for how to\naddress unintended correlations in pre-trained models. We define metrics and\nreveal that it is possible for models with similar accuracy to encode\ncorrelations at very different rates. We show how measured correlations can be\nreduced with general-purpose techniques, and highlight the trade offs different\nstrategies have. With these results, we make recommendations for training\nrobust models: (1) carefully evaluate unintended correlations, (2) be mindful\nof seemingly innocuous configuration differences, and (3) focus on general\nmitigations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:15:29 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 21:04:26 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Webster", "Kellie", ""], ["Wang", "Xuezhi", ""], ["Tenney", "Ian", ""], ["Beutel", "Alex", ""], ["Pitler", "Emily", ""], ["Pavlick", "Ellie", ""], ["Chen", "Jilin", ""], ["Chi", "Ed", ""], ["Petrov", "Slav", ""]]}, {"id": "2010.06040", "submitter": "Dinghan Shen", "authors": "Mingzhi Zheng, Dinghan Shen, Yelong Shen, Weizhu Chen, Lin Xiao", "title": "Improving Self-supervised Pre-training via a Fully-Explored Masked\n  Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Masked Language Model (MLM) framework has been widely adopted for\nself-supervised language pre-training. In this paper, we argue that randomly\nsampled masks in MLM would lead to undesirably large gradient variance. Thus,\nwe theoretically quantify the gradient variance via correlating the gradient\ncovariance with the Hamming distance between two different masks (given a\ncertain text sequence). To reduce the variance due to the sampling of masks, we\npropose a fully-explored masking strategy, where a text sequence is divided\ninto a certain number of non-overlapping segments. Thereafter, the tokens\nwithin one segment are masked for training. We prove, from a theoretical\nperspective, that the gradients derived from this new masking schema have a\nsmaller variance and can lead to more efficient self-supervised training. We\nconduct extensive experiments on both continual pre-training and general\npre-training from scratch. Empirical results confirm that this new masking\nstrategy can consistently outperform standard random masking. Detailed\nefficiency analysis and ablation studies further validate the advantages of our\nfully-explored masking strategy under the MLM framework.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:28:14 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:45:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zheng", "Mingzhi", ""], ["Shen", "Dinghan", ""], ["Shen", "Yelong", ""], ["Chen", "Weizhu", ""], ["Xiao", "Lin", ""]]}, {"id": "2010.06041", "submitter": "Sina Ahmadi", "authors": "Sina Ahmadi, Mariam Masoud", "title": "Towards Machine Translation for the Kurdish Language", "comments": "12 pages - under review in the ACM Transactions on Asian and\n  Low-Resource Language Information Processing (TALLIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine translation is the task of translating texts from one language to\nanother using computers. It has been one of the major tasks in natural language\nprocessing and computational linguistics and has been motivating to facilitate\nhuman communication. Kurdish, an Indo-European language, has received little\nattention in this realm due to the language being less-resourced. Therefore, in\nthis paper, we are addressing the main issues in creating a machine translation\nsystem for the Kurdish language, with a focus on the Sorani dialect. We\ndescribe the available scarce parallel data suitable for training a neural\nmachine translation model for Sorani Kurdish-English translation. We also\ndiscuss some of the major challenges in Kurdish language translation and\ndemonstrate how fundamental text processing tasks, such as tokenization, can\nimprove translation performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:28:57 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ahmadi", "Sina", ""], ["Masoud", "Mariam", ""]]}, {"id": "2010.06047", "submitter": "Saturnino Luz", "authors": "Sofia de la Fuente Garcia, Craig Ritchie and Saturnino Luz", "title": "Artificial Intelligence, speech and language processing approaches to\n  monitoring Alzheimer's Disease: a systematic review", "comments": "Pre-print submitted to the Journal of Alzheimer's Disease", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is a valuable source of clinical information in Alzheimer's Disease,\nas it declines concurrently with neurodegeneration. Consequently, speech and\nlanguage data have been extensively studied in connection with its diagnosis.\nThis paper summarises current findings on the use of artificial intelligence,\nspeech and language processing to predict cognitive decline in the context of\nAlzheimer's Disease, detailing current research procedures, highlighting their\nlimitations and suggesting strategies to address them. We conducted a\nsystematic review of original research between 2000 and 2019, registered in\nPROSPERO (reference CRD42018116606). An interdisciplinary search covered six\ndatabases on engineering (ACM and IEEE), psychology (PsycINFO), medicine\n(PubMed and Embase) and Web of Science. Bibliographies of relevant papers were\nscreened until December 2019. From 3,654 search results 51 articles were\nselected against the eligibility criteria. Four tables summarise their\nfindings: study details (aim, population, interventions, comparisons, methods\nand outcomes), data details (size, type, modalities, annotation, balance,\navailability and language of study), methodology (pre-processing, feature\ngeneration, machine learning, evaluation and results) and clinical\napplicability (research implications, clinical potential, risk of bias and\nstrengths/limitations). While promising results are reported across nearly all\n51 studies, very few have been implemented in clinical research or practice. We\nconcluded that the main limitations of the field are poor standardisation,\nlimited comparability of results, and a degree of disconnect between study aims\nand clinical applications. Attempts to close these gaps should support\ntranslation of future research into clinical practice.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:43:04 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Garcia", "Sofia de la Fuente", ""], ["Ritchie", "Craig", ""], ["Luz", "Saturnino", ""]]}, {"id": "2010.06053", "submitter": "Zhao Song", "authors": "Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora", "title": "TextHide: Tackling Data Privacy in Language Understanding Tasks", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  An unsolved challenge in distributed or federated learning is to effectively\nmitigate privacy risks without slowing down training or reducing accuracy. In\nthis paper, we propose TextHide aiming at addressing this challenge for natural\nlanguage understanding tasks. It requires all participants to add a simple\nencryption step to prevent an eavesdropping attacker from recovering private\ntext data. Such an encryption step is efficient and only affects the task\nperformance slightly. In addition, TextHide fits well with the popular\nframework of fine-tuning pre-trained language models (e.g., BERT) for any\nsentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and\nour experiments show that TextHide can effectively defend attacks on shared\ngradients or representations and the averaged accuracy reduction is only\n$1.9\\%$. We also present an analysis of the security of TextHide using a\nconjecture about the computational intractability of a mathematical problem.\n  Our code is available at https://github.com/Hazelsuko07/TextHide\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 22:22:15 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Huang", "Yangsibo", ""], ["Song", "Zhao", ""], ["Chen", "Danqi", ""], ["Li", "Kai", ""], ["Arora", "Sanjeev", ""]]}, {"id": "2010.06060", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa\n  Patwary, Mohammad Shoeybi, Raghav Mani", "title": "BioMegatron: Larger Biomedical Domain Language Model", "comments": "Accepted for publication at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an influx of biomedical domain-specific language models,\nshowing language models pre-trained on biomedical text perform better on\nbiomedical domain benchmarks than those trained on general domain text corpora\nsuch as Wikipedia and Books. Yet, most works do not study the factors affecting\neach domain language application deeply. Additionally, the study of model size\non domain-specific models has been mostly missing. We empirically study and\nevaluate several factors that can affect performance on domain language\napplications, such as the sub-word vocabulary set, model size, pre-training\ncorpus, and domain transfer. We show consistent improvements on benchmarks with\nour larger BioMegatron model trained on a larger domain corpus, contributing to\nour understanding of domain language model applications. We demonstrate\nnoticeable improvements over the previous state-of-the-art (SOTA) on standard\nbiomedical NLP benchmarks of named entity recognition, relation extraction, and\nquestion answering. Model checkpoints and code are available at\n[https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 22:46:10 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:02:55 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Zhang", "Yang", ""], ["Bakhturina", "Evelina", ""], ["Puri", "Raul", ""], ["Patwary", "Mostofa", ""], ["Shoeybi", "Mohammad", ""], ["Mani", "Raghav", ""]]}, {"id": "2010.06065", "submitter": "Zonghai Yao", "authors": "Zonghai Yao, Liangliang Cao and Huapu Pan", "title": "Zero-shot Entity Linking with Efficient Long Range Sequence Modeling", "comments": "6 pages, 6 figures, Findings of EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of zero-shot entity linking, in which a link\nin the test time may not present in training. Following the prevailing\nBERT-based research efforts, we find a simple yet effective way is to expand\nthe long-range sequence modeling. Unlike many previous methods, our method does\nnot require expensive pre-training of BERT with long position embedding.\nInstead, we propose an efficient position embeddings initialization method\ncalled Embedding-repeat, which initializes larger position embeddings based on\nBERT-Base. On Wikia's zero-shot EL dataset, our method improves the SOTA from\n76.06% to 79.08%, and for its long data, the corresponding improvement is from\n74.57% to 82.14%. Our experiments suggest the effectiveness of long-range\nsequence modeling without retraining the BERT model.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 22:59:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yao", "Zonghai", ""], ["Cao", "Liangliang", ""], ["Pan", "Huapu", ""]]}, {"id": "2010.06069", "submitter": "Shiran Dudy", "authors": "Shiran Dudy and Steven Bedrick", "title": "Are Some Words Worth More than Others?", "comments": "EMNLP 2020 Eval4NLP Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current evaluation metrics for language modeling and generation rely heavily\non the accuracy of predicted (or generated) words as compared to a reference\nground truth. While important, token-level accuracy only captures one aspect of\na language model's behavior, and ignores linguistic properties of words that\nmay allow some mis-predicted tokens to be useful in practice. Furthermore,\nstatistics directly tied to prediction accuracy (including perplexity) may be\nconfounded by the Zipfian nature of written language, as the majority of the\nprediction attempts will occur with frequently-occurring types. A model's\nperformance may vary greatly between high- and low-frequency words, which in\npractice could lead to failure modes such as repetitive and dull generated text\nbeing produced by a downstream consumer of a language model. To address this,\nwe propose two new intrinsic evaluation measures within the framework of a\nsimple word prediction task that are designed to give a more holistic picture\nof a language model's performance. We evaluate several commonly-used large\nEnglish language models using our proposed metrics, and demonstrate that our\napproach reveals functional differences in performance between the models that\nare obscured by more traditional metrics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:12:11 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 03:39:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Dudy", "Shiran", ""], ["Bedrick", "Steven", ""]]}, {"id": "2010.06115", "submitter": "Yuanhe Tian", "authors": "Yuanhe Tian, Yan Song, Fei Xia", "title": "Supertagging Combinatory Categorial Grammar with Attentive Graph\n  Convolutional Networks", "comments": "Natural Language Processing. 8 pages, 4 figures. EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supertagging is conventionally regarded as an important task for combinatory\ncategorial grammar (CCG) parsing, where effective modeling of contextual\ninformation is highly important to this task. However, existing studies have\nmade limited efforts to leverage contextual features except for applying\npowerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph\nconvolutional networks to enhance neural CCG supertagging through a novel\nsolution of leveraging contextual information. Specifically, we build the graph\nfrom chunks (n-grams) extracted from a lexicon and apply attention over the\ngraph, so that different word pairs from the contexts within and across chunks\nare weighted in the model and facilitate the supertagging accordingly. The\nexperiments performed on the CCGbank demonstrate that our approach outperforms\nall previous studies in terms of both supertagging and parsing. Further\nanalyses illustrate the effectiveness of each component in our approach to\ndiscriminatively learn from word pairs to enhance CCG supertagging.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:58:29 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 05:46:34 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Tian", "Yuanhe", ""], ["Song", "Yan", ""], ["Xia", "Fei", ""]]}, {"id": "2010.06119", "submitter": "Qingyun Wang", "authors": "Qingyun Wang, Qi Zeng, Lifu Huang, Kevin Knight, Heng Ji, Nazneen\n  Fatema Rajani", "title": "ReviewRobot: Explainable Paper Review Generation based on Knowledge\n  Synthesis", "comments": "14 pages. Accepted by The 14th International Conference on Natural\n  Language Generation (INLG 2020) Code and resource is available at\n  https://github.com/EagleW/ReviewRobot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assist human review process, we build a novel ReviewRobot to automatically\nassign a review score and write comments for multiple categories such as\nnovelty and meaningful comparison. A good review needs to be knowledgeable,\nnamely that the comments should be constructive and informative to help improve\nthe paper; and explainable by providing detailed evidence. ReviewRobot achieves\nthese goals via three steps: (1) We perform domain-specific Information\nExtraction to construct a knowledge graph (KG) from the target paper under\nreview, a related work KG from the papers cited by the target paper, and a\nbackground KG from a large collection of previous papers in the domain. (2) By\ncomparing these three KGs, we predict a review score and detailed structured\nknowledge as evidence for each review category. (3) We carefully select and\ngeneralize human review sentences into templates, and apply these templates to\ntransform the review scores and evidence into natural language comments.\nExperimental results show that our review score predictor reaches 71.4%-100%\naccuracy. Human assessment by domain experts shows that 41.7%-70.5% of the\ncomments generated by ReviewRobot are valid and constructive, and better than\nhuman-written ones for 20% of the time. Thus, ReviewRobot can serve as an\nassistant for paper reviewers, program chairs and authors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 02:17:58 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 00:30:08 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 22:31:33 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Wang", "Qingyun", ""], ["Zeng", "Qi", ""], ["Huang", "Lifu", ""], ["Knight", "Kevin", ""], ["Ji", "Heng", ""], ["Rajani", "Nazneen Fatema", ""]]}, {"id": "2010.06122", "submitter": "Clara Vania", "authors": "Clara Vania, Ruijie Chen, Samuel R. Bowman", "title": "Asking Crowdworkers to Write Entailment Examples: The Best of Bad\n  Options", "comments": "AACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale natural language inference (NLI) datasets such as SNLI or MNLI\nhave been created by asking crowdworkers to read a premise and write three new\nhypotheses, one for each possible semantic relationships (entailment,\ncontradiction, and neutral). While this protocol has been used to create useful\nbenchmark data, it remains unclear whether the writing-based annotation\nprotocol is optimal for any purpose, since it has not been evaluated directly.\nFurthermore, there is ample evidence that crowdworker writing can introduce\nartifacts in the data. We investigate two alternative protocols which\nautomatically create candidate (premise, hypothesis) pairs for annotators to\nlabel. Using these protocols and a writing-based baseline, we collect several\nnew English NLI datasets of over 3k examples each, each using a fixed amount of\nannotator time, but a varying number of examples to fit that time budget. Our\nexperiments on NLI and transfer learning show negative results: None of the\nalternative protocols outperforms the baseline in evaluations of generalization\nwithin NLI or on transfer to outside target tasks. We conclude that crowdworker\nwriting still the best known option for entailment data, highlighting the need\nfor further data collection work to focus on improving writing-based annotation\nprocesses.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 02:27:05 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Vania", "Clara", ""], ["Chen", "Ruijie", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2010.06127", "submitter": "Yang Chen", "authors": "Yang Chen and Alan Ritter", "title": "Model Selection for Cross-Lingual Transfer using a Learned Scoring\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers that are pre-trained on multilingual text corpora, such as,\nmBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning\nresults. In the zero-shot cross-lingual transfer setting, only English training\ndata is assumed, and the fine-tuned model is evaluated on another target\nlanguage. No target-language validation data is assumed in this setting,\nhowever substantial variance has been observed in target language performance\nbetween different fine-tuning runs. Prior work has relied on English\nvalidation/development data to select among models that are fine-tuned with\ndifferent learning rates, number of steps and other hyperparameters, often\nresulting in suboptimal choices. To address this challenge, we propose a\nmeta-learning approach to model selection that uses the fine-tuned model's own\ninternal representations to predict its cross-lingual capabilities. In\nextensive experiments we find that our approach consistently selects better\nmodels than English validation data across five languages and five well-studied\nNLP tasks, achieving results that are comparable to small amounts of target\nlanguage development data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 02:36:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chen", "Yang", ""], ["Ritter", "Alan", ""]]}, {"id": "2010.06133", "submitter": "XiaoKang Liu", "authors": "Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang and\n  Yaohong Jin", "title": "BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth\n  Mover's Distance", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (e.g., BERT) have achieved significant success in\nvarious natural language processing (NLP) tasks. However, high storage and\ncomputational costs obstruct pre-trained language models to be effectively\ndeployed on resource-constrained devices. In this paper, we propose a novel\nBERT distillation method based on many-to-many layer mapping, which allows each\nintermediate student layer to learn from any intermediate teacher layers. In\nthis way, our model can learn from different teacher layers adaptively for\nvarious NLP tasks. %motivated by the intuition that different NLP tasks require\ndifferent levels of linguistic knowledge contained in the intermediate layers\nof BERT. In addition, we leverage Earth Mover's Distance (EMD) to compute the\nminimum cumulative cost that must be paid to transform knowledge from teacher\nnetwork to student network. EMD enables the effective matching for many-to-many\nlayer mapping. %EMD can be applied to network layers with different sizes and\neffectively measures semantic distance between the teacher network and student\nnetwork. Furthermore, we propose a cost attention mechanism to learn the layer\nweights used in EMD automatically, which is supposed to further improve the\nmodel's performance and accelerate convergence time. Extensive experiments on\nGLUE benchmark demonstrate that our model achieves competitive performance\ncompared to strong competitors in terms of both accuracy and model compression.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 02:53:52 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Li", "Jianquan", ""], ["Liu", "Xiaokang", ""], ["Zhao", "Honghong", ""], ["Xu", "Ruifeng", ""], ["Yang", "Min", ""], ["Jin", "Yaohong", ""]]}, {"id": "2010.06137", "submitter": "Farjana Sultana Mim", "authors": "Farjana Sultana Mim, Naoya Inoue, Paul Reisert, Hiroki Ouchi and\n  Kentaro Inui", "title": "Corruption Is Not All Bad: Incorporating Discourse Structure into\n  Pre-training via Corruption for Essay Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for automated essay scoring and document representation\nlearning typically rely on discourse parsers to incorporate discourse structure\ninto text representation. However, the performance of parsers is not always\nadequate, especially when they are used on noisy texts, such as student essays.\nIn this paper, we propose an unsupervised pre-training approach to capture\ndiscourse structure of essays in terms of coherence and cohesion that does not\nrequire any discourse parser or annotation. We introduce several types of\ntoken, sentence and paragraph-level corruption techniques for our proposed\npre-training approach and augment masked language modeling pre-training with\nour pre-training method to leverage both contextualized and discourse\ninformation. Our proposed unsupervised approach achieves new state-of-the-art\nresult on essay Organization scoring task.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:17:34 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mim", "Farjana Sultana", ""], ["Inoue", "Naoya", ""], ["Reisert", "Paul", ""], ["Ouchi", "Hiroki", ""], ["Inui", "Kentaro", ""]]}, {"id": "2010.06138", "submitter": "Junliang Guo", "authors": "Junliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei, Boxing Chen, Enhong\n  Chen", "title": "Incorporating BERT into Parallel Sequence Decoding with Adapters", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large scale pre-trained language models such as BERT have achieved\ngreat success on various natural language understanding tasks, how to\nefficiently and effectively incorporate them into sequence-to-sequence models\nand the corresponding text generation tasks remains a non-trivial problem. In\nthis paper, we propose to address this problem by taking two different BERT\nmodels as the encoder and decoder respectively, and fine-tuning them by\nintroducing simple and lightweight adapter modules, which are inserted between\nBERT layers and tuned on the task-specific dataset. In this way, we obtain a\nflexible and efficient model which is able to jointly leverage the information\ncontained in the source-side and target-side BERT models, while bypassing the\ncatastrophic forgetting problem. Each component in the framework can be\nconsidered as a plug-in unit, making the framework flexible and task agnostic.\nOur framework is based on a parallel sequence decoding algorithm named\nMask-Predict considering the bi-directional and conditional independent nature\nof BERT, and can be adapted to traditional autoregressive decoding easily. We\nconduct extensive experiments on neural machine translation tasks where the\nproposed method consistently outperforms autoregressive baselines while\nreducing the inference latency by half, and achieves $36.49$/$33.57$ BLEU\nscores on IWSLT14 German-English/WMT14 German-English translation. When adapted\nto autoregressive decoding, the proposed method achieves $30.60$/$43.56$ BLEU\nscores on WMT14 English-German/English-French translation, on par with the\nstate-of-the-art baseline models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:25:15 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Guo", "Junliang", ""], ["Zhang", "Zhirui", ""], ["Xu", "Linli", ""], ["Wei", "Hao-Ran", ""], ["Chen", "Boxing", ""], ["Chen", "Enhong", ""]]}, {"id": "2010.06150", "submitter": "Nan Ding", "authors": "Xi Chen, Nan Ding, Tomer Levinboim, Radu Soricut", "title": "Improving Text Generation Evaluation with Batch Centering and Tempered\n  Word Mover Distance", "comments": "EMNLP 2020 Eval4NLP Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in automatic evaluation metrics for text have shown that deep\ncontextualized word representations, such as those generated by BERT encoders,\nare helpful for designing metrics that correlate well with human judgements. At\nthe same time, it has been argued that contextualized word representations\nexhibit sub-optimal statistical properties for encoding the true similarity\nbetween words or sentences. In this paper, we present two techniques for\nimproving encoding representations for similarity metrics: a batch-mean\ncentering strategy that improves statistical properties; and a computationally\nefficient tempered Word Mover Distance, for better fusion of the information in\nthe contextualized word representations. We conduct numerical experiments that\ndemonstrate the robustness of our techniques, reporting results over various\nBERT-backbone learned metrics and achieving state of the art correlation with\nhuman ratings on several benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:46:25 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chen", "Xi", ""], ["Ding", "Nan", ""], ["Levinboim", "Tomer", ""], ["Soricut", "Radu", ""]]}, {"id": "2010.06185", "submitter": "Avishai Gretz", "authors": "Shai Gretz, Yonatan Bilu, Edo Cohen-Karlik and Noam Slonim", "title": "The workweek is the best time to start a family -- A Study of GPT-2\n  Based Claim Generation", "comments": "Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argument generation is a challenging task whose research is timely\nconsidering its potential impact on social media and the dissemination of\ninformation. Here we suggest a pipeline based on GPT-2 for generating coherent\nclaims, and explore the types of claims that it produces, and their veracity,\nusing an array of manual and automatic assessments. In addition, we explore the\ninterplay between this task and the task of Claim Retrieval, showing how they\ncan complement one another.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 05:22:30 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gretz", "Shai", ""], ["Bilu", "Yonatan", ""], ["Cohen-Karlik", "Edo", ""], ["Slonim", "Noam", ""]]}, {"id": "2010.06189", "submitter": "Zhengbao Jiang", "authors": "Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, Graham\n  Neubig", "title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained\n  Language Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) have proven surprisingly successful at capturing\nfactual knowledge by completing cloze-style fill-in-the-blank questions such as\n\"Punta Cana is located in _.\" However, while knowledge is both written and\nqueried in many languages, studies on LMs' factual representation ability have\nalmost invariably been performed on English. To assess factual knowledge\nretrieval in LMs in different languages, we create a multilingual benchmark of\ncloze-style probes for 23 typologically diverse languages. To properly handle\nlanguage variations, we expand probing methods from single- to multi-word\nentities, and develop several decoding algorithms to generate multi-token\npredictions. Extensive experimental results provide insights about how well (or\npoorly) current state-of-the-art LMs perform at this task in languages with\nmore or fewer available resources. We further propose a code-switching-based\nmethod to improve the ability of multilingual LMs to access knowledge, and\nverify its effectiveness on several benchmark languages. Benchmark data and\ncode have been released at https://x-factr.github.io.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 05:29:56 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 22:23:17 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 15:30:03 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Jiang", "Zhengbao", ""], ["Anastasopoulos", "Antonios", ""], ["Araki", "Jun", ""], ["Ding", "Haibo", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.06196", "submitter": "Zitao Liu", "authors": "Tianqiao Liu, Qian Fang, Wenbiao Ding, Zitao Liu", "title": "Mathematical Word Problem Generation from Commonsense Knowledge Graph\n  and Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in the use of automatic mathematical word\nproblem (MWP) generation in educational assessment. Different from standard\nnatural question generation, MWP generation needs to maintain the underlying\nmathematical operations between quantities and variables, while at the same\ntime ensuring the relevance between the output and the given topic. To address\nabove problem, we develop an end-to-end neural model to generate diverse MWPs\nin real-world scenarios from commonsense knowledge graph and equations. The\nproposed model (1) learns both representations from edge-enhanced Levi graphs\nof symbolic equations and commonsense knowledge; (2) automatically fuses\nequation and commonsense knowledge information via a self-planning module when\ngenerating the MWPs. Experiments on an educational gold-standard set and a\nlarge-scale generated MWP set show that our approach is superior on the MWP\ngeneration task, and it outperforms the state-of-the-art models in terms of\nboth automatic evaluation metrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human\nevaluation metrics, i.e., equation relevance, topic relevance, and language\ncoherence. To encourage reproducible results, we make our code and MWP dataset\npublic available at \\url{https://tinyurl.com/4ppldug7}.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 06:31:53 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 10:57:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liu", "Tianqiao", ""], ["Fang", "Qian", ""], ["Ding", "Wenbiao", ""], ["Liu", "Zitao", ""]]}, {"id": "2010.06203", "submitter": "Art\\=urs Stafanovi\\v{c}s", "authors": "Art\\=urs Stafanovi\\v{c}s, Toms Bergmanis, M\\=arcis Pinnis", "title": "Mitigating Gender Bias in Machine Translation with Target Gender\n  Annotations", "comments": "EMNLP 2020 Fifth Conference on Machine Translation (WMT20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When translating \"The secretary asked for details.\" to a language with\ngrammatical gender, it might be necessary to determine the gender of the\nsubject \"secretary\". If the sentence does not contain the necessary\ninformation, it is not always possible to disambiguate. In such cases, machine\ntranslation systems select the most common translation option, which often\ncorresponds to the stereotypical translations, thus potentially exacerbating\nprejudice and marginalisation of certain groups and people. We argue that the\ninformation necessary for an adequate translation can not always be deduced\nfrom the sentence being translated or even might depend on external knowledge.\nTherefore, in this work, we propose to decouple the task of acquiring the\nnecessary information from the task of learning to translate correctly when\nsuch information is available. To that end, we present a method for training\nmachine translation systems to use word-level annotations containing\ninformation about subject's gender. To prepare training data, we annotate\nregular source language words with grammatical gender information of the\ncorresponding target language words. Using such data to train machine\ntranslation systems reduces their reliance on gender stereotypes when\ninformation about the subject's gender is available. Our experiments on five\nlanguage pairs show that this allows improving accuracy on the WinoMT test set\nby up to 25.8 percentage points.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 07:07:59 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 16:41:56 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Stafanovi\u010ds", "Art\u016brs", ""], ["Bergmanis", "Toms", ""], ["Pinnis", "M\u0101rcis", ""]]}, {"id": "2010.06213", "submitter": "Maxime Peyrard", "authors": "Maxime Peyrard, Robert West", "title": "KLearn: Background Knowledge Inference from Summarization Data", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of text summarization is to compress documents to the relevant\ninformation while excluding background information already known to the\nreceiver. So far, summarization researchers have given considerably more\nattention to relevance than to background knowledge. In contrast, this work\nputs background knowledge in the foreground. Building on the realization that\nthe choices made by human summarizers and annotators contain implicit\ninformation about their background knowledge, we develop and compare techniques\nfor inferring background knowledge from summarization data. Based on this\nframework, we define summary scoring functions that explicitly model background\nknowledge, and show that these scoring functions fit human judgments\nsignificantly better than baselines. We illustrate some of the many potential\napplications of our framework. First, we provide insights into human\ninformation importance priors. Second, we demonstrate that averaging the\nbackground knowledge of multiple, potentially biased annotators or corpora\ngreatly improves summary-scoring performance. Finally, we discuss potential\napplications of our framework beyond summarization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 07:42:25 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Peyrard", "Maxime", ""], ["West", "Robert", ""]]}, {"id": "2010.06251", "submitter": "Mariana Neves", "authors": "Mariana Neves and Jurica Seva", "title": "Annotationsaurus: A Searchable Directory of Annotation Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual annotation of textual documents is a necessary task when constructing\nbenchmark corpora for training and evaluating machine learning algorithms. We\ncreated a comprehensive directory of annotation tools that currently includes\n93 tools. We analyzed the tools over a set of 31 features and implemented\nsimple scripts and a Web application that filters the tools based on chosen\ncriteria. We present two use cases using the directory and propose ideas for\nits maintenance. The directory, source codes for scripts, and link to the Web\napplication are available at: https://github.com/mariananeves/annotation-tools\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:22:48 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 06:41:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Neves", "Mariana", ""], ["Seva", "Jurica", ""]]}, {"id": "2010.06253", "submitter": "Peng Cui", "authors": "Peng Cui, Le Hu, and Yuanchao Liu", "title": "Enhancing Extractive Text Summarization with Topic-Aware Graph Neural\n  Networks", "comments": "Accepted by COLING(2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text summarization aims to compress a textual document to a short summary\nwhile keeping salient information. Extractive approaches are widely used in\ntext summarization because of their fluency and efficiency. However, most of\nexisting extractive models hardly capture inter-sentence relationships,\nparticularly in long documents. They also often ignore the effect of topical\ninformation on capturing important contents. To address these issues, this\npaper proposes a graph neural network (GNN)-based extractive summarization\nmodel, enabling to capture inter-sentence relationships efficiently via\ngraph-structured document representation. Moreover, our model integrates a\njoint neural topic model (NTM) to discover latent topics, which can provide\ndocument-level features for sentence selection. The experimental results\ndemonstrate that our model not only substantially achieves state-of-the-art\nresults on CNN/DM and NYT datasets but also considerably outperforms existing\napproaches on scientific paper datasets consisting of much longer documents,\nindicating its better robustness in document genres and lengths. Further\ndiscussions show that topical information can help the model preselect salient\ncontents from an entire document, which interprets its effectiveness in long\ndocument summarization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:30:04 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Cui", "Peng", ""], ["Hu", "Le", ""], ["Liu", "Yuanchao", ""]]}, {"id": "2010.06269", "submitter": "Tharindu Ranasinghe Mr", "authors": "Hansi Hettiarachchi, Tharindu Ranasinghe", "title": "BRUMS at SemEval-2020 Task 3: Contextualised Embeddings for Predicting\n  the (Graded) Effect of Context in Word Similarity", "comments": "Accepted to SemEval-2020 (International Workshop on Semantic\n  Evaluation) at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the team BRUMS submission to SemEval-2020 Task 3: Graded\nWord Similarity in Context. The system utilises state-of-the-art contextualised\nword embeddings, which have some task-specific adaptations, including stacked\nembeddings and average embeddings. Overall, the approach achieves good\nevaluation scores across all the languages, while maintaining simplicity.\nFollowing the final rankings, our approach is ranked within the top 5 solutions\nof each language while preserving the 1st position of Finnish subtask 2.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:25:18 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 14:44:16 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hettiarachchi", "Hansi", ""], ["Ranasinghe", "Tharindu", ""]]}, {"id": "2010.06278", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Hansi Hettiarachchi", "title": "BRUMS at SemEval-2020 Task 12 : Transformer based Multilingual Offensive\n  Language Identification in Social Media", "comments": "Accepted to SemEval-2020 (International Workshop on Semantic\n  Evaluation) at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe the team \\textit{BRUMS} entry to OffensEval 2:\nMultilingual Offensive Language Identification in Social Media in SemEval-2020.\nThe OffensEval organizers provided participants with annotated datasets\ncontaining posts from social media in Arabic, Danish, English, Greek and\nTurkish. We present a multilingual deep learning model to identify offensive\nlanguage in social media. Overall, the approach achieves acceptable evaluation\nscores, while maintaining flexibility between languages.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:39:14 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Hettiarachchi", "Hansi", ""]]}, {"id": "2010.06281", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Alistair Plum, Constantin Orasan, Ruslan Mitkov", "title": "RGCL at SemEval-2020 Task 6: Neural Approaches to Definition Extraction", "comments": "Accepted to SemEval-2020 (International Workshop on Semantic\n  Evaluation) at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the RGCL team submission to SemEval 2020 Task 6:\nDeftEval, subtasks 1 and 2. The system classifies definitions at the sentence\nand token levels. It utilises state-of-the-art neural network architectures,\nwhich have some task-specific adaptations, including an automatically extended\ntraining set. Overall, the approach achieves acceptable evaluation scores,\nwhile maintaining flexibility in architecture selection.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:48:15 GMT"}], "update_date": "2020-11-22", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Plum", "Alistair", ""], ["Orasan", "Constantin", ""], ["Mitkov", "Ruslan", ""]]}, {"id": "2010.06283", "submitter": "Hendrik Schuff", "authors": "Hendrik Schuff, Heike Adel, Ngoc Thang Vu", "title": "F1 is Not Enough! Models and Evaluation Towards User-Centered\n  Explainable Question Answering", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable question answering systems predict an answer together with an\nexplanation showing why the answer has been selected. The goal is to enable\nusers to assess the correctness of the system and understand its reasoning\nprocess. However, we show that current models and evaluation settings have\nshortcomings regarding the coupling of answer and explanation which might cause\nserious issues in user experience. As a remedy, we propose a hierarchical model\nand a new regularization term to strengthen the answer-explanation coupling as\nwell as two evaluation scores to quantify the coupling. We conduct experiments\non the HOTPOTQA benchmark data set and perform a user study. The user study\nshows that our models increase the ability of the users to judge the\ncorrectness of the system and that scores like F1 are not enough to estimate\nthe usefulness of a model in a practical setting with human users. Our scores\nare better aligned with user experience, making them promising candidates for\nmodel selection.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:53:20 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Schuff", "Hendrik", ""], ["Adel", "Heike", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2010.06294", "submitter": "Li Liang", "authors": "Li Liang, Zheng Zhao and Bonnie Webber", "title": "Extending Implicit Discourse Relation Recognition to the PDTB-3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PDTB-3 contains many more Implicit discourse relations than the previous\nPDTB-2. This is in part because implicit relations have now been annotated\nwithin sentences as well as between them. In addition, some now co-occur with\nexplicit discourse relations, instead of standing on their own. Here we show\nthat while this can complicate the problem of identifying the location of\nimplicit discourse relations, it can in turn simplify the problem of\nidentifying their senses. We present data to support this claim, as well as\nmethods that can serve as a non-trivial baseline for future state-of-the-art\nrecognizers for implicit discourse relations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:19:42 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liang", "Li", ""], ["Zhao", "Zheng", ""], ["Webber", "Bonnie", ""]]}, {"id": "2010.06310", "submitter": "Yue Wang", "authors": "Yue Wang, Zhuo Xu, Lu Bai, Yao Wan, Lixin Cui, Qian Zhao, Edwin R.\n  Hancock, Philip S. Yu", "title": "Cross-Supervised Joint-Event-Extraction with Heterogeneous Information\n  Networks", "comments": "Accepted by ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint-event-extraction, which extracts structural information (i.e., entities\nor triggers of events) from unstructured real-world corpora, has attracted more\nand more research attention in natural language processing. Most existing works\ndo not fully address the sparse co-occurrence relationships between entities\nand triggers, which loses this important information and thus deteriorates the\nextraction performance. To mitigate this issue, we first define the\njoint-event-extraction as a sequence-to-sequence labeling task with a tag set\ncomposed of tags of triggers and entities. Then, to incorporate the missing\ninformation in the aforementioned co-occurrence relationships, we propose a\nCross-Supervised Mechanism (CSM) to alternately supervise the extraction of\neither triggers or entities based on the type distribution of each other.\nMoreover, since the connected entities and triggers naturally form a\nheterogeneous information network (HIN), we leverage the latent pattern along\nmeta-paths for a given corpus to further improve the performance of our\nproposed method. To verify the effectiveness of our proposed method, we conduct\nextensive experiments on four real-world datasets as well as compare our method\nwith state-of-the-art methods. Empirical results and analysis show that our\napproach outperforms the state-of-the-art methods in both entity and trigger\nextraction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:51:17 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:11:09 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wang", "Yue", ""], ["Xu", "Zhuo", ""], ["Bai", "Lu", ""], ["Wan", "Yao", ""], ["Cui", "Lixin", ""], ["Zhao", "Qian", ""], ["Hancock", "Edwin R.", ""], ["Yu", "Philip S.", ""]]}, {"id": "2010.06325", "submitter": "Elena V. Epure", "authors": "Elena V. Epure and Guillaume Salha and Manuel Moussallam and Romain\n  Hennequin", "title": "Modeling the Music Genre Perception across Language-Bound Cultures", "comments": "2020 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The music genre perception expressed through human annotations of artists or\nalbums varies significantly across language-bound cultures. These variations\ncannot be modeled as mere translations since we also need to account for\ncultural differences in the music genre perception. In this work, we study the\nfeasibility of obtaining relevant cross-lingual, culture-specific music genre\nannotations based only on language-specific semantic representations, namely\ndistributed concept embeddings and ontologies. Our study, focused on six\nlanguages, shows that unsupervised cross-lingual music genre annotation is\nfeasible with high accuracy, especially when combining both types of\nrepresentations. This approach of studying music genres is the most extensive\nto date and has many implications in musicology and music information\nretrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus\nto benchmark state of the art multilingual pre-trained embedding models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:20:32 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 11:43:50 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Epure", "Elena V.", ""], ["Salha", "Guillaume", ""], ["Moussallam", "Manuel", ""], ["Hennequin", "Romain", ""]]}, {"id": "2010.06351", "submitter": "Fuli Luo", "authors": "Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun", "title": "CAPT: Contrastive Pre-Training for Learning Denoised Sequence\n  Representations", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained self-supervised models such as BERT have achieved striking\nsuccess in learning sequence representations, especially for natural language\nprocessing. These models typically corrupt the given sequences with certain\ntypes of noise, such as masking, shuffling, or substitution, and then try to\nrecover the original input. However, such pre-training approaches are prone to\nlearning representations that are covariant with the noise, leading to the\ndiscrepancy between the pre-training and fine-tuning stage. To remedy this, we\npresent ContrAstive Pre-Training (CAPT) to learn noise invariant sequence\nrepresentations. The proposed CAPT encourages the consistency between\nrepresentations of the original sequence and its corrupted version via\nunsupervised instance-wise training signals. In this way, it not only\nalleviates the pretrain-finetune discrepancy induced by the noise of\npre-training, but also aids the pre-trained model in better capturing global\nsemantics of the input via more effective sentence-level supervision. Different\nfrom most prior work that focuses on a particular modality, comprehensive\nempirical evidence on 11 natural language understanding and cross-modal tasks\nillustrates that CAPT is applicable for both language and vision-language\ntasks, and obtains surprisingly consistent improvement, including 0.6\\%\nabsolute gain on GLUE benchmarks and 0.8\\% absolute increment on\n$\\text{NLVR}^2$.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:08:34 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 09:30:12 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 06:41:07 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 03:47:06 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Luo", "Fuli", ""], ["Yang", "Pengcheng", ""], ["Li", "Shicheng", ""], ["Ren", "Xuancheng", ""], ["Sun", "Xu", ""]]}, {"id": "2010.06354", "submitter": "J\\\"org Tiedemann", "authors": "J\\\"org Tiedemann", "title": "The Tatoeba Translation Challenge -- Realistic Data Sets for Low\n  Resource and Multilingual MT", "comments": "to be appear at the 5th Conference on Machine Translation (WMT20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the development of a new benchmark for machine\ntranslation that provides training and test data for thousands of language\npairs covering over 500 languages and tools for creating state-of-the-art\ntranslation models from that collection. The main goal is to trigger the\ndevelopment of open translation tools and models with a much broader coverage\nof the World's languages. Using the package it is possible to work on realistic\nlow-resource scenarios avoiding artificially reduced setups that are common\nwhen demonstrating zero-shot or few-shot learning. For the first time, this\npackage provides a comprehensive collection of diverse data sets in hundreds of\nlanguages with systematic language and script annotation and data splits to\nextend the narrow coverage of existing benchmarks. Together with the data\nrelease, we also provide a growing number of pre-trained baseline models for\nindividual language pairs and selected language groups.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:12:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Tiedemann", "J\u00f6rg", ""]]}, {"id": "2010.06359", "submitter": "Eleftherios Avramidis", "authors": "Eleftherios Avramidis, Vivien Macketanz, Ursula Strohriegel, Aljoscha\n  Burchardt and Sebastian M\\\"oller", "title": "Fine-grained linguistic evaluation for state-of-the-art Machine\n  Translation", "comments": "11 pages, 1 figure, Fifth Conference of Machine Translation, WMT20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a test suite submission providing detailed statistics of\nlinguistic performance for the state-of-the-art German-English systems of the\nFifth Conference of Machine Translation (WMT20). The analysis covers 107\nphenomena organized in 14 categories based on about 5,500 test items, including\na manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan)\nappear to have significantly better test suite accuracy than the others,\nalthough the best system of WMT20 is not significantly better than the one from\nWMT19 in a macro-average. Additionally, we identify some linguistic phenomena\nwhere all systems suffer (such as idioms, resultative predicates and\npluperfect), but we are also able to identify particular weaknesses for\nindividual systems (such as quotation marks, lexical ambiguity and sluicing).\nMost of the systems of WMT19 which submitted new versions this year show\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:14:37 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 19:50:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Avramidis", "Eleftherios", ""], ["Macketanz", "Vivien", ""], ["Strohriegel", "Ursula", ""], ["Burchardt", "Aljoscha", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2010.06395", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm", "title": "Aspect-based Document Similarity for Research Papers", "comments": "Accepted for publication at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional document similarity measures provide a coarse-grained distinction\nbetween similar and dissimilar documents. Typically, they do not consider in\nwhat aspects two documents are similar. This limits the granularity of\napplications like recommender systems that rely on document similarity. In this\npaper, we extend similarity with aspect information by performing a pairwise\ndocument classification task. We evaluate our aspect-based document similarity\nfor research papers. Paper citations indicate the aspect-based similarity,\ni.e., the section title in which a citation occurs acts as a label for the pair\nof citing and cited paper. We apply a series of Transformer models such as\nRoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM\nbaseline. We perform our experiments on two newly constructed datasets of\n172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. Our\nresults show SciBERT as the best performing system. A qualitative examination\nvalidates our quantitative results. Our findings motivate future research of\naspect-based document similarity and the development of a recommender system\nbased on the evaluated techniques. We make our datasets, code, and trained\nmodels publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:51:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ostendorff", "Malte", ""], ["Ruas", "Terry", ""], ["Blume", "Till", ""], ["Gipp", "Bela", ""], ["Rehm", "Georg", ""]]}, {"id": "2010.06396", "submitter": "Ekta Sood", "authors": "Ekta Sood, Simon Tannert, Diego Frassinelli, Andreas Bulling and Ngoc\n  Thang Vu", "title": "Interpreting Attention Models with Human Visual Attention in Machine\n  Reading Comprehension", "comments": "CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks with attention mechanisms have achieved superior\nperformance on many natural language processing tasks, it remains unclear to\nwhich extent learned attention resembles human visual attention. In this paper,\nwe propose a new method that leverages eye-tracking data to investigate the\nrelationship between human visual attention and neural attention in machine\nreading comprehension. To this end, we introduce a novel 23 participant eye\ntracking dataset - MQA-RC, in which participants read movie plots and answered\npre-defined questions. We compare state of the art networks based on long\nshort-term memory (LSTM), convolutional neural models (CNN) and XLNet\nTransformer architectures. We find that higher similarity to human attention\nand performance significantly correlates to the LSTM and CNN models. However,\nwe show this relationship does not hold true for the XLNet models -- despite\nthe fact that the XLNet performs best on this challenging task. Our results\nsuggest that different architectures seem to learn rather different neural\nattention strategies and similarity of neural to human attention does not\nguarantee best performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:51:57 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 14:47:51 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sood", "Ekta", ""], ["Tannert", "Simon", ""], ["Frassinelli", "Diego", ""], ["Bulling", "Andreas", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2010.06432", "submitter": "Matan Orbach", "authors": "Orith Toledo-Ronen, Matan Orbach, Yonatan Bilu, Artem Spector, Noam\n  Slonim", "title": "Multilingual Argument Mining: Datasets and Analysis", "comments": "Accepted to Findings of EMNLP 2020 (Long Paper). For the associated\n  multilingual arguments and evidence corpus, see\n  https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Multilingual%20Argument%20Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing interest in argument mining and computational argumentation\nbrings with it a plethora of Natural Language Understanding (NLU) tasks and\ncorresponding datasets. However, as with many other NLU tasks, the dominant\nlanguage is English, with resources in other languages being few and far\nbetween. In this work, we explore the potential of transfer learning using the\nmultilingual BERT model to address argument mining tasks in non-English\nlanguages, based on English datasets and the use of machine translation. We\nshow that such methods are well suited for classifying the stance of arguments\nand detecting evidence, but less so for assessing the quality of arguments,\npresumably because quality is harder to preserve under translation. In\naddition, focusing on the translate-train approach, we show how the choice of\nlanguages for translation, and the relations among them, affect the accuracy of\nthe resultant model. Finally, to facilitate evaluation of transfer learning on\nargument mining tasks, we provide a human-generated dataset with more than 10k\narguments in multiple languages, as well as machine translation of the English\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:49:10 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Toledo-Ronen", "Orith", ""], ["Orbach", "Matan", ""], ["Bilu", "Yonatan", ""], ["Spector", "Artem", ""], ["Slonim", "Noam", ""]]}, {"id": "2010.06436", "submitter": "Andrey Kutuzov", "authors": "Julia Rodina, Andrey Kutuzov", "title": "RuSemShift: a dataset of historical lexical semantic change in Russian", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RuSemShift, a large-scale manually annotated test set for the task\nof semantic change modeling in Russian for two long-term time period pairs:\nfrom the pre-Soviet through the Soviet times and from the Soviet through the\npost-Soviet times. Target words were annotated by multiple crowd-source\nworkers. The annotation process was organized following the DURel framework and\nwas based on sentence contexts extracted from the Russian National Corpus.\nAdditionally, we report the performance of several distributional approaches on\nRuSemShift, achieving promising results, which at the same time leave room for\nother researchers to improve.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:54:05 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Rodina", "Julia", ""], ["Kutuzov", "Andrey", ""]]}, {"id": "2010.06444", "submitter": "Thiago H. Silva", "authors": "Frances Santos, Thiago H Silva, Antonio A F Loureiro, Leandro Villas", "title": "Automatic Extraction of Urban Outdoor Perception from Geolocated\n  Free-Texts", "comments": "Paper accepted - to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic extraction of urban perception shared by people on\nlocation-based social networks (LBSNs) is an important multidisciplinary\nresearch goal. One of the reasons is because it facilitates the understanding\nof the intrinsic characteristics of urban areas in a scalable way, helping to\nleverage new services. However, content shared on LBSNs is diverse,\nencompassing several topics, such as politics, sports, culture, religion, and\nurban perceptions, making the task of content extraction regarding a particular\ntopic very challenging. Considering free-text messages shared on LBSNs, we\npropose an automatic and generic approach to extract people's perceptions. For\nthat, our approach explores opinions that are spatial-temporal and semantically\nsimilar. We exemplify our approach in the context of urban outdoor areas in\nChicago, New York City and London. Studying those areas, we found evidence that\nLBSN data brings valuable information about urban regions. To analyze and\nvalidate our outcomes, we conducted a temporal analysis to measure the results'\nrobustness over time. We show that our approach can be helpful to better\nunderstand urban areas considering different perspectives. We also conducted a\ncomparative analysis based on a public dataset, which contains volunteers'\nperceptions regarding urban areas expressed in a controlled experiment. We\nobserve that both results yield a very similar level of agreement.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:59:46 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Santos", "Frances", ""], ["Silva", "Thiago H", ""], ["Loureiro", "Antonio A F", ""], ["Villas", "Leandro", ""]]}, {"id": "2010.06447", "submitter": "Dan John Velasco", "authors": "Dan John Velasco", "title": "Pagsusuri ng RNN-based Transfer Learning Technique sa Low-Resource\n  Language", "comments": "5 pages, 3 tables, 1 figure. in Filipino language; typos corrected,\n  rephrased sentences, thoughts and results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resource languages such as Filipino suffer from data scarcity which makes\nit challenging to develop NLP applications for Filipino language. The use of\nTransfer Learning (TL) techniques alleviates this problem in low-resource\nsetting. In recent years, transformer-based models are proven to be effective\nin low-resource tasks but faces challenges in accessibility due to its high\ncompute and memory requirements. For this reason, there's a need for a cheaper\nbut effective alternative. This paper has three contributions. First, release a\npre-trained AWD-LSTM language model for Filipino language. Second, benchmark\nAWD-LSTM in the Hate Speech classification task and show that it performs on\npar with transformer-based models. Third, analyze the the performance of\nAWD-LSTM in low-resource setting using degradation test and compare it with\ntransformer-based models.\n  -----\n  Ang mga low-resource languages tulad ng Filipino ay gipit sa accessible na\ndatos kaya't mahirap gumawa ng mga applications sa wikang ito. Ang mga Transfer\nLearning (TL) techniques ay malaking tulong para sa low-resource setting o mga\npagkakataong gipit sa datos. Sa mga nagdaang taon, nanaig ang mga\ntransformer-based TL techniques pagdating sa low-resource tasks ngunit ito ay\nmataas na compute and memory requirements kaya nangangailangan ng mas mura pero\nepektibong alternatibo. Ang papel na ito ay may tatlong kontribusyon. Una,\nmaglabas ng pre-trained AWD-LSTM language model sa wikang Filipino upang maging\ntuntungan sa pagbuo ng mga NLP applications sa wikang Filipino. Pangalawa, mag\nbenchmark ng AWD-LSTM sa Hate Speech classification task at ipakita na kayang\nnitong makipagsabayan sa mga transformer-based models. Pangatlo, suriin ang\nperformance ng AWD-LSTM sa low-resource setting gamit ang degradation test at\nikumpara ito sa mga transformer-based models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:06:07 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:50:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Velasco", "Dan John", ""]]}, {"id": "2010.06467", "submitter": "Jimmy Lin", "authors": "Jimmy Lin, Rodrigo Nogueira, and Andrew Yates", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:20:32 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 13:35:29 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lin", "Jimmy", ""], ["Nogueira", "Rodrigo", ""], ["Yates", "Andrew", ""]]}, {"id": "2010.06472", "submitter": "Aaron Mueller", "authors": "Aaron Mueller, Zach Wood-Doughty, Silvio Amir, Mark Dredze, Alicia L.\n  Nobles", "title": "Demographic Representation and Collective Storytelling in the Me Too\n  Twitter Hashtag Activism Movement", "comments": "27 pages (incl. 5 for references). Submitted to CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The #MeToo movement on Twitter has drawn attention to the pervasive nature of\nsexual harassment and violence. While #MeToo has been praised for providing\nsupport for self-disclosures of harassment or violence and shifting societal\nresponse, it has also been criticized for exemplifying how women of color have\nbeen discounted for their historical contributions to and excluded from\nfeminist movements. Through an analysis of over 600,000 tweets from over\n256,000 unique users, we examine online #MeToo conversations across gender and\nracial/ethnic identities and the topics that each demographic emphasized. We\nfound that tweets authored by white women were overrepresented in the movement\ncompared to other demographics, aligning with criticism of unequal\nrepresentation. We found that intersected identities contributed differing\nnarratives to frame the movement, co-opted the movement to raise visibility in\nparallel ongoing movements, employed the same hashtags both critically and\nsupportively, and revived and created new hashtags in response to pivotal\nmoments. Notably, tweets authored by black women often expressed emotional\nsupport and were critical about differential treatment in the justice system\nand by police. In comparison, tweets authored by white women and men often\nhighlighted sexual harassment and violence by public figures and weaved in more\ngeneral political discussions. We discuss the implications of work for digital\nactivism research and design including suggestions to raise visibility by those\nwho were under-represented in this hashtag activism movement. Content warning:\nthis article discusses issues of sexual harassment and violence.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:25:33 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Mueller", "Aaron", ""], ["Wood-Doughty", "Zach", ""], ["Amir", "Silvio", ""], ["Dredze", "Mark", ""], ["Nobles", "Alicia L.", ""]]}, {"id": "2010.06478", "submitter": "Tommaso Pasini", "authors": "Alessandro Raganato, Tommaso Pasini, Jose Camacho-Collados, Mohammad\n  Taher Pilehvar", "title": "XL-WiC: A Multilingual Benchmark for Evaluating Semantic\n  Contextualization", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to correctly model distinct meanings of a word is crucial for the\neffectiveness of semantic representation techniques. However, most existing\nevaluation benchmarks for assessing this criterion are tied to sense\ninventories (usually WordNet), restricting their usage to a small subset of\nknowledge-based representation techniques. The Word-in-Context dataset (WiC)\naddresses the dependence on sense inventories by reformulating the standard\ndisambiguation task as a binary classification problem; but, it is limited to\nthe English language. We put forward a large multilingual benchmark, XL-WiC,\nfeaturing gold standards in 12 new languages from varied language families and\nwith different degrees of resource availability, opening room for evaluation\nscenarios such as zero-shot cross-lingual transfer. We perform a series of\nexperiments to determine the reliability of the datasets and to set performance\nbaselines for several recent contextualized multilingual models. Experimental\nresults show that even when no tagged instances are available for a target\nlanguage, models trained solely on the English data can attain competitive\nperformance in the task of distinguishing different meanings of a word, even\nfor distant languages. XL-WiC is available at\nhttps://pilehvar.github.io/xlwic/.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:32:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Raganato", "Alessandro", ""], ["Pasini", "Tommaso", ""], ["Camacho-Collados", "Jose", ""], ["Pilehvar", "Mohammad Taher", ""]]}, {"id": "2010.06521", "submitter": "Michael Kruse", "authors": "Michael Kruse, Hal Finkel, Xingfu Wu", "title": "Autotuning Search Space for Loop Transformations", "comments": "LLVM-in-HPC 2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges for optimizing compilers is to predict whether applying\nan optimization will improve its execution speed. Programmers may override the\ncompiler's profitability heuristic using optimization directives such as\npragmas in the source code. Machine learning in the form of autotuning can\nassist users in finding the best optimizations for each platform.\n  In this paper we propose a loop transformation search space that takes the\nform of a tree, in contrast to previous approaches that usually use vector\nspaces to represent loop optimization configurations. We implemented a simple\nautotuner exploring the search space and applied it to a selected set of\nPolyBench kernels. While the autotuner is capable of representing every\npossible sequence of loop transformations and their relations, the results\nmotivate the use of better search strategies such as Monte Carlo tree search to\nfind sophisticated loop transformations such as multilevel tiling.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 16:26:57 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kruse", "Michael", ""], ["Finkel", "Hal", ""], ["Wu", "Xingfu", ""]]}, {"id": "2010.06549", "submitter": "Ghazi Felhi", "authors": "Ghazi Felhi, Joseph Leroux, Djam\\'e Seddah", "title": "Controlling the Interaction Between Generation and Inference in\n  Semi-Supervised Variational Autoencoders Using Importance Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though Variational Autoencoders (VAEs) are widely used for\nsemi-supervised learning, the reason why they work remains unclear. In fact,\nthe addition of the unsupervised objective is most often vaguely described as a\nregularization. The strength of this regularization is controlled by\ndown-weighting the objective on the unlabeled part of the training set. Through\nan analysis of the objective of semi-supervised VAEs, we observe that they use\nthe posterior of the learned generative model to guide the inference model in\nlearning the partially observed latent variable. We show that given this\nobservation, it is possible to gain finer control on the effect of the\nunsupervised objective on the training procedure. Using importance weighting,\nwe derive two novel objectives that prioritize either one of the partially\nobserved latent variable, or the unobserved latent variable. Experiments on the\nIMDB english sentiment analysis dataset and on the AG News topic classification\ndataset show the improvements brought by our prioritization mechanism and\nexhibit a behavior that is inline with our description of the inner working of\nSemi-Supervised VAEs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:01:40 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 13:04:47 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Felhi", "Ghazi", ""], ["Leroux", "Joseph", ""], ["Seddah", "Djam\u00e9", ""]]}, {"id": "2010.06572", "submitter": "Jack Hessel", "authors": "Jack Hessel and Lillian Lee", "title": "Does my multimodal model learn cross-modal interactions? It's harder to\n  tell than you might think!", "comments": null, "journal-ref": "Published in EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling expressive cross-modal interactions seems crucial in multimodal\ntasks, such as visual question answering. However, sometimes high-performing\nblack-box algorithms turn out to be mostly exploiting unimodal signals in the\ndata. We propose a new diagnostic tool, empirical multimodally-additive\nfunction projection (EMAP), for isolating whether or not cross-modal\ninteractions improve performance for a given model on a given task. This\nfunction projection modifies model predictions so that cross-modal interactions\nare eliminated, isolating the additive, unimodal structure. For seven\nimage+text classification tasks (on each of which we set new state-of-the-art\nbenchmarks), we find that, in many cases, removing cross-modal interactions\nresults in little to no performance degradation. Surprisingly, this holds even\nwhen expressive models, with capacity to consider interactions, otherwise\noutperform less expressive models; thus, performance improvements, even when\npresent, often cannot be attributed to consideration of cross-modal feature\ninteractions. We hence recommend that researchers in multimodal machine\nlearning report the performance not only of unimodal baselines, but also the\nEMAP of their best-performing model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:45:28 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hessel", "Jack", ""], ["Lee", "Lillian", ""]]}, {"id": "2010.06579", "submitter": "Jekaterina Novikova Dr.", "authors": "Benjamin Eyre, Aparna Balagopalan, Jekaterina Novikova", "title": "Fantastic Features and Where to Find Them: Detecting Cognitive\n  Impairment with a Subsequence Classification Guided Approach", "comments": "EMNLP Workshop on Noisy User-generated Text (W-NUT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widely reported success of embedding-based machine learning\nmethods on natural language processing tasks, the use of more easily\ninterpreted engineered features remains common in fields such as cognitive\nimpairment (CI) detection. Manually engineering features from noisy text is\ntime and resource consuming, and can potentially result in features that do not\nenhance model performance. To combat this, we describe a new approach to\nfeature engineering that leverages sequential machine learning models and\ndomain knowledge to predict which features help enhance performance. We provide\na concrete example of this method on a standard data set of CI speech and\ndemonstrate that CI classification accuracy improves by 2.3% over a strong\nbaseline when using features produced by this method. This demonstration\nprovides an ex-ample of how this method can be used to assist classification in\nfields where interpretability is important, such as health care.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:57:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Eyre", "Benjamin", ""], ["Balagopalan", "Aparna", ""], ["Novikova", "Jekaterina", ""]]}, {"id": "2010.06595", "submitter": "Dallas Card", "authors": "Dallas Card and Peter Henderson and Urvashi Khandelwal and Robin Jia\n  and Kyle Mahowald and Dan Jurafsky", "title": "With Little Power Comes Great Responsibility", "comments": "To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its importance to experimental design, statistical power (the\nprobability that, given a real effect, an experiment will reject the null\nhypothesis) has largely been ignored by the NLP community. Underpowered\nexperiments make it more difficult to discern the difference between\nstatistical noise and meaningful model improvements, and increase the chances\nof exaggerated findings. By meta-analyzing a set of existing NLP papers and\ndatasets, we characterize typical power for a variety of settings and conclude\nthat underpowered experiments are common in the NLP literature. In particular,\nfor several tasks in the popular GLUE benchmark, small test sets mean that most\nattempted comparisons to state of the art models will not be adequately\npowered. Similarly, based on reasonable assumptions, we find that the most\ntypical experimental design for human rating studies will be underpowered to\ndetect small model differences, of the sort that are frequently studied. For\nmachine translation, we find that typical test sets of 2000 sentences have\napproximately 75% power to detect differences of 1 BLEU point. To improve the\nsituation going forward, we give an overview of best practices for power\nanalysis in NLP and release a series of notebooks to assist with future power\nanalyses.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 18:00:02 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Card", "Dallas", ""], ["Henderson", "Peter", ""], ["Khandelwal", "Urvashi", ""], ["Jia", "Robin", ""], ["Mahowald", "Kyle", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2010.06640", "submitter": "Gathika Ratnayaka", "authors": "Gathika Ratnayaka, Thushari Atapattu, Mahen Herath, Georgia Zhang,\n  Katrina Falkner", "title": "Enhancing the Identification of Cyberbullying through Participant Roles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying is a prevalent social problem that inflicts detrimental\nconsequences to the health and safety of victims such as psychological\ndistress, anti-social behaviour, and suicide. The automation of cyberbullying\ndetection is a recent but widely researched problem, with current research\nhaving a strong focus on a binary classification of bullying versus\nnon-bullying. This paper proposes a novel approach to enhancing cyberbullying\ndetection through role modeling. We utilise a dataset from ASKfm to perform\nmulti-class classification to detect participant roles (e.g. victim, harasser).\nOur preliminary results demonstrate promising performance including 0.83 and\n0.76 of F1-score for cyberbullying and role classification respectively,\noutperforming baselines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:13:07 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 01:15:20 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ratnayaka", "Gathika", ""], ["Atapattu", "Thushari", ""], ["Herath", "Mahen", ""], ["Zhang", "Georgia", ""], ["Falkner", "Katrina", ""]]}, {"id": "2010.06657", "submitter": "Hancheng Cao", "authors": "Hancheng Cao, Mengjie Cheng, Zhepeng Cen, Daniel A. McFarland, Xiang\n  Ren", "title": "Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer\n  of Scientific Concepts across Text Corpora", "comments": "EMNLP 2020 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What kind of basic research ideas are more likely to get applied in practice?\nThere is a long line of research investigating patterns of knowledge transfer,\nbut it generally focuses on documents as the unit of analysis and follow their\ntransfer into practice for a specific scientific domain. Here we study\ntranslational research at the level of scientific concepts for all scientific\nfields. We do this through text mining and predictive modeling using three\ncorpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28\nmillion clinical trials. We extract scientific concepts (i.e., phrases) from\ncorpora as instantiations of \"research ideas\", create concept-level features as\nmotivated by literature, and then follow the trajectories of over 450,000 new\nconcepts (emerged from 1995-2014) to identify factors that lead only a small\nproportion of these ideas to be used in inventions and drug trials. Results\nfrom our analysis suggest several mechanisms that distinguish which scientific\nconcept will be adopted in practice, and which will not. We also demonstrate\nthat our derived features can be used to explain and predict knowledge transfer\nwith high accuracy. Our work provides greater understanding of knowledge\ntransfer for researchers, practitioners, and government agencies interested in\nencouraging translational research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:46:59 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Cao", "Hancheng", ""], ["Cheng", "Mengjie", ""], ["Cen", "Zhepeng", ""], ["McFarland", "Daniel A.", ""], ["Ren", "Xiang", ""]]}, {"id": "2010.06666", "submitter": "Devin Johnson", "authors": "Devin Johnson, Denise Mak, Drew Barker, Lexi Loessberg-Zahl", "title": "Probing for Multilingual Numerical Understanding in Transformer-Based\n  Language Models", "comments": "BlackboxNLP (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Natural language numbers are an example of compositional structures, where\nlarger numbers are composed of operations on smaller numbers. Given that\ncompositional reasoning is a key to natural language understanding, we propose\nnovel multilingual probing tasks tested on DistilBERT, XLM, and BERT to\ninvestigate for evidence of compositional reasoning over numerical data in\nvarious natural language number systems. By using both grammaticality judgment\nand value comparison classification tasks in English, Japanese, Danish, and\nFrench, we find evidence that the information encoded in these pretrained\nmodels' embeddings is sufficient for grammaticality judgments but generally not\nfor value comparisons. We analyze possible reasons for this and discuss how our\ntasks could be extended in further studies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 19:56:02 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Johnson", "Devin", ""], ["Mak", "Denise", ""], ["Barker", "Drew", ""], ["Loessberg-Zahl", "Lexi", ""]]}, {"id": "2010.06671", "submitter": "Pedram Hosseini", "authors": "Lily Li, Or Levi, Pedram Hosseini, David A. Broniatowski", "title": "A Multi-Modal Method for Satire Detection using Textual and Visual Cues", "comments": "Accepted to the Third Workshop on NLP for Internet Freedom (NLP4IF):\n  Censorship, Disinformation, and Propaganda. Co-located with COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satire is a form of humorous critique, but it is sometimes misinterpreted by\nreaders as legitimate news, which can lead to harmful consequences. We observe\nthat the images used in satirical news articles often contain absurd or\nridiculous content and that image manipulation is used to create fictional\nscenarios. While previous work have studied text-based methods, in this work we\npropose a multi-modal approach based on state-of-the-art visiolinguistic model\nViLBERT. To this end, we create a new dataset consisting of images and\nheadlines of regular and satirical news for the task of satire detection. We\nfine-tune ViLBERT on the dataset and train a convolutional neural network that\nuses an image forensics technique. Evaluation on the dataset shows that our\nproposed multi-modal approach outperforms image-only, text-only, and simple\nfusion baselines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 20:08:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Li", "Lily", ""], ["Levi", "Or", ""], ["Hosseini", "Pedram", ""], ["Broniatowski", "David A.", ""]]}, {"id": "2010.06705", "submitter": "Jiaxin Huang", "authors": "Jiaxin Huang, Yu Meng, Fang Guo, Heng Ji, Jiawei Han", "title": "Weakly-Supervised Aspect-Based Sentiment Analysis via Joint\n  Aspect-Sentiment Topic Embedding", "comments": "accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis of review texts is of great value for\nunderstanding user feedback in a fine-grained manner. It has in general two\nsub-tasks: (i) extracting aspects from each review, and (ii) classifying\naspect-based reviews by sentiment polarity. In this paper, we propose a\nweakly-supervised approach for aspect-based sentiment analysis, which uses only\na few keywords describing each aspect/sentiment without using any labeled\nexamples. Existing methods are either designed only for one of the sub-tasks,\nneglecting the benefit of coupling both, or are based on topic models that may\ncontain overlapping concepts. We propose to first learn <sentiment, aspect>\njoint topic embeddings in the word embedding space by imposing regularizations\nto encourage topic distinctiveness, and then use neural models to generalize\nthe word-level discriminative information by pre-training the classifiers with\nembedding-based predictions and self-training them on unlabeled data. Our\ncomprehensive performance analysis shows that our method generates quality\njoint topics and outperforms the baselines significantly (7.4% and 5.1%\nF1-score gain on average for aspect and sentiment classification respectively)\non benchmark datasets. Our code and data are available at\nhttps://github.com/teapot123/JASen.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 21:33:24 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Huang", "Jiaxin", ""], ["Meng", "Yu", ""], ["Guo", "Fang", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.06710", "submitter": "Diego Amancio", "authors": "Jorge A. V. Tohalino and Diego R. Amancio", "title": "Language Networks: a Practical Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript provides a short and practical introduction to the topic of\nlanguage networks. This text aims at assisting researchers with no practical\nexperience in text and/or network analysis. We provide a practical tutorial on\nhow to model and characterize texts using network-based features. In this\ntutorial, we also include examples of pre-processing and network\nrepresentations. A brief description of the main tasks allying network science\nand text analysis is also provided. A further development of this text shall\ninclude a practical description of network classification via machine learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 21:51:14 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Tohalino", "Jorge A. V.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "2010.06714", "submitter": "Jiaxin Huang", "authors": "Jiaxin Huang, Yiqing Xie, Yu Meng, Yunyi Zhang, Jiawei Han", "title": "CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and\n  Relation Transferring", "comments": "KDD 2020 Research Track", "journal-ref": null, "doi": "10.1145/3394486.3403244", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomy is not only a fundamental form of knowledge representation, but also\ncrucial to vast knowledge-rich applications, such as question answering and web\nsearch. Most existing taxonomy construction methods extract hypernym-hyponym\nentity pairs to organize a \"universal\" taxonomy. However, these generic\ntaxonomies cannot satisfy user's specific interest in certain areas and\nrelations. Moreover, the nature of instance taxonomy treats each node as a\nsingle word, which has low semantic coverage. In this paper, we propose a\nmethod for seed-guided topical taxonomy construction, which takes a corpus and\na seed taxonomy described by concept names as input, and constructs a more\ncomplete taxonomy based on user's interest, wherein each node is represented by\na cluster of coherent terms. Our framework, CoRel, has two modules to fulfill\nthis goal. A relation transferring module learns and transfers the user's\ninterested relation along multiple paths to expand the seed taxonomy structure\nin width and depth. A concept learning module enriches the semantics of each\nconcept node by jointly embedding the taxonomy and text. Comprehensive\nexperiments conducted on real-world datasets show that Corel generates\nhigh-quality topical taxonomies and outperforms all the baselines\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:00:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Huang", "Jiaxin", ""], ["Xie", "Yiqing", ""], ["Meng", "Yu", ""], ["Zhang", "Yunyi", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.06715", "submitter": "Liam Fowl", "authors": "Liam Fowl, Micah Goldblum, Arjun Gupta, Amr Sharaf, Tom Goldstein", "title": "Random Network Distillation as a Diversity Metric for Both Image and\n  Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are increasingly able to produce remarkably high quality\nimages and text. The community has developed numerous evaluation metrics for\ncomparing generative models. However, these metrics do not effectively quantify\ndata diversity. We develop a new diversity metric that can readily be applied\nto data, both synthetic and natural, of any type. Our method employs random\nnetwork distillation, a technique introduced in reinforcement learning. We\nvalidate and deploy this metric on both images and text. We further explore\ndiversity in few-shot image generation, a setting which was previously\ndifficult to evaluate.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:03:52 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Fowl", "Liam", ""], ["Goldblum", "Micah", ""], ["Gupta", "Arjun", ""], ["Sharaf", "Amr", ""], ["Goldstein", "Tom", ""]]}, {"id": "2010.06716", "submitter": "Oleg Vasilyev", "authors": "Oleg Vasilyev, Vedant Dharnidharka, Nicholas Egan, Charlene Chambliss,\n  John Bohannon", "title": "Sensitivity of BLANC to human-scored qualities of text summaries", "comments": "6 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the sensitivity of a document summary quality estimator, BLANC, to\nhuman assessment of qualities for the same summaries. In our human evaluations,\nwe distinguish five summary qualities, defined by how fluent, understandable,\ninformative, compact, and factually correct the summary is. We make the case\nfor optimal BLANC parameters, at which the BLANC sensitivity to almost all of\nsummary qualities is about as good as the sensitivity of a human annotator.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:08:11 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Vasilyev", "Oleg", ""], ["Dharnidharka", "Vedant", ""], ["Egan", "Nicholas", ""], ["Chambliss", "Charlene", ""], ["Bohannon", "John", ""]]}, {"id": "2010.06721", "submitter": "Steven Reich", "authors": "Steven Reich, David Mueller, Nicholas Andrews", "title": "Ensemble Distillation for Structured Prediction: Calibrated, Accurate,\n  Fast-Choose Three", "comments": "EMNLP 2020. v2: Changed formatting of title in metadata; no other\n  changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks do not always produce well-calibrated predictions,\neven when trained with a proper scoring function such as cross-entropy. In\nclassification settings, simple methods such as isotonic regression or\ntemperature scaling may be used in conjunction with a held-out dataset to\ncalibrate model outputs. However, extending these methods to structured\nprediction is not always straightforward or effective; furthermore, a held-out\ncalibration set may not always be available. In this paper, we study ensemble\ndistillation as a general framework for producing well-calibrated structured\nprediction models while avoiding the prohibitive inference-time cost of\nensembles. We validate this framework on two tasks: named-entity recognition\nand machine translation. We find that, across both tasks, ensemble distillation\nproduces models which retain much of, and occasionally improve upon, the\nperformance and calibration benefits of ensembles, while only requiring a\nsingle model during test-time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:30:06 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 17:32:03 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Reich", "Steven", ""], ["Mueller", "David", ""], ["Andrews", "Nicholas", ""]]}, {"id": "2010.06724", "submitter": "Muhao Chen", "authors": "Muhao Chen, Hongming Zhang, Haoyu Wang, Dan Roth", "title": "\"What Are You Trying to Do?\" Semantic Typing of Event Processes", "comments": "CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a new cognitively motivated semantic typing task,\nmulti-axis event process typing, that, given an event process, attempts to\ninfer free-form type labels describing (i) the type of action made by the\nprocess and (ii) the type of object the process seeks to affect. This task is\ninspired by computational and cognitive studies of event understanding, which\nsuggest that understanding processes of events is often directed by recognizing\nthe goals, plans or intentions of the protagonist(s). We develop a large\ndataset containing over 60k event processes, featuring ultra fine-grained\ntyping on both the action and object type axes with very large ($10^3\\sim\n10^4$) label vocabularies. We then propose a hybrid learning framework, P2GT,\nwhich addresses the challenging typing problem with indirect supervision from\nglosses1and a joint learning-to-rank framework. As our experiments indicate,\nP2GT supports identifying the intent of processes, as well as the fine semantic\ntype of the affected object. It also demonstrates the capability of handling\nfew-shot cases, and strong generalizability on out-of-domain event processes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:37:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chen", "Muhao", ""], ["Zhang", "Hongming", ""], ["Wang", "Haoyu", ""], ["Roth", "Dan", ""]]}, {"id": "2010.06727", "submitter": "Haoyu Wang", "authors": "Haoyu Wang, Muhao Chen, Hongming Zhang, Dan Roth", "title": "Joint Constrained Learning for Event-Event Relation Extraction", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding natural language involves recognizing how multiple event\nmentions structurally and temporally interact with each other. In this process,\none can induce event complexes that organize multi-granular events with\ntemporal order and membership relations interweaving among them. Due to the\nlack of jointly labeled data for these relational phenomena and the restriction\non the structures they articulate, we propose a joint constrained learning\nframework for modeling event-event relations. Specifically, the framework\nenforces logical constraints within and across multiple temporal and subevent\nrelations by converting these constraints into differentiable learning\nobjectives. We show that our joint constrained learning approach effectively\ncompensates for the lack of jointly labeled data, and outperforms SOTA methods\non benchmarks for both temporal relation extraction and event hierarchy\nconstruction, replacing a commonly used but more expensive global inference\nprocess. We also present a promising case study showing the effectiveness of\nour approach in inducing event complexes on an external corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:45:28 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 21:51:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Haoyu", ""], ["Chen", "Muhao", ""], ["Zhang", "Hongming", ""], ["Roth", "Dan", ""]]}, {"id": "2010.06775", "submitter": "Hao Tan", "authors": "Hao Tan, Mohit Bansal", "title": "Vokenization: Improving Language Understanding with Contextualized,\n  Visual-Grounded Supervision", "comments": "EMNLP 2020 (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn language by listening, speaking, writing, reading, and also, via\ninteraction with the multimodal real world. Existing language pre-training\nframeworks show the effectiveness of text-only self-supervision while we\nexplore the idea of a visually-supervised language model in this paper. We find\nthat the main reason hindering this exploration is the large divergence in\nmagnitude and distributions between the visually-grounded language datasets and\npure-language corpora. Therefore, we develop a technique named \"vokenization\"\nthat extrapolates multimodal alignments to language-only data by contextually\nmapping language tokens to their related images (which we call \"vokens\"). The\n\"vokenizer\" is trained on relatively small image captioning datasets and we\nthen apply it to generate vokens for large language corpora. Trained with these\ncontextually generated vokens, our visually-supervised language models show\nconsistent improvements over self-supervised alternatives on multiple\npure-language tasks such as GLUE, SQuAD, and SWAG. Code and pre-trained models\npublicly available at https://github.com/airsplay/vokenization\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:11:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.06778", "submitter": "Alexander Gutkin", "authors": "Alena Butryna and Shan-Hui Cathy Chu and Isin Demirsahin and Alexander\n  Gutkin and Linne Ha and Fei He and Martin Jansche and Cibu Johny and Anna\n  Katanova and Oddur Kjartansson and Chenfang Li and Tatiana Merkulova and Yin\n  May Oo and Knot Pipatsrisawat and Clara Rivera and Supheakmungkol Sarin and\n  Pasindu de Silva and Keshan Sodimana and Richard Sproat and Theeraphol\n  Wattanavekin and Jaka Aris Eko Wibawa", "title": "Google Crowdsourced Speech Corpora and Related Open-Source Resources for\n  Low-Resource Languages and Dialects: An Overview", "comments": "Appeared in 2019 UNESCO International Conference Language\n  Technologies for All (LT4All): Enabling Linguistic Diversity and\n  Multilingualism Worldwide, 4-6 December, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an overview of a program designed to address the growing\nneed for developing freely available speech resources for under-represented\nlanguages. At present we have released 38 datasets for building text-to-speech\nand automatic speech recognition applications for languages and dialects of\nSouth and Southeast Asia, Africa, Europe and South America. The paper describes\nthe methodology used for developing such corpora and presents some of our\nfindings that could benefit under-represented language communities.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:24:04 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Butryna", "Alena", ""], ["Chu", "Shan-Hui Cathy", ""], ["Demirsahin", "Isin", ""], ["Gutkin", "Alexander", ""], ["Ha", "Linne", ""], ["He", "Fei", ""], ["Jansche", "Martin", ""], ["Johny", "Cibu", ""], ["Katanova", "Anna", ""], ["Kjartansson", "Oddur", ""], ["Li", "Chenfang", ""], ["Merkulova", "Tatiana", ""], ["Oo", "Yin May", ""], ["Pipatsrisawat", "Knot", ""], ["Rivera", "Clara", ""], ["Sarin", "Supheakmungkol", ""], ["de Silva", "Pasindu", ""], ["Sodimana", "Keshan", ""], ["Sproat", "Richard", ""], ["Wattanavekin", "Theeraphol", ""], ["Wibawa", "Jaka Aris Eko", ""]]}, {"id": "2010.06786", "submitter": "Fereshteh Jafariakinabad", "authors": "Fereshteh Jafariakinabad, Kien A. Hua", "title": "A Self-supervised Representation Learning of Sentence Structure for\n  Authorship Attribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic structure of sentences in a document substantially informs about\nits authorial writing style. Sentence representation learning has been widely\nexplored in recent years and it has been shown that it improves the\ngeneralization of different downstream tasks across many domains. Even though\nutilizing probing methods in several studies suggests that these learned\ncontextual representations implicitly encode some amount of syntax, explicit\nsyntactic information further improves the performance of deep neural models in\nthe domain of authorship attribution. These observations have motivated us to\ninvestigate the explicit representation learning of syntactic structure of\nsentences. In this paper, we propose a self-supervised framework for learning\nstructural representations of sentences. The self-supervised network contains\ntwo components; a lexical sub-network and a syntactic sub-network which take\nthe sequence of words and their corresponding structural labels as the input,\nrespectively. Due to the n-to-1 mapping of words to their structural labels,\neach word will be embedded into a vector representation which mainly carries\nstructural information. We evaluate the learned structural representations of\nsentences using different probing tasks, and subsequently utilize them in the\nauthorship attribution task. Our experimental results indicate that the\nstructural embeddings significantly improve the classification tasks when\nconcatenated with the existing pre-trained word embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 02:57:10 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Jafariakinabad", "Fereshteh", ""], ["Hua", "Kien A.", ""]]}, {"id": "2010.06792", "submitter": "Bowen Tan", "authors": "Bowen Tan, Lianhui Qin, Eric P. Xing, Zhiting Hu", "title": "Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised\n  Approach", "comments": "EMNLP 2020, code and data available at\n  https://github.com/tanyuqian/aspect-based-summarization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a document and a target aspect (e.g., a topic of interest),\naspect-based abstractive summarization attempts to generate a summary with\nrespect to the aspect. Previous studies usually assume a small pre-defined set\nof aspects and fall short of summarizing on other diverse topics. In this work,\nwe study summarizing on arbitrary aspects relevant to the document, which\nsignificantly expands the application of the task in practice. Due to the lack\nof supervision data, we develop a new weak supervision construction method and\nan aspect modeling scheme, both of which integrate rich external knowledge\nsources such as ConceptNet and Wikipedia. Experiments show our approach\nachieves performance boosts on summarizing both real and synthetic documents\ngiven pre-defined or arbitrary aspects.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 03:20:46 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 08:58:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tan", "Bowen", ""], ["Qin", "Lianhui", ""], ["Xing", "Eric P.", ""], ["Hu", "Zhiting", ""]]}, {"id": "2010.06801", "submitter": "Ming Gong", "authors": "Xingyao Zhang, Linjun Shou, Jian Pei, Ming Gong, Lijie Wen, Daxin\n  Jiang", "title": "A Graph Representation of Semi-structured Data for Web Question\n  Answering", "comments": "Accepted as long paper in COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundant semi-structured data on the Web, such as HTML-based tables and\nlists, provide commercial search engines a rich information source for question\nanswering (QA). Different from plain text passages in Web documents, Web tables\nand lists have inherent structures, which carry semantic correlations among\nvarious elements in tables and lists. Many existing studies treat tables and\nlists as flat documents with pieces of text and do not make good use of\nsemantic information hidden in structures. In this paper, we propose a novel\ngraph representation of Web tables and lists based on a systematic\ncategorization of the components in semi-structured data as well as their\nrelations. We also develop pre-training and reasoning techniques on the graph\nmodel for the QA task. Extensive experiments on several real datasets collected\nfrom a commercial engine verify the effectiveness of our approach. Our method\nimproves F1 score by 3.90 points over the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 04:01:54 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zhang", "Xingyao", ""], ["Shou", "Linjun", ""], ["Pei", "Jian", ""], ["Gong", "Ming", ""], ["Wen", "Lijie", ""], ["Jiang", "Daxin", ""]]}, {"id": "2010.06804", "submitter": "Ankur Goswami", "authors": "Ankur Goswami, Akshata Bhat, Hadar Ohana, Theodoros Rekatsinas", "title": "Unsupervised Relation Extraction from Language Models using Constrained\n  Cloze Completion", "comments": "14 pages, 5 figures, Accepted to Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that state-of-the-art self-supervised language models can be readily\nused to extract relations from a corpus without the need to train a fine-tuned\nextractive head. We introduce RE-Flex, a simple framework that performs\nconstrained cloze completion over pretrained language models to perform\nunsupervised relation extraction. RE-Flex uses contextual matching to ensure\nthat language model predictions matches supporting evidence from the input\ncorpus that is relevant to a target relation. We perform an extensive\nexperimental study over multiple relation extraction benchmarks and demonstrate\nthat RE-Flex outperforms competing unsupervised relation extraction methods\nbased on pretrained language models by up to 27.8 $F_1$ points compared to the\nnext-best method. Our results show that constrained inference queries against a\nlanguage model can enable accurate unsupervised relation extraction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 04:21:57 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Goswami", "Ankur", ""], ["Bhat", "Akshata", ""], ["Ohana", "Hadar", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "2010.06822", "submitter": "Faeze Brahman", "authors": "Faeze Brahman, Snigdha Chaturvedi", "title": "Modeling Protagonist Emotions for Emotion-Aware Storytelling", "comments": "EMNLP 2020, update: Conference version of Weber et al. (2020) is\n  cited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions and their evolution play a central role in creating a captivating\nstory. In this paper, we present the first study on modeling the emotional\ntrajectory of the protagonist in neural storytelling. We design methods that\ngenerate stories that adhere to given story titles and desired emotion arcs for\nthe protagonist. Our models include Emotion Supervision (EmoSup) and two\nEmotion-Reinforced (EmoRL) models. The EmoRL models use special rewards\ndesigned to regularize the story generation process through reinforcement\nlearning. Our automatic and manual evaluations demonstrate that these models\nare significantly better at generating stories that follow the desired emotion\narcs compared to baseline methods, without sacrificing story quality.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:24:25 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 19:23:52 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Brahman", "Faeze", ""], ["Chaturvedi", "Snigdha", ""]]}, {"id": "2010.06823", "submitter": "JingHui Qin", "authors": "Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, Liang Lin", "title": "Semantically-Aligned Universal Tree-Structured Solver for Math Word\n  Problems", "comments": "EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical automatic textual math word problems (MWPs) solver should be able\nto solve various textual MWPs while most existing works only focused on\none-unknown linear MWPs. Herein, we propose a simple but efficient method\ncalled Universal Expression Tree (UET) to make the first attempt to represent\nthe equations of various MWPs uniformly. Then a semantically-aligned universal\ntree-structured solver (SAU-Solver) based on an encoder-decoder framework is\nproposed to resolve multiple types of MWPs in a unified model, benefiting from\nour UET representation. Our SAU-Solver generates a universal expression tree\nexplicitly by deciding which symbol to generate according to the generated\nsymbols' semantic meanings like human solving MWPs. Besides, our SAU-Solver\nalso includes a novel subtree-level semanticallyaligned regularization to\nfurther enforce the semantic constraints and rationality of the generated\nexpression tree by aligning with the contextual information. Finally, to\nvalidate the universality of our solver and extend the research boundary of\nMWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP),\nconsisting of three types of MWPs. Experimental results on several MWPs\ndatasets show that our model can solve universal types of MWPs and outperforms\nseveral state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:27:07 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Qin", "Jinghui", ""], ["Lin", "Lihui", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Rumin", ""], ["Lin", "Liang", ""]]}, {"id": "2010.06835", "submitter": "Zhucheng Tu", "authors": "Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu, Raviteja Anantha", "title": "A Wrong Answer or a Wrong Question? An Intricate Relationship between\n  Question Reformulation and Answer Selection in Conversational Question\n  Answering", "comments": "Accepted at the Workshop on Search-Oriented Conversational AI (SCAI)\n  2020. arXiv admin note: text overlap with arXiv:2004.14652", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dependency between an adequate question formulation and correct answer\nselection is a very intriguing but still underexplored area. In this paper, we\nshow that question rewriting (QR) of the conversational context allows to shed\nmore light on this phenomenon and also use it to evaluate robustness of\ndifferent answer selection approaches. We introduce a simple framework that\nenables an automated analysis of the conversational question answering (QA)\nperformance using question rewrites, and present the results of this analysis\non the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover\nsensitivity to question formulation of the popular state-of-the-art models for\nreading comprehension and passage ranking. Our results demonstrate that the\nreading comprehension model is insensitive to question formulation, while the\npassage ranking changes dramatically with a little variation in the input\nquestion. The benefit of QR is that it allows us to pinpoint and group such\ncases automatically. We show how to use this methodology to verify whether QA\nmodels are really learning the task or just finding shortcuts in the dataset,\nand better understand the frequent types of error they make.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 06:29:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Longpre", "Shayne", ""], ["Tu", "Zhucheng", ""], ["Anantha", "Raviteja", ""]]}, {"id": "2010.06857", "submitter": "Hatem Haddad", "authors": "Abir Messaoudi and Hatem Haddad and Moez Ben HajHmida and Chayma\n  Fourati and Abderrazak Ben Hamida", "title": "Learning Word Representations for Tunisian Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tunisians on social media tend to express themselves in their local dialect\nusing Latin script (TUNIZI). This raises an additional challenge to the process\nof exploring and recognizing online opinions. To date, very little work has\naddressed TUNIZI sentiment analysis due to scarce resources for training an\nautomated system. In this paper, we focus on the Tunisian dialect sentiment\nanalysis used on social media. Most of the previous work used machine learning\ntechniques combined with handcrafted features. More recently, Deep Neural\nNetworks were widely used for this task, especially for the English language.\nIn this paper, we explore the importance of various unsupervised word\nrepresentations (word2vec, BERT) and we investigate the use of Convolutional\nNeural Networks and Bidirectional Long Short-Term Memory. Without using any\nkind of handcrafted features, our experimental results on two publicly\navailable datasets showed comparable performances to other languages.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:47:33 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Messaoudi", "Abir", ""], ["Haddad", "Hatem", ""], ["HajHmida", "Moez Ben", ""], ["Fourati", "Chayma", ""], ["Hamida", "Abderrazak Ben", ""]]}, {"id": "2010.06858", "submitter": "Paul O'Leary McCann", "authors": "Paul McCann", "title": "fugashi, a Tool for Tokenizing Japanese in Python", "comments": "Accepted at EMNLP2020's NLP-OSS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent years have seen an increase in the number of large-scale multilingual\nNLP projects. However, even in such projects, languages with special processing\nrequirements are often excluded. One such language is Japanese. Japanese is\nwritten without spaces, tokenization is non-trivial, and while high quality\nopen source tokenizers exist they can be hard to use and lack English\ndocumentation. This paper introduces fugashi, a MeCab wrapper for Python, and\ngives an introduction to tokenizing Japanese.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:52:47 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["McCann", "Paul", ""]]}, {"id": "2010.06891", "submitter": "Qingyang Wu", "authors": "Qingyang Wu, Zhenzhong Lan, Jing Gu, Zhou Yu", "title": "Memformer: The Memory-Augmented Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have obtained remarkable accomplishments in various NLP\ntasks. However, these models have efficiency issues on long sequences, as the\ncomplexity of their self-attention module scales quadratically with the\nsequence length. To remedy the limitation, we present Memformer, a novel\nlanguage model that utilizes a single unified memory to encode and retrieve\npast information. It includes a new optimization scheme, Memory Replay\nBack-Propagation, which promotes long-range back-propagation through time with\na significantly reduced memory requirement. Memformer achieves $\\mathcal{O}(n)$\ntime complexity and $\\mathcal{O}(1)$ space complexity in processing long\nsequences, meaning that the model can handle an infinite length sequence during\ninference. Our model is also compatible with other self-supervised tasks to\nfurther improve the performance on language modeling. Experimental results show\nthat Memformer outperforms the previous long-range sequence models on\nWikiText-103, including Transformer-XL and compressive Transformer.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:03:36 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wu", "Qingyang", ""], ["Lan", "Zhenzhong", ""], ["Gu", "Jing", ""], ["Yu", "Zhou", ""]]}, {"id": "2010.06906", "submitter": "Debanjana Kar", "authors": "Debanjana Kar, Mohit Bhardwaj, Suranjana Samanta, Amar Prakash Azad", "title": "No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet\n  Detection", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sudden widespread menace created by the present global pandemic COVID-19\nhas had an unprecedented effect on our lives. Man-kind is going through\nhumongous fear and dependence on social media like never before. Fear\ninevitably leads to panic, speculations, and the spread of misinformation. Many\ngovernments have taken measures to curb the spread of such misinformation for\npublic well being. Besides global measures, to have effective outreach, systems\nfor demographically local languages have an important role to play in this\neffort. Towards this, we propose an approach to detect fake news about COVID-19\nearly on from social media, such as tweets, for multiple Indic-Languages\nbesides English. In addition, we also create an annotated dataset of Hindi and\nBengali tweet for fake news detection. We propose a BERT based model augmented\nwith additional relevant features extracted from Twitter to identify fake\ntweets. To expand our approach to multiple Indic languages, we resort to mBERT\nbased model which is fine-tuned over created dataset in Hindi and Bengali. We\nalso propose a zero-shot learning approach to alleviate the data scarcity issue\nfor such low resource languages. Through rigorous experiments, we show that our\napproach reaches around 89% F-Score in fake tweet detection which supercedes\nthe state-of-the-art (SOTA) results. Moreover, we establish the first benchmark\nfor two Indic-Languages, Hindi and Bengali. Using our annotated data, our model\nachieves about 79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our\nzero-shot model achieves about 81% F-Score in Hindi and 78% F-Score for Bengali\nTweets without any annotated data, which clearly indicates the efficacy of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:37:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kar", "Debanjana", ""], ["Bhardwaj", "Mohit", ""], ["Samanta", "Suranjana", ""], ["Azad", "Amar Prakash", ""]]}, {"id": "2010.06925", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang", "title": "DA-Transformer: Distance-aware Transformer", "comments": "To appear in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer has achieved great success in the NLP field by composing various\nadvanced models like BERT and GPT. However, Transformer and its existing\nvariants may not be optimal in capturing token distances because the position\nor distance embeddings used by these methods usually cannot keep the precise\ninformation of real distances, which may not be beneficial for modeling the\norders and relations of contexts. In this paper, we propose DA-Transformer,\nwhich is a distance-aware Transformer that can exploit the real distance. We\npropose to incorporate the real distances between tokens to re-scale the raw\nself-attention weights, which are computed by the relevance between attention\nquery and key. Concretely, in different self-attention heads the relative\ndistance between each pair of tokens is weighted by different learnable\nparameters, which control the different preferences on long- or short-term\ninformation of these heads. Since the raw weighted real distances may not be\noptimal for adjusting self-attention weights, we propose a learnable sigmoid\nfunction to map them into re-scaled coefficients that have proper ranges. We\nfirst clip the raw self-attention weights via the ReLU function to keep\nnon-negativity and introduce sparsity, and then multiply them with the\nre-scaled coefficients to encode real distance information into self-attention.\nExtensive experiments on five benchmark datasets show that DA-Transformer can\neffectively improve the performance of many tasks and outperform the vanilla\nTransformer and its several variants.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:09:01 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 09:01:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2010.06943", "submitter": "Jiwei Li", "authors": "Yuxian Meng, Chun Fan, Zijun Sun, Eduard Hovy, Fei Wu and Jiwei Li", "title": "Pair the Dots: Jointly Examining Training History and Test Stimuli for\n  Model Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any prediction from a model is made by a combination of learning history and\ntest stimuli. This provides significant insights for improving model\ninterpretability: {\\it because of which part(s) of which training example(s),\nthe model attends to which part(s) of a test example}. Unfortunately, existing\nmethods to interpret a model's predictions are only able to capture a single\naspect of either test stimuli or learning history, and evidences from both are\nnever combined or integrated. In this paper, we propose an efficient and\ndifferentiable approach to make it feasible to interpret a model's prediction\nby jointly examining training history and test stimuli. Test stimuli is first\nidentified by gradient-based methods, signifying {\\it the part of a test\nexample that the model attends to}. The gradient-based saliency scores are then\npropagated to training examples using influence functions to identify {\\it\nwhich part(s) of which training example(s)} make the model attends to the test\nstimuli. The system is differentiable and time efficient: the adoption of\nsaliency scores from gradient-based methods allows us to efficiently trace a\nmodel's prediction through test stimuli, and then back to training examples\nthrough influence functions. We demonstrate that the proposed methodology\noffers clear explanations about neural model decisions, along with being useful\nfor performing error analysis, crafting adversarial examples and fixing\nerroneously classified examples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:45:01 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 01:58:47 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Meng", "Yuxian", ""], ["Fan", "Chun", ""], ["Sun", "Zijun", ""], ["Hovy", "Eduard", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""]]}, {"id": "2010.06973", "submitter": "James Thorne", "authors": "James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri,\n  Sebastian Riedel, Alon Halevy", "title": "Neural Databases", "comments": "Submitted to PVLDB vol 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural networks have shown impressive performance gains on\nlong-standing AI problems, and in particular, answering queries from natural\nlanguage text. These advances raise the question of whether they can be\nextended to a point where we can relax the fundamental assumption of database\nmanagement, namely, that our data is represented as fields of a pre-defined\nschema.\n  This paper presents a first step in answering that question. We describe\nNeuralDB, a database system with no pre-defined schema, in which updates and\nqueries are given in natural language. We develop query processing techniques\nthat build on the primitives offered by the state of the art Natural Language\nProcessing methods.\n  We begin by demonstrating that at the core, recent NLP transformers, powered\nby pre-trained language models, can answer select-project-join queries if they\nare given the exact set of relevant facts. However, they cannot scale to\nnon-trivial databases and cannot perform aggregation queries. Based on these\nfindings, we describe a NeuralDB architecture that runs multiple Neural SPJ\noperators in parallel, each with a set of database sentences that can produce\none of the answers to the query. The result of these operators is fed to an\naggregation operator if needed. We describe an algorithm that learns how to\ncreate the appropriate sets of facts to be fed into each of the Neural SPJ\noperators. Importantly, this algorithm can be trained by the Neural SPJ\noperator itself. We experimentally validate the accuracy of NeuralDB and its\ncomponents, showing that we can answer queries over thousands of sentences with\nvery high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:31:53 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Thorne", "James", ""], ["Yazdani", "Majid", ""], ["Saeidi", "Marzieh", ""], ["Silvestri", "Fabrizio", ""], ["Riedel", "Sebastian", ""], ["Halevy", "Alon", ""]]}, {"id": "2010.06975", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji and Shirui Pan and Pekka Marttinen", "title": "Medical Code Assignment with Gated Convolution and Note-Code Interaction", "comments": "Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical code assignment from clinical text is a fundamental task in clinical\ninformation system management. As medical notes are typically lengthy and the\nmedical coding system's code space is large, this task is a long-standing\nchallenge. Recent work applies deep neural network models to encode the medical\nnotes and assign medical codes to clinical documents. However, these methods\nare still ineffective as they do not fully encode and capture the lengthy and\nrich semantic information of medical notes nor explicitly exploit the\ninteractions between the notes and codes. We propose a novel method, gated\nconvolutional neural networks, and a note-code interaction (GatedCNN-NCI), for\nautomatic medical code assignment to overcome these challenges. Our methods\ncapture the rich semantic information of the lengthy clinical text for better\nrepresentation by utilizing embedding injection and gated information\npropagation in the medical note encoding module. With a novel note-code\ninteraction design and a graph message passing mechanism, we explicitly capture\nthe underlying dependency between notes and codes, enabling effective code\nprediction. A weight sharing scheme is further designed to decrease the number\nof trainable parameters. Empirical experiments on real-world clinical datasets\nshow that our proposed model outperforms state-of-the-art models in most cases,\nand our model size is on par with light-weighted baselines.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:37:24 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:11:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ji", "Shaoxiong", ""], ["Pan", "Shirui", ""], ["Marttinen", "Pekka", ""]]}, {"id": "2010.06993", "submitter": "Pavel Kalaidin", "authors": "Artem Chumachenko and Daniil Gavrilov and Nikita Balagansky and Pavel\n  Kalaidin", "title": "Weight Squeezing: Reparameterization for Extreme Compression and Fast\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel approach for simultaneous knowledge transfer\nand model compression called Weight Squeezing. With this method, we perform\nknowledge transfer from a pre-trained teacher model by learning the mapping\nfrom its weights to smaller student model weights, without significant loss of\nmodel accuracy.\n  We applied Weight Squeezing to a pre-trained text classification model and\ncompared our method to various other knowledge transfer and model compression\nmethods on several downstream text classification tasks based on the GLUE\ndataset. We observed that our approach produces better results than other\nmethods for training student models without any loss in inference speed. We\nalso compared Weight Squeezing with Low-Rank Factorization approach and\nobserved that our method is significantly faster at inference while being\ncompetitive in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:13:28 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 11:32:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chumachenko", "Artem", ""], ["Gavrilov", "Daniil", ""], ["Balagansky", "Nikita", ""], ["Kalaidin", "Pavel", ""]]}, {"id": "2010.07003", "submitter": "Gyuwan Kim", "authors": "Gyuwan Kim and Kyunghyun Cho", "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime\n  with Search", "comments": "ACL 2021; 11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite transformers' impressive accuracy, their computational cost is often\nprohibitive to use with limited computational resources. Most previous\napproaches to improve inference efficiency require a separate model for each\npossible computational budget. In this paper, we extend PoWER-BERT (Goyal et\nal., 2020) and propose Length-Adaptive Transformer that can be used for various\ninference scenarios after one-shot training. We train a transformer with\nLengthDrop, a structural variant of dropout, which stochastically determines a\nsequence length at each layer. We then conduct a multi-objective evolutionary\nsearch to find a length configuration that maximizes the accuracy and minimizes\nthe efficiency metric under any given computational budget. Additionally, we\nsignificantly extend the applicability of PoWER-BERT beyond sequence-level\nclassification into token-level classification with Drop-and-Restore process\nthat drops word-vectors temporarily in intermediate layers and restores at the\nlast layer if necessary. We empirically verify the utility of the proposed\napproach by demonstrating the superior accuracy-efficiency trade-off under\nvarious setups, including span-based question answering and text\nclassification. Code is available at\nhttps://github.com/clovaai/length-adaptive-transformer.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:28:08 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 20:00:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kim", "Gyuwan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2010.07017", "submitter": "Christopher Wild", "authors": "Wesley Burr, Fanny Chevalier, Christopher Collins, Alison L Gibbs,\n  Raymond Ng, Chris Wild", "title": "Computational Skills by Stealth in Secondary School Data Science", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented growth in the availability of data of all types and\nqualities and the emergence of the field of data science has provided an\nimpetus to finally realizing the implementation of the full breadth of the\nNolan and Temple Lang proposed integration of computing concepts into\nstatistics curricula at all levels in statistics and new data science programs\nand courses. Moreover, data science, implemented carefully, opens accessible\npathways to stem for students for whom neither mathematics nor computer science\nare natural affinities, and who would traditionally be excluded. We discuss a\nproposal for the stealth development of computational skills in students' first\nexposure to data science through careful, scaffolded exposure to computation\nand its power. The intent of this approach is to support students, regardless\nof interest and self-efficacy in coding, in becoming data-driven learners, who\nare capable of asking complex questions about the world around them, and then\nanswering those questions through the use of data-driven inquiry. This\ndiscussion is presented in the context of the International Data Science in\nSchools Project which recently published computer science and statistics\nconsensus curriculum frameworks for a two-year secondary school data science\nprogram, designed to make data science accessible to all.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 09:11:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Burr", "Wesley", ""], ["Chevalier", "Fanny", ""], ["Collins", "Christopher", ""], ["Gibbs", "Alison L", ""], ["Ng", "Raymond", ""], ["Wild", "Chris", ""]]}, {"id": "2010.07048", "submitter": "Jipeng Qiang", "authors": "Jipeng Qiang and Xinyu Lu and Yun Li and Yunhao Yuan and Yang Shi and\n  Xindong Wu", "title": "Chinese Lexical Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical simplification has attracted much attention in many languages, which\nis the process of replacing complex words in a given sentence with simpler\nalternatives of equivalent meaning. Although the richness of vocabulary in\nChinese makes the text very difficult to read for children and non-native\nspeakers, there is no research work for Chinese lexical simplification (CLS)\ntask. To circumvent difficulties in acquiring annotations, we manually create\nthe first benchmark dataset for CLS, which can be used for evaluating the\nlexical simplification systems automatically. In order to acquire more thorough\ncomparison, we present five different types of methods as baselines to generate\nsubstitute candidates for the complex word that include synonym-based approach,\nword embedding-based approach, pretrained language model-based approach,\nsememe-based approach, and a hybrid approach. Finally, we design the\nexperimental evaluation of these baselines and discuss their advantages and\ndisadvantages. To our best knowledge, this is the first study for CLS task.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:55:36 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Qiang", "Jipeng", ""], ["Lu", "Xinyu", ""], ["Li", "Yun", ""], ["Yuan", "Yunhao", ""], ["Shi", "Yang", ""], ["Wu", "Xindong", ""]]}, {"id": "2010.07074", "submitter": "Jiwei Li", "authors": "Xiaofei Sun, Chun Fan, Zijun Sun, Yuxian Meng, Fei Wu and Jiwei Li", "title": "Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical\n  Supervision from Extractive Summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-text generation remains a challenge. The difficulty of generating\ncoherent long texts lies in the fact that existing models overwhelmingly focus\non the tasks of local word prediction, and cannot make high level plans on what\nto generate or capture the high-level discourse dependencies between chunks of\ntexts. Inspired by how humans write, where a list of bullet points or a catalog\nis first outlined, and then each bullet point is expanded to form the whole\narticle, we propose {\\it SOE}, a pipelined system that involves of summarizing,\noutlining and elaborating for long text generation: the model first outlines\nthe summaries for different segments of long texts, and then elaborates on each\nbullet point to generate the corresponding segment. To avoid the\nlabor-intensive process of summary soliciting, we propose the {\\it\nreconstruction} strategy, which extracts segment summaries in an unsupervised\nmanner by selecting its most informative part to reconstruct the segment.The\nproposed generation system comes with the following merits: (1) the summary\nprovides high-level guidances for text generation and avoids the local minimum\nof individual word predictions; (2) the high-level discourse dependencies are\ncaptured in the conditional dependencies between summaries and are preserved\nduring the summary expansion process and (3) additionally, we are able to\nconsider significantly more contexts by representing contexts as concise\nsummaries. Extensive experiments demonstrate that SOE produces long texts with\nsignificantly better quality, along with faster convergence speed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:22:20 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Sun", "Xiaofei", ""], ["Fan", "Chun", ""], ["Sun", "Zijun", ""], ["Meng", "Yuxian", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""]]}, {"id": "2010.07075", "submitter": "Yujing Wang", "authors": "Yiren Chen, Yaming Yang, Hong Sun, Yujing Wang, Yu Xu, Wei Shen, Rong\n  Zhou, Yunhai Tong, Jing Bai, Ruofei Zhang", "title": "AutoADR: Automatic Model Design for Ad Relevance", "comments": "CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained models have attracted extensive attention in the\nresearch community and shown promising results on various tasks of natural\nlanguage processing. However, these pre-trained models are memory and\ncomputation intensive, hindering their deployment into industrial online\nsystems like Ad Relevance. Meanwhile, how to design an effective yet efficient\nmodel architecture is another challenging problem in online Ad Relevance.\nRecently, AutoML shed new lights on architecture design, but how to integrate\nit with pre-trained language models remains unsettled. In this paper, we\npropose AutoADR (Automatic model design for AD Relevance) -- a novel end-to-end\nframework to address this challenge, and share our experience to ship these\ncutting-edge techniques into online Ad Relevance system at Microsoft Bing.\nSpecifically, AutoADR leverages a one-shot neural architecture search algorithm\nto find a tailored network architecture for Ad Relevance. The search process is\nsimultaneously guided by knowledge distillation from a large pre-trained\nteacher model (e.g. BERT), while taking the online serving constraints (e.g.\nmemory and latency) into consideration. We add the model designed by AutoADR as\na sub-model into the production Ad Relevance model. This additional sub-model\nimproves the Precision-Recall AUC (PR AUC) on top of the original Ad Relevance\nmodel by 2.65X of the normalized shipping bar. More importantly, adding this\nautomatically designed sub-model leads to a statistically significant 4.6%\nBad-Ad ratio reduction in online A/B testing. This model has been shipped into\nMicrosoft Bing Ad Relevance Production model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:24:43 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chen", "Yiren", ""], ["Yang", "Yaming", ""], ["Sun", "Hong", ""], ["Wang", "Yujing", ""], ["Xu", "Yu", ""], ["Shen", "Wei", ""], ["Zhou", "Rong", ""], ["Tong", "Yunhai", ""], ["Bai", "Jing", ""], ["Zhang", "Ruofei", ""]]}, {"id": "2010.07079", "submitter": "Emily Dinan", "authors": "Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, Emily Dinan", "title": "Recipes for Safety in Open-domain Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models trained on large unlabeled corpora of human interactions will learn\npatterns and mimic behaviors therein, which include offensive or otherwise\ntoxic behavior and unwanted biases. We investigate a variety of methods to\nmitigate these issues in the context of open-domain generative dialogue models.\nWe introduce a new human-and-model-in-the-loop framework for both training\nsafer models and for evaluating them, as well as a novel method to distill\nsafety considerations inside generative models without the use of an external\nclassifier at deployment time. We conduct experiments comparing these methods\nand find our new techniques are (i) safer than existing models as measured by\nautomatic and human evaluations while (ii) maintaining usability metrics such\nas engagingness relative to the state of the art. We then discuss the\nlimitations of this work by analyzing failure cases of our models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:26:39 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 16:56:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Xu", "Jing", ""], ["Ju", "Da", ""], ["Li", "Margaret", ""], ["Boureau", "Y-Lan", ""], ["Weston", "Jason", ""], ["Dinan", "Emily", ""]]}, {"id": "2010.07095", "submitter": "Xu Zhao", "authors": "Xu Zhao, Zihao Wang, Hao Wu, Yong Zhang", "title": "A Relaxed Matching Procedure for Unsupervised BLI", "comments": "6 pages,1 figures, accepted as short paper by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently unsupervised Bilingual Lexicon Induction (BLI) without any parallel\ncorpus has attracted much research interest. One of the crucial parts in\nmethods for the BLI task is the matching procedure. Previous works impose a too\nstrong constraint on the matching and lead to many counterintuitive translation\npairings. Thus, We propose a relaxed matching procedure to find a more precise\nmatching between two languages. We also find that aligning source and target\nlanguage embedding space bidirectionally will bring significant improvement. We\nfollow the previous iterative framework to conduct experiments. Results on\nstandard benchmark demonstrate the effectiveness of our proposed method, which\nsubstantially outperforms previous unsupervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:53:08 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zhao", "Xu", ""], ["Wang", "Zihao", ""], ["Wu", "Hao", ""], ["Zhang", "Yong", ""]]}, {"id": "2010.07100", "submitter": "Manik Bhandari", "authors": "Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu and Graham\n  Neubig", "title": "Re-evaluating Evaluation in Text Summarization", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated evaluation metrics as a stand-in for manual evaluation are an\nessential part of the development of text-generation tasks such as text\nsummarization. However, while the field has progressed, our standard metrics\nhave not -- for nearly 20 years ROUGE has been the standard evaluation in most\nsummarization papers. In this paper, we make an attempt to re-evaluate the\nevaluation method for text summarization: assessing the reliability of\nautomatic metrics using top-scoring system outputs, both abstractive and\nextractive, on recently popular datasets for both system-level and\nsummary-level evaluation settings. We find that conclusions about evaluation\nmetrics on older datasets do not necessarily hold on modern datasets and\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:58:53 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Bhandari", "Manik", ""], ["Gour", "Pranav", ""], ["Ashfaq", "Atabak", ""], ["Liu", "Pengfei", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.07101", "submitter": "Xu Zhao", "authors": "Xu Zhao, Zihao Wang, Hao Wu, Yong Zhang", "title": "Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction", "comments": "12 pages, 2 figures, 6 tables, accepted as long paper by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervision is a promising paradigm for Bilingual Lexicon Induction\n(BLI) with limited annotations. However, previous semisupervised methods do not\nfully utilize the knowledge hidden in annotated and nonannotated data, which\nhinders further improvement of their performance. In this paper, we propose a\nnew semi-supervised BLI framework to encourage the interaction between the\nsupervised signal and unsupervised alignment. We design two message-passing\nmechanisms to transfer knowledge between annotated and non-annotated data,\nnamed prior optimal transport and bi-directional lexicon update respectively.\nThen, we perform semi-supervised learning based on a cyclic or a parallel\nparameter feeding routine to update our models. Our framework is a general\nframework that can incorporate any supervised and unsupervised BLI methods\nbased on optimal transport. Experimental results on MUSE and VecMap datasets\nshow significant improvement of our models. Ablation study also proves that the\ntwo-way interaction between the supervised signal and unsupervised alignment\naccounts for the gain of the overall performance. Results on distant language\npairs further illustrate the advantage and robustness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:59:07 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zhao", "Xu", ""], ["Wang", "Zihao", ""], ["Wu", "Hao", ""], ["Zhang", "Yong", ""]]}, {"id": "2010.07109", "submitter": "Zihan Zhao", "authors": "Zihan Zhao, Yuncong Liu, Lu Chen, Qi Liu, Rao Ma and Kai Yu", "title": "An Investigation on Different Underlying Quantization Schemes for\n  Pre-trained Language Models", "comments": "Accepted to NLPCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained language models like BERT have shown promising\nperformance on multiple natural language processing tasks. However, the\napplication of these models has been limited due to their huge size. To reduce\nits size, a popular and efficient way is quantization. Nevertheless, most of\nthe works focusing on BERT quantization adapted primary linear clustering as\nthe quantization scheme, and few works try to upgrade it. That limits the\nperformance of quantization significantly. In this paper, we implement k-means\nquantization and compare its performance on the fix-precision quantization of\nBERT with linear quantization. Through the comparison, we verify that the\neffect of the underlying quantization scheme upgrading is underestimated and\nthere is a huge development potential of k-means quantization. Besides, we also\ncompare the two quantization schemes on ALBERT models to explore the robustness\ndifferences between different pre-trained models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 14:05:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Zhao", "Zihan", ""], ["Liu", "Yuncong", ""], ["Chen", "Lu", ""], ["Liu", "Qi", ""], ["Ma", "Rao", ""], ["Yu", "Kai", ""]]}, {"id": "2010.07130", "submitter": "Pradeep R", "authors": "Pradeep Rangan, Sundeep Teki, and Hemant Misra", "title": "Exploiting Spectral Augmentation for Code-Switched Spoken Language\n  Identification", "comments": "5 pages, 3 figures, Accepted for INTERSPEECH-2020 - \"First Workshop\n  on Speech Technologies for Code-switching in Multilingual Communities 2020\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language Identification (LID) systems are needed to identify the\nlanguage(s) present in a given audio sample, and typically could be the first\nstep in many speech processing related tasks such as automatic speech\nrecognition (ASR). Automatic identification of the languages present in a\nspeech signal is not only scientifically interesting, but also of practical\nimportance in a multilingual country such as India. In many of the Indian\ncities, when people interact with each other, as many as three languages may\nget mixed. These may include the official language of that province, Hindi and\nEnglish (at times the languages of the neighboring provinces may also get mixed\nduring these interactions). This makes the spoken LID task extremely\nchallenging in Indian context. While quite a few LID systems in the context of\nIndian languages have been implemented, most such systems have used small scale\nspeech data collected internally within an organization. In the current work,\nwe perform spoken LID on three Indian languages (Gujarati, Telugu, and Tamil)\ncode-mixed with English. This task was organized by the Microsoft research team\nas a spoken LID challenge. In our work, we modify the usual spectral\naugmentation approach and propose a language mask that discriminates the\nlanguage ID pairs, which leads to a noise robust spoken LID system. The\nproposed method gives a relative improvement of approximately 3-5% in the LID\naccuracy over a baseline system proposed by Microsoft on the three language\npairs for two shared tasks suggested in the challenge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 14:37:03 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Rangan", "Pradeep", ""], ["Teki", "Sundeep", ""], ["Misra", "Hemant", ""]]}, {"id": "2010.07152", "submitter": "Kai Wang", "authors": "Kai Wang, Yu Liu, Qian Ma, Quan Z. Sheng", "title": "MulDE: Multi-teacher Knowledge Distillation for Low-dimensional\n  Knowledge Graph Embeddings", "comments": "Accepted for publication at the Web Conference 2021", "journal-ref": null, "doi": "10.1145/3442381.3449898", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction based on knowledge graph embeddings (KGE) aims to predict new\ntriples to automatically construct knowledge graphs (KGs). However, recent KGE\nmodels achieve performance improvements by excessively increasing the embedding\ndimensions, which may cause enormous training costs and require more storage\nspace. In this paper, instead of training high-dimensional models, we propose\nMulDE, a novel knowledge distillation framework, which includes multiple\nlow-dimensional hyperbolic KGE models as teachers and two student components,\nnamely Junior and Senior. Under a novel iterative distillation strategy, the\nJunior component, a low-dimensional KGE model, asks teachers actively based on\nits preliminary prediction results, and the Senior component integrates\nteachers' knowledge adaptively to train the Junior component based on two\nmechanisms: relation-specific scaling and contrast attention. The experimental\nresults show that MulDE can effectively improve the performance and training\nspeed of low-dimensional KGE models. The distilled 32-dimensional model is\ncompetitive compared to the state-of-the-art high-dimensional methods on\nseveral widely-used datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:09:27 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 09:33:41 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 14:29:34 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 08:09:33 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Kai", ""], ["Liu", "Yu", ""], ["Ma", "Qian", ""], ["Sheng", "Quan Z.", ""]]}, {"id": "2010.07174", "submitter": "Benjamin Newman", "authors": "Benjamin Newman, John Hewitt, Percy Liang, Christopher D. Manning", "title": "The EOS Decision and Length Extrapolation", "comments": "16 page, 7 Figures, 9 Tables, Blackbox NLP Workshop at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrapolation to unseen sequence lengths is a challenge for neural generative\nmodels of language. In this work, we characterize the effect on length\nextrapolation of a modeling decision often overlooked: predicting the end of\nthe generative process through the use of a special end-of-sequence (EOS)\nvocabulary item. We study an oracle setting - forcing models to generate to the\ncorrect sequence length at test time - to compare the length-extrapolative\nbehavior of networks trained to predict EOS (+EOS) with networks not trained to\n(-EOS). We find that -EOS substantially outperforms +EOS, for example\nextrapolating well to lengths 10 times longer than those seen at training time\nin a bracket closing task, as well as achieving a 40% improvement over +EOS in\nthe difficult SCAN dataset length generalization task. By comparing the hidden\nstates and dynamics of -EOS and +EOS models, we observe that +EOS models fail\nto generalize because they (1) unnecessarily stratify their hidden states by\ntheir linear position is a sequence (structures we call length manifolds) or\n(2) get stuck in clusters (which we refer to as length attractors) once the EOS\ntoken is the highest-probability prediction.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:46:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Newman", "Benjamin", ""], ["Hewitt", "John", ""], ["Liang", "Percy", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2010.07212", "submitter": "Debajyoti Datta", "authors": "Debajyoti Datta, Shashwat Kumar, Laura Barnes, Tom Fletcher", "title": "Geometry matters: Exploring language examples at the decision boundary", "comments": "Preprint: Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of recent evidence has highlighted the limitations of natural\nlanguage processing (NLP) datasets and classifiers. These include the presence\nof annotation artifacts in datasets, classifiers relying on shallow features\nlike a single word (e.g., if a movie review has the word \"romantic\", the review\ntends to be positive), or unnecessary words (e.g., learning a proper noun to\nclassify a movie as positive or negative). The presence of such artifacts has\nsubsequently led to the development of challenging datasets to force the model\nto generalize better. While a variety of heuristic strategies, such as\ncounterfactual examples and contrast sets, have been proposed, the theoretical\njustification about what makes these examples difficult for the classifier is\noften lacking or unclear. In this paper, using tools from information geometry,\nwe propose a theoretical way to quantify the difficulty of an example in NLP.\nUsing our approach, we explore difficult examples for several deep learning\narchitectures. We discover that both BERT, CNN and fasttext are susceptible to\nword substitutions in high difficulty examples. These classifiers tend to\nperform poorly on the FIM test set. (generated by sampling and perturbing\ndifficult examples, with accuracy dropping below 50%). We replicate our\nexperiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews,\nYelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a\ncorrelation coefficient of -0.4 between resilience to perturbations and the\ndifficulty score. Similarly we observe a correlation of 0.35 between the\ndifficulty score and the empirical success probability of random substitutions.\nOur approach is simple, architecture agnostic and can be used to study the\nfragilities of text classification models. All the code used will be made\npublicly available, including a tool to explore the difficult examples for\nother datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:26:13 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 05:48:40 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Datta", "Debajyoti", ""], ["Kumar", "Shashwat", ""], ["Barnes", "Laura", ""], ["Fletcher", "Tom", ""]]}, {"id": "2010.07245", "submitter": "Yu Meng", "authors": "Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao\n  Zhang, Jiawei Han", "title": "Text Classification Using Label Names Only: A Language Model\n  Self-Training Approach", "comments": "EMNLP 2020. (Code: https://github.com/yumeng5/LOTClass)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current text classification methods typically require a good number of\nhuman-labeled documents as training data, which can be costly and difficult to\nobtain in real applications. Humans can perform classification without seeing\nany labeled examples but only based on a small set of words describing the\ncategories to be classified. In this paper, we explore the potential of only\nusing the label name of each class to train classification models on unlabeled\ndata, without using any labeled documents. We use pre-trained neural language\nmodels both as general linguistic knowledge sources for category understanding\nand as representation learning models for document classification. Our method\n(1) associates semantically related words with the label names, (2) finds\ncategory-indicative words and trains the model to predict their implied\ncategories, and (3) generalizes the model via self-training. We show that our\nmodel achieves around 90% accuracy on four benchmark datasets including topic\nand sentiment classification without using any labeled documents but learning\nfrom unlabeled data supervised by at most 3 words (1 in most cases) per class\nas the label name.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:06:41 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Meng", "Yu", ""], ["Zhang", "Yunyi", ""], ["Huang", "Jiaxin", ""], ["Xiong", "Chenyan", ""], ["Ji", "Heng", ""], ["Zhang", "Chao", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.07261", "submitter": "Makesh Narsimhan Sreedhar", "authors": "Makesh Narsimhan Sreedhar, Kun Ni, Siva Reddy", "title": "Learning Improvised Chatbots from Adversarial Modifications of Natural\n  Language Feedback", "comments": "Accepted for publication at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ubiquitous nature of chatbots and their interaction with users generate\nan enormous amount of data. Can we improve chatbots using this data? A\nself-feeding chatbot improves itself by asking natural language feedback when a\nuser is dissatisfied with its response and uses this feedback as an additional\ntraining sample. However, user feedback in most cases contains extraneous\nsequences hindering their usefulness as a training sample. In this work, we\npropose a generative adversarial model that converts noisy feedback into a\nplausible natural response in a conversation. The generator's goal is to\nconvert the feedback into a response that answers the user's previous utterance\nand to fool the discriminator which distinguishes feedback from natural\nresponses. We show that augmenting original training data with these modified\nfeedback responses improves the original chatbot performance from 69.94% to\n75.96% in ranking correct responses on the Personachat dataset, a large\nimprovement given that the original model is already trained on 131k samples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:33:37 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 02:19:13 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Sreedhar", "Makesh Narsimhan", ""], ["Ni", "Kun", ""], ["Reddy", "Siva", ""]]}, {"id": "2010.07279", "submitter": "Khyathi Raghavi Chandu", "authors": "Khyathi Raghavi Chandu and Alan W Black", "title": "Positioning yourself in the maze of Neural Text Generation: A\n  Task-Agnostic Survey", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation metamorphosed into several critical natural language\napplications ranging from text completion to free form narrative generation. In\norder to progress research in text generation, it is critical to absorb the\nexisting research works and position ourselves in this massively growing field.\nSpecifically, this paper surveys the fundamental components of modeling\napproaches relaying task agnostic impacts across various generation tasks such\nas storytelling, summarization, translation etc., In this context, we present\nan abstraction of the imperative techniques with respect to learning paradigms,\npretraining, modeling approaches, decoding and the key challenges outstanding\nin the field in each of them. Thereby, we deliver a one-stop destination for\nresearchers in the field to facilitate a perspective on where to situate their\nwork and how it impacts other closely related generation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:54:42 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 00:31:52 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Chandu", "Khyathi Raghavi", ""], ["Black", "Alan W", ""]]}, {"id": "2010.07292", "submitter": "Hancheng Cao", "authors": "Hancheng Cao, Vivian Yang, Victor Chen, Yu Jin Lee, Lydia Stone,\n  N'godjigui Junior Diarrassouba, Mark E. Whiting, Michael S. Bernstein", "title": "My Team Will Go On: Differentiating High and Low Viability Teams through\n  Team Interaction", "comments": "CSCW 2020 Honorable Mention Award", "journal-ref": "Proc. ACM Hum.-Comput. Interact. 4, CSCW3, Article 230 (December\n  2020)", "doi": "10.1145/3432929", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding team viability -- a team's capacity for sustained and future\nsuccess -- is essential for building effective teams. In this study, we\naggregate features drawn from the organizational behavior literature to train a\nviability classification model over a dataset of 669 10-minute text\nconversations of online teams. We train classifiers to identify teams at the\ntop decile (most viable teams), 50th percentile (above a median split), and\nbottom decile (least viable teams), then characterize the attributes of teams\nat each of these viability levels. We find that a lasso regression model\nachieves an accuracy of .74--.92 AUC ROC under different thresholds of\nclassifying viability scores. From these models, we identify the use of\nexclusive language such as `but' and `except', and the use of second person\npronouns, as the most predictive features for detecting the most viable teams,\nsuggesting that active engagement with others' ideas is a crucial signal of a\nviable team. Only a small fraction of the 10-minute discussion, as little as 70\nseconds, is required for predicting the viability of team interaction. This\nwork suggests opportunities for teams to assess, track, and visualize their own\nviability in real time as they collaborate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:33:36 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 22:30:20 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Cao", "Hancheng", ""], ["Yang", "Vivian", ""], ["Chen", "Victor", ""], ["Lee", "Yu Jin", ""], ["Stone", "Lydia", ""], ["Diarrassouba", "N'godjigui Junior", ""], ["Whiting", "Mark E.", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "2010.07375", "submitter": "Alexandra DeLucia", "authors": "Alexandra DeLucia, Aaron Mueller, Xiang Lisa Li, Jo\\~ao Sedoc", "title": "Decoding Methods for Neural Narrative Generation", "comments": "20 pages. Updated to the accepted version in Workshop on Generation\n  Evaluation and Metrics at ACL 2021 (GEM'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narrative generation is an open-ended NLP task in which a model generates a\nstory given a prompt. The task is similar to neural response generation for\nchatbots; however, innovations in response generation are often not applied to\nnarrative generation, despite the similarity between these tasks. We aim to\nbridge this gap by applying and evaluating advances in decoding methods for\nneural response generation to neural narrative generation. In particular, we\nemploy GPT-2 and perform ablations across nucleus sampling thresholds and\ndiverse decoding hyperparameters -- specifically, maximum mutual information --\nanalyzing results over multiple criteria with automatic and human evaluation.\nWe find that (1) nucleus sampling is generally best with thresholds between 0.7\nand 0.9; (2) a maximum mutual information objective can improve the quality of\ngenerated stories; and (3) established automatic metrics do not correlate well\nwith human judgments of narrative quality on any qualitative metric.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 19:32:56 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:50:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["DeLucia", "Alexandra", ""], ["Mueller", "Aaron", ""], ["Li", "Xiang Lisa", ""], ["Sedoc", "Jo\u00e3o", ""]]}, {"id": "2010.07410", "submitter": "Ilan Price", "authors": "Ilan Price, Jordan Gifford-Moore, Jory Fleming, Saul Musker, Maayan\n  Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, Jeffrey Sorensen", "title": "Six Attributes of Unhealthy Conversation", "comments": "Appearing in the 4th Workshop on Online Abuse and Harms (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new dataset of approximately 44000 comments labeled by\ncrowdworkers. Each comment is labelled as either 'healthy' or 'unhealthy', in\naddition to binary labels for the presence of six potentially 'unhealthy'\nsub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or\ntrolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic;\nand/or (6) an unfair generalisation. Each label also has an associated\nconfidence score. We argue that there is a need for datasets which enable\nresearch based on a broad notion of 'unhealthy online conversation'. We build\nthis typology to encompass a substantial proportion of the individual comments\nwhich contribute to unhealthy online conversation. For some of these\nattributes, this is the first publicly available dataset of this scale. We\nexplore the quality of the dataset, present some summary statistics and initial\nmodels to illustrate the utility of this data, and highlight limitations and\ndirections for further research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:28:06 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Price", "Ilan", ""], ["Gifford-Moore", "Jordan", ""], ["Fleming", "Jory", ""], ["Musker", "Saul", ""], ["Roichman", "Maayan", ""], ["Sylvain", "Guillaume", ""], ["Thain", "Nithum", ""], ["Dixon", "Lucas", ""], ["Sorensen", "Jeffrey", ""]]}, {"id": "2010.07414", "submitter": "Isar Nejadgholi", "authors": "Isar Nejadgholi and Svetlana Kiritchenko", "title": "On Cross-Dataset Generalization in Automatic Detection of Online Abuse", "comments": "13 pages, 3 figures, published at WOAH-2020 (The 4th Workshop on\n  Online Abuse and Harms)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP research has attained high performances in abusive language detection as\na supervised classification task. While in research settings, training and test\ndatasets are usually obtained from similar data samples, in practice systems\nare often applied on data that are different from the training set in topic and\nclass distributions. Also, the ambiguity in class definitions inherited in this\ntask aggravates the discrepancies between source and target datasets. We\nexplore the topic bias and the task formulation bias in cross-dataset\ngeneralization. We show that the benign examples in the Wikipedia Detox dataset\nare biased towards platform-specific topics. We identify these examples using\nunsupervised topic modeling and manual inspection of topics' keywords. Removing\nthese topics increases cross-dataset generalization, without reducing in-domain\nclassification performance. For a robust dataset design, we suggest applying\ninexpensive unsupervised methods to inspect the collected data and downsize the\nnon-generalizable content before manually annotating for class labels.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:47:03 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 15:28:12 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 18:47:03 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nejadgholi", "Isar", ""], ["Kiritchenko", "Svetlana", ""]]}, {"id": "2010.07432", "submitter": "Alex Tamkin", "authors": "Alex Tamkin, Mike Wu, Noah Goodman", "title": "Viewmaker Networks: Learning Views for Unsupervised Representation\n  Learning", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent methods for unsupervised representation learning train models to\nbe invariant to different \"views,\" or distorted versions of an input. However,\ndesigning these views requires considerable trial and error by human experts,\nhindering widespread adoption of unsupervised representation learning methods\nacross domains and modalities. To address this, we propose viewmaker networks:\ngenerative models that learn to produce useful views from a given input.\nViewmakers are stochastic bounded adversaries: they produce views by generating\nand then adding an $\\ell_p$-bounded perturbation to the input, and are trained\nadversarially with respect to the main encoder network. Remarkably, when\npretraining on CIFAR-10, our learned views enable comparable transfer accuracy\nto the well-tuned SimCLR augmentations -- despite not including transformations\nlike cropping or color jitter. Furthermore, our learned views significantly\noutperform baseline augmentations on speech recordings (+9% points, on average)\nand wearable sensor data (+17% points). Viewmakers can also be combined with\nhandcrafted views: they improve robustness to common image corruptions and can\nincrease transfer performance in cases where handcrafted views are less\nexplored. These results suggest that viewmakers may provide a path towards more\ngeneral representation learning algorithms -- reducing the domain expertise and\neffort needed to pretrain on a much wider set of domains. Code is available at\nhttps://github.com/alextamkin/viewmaker.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 23:03:31 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 06:49:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tamkin", "Alex", ""], ["Wu", "Mike", ""], ["Goodman", "Noah", ""]]}, {"id": "2010.07435", "submitter": "Maryam Hashemzadeh", "authors": "Maryam Hashemzadeh, Greta Kaufeld, Martha White, Andrea E. Martin,\n  Alona Fyshe", "title": "From Language to Language-ish: How Brain-Like is an LSTM's\n  Representation of Nonsensical Language Stimuli?", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representations generated by many models of language (word embeddings,\nrecurrent neural networks and transformers) correlate to brain activity\nrecorded while people read. However, these decoding results are usually based\non the brain's reaction to syntactically and semantically sound language\nstimuli. In this study, we asked: how does an LSTM (long short term memory)\nlanguage model, trained (by and large) on semantically and syntactically intact\nlanguage, represent a language sample with degraded semantic or syntactic\ninformation? Does the LSTM representation still resemble the brain's reaction?\nWe found that, even for some kinds of nonsensical language, there is a\nstatistically significant relationship between the brain's activity and the\nrepresentations of an LSTM. This indicates that, at least in some instances,\nLSTMs and the human brain handle nonsensical data similarly.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 23:26:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Hashemzadeh", "Maryam", ""], ["Kaufeld", "Greta", ""], ["White", "Martha", ""], ["Martin", "Andrea E.", ""], ["Fyshe", "Alona", ""]]}, {"id": "2010.07440", "submitter": "Elena Del Olmo Su\\'arez", "authors": "Elena del Olmo Su\\'arez and Ana Mar\\'ia Fern\\'andez-Pampill\\'on\n  Cesteros", "title": "A new approach for extracting the conceptual schema of texts based on\n  the linguistic Thematic Progression theory", "comments": null, "journal-ref": "Del Olmo, E.; Fern\\'andez-Pampill\\'on, A. A new approach for\n  extracting the conceptual schema of texts based on the linguistic Thematic\n  Progression theory. Proceedings of the Workshop on Hybrid Intelligence for\n  NLP Tasks (ECAI-2020), 23-27", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this article is to present a new approach for the discovery\nand labelling of the implicit conceptual schema of texts through the\napplication of the Thematic Progression theory. The underlying conceptual\nschema is the core component for the generation of summaries that are genuinely\nconsistent with the semantics of the text.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 23:50:25 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Su\u00e1rez", "Elena del Olmo", ""], ["Cesteros", "Ana Mar\u00eda Fern\u00e1ndez-Pampill\u00f3n", ""]]}, {"id": "2010.07447", "submitter": "Michal Lukasik", "authors": "Michal Lukasik, Himanshu Jain, Aditya Krishna Menon, Seungyeon Kim,\n  Srinadh Bhojanapalli, Felix Yu, Sanjiv Kumar", "title": "Semantic Label Smoothing for Sequence to Sequence Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label smoothing has been shown to be an effective regularization strategy in\nclassification, that prevents overfitting and helps in label de-noising.\nHowever, extending such methods directly to seq2seq settings, such as Machine\nTranslation, is challenging: the large target output space of such problems\nmakes it intractable to apply label smoothing over all possible outputs. Most\nexisting approaches for seq2seq settings either do token level smoothing, or\nsmooth over sequences generated by randomly substituting tokens in the target\nsequence. Unlike these works, in this paper, we propose a technique that\nsmooths over \\emph{well formed} relevant sequences that not only have\nsufficient n-gram overlap with the target sequence, but are also\n\\emph{semantically similar}. Our method shows a consistent and significant\nimprovement over the state-of-the-art techniques on different datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 00:31:15 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Lukasik", "Michal", ""], ["Jain", "Himanshu", ""], ["Menon", "Aditya Krishna", ""], ["Kim", "Seungyeon", ""], ["Bhojanapalli", "Srinadh", ""], ["Yu", "Felix", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2010.07466", "submitter": "Hoang Nguyen Hung Van", "authors": "Hoang Van, Ahmad Musa, Mihai Surdeanu and Stephen Kobourov", "title": "The Language of Food during the Pandemic: Hints about the Dietary\n  Effects of Covid-19", "comments": "9 page of main contents plus 1 page of references. 4 figures and 9\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the language of food on Twitter during the pandemic lockdown in the\nUnited States, focusing on the two month period of March 15 to May 15, 2020.\nSpecifically, we analyze over770,000 tweets published during the lockdown and\nthe equivalent period in the five previous years and highlight several worrying\ntrends. First, we observe that during the lockdown there was a notable shift\nfrom mentions of healthy foods to unhealthy foods. Second, we show an increased\npointwise mutual information of depression hashtags with food-related tweets\nposted during the lockdown and an increased association between depression\nhashtags and unhealthy foods, tobacco, and alcohol during the lockdown.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:33:05 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Van", "Hoang", ""], ["Musa", "Ahmad", ""], ["Surdeanu", "Mihai", ""], ["Kobourov", "Stephen", ""]]}, {"id": "2010.07475", "submitter": "Wanjun Zhong", "authors": "Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou,\n  Jiahai Wang, Jian Yin", "title": "Neural Deepfake Detection with Factual Structure of Text", "comments": "EMNLP2020;10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake detection, the task of automatically discriminating\nmachine-generated text, is increasingly critical with recent advances in\nnatural language generative models. Existing approaches to deepfake detection\ntypically represent documents with coarse-grained representations. However,\nthey struggle to capture factual structures of documents, which is a\ndiscriminative factor between machine-generated and human-written text\naccording to our statistical analysis. To address this, we propose a\ngraph-based model that utilizes the factual structure of a document for\ndeepfake detection of text. Our approach represents the factual structure of a\ngiven document as an entity graph, which is further utilized to learn sentence\nrepresentations with a graph neural network. Sentence representations are then\ncomposed to a document representation for making predictions, where consistent\nrelations between neighboring sentences are sequentially modeled. Results of\nexperiments on two public deepfake datasets show that our approach\nsignificantly improves strong base models built with RoBERTa. Model analysis\nfurther indicates that our model can distinguish the difference in the factual\nstructure between machine-generated text and human-written text.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 02:35:31 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhong", "Wanjun", ""], ["Tang", "Duyu", ""], ["Xu", "Zenan", ""], ["Wang", "Ruize", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""], ["Wang", "Jiahai", ""], ["Yin", "Jian", ""]]}, {"id": "2010.07497", "submitter": "Jianheng Tang", "authors": "Wenge Liu, Jianheng Tang, Jinghui Qin, Lin Xu, Zhen Li, Xiaodan Liang", "title": "MedDG: A Large-scale Medical Consultation Dataset for Building Medical\n  Dialogue System", "comments": "Data and code are available at https://github.com/lwgkzl/MedDG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing conversational agents to interact with patients and provide\nprimary clinical advice has attracted increasing attention due to its huge\napplication potential, especially in the time of COVID-19 Pandemic. However,\nthe training of end-to-end neural-based medical dialogue system is restricted\nby an insufficient quantity of medical dialogue corpus. In this work, we make\nthe first attempt to build and release a large-scale high-quality Medical\nDialogue dataset related to 12 types of common Gastrointestinal diseases named\nMedDG, with more than 17K conversations collected from the online health\nconsultation community. Five different categories of entities, including\ndiseases, symptoms, attributes, tests, and medicines, are annotated in each\nconversation of MedDG as additional labels. To push forward the future research\non building expert-sensitive medical dialogue system, we proposes two kinds of\nmedical dialogue tasks based on MedDG dataset. One is the next entity\nprediction and the other is the doctor response generation. To acquire a clear\ncomprehension on these two medical dialogue tasks, we implement several\nstate-of-the-art benchmarks, as well as design two dialogue models with a\nfurther consideration on the predicted entities. Experimental results show that\nthe pre-train language models and other baselines struggle on both tasks with\npoor performance in our dataset, and the response quality can be enhanced with\nthe help of auxiliary entity information. From human evaluation, the simple\nretrieval model outperforms several state-of-the-art generative models,\nindicating that there still remains a large room for improvement on generating\nmedically meaningful responses.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 03:34:33 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Liu", "Wenge", ""], ["Tang", "Jianheng", ""], ["Qin", "Jinghui", ""], ["Xu", "Lin", ""], ["Li", "Zhen", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2010.07503", "submitter": "Sho Takase", "authors": "Sho Takase and Naoaki Okazaki", "title": "Multi-Task Learning for Cross-Lingual Abstractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-task learning framework for cross-lingual abstractive\nsummarization to augment training data. Recent studies constructed pseudo\ncross-lingual abstractive summarization data to train their neural\nencoder-decoders. Meanwhile, we introduce existing genuine data such as\ntranslation pairs and monolingual abstractive summarization data into training.\nOur proposed method, Transum, attaches a special token to the beginning of the\ninput sentence to indicate the target task. The special token enables us to\nincorporate the genuine data into the training data easily. The experimental\nresults show that Transum achieves better performance than the model trained\nwith only pseudo cross-lingual summarization data. In addition, we achieve the\ntop ROUGE score on Chinese-English and Arabic-English abstractive\nsummarization. Moreover, Transum also has a positive effect on machine\ntranslation. Experimental results indicate that Transum improves the\nperformance from the strong baseline, Transformer, in Chinese-English,\nArabic-English, and English-Japanese translation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:03:00 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Takase", "Sho", ""], ["Okazaki", "Naoaki", ""]]}, {"id": "2010.07515", "submitter": "John Hewitt", "authors": "John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, Christopher D.\n  Manning", "title": "RNNs can generate bounded hierarchical languages with optimal memory", "comments": "EMNLP2020 + appendix typo fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks empirically generate natural language with high\nsyntactic fidelity. However, their success is not well-understood\ntheoretically. We provide theoretical insight into this success, proving in a\nfinite-precision setting that RNNs can efficiently generate bounded\nhierarchical languages that reflect the scaffolding of natural language syntax.\nWe introduce Dyck-($k$,$m$), the language of well-nested brackets (of $k$\ntypes) and $m$-bounded nesting depth, reflecting the bounded memory needs and\nlong-distance dependencies of natural language syntax. The best known results\nuse $O(k^{\\frac{m}{2}})$ memory (hidden units) to generate these languages. We\nprove that an RNN with $O(m \\log k)$ hidden units suffices, an exponential\nreduction in memory, by an explicit construction. Finally, we show that no\nalgorithm, even with unbounded computation, can suffice with $o(m \\log k)$\nhidden units.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:42:29 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Hewitt", "John", ""], ["Hahn", "Michael", ""], ["Ganguli", "Surya", ""], ["Liang", "Percy", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2010.07522", "submitter": "Youmi Ma", "authors": "Youmi Ma, Tatsuya Hiraoka, Naoaki Okazaki", "title": "Named Entity Recognition and Relation Extraction using Enhanced Table\n  Filling by Contextualized Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel method for extracting named entities and relations\nfrom unstructured text based on the table representation is presented. By using\ncontextualized word embeddings, the proposed method computes representations\nfor entity mentions and long-range dependencies without complicated\nhand-crafted features or neural-network architectures. We also adapt a tensor\ndot-product to predict relation labels all at once without resorting to\nhistory-based predictions or search strategies. These advances significantly\nsimplify the model and algorithm for the extraction of named entities and\nrelations. Despite its simplicity, the experimental results demonstrate that\nthe proposed method outperforms the state-of-the-art methods on the CoNLL04 and\nACE05 English datasets. We also confirm that the proposed method achieves a\ncomparable performance with the state-of-the-art NER models on the ACE05\ndatasets when multiple sentences are provided for context aggregation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:58:23 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Ma", "Youmi", ""], ["Hiraoka", "Tatsuya", ""], ["Okazaki", "Naoaki", ""]]}, {"id": "2010.07523", "submitter": "Zhengxuan Wu", "authors": "Zhengxuan Wu, Desmond C. Ong", "title": "Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis", "comments": null, "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow\nfiner-grained inferences about sentiment to be drawn from the same text,\ndepending on context. For example, a given text can have different targets\n(e.g., neighborhoods) and different aspects (e.g., price or safety), with\ndifferent sentiment associated with each target-aspect pair. In this paper, we\ninvestigate whether adding context to self-attention models improves\nperformance on (T)ABSA. We propose two variants of Context-Guided BERT\n(CG-BERT) that learn to distribute attention under different contexts. We first\nadapt a context-aware Transformer to produce a CG-BERT that uses context-guided\nsoftmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model\nthat learns a compositional attention that supports subtractive attention. We\ntrain both models with pretrained BERT on two (T)ABSA datasets: SentiHood and\nSemEval-2014 (Task 4). Both models achieve new state-of-the-art results with\nour QACG-BERT model having the best performance. Furthermore, we provide\nanalyses of the impact of context in the our proposed models. Our work provides\nmore evidence for the utility of adding context-dependencies to pretrained\nself-attention-based language models for context-based natural language tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 05:01:20 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:33:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Ong", "Desmond C.", ""]]}, {"id": "2010.07526", "submitter": "Ana Marasovi\\'c", "authors": "Ana Marasovi\\'c, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras,\n  Noah A. Smith, Yejin Choi", "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From\n  Pixels to Semantic Frames to Commonsense Graphs", "comments": "Accepted to Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language rationales could provide intuitive, higher-level\nexplanations that are easily understandable by humans, complementing the more\nbroadly studied lower-level explanations based on gradients or attention\nweights. We present the first study focused on generating natural language\nrationales across several complex visual reasoning tasks: visual commonsense\nreasoning, visual-textual entailment, and visual question answering. The key\nchallenge of accurate rationalization is comprehensive image understanding at\nall levels: not just their explicit content at the pixel level, but their\ncontextual contents at the semantic and pragmatic levels. We present\nRationale^VT Transformer, an integrated model that learns to generate free-text\nrationales by combining pretrained language models with object recognition,\ngrounded visual semantic frames, and visual commonsense graphs. Our experiments\nshow that the base pretrained language model benefits from visual adaptation\nand that free-text rationalization is a promising research direction to\ncomplement model interpretability for complex visual-textual reasoning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 05:08:56 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Marasovi\u0107", "Ana", ""], ["Bhagavatula", "Chandra", ""], ["Park", "Jae Sung", ""], ["Bras", "Ronan Le", ""], ["Smith", "Noah A.", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.07543", "submitter": "Yuanhe Tian", "authors": "Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang", "title": "Improving Constituency Parsing with Span Attention", "comments": "Natural Language Processing. 13 pages, 6 figures. Findings of\n  EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constituency parsing is a fundamental and important task for natural language\nunderstanding, where a good representation of contextual information can help\nthis task. N-grams, which is a conventional type of feature for contextual\ninformation, have been demonstrated to be useful in many tasks, and thus could\nalso be beneficial for constituency parsing if they are appropriately modeled.\nIn this paper, we propose span attention for neural chart-based constituency\nparsing to leverage n-gram information. Considering that current chart-based\nparsers with Transformer-based encoder represent spans by subtraction of the\nhidden states at the span boundaries, which may cause information loss\nespecially for long spans, we incorporate n-grams into span representations by\nweighting them according to their contributions to the parsing process.\nMoreover, we propose categorical span attention to further enhance the model by\nweighting n-grams within different length categories, and thus benefit\nlong-sentence parsing. Experimental results on three widely used benchmark\ndatasets demonstrate the effectiveness of our approach in parsing Arabic,\nChinese, and English, where state-of-the-art performance is obtained by our\napproach on all of them.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:36:39 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Tian", "Yuanhe", ""], ["Song", "Yan", ""], ["Xia", "Fei", ""], ["Zhang", "Tong", ""]]}, {"id": "2010.07557", "submitter": "Roman Klinger", "authors": "Laura Oberl\\\"ander, Roman Klinger", "title": "Token Sequence Labeling vs. Clause Classification for English Emotion\n  Stimulus Detection", "comments": "accepted at *SEM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion stimulus detection is the task of finding the cause of an emotion in\na textual description, similar to target or aspect detection for sentiment\nanalysis. Previous work approached this in three ways, namely (1) as text\nclassification into an inventory of predefined possible stimuli (\"Is the\nstimulus category A or B?\"), (2) as sequence labeling of tokens (\"Which tokens\ndescribe the stimulus?\"), and (3) as clause classification (\"Does this clause\ncontain the emotion stimulus?\"). So far, setting (3) has been evaluated broadly\non Mandarin and (2) on English, but no comparison has been performed.\nTherefore, we aim to answer whether clause classification or sequence labeling\nis better suited for emotion stimulus detection in English. To accomplish that,\nwe propose an integrated framework which enables us to evaluate the two\ndifferent approaches comparably, implement models inspired by state-of-the-art\napproaches in Mandarin, and test them on four English data sets from different\ndomains. Our results show that sequence labeling is superior on three out of\nfour datasets, in both clause-based and sequence-based evaluation. The only\ncase in which clause classification performs better is one data set with a high\ndensity of clause annotations. Our error analysis further confirms\nquantitatively and qualitatively that clauses are not the appropriate stimulus\nunit in English.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 07:11:04 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 07:37:02 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 09:56:01 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Oberl\u00e4nder", "Laura", ""], ["Klinger", "Roman", ""]]}, {"id": "2010.07574", "submitter": "Simon Flachs", "authors": "Simon Flachs, Oph\\'elie Lacroix, Helen Yannakoudakis, Marek Rei,\n  Anders S{\\o}gaard", "title": "Grammatical Error Correction in Low Error Density Domains: A New\n  Benchmark and Analyses", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 07:52:01 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Flachs", "Simon", ""], ["Lacroix", "Oph\u00e9lie", ""], ["Yannakoudakis", "Helen", ""], ["Rei", "Marek", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2010.07576", "submitter": "Yu Cao", "authors": "Yu Cao, Wei Bi, Meng Fang, Dacheng Tao", "title": "Pretrained Language Models for Dialogue Generation with Multiple Input\n  Sources", "comments": "9 pages (containing 4 pages of references and appendix), accepted to\n  EMNLP2020-Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pretrained language models have achieved outstanding performance\non natural language understanding tasks. However, it is still under\ninvestigating how to apply them to dialogue generation tasks, especially those\nwith responses conditioned on multiple sources. Previous work simply\nconcatenates all input sources or averages information from different input\nsources. In this work, we study dialogue models with multiple input sources\nadapted from the pretrained language model GPT2. We explore various methods to\nfuse multiple separate attention information corresponding to different\nsources. Our experimental results show that proper fusion methods deliver\nhigher relevance with dialogue history than simple fusion baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 07:53:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cao", "Yu", ""], ["Bi", "Wei", ""], ["Fang", "Meng", ""], ["Tao", "Dacheng", ""]]}, {"id": "2010.07606", "submitter": "Liang Li", "authors": "Liang Li, Can Ma, Yinliang Yue, Linjun Shou and Dayong Hu", "title": "Learning Better Representation for Tables by Self-Supervised Tasks", "comments": "This article is writing messy, and some of the experiments are\n  inadequate, which may mislead the reader about our work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table-to-text generation aims at automatically generating natural text to\nhelp people to conveniently obtain the important information in tables.\nAlthough neural models for table-to-text have achieved remarkable progress,\nsome problems still overlooked. The first is that the values recorded in many\ntables are mostly numbers in practice. The existing approaches do not do\nspecial treatment for these, and still regard these as words in natural\nlanguage text. Secondly, the target texts in training dataset may contain\nredundant information or facts do not exist in the input tables. These may give\nwrong supervision signals to some methods based on content selection and\nplanning and auxiliary supervision. To solve these problems, we propose two\nself-supervised tasks, Number Ordering and Significance Ordering, to help to\nlearn better table representation. The former works on the column dimension to\nhelp to incorporate the size property of numbers into table representation. The\nlatter acts on row dimension and help to learn a significance-aware table\nrepresentation. We test our methods on the widely used dataset ROTOWIRE which\nconsists of NBA game statistic and related news. The experimental results\ndemonstrate that the model trained together with these two self-supervised\ntasks can generate text that contains more salient and well-organized facts,\neven without modeling context selection and planning. And we achieve the\nstate-of-the-art performance on automatic metrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:03:38 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 11:57:31 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 06:36:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Liang", ""], ["Ma", "Can", ""], ["Yue", "Yinliang", ""], ["Shou", "Linjun", ""], ["Hu", "Dayong", ""]]}, {"id": "2010.07620", "submitter": "Yao Zhang", "authors": "Yao Zhang, Xu Zhang, Jun Wang, Hongru Liang, Adam Jatowt, Wenqiang\n  Lei, Zhenglu Yang", "title": "GMH: A General Multi-hop Reasoning Model for KG Completion", "comments": "11 pages, 5 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are essential for numerous downstream natural language\nprocessing applications, but are typically incomplete with many facts missing.\nThis results in research efforts on multi-hop reasoning task, which can be\nformulated as a search process and current models typically perform short\ndistance reasoning. However, the long-distance reasoning is also vital with the\nability to connect the superficially unrelated entities. To the best of our\nknowledge, there lacks a general framework that approaches multi-hop reasoning\nin both short and long scenarios. We argue that there are two key issues for\nlong distance reasoning: i) which edge to select, and ii) when to stop the\nsearch. In this work, we propose a general model which resolves the issues with\nthree modules: 1) the local-global knowledge module to estimate the possible\npaths, 2) the differentiated action dropout module to explore a diverse set of\npaths, and 3) the adaptive stopping search module to avoid over searching. The\ncomprehensive results on three datasets demonstrate the superiority of our\nmodel with significant improvements against baselines in both short and long\ndistance reasoning scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:30:46 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 13:48:28 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhang", "Yao", ""], ["Zhang", "Xu", ""], ["Wang", "Jun", ""], ["Liang", "Hongru", ""], ["Jatowt", "Adam", ""], ["Lei", "Wenqiang", ""], ["Yang", "Zhenglu", ""]]}, {"id": "2010.07637", "submitter": "Yuzhao Mao", "authors": "Yuzhao Mao, Qi Sun, Guang Liu, Xiaojie Wang, Weiguo Gao, Xuan Li,\n  Jianping Shen", "title": "DialogueTRM: Exploring the Intra- and Inter-Modal Emotional Behaviors in\n  the Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion Recognition in Conversations (ERC) is essential for building\nempathetic human-machine systems. Existing studies on ERC primarily focus on\nsummarizing the context information in a conversation, however, ignoring the\ndifferentiated emotional behaviors within and across different modalities.\nDesigning appropriate strategies that fit the differentiated multi-modal\nemotional behaviors can produce more accurate emotional predictions. Thus, we\npropose the DialogueTransformer to explore the differentiated emotional\nbehaviors from the intra- and inter-modal perspectives. For intra-modal, we\nconstruct a novel Hierarchical Transformer that can easily switch between\nsequential and feed-forward structures according to the differentiated context\npreference within each modality. For inter-modal, we constitute a novel\nMulti-Grained Interactive Fusion that applies both neuron- and vector-grained\nfeature interactions to learn the differentiated contributions across all\nmodalities. Experimental results show that DialogueTRM outperforms the\nstate-of-the-art by a significant margin on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 10:10:41 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mao", "Yuzhao", ""], ["Sun", "Qi", ""], ["Liu", "Guang", ""], ["Wang", "Xiaojie", ""], ["Gao", "Weiguo", ""], ["Li", "Xuan", ""], ["Shen", "Jianping", ""]]}, {"id": "2010.07638", "submitter": "Prathyusha Jwalapuram", "authors": "Prathyusha Jwalapuram, Shafiq Joty, Youlin Shen", "title": "Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular Neural Machine Translation model training uses strategies like\nbacktranslation to improve BLEU scores, requiring large amounts of additional\ndata and training. We introduce a class of conditional\ngenerative-discriminative hybrid losses that we use to fine-tune a trained\nmachine translation model. Through a combination of targeted fine-tuning\nobjectives and intuitive re-use of the training data the model has failed to\nadequately learn from, we improve the model performance of both a\nsentence-level and a contextual model without using any additional data. We\ntarget the improvement of pronoun translations through our fine-tuning and\nevaluate our models on a pronoun benchmark testset. Our sentence-level model\nshows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets,\nwhile our contextual model achieves the best results, improving from 31.81 to\n32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En\ntestset, with corresponding improvements in pronoun translation. We further\nshow the generalizability of our method by reproducing the improvements on two\nadditional language pairs, Fr-En and Cs-En. Code available at\n<https://github.com/ntunlp/pronoun-finetuning>.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 10:11:40 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jwalapuram", "Prathyusha", ""], ["Joty", "Shafiq", ""], ["Shen", "Youlin", ""]]}, {"id": "2010.07665", "submitter": "Hareesh Bahuleyan", "authors": "Hareesh Bahuleyan and Layla El Asri", "title": "Diverse Keyphrase Generation with Neural Unlikelihood Training", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study sequence-to-sequence (S2S) keyphrase generation\nmodels from the perspective of diversity. Recent advances in neural natural\nlanguage generation have made possible remarkable progress on the task of\nkeyphrase generation, demonstrated through improvements on quality metrics such\nas F1-score. However, the importance of diversity in keyphrase generation has\nbeen largely ignored. We first analyze the extent of information redundancy\npresent in the outputs generated by a baseline model trained using maximum\nlikelihood estimation (MLE). Our findings show that repetition of keyphrases is\na major issue with MLE training. To alleviate this issue, we adopt neural\nunlikelihood (UL) objective for training the S2S model. Our version of UL\ntraining operates at (1) the target token level to discourage the generation of\nrepeating tokens; (2) the copy token level to avoid copying repetitive tokens\nfrom the source text. Further, to encourage better model planning during the\ndecoding process, we incorporate K-step ahead token prediction objective that\ncomputes both MLE and UL losses on future tokens as well. Through extensive\nexperiments on datasets from three different domains we demonstrate that the\nproposed approach attains considerably large diversity gains, while maintaining\ncompetitive output quality.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:12:26 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bahuleyan", "Hareesh", ""], ["Asri", "Layla El", ""]]}, {"id": "2010.07668", "submitter": "Peng Cui", "authors": "Peng Cui, Le Hu, Yuanchao Liu", "title": "Inducing Alignment Structure with Gated Graph Attention Networks for\n  Sentence Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence matching is a fundamental task of natural language processing with\nvarious applications. Most recent approaches adopt attention-based neural\nmodels to build word- or phrase-level alignment between two sentences. However,\nthese models usually ignore the inherent structure within the sentences and\nfail to consider various dependency relationships among text units. To address\nthese issues, this paper proposes a graph-based approach for sentence matching.\nFirst, we represent a sentence pair as a graph with several carefully design\nstrategies. We then employ a novel gated graph attention network to encode the\nconstructed graph for sentence matching. Experimental results demonstrate that\nour method substantially achieves state-of-the-art performance on two datasets\nacross tasks of natural language and paraphrase identification. Further\ndiscussions show that our model can learn meaningful graph structure,\nindicating its superiority on improved interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:25:54 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cui", "Peng", ""], ["Hu", "Le", ""], ["Liu", "Yuanchao", ""]]}, {"id": "2010.07676", "submitter": "Guanhua Zhang", "authors": "Guanhua Zhang, Bing Bai, Jian Liang, Kun Bai, Conghui Zhu, Tiejun Zhao", "title": "Reliable Evaluations for Natural Language Inference based on a Unified\n  Cross-dataset Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show that crowd-sourced Natural Language Inference (NLI)\ndatasets may suffer from significant biases like annotation artifacts. Models\nutilizing these superficial clues gain mirage advantages on the in-domain\ntesting set, which makes the evaluation results over-estimated. The lack of\ntrustworthy evaluation settings and benchmarks stalls the progress of NLI\nresearch. In this paper, we propose to assess a model's trustworthy\ngeneralization performance with cross-datasets evaluation. We present a new\nunified cross-datasets benchmark with 14 NLI datasets, and re-evaluate 9\nwidely-used neural network-based NLI models as well as 5 recently proposed\ndebiasing methods for annotation artifacts. Our proposed evaluation scheme and\nexperimental baselines could provide a basis to inspire future reliable NLI\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 11:50:12 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhang", "Guanhua", ""], ["Bai", "Bing", ""], ["Liang", "Jian", ""], ["Bai", "Kun", ""], ["Zhu", "Conghui", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2010.07711", "submitter": "Yile Wang", "authors": "Yile Wang, Leyang Cui, Yue Zhang", "title": "Does Chinese BERT Encode Word Structure?", "comments": "Accepted by COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized representations give significantly improved results for a wide\nrange of NLP tasks. Much work has been dedicated to analyzing the features\ncaptured by representative models such as BERT. Existing work finds that\nsyntactic, semantic and word sense knowledge are encoded in BERT. However,\nlittle work has investigated word features for character-based languages such\nas Chinese. We investigate Chinese BERT using both attention weight\ndistribution statistics and probing tasks, finding that (1) word information is\ncaptured by BERT; (2) word-level features are mostly in the middle\nrepresentation layers; (3) downstream tasks make different use of word features\nin BERT, with POS tagging and chunking relying the most on word features, and\nnatural language inference relying the least on such features.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:40:56 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Yile", ""], ["Cui", "Leyang", ""], ["Zhang", "Yue", ""]]}, {"id": "2010.07717", "submitter": "Weijie Yu", "authors": "Weijie Yu, Chen Xu, Jun Xu, Liang Pang, Xiaopeng Gao, Xiaozhao Wang\n  and Ji-Rong Wen", "title": "Wasserstein Distance Regularized Sequence Representation for Text\n  Matching in Asymmetrical Domains", "comments": "accepted as long paper by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to matching texts from asymmetrical domains is projecting the\ninput sequences into a common semantic space as feature vectors upon which the\nmatching function can be readily defined and learned. In real-world matching\npractices, it is often observed that with the training goes on, the feature\nvectors projected from different domains tend to be indistinguishable. The\nphenomenon, however, is often overlooked in existing matching models. As a\nresult, the feature vectors are constructed without any regularization, which\ninevitably increases the difficulty of learning the downstream matching\nfunctions. In this paper, we propose a novel match method tailored for text\nmatching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein\ndistance-based regularizer is defined to regularize the features vectors\nprojected from different domains. As a result, the method enforces the feature\nprojection function to generate vectors such that those correspond to different\ndomains cannot be easily discriminated. The training process of WD-Match\namounts to a game that minimizes the matching loss regularized by the\nWasserstein distance. WD-Match can be used to improve different text matching\nmethods, by using the method as its underlying matching model. Four popular\ntext matching methods have been exploited in the paper. Experimental results\nbased on four publicly available benchmarks showed that WD-Match consistently\noutperformed the underlying methods and the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:52:09 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 01:32:08 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yu", "Weijie", ""], ["Xu", "Chen", ""], ["Xu", "Jun", ""], ["Pang", "Liang", ""], ["Gao", "Xiaopeng", ""], ["Wang", "Xiaozhao", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.07761", "submitter": "Phillip Keung", "authors": "Phillip Keung, Julian Salazar, Yichao Lu, Noah A. Smith", "title": "Unsupervised Bitext Mining and Translation via Self-trained Contextual\n  Embeddings", "comments": "To appear in the Transactions of the Association for Computational\n  Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an unsupervised method to create pseudo-parallel corpora for\nmachine translation (MT) from unaligned text. We use multilingual BERT to\ncreate source and target sentence embeddings for nearest-neighbor search and\nadapt the model via self-training. We validate our technique by extracting\nparallel sentence pairs on the BUCC 2017 bitext mining task and observe up to a\n24.5 point increase (absolute) in F1 scores over previous unsupervised methods.\nWe then improve an XLM-based unsupervised neural MT system pre-trained on\nWikipedia by supplementing it with pseudo-parallel text mined from the same\ncorpus, boosting unsupervised translation performance by up to 3.5 BLEU on the\nWMT'14 French-English and WMT'16 German-English tasks and outperforming the\nprevious state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese\ncorpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU\nimprovement on the low-resource MT task. We demonstrate that unsupervised\nbitext mining is an effective way of augmenting MT datasets and complements\nexisting techniques like initializing with pre-trained contextual embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:04:03 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Keung", "Phillip", ""], ["Salazar", "Julian", ""], ["Lu", "Yichao", ""], ["Smith", "Noah A.", ""]]}, {"id": "2010.07773", "submitter": "Shubhanker Banerjee", "authors": "Shubhanker Banerjee, Arun Jayapal and Sajeetha Thavareesan", "title": "NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of\n  Code-Mixed Dravidian text using XLNet", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social media has penetrated into multilingual societies, however most of them\nuse English to be a preferred language for communication. So it looks natural\nfor them to mix their cultural language with English during conversations\nresulting in abundance of multilingual data, call this code-mixed data,\navailable in todays' world.Downstream NLP tasks using such data is challenging\ndue to the semantic nature of it being spread across multiple languages.One\nsuch Natural Language Processing task is sentiment analysis, for this we use an\nauto-regressive XLNet model to perform sentiment analysis on code-mixed\nTamil-English and Malayalam-English datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:09:02 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Banerjee", "Shubhanker", ""], ["Jayapal", "Arun", ""], ["Thavareesan", "Sajeetha", ""]]}, {"id": "2010.07785", "submitter": "Weishi Wang", "authors": "Weishi Wang, Shafiq Joty, Steven C.H. Hoi", "title": "Response Selection for Multi-Party Conversations with Dynamic Topic\n  Tracking", "comments": "9 pages, EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While participants in a multi-party multi-turn conversation simultaneously\nengage in multiple conversation topics, existing response selection methods are\ndeveloped mainly focusing on a two-party single-conversation scenario. Hence,\nthe prolongation and transition of conversation topics are ignored by current\nmethods. In this work, we frame response selection as a dynamic topic tracking\ntask to match the topic between the response and relevant conversation context.\nWith this new formulation, we propose a novel multi-task learning framework\nthat supports efficient encoding through large pretrained models with only two\nutterances at once to perform dynamic topic disentanglement and response\nselection. We also propose Topic-BERT an essential pretraining step to embed\ntopic information into BERT with self-supervised learning. Experimental results\non the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response\nselection and topic disentanglement tasks outperforming existing methods by a\ngood margin.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:21:38 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wang", "Weishi", ""], ["Joty", "Shafiq", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2010.07792", "submitter": "Yinuo Guo", "authors": "Yinuo Guo, Zeqi Lin, Jian-Guang Lou, Dongmei Zhang", "title": "Hierarchical Poset Decoding for Compositional Generalization in Language", "comments": "Accepted by Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize human language understanding as a structured prediction task\nwhere the output is a partially ordered set (poset). Current encoder-decoder\narchitectures do not take the poset structure of semantics into account\nproperly, thus suffering from poor compositional generalization ability. In\nthis paper, we propose a novel hierarchical poset decoding paradigm for\ncompositional generalization in language. Intuitively: (1) the proposed\nparadigm enforces partial permutation invariance in semantics, thus avoiding\noverfitting to bias ordering information; (2) the hierarchical mechanism allows\nto capture high-level structures of posets. We evaluate our proposed decoder on\nCompositional Freebase Questions (CFQ), a large and realistic natural language\nquestion answering dataset that is specifically designed to measure\ncompositional generalization. Results show that it outperforms current\ndecoders.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:34:26 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Guo", "Yinuo", ""], ["Lin", "Zeqi", ""], ["Lou", "Jian-Guang", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2010.07816", "submitter": "Georgios MIchalopoulos", "authors": "George Michalopoulos, Helen Chen, Alexander Wong", "title": "Where's the Question? A Multi-channel Deep Convolutional Neural Network\n  for Question Identification in Textual Data", "comments": "12 pages, 4 figures, to be published in The 3rd Clinical Natural\n  Language Processing Workshop", "journal-ref": null, "doi": "10.18653/v1/2020.clinicalnlp-1.24", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most clinical practice settings, there is no rigorous reviewing of the\nclinical documentation, resulting in inaccurate information captured in the\npatient medical records. The gold standard in clinical data capturing is\nachieved via \"expert-review\", where clinicians can have a dialogue with a\ndomain expert (reviewers) and ask them questions about data entry rules.\nAutomatically identifying \"real questions\" in these dialogues could uncover\nambiguities or common problems in data capturing in a given clinical setting.\n  In this study, we proposed a novel multi-channel deep convolutional neural\nnetwork architecture, namely Quest-CNN, for the purpose of separating real\nquestions that expect an answer (information or help) about an issue from\nsentences that are not questions, as well as from questions referring to an\nissue mentioned in a nearby sentence (e.g., can you clarify this?), which we\nwill refer as \"c-questions\". We conducted a comprehensive performance\ncomparison analysis of the proposed multi-channel deep convolutional neural\nnetwork against other deep neural networks. Furthermore, we evaluated the\nperformance of traditional rule-based and learning-based methods for detecting\nquestion sentences. The proposed Quest-CNN achieved the best F1 score both on a\ndataset of data entry-review dialogue in a dialysis care setting, and on a\ngeneral domain dataset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:11:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Michalopoulos", "George", ""], ["Chen", "Helen", ""], ["Wong", "Alexander", ""]]}, {"id": "2010.07835", "submitter": "Yue Yu", "authors": "Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao and Chao Zhang", "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A\n  Contrastive-Regularized Self-Training Approach", "comments": "NAACL-HLT 2021. Code: \\url{https://github.com/yueyu1030/COSINE}", "journal-ref": "NAACL-HLT 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fine-tuned pre-trained language models (LMs) have achieved enormous success\nin many natural language processing (NLP) tasks, but they still require\nexcessive labeled data in the fine-tuning stage. We study the problem of\nfine-tuning pre-trained LMs using only weak supervision, without any labeled\ndata. This problem is challenging because the high capacity of LMs makes them\nprone to overfitting the noisy labels generated by weak supervision. To address\nthis problem, we develop a contrastive self-training framework, COSINE, to\nenable fine-tuning LMs with weak supervision. Underpinned by contrastive\nregularization and confidence-based reweighting, this contrastive self-training\nframework can gradually improve model fitting while effectively suppressing\nerror propagation. Experiments on sequence, token, and sentence pair\nclassification tasks show that our model outperforms the strongest baseline by\nlarge margins on 7 benchmarks in 6 tasks, and achieves competitive performance\nwith fully-supervised fine-tuning methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:55:08 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 17:05:28 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 02:25:55 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Yue", ""], ["Zuo", "Simiao", ""], ["Jiang", "Haoming", ""], ["Ren", "Wendi", ""], ["Zhao", "Tuo", ""], ["Zhang", "Chao", ""]]}, {"id": "2010.07865", "submitter": "Vladislav Lialin", "authors": "Vladislav Lialin, Rahul Goel, Andrey Simanovsky, Anna Rumshisky,\n  Rushin Shah", "title": "Update Frequently, Update Fast: Retraining Semantic Parsing Systems in a\n  Fraction of Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently used semantic parsing systems deployed in voice assistants can\nrequire weeks to train. Datasets for these models often receive small and\nfrequent updates, data patches. Each patch requires training a new model. To\nreduce training time, one can fine-tune the previously trained model on each\npatch, but naive fine-tuning exhibits catastrophic forgetting - degradation of\nthe model performance on the data not represented in the data patch. In this\nwork, we propose a simple method that alleviates catastrophic forgetting and\nshow that it is possible to match the performance of a model trained from\nscratch in less than 10% of a time via fine-tuning. The key to achieving this\nis supersampling and EWC regularization. We demonstrate the effectiveness of\nour method on multiple splits of the Facebook TOP and SNIPS datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:37:41 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 16:33:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lialin", "Vladislav", ""], ["Goel", "Rahul", ""], ["Simanovsky", "Andrey", ""], ["Rumshisky", "Anna", ""], ["Shah", "Rushin", ""]]}, {"id": "2010.07874", "submitter": "Peter Belc\\'ak", "authors": "Peter Belcak", "title": "The LL(finite) strategy for optimal LL(k) parsing", "comments": "An error was found in one of the algorithms for weak LL(k) grammars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LL(finite) parsing strategy for parsing of LL(k) grammars where k needs\nnot to be known is presented. The strategy parses input in linear time, uses\narbitrary but always minimal lookahead necessary to disambiguate between\nalternatives of nonterminals, and it is optimal in the number of lookahead\nterminal scans performed. Modifications to the algorithm are shown that allow\nfor resolution of grammar ambiguities by precedence -- effectively interpreting\nthe input as a parsing expression grammar -- as well as for the use of\npredicates, and a proof of concept, the open-source parser generator Astir,\nemploys the LL(finite) strategy in the output it generates.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:52:29 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 10:22:50 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Belcak", "Peter", ""]]}, {"id": "2010.07878", "submitter": "Matthias Hertel", "authors": "Hannah Bast, Matthias Hertel, Mostafa M. Mohamed", "title": "Tokenization Repair in the Presence of Spelling Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following tokenization repair problem: Given a natural\nlanguage text with any combination of missing or spurious spaces, correct\nthese. Spelling errors can be present, but it's not part of the problem to\ncorrect them. For example, given: \"Tispa per isabout token izaionrep air\",\ncompute \"Tis paper is about tokenizaion repair\". It is tempting to think of\nthis problem as a special case of spelling correction or to treat the two\nproblems together. We make a case that tokenization repair and spelling\ncorrection should and can be treated as separate problems. We investigate a\nvariety of neural models as well as a number of strong baselines. We identify\nthree main ingredients to high-quality tokenization repair: deep language\nmodels with a bidirectional component, training the models on text with\nspelling errors, and making use of the space information already present. Our\nbest methods can repair all tokenization errors on 97.5% of the correctly\nspelled test sentences and on 96.0% of the misspelled test sentences. With all\nspaces removed from the given text (the scenario from previous work), the\naccuracy falls to 94.5% and 90.1%, respectively. We conduct a detailed error\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:55:45 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bast", "Hannah", ""], ["Hertel", "Matthias", ""], ["Mohamed", "Mostafa M.", ""]]}, {"id": "2010.07882", "submitter": "Jiacheng Xu", "authors": "Jiacheng Xu, Shrey Desai, Greg Durrett", "title": "Understanding Neural Abstractive Summarization Models via Uncertainty", "comments": "To appear in EMNLP 2020; code available at\n  https://github.com/jiacheng-xu/text-sum-uncertainty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An advantage of seq2seq abstractive summarization models is that they\ngenerate text in a free-form manner, but this flexibility makes it difficult to\ninterpret model behavior. In this work, we analyze summarization decoders in\nboth blackbox and whitebox ways by studying on the entropy, or uncertainty, of\nthe model's token-level predictions. For two strong pre-trained models, PEGASUS\nand BART on two summarization datasets, we find a strong correlation between\nlow prediction entropy and where the model copies tokens rather than generating\nnovel text. The decoder's uncertainty also connects to factors like sentence\nposition and syntactic distance between adjacent pairs of tokens, giving a\nsense of what factors make a context particularly selective for the model's\nnext output token. Finally, we study the relationship of decoder uncertainty\nand attention behavior to understand how attention gives rise to these observed\neffects in the model. We show that uncertainty is a useful perspective for\nanalyzing summarization and text generation models more broadly.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:57:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Xu", "Jiacheng", ""], ["Desai", "Shrey", ""], ["Durrett", "Greg", ""]]}, {"id": "2010.07886", "submitter": "Shrey Desai", "authors": "Shrey Desai and Jiacheng Xu and Greg Durrett", "title": "Compressive Summarization with Plausibility and Salience Modeling", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive summarization systems typically rely on a crafted set of\nsyntactic rules to determine what spans of possible summary sentences can be\ndeleted, then learn a model of what to actually delete by optimizing for\ncontent selection (ROUGE). In this work, we propose to relax the rigid\nsyntactic constraints on candidate spans and instead leave compression\ndecisions to two data-driven criteria: plausibility and salience. Deleting a\nspan is plausible if removing it maintains the grammaticality and factuality of\na sentence, and spans are salient if they contain important information from\nthe summary. Each of these is judged by a pre-trained Transformer model, and\nonly deletions that are both plausible and not salient can be applied. When\nintegrated into a simple extraction-compression pipeline, our method achieves\nstrong in-domain results on benchmark summarization datasets, and human\nevaluation shows that the plausibility model generally selects for grammatical\nand factual deletions. Furthermore, the flexibility of our approach allows it\nto generalize cross-domain: our system fine-tuned on only 500 samples from a\nnew domain can match or exceed an in-domain extractive model trained on much\nmore data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:07:10 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Desai", "Shrey", ""], ["Xu", "Jiacheng", ""], ["Durrett", "Greg", ""]]}, {"id": "2010.07891", "submitter": "Ekta Sood", "authors": "Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling", "title": "Improving Natural Language Processing Tasks with Human Gaze-Guided\n  Neural Attention", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lack of corpora has so far limited advances in integrating human gaze data\nas a supervisory signal in neural attention mechanisms for natural language\nprocessing(NLP). We propose a novel hybrid text saliency model(TSM) that, for\nthe first time, combines a cognitive model of reading with explicit human gaze\nsupervision in a single machine learning framework. On four different corpora\nwe demonstrate that our hybrid TSM duration predictions are highly correlated\nwith human gaze ground truth. We further propose a novel joint modeling\napproach to integrate TSM predictions into the attention layer of a network\ndesigned for a specific upstream NLP task without the need for any\ntask-specific human gaze data. We demonstrate that our joint model outperforms\nthe state of the art in paraphrase generation on the Quora Question Pairs\ncorpus by more than 10% in BLEU-4 and achieves state of the art performance for\nsentence compression on the challenging Google Sentence Compression corpus. As\nsuch, our work introduces a practical approach for bridging between data-driven\nand cognitive models and demonstrates a new way to integrate human gaze-guided\nneural attention into NLP tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:14:09 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 16:16:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sood", "Ekta", ""], ["Tannert", "Simon", ""], ["Mueller", "Philipp", ""], ["Bulling", "Andreas", ""]]}, {"id": "2010.07954", "submitter": "Alexander Ku", "authors": "Alexander Ku and Peter Anderson and Roma Patel and Eugene Ie and Jason\n  Baldridge", "title": "Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense\n  Spatiotemporal Grounding", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation\n(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger\n(more paths and instructions) than other VLN datasets. It emphasizes the role\nof language in VLN by addressing known biases in paths and eliciting more\nreferences to visible entities. Furthermore, each word in an instruction is\ntime-aligned to the virtual poses of instruction creators and validators. We\nestablish baseline scores for monolingual and multilingual settings and\nmultitask learning when including Room-to-Room annotations. We also provide\nresults for a model that learns from synchronized pose traces by focusing only\non portions of the panorama attended to in human demonstrations. The size,\nscope and detail of RxR dramatically expands the frontier for research on\nembodied language agents in simulated, photo-realistic environments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 18:01:15 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ku", "Alexander", ""], ["Anderson", "Peter", ""], ["Patel", "Roma", ""], ["Ie", "Eugene", ""], ["Baldridge", "Jason", ""]]}, {"id": "2010.07972", "submitter": "Junjie Hu", "authors": "Junjie Hu and Melvin Johnson and Orhan Firat and Aditya Siddhant and\n  Graham Neubig", "title": "Explicit Alignment Objectives for Multilingual Bidirectional Encoders", "comments": "published at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and\nXLMR (Conneau et al., 2020) have proven to be impressively effective at\nenabling transfer-learning of NLP systems from high-resource languages to\nlow-resource languages. This success comes despite the fact that there is no\nexplicit objective to align the contextual embeddings of words/sentences with\nsimilar meanings across languages together in the same space. In this paper, we\npresent a new method for learning multilingual encoders, AMBER (Aligned\nMultilingual Bidirectional EncodeR). AMBER is trained on additional parallel\ndata using two explicit alignment objectives that align the multilingual\nrepresentations at different granularities. We conduct experiments on zero-shot\ncross-lingual transfer learning for different tasks including sequence tagging,\nsentence retrieval and sentence classification. Experimental results show that\nAMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to\n27.3 average accuracy on retrieval over the XLMR-large model which has 3.2x the\nparameters of AMBER. Our code and models are available at\nhttp://github.com/junjiehu/amber.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 18:34:13 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 01:57:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Junjie", ""], ["Johnson", "Melvin", ""], ["Firat", "Orhan", ""], ["Siddhant", "Aditya", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.07987", "submitter": "Nadezhda Chirkova", "authors": "Nadezhda Chirkova, Sergey Troshin", "title": "Empirical Study of Transformers for Source Code", "comments": "Published at the ACM Joint European Software Engineering Conference\n  and Symposium on the Foundations of Software Engineering 2021 (ESEC/FSE'21)", "journal-ref": null, "doi": "10.1145/3468264.3468611", "report-no": null, "categories": "cs.LG cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially developed for natural language processing (NLP), Transformers are\nnow widely used for source code processing, due to the format similarity\nbetween source code and text. In contrast to natural language, source code is\nstrictly structured, i.e., it follows the syntax of the programming language.\nSeveral recent works develop Transformer modifications for capturing syntactic\ninformation in source code. The drawback of these works is that they do not\ncompare to each other and consider different tasks. In this work, we conduct a\nthorough empirical study of the capabilities of Transformers to utilize\nsyntactic information in different tasks. We consider three tasks (code\ncompletion, function naming and bug fixing) and re-implement different\nsyntax-capturing modifications in a unified framework. We show that\nTransformers are able to make meaningful predictions based purely on syntactic\ninformation and underline the best practices of taking the syntactic\ninformation into account for improving the performance of the model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:09:15 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 11:32:30 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chirkova", "Nadezhda", ""], ["Troshin", "Sergey", ""]]}, {"id": "2010.07988", "submitter": "Harish Tayyar Madabushi PhD", "authors": "Calum Perrio and Harish Tayyar Madabushi", "title": "CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets --\n  RoBERTa Ensembles and The Continued Relevance of Handcrafted Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our submission to Task 2 of the Workshop on Noisy\nUser-generated Text. We explore improving the performance of a pre-trained\ntransformer-based language model fine-tuned for text classification through an\nensemble implementation that makes use of corpus level information and a\nhandcrafted feature. We test the effectiveness of including the aforementioned\nfeatures in accommodating the challenges of a noisy data set centred on a\nspecific subject outside the remit of the pre-training data. We show that\ninclusion of additional features can improve classification results and achieve\na score within 2 points of the top performing team.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:12:52 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Perrio", "Calum", ""], ["Madabushi", "Harish Tayyar", ""]]}, {"id": "2010.07999", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "What is More Likely to Happen Next? Video-and-Language Future Event\n  Prediction", "comments": "EMNLP 2020 (17 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video with aligned dialogue, people can often infer what is more\nlikely to happen next. Making such predictions requires not only a deep\nunderstanding of the rich dynamics underlying the video and dialogue, but also\na significant amount of commonsense knowledge. In this work, we explore whether\nAI models are able to learn to make such multimodal commonsense next-event\npredictions. To support research in this direction, we collect a new dataset,\nnamed Video-and-Language Event Prediction (VLEP), with 28,726 future event\nprediction examples (along with their rationales) from 10,234 diverse TV Show\nand YouTube Lifestyle Vlog video clips. In order to promote the collection of\nnon-trivial challenging examples, we employ an adversarial\nhuman-and-model-in-the-loop data collection procedure. We also present a strong\nbaseline incorporating information from video, dialogue, and commonsense\nknowledge. Experiments show that each type of information is useful for this\nchallenging task, and that compared to the high human performance on VLEP, our\nmodel provides a good starting point but leaves large room for future work. Our\ndataset and code are available at:\nhttps://github.com/jayleicn/VideoLanguageFuturePred\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:56:47 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.08014", "submitter": "Zi-Yi Dou", "authors": "Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig", "title": "GSum: A General Framework for Guided Neural Abstractive Summarization", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural abstractive summarization models are flexible and can produce coherent\nsummaries, but they are sometimes unfaithful and can be difficult to control.\nWhile previous studies attempt to provide different types of guidance to\ncontrol the output and increase faithfulness, it is not clear how these\nstrategies compare and contrast to each other. In this paper, we propose a\ngeneral and extensible guided summarization framework (GSum) that can\neffectively take different kinds of external guidance as input, and we perform\nexperiments across several different varieties. Experiments demonstrate that\nthis model is effective, achieving state-of-the-art performance according to\nROUGE on 4 popular summarization datasets when using highlighted sentences as\nguidance. In addition, we show that our guided model can generate more faithful\nsummaries and demonstrate how different types of guidance generate\nqualitatively different summaries, lending a degree of controllability to the\nlearned models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 20:46:14 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:13:45 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 09:39:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dou", "Zi-Yi", ""], ["Liu", "Pengfei", ""], ["Hayashi", "Hiroaki", ""], ["Jiang", "Zhengbao", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.08021", "submitter": "Udit Arora", "authors": "Aman Khullar, Udit Arora", "title": "MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical\n  Attention", "comments": "To appear in the first EMNLP Workshop on NLP Beyond Text, 2020. Aman\n  Khullar and Udit Arora have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MAST, a new model for Multimodal Abstractive Text\nSummarization that utilizes information from all three modalities -- text,\naudio and video -- in a multimodal video. Prior work on multimodal abstractive\ntext summarization only utilized information from the text and video\nmodalities. We examine the usefulness and challenges of deriving information\nfrom the audio modality and present a sequence-to-sequence trimodal\nhierarchical attention-based model that overcomes these challenges by letting\nthe model pay more attention to the text modality. MAST outperforms the current\nstate of the art model (video-text) by 2.51 points in terms of Content F1 score\nand 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal\nlanguage understanding.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:08:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Khullar", "Aman", ""], ["Arora", "Udit", ""]]}, {"id": "2010.08067", "submitter": "Gene Louis Kim", "authors": "Gene Louis Kim and Aaron Steven White", "title": "Montague Grammar Induction", "comments": "18 pages, 2 figures, to be published in SALT 30", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational modeling framework for inducing combinatory\ncategorial grammars from arbitrary behavioral data. This framework provides the\nanalyst fine-grained control over the assumptions that the induced grammar\nshould conform to: (i) what the primitive types are; (ii) how complex types are\nconstructed; (iii) what set of combinators can be used to combine types; and\n(iv) whether (and to what) the types of some lexical items should be fixed. In\na proof-of-concept experiment, we deploy our framework for use in\ndistributional analysis. We focus on the relationship between\ns(emantic)-selection and c(ategory)-selection, using as input a lexicon-scale\nacceptability judgment dataset focused on English verbs' syntactic distribution\n(the MegaAcceptability dataset) and enforcing standard assumptions from the\nsemantics literature on the induced grammar.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 23:25:01 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Kim", "Gene Louis", ""], ["White", "Aaron Steven", ""]]}, {"id": "2010.08090", "submitter": "Chelsea Tanchip", "authors": "Chelsea Tanchip, Lei Yu, Aotao Xu, Yang Xu", "title": "Inferring symmetry in natural language", "comments": "10 pages, 4 figures, Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodological framework for inferring symmetry of verb\npredicates in natural language. Empirical work on predicate symmetry has taken\ntwo main approaches. The feature-based approach focuses on linguistic features\npertaining to symmetry. The context-based approach denies the existence of\nabsolute symmetry but instead argues that such inference is context dependent.\nWe develop methods that formalize these approaches and evaluate them against a\nnovel symmetry inference sentence (SIS) dataset comprised of 400 naturalistic\nusages of literature-informed verbs spanning the spectrum of\nsymmetry-asymmetry. Our results show that a hybrid transfer learning model that\nintegrates linguistic features with contextualized language models most\nfaithfully predicts the empirical data. Our work integrates existing approaches\nto symmetry in natural language and suggests how symmetry inference can improve\nsystematicity in state-of-the-art language models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 01:25:01 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Tanchip", "Chelsea", ""], ["Yu", "Lei", ""], ["Xu", "Aotao", ""], ["Xu", "Yang", ""]]}, {"id": "2010.08114", "submitter": "Lingbing Guo", "authors": "Lingbing Guo, Weiqing Wang, Zequn Sun, Chenghao Liu, Wei Hu", "title": "Decentralized Knowledge Graph Representation Learning", "comments": "submitted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph (KG) representation learning methods have achieved\ncompetitive performance in many KG-oriented tasks, among which the best ones\nare usually based on graph neural networks (GNNs), a powerful family of\nnetworks that learns the representation of an entity by aggregating the\nfeatures of its neighbors and itself. However, many KG representation learning\nscenarios only provide the structure information that describes the\nrelationships among entities, causing that entities have no input features. In\nthis case, existing aggregation mechanisms are incapable of inducing embeddings\nof unseen entities as these entities have no pre-defined features for\naggregation. In this paper, we present a decentralized KG representation\nlearning approach, decentRL, which encodes each entity from and only from the\nembeddings of its neighbors. For optimization, we design an algorithm to\ndistill knowledge from the model itself such that the output embeddings can\ncontinuously gain knowledge from the corresponding original embeddings.\nExtensive experiments show that the proposed approach performed better than\nmany cutting-edge models on the entity alignment task, and achieved competitive\nperformance on the entity prediction task. Furthermore, under the inductive\nsetting, it significantly outperformed all baselines on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:31:22 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Guo", "Lingbing", ""], ["Wang", "Weiqing", ""], ["Sun", "Zequn", ""], ["Liu", "Chenghao", ""], ["Hu", "Wei", ""]]}, {"id": "2010.08178", "submitter": "Xuanfu Wu", "authors": "Xuanfu Wu, Yang Feng, Chenze Shao", "title": "Generating Diverse Translation from Model Distribution with Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the improvement of translation quality, neural machine translation\n(NMT) often suffers from the lack of diversity in its generation. In this\npaper, we propose to generate diverse translations by deriving a large number\nof possible models with Bayesian modelling and sampling models from them for\ninference. The possible models are obtained by applying concrete dropout to the\nNMT model and each of them has specific confidence for its prediction, which\ncorresponds to a posterior model distribution under specific training data in\nthe principle of Bayesian modeling. With variational inference, the posterior\nmodel distribution can be approximated with a variational distribution, from\nwhich the final models for inference are sampled. We conducted experiments on\nChinese-English and English-German translation tasks and the results shows that\nour method makes a better trade-off between diversity and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 05:50:00 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wu", "Xuanfu", ""], ["Feng", "Yang", ""], ["Shao", "Chenze", ""]]}, {"id": "2010.08185", "submitter": "Tanfang Chen", "authors": "Tanfang Chen, Weiwei Wang, Wenyang Wei, Xing Shi, Xiangang Li, Jieping\n  Ye, Kevin Knight", "title": "DiDi's Machine Translation System for WMT2020", "comments": "Accepted at WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes DiDi AI Labs' submission to the WMT2020 news translation\nshared task. We participate in the translation direction of Chinese->English.\nIn this direction, we use the Transformer as our baseline model, and integrate\nseveral techniques for model enhancement, including data filtering, data\nselection, back-translation, fine-tuning, model ensembling, and re-ranking. As\na result, our submission achieves a BLEU score of $36.6$ in Chinese->English.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:25:48 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Chen", "Tanfang", ""], ["Wang", "Weiwei", ""], ["Wei", "Wenyang", ""], ["Shi", "Xing", ""], ["Li", "Xiangang", ""], ["Ye", "Jieping", ""], ["Knight", "Kevin", ""]]}, {"id": "2010.08187", "submitter": "Guang-Neng Hu", "authors": "Guangneng Hu, Qiang Yang", "title": "PrivNet: Safeguarding Private Attributes in Transfer Learning for\n  Recommendation", "comments": "Findings of EMNLP 2020", "journal-ref": "Findings of ACL: EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is an effective technique to improve a target recommender\nsystem with the knowledge from a source domain. Existing research focuses on\nthe recommendation performance of the target domain while ignores the privacy\nleakage of the source domain. The transferred knowledge, however, may\nunintendedly leak private information of the source domain. For example, an\nattacker can accurately infer user demographics from their historical purchase\nprovided by a source domain data owner. This paper addresses the above\nprivacy-preserving issue by learning a privacy-aware neural representation by\nimproving target performance while protecting source privacy. The key idea is\nto simulate the attacks during the training for protecting unseen users'\nprivacy in the future, modeled by an adversarial game, so that the transfer\nlearning model becomes robust to attacks. Experiments show that the proposed\nPrivNet model can successfully disentangle the knowledge benefitting the\ntransfer from leaking the privacy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:33:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hu", "Guangneng", ""], ["Yang", "Qiang", ""]]}, {"id": "2010.08191", "submitter": "Jing Liu", "authors": "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin\n  Zhao, Daxiang Dong, Hua Wu, Haifeng Wang", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open-domain question answering, dense passage retrieval has become a new\nparadigm to retrieve relevant passages for finding answers. Typically, the\ndual-encoder architecture is adopted to learn dense representations of\nquestions and passages for semantic matching. However, it is difficult to\neffectively train a dual-encoder due to the challenges including the\ndiscrepancy between training and inference, the existence of unlabeled\npositives and limited training data. To address these challenges, we propose an\noptimized training approach, called RocketQA, to improving dense passage\nretrieval. We make three major technical contributions in RocketQA, namely\ncross-batch negatives, denoised hard negatives and data augmentation. The\nexperiment results show that RocketQA significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions. We also conduct\nextensive experiments to examine the effectiveness of the three strategies in\nRocketQA. Besides, we demonstrate that the performance of end-to-end QA can be\nimproved based on our RocketQA retriever.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:54:05 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 07:52:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Qu", "Yingqi", ""], ["Ding", "Yuchen", ""], ["Liu", "Jing", ""], ["Liu", "Kai", ""], ["Ren", "Ruiyang", ""], ["Zhao", "Wayne Xin", ""], ["Dong", "Daxiang", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2010.08197", "submitter": "Boyan Wan", "authors": "Boyan Wan, Zhuo Tang, Li Yang", "title": "Lexicon-constrained Copying Network for Chinese Abstractive\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy mechanism allows sequence-to-sequence models to choose words from the\ninput and put them directly into the output, which is finding increasing use in\nabstractive summarization. However, since there is no explicit delimiter in\nChinese sentences, most existing models for Chinese abstractive summarization\ncan only perform character copy, resulting in inefficient. To solve this\nproblem, we propose a lexicon-constrained copying network that models\nmulti-granularity in both encoder and decoder. On the source side, words and\ncharacters are aggregated into the same input memory using a Transformerbased\nencoder. On the target side, the decoder can copy either a character or a\nmulti-character word at each time step, and the decoding process is guided by a\nword-enhanced search algorithm that facilitates the parallel computation and\nencourages the model to copy more words. Moreover, we adopt a word selector to\nintegrate keyword information. Experiments results on a Chinese social media\ndataset show that our model can work standalone or with the word selector. Both\nforms can outperform previous character-based models and achieve competitive\nperformances.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:59:34 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wan", "Boyan", ""], ["Tang", "Zhuo", ""], ["Yang", "Li", ""]]}, {"id": "2010.08200", "submitter": "Guangyu Zheng", "authors": "Wanyun Cui, Guangyu Zheng, Wei Wang", "title": "Unsupervised Natural Language Inference via Decoupled Multimodal\n  Contrastive Learning", "comments": "Published at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to solve the natural language inference problem without any\nsupervision from the inference labels via task-agnostic multimodal pretraining.\nAlthough recent studies of multimodal self-supervised learning also represent\nthe linguistic and visual context, their encoders for different modalities are\ncoupled. Thus they cannot incorporate visual information when encoding plain\ntext alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled\nlearning (MACD) network. MACD forces the decoupled text encoder to represent\nthe visual information via contrastive learning. Therefore, it embeds visual\nknowledge even for plain text inference. We conducted comprehensive experiments\nover plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD\neven outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 07:12:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Cui", "Wanyun", ""], ["Zheng", "Guangyu", ""], ["Wang", "Wei", ""]]}, {"id": "2010.08210", "submitter": "Xue Mengge", "authors": "Mengge Xue, Bowen Yu, Zhenyu Zhang, Tingwen Liu, Yue Zhang, Bin Wang", "title": "Coarse-to-Fine Pre-training for Named Entity Recognition", "comments": null, "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More recently, Named Entity Recognition hasachieved great advances aided by\npre-trainingapproaches such as BERT. However, currentpre-training techniques\nfocus on building lan-guage modeling objectives to learn a gen-eral\nrepresentation, ignoring the named entity-related knowledge. To this end, we\nproposea NER-specific pre-training framework to in-ject coarse-to-fine\nautomatically mined entityknowledge into pre-trained models. Specifi-cally, we\nfirst warm-up the model via an en-tity span identification task by training it\nwithWikipedia anchors, which can be deemed asgeneral-typed entities. Then we\nleverage thegazetteer-based distant supervision strategy totrain the model\nextract coarse-grained typedentities. Finally, we devise a\nself-supervisedauxiliary task to mine the fine-grained namedentity knowledge\nvia clustering.Empiricalstudies on three public NER datasets demon-strate that\nour framework achieves significantimprovements against several pre-trained\nbase-lines, establishing the new state-of-the-art per-formance on three\nbenchmarks. Besides, weshow that our framework gains promising re-sults without\nusing human-labeled trainingdata, demonstrating its effectiveness in label-few\nand low-resource scenarios\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 07:39:20 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Xue", "Mengge", ""], ["Yu", "Bowen", ""], ["Zhang", "Zhenyu", ""], ["Liu", "Tingwen", ""], ["Zhang", "Yue", ""], ["Wang", "Bin", ""]]}, {"id": "2010.08213", "submitter": "Yanghoon Kim", "authors": "Yanghoon Kim, Seungpil Won, Seunghyun Yoon and Kyomin Jung", "title": "Collaborative Training of GANs in Continuous and Discrete Spaces for\n  Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying generative adversarial networks (GANs) to text-related tasks is\nchallenging due to the discrete nature of language. One line of research\nresolves this issue by employing reinforcement learning (RL) and optimizing the\nnext-word sampling policy directly in a discrete action space. Such methods\ncompute the rewards from complete sentences and avoid error accumulation due to\nexposure bias. Other approaches employ approximation techniques that map the\ntext to continuous representation in order to circumvent the non-differentiable\ndiscrete process. Particularly, autoencoder-based methods effectively produce\nrobust representations that can model complex discrete structures. In this\npaper, we propose a novel text GAN architecture that promotes the collaborative\ntraining of the continuous-space and discrete-space methods. Our method employs\nan autoencoder to learn an implicit data manifold, providing a learning\nobjective for adversarial training in a continuous space. Furthermore, the\ncomplete textual output is directly evaluated and updated via RL in a discrete\nspace. The collaborative interplay between the two adversarial trainings\neffectively regularize the text representations in different spaces. The\nexperimental results on three standard benchmark datasets show that our model\nsubstantially outperforms state-of-the-art text GANs with respect to quality,\ndiversity, and global consistency.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 07:51:16 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 10:13:31 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kim", "Yanghoon", ""], ["Won", "Seungpil", ""], ["Yoon", "Seunghyun", ""], ["Jung", "Kyomin", ""]]}, {"id": "2010.08232", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Thanh Vu, Afshin Rahimi, Mai Hoang Dao, Linh The\n  Nguyen and Long Doan", "title": "WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets", "comments": "In Proceedings of the 6th Workshop on Noisy User-generated Text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide an overview of the WNUT-2020 shared task on the\nidentification of informative COVID-19 English Tweets. We describe how we\nconstruct a corpus of 10K Tweets and organize the development and evaluation\nphases for this task. In addition, we also present a brief summary of results\nobtained from the final system evaluation submissions of 55 teams, finding that\n(i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the\nmajority of the submissions achieve substantially higher results than the\nbaseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained\nlanguage models on relevant language data followed by supervised training\nperforms well in this task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:28:05 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Vu", "Thanh", ""], ["Rahimi", "Afshin", ""], ["Dao", "Mai Hoang", ""], ["Nguyen", "Linh The", ""], ["Doan", "Long", ""]]}, {"id": "2010.08240", "submitter": "Nandan Thakur", "authors": "Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych", "title": "Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for\n  Pairwise Sentence Scoring Tasks", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There are two approaches for pairwise sentence scoring: Cross-encoders, which\nperform full-attention over the input pair, and Bi-encoders, which map each\ninput independently to a dense vector space. While cross-encoders often achieve\nhigher performance, they are too slow for many practical use cases.\nBi-encoders, on the other hand, require substantial training data and\nfine-tuning over the target task to achieve competitive performance. We present\na simple yet efficient data augmentation strategy called Augmented SBERT, where\nwe use the cross-encoder to label a larger set of input pairs to augment the\ntraining data for the bi-encoder. We show that, in this process, selecting the\nsentence pairs is non-trivial and crucial for the success of the method. We\nevaluate our approach on multiple tasks (in-domain) as well as on a domain\nadaptation task. Augmented SBERT achieves an improvement of up to 6 points for\nin-domain and of up to 37 points for domain adaptation tasks compared to the\noriginal bi-encoder performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:43:27 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 10:02:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Thakur", "Nandan", ""], ["Reimers", "Nils", ""], ["Daxenberger", "Johannes", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.08242", "submitter": "Shusheng Xu", "authors": "Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei and Ming Zhou", "title": "Unsupervised Extractive Summarization by Pre-training Hierarchical\n  Transformers", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised extractive document summarization aims to select important\nsentences from a document without using labeled summaries during training.\nExisting methods are mostly graph-based with sentences as nodes and edge\nweights measured by sentence similarities. In this work, we find that\ntransformer attentions can be used to rank sentences for unsupervised\nextractive summarization. Specifically, we first pre-train a hierarchical\ntransformer model using unlabeled documents only. Then we propose a method to\nrank sentences using sentence-level self-attentions and pre-training\nobjectives. Experiments on CNN/DailyMail and New York Times datasets show our\nmodel achieves state-of-the-art performance on unsupervised summarization. We\nalso find in experiments that our model is less dependent on sentence\npositions. When using a linear combination of our model and a recent\nunsupervised model explicitly modeling sentence positions, we obtain even\nbetter results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:44:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Xu", "Shusheng", ""], ["Zhang", "Xingxing", ""], ["Wu", "Yi", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "2010.08246", "submitter": "Johannes Bjerva", "authors": "Johannes Bjerva and Elizabeth Salesky and Sabrina J. Mielke and Aditi\n  Chaudhary and Giuseppe G. A. Celano and Edoardo M. Ponti and Ekaterina\n  Vylomova and Ryan Cotterell and Isabelle Augenstein", "title": "SIGTYP 2020 Shared Task: Prediction of Typological Features", "comments": "SigTyp 2020 Shared Task Description Paper @ EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013)\ncontain information about linguistic properties of the world's languages. They\nhave been shown to be useful for downstream applications, including\ncross-lingual transfer learning and linguistic probing. A major drawback\nhampering broader adoption of typological KBs is that they are sparsely\npopulated, in the sense that most languages only have annotations for some\nfeatures, and skewed, in that few features have wide coverage. As typological\nfeatures often correlate with one another, it is possible to predict them and\nthus automatically populate typological KBs, which is also the focus of this\nshared task. Overall, the task attracted 8 submissions from 5 teams, out of\nwhich the most successful methods make use of such feature correlations.\nHowever, our error analysis reveals that even the strongest submitted systems\nstruggle with predicting feature values for languages where few features are\nknown.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:47:24 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 07:29:45 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bjerva", "Johannes", ""], ["Salesky", "Elizabeth", ""], ["Mielke", "Sabrina J.", ""], ["Chaudhary", "Aditi", ""], ["Celano", "Giuseppe G. A.", ""], ["Ponti", "Edoardo M.", ""], ["Vylomova", "Ekaterina", ""], ["Cotterell", "Ryan", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2010.08265", "submitter": "Qiang Wang", "authors": "Qiang Wang, Tong Xiao, Jingbo Zhu", "title": "Training Flexible Depth Model by Multi-Task Learning for Neural Machine\n  Translation", "comments": "Accepted at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard neural machine translation model can only decode with the same\ndepth configuration as training. Restricted by this feature, we have to deploy\nmodels of various sizes to maintain the same translation latency, because the\nhardware conditions on different terminal devices (e.g., mobile phones) may\nvary greatly. Such individual training leads to increased model maintenance\ncosts and slower model iterations, especially for the industry. In this work,\nwe propose to use multi-task learning to train a flexible depth model that can\nadapt to different depth configurations during inference. Experimental results\nshow that our approach can simultaneously support decoding in 24 depth\nconfigurations and is superior to the individual training and another flexible\ndepth model training method -- LayerDrop.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:37:27 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wang", "Qiang", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""]]}, {"id": "2010.08269", "submitter": "Mark Berger", "authors": "Mark Berger, Jakub Zavrel, Paul Groth", "title": "Effective Distributed Representations for Academic Expert Search", "comments": "To be published in the Scholarly Document Processing 2020 Workshop @\n  EMNLP 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert search aims to find and rank experts based on a user's query. In\nacademia, retrieving experts is an efficient way to navigate through a large\namount of academic knowledge. Here, we study how different distributed\nrepresentations of academic papers (i.e. embeddings) impact academic expert\nretrieval. We use the Microsoft Academic Graph dataset and experiment with\ndifferent configurations of a document-centric voting model for retrieval. In\nparticular, we explore the impact of the use of contextualized embeddings on\nsearch performance. We also present results for paper embeddings that\nincorporate citation information through retrofitting. Additionally,\nexperiments are conducted using different techniques for assigning author\nweights based on author order. We observe that using contextual embeddings\nproduced by a transformer model trained for sentence similarity tasks produces\nthe most effective paper representations for document-centric expert retrieval.\nHowever, retrofitting the paper embeddings and using elaborate author\ncontribution weighting strategies did not improve retrieval performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:43:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Berger", "Mark", ""], ["Zavrel", "Jakub", ""], ["Groth", "Paul", ""]]}, {"id": "2010.08275", "submitter": "Hila Gonen", "authors": "Hila Gonen, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg", "title": "It's not Greek to mBERT: Inducing Word-Level Translations from\n  Multilingual BERT", "comments": "BlackboxNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated that multilingual BERT (mBERT) learns rich\ncross-lingual representations, that allow for transfer across languages. We\nstudy the word-level translation information embedded in mBERT and present two\nsimple methods that expose remarkable translation capabilities with no\nfine-tuning. The results suggest that most of this information is encoded in a\nnon-linear way, while some of it can also be recovered with purely linear\ntools. As part of our analysis, we test the hypothesis that mBERT learns\nrepresentations which contain both a language-encoding component and an\nabstract, cross-lingual component, and explicitly identify an empirical\nlanguage-identity subspace within mBERT representations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:49:32 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Gonen", "Hila", ""], ["Ravfogel", "Shauli", ""], ["Elazar", "Yanai", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2010.08318", "submitter": "Andrew Moore", "authors": "Andrew Moore and Jeremy Barnes", "title": "Multi-task Learning of Negation and Speculation for Targeted Sentiment\n  Classification", "comments": "To appear at NAACL 2021 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of work in targeted sentiment analysis has concentrated on\nfinding better methods to improve the overall results. Within this paper we\nshow that these models are not robust to linguistic phenomena, specifically\nnegation and speculation. In this paper, we propose a multi-task learning\nmethod to incorporate information from syntactic and semantic auxiliary tasks,\nincluding negation and speculation scope detection, to create English-language\nmodels that are more robust to these phenomena. Further we create two challenge\ndatasets to evaluate model performance on negated and speculative samples. We\nfind that multi-task models and transfer learning via language modelling can\nimprove performance on these challenge datasets, but the overall performances\nindicate that there is still much room for improvement. We release both the\ndatasets and the source code at\nhttps://github.com/jerbarnes/multitask_negation_for_targeted_sentiment.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:20:03 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 08:17:40 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Moore", "Andrew", ""], ["Barnes", "Jeremy", ""]]}, {"id": "2010.08319", "submitter": "Tim Nugent", "authors": "Tim Nugent, Nicole Stelea and Jochen L. Leidner", "title": "Detecting ESG topics using domain-specific language models and data\n  augmentation approaches", "comments": "11 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in deep learning-based language modelling, many\nnatural language processing (NLP) tasks in the financial domain remain\nchallenging due to the paucity of appropriately labelled data. Other issues\nthat can limit task performance are differences in word distribution between\nthe general corpora - typically used to pre-train language models - and\nfinancial corpora, which often exhibit specialized language and symbology.\nHere, we investigate two approaches that may help to mitigate these issues.\nFirstly, we experiment with further language model pre-training using large\namounts of in-domain data from business and financial news. We then apply\naugmentation approaches to increase the size of our dataset for model\nfine-tuning. We report our findings on an Environmental, Social and Governance\n(ESG) controversies dataset and demonstrate that both approaches are beneficial\nto accuracy in classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:20:07 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nugent", "Tim", ""], ["Stelea", "Nicole", ""], ["Leidner", "Jochen L.", ""]]}, {"id": "2010.08323", "submitter": "Kuldeep Singh", "authors": "Saeedeh Shekarpour, Abhishek Nadgeri and Kuldeep Singh", "title": "QA2Explanation: Generating and Evaluating Explanations for Question\n  Answering Systems over Knowledge Graph", "comments": "Accepted in IntEx-SemPar: Interactive and Executable Semantic Parsing\n  EMNLP 2020 Workshop", "journal-ref": "EMNLP2020 Workshop:IntEx-SemPar", "doi": "10.18653/v1/P17", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Big Knowledge Graphs, Question Answering (QA) systems have\nreached a milestone in their performance and feasibility. However, their\napplicability, particularly in specific domains such as the biomedical domain,\nhas not gained wide acceptance due to their \"black box\" nature, which hinders\ntransparency, fairness, and accountability of QA systems. Therefore, users are\nunable to understand how and why particular questions have been answered,\nwhereas some others fail. To address this challenge, in this paper, we develop\nan automatic approach for generating explanations during various stages of a\npipeline-based QA system. Our approach is a supervised and automatic approach\nwhich considers three classes (i.e., success, no answer, and wrong answer) for\nannotating the output of involved QA components. Upon our prediction, a\ntemplate explanation is chosen and integrated into the output of the\ncorresponding component. To measure the effectiveness of the approach, we\nconducted a user survey as to how non-expert users perceive our generated\nexplanations. The results of our study show a significant increase in the four\ndimensions of the human factor from the Human-computer interaction community.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:32:12 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Shekarpour", "Saeedeh", ""], ["Nadgeri", "Abhishek", ""], ["Singh", "Kuldeep", ""]]}, {"id": "2010.08346", "submitter": "Vili H\\\"at\\\"onen", "authors": "Vili H\\\"at\\\"onen and Fiona Melzer", "title": "From Talk to Action with Accountability: Monitoring the Public\n  Discussion of Policy Makers with Deep Neural Networks and Topic Modelling", "comments": "Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decades of research on climate have provided a consensus that human activity\nhas changed the climate and we are currently heading into a climate crisis.\nWhile public discussion and research efforts on climate change mitigation have\nincreased, potential solutions need to not only be discussed but also\neffectively deployed. For preventing mismanagement and holding policy makers\naccountable, transparency and degree of information about government processes\nhave been shown to be crucial. However, currently the quantity of information\nabout climate change discussions and the range of sources make it increasingly\ndifficult for the public and civil society to maintain an overview to hold\npoliticians accountable.\n  In response, we propose a multi-source topic aggregation system (MuSTAS)\nwhich processes policy makers speech and rhetoric from several publicly\navailable sources into an easily digestible topic summary. MuSTAS uses novel\nmulti-source hybrid latent Dirichlet allocation to model topics from a variety\nof documents. This topic digest will serve the general public and civil society\nin assessing where, how, and when politicians talk about climate and climate\npolicies, enabling them to hold politicians accountable for their actions to\nmitigate climate change and lack thereof.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 12:21:01 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 13:17:37 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 16:12:22 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["H\u00e4t\u00f6nen", "Vili", ""], ["Melzer", "Fiona", ""]]}, {"id": "2010.08412", "submitter": "Rumen Dangovski", "authors": "Matthew Khoury and Rumen Dangovski and Longwu Ou and Preslav Nakov and\n  Yichen Shen and Li Jing", "title": "Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for\n  Low-Latency Inference in NLP Applications", "comments": "To appear at the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP '20), November 16-20, 2020, NMT, AI accelerators,\n  co-design, TPU, OPU, 10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become the standard approach to building reliable\nNatural Language Processing (NLP) applications, ranging from Neural Machine\nTranslation (NMT) to dialogue systems. However, improving accuracy by\nincreasing the model size requires a large number of hardware computations,\nwhich can slow down NLP applications significantly at inference time. To\naddress this issue, we propose a novel vector-vector-matrix architecture\n(VVMA), which greatly reduces the latency at inference time for NMT. This\narchitecture takes advantage of specialized hardware that has low-latency\nvector-vector operations and higher-latency vector-matrix operations. It also\nreduces the number of parameters and FLOPs for virtually all models that rely\non efficient matrix multipliers without significantly impacting accuracy. We\npresent empirical results suggesting that our framework can reduce the latency\nof sequence-to-sequence and Transformer models used for NMT by a factor of\nfour. Finally, we show evidence suggesting that our VVMA extends to other\ndomains, and we discuss novel hardware for its efficient use.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:54:08 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Khoury", "Matthew", ""], ["Dangovski", "Rumen", ""], ["Ou", "Longwu", ""], ["Nakov", "Preslav", ""], ["Shen", "Yichen", ""], ["Jing", "Li", ""]]}, {"id": "2010.08422", "submitter": "Wissam Siblini", "authors": "Wissam Siblini, Mohamed Challal and Charlotte Pasqual", "title": "Delaying Interaction Layers in Transformer-based Encoders for Efficient\n  Open Domain Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Domain Question Answering (ODQA) on a large-scale corpus of documents\n(e.g. Wikipedia) is a key challenge in computer science. Although\ntransformer-based language models such as Bert have shown on SQuAD the ability\nto surpass humans for extracting answers in small passages of text, they suffer\nfrom their high complexity when faced to a much larger search space. The most\ncommon way to tackle this problem is to add a preliminary Information Retrieval\nstep to heavily filter the corpus and only keep the relevant passages. In this\npaper, we propose a more direct and complementary solution which consists in\napplying a generic change in the architecture of transformer-based models to\ndelay the attention between subparts of the input and allow a more efficient\nmanagement of computations. The resulting variants are competitive with the\noriginal models on the extractive task and allow, on the ODQA setting, a\nsignificant speedup and even a performance improvement in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:36:38 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Siblini", "Wissam", ""], ["Challal", "Mohamed", ""], ["Pasqual", "Charlotte", ""]]}, {"id": "2010.08432", "submitter": "Haozhou Wang", "authors": "Haozhou Wang, James Henderson, Paola Merlo", "title": "Multi-Adversarial Learning for Cross-Lingual Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have succeeded in inducing\ncross-lingual word embeddings -- maps of matching words across languages --\nwithout supervision. Despite these successes, GANs' performance for the\ndifficult case of distant languages is still not satisfactory. These\nlimitations have been explained by GANs' incorrect assumption that source and\ntarget embedding spaces are related by a single linear mapping and are\napproximately isomorphic. We assume instead that, especially across distant\nlanguages, the mapping is only piece-wise linear, and propose a\nmulti-adversarial learning method. This novel method induces the seed\ncross-lingual dictionary through multiple mappings, each induced to fit the\nmapping for one subspace. Our experiments on unsupervised bilingual lexicon\ninduction show that this method improves performance over previous\nsingle-mapping methods, especially for distant languages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:54:28 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wang", "Haozhou", ""], ["Henderson", "James", ""], ["Merlo", "Paola", ""]]}, {"id": "2010.08433", "submitter": "Andrey Kormilitzin", "authors": "Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Hao Ni, Goran Nenadic,\n  Alejo Nevado-Holgado", "title": "An efficient representation of chronological events in medical texts", "comments": "4 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we addressed the problem of capturing sequential information\ncontained in longitudinal electronic health records (EHRs). Clinical notes,\nwhich is a particular type of EHR data, are a rich source of information and\npractitioners often develop clever solutions how to maximise the sequential\ninformation contained in free-texts. We proposed a systematic methodology for\nlearning from chronological events available in clinical notes. The proposed\nmethodological {\\it path signature} framework creates a non-parametric\nhierarchical representation of sequential events of any type and can be used as\nfeatures for downstream statistical learning tasks. The methodology was\ndeveloped and externally validated using the largest in the UK secondary care\nmental health EHR data on a specific task of predicting survival risk of\npatients diagnosed with Alzheimer's disease. The signature-based model was\ncompared to a common survival random forest model. Our results showed a\n15.4$\\%$ increase of risk prediction AUC at the time point of 20 months after\nthe first admission to a specialist memory clinic and the signature method\noutperformed the baseline mixed-effects model by 13.2 $\\%$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:54:29 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:52:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kormilitzin", "Andrey", ""], ["Vaci", "Nemanja", ""], ["Liu", "Qiang", ""], ["Ni", "Hao", ""], ["Nenadic", "Goran", ""], ["Nevado-Holgado", "Alejo", ""]]}, {"id": "2010.08518", "submitter": "Biao Zhang", "authors": "Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich", "title": "Adaptive Feature Selection for End-to-End Speech Translation", "comments": "EMNLP2020 Findings; source code is at\n  https://github.com/bzhangGo/zero", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information in speech signals is not evenly distributed, making it an\nadditional challenge for end-to-end (E2E) speech translation (ST) to learn to\nfocus on informative features. In this paper, we propose adaptive feature\nselection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR\nencoder and apply AFS to dynamically estimate the importance of each encoded\nspeech feature to SR. A ST encoder, stacked on top of the ASR encoder, then\nreceives the filtered features from the (frozen) ASR encoder. We take L0DROP\n(Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech\nfeatures with respect to both temporal and feature dimensions. Results on\nLibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of\nST by pruning out ~84% temporal features, yielding an average translation gain\nof ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces\nthe performance gap compared to the cascade baseline, and outperforms it on\nLibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation)\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:21:00 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:53:39 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zhang", "Biao", ""], ["Titov", "Ivan", ""], ["Haddow", "Barry", ""], ["Sennrich", "Rico", ""]]}, {"id": "2010.08525", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Muhao Chen, Haoyu Wang, Yangqiu Song, Dan Roth", "title": "Analogous Process Structure Induction for Sub-event Sequence Prediction", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational and cognitive studies of event understanding suggest that\nidentifying, comprehending, and predicting events depend on having structured\nrepresentations of a sequence of events and on conceptualizing (abstracting)\nits components into (soft) event categories. Thus, knowledge about a known\nprocess such as \"buying a car\" can be used in the context of a new but\nanalogous process such as \"buying a house\". Nevertheless, most event\nunderstanding work in NLP is still at the ground level and does not consider\nabstraction. In this paper, we propose an Analogous Process Structure Induction\nAPSI framework, which leverages analogies among processes and conceptualization\nof sub-event instances to predict the whole sub-event sequence of previously\nunseen open-domain processes. As our experiments and analysis indicate, APSI\nsupports the generation of meaningful sub-event sequences for unseen processes\nand can help predict missing events.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:35:40 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhang", "Hongming", ""], ["Chen", "Muhao", ""], ["Wang", "Haoyu", ""], ["Song", "Yangqiu", ""], ["Roth", "Dan", ""]]}, {"id": "2010.08540", "submitter": "Kyle Gorman", "authors": "Angie Waller and Kyle Gorman", "title": "Detecting Objectifying Language in Online Professor Reviews", "comments": "To appear at the 6th Workshop on Noisy User-generated Text, a\n  workshop of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student reviews often make reference to professors' physical appearances.\nUntil recently RateMyProfessors.com, the website of this study's focus, used a\ndesign feature to encourage a \"hot or not\" rating of college professors. In the\nwake of recent #MeToo and #TimesUp movements, social awareness of the\ninappropriateness of these reviews has grown; however, objectifying comments\nremain and continue to be posted in this online context. We describe two\nsupervised text classifiers for detecting objectifying commentary in professor\nreviews. We then ensemble these classifiers and use the resulting model to\ntrack objectifying commentary at scale. We measure correlations between\nobjectifying commentary, changes to the review website interface, and teacher\ngender across a ten-year period.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:49:59 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Waller", "Angie", ""], ["Gorman", "Kyle", ""]]}, {"id": "2010.08542", "submitter": "Adrian de Wynter", "authors": "Adrian de Wynter", "title": "Mischief: A Simple Black-Box Attack Against Transformer Architectures", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Mischief, a simple and lightweight method to produce a class of\nhuman-readable, realistic adversarial examples for language models. We perform\nexhaustive experimentations of our algorithm on four transformer-based\narchitectures, across a variety of downstream tasks, as well as under varying\nconcentrations of said examples. Our findings show that the presence of\nMischief-generated adversarial samples in the test set significantly degrades\n(by up to $20\\%$) the performance of these models with respect to their\nreported baselines. Nonetheless, we also demonstrate that, by including similar\nexamples in the training set, it is possible to restore the baseline scores on\nthe adversarial test set. Moreover, for certain tasks, the models trained with\nMischief set show a modest increase on performance with respect to their\noriginal, non-adversarial baseline.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:52:06 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["de Wynter", "Adrian", ""]]}, {"id": "2010.08566", "submitter": "Peter West", "authors": "Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena Hwang,\n  Yejin Choi", "title": "Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publicly available, large pretrained LanguageModels (LMs) generate text with\nremarkable quality, but only sequentially from left to right. As a result, they\nare not immediately applicable to generation tasks that break the\nunidirectional assumption, such as paraphrasing or text-infilling,\nnecessitating task-specific supervision.\n  In this paper, we present Reflective Decoding, a novel unsupervised algorithm\nthat allows for direct application of unidirectional LMs to non-sequential\ntasks. Our 2-step approach requires no supervision or even parallel corpora,\nonly two off-the-shelf pretrained LMs in opposite directions: forward and\nbackward. First, in the contextualization step, we use LMs to generate\nensembles of past and future contexts which collectively capture the input\n(e.g. the source sentence for paraphrasing). Second, in the reflection step, we\ncondition on these \"context ensembles\", generating outputs that are compatible\nwith them. Comprehensive empirical results demonstrate that Reflective Decoding\noutperforms strong unsupervised baselines on both paraphrasing and abductive\ntext infilling, significantly narrowing the gap between unsupervised and\nsupervised methods. Reflective Decoding surpasses multiple supervised baselines\non various metrics including human evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:02:07 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 20:58:15 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 19:45:19 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["West", "Peter", ""], ["Lu", "Ximing", ""], ["Holtzman", "Ari", ""], ["Bhagavatula", "Chandra", ""], ["Hwang", "Jena", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.08570", "submitter": "Markus Leippold", "authors": "Rahul Mishra and Dhruv Gupta and Markus Leippold", "title": "Generating Fact Checking Summaries for Web Claims", "comments": "Accepted paper; The 2020 Conference on Empirical Methods in Natural\n  Language Processing EMNLP - WNUT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SUMO, a neural attention-based approach that learns to establish\nthe correctness of textual claims based on evidence in the form of text\ndocuments (e.g., news articles or Web documents). SUMO further generates an\nextractive summary by presenting a diversified set of sentences from the\ndocuments that explain its decision on the correctness of the textual claim.\nPrior approaches to address the problem of fact checking and evidence\nextraction have relied on simple concatenation of claim and document word\nembeddings as an input to claim driven attention weight computation. This is\ndone so as to extract salient words and sentences from the documents that help\nestablish the correctness of the claim. However, this design of claim-driven\nattention does not capture the contextual information in documents properly. We\nimprove on the prior art by using improved claim and title guided hierarchical\nattention to model effective contextual cues. We show the efficacy of our\napproach on datasets concerning political, healthcare, and environmental\nissues.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:10:47 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mishra", "Rahul", ""], ["Gupta", "Dhruv", ""], ["Leippold", "Markus", ""]]}, {"id": "2010.08580", "submitter": "Zeyu Liu", "authors": "Chuanrong Li, Lin Shengshuo, Leo Z. Liu, Xinyi Wu, Xuhui Zhou, Shane\n  Steinert-Threlkeld", "title": "Linguistically-Informed Transformations (LIT): A Method for\n  Automatically Generating Contrast Sets", "comments": "Appears at EMNLP BlackboxNLP Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although large-scale pretrained language models, such as BERT and RoBERTa,\nhave achieved superhuman performance on in-distribution test sets, their\nperformance suffers on out-of-distribution test sets (e.g., on contrast sets).\nBuilding contrast sets often re-quires human-expert annotation, which is\nexpensive and hard to create on a large scale. In this work, we propose a\nLinguistically-Informed Transformation (LIT) method to automatically generate\ncontrast sets, which enables practitioners to explore linguistic phenomena of\ninterests as well as compose different phenomena. Experimenting with our method\non SNLI and MNLI shows that current pretrained language models, although being\nclaimed to contain sufficient linguistic knowledge, struggle on our\nautomatically generated contrast sets. Furthermore, we improve models'\nperformance on the contrast sets by apply-ing LIT to augment the training data,\nwithout affecting performance on the original data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:23:05 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 01:39:23 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 18:16:58 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Li", "Chuanrong", ""], ["Shengshuo", "Lin", ""], ["Liu", "Leo Z.", ""], ["Wu", "Xinyi", ""], ["Zhou", "Xuhui", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "2010.08593", "submitter": "Suraj Kothawade", "authors": "Suraj Kothawade, Jiten Girdhar, Chandrashekhar Lavania, Rishabh Iyer", "title": "Deep Submodular Networks for Extractive Data Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Models are increasingly becoming prevalent in summarization problems\n(e.g. document, video and images) due to their ability to learn complex feature\ninteractions and representations. However, they do not model characteristics\nsuch as diversity, representation, and coverage, which are also very important\nfor summarization tasks. On the other hand, submodular functions naturally\nmodel these characteristics because of their diminishing returns property. Most\napproaches for modelling and learning submodular functions rely on very simple\nmodels, such as weighted mixtures of submodular functions. Unfortunately, these\nmodels only learn the relative importance of the different submodular functions\n(such as diversity, representation or importance), but cannot learn more\ncomplex feature representations, which are often required for state-of-the-art\nperformance. We propose Deep Submodular Networks (DSN), an end-to-end learning\nframework that facilitates the learning of more complex features and richer\nfunctions, crafted for better modelling of all aspects of summarization. The\nDSN framework can be used to learn features appropriate for summarization from\nscratch. We demonstrate the utility of DSNs on both generic and query focused\nimage-collection summarization, and show significant improvement over the\nstate-of-the-art. In particular, we show that DSNs outperform simple mixture\nmodels using off the shelf features. Secondly, we also show that just using\nfour submodular functions in a DSN with end-to-end learning performs comparably\nto the state-of-the-art mixture model with a hand-crafted set of 594 components\nand outperforms other methods for image collection summarization.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 19:06:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kothawade", "Suraj", ""], ["Girdhar", "Jiten", ""], ["Lavania", "Chandrashekhar", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2010.08606", "submitter": "Yiding Hao", "authors": "Yiding Hao", "title": "Evaluating Attribution Methods using White-Box LSTMs", "comments": "To appear in the Proceedings of the 2020 EMNLP Workshop BlackboxNLP:\n  Analyzing and Interpreting Neural Networks for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability methods for neural networks are difficult to evaluate\nbecause we do not understand the black-box models typically used to test them.\nThis paper proposes a framework in which interpretability methods are evaluated\nusing manually constructed networks, which we call white-box networks, whose\nbehavior is understood a priori. We evaluate five methods for producing\nattribution heatmaps by applying them to white-box LSTM classifiers for tasks\nbased on formal languages. Although our white-box classifiers solve their tasks\nperfectly and transparently, we find that all five attribution methods fail to\nproduce the expected model explanations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 19:55:32 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hao", "Yiding", ""]]}, {"id": "2010.08618", "submitter": "Sudha Rao", "authors": "Allison Hegel, Sudha Rao, Asli Celikyilmaz and Bill Dolan", "title": "Substance over Style: Document-Level Targeted Content Transfer", "comments": "This paper has been accepted to be published at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing language models excel at writing from scratch, but many real-world\nscenarios require rewriting an existing document to fit a set of constraints.\nAlthough sentence-level rewriting has been fairly well-studied, little work has\naddressed the challenge of rewriting an entire document coherently. In this\nwork, we introduce the task of document-level targeted content transfer and\naddress it in the recipe domain, with a recipe as the document and a dietary\nrestriction (such as vegan or dairy-free) as the targeted constraint. We\npropose a novel model for this task based on the generative pre-trained\nlanguage model (GPT-2) and train on a large number of roughly-aligned recipe\npairs (https://github.com/microsoft/document-level-targeted-content-transfer).\nBoth automatic and human evaluations show that our model out-performs existing\nmethods by generating coherent and diverse rewrites that obey the constraint\nwhile remaining close to the original document. Finally, we analyze our model's\nrewrites to assess progress toward the goal of making language generation more\nattuned to constraints that are substantive rather than stylistic.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 20:26:10 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hegel", "Allison", ""], ["Rao", "Sudha", ""], ["Celikyilmaz", "Asli", ""], ["Dolan", "Bill", ""]]}, {"id": "2010.08642", "submitter": "Tejas Srinivasan", "authors": "Tejas Srinivasan, Ramon Sanabria, Florian Metze, Desmond Elliott", "title": "Multimodal Speech Recognition with Unstructured Audio Masking", "comments": "Accepted to NLP Beyond Text workshop, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual context has been shown to be useful for automatic speech recognition\n(ASR) systems when the speech signal is noisy or corrupted. Previous work,\nhowever, has only demonstrated the utility of visual context in an unrealistic\nsetting, where a fixed set of words are systematically masked in the audio. In\nthis paper, we simulate a more realistic masking scenario during model\ntraining, called RandWordMask, where the masking can occur for any word\nsegment. Our experiments on the Flickr 8K Audio Captions Corpus show that\nmultimodal ASR can generalize to recover different types of masked words in\nthis unstructured masking setting. Moreover, our analysis shows that our models\nare capable of attending to the visual signal when the audio signal is\ncorrupted. These results show that multimodal ASR systems can leverage the\nvisual signal in more generalized noisy scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 21:49:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Srinivasan", "Tejas", ""], ["Sanabria", "Ramon", ""], ["Metze", "Florian", ""], ["Elliott", "Desmond", ""]]}, {"id": "2010.08652", "submitter": "Jian Ni", "authors": "Jian Ni and Taesun Moon and Parul Awasthy and Radu Florian", "title": "Cross-Lingual Relation Extraction with Transformers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) is one of the most important tasks in information\nextraction, as it provides essential information for many NLP applications. In\nthis paper, we propose a cross-lingual RE approach that does not require any\nhuman annotation in a target language or any cross-lingual resources. Building\nupon unsupervised cross-lingual representation learning frameworks, we develop\nseveral deep Transformer based RE models with a novel encoding scheme that can\neffectively encode both entity location and entity type information. Our RE\nmodels, when trained with English data, outperform several deep neural network\nbased English RE models. More importantly, our models can be applied to perform\nzero-shot cross-lingual RE, achieving the state-of-the-art cross-lingual RE\nperformance on two datasets (68-89% of the accuracy of the supervised\ntarget-language RE model). The high cross-lingual transfer efficiency without\nrequiring additional training data or cross-lingual resources shows that our RE\nmodels are especially useful for low-resource languages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 22:23:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ni", "Jian", ""], ["Moon", "Taesun", ""], ["Awasthy", "Parul", ""], ["Florian", "Radu", ""]]}, {"id": "2010.08670", "submitter": "Yanru Qu", "authors": "Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Jiawei Han, Weizhu\n  Chen", "title": "CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for\n  Natural Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation has been demonstrated as an effective strategy for\nimproving model generalization and data efficiency. However, due to the\ndiscrete nature of natural language, designing label-preserving transformations\nfor text data tends to be more challenging. In this paper, we propose a novel\ndata augmentation framework dubbed CoDA, which synthesizes diverse and\ninformative augmented examples by integrating multiple transformations\norganically. Moreover, a contrastive regularization objective is introduced to\ncapture the global relationship among all the data samples. A momentum encoder\nalong with a memory bank is further leveraged to better estimate the\ncontrastive loss. To verify the effectiveness of the proposed framework, we\napply CoDA to Transformer-based models on a wide range of natural language\nunderstanding tasks. On the GLUE benchmark, CoDA gives rise to an average\nimprovement of 2.2% while applied to the RoBERTa-large model. More importantly,\nit consistently exhibits stronger results relative to several competitive data\naugmentation and adversarial training base-lines (including the low-resource\nsettings). Extensive experiments show that the proposed contrastive objective\ncan be flexibly combined with various data augmentation approaches to further\nboost their performance, highlighting the wide applicability of the CoDA\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 23:57:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Qu", "Yanru", ""], ["Shen", "Dinghan", ""], ["Shen", "Yelong", ""], ["Sajeev", "Sandra", ""], ["Han", "Jiawei", ""], ["Chen", "Weizhu", ""]]}, {"id": "2010.08684", "submitter": "Mihail Eric", "authors": "Shikib Mehri and Mihail Eric", "title": "Example-Driven Intent Prediction with Observers", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key challenge of dialog systems research is to effectively and efficiently\nadapt to new domains. A scalable paradigm for adaptation necessitates the\ndevelopment of generalizable models that perform well in few-shot settings. In\nthis paper, we focus on the intent classification problem which aims to\nidentify user intents given utterances addressed to the dialog system. We\npropose two approaches for improving the generalizability of utterance\nclassification models: (1) observers and (2) example-driven training. Prior\nwork has shown that BERT-like models tend to attribute a significant amount of\nattention to the [CLS] token, which we hypothesize results in diluted\nrepresentations. Observers are tokens that are not attended to, and are an\nalternative to the [CLS] token as a semantic representation of utterances.\nExample-driven training learns to classify utterances by comparing to examples,\nthereby using the underlying encoder as a sentence similarity model. These\nmethods are complementary; improving the representation through observers\nallows the example-driven model to better measure sentence similarities. When\ncombined, the proposed methods attain state-of-the-art results on three intent\nprediction datasets (\\textsc{banking77}, \\textsc{clinc150}, \\textsc{hwu64}) in\nboth the full data and few-shot (10 examples per intent) settings. Furthermore,\nwe demonstrate that the proposed approach can transfer to new intents and\nacross datasets without any additional training.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:03:06 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 02:09:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mehri", "Shikib", ""], ["Eric", "Mihail", ""]]}, {"id": "2010.08708", "submitter": "Wei Han", "authors": "Hantao Huang, Tao Han, Wei Han, Deep Yap, Cheng-Ming Chiang", "title": "Answer-checking in Context: A Multi-modal FullyAttention Network for\n  Visual Question Answering", "comments": "Accepted in ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is challenging due to the complex cross-modal\nrelations. It has received extensive attention from the research community.\nFrom the human perspective, to answer a visual question, one needs to read the\nquestion and then refer to the image to generate an answer. This answer will\nthen be checked against the question and image again for the final\nconfirmation. In this paper, we mimic this process and propose a fully\nattention based VQA architecture. Moreover, an answer-checking module is\nproposed to perform a unified attention on the jointly answer, question and\nimage representation to update the answer. This mimics the human answer\nchecking process to consider the answer in the context. With answer-checking\nmodules and transferred BERT layers, our model achieves the state-of-the-art\naccuracy 71.57\\% using fewer parameters on VQA-v2.0 test-standard split.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:37:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Huang", "Hantao", ""], ["Han", "Tao", ""], ["Han", "Wei", ""], ["Yap", "Deep", ""], ["Chiang", "Cheng-Ming", ""]]}, {"id": "2010.08712", "submitter": "Meng Cao", "authors": "Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung", "title": "Factual Error Correction for Abstractive Summarization Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural abstractive summarization systems have achieved promising progress,\nthanks to the availability of large-scale datasets and models pre-trained with\nself-supervised methods. However, ensuring the factual consistency of the\ngenerated summaries for abstractive summarization systems is a challenge. We\npropose a post-editing corrector module to address this issue by identifying\nand correcting factual errors in generated summaries. The neural corrector\nmodel is pre-trained on artificial examples that are created by applying a\nseries of heuristic transformations on reference summaries. These\ntransformations are inspired by an error analysis of state-of-the-art\nsummarization model outputs. Experimental results show that our model is able\nto correct factual errors in summaries generated by other neural summarization\nmodels and outperforms previous models on factual consistency evaluation on the\nCNN/DailyMail dataset. We also find that transferring from artificial error\ncorrection to downstream settings is still very challenging.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 04:24:16 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 22:56:36 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Cao", "Meng", ""], ["Dong", "Yue", ""], ["Wu", "Jiapeng", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "2010.08725", "submitter": "Chenhui Chu", "authors": "Andrew Merritt, Chenhui Chu, Yuki Arase", "title": "A Corpus for English-Japanese Multimodal Neural Machine Translation with\n  Comparable Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal neural machine translation (NMT) has become an increasingly\nimportant area of research over the years because additional modalities, such\nas image data, can provide more context to textual data. Furthermore, the\nviability of training multimodal NMT models without a large parallel corpus\ncontinues to be investigated due to low availability of parallel sentences with\nimages, particularly for English-Japanese data. However, this void can be\nfilled with comparable sentences that contain bilingual terms and parallel\nphrases, which are naturally created through media such as social network posts\nand e-commerce product descriptions. In this paper, we propose a new multimodal\nEnglish-Japanese corpus with comparable sentences that are compiled from\nexisting image captioning datasets. In addition, we supplement our comparable\nsentences with a smaller parallel corpus for validation and test purposes. To\ntest the performance of this comparable sentence translation scenario, we train\nseveral baseline NMT models with our comparable corpus and evaluate their\nEnglish-Japanese translation performance. Due to low translation scores in our\nbaseline experiments, we believe that current multimodal NMT models are not\ndesigned to effectively utilize comparable sentence data. Despite this, we hope\nfor our corpus to be used to further research into multimodal NMT with\ncomparable sentences.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 06:12:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Merritt", "Andrew", ""], ["Chu", "Chenhui", ""], ["Arase", "Yuki", ""]]}, {"id": "2010.08728", "submitter": "Jin Xu", "authors": "Jin Xu, Yinuo Guo, Junfeng Hu", "title": "Incorporate Semantic Structures into Machine Translation Evaluation via\n  UCCA", "comments": "WMT2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copying mechanism has been commonly used in neural paraphrasing networks and\nother text generation tasks, in which some important words in the input\nsequence are preserved in the output sequence. Similarly, in machine\ntranslation, we notice that there are certain words or phrases appearing in all\ngood translations of one source text, and these words tend to convey important\nsemantic information. Therefore, in this work, we define words carrying\nimportant semantic meanings in sentences as semantic core words. Moreover, we\npropose an MT evaluation approach named Semantically Weighted Sentence\nSimilarity (SWSS). It leverages the power of UCCA to identify semantic core\nwords, and then calculates sentence similarity scores on the overlap of\nsemantic core words. Experimental results show that SWSS can consistently\nimprove the performance of popular MT evaluation metrics which are based on\nlexical similarity.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 06:47:58 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 03:38:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Xu", "Jin", ""], ["Guo", "Yinuo", ""], ["Hu", "Junfeng", ""]]}, {"id": "2010.08738", "submitter": "Jun Quan", "authors": "Jun Quan, Shian Zhang, Qian Cao, Zizhong Li and Deyi Xiong", "title": "RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich\n  Semantic Annotations for Task-Oriented Dialogue Modeling", "comments": "EMNLP 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to alleviate the shortage of multi-domain data and to capture\ndiscourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a\nlarge-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic\nAnnotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn\nsemantically annotated dialogues, with more than 150K utterances spanning over\n12 domains, which is larger than all previous annotated H2H conversational\ndatasets. Both single- and multi-domain dialogues are constructed, accounting\nfor 65% and 35%, respectively. Each dialogue is labeled with comprehensive\ndialogue annotations, including dialogue goal in the form of natural language\ndescription, domain, dialogue states and acts at both the user and system side.\nIn addition to traditional dialogue annotations, we especially provide\nlinguistic annotations on discourse phenomena, e.g., ellipsis and coreference,\nin dialogues, which are useful for dialogue coreference and ellipsis resolution\ntasks. Apart from the fully annotated dataset, we also present a detailed\ndescription of the data collection procedure, statistics and analysis of the\ndataset. A series of benchmark models and results are reported, including\nnatural language understanding (intent detection & slot filling), dialogue\nstate tracking and dialogue context-to-text generation, as well as coreference\nand ellipsis resolution, which facilitate the baseline comparison for future\nresearch on this corpus.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 08:18:59 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Quan", "Jun", ""], ["Zhang", "Shian", ""], ["Cao", "Qian", ""], ["Li", "Zizhong", ""], ["Xiong", "Deyi", ""]]}, {"id": "2010.08743", "submitter": "Ismini Lourentzou", "authors": "Arkin Dharawat and Ismini Lourentzou and Alex Morales and ChengXiang\n  Zhai", "title": "Drink bleach or do what now? Covid-HeRA: A dataset for risk-informed\n  health decision making in the presence of COVID19 misinformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the wide spread of inaccurate medical advice related to the 2019\ncoronavirus pandemic (COVID-19), such as fake remedies, treatments and\nprevention suggestions, misinformation detection has emerged as an open problem\nof high importance and interest for the NLP community. To combat potential harm\nof COVID19-related misinformation, we release Covid-HeRA, a dataset for health\nrisk assessment of COVID-19-related social media posts. More specifically, we\nstudy the severity of each misinformation story, i.e., how harmful a message\nbelieved by the audience can be and what type of signals can be used to\ndiscover high malicious fake news and detect refuted claims. We present a\ndetailed analysis, evaluate several simple and advanced classification models,\nand conclude with our experimental analysis that presents open challenges and\nfuture directions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 08:34:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Dharawat", "Arkin", ""], ["Lourentzou", "Ismini", ""], ["Morales", "Alex", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "2010.08756", "submitter": "Sara Renjit", "authors": "Sara Renjit, Sumam Mary Idicula", "title": "CUSATNLP@HASOC-Dravidian-CodeMix-FIRE2020:Identifying Offensive Language\n  from ManglishTweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the popularity of social media, communications through blogs, Facebook,\nTwitter, and other plat-forms have increased. Initially, English was the only\nmedium of communication. Fortunately, now we can communicate in any language.\nIt has led to people using English and their own native or mother tongue\nlanguage in a mixed form. Sometimes, comments in other languages have English\ntransliterated format or other cases; people use the intended language scripts.\nIdentifying sentiments and offensive content from such code mixed tweets is a\nnecessary task in these times. We present a working model submitted for Task2\nof the sub-track HASOC Offensive Language Identification- DravidianCodeMix in\nForum for Information Retrieval Evaluation, 2020. It is a message level\nclassification task. An embedding model-based classifier identifies offensive\nand not offensive comments in our approach. We applied this method in the\nManglish dataset provided along with the sub-track.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 10:11:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Renjit", "Sara", ""], ["Idicula", "Sumam Mary", ""]]}, {"id": "2010.08768", "submitter": "Fatima Haouari", "authors": "Fatima Haouari, Maram Hasanain, Reem Suwaileh, Tamer Elsayed", "title": "ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation\n  Detection", "comments": "This work was accepted at the Sixth Arabic Natural Language\n  Processing Workshop (EACL/WANLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset\nfor misinformation detection composed of tweets containing claims from 27th\nJanuary till the end of April 2020. We collected 138 verified claims, mostly\nfrom popular fact-checking websites, and identified 9.4K relevant tweets to\nthose claims. Tweets were manually-annotated by veracity to support research on\nmisinformation detection, which is one of the major problems faced during a\npandemic. ArCOV19-Rumors supports two levels of misinformation detection over\nTwitter: verifying free-text claims (called claim-level verification) and\nverifying claims expressed in tweets (called tweet-level verification). Our\ndataset covers, in addition to health, claims related to other topical\ncategories that were influenced by COVID-19, namely, social, politics, sports,\nentertainment, and religious. Moreover, we present benchmarking results for\ntweet-level verification on the dataset. We experimented with SOTA models of\nversatile approaches that either exploit content, user profiles features,\ntemporal features and propagation structure of the conversational threads for\ntweet verification.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 11:21:40 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 20:26:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Haouari", "Fatima", ""], ["Hasanain", "Maram", ""], ["Suwaileh", "Reem", ""], ["Elsayed", "Tamer", ""]]}, {"id": "2010.08770", "submitter": "Ismail Shahin", "authors": "Mohamed Bader, Ismail Shahin, Abdelfatah Hassan", "title": "Studying the Similarity of COVID-19 Sounds based on Correlation Analysis\n  of MFCC", "comments": "5 pages, 4 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a formidable work which has been put up from the\npeople who are working in the frontlines such as hospitals, clinics, and labs\nalongside researchers and scientists who are also putting tremendous efforts in\nthe fight against COVID-19 pandemic. Due to the preposterous spread of the\nvirus, the integration of the artificial intelligence has taken a considerable\npart in the health sector, by implementing the fundamentals of Automatic Speech\nRecognition (ASR) and deep learning algorithms. In this paper, we illustrate\nthe importance of speech signal processing in the extraction of the\nMel-Frequency Cepstral Coefficients (MFCCs) of the COVID-19 and non-COVID-19\nsamples and find their relationship using Pearson correlation coefficients. Our\nresults show high similarity in MFCCs between different COVID-19 cough and\nbreathing sounds, while MFCC of voice is more robust between COVID-19 and\nnon-COVID-19 samples. Moreover, our results are preliminary, and there is a\npossibility to exclude the voices of COVID-19 patients from further processing\nin diagnosing the disease.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 11:38:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bader", "Mohamed", ""], ["Shahin", "Ismail", ""], ["Hassan", "Abdelfatah", ""]]}, {"id": "2010.08777", "submitter": "Pengshuai Li", "authors": "Pengshuai Li, Xinsong Zhang, Weijia Jia and Wei Zhao", "title": "Active Testing: An Unbiased Evaluation Method for Distantly Supervised\n  Relation Extraction", "comments": "accepted to appear at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision has been a widely used method for neural relation\nextraction for its convenience of automatically labeling datasets. However,\nexisting works on distantly supervised relation extraction suffer from the low\nquality of test set, which leads to considerable biased performance evaluation.\nThese biases not only result in unfair evaluations but also mislead the\noptimization of neural relation extraction. To mitigate this problem, we\npropose a novel evaluation method named active testing through utilizing both\nthe noisy test set and a few manual annotations. Experiments on a widely used\nbenchmark show that our proposed approach can yield approximately unbiased\nevaluations for distantly supervised relation extractors.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 12:29:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Pengshuai", ""], ["Zhang", "Xinsong", ""], ["Jia", "Weijia", ""], ["Zhao", "Wei", ""]]}, {"id": "2010.08822", "submitter": "Wei Wang", "authors": "Wei Wang, Piji Li, Hai-Tao Zheng", "title": "Consistency and Coherency Enhanced Story Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Story generation is a challenging task, which demands to maintain consistency\nof the plots and characters throughout the story. Previous works have shown\nthat GPT2, a large-scale language model, has achieved good performance on story\ngeneration. However, we observe that several serious issues still exist in the\nstories generated by GPT2 which can be categorized into two folds: consistency\nand coherency. In terms of consistency, on one hand, GPT2 cannot guarantee the\nconsistency of the plots explicitly. On the other hand, the generated stories\nusually contain coreference errors. In terms of coherency, GPT2 does not take\naccount of the discourse relations between sentences of stories directly. To\nenhance the consistency and coherency of the generated stories, we propose a\ntwo-stage generation framework, where the first stage is to organize the story\noutline which depicts the story plots and events, and the second stage is to\nexpand the outline into a complete story. Therefore the plots consistency can\nbe controlled and guaranteed explicitly. In addition, coreference supervision\nsignals are incorporated to reduce coreference errors and improve the\ncoreference consistency. Moreover, we design an auxiliary task of discourse\nrelation modeling to improve the coherency of the generated stories.\nExperimental results on a story dataset show that our model outperforms the\nbaseline approaches in terms of both automatic metrics and human evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 16:40:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Piji", ""], ["Zheng", "Hai-Tao", ""]]}, {"id": "2010.08824", "submitter": "Xueliang Zhao", "authors": "Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan", "title": "Knowledge-Grounded Dialogue Generation with Pre-trained Language Models", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study knowledge-grounded dialogue generation with pre-trained language\nmodels. To leverage the redundant external knowledge under capacity constraint,\nwe propose equipping response generation defined by a pre-trained language\nmodel with a knowledge selection module, and an unsupervised approach to\njointly optimizing knowledge selection and response generation with unlabeled\ndialogues. Empirical results on two benchmarks indicate that our model can\nsignificantly outperform state-of-the-art methods in both automatic evaluation\nand human judgment.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 16:49:43 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhao", "Xueliang", ""], ["Wu", "Wei", ""], ["Xu", "Can", ""], ["Tao", "Chongyang", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2010.08865", "submitter": "Thanh Tran", "authors": "Thanh Tran, Yifan Hu, Changwei Hu, Kevin Yen, Fei Tan, Kyumin Lee,\n  Serim Park", "title": "HABERTOR: An Efficient and Effective Deep Hatespeech Detector", "comments": null, "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our HABERTOR model for detecting hatespeech in large scale\nuser-generated content. Inspired by the recent success of the BERT model, we\npropose several modifications to BERT to enhance the performance on the\ndownstream hatespeech classification task. HABERTOR inherits BERT's\narchitecture, but is different in four aspects: (i) it generates its own\nvocabularies and is pre-trained from the scratch using the largest scale\nhatespeech dataset; (ii) it consists of Quaternion-based factorized components,\nresulting in a much smaller number of parameters, faster training and\ninferencing, as well as less memory usage; (iii) it uses our proposed\nmulti-source ensemble heads with a pooling layer for separate input sources, to\nfurther enhance its effectiveness; and (iv) it uses a regularized adversarial\ntraining with our proposed fine-grained and adaptive noise magnitude to enhance\nits robustness. Through experiments on the large-scale real-world hatespeech\ndataset with 1.4M annotated comments, we show that HABERTOR works better than\n15 state-of-the-art hatespeech detection methods, including fine-tuning\nLanguage Models. In particular, comparing with BERT, our HABERTOR is 4~5 times\nfaster in the training/inferencing phase, uses less than 1/3 of the memory, and\nhas better performance, even though we pre-train it by using less than 1% of\nthe number of words. Our generalizability analysis shows that HABERTOR\ntransfers well to other unseen hatespeech datasets and is a more efficient and\neffective alternative to BERT for the hatespeech classification.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:10:08 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tran", "Thanh", ""], ["Hu", "Yifan", ""], ["Hu", "Changwei", ""], ["Yen", "Kevin", ""], ["Tan", "Fei", ""], ["Lee", "Kyumin", ""], ["Park", "Serim", ""]]}, {"id": "2010.08883", "submitter": "Sai Sharath Japa", "authors": "Sai Sharath Japa and Rekabdar Banafsheh", "title": "Question Answering over Knowledge Base using Language Model Embeddings", "comments": null, "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9206698", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Base, represents facts about the world, often in some form of\nsubsumption ontology, rather than implicitly, embedded in procedural code, the\nway a conventional computer program does. While there is a rapid growth in\nknowledge bases, it poses a challenge of retrieving information from them.\nKnowledge Base Question Answering is one of the promising approaches for\nextracting substantial knowledge from Knowledge Bases. Unlike web search,\nQuestion Answering over a knowledge base gives accurate and concise results,\nprovided that natural language questions can be understood and mapped precisely\nto an answer in the knowledge base. However, some of the existing\nembedding-based methods for knowledge base question answering systems ignore\nthe subtle correlation between the question and the Knowledge Base (e.g.,\nentity types, relation paths, and context) and suffer from the Out Of\nVocabulary problem. In this paper, we focused on using a pre-trained language\nmodel for the Knowledge Base Question Answering task. Firstly, we used Bert\nbase uncased for the initial experiments. We further fine-tuned these\nembeddings with a two-way attention mechanism from the knowledge base to the\nasked question and from the asked question to the knowledge base answer\naspects. Our method is based on a simple Convolutional Neural Network\narchitecture with a Multi-Head Attention mechanism to represent the asked\nquestion dynamically in multiple aspects. Our experimental results show the\neffectiveness and the superiority of the Bert pre-trained language model\nembeddings for question answering systems on knowledge bases over other\nwell-known embedding methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 22:59:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Japa", "Sai Sharath", ""], ["Banafsheh", "Rekabdar", ""]]}, {"id": "2010.08892", "submitter": "Chenguang Zhu", "authors": "Ruochen Xu, Chenguang Zhu, Yu Shi, Michael Zeng, Xuedong Huang", "title": "Mixed-Lingual Pre-training for Cross-lingual Summarization", "comments": "Accepted at Asia-Pacific Chapter of the Association for Computational\n  Linguistics (AACL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual Summarization (CLS) aims at producing a summary in the target\nlanguage for an article in the source language. Traditional solutions employ a\ntwo-step approach, i.e. translate then summarize or summarize then translate.\nRecently, end-to-end models have achieved better results, but these approaches\nare mostly limited by their dependence on large-scale labeled data. We propose\na solution based on mixed-lingual pre-training that leverages both\ncross-lingual tasks such as translation and monolingual tasks like masked\nlanguage models. Thus, our model can leverage the massive monolingual data to\nenhance its modeling of language. Moreover, the architecture has no\ntask-specific components, which saves memory and increases optimization\nefficiency. We show in experiments that this pre-training scheme can\neffectively boost the performance of cross-lingual summarization. In Neural\nCross-Lingual Summarization (NCLS) dataset, our model achieves an improvement\nof 2.82 (English to Chinese) and 1.15 (Chinese to English) ROUGE-1 scores over\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 00:21:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xu", "Ruochen", ""], ["Zhu", "Chenguang", ""], ["Shi", "Yu", ""], ["Zeng", "Michael", ""], ["Huang", "Xuedong", ""]]}, {"id": "2010.08923", "submitter": "Chenyu You", "authors": "Chenyu You, Nuo Chen, Fenglin Liu, Dongchao Yang, Yuexian Zou", "title": "Towards Data Distillation for End-to-end Spoken Conversational Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI eess.AS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spoken question answering, QA systems are designed to answer questions\nfrom contiguous text spans within the related speech transcripts. However, the\nmost natural way that human seek or test their knowledge is via human\nconversations. Therefore, we propose a new Spoken Conversational Question\nAnswering task (SCQA), aiming at enabling QA systems to model complex dialogues\nflow given the speech utterances and text corpora. In this task, our main\nobjective is to build a QA system to deal with conversational questions both in\nspoken and text forms, and to explore the plausibility of providing more cues\nin spoken documents with systems in information gathering. To this end, instead\nof adopting automatically generated speech transcripts with highly noisy data,\nwe propose a novel unified data distillation approach, DDNet, which directly\nfuse audio-text features to reduce the misalignment between automatic speech\nrecognition hypotheses and the reference transcriptions. In addition, to\nevaluate the capacity of QA systems in a dialogue-style interaction, we\nassemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with\nmore than 120k question-answer pairs. Experiments demonstrate that our proposed\nmethod achieves superior performance in spoken conversational question\nanswering.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 05:53:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["You", "Chenyu", ""], ["Chen", "Nuo", ""], ["Liu", "Fenglin", ""], ["Yang", "Dongchao", ""], ["Zou", "Yuexian", ""]]}, {"id": "2010.08961", "submitter": "Zewei Sun", "authors": "Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang,\n  Jiajun Chen, Lei Li", "title": "Capturing Longer Context for Document-level Neural Machine Translation:\n  A Multi-resolutional Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse context has been proven useful when translating documents. It is\nquite a challenge to incorporate long document context in the prevailing neural\nmachine translation models such as Transformer. In this paper, we propose\nmulti-resolutional (MR) Doc2Doc, a method to train a neural\nsequence-to-sequence model for document-level translation. Our trained model\ncan simultaneously translate sentence by sentence as well as a document as a\nwhole. We evaluate our method and several recent approaches on nine\ndocument-level datasets and two sentence-level datasets across six languages.\nExperiments show that MR Doc2Doc outperforms sentence-level models and previous\nmethods in a comprehensive set of metrics, including BLEU, four lexical\nindices, three newly proposed assistant linguistic indicators, and human\nevaluation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 11:18:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sun", "Zewei", ""], ["Wang", "Mingxuan", ""], ["Zhou", "Hao", ""], ["Zhao", "Chengqi", ""], ["Huang", "Shujian", ""], ["Chen", "Jiajun", ""], ["Li", "Lei", ""]]}, {"id": "2010.08974", "submitter": "Piyush Makhija", "authors": "Piyush Makhija, Ankit Kumar, Anuj Gupta", "title": "hinglishNorm -- A Corpus of Hindi-English Code Mixed Sentences for Text\n  Normalization", "comments": "Accepted in COLING2020 industry track, 8 pages (including\n  references), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present hinglishNorm -- a human annotated corpus of Hindi-English\ncode-mixed sentences for text normalization task. Each sentence in the corpus\nis aligned to its corresponding human annotated normalized form. To the best of\nour knowledge, there is no corpus of Hindi-English code-mixed sentences for\ntext normalization task that is publicly available. Our work is the first\nattempt in this direction. The corpus contains 13494 parallel segments.\nFurther, we present baseline normalization results on this corpus. We obtain a\nWord Error Rate (WER) of 15.55, BiLingual Evaluation Understudy (BLEU) score of\n71.2, and Metric for Evaluation of Translation with Explicit ORdering (METEOR)\nscore of 0.50.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 12:21:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Makhija", "Piyush", ""], ["Kumar", "Ankit", ""], ["Gupta", "Anuj", ""]]}, {"id": "2010.08980", "submitter": "Tom Hosking", "authors": "Laurie Burchell, Jie Chi, Tom Hosking, Nina Markl, Bonnie Webber", "title": "Querent Intent in Multi-Sentence Questions", "comments": "LAW XIV, COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sentence questions (MSQs) are sequences of questions connected by\nrelations which, unlike sequences of standalone questions, need to be answered\nas a unit. Following Rhetorical Structure Theory (RST), we recognise that\ndifferent \"question discourse relations\" between the subparts of MSQs reflect\ndifferent speaker intents, and consequently elicit different answering\nstrategies. Correctly identifying these relations is therefore a crucial step\nin automatically answering MSQs. We identify five different types of MSQs in\nEnglish, and define five novel relations to describe them. We extract over\n162,000 MSQs from Stack Exchange to enable future research. Finally, we\nimplement a high-precision baseline classifier based on surface features.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 13:17:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Burchell", "Laurie", ""], ["Chi", "Jie", ""], ["Hosking", "Tom", ""], ["Markl", "Nina", ""], ["Webber", "Bonnie", ""]]}, {"id": "2010.08983", "submitter": "Sahana Ramnath", "authors": "Sahana Ramnath, Preksha Nema, Deep Sahni, Mitesh M. Khapra", "title": "Towards Interpreting BERT for Reading Comprehension Based QA", "comments": "7 pages including references and appendix. Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT and its variants have achieved state-of-the-art performance in various\nNLP tasks. Since then, various works have been proposed to analyze the\nlinguistic information being captured in BERT. However, the current works do\nnot provide an insight into how BERT is able to achieve near human-level\nperformance on the task of Reading Comprehension based Question Answering. In\nthis work, we attempt to interpret BERT for RCQA. Since BERT layers do not have\npredefined roles, we define a layer's role or functionality using Integrated\nGradients. Based on the defined roles, we perform a preliminary analysis across\nall layers. We observed that the initial layers focus on query-passage\ninteraction, whereas later layers focus more on contextual understanding and\nenhancing the answer prediction. Specifically for quantifier questions (how\nmuch/how many), we notice that BERT focuses on confusing words (i.e., on other\nnumerical quantities in the passage) in the later layers, but still manages to\npredict the answer correctly. The fine-tuning and analysis scripts will be\npublicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA .\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 13:33:49 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ramnath", "Sahana", ""], ["Nema", "Preksha", ""], ["Sahni", "Deep", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "2010.08995", "submitter": "Jinta Weng", "authors": "Jinta Weng, Ying Gao, Jing Qiu, Guozhu Ding, Huanqin Zheng", "title": "Construction and Application of Teaching System Based on Crowdsourcing\n  Knowledge Graph", "comments": "Number of references:15 Classification code:903.3 Information\n  Retrieval and Use Conference code: 235759", "journal-ref": "4th China Conference on Knowledge Graph and Semantic Computing,\n  CCKS 2019", "doi": "10.1007/978-981-15-1956-7_3", "report-no": null, "categories": "cs.DB cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the combination of crowdsourcing knowledge graph and teaching system,\nresearch methods to generate knowledge graph and its applications. Using two\ncrowdsourcing approaches, crowdsourcing task distribution and reverse captcha\ngeneration, to construct knowledge graph in the field of teaching system.\nGenerating a complete hierarchical knowledge graph of the teaching domain by\nnodes of school, student, teacher, course, knowledge point and exercise type.\nThe knowledge graph constructed in a crowdsourcing manner requires many users\nto participate collaboratively with fully consideration of teachers' guidance\nand users' mobilization issues. Based on the three subgraphs of knowledge\ngraph, prominent teacher, student learning situation and suitable learning\nroute could be visualized. Personalized exercises recommendation model is used\nto formulate the personalized exercise by algorithm based on the knowledge\ngraph. Collaborative creation model is developed to realize the crowdsourcing\nconstruction mechanism. Though unfamiliarity with the learning mode of\nknowledge graph and learners' less attention to the knowledge structure, system\nbased on Crowdsourcing Knowledge Graph can still get high acceptance around\nstudents and teachers\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 14:26:10 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Weng", "Jinta", ""], ["Gao", "Ying", ""], ["Qiu", "Jing", ""], ["Ding", "Guozhu", ""], ["Zheng", "Huanqin", ""]]}, {"id": "2010.09030", "submitter": "Nazneen Fatema Rajani", "authors": "Nazneen Fatema Rajani, Ben Krause, Wengpeng Yin, Tong Niu, Richard\n  Socher, Caiming Xiong", "title": "Explaining and Improving Model Behavior with k Nearest Neighbor\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretability techniques in NLP have mainly focused on understanding\nindividual predictions using attention visualization or gradient-based saliency\nmaps over tokens. We propose using k nearest neighbor (kNN) representations to\nidentify training examples responsible for a model's predictions and obtain a\ncorpus-level understanding of the model's behavior. Apart from\ninterpretability, we show that kNN representations are effective at uncovering\nlearned spurious associations, identifying mislabeled examples, and improving\nthe fine-tuned model's performance. We focus on Natural Language Inference\n(NLI) as a case study and experiment with multiple datasets. Our method deploys\nbackoff to kNN for BERT and RoBERTa on examples with low model confidence\nwithout any update to the model parameters. Our results indicate that the kNN\napproach makes the finetuned model more robust to adversarial inputs.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:55:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Rajani", "Nazneen Fatema", ""], ["Krause", "Ben", ""], ["Yin", "Wengpeng", ""], ["Niu", "Tong", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.09046", "submitter": "Cheonbok  Park", "authors": "Cheonbok Park, Yunwon Tae, Taehee Kim, Soyoung Yang, Mohammad Azam\n  Khan, Eunjeong Park and Jaegul Choo", "title": "Unsupervised Neural Machine Translation for Low-Resource Domains via\n  Meta-Learning", "comments": "to be published in ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised machine translation, which utilizes unpaired monolingual corpora\nas training data, has achieved comparable performance against supervised\nmachine translation. However, it still suffers from data-scarce domains. To\naddress this issue, this paper presents a novel meta-learning algorithm for\nunsupervised neural machine translation (UNMT) that trains the model to adapt\nto another domain by utilizing only a small amount of training data. We assume\nthat domain-general knowledge is a significant factor in handling data-scarce\ndomains. Hence, we extend the meta-learning algorithm, which utilizes knowledge\nlearned from high-resource domains, to boost the performance of low-resource\nUNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU\nscores. Extensive experimental results show that our proposed algorithm is\npertinent for fast adaptation and consistently outperforms other baseline\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 17:54:13 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 14:14:06 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Park", "Cheonbok", ""], ["Tae", "Yunwon", ""], ["Kim", "Taehee", ""], ["Yang", "Soyoung", ""], ["Khan", "Mohammad Azam", ""], ["Park", "Eunjeong", ""], ["Choo", "Jaegul", ""]]}, {"id": "2010.09072", "submitter": "Harish Tayyar Madabushi PhD", "authors": "Eleri Sarsfield and Harish Tayyar Madabushi", "title": "UoB at SemEval-2020 Task 1: Automatic Identification of Novel Word\n  Senses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much as the social landscape in which languages are spoken shifts, language\ntoo evolves to suit the needs of its users. Lexical semantic change analysis is\na burgeoning field of semantic analysis which aims to trace changes in the\nmeanings of words over time. This paper presents an approach to lexical\nsemantic change detection based on Bayesian word sense induction suitable for\nnovel word sense identification. This approach is used for a submission to\nSemEval-2020 Task 1, which shows the approach to be capable of the SemEval\ntask. The same approach is also applied to a corpus gleaned from 15 years of\nTwitter data, the results of which are then used to identify words which may be\ninstances of slang.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:27:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sarsfield", "Eleri", ""], ["Madabushi", "Harish Tayyar", ""]]}, {"id": "2010.09078", "submitter": "Harish Tayyar Madabushi PhD", "authors": "Anushka Prakash and Harish Tayyar Madabushi", "title": "Incorporating Count-Based Features into Pre-Trained Models for Improved\n  Stance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The explosive growth and popularity of Social Media has revolutionised the\nway we communicate and collaborate. Unfortunately, this same ease of accessing\nand sharing information has led to an explosion of misinformation and\npropaganda. Given that stance detection can significantly aid in veracity\nprediction, this work focuses on boosting automated stance detection, a task on\nwhich pre-trained models have been extremely successful on, as on several other\ntasks. This work shows that the task of stance detection can benefit from\nfeature based information, especially on certain under performing classes,\nhowever, integrating such features into pre-trained models using ensembling is\nchallenging. We propose a novel architecture for integrating features with\npre-trained models that address these challenges and test our method on the\nRumourEval 2019 dataset. This method achieves state-of-the-art results with an\nF1-score of 63.94 on the test set.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:37:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Prakash", "Anushka", ""], ["Madabushi", "Harish Tayyar", ""]]}, {"id": "2010.09142", "submitter": "Enamul Hoque", "authors": "Jason Obeid and Enamul Hoque", "title": "Chart-to-Text: Generating Natural Language Descriptions for Charts by\n  Adapting the Transformer Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information visualizations such as bar charts and line charts are very\npopular for exploring data and communicating insights. Interpreting and making\nsense of such visualizations can be challenging for some people, such as those\nwho are visually impaired or have low visualization literacy. In this work, we\nintroduce a new dataset and present a neural model for automatically generating\nnatural language summaries for charts. The generated summaries provide an\ninterpretation of the chart and convey the key insights found within that\nchart. Our neural model is developed by extending the state-of-the-art model\nfor the data-to-text generation task, which utilizes a transformer-based\nencoder-decoder architecture. We found that our approach outperforms the base\nmodel on a content selection metric by a wide margin (55.42% vs. 8.49%) and\ngenerates more informative, concise, and coherent summaries.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 23:57:33 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 21:17:49 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Obeid", "Jason", ""], ["Hoque", "Enamul", ""]]}, {"id": "2010.09189", "submitter": "Sheng Zhang", "authors": "Ye Liu, Sheng Zhang, Rui Song, Suo Feng, Yanghua Xiao", "title": "Knowledge-guided Open Attribute Value Extraction with Reinforcement\n  Learning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open attribute value extraction for emerging entities is an important but\nchallenging task. A lot of previous works formulate the problem as a\n\\textit{question-answering} (QA) task. While the collections of articles from\nweb corpus provide updated information about the emerging entities, the\nretrieved texts can be noisy, irrelevant, thus leading to inaccurate answers.\nEffectively filtering out noisy articles as well as bad answers is the key to\nimproving extraction accuracy. Knowledge graph (KG), which contains rich, well\norganized information about entities, provides a good resource to address the\nchallenge. In this work, we propose a knowledge-guided reinforcement learning\n(RL) framework for open attribute value extraction. Informed by relevant\nknowledge in KG, we trained a deep Q-network to sequentially compare extracted\nanswers to improve extraction accuracy. The proposed framework is applicable to\ndifferent information extraction system. Our experimental results show that our\nmethod outperforms the baselines by 16.5 - 27.8\\%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 03:28:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liu", "Ye", ""], ["Zhang", "Sheng", ""], ["Song", "Rui", ""], ["Feng", "Suo", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2010.09190", "submitter": "Jiaxin Ju", "authors": "Jiaxin Ju, Ming Liu, Longxiang Gao and Shirui Pan", "title": "SciSummPip: An Unsupervised Scientific Paper Summarization Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Scholarly Document Processing (SDP) workshop is to encourage more efforts\non natural language understanding of scientific task. It contains three shared\ntasks and we participate in the LongSumm shared task. In this paper, we\ndescribe our text summarization system, SciSummPip, inspired by SummPip (Zhao\net al., 2020) that is an unsupervised text summarization system for\nmulti-document in news domain. Our SciSummPip includes a transformer-based\nlanguage model SciBERT (Beltagy et al., 2019) for contextual sentence\nrepresentation, content selection with PageRank (Page et al., 1999), sentence\ngraph construction with both deep and linguistic information, sentence graph\nclustering and within-graph summary generation. Our work differs from previous\nmethod in that content selection and a summary length constraint is applied to\nadapt to the scientific domain. The experiment results on both training dataset\nand blind test dataset show the effectiveness of our method, and we empirically\nverify the robustness of modules used in SciSummPip with BERTScore (Zhang et\nal., 2019a).\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 03:29:21 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ju", "Jiaxin", ""], ["Liu", "Ming", ""], ["Gao", "Longxiang", ""], ["Pan", "Shirui", ""]]}, {"id": "2010.09194", "submitter": "Pan Xie", "authors": "Pan Xie, Zhi Cui, Xiuyin Chen, Xiaohui Hu, Jianwei Cui, Bin Wang", "title": "Infusing Sequential Information into Conditional Masked Translation\n  Model with Self-Review Mechanism", "comments": "accepted to coling 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive models generate target words in a parallel way, which\nachieve a faster decoding speed but at the sacrifice of translation accuracy.\nTo remedy a flawed translation by non-autoregressive models, a promising\napproach is to train a conditional masked translation model (CMTM), and refine\nthe generated results within several iterations. Unfortunately, such approach\nhardly considers the \\textit{sequential dependency} among target words, which\ninevitably results in a translation degradation. Hence, instead of solely\ntraining a Transformer-based CMTM, we propose a Self-Review Mechanism to infuse\nsequential information into it. Concretely, we insert a left-to-right mask to\nthe same decoder of CMTM, and then induce it to autoregressively review whether\neach generated word from CMTM is supposed to be replaced or kept. The\nexperimental results (WMT14 En$\\leftrightarrow$De and WMT16\nEn$\\leftrightarrow$Ro) demonstrate that our model uses dramatically less\ntraining computations than the typical CMTM, as well as outperforms several\nstate-of-the-art non-autoregressive models by over 1 BLEU. Through knowledge\ndistillation, our model even surpasses a typical left-to-right Transformer\nmodel, while significantly speeding up decoding.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 03:38:56 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 13:22:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xie", "Pan", ""], ["Cui", "Zhi", ""], ["Chen", "Xiuyin", ""], ["Hu", "Xiaohui", ""], ["Cui", "Jianwei", ""], ["Wang", "Bin", ""]]}, {"id": "2010.09233", "submitter": "Dang Pham Nhu Hai", "authors": "Dang Pham, Tuan M.V.Le", "title": "Auto-Encoding Variational Bayes for Inferring Topics and Visualization", "comments": "Accepted at the 28th International Conference on Computational\n  Linguistics (COLING 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization and topic modeling are widely used approaches for text\nanalysis. Traditional visualization methods find low-dimensional\nrepresentations of documents in the visualization space (typically 2D or 3D)\nthat can be displayed using a scatterplot. In contrast, topic modeling aims to\ndiscover topics from text, but for visualization, one needs to perform a\npost-hoc embedding using dimensionality reduction methods. Recent approaches\npropose using a generative model to jointly find topics and visualization,\nallowing the semantics to be infused in the visualization space for a\nmeaningful interpretation. A major challenge that prevents these methods from\nbeing used practically is the scalability of their inference algorithms. We\npresent, to the best of our knowledge, the first fast Auto-Encoding Variational\nBayes based inference method for jointly inferring topics and visualization.\nSince our method is black box, it can handle model changes efficiently with\nlittle mathematical rederivation effort. We demonstrate the efficiency and\neffectiveness of our method on real-world large datasets and compare it with\nexisting baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 05:57:11 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 19:37:56 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Pham", "Dang", ""], ["Le", "Tuan M. V.", ""]]}, {"id": "2010.09240", "submitter": "Dan Su", "authors": "Dan Su, Yan Xu, Wenliang Dai, Ziwei Ji, Tiezheng Yu, Pascale Fung", "title": "Multi-hop Question Generation with Graph Convolutional Network", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.416", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop Question Generation (QG) aims to generate answer-related questions\nby aggregating and reasoning over multiple scattered evidence from different\nparagraphs. It is a more challenging yet under-explored task compared to\nconventional single-hop QG, where the questions are generated from the sentence\ncontaining the answer or nearby sentences in the same paragraph without complex\nreasoning. To address the additional challenges in multi-hop QG, we propose\nMulti-Hop Encoding Fusion Network for Question Generation (MulQG), which does\ncontext encoding in multiple hops with Graph Convolutional Network and encoding\nfusion via an Encoder Reasoning Gate. To the best of our knowledge, we are the\nfirst to tackle the challenge of multi-hop reasoning over paragraphs without\nany sentence-level information. Empirical results on HotpotQA dataset\ndemonstrate the effectiveness of our method, in comparison with baselines on\nautomatic evaluation metrics. Moreover, from the human evaluation, our proposed\nmodel is able to generate fluent questions with high completeness and\noutperforms the strongest baseline by 20.8% in the multi-hop evaluation. The\ncode is publicly available at\nhttps://github.com/HLTCHKUST/MulQG}{https://github.com/HLTCHKUST/MulQG .\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:15:36 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 08:59:23 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Su", "Dan", ""], ["Xu", "Yan", ""], ["Dai", "Wenliang", ""], ["Ji", "Ziwei", ""], ["Yu", "Tiezheng", ""], ["Fung", "Pascale", ""]]}, {"id": "2010.09252", "submitter": "Tiezheng Yu", "authors": "Tiezheng Yu and Dan Su and Wenliang Dai and Pascale Fung", "title": "Dimsum @LaySumm 20: BART-based Approach for Scientific Document\n  Summarization", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lay summarization aims to generate lay summaries of scientific papers\nautomatically. It is an essential task that can increase the relevance of\nscience for all of society. In this paper, we build a lay summary generation\nsystem based on the BART model. We leverage sentence labels as extra\nsupervision signals to improve the performance of lay summarization. In the\nCL-LaySumm 2020 shared task, our model achieves 46.00\\% Rouge1-F1 score.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:36:11 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yu", "Tiezheng", ""], ["Su", "Dan", ""], ["Dai", "Wenliang", ""], ["Fung", "Pascale", ""]]}, {"id": "2010.09254", "submitter": "Jingang Wang", "authors": "Yang Yang, Junmei Hao, Canjia Li, Zili Wang, Jingang Wang, Fuzheng\n  Zhang, Rao Fu, Peixu Hou, Gong Zhang, Zhongyuan Wang", "title": "Query-aware Tip Generation for Vertical Search", "comments": "Accepted By CIKM 2020 Applied Research Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a concise form of user reviews, tips have unique advantages to explain the\nsearch results, assist users' decision making, and further improve user\nexperience in vertical search scenarios. Existing work on tip generation does\nnot take query into consideration, which limits the impact of tips in search\nscenarios. To address this issue, this paper proposes a query-aware tip\ngeneration framework, integrating query information into encoding and\nsubsequent decoding processes. Two specific adaptations of Transformer and\nRecurrent Neural Network (RNN) are proposed. For Transformer, the query impact\nis incorporated into the self-attention computation of both the encoder and the\ndecoder. As for RNN, the query-aware encoder adopts a selective network to\ndistill query-relevant information from the review, while the query-aware\ndecoder integrates the query information into the attention computation during\ndecoding. The framework consistently outperforms the competing methods on both\npublic and real-world industrial datasets. Last but not least, online\ndeployment experiments on Dianping demonstrate the advantage of the proposed\nframework for tip generation as well as its online business values.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:48:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Yang", ""], ["Hao", "Junmei", ""], ["Li", "Canjia", ""], ["Wang", "Zili", ""], ["Wang", "Jingang", ""], ["Zhang", "Fuzheng", ""], ["Fu", "Rao", ""], ["Hou", "Peixu", ""], ["Zhang", "Gong", ""], ["Wang", "Zhongyuan", ""]]}, {"id": "2010.09270", "submitter": "Boliang Zhang", "authors": "Boliang Zhang, Spencer Whitehead, Lifu Huang and Heng Ji", "title": "Global Attention for Name Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many name tagging approaches use local contextual information with much\nsuccess, but fail when the local context is ambiguous or limited. We present a\nnew framework to improve name tagging by utilizing local, document-level, and\ncorpus-level contextual information. We retrieve document-level context from\nother sentences within the same document and corpus-level context from\nsentences in other topically related documents. We propose a model that learns\nto incorporate document-level and corpus-level contextual information alongside\nlocal contextual information via global attentions, which dynamically weight\ntheir respective contextual information, and gating mechanisms, which determine\nthe influence of this information. Extensive experiments on benchmark datasets\nshow the effectiveness of our approach, which achieves state-of-the-art results\nfor Dutch, German, and Spanish on the CoNLL-2002 and CoNLL-2003 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:27:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Boliang", ""], ["Whitehead", "Spencer", ""], ["Huang", "Lifu", ""], ["Ji", "Heng", ""]]}, {"id": "2010.09313", "submitter": "Jonas Wallat", "authors": "Jonas Wallat, Jaspreet Singh, Avishek Anand", "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT", "comments": "BBNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\npaper, we probe BERT specifically to understand and measure the relational\nknowledge it captures. We utilize knowledge base completion tasks to probe\nevery layer of pre-trained as well as fine-tuned BERT (ranking, question\nanswering, NER). Our findings show that knowledge is not just contained in\nBERT's final layers. Intermediate layers contribute a significant amount\n(17-60%) to the total knowledge found. Probing intermediate layers also reveals\nhow different types of knowledge emerge at varying rates. When BERT is\nfine-tuned, relational knowledge is forgotten but the extent of forgetting is\nimpacted by the fine-tuning objective but not the size of the dataset. We found\nthat ranking models forget the least and retain more knowledge in their final\nlayer. We release our code on github to repeat the experiments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:46:30 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wallat", "Jonas", ""], ["Singh", "Jaspreet", ""], ["Anand", "Avishek", ""]]}, {"id": "2010.09322", "submitter": "Anuj Diwan", "authors": "Anuj Diwan, Preethi Jyothi", "title": "Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages", "comments": "5 pages, 1 figure. Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a seemingly simple but effective technique to improve\nlow-resource ASR systems for phonetic languages. By identifying sets of\nacoustically similar graphemes in these languages, we first reduce the output\nalphabet of the ASR system using linguistically meaningful reductions and then\nreconstruct the original alphabet using a standalone module. We demonstrate\nthat this lessens the burden and improves the performance of low-resource\nend-to-end ASR systems (because only reduced-alphabet predictions are needed)\nand that it is possible to design a very simple but effective reconstruction\nmodule that recovers sequences in the original alphabet from sequences in the\nreduced alphabet. We present a finite state transducer-based reconstruction\nmodule that operates on the 1-best ASR hypothesis in the reduced alphabet. We\ndemonstrate the efficacy of our proposed technique using ASR systems for two\nIndian languages, Gujarati and Telugu. With access to only 10 hrs of speech\ndata, we obtain relative WER reductions of up to 7% compared to systems that do\nnot use any reduction.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:59:58 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 10:11:49 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Diwan", "Anuj", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2010.09366", "submitter": "Xiaoyu Guo", "authors": "Xiao-Yu Guo and Yuan-Fang Li and Gholamreza Haffari", "title": "Understanding Unnatural Questions Improves Reasoning over Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex question answering (CQA) over raw text is a challenging task. A\nprominent approach to this task is based on the programmer-interpreter\nframework, where the programmer maps the question into a sequence of reasoning\nactions which is then executed on the raw text by the interpreter. Learning an\neffective CQA model requires large amounts of human-annotated data,consisting\nof the ground-truth sequence of reasoning actions, which is time-consuming and\nexpensive to collect at scale. In this paper, we address the challenge of\nlearning a high-quality programmer (parser) by projecting natural\nhuman-generated questions into unnatural machine-generated questions which are\nmore convenient to parse. We firstly generate synthetic (question,action\nsequence) pairs by a data generator, and train a semantic parser that\nassociates synthetic questions with their corresponding action sequences. To\ncapture the diversity when applied tonatural questions, we learn a projection\nmodel to map natural questions into their most similar unnatural questions for\nwhich the parser can work well. Without any natural training data, our\nprojection model provides high-quality action sequences for the CQA task.\nExperimental results show that the QA model trained exclusively with synthetic\ndata generated by our method outperforms its state-of-the-art counterpart\ntrained on human-labeled data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 10:22:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Guo", "Xiao-Yu", ""], ["Li", "Yuan-Fang", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2010.09381", "submitter": "Abdullatif K\\\"oksal", "authors": "Abdullatif K\\\"oksal, Arzucan \\\"Ozg\\\"ur", "title": "The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual\n  Relation Classification", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification is one of the key topics in information extraction,\nwhich can be used to construct knowledge bases or to provide useful information\nfor question answering. Current approaches for relation classification are\nmainly focused on the English language and require lots of training data with\nhuman annotations. Creating and annotating a large amount of training data for\nlow-resource languages is impractical and expensive. To overcome this issue, we\npropose two cross-lingual relation classification models: a baseline model\nbased on Multilingual BERT and a new multilingual pretraining setup, which\nsignificantly improves the baseline with distant supervision. For evaluation,\nwe introduce a new public benchmark dataset for cross-lingual relation\nclassification in English, French, German, Spanish, and Turkish, called RELX.\nWe also provide the RELX-Distant dataset, which includes hundreds of thousands\nof sentences with relations from Wikipedia and Wikidata collected by distant\nsupervision for these languages. Our code and data are available at:\nhttps://github.com/boun-tabi/RELX\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:08:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["K\u00f6ksal", "Abdullatif", ""], ["\u00d6zg\u00fcr", "Arzucan", ""]]}, {"id": "2010.09402", "submitter": "Sungwon Lyu", "authors": "Sungwon Lyu, Bokyung Son, Kichang Yang, and Jaekyoung Bae", "title": "Revisiting Modularized Multilingual NMT to Meet Industrial Demands", "comments": "The 2020 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complete sharing of parameters for multilingual translation (1-1) has\nbeen the mainstream approach in current research. However, degraded performance\ndue to the capacity bottleneck and low maintainability hinders its extensive\nadoption in industries. In this study, we revisit the multilingual neural\nmachine translation model that only share modules among the same languages (M2)\nas a practical alternative to 1-1 to satisfy industrial requirements. Through\ncomprehensive experiments, we identify the benefits of multi-way training and\ndemonstrate that the M2 can enjoy these benefits without suffering from the\ncapacity bottleneck. Furthermore, the interlingual space of the M2 allows\nconvenient modification of the model. By leveraging trained modules, we find\nthat incrementally added modules exhibit better performance than singly trained\nmodels. The zero-shot performance of the added modules is even comparable to\nsupervised models. Our findings suggest that the M2 can be a competent\ncandidate for multilingual translation in industries.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:51:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Lyu", "Sungwon", ""], ["Son", "Bokyung", ""], ["Yang", "Kichang", ""], ["Bae", "Jaekyoung", ""]]}, {"id": "2010.09403", "submitter": "Du\\v{s}an Vari\\v{s}", "authors": "Du\\v{s}an Vari\\v{s} and Ond\\v{r}ej Bojar", "title": "Unsupervised Pretraining for Neural Machine Translation Using Elastic\n  Weight Consolidation", "comments": "ACL-SRW 2019 (camera-ready)", "journal-ref": null, "doi": "10.18653/v1/P19-2017", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents our ongoing research of unsupervised pretraining in neural\nmachine translation (NMT). In our method, we initialize the weights of the\nencoder and decoder with two language models that are trained with monolingual\ndata and then fine-tune the model on parallel data using Elastic Weight\nConsolidation (EWC) to avoid forgetting of the original language modeling\ntasks. We compare the regularization by EWC with the previous work that focuses\non regularization by language modeling objectives. The positive result is that\nusing EWC with the decoder achieves BLEU scores similar to the previous work.\nHowever, the model converges 2-3 times faster and does not require the original\nunlabeled training data during the fine-tuning stage. In contrast, the\nregularization using EWC is less effective if the original and new tasks are\nnot closely related. We show that initializing the bidirectional NMT encoder\nwith a left-to-right language model and forcing the model to remember the\noriginal left-to-right language modeling task limits the learning capacity of\nthe encoder for the whole bidirectional context.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:51:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Vari\u0161", "Du\u0161an", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2010.09413", "submitter": "Du\\v{s}an Vari\\v{s}", "authors": "Du\\v{s}an Vari\\v{s}, Katsuhito Sudoh, and Satoshi Nakamura", "title": "Image Captioning with Visual Object Representations Grounded in the\n  Textual Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our work in progress exploring the possibilities of a shared\nembedding space between textual and visual modality. Leveraging the textual\nnature of object detection labels and the hypothetical expressiveness of\nextracted visual object representations, we propose an approach opposite to the\ncurrent trend, grounding of the representations in the word embedding space of\nthe captioning system instead of grounding words or sentences in their\nassociated images. Based on the previous work, we apply additional grounding\nlosses to the image captioning training objective aiming to force visual object\nrepresentations to create more heterogeneous clusters based on their class\nlabel and copy a semantic structure of the word embedding space. In addition,\nwe provide an analysis of the learned object vector space projection and its\nimpact on the IC system performance. With only slight change in performance,\ngrounded models reach the stopping criterion during training faster than the\nunconstrained model, needing about two to three times less training updates.\nAdditionally, an improvement in structural correlation between the word\nembeddings and both original and projected object vectors suggests that the\ngrounding is actually mutual.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:21:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:24:39 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Vari\u0161", "Du\u0161an", ""], ["Sudoh", "Katsuhito", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "2010.09459", "submitter": "Leshem Choshen", "authors": "Eyal Shnarch, Leshem Choshen, Guy Moshkowich, Noam Slonim, Ranit\n  Aharonov", "title": "Unsupervised Expressive Rules Provide Explainability and Assist Human\n  Experts Grasping New Domains", "comments": "Accepted to Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaching new data can be quite deterrent; you do not know how your\ncategories of interest are realized in it, commonly, there is no labeled data\nat hand, and the performance of domain adaptation methods is unsatisfactory.\n  Aiming to assist domain experts in their first steps into a new task over a\nnew corpus, we present an unsupervised approach to reveal complex rules which\ncluster the unexplored corpus by its prominent categories (or facets).\n  These rules are human-readable, thus providing an important ingredient which\nhas become in short supply lately - explainability. Each rule provides an\nexplanation for the commonality of all the texts it clusters together.\n  We present an extensive evaluation of the usefulness of these rules in\nidentifying target categories, as well as a user study which assesses their\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:07:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Shnarch", "Eyal", ""], ["Choshen", "Leshem", ""], ["Moshkowich", "Guy", ""], ["Slonim", "Noam", ""], ["Aharonov", "Ranit", ""]]}, {"id": "2010.09482", "submitter": "Shahram Khadivi", "authors": "Jingjing Huo, Christian Herold, Yingbo Gao, Leonard Dahlmann, Shahram\n  Khadivi, and Hermann Ney", "title": "Diving Deep into Context-Aware Neural Machine Translation", "comments": "Accepted at 5th Conference on Machine Translation (WMT20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware neural machine translation (NMT) is a promising direction to\nimprove the translation quality by making use of the additional context, e.g.,\ndocument-level translation, or having meta-information. Although there exist\nvarious architectures and analyses, the effectiveness of different\ncontext-aware NMT models is not well explored yet. This paper analyzes the\nperformance of document-level NMT models on four diverse domains with a varied\namount of parallel document-level bilingual data. We conduct a comprehensive\nset of experiments to investigate the impact of document-level NMT. We find\nthat there is no single best approach to document-level NMT, but rather that\ndifferent architectures come out on top on different tasks. Looking at\ntask-specific problems, such as pronoun resolution or headline translation, we\nfind improvements in the context-aware systems, even in cases where the\ncorpus-level metrics like BLEU show no significant improvement. We also show\nthat document-level back-translation significantly helps to compensate for the\nlack of document-level bi-texts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:23:12 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Huo", "Jingjing", ""], ["Herold", "Christian", ""], ["Gao", "Yingbo", ""], ["Dahlmann", "Leonard", ""], ["Khadivi", "Shahram", ""], ["Ney", "Hermann", ""]]}, {"id": "2010.09517", "submitter": "Bowen Li", "authors": "Bowen Li, Taeuk Kim, Reinald Kim Amplayo, Frank Keller", "title": "Heads-up! Unsupervised Constituency Parsing via Self-Attention Heads", "comments": "AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based pre-trained language models (PLMs) have dramatically\nimproved the state of the art in NLP across many tasks. This has led to\nsubstantial interest in analyzing the syntactic knowledge PLMs learn. Previous\napproaches to this question have been limited, mostly using test suites or\nprobes. Here, we propose a novel fully unsupervised parsing approach that\nextracts constituency trees from PLM attention heads. We rank transformer\nattention heads based on their inherent properties, and create an ensemble of\nhigh-ranking heads to produce the final tree. Our method is adaptable to\nlow-resource languages, as it does not rely on development sets, which can be\nexpensive to annotate. Our experiments show that the proposed method often\noutperform existing approaches if there is no development set present. Our\nunsupervised parser can also be used as a tool to analyze the grammars PLMs\nlearn implicitly. For this, we use the parse trees induced by our method to\ntrain a neural PCFG and compare it to a grammar derived from a human-annotated\ntreebank.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:51:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Bowen", ""], ["Kim", "Taeuk", ""], ["Amplayo", "Reinald Kim", ""], ["Keller", "Frank", ""]]}, {"id": "2010.09522", "submitter": "Devamanyu Hazarika", "authors": "Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumdar,\n  Soujanya Poria, Roger Zimmermann, and Amir Zadeh", "title": "Multimodal Research in Vision and Language: A Review of Current and\n  Emerging Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep Learning and its applications have cascaded impactful research and\ndevelopment with a diverse range of modalities present in the real-world data.\nMore recently, this has enhanced research interests in the intersection of the\nVision and Language arena with its numerous applications and fast-paced growth.\nIn this paper, we present a detailed overview of the latest trends in research\npertaining to visual and language modalities. We look at its applications in\ntheir task formulations and how to solve various problems related to semantic\nperception and content generation. We also address task-specific trends, along\nwith their evaluation strategies and upcoming challenges. Moreover, we shed\nsome light on multi-disciplinary patterns and insights that have emerged in the\nrecent past, directing this field towards more modular and transparent\nintelligent systems. This survey identifies key trends gravitating recent\nliterature in VisLang research and attempts to unearth directions that the\nfield is heading towards.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:55:10 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 04:43:20 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Uppal", "Shagun", ""], ["Bhagat", "Sarthak", ""], ["Hazarika", "Devamanyu", ""], ["Majumdar", "Navonil", ""], ["Poria", "Soujanya", ""], ["Zimmermann", "Roger", ""], ["Zadeh", "Amir", ""]]}, {"id": "2010.09535", "submitter": "Michelle Yuan", "authors": "Michelle Yuan, Hsuan-Tien Lin, Jordan Boyd-Graber", "title": "Cold-start Active Learning through Self-supervised Language Modeling", "comments": "Published in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning strives to reduce annotation costs by choosing the most\ncritical examples to label. Typically, the active learning strategy is\ncontingent on the classification model. For instance, uncertainty sampling\ndepends on poorly calibrated model confidence scores. In the cold-start\nsetting, active learning is impractical because of model instability and data\nscarcity. Fortunately, modern NLP provides an additional source of information:\npre-trained language models. The pre-training loss can find examples that\nsurprise the model and should be labeled for efficient fine-tuning. Therefore,\nwe treat the language modeling loss as a proxy for classification uncertainty.\nWith BERT, we develop a simple strategy based on the masked language modeling\nloss that minimizes labeling costs for text classification. Compared to other\nbaselines, our approach reaches higher accuracy within less sampling iterations\nand computation time.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:09:17 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:51:04 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yuan", "Michelle", ""], ["Lin", "Hsuan-Tien", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2010.09598", "submitter": "Jeroen Offerijns", "authors": "Jeroen Offerijns, Suzan Verberne, Tessa Verhoef", "title": "Better Distractions: Transformer-based Distractor Generation and\n  Multiple Choice Question Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the field of education, being able to generate semantically correct and\neducationally relevant multiple choice questions (MCQs) could have a large\nimpact. While question generation itself is an active research topic,\ngenerating distractors (the incorrect multiple choice options) receives much\nless attention. A missed opportunity, since there is still a lot of room for\nimprovement in this area. In this work, we train a GPT-2 language model to\ngenerate three distractors for a given question and text context, using the\nRACE dataset. Next, we train a BERT language model to answer MCQs, and use this\nmodel as a filter, to select only questions that can be answered and therefore\npresumably make sense. To evaluate our work, we start by using text generation\nmetrics, which show that our model outperforms earlier work on distractor\ngeneration (DG) and achieves state-of-the-art performance. Also, by calculating\nthe question answering ability, we show that larger base models lead to better\nperformance. Moreover, we conducted a human evaluation study, which confirmed\nthe quality of the generated questions, but showed no statistically significant\neffect of the QA filter.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:23:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Offerijns", "Jeroen", ""], ["Verberne", "Suzan", ""], ["Verhoef", "Tessa", ""]]}, {"id": "2010.09600", "submitter": "Dalton Schutte", "authors": "Rui Zhang, Dimitar Hristovski, Dalton Schutte, Andrej Kastrin, Marcelo\n  Fiszman, Halil Kilicoglu", "title": "Drug Repurposing for COVID-19 via Knowledge Graph Completion", "comments": "47 pages, 3 figures, Accepted to Journal of Biomedical Informatics", "journal-ref": null, "doi": "10.1016/j.jbi.2021.103696", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To discover candidate drugs to repurpose for COVID-19 using\nliterature-derived knowledge and knowledge graph completion methods. Methods:\nWe propose a novel, integrative, and neural network-based literature-based\ndiscovery (LBD) approach to identify drug candidates from both PubMed and\nCOVID-19-focused research literature. Our approach relies on semantic triples\nextracted using SemRep (via SemMedDB). We identified an informative subset of\nsemantic triples using filtering rules and an accuracy classifier developed on\na BERT variant, and used this subset to construct a knowledge graph. Five SOTA,\nneural knowledge graph completion algorithms were used to predict drug\nrepurposing candidates. The models were trained and assessed using a time\nslicing approach and the predicted drugs were compared with a list of drugs\nreported in the literature and evaluated in clinical trials. These models were\ncomplemented by a discovery pattern-based approach. Results: Accuracy\nclassifier based on PubMedBERT achieved the best performance (F1= 0.854) in\nclassifying semantic predications. Among five knowledge graph completion\nmodels, TransE outperformed others (MR = 0.923, Hits@1=0.417). Some known drugs\nlinked to COVID-19 in the literature were identified, as well as some candidate\ndrugs that have not yet been studied. Discovery patterns enabled generation of\nplausible hypotheses regarding the relationships between the candidate drugs\nand COVID-19. Among them, five highly ranked and novel drugs (paclitaxel, SB\n203580, alpha 2-antiplasmin, pyrrolidine dithiocarbamate, and butylated\nhydroxytoluene) with their mechanistic explanations were further discussed.\nConclusion: We show that an LBD approach can be feasible for discovering drug\ncandidates for COVID-19, and for generating mechanistic explanations. Our\napproach can be generalized to other diseases as well as to other clinical\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:30:51 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:23:14 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Rui", ""], ["Hristovski", "Dimitar", ""], ["Schutte", "Dalton", ""], ["Kastrin", "Andrej", ""], ["Fiszman", "Marcelo", ""], ["Kilicoglu", "Halil", ""]]}, {"id": "2010.09602", "submitter": "Yusuke Yasuda", "authors": "Yusuke Yasuda, Xin Wang, Junichi Yamagishi", "title": "End-to-End Text-to-Speech using Latent Duration based on VQ-VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit duration modeling is a key to achieving robust and efficient\nalignment in text-to-speech synthesis (TTS). We propose a new TTS framework\nusing explicit duration modeling that incorporates duration as a discrete\nlatent variable to TTS and enables joint optimization of whole modules from\nscratch. We formulate our method based on conditional VQ-VAE to handle discrete\nduration in a variational autoencoder and provide a theoretical explanation to\njustify our method. In our framework, a connectionist temporal classification\n(CTC) -based force aligner acts as the approximate posterior, and\ntext-to-duration works as the prior in the variational autoencoder. We\nevaluated our proposed method with a listening test and compared it with other\nTTS methods based on soft-attention or explicit duration modeling. The results\nshowed that our systems rated between soft-attention-based methods\n(Transformer-TTS, Tacotron2) and explicit duration modeling-based methods\n(Fastspeech).\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:34:49 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:46:35 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Yasuda", "Yusuke", ""], ["Wang", "Xin", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "2010.09608", "submitter": "David Wan", "authors": "David Wan, Chris Kedzie, Faisal Ladhak, Marine Carpuat and Kathleen\n  McKeown", "title": "Incorporating Terminology Constraints in Automatic Post-Editing", "comments": "To appear in WMT, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of machine translation (MT) may want to ensure the use of specific\nlexical terminologies. While there exist techniques for incorporating\nterminology constraints during inference for MT, current APE approaches cannot\nensure that they will appear in the final translation. In this paper, we\npresent both autoregressive and non-autoregressive models for lexically\nconstrained APE, demonstrating that our approach enables preservation of 95% of\nthe terminologies and also improves translation quality on English-German\nbenchmarks. Even when applied to lexically constrained MT output, our approach\nis able to improve preservation of the terminologies. However, we show that our\nmodels do not learn to copy constraints systematically and suggest a simple\ndata augmentation technique that leads to improved performance and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:44:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wan", "David", ""], ["Kedzie", "Chris", ""], ["Ladhak", "Faisal", ""], ["Carpuat", "Marine", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2010.09623", "submitter": "Vi Tran Tuan", "authors": "Tuan-Vi Tran, Xuan-Thien Pham, Duc-Vu Nguyen, Kiet Van Nguyen, Ngan\n  Luu-Thuy Nguyen", "title": "An Empirical Study for Vietnamese Constituency Parsing with Pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use a span-based approach for Vietnamese constituency\nparsing. Our method follows the self-attention encoder architecture and a chart\ndecoder using a CKY-style inference algorithm. We present analyses of the\nexperiment results of the comparison of our empirical method using pre-training\nmodels XLM-Roberta and PhoBERT on both Vietnamese datasets VietTreebank and\nNIIVTB1. The results show that our model with XLM-Roberta archived the\nsignificantly F1-score better than other pre-training models, VietTreebank at\n81.19% and NIIVTB1 at 85.70%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:02:00 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 02:44:02 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Tran", "Tuan-Vi", ""], ["Pham", "Xuan-Thien", ""], ["Nguyen", "Duc-Vu", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2010.09638", "submitter": "Jiawei Sheng", "authors": "Jiawei Sheng, Shu Guo, Zhenyu Chen, Juwei Yue, Lihong Wang, Tingwen\n  Liu and Hongbo Xu", "title": "Adaptive Attentional Network for Few-Shot Knowledge Graph Completion", "comments": "11 pages, 3 figures", "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot Knowledge Graph (KG) completion is a focus of current research,\nwhere each task aims at querying unseen facts of a relation given its few-shot\nreference entity pairs. Recent attempts solve this problem by learning static\nrepresentations of entities and references, ignoring their dynamic properties,\ni.e., entities may exhibit diverse roles within task relations, and references\nmay make different contributions to queries. This work proposes an adaptive\nattentional network for few-shot KG completion by learning adaptive entity and\nreference representations. Specifically, entities are modeled by an adaptive\nneighbor encoder to discern their task-oriented roles, while references are\nmodeled by an adaptive query-aware aggregator to differentiate their\ncontributions. Through the attention mechanism, both entities and references\ncan capture their fine-grained semantic meanings, and thus render more\nexpressive representations. This will be more predictive for knowledge\nacquisition in the few-shot scenario. Evaluation in link prediction on two\npublic datasets shows that our approach achieves new state-of-the-art results\nwith different few-shot sizes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:27:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sheng", "Jiawei", ""], ["Guo", "Shu", ""], ["Chen", "Zhenyu", ""], ["Yue", "Juwei", ""], ["Wang", "Lihong", ""], ["Liu", "Tingwen", ""], ["Xu", "Hongbo", ""]]}, {"id": "2010.09657", "submitter": "Nipun Sadvilkar", "authors": "Nipun Sadvilkar and Mark Neumann", "title": "PySBD: Pragmatic Sentence Boundary Disambiguation", "comments": "'PySBD: Pragmatic Sentence Boundary Disambiguation' is a short paper\n  (5 Pages with references) accepted into 2nd Workshop for Natural Language\n  Processing Open Source Software (NLP-OSS) at EMNLP 2020 happening on 19 Nov\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a rule-based sentence boundary disambiguation\nPython package that works out-of-the-box for 22 languages. We aim to provide a\nrealistic segmenter which can provide logical sentences even when the format\nand domain of the input text is unknown. In our work, we adapt the Golden Rules\nSet (a language-specific set of sentence boundary exemplars) originally\nimplemented as a ruby gem - pragmatic_segmenter - which we ported to Python\nwith additional improvements and functionality. PySBD passes 97.92% of the\nGolden Rule Set exemplars for English, an improvement of 25% over the next best\nopen-source Python tool.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:56:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sadvilkar", "Nipun", ""], ["Neumann", "Mark", ""]]}, {"id": "2010.09692", "submitter": "Xusen Yin", "authors": "Xusen Yin, Li Zhou, Kevin Small, Jonathan May", "title": "Summary-Oriented Question Generation for Informational Queries", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users frequently ask simple factoid questions for question answering (QA)\nsystems, attenuating the impact of myriad recent works that support more\ncomplex questions. Prompting users with automatically generated suggested\nquestions (SQs) can improve user understanding of QA system capabilities and\nthus facilitate more effective use. We aim to produce self-explanatory\nquestions that focus on main document topics and are answerable with variable\nlength passages as appropriate. We satisfy these requirements by using a\nBERT-based Pointer-Generator Network trained on the Natural Questions (NQ)\ndataset. Our model shows SOTA performance of SQ generation on the NQ dataset\n(20.1 BLEU-4). We further apply our model on out-of-domain news articles,\nevaluating with a QA system due to the lack of gold questions and demonstrate\nthat our model produces better SQs for news articles -- with further\nconfirmation via a human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:30:08 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 17:24:01 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Yin", "Xusen", ""], ["Zhou", "Li", ""], ["Small", "Kevin", ""], ["May", "Jonathan", ""]]}, {"id": "2010.09693", "submitter": "David Wan", "authors": "David Wan, Zhengping Jiang, Chris Kedzie, Elsbeth Turcan, Peter Bell\n  and Kathleen McKeown", "title": "Subtitles to Segmentation: Improving Low-Resource Speech-to-Text\n  Translation Pipelines", "comments": null, "journal-ref": "CLSST@LREC 2020 68-73", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on improving ASR output segmentation in the context of\nlow-resource language speech-to-text translation. ASR output segmentation is\ncrucial, as ASR systems segment the input audio using purely acoustic\ninformation and are not guaranteed to output sentence-like segments. Since most\nMT systems expect sentences as input, feeding in longer unsegmented passages\ncan lead to sub-optimal performance. We explore the feasibility of using\ndatasets of subtitles from TV shows and movies to train better ASR segmentation\nmodels. We further incorporate part-of-speech (POS) tag and dependency label\ninformation (derived from the unsegmented ASR outputs) into our segmentation\nmodel. We show that this noisy syntactic information can improve model\naccuracy. We evaluate our models intrinsically on segmentation quality and\nextrinsically on downstream MT performance, as well as downstream tasks\nincluding cross-lingual information retrieval (CLIR) tasks and human relevance\nassessments. Our model shows improved performance on downstream tasks for\nLithuanian and Bulgarian.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:32:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wan", "David", ""], ["Jiang", "Zhengping", ""], ["Kedzie", "Chris", ""], ["Turcan", "Elsbeth", ""], ["Bell", "Peter", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2010.09697", "submitter": "William Merrill", "authors": "William Merrill and Vivek Ramanujan and Yoav Goldberg and Roy Schwartz\n  and Noah Smith", "title": "Parameter Norm Growth During Training of Transformers", "comments": "Preprint. 9 body pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically some variant of gradient\ndescent (GD). To better understand this bias, we study the tendency of\ntransformer parameters to grow in magnitude during training. We find, both\ntheoretically and empirically, that, in certain contexts, GD increases the\nparameter $L_2$ norm up to a threshold that itself increases with training-set\naccuracy. This means increasing training accuracy over time enables the norm to\nincrease. Empirically, we show that the norm grows continuously over\npretraining for T5 (Raffel et al., 2019). We show that pretrained T5\napproximates a semi-discretized network with saturated activation functions.\nSuch \"saturated\" networks are known to have a reduced capacity compared to the\noriginal network family that can be described in automata-theoretic terms. This\nsuggests saturation is a new characterization of an inductive bias implicit in\nGD that is of particular interest for NLP. While our experiments focus on\ntransformers, our theoretical analysis extends to other architectures with\nsimilar formal properties, such as feedforward ReLU networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:40:38 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 10:26:55 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Merrill", "William", ""], ["Ramanujan", "Vivek", ""], ["Goldberg", "Yoav", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah", ""]]}, {"id": "2010.09780", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Lingfei Wu, Yu Deng, Qingkai Zeng, Ruchi Mahindru, Sinem\n  Guven, Meng Jiang", "title": "Technical Question Answering across Tasks and Domains", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building automatic technical support system is an important yet challenge\ntask. Conceptually, to answer a user question on a technical forum, a human\nexpert has to first retrieve relevant documents, and then read them carefully\nto identify the answer snippet. Despite huge success the researchers have\nachieved in coping with general domain question answering (QA), much less\nattentions have been paid for investigating technical QA. Specifically,\nexisting methods suffer from several unique challenges (i) the question and\nanswer rarely overlaps substantially and (ii) very limited data size. In this\npaper, we propose a novel framework of deep transfer learning to effectively\naddress technical QA across tasks and domains. To this end, we present an\nadjustable joint learning approach for document retrieval and reading\ncomprehension tasks. Our experiments on the TechQA demonstrates superior\nperformance compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:39:30 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 05:00:18 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yu", "Wenhao", ""], ["Wu", "Lingfei", ""], ["Deng", "Yu", ""], ["Zeng", "Qingkai", ""], ["Mahindru", "Ruchi", ""], ["Guven", "Sinem", ""], ["Jiang", "Meng", ""]]}, {"id": "2010.09788", "submitter": "Yufei Feng", "authors": "Mo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan,\n  Murray Campbell", "title": "Deriving Commonsense Inference Tasks from Interactive Fictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning simulates the human ability to make presumptions about\nour physical world, and it is an indispensable cornerstone in building general\nAI systems. We propose a new commonsense reasoning dataset based on human's\ninteractive fiction game playings as human players demonstrate plentiful and\ndiverse commonsense reasoning. The new dataset mitigates several limitations of\nthe prior art. Experiments show that our task is solvable to human experts with\nsufficient commonsense knowledge but poses challenges to existing machine\nreading models, with a big performance gap of more than 30%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:02:34 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Feng", "Yufei", ""], ["Zhu", "Xiaodan", ""], ["Greenspan", "Michael", ""], ["Campbell", "Murray", ""]]}, {"id": "2010.09803", "submitter": "Jie Zhao", "authors": "Jie Zhao, Huan Sun", "title": "Adversarial Training for Code Retrieval with Question-Description\n  Relevance Regularization", "comments": "Accepted to Findings of EMNLP 2020. 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code retrieval is a key task aiming to match natural and programming\nlanguages. In this work, we propose adversarial learning for code retrieval,\nthat is regularized by question-description relevance. First, we adapt a simple\nadversarial learning technique to generate difficult code snippets given the\ninput question, which can help the learning of code retrieval that faces\nbi-modal and data-scarce challenges. Second, we propose to leverage\nquestion-description relevance to regularize adversarial learning, such that a\ngenerated code snippet should contribute more to the code retrieval training\nloss, only if its paired natural language description is predicted to be less\nrelevant to the user given question. Experiments on large-scale code retrieval\ndatasets of two programming languages show that our adversarial learning method\nis able to improve the performance of state-of-the-art models. Moreover, using\nan additional duplicate question prediction model to regularize adversarial\nlearning further improves the performance, and this is more effective than\nusing the duplicated questions in strong multi-task learning baselines\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:32:03 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 05:49:02 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhao", "Jie", ""], ["Sun", "Huan", ""]]}, {"id": "2010.09828", "submitter": "Elliot Schumacher", "authors": "Elliot Schumacher, James Mayfield, Mark Dredze", "title": "Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking", "comments": "Accepted in the Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-language entity linking grounds mentions in multiple languages to a\nsingle-language knowledge base. We propose a neural ranking architecture for\nthis task that uses multilingual BERT representations of the mention and the\ncontext in a neural network. We find that the multilingual ability of BERT\nleads to robust performance in monolingual and multilingual settings.\nFurthermore, we explore zero-shot language transfer and find surprisingly\nrobust performance. We investigate the zero-shot degradation and find that it\ncan be partially mitigated by a proposed auxiliary training objective, but that\nthe remaining error can best be attributed to domain shift rather than language\ntransfer.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:08:26 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 19:44:49 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Schumacher", "Elliot", ""], ["Mayfield", "James", ""], ["Dredze", "Mark", ""]]}, {"id": "2010.09885", "submitter": "Seyone Chithrananda", "authors": "Seyone Chithrananda, Gabriel Grand and Bharath Ramsundar", "title": "ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular\n  Property Prediction", "comments": "Submitted to NeurIPS 2020 ML for Molecules Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL physics.chem-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GNNs and chemical fingerprints are the predominant approaches to representing\nmolecules for property prediction. However, in NLP, transformers have become\nthe de-facto standard for representation learning thanks to their strong\ndownstream task transfer. In parallel, the software ecosystem around\ntransformers is maturing rapidly, with libraries like HuggingFace and BertViz\nenabling streamlined training and introspection. In this work, we make one of\nthe first attempts to systematically evaluate transformers on molecular\nproperty prediction tasks via our ChemBERTa model. ChemBERTa scales well with\npretraining dataset size, offering competitive downstream performance on\nMoleculeNet and useful attention-based visualization modalities. Our results\nsuggest that transformers offer a promising avenue of future work for molecular\nrepresentation learning and property prediction. To facilitate these efforts,\nwe release a curated dataset of 77M SMILES from PubChem suitable for\nlarge-scale self-supervised pretraining.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 21:41:41 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 04:22:37 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chithrananda", "Seyone", ""], ["Grand", "Gabriel", ""], ["Ramsundar", "Bharath", ""]]}, {"id": "2010.09905", "submitter": "Ilya Valmianski", "authors": "Ilya Valmianski, Ian M. Finn, Nave Frost, Yang Wang, Baodong Liu,\n  James J. Zhu, Sunil Karumuri and Daniel S. Zisook", "title": "SmartTriage: A system for personalized patient data capture,\n  documentation generation, and decision support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symptom checkers have emerged as an important tool for collecting symptoms\nand diagnosing patients, minimizing the involvement of clinical personnel. We\ndeveloped a machine-learning-backed system, SmartTriage, which goes beyond\nconventional symptom checking through a tight bi-directional integration with\nthe electronic medical record (EMR). Conditioned on EMR-derived patient\nhistory, our system identifies the patient's chief complaint from a free-text\nentry and then asks a series of discrete questions to obtain relevant\nsymptomatology. The patient-specific data are used to predict detailed\nICD-10-CM codes as well as medication, laboratory, and imaging orders. Patient\nresponses and clinical decision support (CDS) predictions are then inserted\nback into the EMR. To train the machine learning components of SmartTriage, we\nemployed novel data sets of over 25 million primary care encounters and 1\nmillion patient free-text reason-for-visit entries. These data sets were used\nto construct: (1) a long short-term memory (LSTM) based patient history\nrepresentation, (2) a fine-tuned transformer model for chief complaint\nextraction, (3) a random forest model for question sequencing, and (4) a\nfeed-forward network for CDS predictions. We also present the full production\narchitecture for the pilot deployment of SmartTriage that covers 337 patient\nchief complaints.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:45:27 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 01:28:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Valmianski", "Ilya", ""], ["Finn", "Ian M.", ""], ["Frost", "Nave", ""], ["Wang", "Yang", ""], ["Liu", "Baodong", ""], ["Zhu", "James J.", ""], ["Karumuri", "Sunil", ""], ["Zisook", "Daniel S.", ""]]}, {"id": "2010.09926", "submitter": "Neema Kotonya", "authors": "Neema Kotonya and Francesca Toni", "title": "Explainable Automated Fact-Checking for Public Health Claims", "comments": "Accepted to EMNLP 2020. 15 pages, 7 figures, 9 tables. The dataset is\n  available at https://github.com/neemakot/Health-Fact-Checking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact-checking is the task of verifying the veracity of claims by assessing\ntheir assertions against credible evidence. The vast majority of fact-checking\nstudies focus exclusively on political claims. Very little research explores\nfact-checking for other topics, specifically subject matters for which\nexpertise is required. We present the first study of explainable fact-checking\nfor claims which require specific expertise. For our case study we choose the\nsetting of public health. To support this case study we construct a new dataset\nPUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard\nexplanations (i.e., judgments) to support the fact-check labels for claims. We\nexplore two tasks: veracity prediction and explanation generation. We also\ndefine and evaluate, with humans and computationally, three coherence\nproperties of explanation quality. Our results indicate that, by training on\nin-domain data, gains can be made in explainable, automated fact-checking for\nclaims which require specific expertise.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:51:33 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Kotonya", "Neema", ""], ["Toni", "Francesca", ""]]}, {"id": "2010.09927", "submitter": "Arvind Srikantan", "authors": "Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin", "title": "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries", "comments": "IntEx-SemPar Workshop at EMNLP 2020, 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating natural language utterances to executable queries is a helpful\ntechnique in making the vast amount of data stored in relational databases\naccessible to a wider range of non-tech-savvy end users. Prior work in this\narea has largely focused on textual input that is linguistically correct and\nsemantically unambiguous. However, real-world user queries are often succinct,\ncolloquial, and noisy, resembling the input of a search engine. In this work,\nwe introduce data augmentation techniques and a sampling-based content-aware\nBERT model (ColloQL) to achieve robust text-to-SQL modeling over natural\nlanguage search (NLS) questions. Due to the lack of evaluation data, we curate\na new dataset of NLS questions and demonstrate the efficacy of our approach.\nColloQL's superior performance extends to well-formed text, achieving 84.9%\n(logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to\nthe best of our knowledge, the highest performing model that does not use\nexecution guided decoding.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:53:17 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Radhakrishnan", "Karthik", ""], ["Srikantan", "Arvind", ""], ["Lin", "Xi Victoria", ""]]}, {"id": "2010.09934", "submitter": "Chengzhi Zhang", "authors": "Yingyi Zhang and Chengzhi Zhang", "title": "Enhancing Keyphrase Extraction from Microblogs using Human Reading Time", "comments": null, "journal-ref": "Journal of the Association for Information Science and\n  Technology,2021", "doi": "10.1002/ASI.24430", "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The premise of manual keyphrase annotation is to read the corresponding\ncontent of an annotated object. Intuitively, when we read, more important words\nwill occupy a longer reading time. Hence, by leveraging human reading time, we\ncan find the salient words in the corresponding content. However, previous\nstudies on keyphrase extraction ignore human reading features. In this article,\nwe aim to leverage human reading time to extract keyphrases from microblog\nposts. There are two main tasks in this study. One is to determine how to\nmeasure the time spent by a human on reading a word. We use eye fixation\ndurations extracted from an open source eye-tracking corpus (OSEC). Moreover,\nwe propose strategies to make eye fixation duration more effective on keyphrase\nextraction. The other task is to determine how to integrate human reading time\ninto keyphrase extraction models. We propose two novel neural network models.\nThe first is a model in which the human reading time is used as the ground\ntruth of the attention mechanism. In the second model, we use human reading\ntime as the external feature. Quantitative and qualitative experiments show\nthat our proposed models yield better performance than the baseline models on\ntwo microblog datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:18:44 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 11:24:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Yingyi", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2010.09935", "submitter": "Faeze Brahman", "authors": "Faeze Brahman, Alexandru Petrusca, and Snigdha Chaturvedi", "title": "Cue Me In: Content-Inducing Approaches to Interactive Story Generation", "comments": "AACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating stories is a challenging problem that requires\nproducing causally related and logical sequences of events about a topic.\nPrevious approaches in this domain have focused largely on one-shot generation,\nwhere a language model outputs a complete story based on limited initial input\nfrom a user. Here, we instead focus on the task of interactive story\ngeneration, where the user provides the model mid-level sentence abstractions\nin the form of cue phrases during the generation process. This provides an\ninterface for human users to guide the story generation. We present two\ncontent-inducing approaches to effectively incorporate this additional\ninformation. Experimental results from both automatic and human evaluations\nshow that these methods produce more topically coherent and personalized\nstories compared to baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:36:15 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Brahman", "Faeze", ""], ["Petrusca", "Alexandru", ""], ["Chaturvedi", "Snigdha", ""]]}, {"id": "2010.09954", "submitter": "Runzhe Yang", "authors": "Runzhe Yang, Jingxiao Chen, Karthik Narasimhan", "title": "Improving Dialog Systems for Negotiation with Personality Modeling", "comments": "ACL 2021. 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the ability to model and infer personality types of\nopponents, predict their responses, and use this information to adapt a dialog\nagent's high-level strategy in negotiation tasks. Inspired by the idea of\nincorporating a theory of mind (ToM) into machines, we introduce a\nprobabilistic formulation to encapsulate the opponent's personality type during\nboth learning and inference. We test our approach on the CraigslistBargain\ndataset and show that our method using ToM inference achieves a 20% higher\ndialog agreement rate compared to baselines on a mixed population of opponents.\nWe also find that our model displays diverse negotiation behavior with\ndifferent types of opponents.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 01:46:03 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 06:28:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yang", "Runzhe", ""], ["Chen", "Jingxiao", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2010.09997", "submitter": "Haohan Wang", "authors": "Haohan Wang, Peiyan Zhang, Eric P. Xing", "title": "Word Shape Matters: Robust Machine Translation with Visual Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation has achieved remarkable empirical performance over\nstandard benchmark datasets, yet recent evidence suggests that the models can\nstill fail easily dealing with substandard inputs such as misspelled words, To\novercome this issue, we introduce a new encoding heuristic of the input symbols\nfor character-level NLP models: it encodes the shape of each character through\nthe images depicting the letters when printed. We name this new strategy visual\nembedding and it is expected to improve the robustness of NLP models because\nhumans also process the corpus visually through printed letters, instead of\nmachinery one-hot vectors. Empirically, our method improves models' robustness\nagainst substandard inputs, even in the test scenario where the models are\ntested with the noises that are beyond what is available during the training\nphase.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:08:03 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wang", "Haohan", ""], ["Zhang", "Peiyan", ""], ["Xing", "Eric P.", ""]]}, {"id": "2010.10035", "submitter": "Neha Srikanth", "authors": "Neha Srikanth, Junyi Jessy Li", "title": "Elaborative Simplification: Content Addition and Explanation Generation\n  in Text Simplification", "comments": "To appear in Findings of ACL 2021", "journal-ref": "Findings of the Association of Computational Linguistics 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of modern-day text simplification research focuses on sentence-level\nsimplification, transforming original, more complex sentences into simplified\nversions. However, adding content can often be useful when difficult concepts\nand reasoning need to be explained. In this work, we present the first\ndata-driven study of content addition in text simplification, which we call\nelaborative simplification. We introduce a new annotated dataset of 1.3K\ninstances of elaborative simplification in the Newsela corpus, and analyze how\nentities, ideas, and concepts are elaborated through the lens of contextual\nspecificity. We establish baselines for elaboration generation using\nlarge-scale pre-trained language models, and demonstrate that considering\ncontextual specificity during generation can improve performance. Our results\nillustrate the complexities of elaborative simplification, suggesting many\ninteresting directions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:06:23 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 01:45:26 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 19:01:09 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Srikanth", "Neha", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2010.10038", "submitter": "Sameer Dharur", "authors": "Sameer Dharur, Purva Tendulkar, Dhruv Batra, Devi Parikh, Ramprasaath\n  R. Selvaraju", "title": "SOrT-ing VQA Models : Contrastive Gradient Learning for Improved\n  Consistency", "comments": "Accepted to the NeurIPS 2020 workshop on Interpretable Inductive\n  Biases and Physically Structured Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in Visual Question Answering (VQA) has revealed\nstate-of-the-art models to be inconsistent in their understanding of the world\n-- they answer seemingly difficult questions requiring reasoning correctly but\nget simpler associated sub-questions wrong. These sub-questions pertain to\nlower level visual concepts in the image that models ideally should understand\nto be able to answer the higher level question correctly. To address this, we\nfirst present a gradient-based interpretability approach to determine the\nquestions most strongly correlated with the reasoning question on an image, and\nuse this to evaluate VQA models on their ability to identify the relevant\nsub-questions needed to answer a reasoning question. Next, we propose a\ncontrastive gradient learning based approach called Sub-question Oriented\nTuning (SOrT) which encourages models to rank relevant sub-questions higher\nthan irrelevant questions for an <image, reasoning-question> pair. We show that\nSOrT improves model consistency by upto 6.5% points over existing baselines,\nwhile also improving visual grounding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:15:48 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 02:11:13 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dharur", "Sameer", ""], ["Tendulkar", "Purva", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Selvaraju", "Ramprasaath R.", ""]]}, {"id": "2010.10041", "submitter": "Chi-Liang Liu", "authors": "Chi-Liang Liu and Tsung-Yuan Hsu and Yung-Sung Chuang and Chung-Yi Li\n  and Hung-yi Lee", "title": "Language Representation in Multilingual BERT and its applications to\n  improve Cross-lingual Generalization", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A token embedding in multilingual BERT (m-BERT) contains both language and\nsemantic information. We find that representation of a language can be obtained\nby simply averaging the embeddings of the tokens of the language. With the\nlanguage representation, we can control the output languages of multilingual\nBERT by manipulating the token embeddings and achieve unsupervised token\ntranslation. We further propose a computationally cheap but effective approach\nto improve the cross-lingual ability of m-BERT based on the observation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:41:35 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 07:26:02 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 05:47:46 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Liu", "Chi-Liang", ""], ["Hsu", "Tsung-Yuan", ""], ["Chuang", "Yung-Sung", ""], ["Li", "Chung-Yi", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2010.10042", "submitter": "Yasuhide Miura", "authors": "Yasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P. Langlotz, Dan\n  Jurafsky", "title": "Improving Factual Completeness and Consistency of Image-to-Text\n  Radiology Report Generation", "comments": "Accepted to NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural image-to-text radiology report generation systems offer the potential\nto improve radiology reporting by reducing the repetitive process of report\ndrafting and identifying possible medical errors. However, existing report\ngeneration systems, despite achieving high performances on natural language\ngeneration metrics such as CIDEr or BLEU, still suffer from incomplete and\ninconsistent generations. Here we introduce two new simple rewards to encourage\nthe generation of factually complete and consistent radiology reports: one that\nencourages the system to generate radiology domain entities consistent with the\nreference, and one that uses natural language inference to encourage these\nentities to be described in inferentially consistent ways. We combine these\nwith the novel use of an existing semantic equivalence metric (BERTScore). We\nfurther propose a report generation system that optimizes these rewards via\nreinforcement learning. On two open radiology report datasets, our system\nsubstantially improved the F1 score of a clinical information extraction\nperformance by +22.1 (Delta +63.9%). We further show via a human evaluation and\na qualitative analysis that our system leads to generations that are more\nfactually complete and consistent compared to the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:42:47 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 20:41:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Miura", "Yasuhide", ""], ["Zhang", "Yuhao", ""], ["Tsai", "Emily Bao", ""], ["Langlotz", "Curtis P.", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2010.10044", "submitter": "Xiachong Feng", "authors": "Xiachong Feng, Xiaocheng Feng, Bing Qin, Ting Liu", "title": "Incorporating Commonsense Knowledge into Abstractive Dialogue\n  Summarization via Heterogeneous Graph Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive dialogue summarization is the task of capturing the highlights of\na dialogue and rewriting them into a concise version. In this paper, we present\na novel multi-speaker dialogue summarizer to demonstrate how large-scale\ncommonsense knowledge can facilitate dialogue understanding and summary\ngeneration. In detail, we consider utterance and commonsense knowledge as two\ndifferent types of data and design a Dialogue Heterogeneous Graph Network\n(D-HGN) for modeling both information. Meanwhile, we also add speakers as\nheterogeneous nodes to facilitate information flow. Experimental results on the\nSAMSum dataset show that our model can outperform various methods. We also\nconduct zero-shot setting experiments on the Argumentative Dialogue Summary\nCorpus, the results show that our model can better generalized to the new\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:44:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Feng", "Xiachong", ""], ["Feng", "Xiaocheng", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""]]}, {"id": "2010.10048", "submitter": "Renjie Zheng", "authors": "Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Jiahong Yuan,\n  Kenneth Church, Liang Huang", "title": "Fluent and Low-latency Simultaneous Speech-to-Speech Translation with\n  Self-adaptive Training", "comments": "10 pages, accepted by Findings of EMNLP 2020", "journal-ref": "Findings of EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous speech-to-speech translation is widely useful but extremely\nchallenging, since it needs to generate target-language speech concurrently\nwith the source-language speech, with only a few seconds delay. In addition, it\nneeds to continuously translate a stream of sentences, but all recent solutions\nmerely focus on the single-sentence scenario. As a result, current approaches\naccumulate latencies progressively when the speaker talks faster, and introduce\nunnatural pauses when the speaker talks slower. To overcome these issues, we\npropose Self-Adaptive Translation (SAT) which flexibly adjusts the length of\ntranslations to accommodate different source speech rates. At similar levels of\ntranslation quality (as measured by BLEU), our method generates more fluent\ntarget speech (as measured by the naturalness metric MOS) with substantially\nlower latency than the baseline, in both Zh <-> En directions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 06:02:15 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 19:12:17 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zheng", "Renjie", ""], ["Ma", "Mingbo", ""], ["Zheng", "Baigong", ""], ["Liu", "Kaibo", ""], ["Yuan", "Jiahong", ""], ["Church", "Kenneth", ""], ["Huang", "Liang", ""]]}, {"id": "2010.10077", "submitter": "Aman Madaan", "authors": "Aman Madaan, Yiming Yang", "title": "Neural Language Modeling for Contextualized Temporal Graph Generation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first study on using large-scale pre-trained language\nmodels for automated generation of an event-level temporal graph for a\ndocument. Despite the huge success of neural pre-training methods in NLP tasks,\nits potential for temporal reasoning over event graphs has not been\nsufficiently explored. Part of the reason is the difficulty in obtaining large\ntraining corpora with human-annotated events and temporal links. We address\nthis challenge by using existing IE/NLP tools to automatically generate a large\nquantity (89,000) of system-produced document-graph pairs, and propose a novel\nformulation of the contextualized graph generation problem as a\nsequence-to-sequence mapping task. These strategies enable us to leverage and\nfine-tune pre-trained language models on the system-induced training data for\nthe graph generation task. Our experiments show that our approach is highly\neffective in generating structurally and semantically valid graphs. Further,\nevaluation on a challenging hand-labeled, out-domain corpus shows that our\nmethod outperforms the closest existing method by a large margin on several\nmetrics. Code and pre-trained models are available at\nhttps://github.com/madaan/temporal-graph-gen.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:08:00 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:37:31 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Madaan", "Aman", ""], ["Yang", "Yiming", ""]]}, {"id": "2010.10095", "submitter": "Hung Le", "authors": "Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C.H. Hoi", "title": "BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded\n  Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-grounded dialogues are very challenging due to (i) the complexity of\nvideos which contain both spatial and temporal variations, and (ii) the\ncomplexity of user utterances which query different segments and/or different\nobjects in videos over multiple dialogue turns. However, existing approaches to\nvideo-grounded dialogues often focus on superficial temporal-level visual cues,\nbut neglect more fine-grained spatial signals from videos. To address this\ndrawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a\nvision-language neural framework for high-resolution queries in videos based on\ntextual cues. Specifically, our approach not only exploits both spatial and\ntemporal-level information, but also learns dynamic information diffusion\nbetween the two feature spaces through spatial-to-temporal and\ntemporal-to-spatial reasoning. The bidirectional strategy aims to tackle the\nevolving semantics of user queries in the dialogue setting. The retrieved\nvisual cues are used as contextual information to construct relevant responses\nto the users. Our empirical results and comprehensive qualitative analysis show\nthat BiST achieves competitive performance and generates reasonable responses\non a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA\nsetting, and substantially outperform prior approaches on the TGIF-QA\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 07:43:00 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Le", "Hung", ""], ["Sahoo", "Doyen", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2010.10111", "submitter": "Sainik Mahata", "authors": "Sainik Kumar Mahata, Dipankar Das, Sivaji Bandyopadhyay", "title": "JUNLP@Dravidian-CodeMix-FIRE2020: Sentiment Classification of Code-Mixed\n  Tweets using Bi-Directional RNN and Language Tags", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis has been an active area of research in the past two\ndecades and recently, with the advent of social media, there has been an\nincreasing demand for sentiment analysis on social media texts. Since the\nsocial media texts are not in one language and are largely code-mixed in\nnature, the traditional sentiment classification models fail to produce\nacceptable results. This paper tries to solve this very research problem and\nuses bi-directional LSTMs along with language tagging, to facilitate sentiment\ntagging of code-mixed Tamil texts that have been extracted from social media.\nThe presented algorithm, when evaluated on the test data, garnered precision,\nrecall, and F1 scores of 0.59, 0.66, and 0.58 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 08:10:29 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mahata", "Sainik Kumar", ""], ["Das", "Dipankar", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "2010.10150", "submitter": "Wei Ping", "authors": "Sashank Santhanam, Wei Ping, Raul Puri, Mohammad Shoeybi, Mostofa\n  Patwary, Bryan Catanzaro", "title": "Local Knowledge Powered Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art conversational agents have advanced significantly in\nconjunction with the use of large transformer-based language models. However,\neven with these advancements, conversational agents still lack the ability to\nproduce responses that are informative and coherent with the local context. In\nthis work, we propose a dialog framework that incorporates both local knowledge\nas well as users' past dialogues to generate high quality conversations. We\nintroduce an approach to build a dataset based on Reddit conversations, where\noutbound URL links are widely available in the conversations and the\nhyperlinked documents can be naturally included as local external knowledge.\nUsing our framework and dataset, we demonstrate that incorporating local\nknowledge can largely improve informativeness, coherency and realisticness\nmeasures using human evaluations. In particular, our approach consistently\noutperforms the state-of-the-art conversational model on the Reddit dataset\nacross all three measures. We also find that scaling the size of our models\nfrom 117M to 8.3B parameters yields consistent improvement of validation\nperplexity as well as human evaluated metrics. Our model with 8.3B parameters\ncan generate human-like responses as rated by various human evaluations in a\nsingle-turn dialog setting.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:34:40 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Santhanam", "Sashank", ""], ["Ping", "Wei", ""], ["Puri", "Raul", ""], ["Shoeybi", "Mohammad", ""], ["Patwary", "Mostofa", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2010.10156", "submitter": "Shivali Agarwal", "authors": "Shivali Agarwal, Shubham Atreja, Vikas Agarwal", "title": "Extracting Procedural Knowledge from Technical Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedures are an important knowledge component of documents that can be\nleveraged by cognitive assistants for automation, question-answering or driving\na conversation. It is a challenging problem to parse big dense documents like\nproduct manuals, user guides to automatically understand which parts are\ntalking about procedures and subsequently extract them. Most of the existing\nresearch has focused on extracting flows in given procedures or understanding\nthe procedures in order to answer conceptual questions. Identifying and\nextracting multiple procedures automatically from documents of diverse formats\nremains a relatively less addressed problem. In this work, we cover some of\nthis ground by -- 1) Providing insights on how structural and linguistic\nproperties of documents can be grouped to define types of procedures, 2)\nAnalyzing documents to extract the relevant linguistic and structural\nproperties, and 3) Formulating procedure identification as a classification\nproblem that leverages the features of the document derived from the above\nanalysis. We first implemented and deployed unsupervised techniques which were\nused in different use cases. Based on the evaluation in different use cases, we\nfigured out the weaknesses of the unsupervised approach. We then designed an\nimproved version which was supervised. We demonstrate that our technique is\neffective in identifying procedures from big and complex documents alike by\nachieving accuracy of 89%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:47:52 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Agarwal", "Shivali", ""], ["Atreja", "Shubham", ""], ["Agarwal", "Vikas", ""]]}, {"id": "2010.10176", "submitter": "Markus J. Hofmann", "authors": "Markus J. Hofmann, Lara M\\\"uller, Andre R\\\"olke, Ralph Radach and\n  Chris Biemann", "title": "Individual corpora predict fast memory retrieval during reading", "comments": "Proceedings of the 6th workshop on Cognitive Aspects of the Lexicon\n  (CogALex-VI), Barcelona, Spain, December 12, 2020; accepted manuscript; 11\n  pages, 2 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The corpus, from which a predictive language model is trained, can be\nconsidered the experience of a semantic system. We recorded everyday reading of\ntwo participants for two months on a tablet, generating individual corpus\nsamples of 300/500K tokens. Then we trained word2vec models from individual\ncorpora and a 70 million-sentence newspaper corpus to obtain individual and\nnorm-based long-term memory structure. To test whether individual corpora can\nmake better predictions for a cognitive task of long-term memory retrieval, we\ngenerated stimulus materials consisting of 134 sentences with uncorrelated\nindividual and norm-based word probabilities. For the subsequent eye tracking\nstudy 1-2 months later, our regression analyses revealed that individual, but\nnot norm-corpus-based word probabilities can account for first-fixation\nduration and first-pass gaze duration. Word length additionally affected gaze\nduration and total viewing duration. The results suggest that corpora\nrepresentative for an individual's longterm memory structure can better explain\nreading performance than a norm corpus, and that recently acquired information\nis lexically accessed rapidly.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 10:18:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hofmann", "Markus J.", ""], ["M\u00fcller", "Lara", ""], ["R\u00f6lke", "Andre", ""], ["Radach", "Ralph", ""], ["Biemann", "Chris", ""]]}, {"id": "2010.10203", "submitter": "Ondrej Skopek", "authors": "Daria Soboleva, Ondrej Skopek, M\\'arius \\v{S}ajgal\\'ik, Victor\n  C\\u{a}rbune, Felix Weissenberger, Julia Proskurnia, Bogdan Prisacari, Daniel\n  Valcarce, Justin Lu, Rohit Prabhavalkar, Balint Miklos", "title": "Replacing Human Audio with Synthetic Audio for On-device Unspoken\n  Punctuation Prediction", "comments": "Accepted to IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel multi-modal unspoken punctuation prediction system for the\nEnglish language which combines acoustic and text features. We demonstrate for\nthe first time, that by relying exclusively on synthetic data generated using a\nprosody-aware text-to-speech system, we can outperform a model trained with\nexpensive human audio recordings on the unspoken punctuation prediction\nproblem. Our model architecture is well suited for on-device use. This is\nachieved by leveraging hash-based embeddings of automatic speech recognition\ntext output in conjunction with acoustic features as input to a quasi-recurrent\nneural network, keeping the model size small and latency low.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:30:26 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:01:35 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Soboleva", "Daria", ""], ["Skopek", "Ondrej", ""], ["\u0160ajgal\u00edk", "M\u00e1rius", ""], ["C\u0103rbune", "Victor", ""], ["Weissenberger", "Felix", ""], ["Proskurnia", "Julia", ""], ["Prisacari", "Bogdan", ""], ["Valcarce", "Daniel", ""], ["Lu", "Justin", ""], ["Prabhavalkar", "Rohit", ""], ["Miklos", "Balint", ""]]}, {"id": "2010.10216", "submitter": "Danish Contractor", "authors": "Biswesh Mohapatra, Gaurav Pandey, Danish Contractor, Sachindra Joshi", "title": "Simulated Chats for Task-oriented Dialog: Learning to Generate\n  Conversations from Instructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular task-oriented dialog data sets such as MultiWOZ (Budzianowski et al.\n2018) are created by providing crowd-sourced workers a goal instruction,\nexpressed in natural language, that describes the task to be accomplished.\nCrowd-sourced workers play the role of a user and an agent to generate dialogs\nto accomplish tasks involving booking restaurant tables, making train\nreservations, calling a taxi etc. However, creating large crowd-sourced\ndatasets can be time consuming and expensive. To reduce the cost associated\nwith generating such dialog datasets, recent work has explored methods to\nautomatically create larger datasets from small samples.In this paper, we\npresent a data creation strategy that uses the pre-trained language model, GPT2\n(Radford et al. 2018), to simulate the interaction between crowd-sourced\nworkers by creating a user bot and an agent bot. We train the simulators using\na smaller percentage of actual crowd-generated conversations and their\ncorresponding goal instructions. We demonstrate that by using the simulated\ndata, we achieve significant improvements in both low-resource setting as well\nas in over-all task performance. To the best of our knowledge we are the first\nto present a model for generating entire conversations by simulating the\ncrowd-sourced data collection process\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 12:04:19 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mohapatra", "Biswesh", ""], ["Pandey", "Gaurav", ""], ["Contractor", "Danish", ""], ["Joshi", "Sachindra", ""]]}, {"id": "2010.10238", "submitter": "Thomas Ruprecht", "authors": "Richard M\\\"orbitz and Thomas Ruprecht", "title": "Supertagging-based Parsing with Linear Context-free Rewriting Systems", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first supertagging-based parser for LCFRS. It utilizes neural\nclassifiers and tremendously outperforms previous LCFRS-based parsers in both\naccuracy and parsing speed. Moreover, our results keep up with the best\n(general) discontinuous parsers, particularly the scores for discontinuous\nconstitutents are excellent. The heart of our approach is an efficient\nlexicalization procedure which induces a lexical LCFRS from any discontinuous\ntreebank. It is an adaptation of previous work by M\\\"orbitz and Ruprecht\n(2020). We also describe a modification to usual chart-based LCFRS parsing that\naccounts for supertagging and introduce a procedure for the transformation of\nlexical LCFRS derivations into equivalent parse trees of the original treebank.\nOur approach is implemented and evaluated on the English Discontinuous Penn\nTreebank and the German corpora NeGra and Tiger.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:02:42 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["M\u00f6rbitz", "Richard", ""], ["Ruprecht", "Thomas", ""]]}, {"id": "2010.10239", "submitter": "Markus Freitag", "authors": "Markus Freitag, Orhan Firat", "title": "Complete Multilingual Neural Machine Translation", "comments": "Accepted at WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual Neural Machine Translation (MNMT) models are commonly trained on\na joint set of bilingual corpora which is acutely English-centric (i.e. English\neither as the source or target language). While direct data between two\nlanguages that are non-English is explicitly available at times, its use is not\ncommon. In this paper, we first take a step back and look at the commonly used\nbilingual corpora (WMT), and resurface the existence and importance of implicit\nstructure that existed in it: multi-way alignment across examples (the same\nsentence in more than two languages). We set out to study the use of multi-way\naligned examples to enrich the original English-centric parallel corpora. We\nreintroduce this direct parallel data from multi-way aligned corpora between\nall source and target languages. By doing so, the English-centric graph expands\ninto a complete graph, every language pair being connected. We call MNMT with\nsuch connectivity pattern complete Multilingual Neural Machine Translation\n(cMNMT) and demonstrate its utility and efficacy with a series of experiments\nand analysis. In combination with a novel training data sampling strategy that\nis conditioned on the target language only, cMNMT yields competitive\ntranslation quality for all language pairs. We further study the size effect of\nmulti-way aligned data, its transfer learning capabilities and how it eases\nadding a new language in MNMT. Finally, we stress test cMNMT at scale and\ndemonstrate that we can train a cMNMT model with up to 111*112=12,432 language\npairs that provides competitive translation quality for all language pairs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:03:48 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Freitag", "Markus", ""], ["Firat", "Orhan", ""]]}, {"id": "2010.10245", "submitter": "Markus Freitag", "authors": "Markus Freitag, George Foster, David Grangier, Colin Cherry", "title": "Human-Paraphrased References Improve Neural Machine Translation", "comments": "Accepted at WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluation comparing candidate translations to human-generated\nparaphrases of reference translations has recently been proposed by Freitag et\nal. When used in place of original references, the paraphrased versions produce\nmetric scores that correlate better with human judgment. This effect holds for\na variety of different automatic metrics, and tends to favor natural\nformulations over more literal (translationese) ones. In this paper we compare\nthe results of performing end-to-end system development using standard and\nparaphrased references. With state-of-the-art English-German NMT components, we\nshow that tuning to paraphrased references produces a system that is\nsignificantly better according to human judgment, but 5 BLEU points worse when\ntested on standard references. Our work confirms the finding that paraphrased\nreferences yield metric scores that correlate better with human judgment, and\ndemonstrates for the first time that using these scores for system development\ncan lead to significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:14:57 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Freitag", "Markus", ""], ["Foster", "George", ""], ["Grangier", "David", ""], ["Cherry", "Colin", ""]]}, {"id": "2010.10252", "submitter": "Marco Wrzalik", "authors": "Marco Wrzalik and Dirk Krechel", "title": "CoRT: Complementary Rankings from Transformers", "comments": "NAACL-HLT 2021, Long Paper", "journal-ref": "Proceedings of the 2021 Conference of the North American Chapter\n  of the Association for Computational Linguistics: Human Language Technologies\n  (pp. 4194-4204). Anthology ID: 2021.naacl-main.331", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent approaches towards neural information retrieval mitigate their\ncomputational costs by using a multi-stage ranking pipeline. In the first\nstage, a number of potentially relevant candidates are retrieved using an\nefficient retrieval model such as BM25. Although BM25 has proven decent\nperformance as a first-stage ranker, it tends to miss relevant passages. In\nthis context we propose CoRT, a simple neural first-stage ranking model that\nleverages contextual representations from pretrained language models such as\nBERT to complement term-based ranking functions while causing no significant\ndelay at query time. Using the MS MARCO dataset, we show that CoRT\nsignificantly increases the candidate recall by complementing BM25 with missing\ncandidates. Consequently, we find subsequent re-rankers achieve superior\nresults with less candidates. We further demonstrate that passage retrieval\nusing CoRT can be realized with surprisingly low latencies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:28:27 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:15:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wrzalik", "Marco", ""], ["Krechel", "Dirk", ""]]}, {"id": "2010.10267", "submitter": "Kakia Chatsiou", "authors": "Kakia Chatsiou", "title": "Text Classification of Manifestos and COVID-19 Press Briefings using\n  BERT and Convolutional Neural Networks", "comments": "12 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We build a sentence-level political discourse classifier using existing human\nexpert annotated corpora of political manifestos from the Manifestos Project\n(Volkens et al., 2020a) and applying them to a corpus ofCOVID-19Press Briefings\n(Chatsiou, 2020). We use manually annotated political manifestos as training\ndata to train a local topic ConvolutionalNeural Network (CNN) classifier; then\napply it to the COVID-19PressBriefings Corpus to automatically classify\nsentences in the test corpus.We report on a series of experiments with CNN\ntrained on top of pre-trained embeddings for sentence-level classification\ntasks. We show thatCNN combined with transformers like BERT outperforms CNN\ncombined with other embeddings (Word2Vec, Glove, ELMo) and that it is possible\nto use a pre-trained classifier to conduct automatic classification on\ndifferent political texts without additional training.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:39:58 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 14:53:04 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chatsiou", "Kakia", ""]]}, {"id": "2010.10286", "submitter": "Wei Peng", "authors": "Wei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Jing Yu, Yajing Sun,\n  Xiangpeng Wei", "title": "Bi-directional Cognitive Thinking Network for Machine Reading\n  Comprehension", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bi-directional Cognitive Knowledge Framework (BCKF) for\nreading comprehension from the perspective of complementary learning systems\ntheory. It aims to simulate two ways of thinking in the brain to answer\nquestions, including reverse thinking and inertial thinking. To validate the\neffectiveness of our framework, we design a corresponding Bi-directional\nCognitive Thinking Network (BCTN) to encode the passage and generate a question\n(answer) given an answer (question) and decouple the bi-directional knowledge.\nThe model has the ability to reverse reasoning questions which can assist\ninertial thinking to generate more accurate answers. Competitive improvement is\nobserved in DuReader dataset, confirming our hypothesis that bi-directional\nknowledge helps the QA task. The novel framework shows an interesting\nperspective on machine reading comprehension and cognitive science.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:56:30 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Peng", "Wei", ""], ["Hu", "Yue", ""], ["Xing", "Luxi", ""], ["Xie", "Yuqiang", ""], ["Yu", "Jing", ""], ["Sun", "Yajing", ""], ["Wei", "Xiangpeng", ""]]}, {"id": "2010.10323", "submitter": "Chujie Zheng", "authors": "Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, Ling Fan", "title": "Topic-Aware Abstractive Text Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization aims at condensing a document to a shorter\nversion while preserving the key information. Different from extractive\nsummarization which simply selects text fragments from the document,\nabstractive summarization generates the summary in a word-by-word manner. Most\ncurrent state-of-the-art (SOTA) abstractive summarization methods are based on\nthe Transformer-based encoder-decoder architecture and focus on novel\nself-supervised objectives in pre-training. While these models well capture the\ncontextual information among words in documents, little attention has been paid\nto incorporating global semantics to better fine-tune for the downstream\nabstractive summarization task.\n  In this study, we propose a topic-aware abstractive summarization (TAAS)\nframework by leveraging the underlying semantic structure of documents\nrepresented by their latent topics. Specifically, TAAS seamlessly incorporates\na neural topic modeling into an encoder-decoder based sequence generation\nprocedure via attention for summarization. This design is able to learn and\npreserve global semantics of documents and thus makes summarization effective,\nwhich has been proved by our experiments on real-world datasets. As compared to\nseveral cutting-edge baseline methods, we show that TAAS outperforms BART, a\nwell-recognized SOTA model, by 2%, 8%, and 12% regarding the F measure of\nROUGE-1, ROUGE-2, and ROUGE-L, respectively. TAAS also achieves comparable\nperformance to PEGASUS and ProphetNet, which is difficult to accomplish given\nthat training PEGASUS and ProphetNet requires enormous computing capacity\nbeyond what we used in this study.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 14:45:25 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zheng", "Chujie", ""], ["Zhang", "Kunpeng", ""], ["Wang", "Harry Jiannan", ""], ["Fan", "Ling", ""]]}, {"id": "2010.10333", "submitter": "Wenchang Ma", "authors": "Wenchang Ma, Ryuichi Takanobu, Minghao Tu, Minlie Huang", "title": "Bridging the Gap between Conversational Reasoning and Interactive\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been growing interests in building a conversational recommender\nsystem, where the system simultaneously interacts with the user and explores\nthe user's preference throughout conversational interactions. Recommendation\nand conversation were usually treated as two separate modules with limited\ninformation exchange in existing works, which hinders the capability of both\nsystems: (1) dialog merely incorporated recommendation entities without being\nguided by an explicit recommendation-oriented policy; (2) recommendation\nutilized dialog only as a form of interaction instead of improving\nrecommendation effectively. To address the above issues, we propose a novel\nrecommender dialog model: CR-Walker. In order to view the two separate systems\nwithin a unified framework, we seek high-level mapping between hierarchical\ndialog acts and multi-hop knowledge graph reasoning. The model walks on a\nlarge-scale knowledge graph to form a reasoning tree at each turn, then mapped\nto dialog acts to guide response generation. With such a mapping mechanism as a\nbridge between recommendation and conversation, our framework maximizes the\nmutual benefit between two systems: dialog as an enhancement to recommendation\nquality and explainability, recommendation as a goal and enrichment to dialog\nsemantics. Quantitative evaluation shows that our model excels in conversation\ninformativeness and recommendation effectiveness, at the same time explainable\non the policy level.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 14:53:22 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ma", "Wenchang", ""], ["Takanobu", "Ryuichi", ""], ["Tu", "Minghao", ""], ["Huang", "Minlie", ""]]}, {"id": "2010.10363", "submitter": "Laurel Orr", "authors": "Laurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao\n  Ling, Christopher Re", "title": "Bootleg: Chasing the Tail with Self-Supervised Named Entity\n  Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge for named entity disambiguation (NED), the task of mapping\ntextual mentions to entities in a knowledge base, is how to disambiguate\nentities that appear rarely in the training data, termed tail entities. Humans\nuse subtle reasoning patterns based on knowledge of entity facts, relations,\nand types to disambiguate unfamiliar entities. Inspired by these patterns, we\nintroduce Bootleg, a self-supervised NED system that is explicitly grounded in\nreasoning patterns for disambiguation. We define core reasoning patterns for\ndisambiguation, create a learning procedure to encourage the self-supervised\nmodel to learn the patterns, and show how to use weak supervision to enhance\nthe signals in the training data. Encoding the reasoning patterns in a simple\nTransformer architecture, Bootleg meets or exceeds state-of-the-art on three\nNED benchmarks. We further show that the learned representations from Bootleg\nsuccessfully transfer to other non-disambiguation tasks that require\nentity-based knowledge: we set a new state-of-the-art in the popular TACRED\nrelation extraction task by 1.0 F1 points and demonstrate up to 8% performance\nlift in highly optimized production search and assistant tasks at a major\ntechnology company\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:17:49 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 02:19:08 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 16:21:13 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Orr", "Laurel", ""], ["Leszczynski", "Megan", ""], ["Arora", "Simran", ""], ["Wu", "Sen", ""], ["Guha", "Neel", ""], ["Ling", "Xiao", ""], ["Re", "Christopher", ""]]}, {"id": "2010.10386", "submitter": "Spyretta Leivaditi", "authors": "Spyretta Leivaditi, Julien Rossi, Evangelos Kanoulas", "title": "A Benchmark for Lease Contract Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting entities and other useful information from legal contracts is an\nimportant task whose automation can help legal professionals perform contract\nreviews more efficiently and reduce relevant risks. In this paper, we tackle\nthe problem of detecting two different types of elements that play an important\nrole in a contract review, namely entities and red flags. The latter are terms\nor sentences that indicate that there is some danger or other potentially\nproblematic situation for one or more of the signing parties. We focus on\nsupporting the review of lease agreements, a contract type that has received\nlittle attention in the legal information extraction literature, and we define\nthe types of entities and red flags needed for that task. We release a new\nbenchmark dataset of 179 lease agreement documents that we have manually\nannotated with the entities and red flags they contain, and which can be used\nto train and test relevant extraction algorithms. Finally, we release a new\nlanguage model, called ALeaseBERT, pre-trained on this dataset and fine-tuned\nfor the detection of the aforementioned elements, providing a baseline for\nfurther research\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:50:50 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Leivaditi", "Spyretta", ""], ["Rossi", "Julien", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2010.10391", "submitter": "Georgios Michalopoulos", "authors": "George Michalopoulos, Yuanxin Wang, Hussam Kaka, Helen Chen and\n  Alexander Wong", "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual\n  Embeddings Using the Unified Medical Language System Metathesaurus", "comments": "10 pages, 3 figures, accepted in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have\nachieved state-of-the-art results in biomedical natural language processing\ntasks by focusing their pre-training process on domain-specific corpora.\nHowever, such models do not take into consideration expert domain knowledge.\n  In this work, we introduced UmlsBERT, a contextual embedding model that\nintegrates domain knowledge during the pre-training process via a novel\nknowledge augmentation strategy. More specifically, the augmentation on\nUmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was\nperformed in two ways: i) connecting words that have the same underlying\n`concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to\ncreate clinically meaningful input embeddings. By applying these two\nstrategies, UmlsBERT can encode clinical domain knowledge into word embeddings\nand outperform existing domain-specific models on common named-entity\nrecognition (NER) and clinical natural language inference clinical NLP tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:56:31 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 07:21:08 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 21:30:38 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 19:59:23 GMT"}, {"version": "v5", "created": "Thu, 3 Jun 2021 15:07:58 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Michalopoulos", "George", ""], ["Wang", "Yuanxin", ""], ["Kaka", "Hussam", ""], ["Chen", "Helen", ""], ["Wong", "Alexander", ""]]}, {"id": "2010.10392", "submitter": "Hicham El Boukkouri", "authors": "Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji,\n  Pierre Zweigenbaum, Junichi Tsujii", "title": "CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary\n  Representations From Characters", "comments": "13 pages, 8 figures and 3 tables. Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the compelling improvements brought by BERT, many recent\nrepresentation models adopted the Transformer architecture as their main\nbuilding block, consequently inheriting the wordpiece tokenization system\ndespite it not being intrinsically linked to the notion of Transformers. While\nthis system is thought to achieve a good balance between the flexibility of\ncharacters and the efficiency of full words, using predefined wordpiece\nvocabularies from the general domain is not always suitable, especially when\nbuilding models for specialized domains (e.g., the medical domain). Moreover,\nadopting a wordpiece tokenization shifts the focus from the word level to the\nsubword level, making the models conceptually more complex and arguably less\nconvenient in practice. For these reasons, we propose CharacterBERT, a new\nvariant of BERT that drops the wordpiece system altogether and uses a\nCharacter-CNN module instead to represent entire words by consulting their\ncharacters. We show that this new model improves the performance of BERT on a\nvariety of medical domain tasks while at the same time producing robust,\nword-level and open-vocabulary representations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:58:53 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 14:57:52 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 21:29:04 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Boukkouri", "Hicham El", ""], ["Ferret", "Olivier", ""], ["Lavergne", "Thomas", ""], ["Noji", "Hiroshi", ""], ["Zweigenbaum", "Pierre", ""], ["Tsujii", "Junichi", ""]]}, {"id": "2010.10418", "submitter": "Swarnadeep Saha", "authors": "Swarnadeep Saha, Yixin Nie, Mohit Bansal", "title": "ConjNLI: Natural Language Inference Over Conjunctive Sentences", "comments": "EMNLP 2020 (14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about conjuncts in conjunctive sentences is important for a deeper\nunderstanding of conjunctions in English and also how their usages and\nsemantics differ from conjunctive and disjunctive boolean logic. Existing NLI\nstress tests do not consider non-boolean usages of conjunctions and use\ntemplates for testing such model knowledge. Hence, we introduce ConjNLI, a\nchallenge stress-test for natural language inference over conjunctive\nsentences, where the premise differs from the hypothesis by conjuncts removed,\nadded, or replaced. These sentences contain single and multiple instances of\ncoordinating conjunctions (\"and\", \"or\", \"but\", \"nor\") with quantifiers,\nnegations, and requiring diverse boolean and non-boolean inferences over\nconjuncts. We find that large-scale pre-trained language models like RoBERTa do\nnot understand conjunctive semantics well and resort to shallow heuristics to\nmake inferences over such sentences. As some initial solutions, we first\npresent an iterative adversarial fine-tuning method that uses synthetically\ncreated training data based on boolean and non-boolean heuristics. We also\npropose a direct model advancement by making RoBERTa aware of predicate\nsemantic roles. While we observe some performance gains, ConjNLI is still\nchallenging for current methods, thus encouraging interesting future work for\nbetter understanding of conjunctions. Our data and code are publicly available\nat: https://github.com/swarnaHub/ConjNLI\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:29:13 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:49:00 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Saha", "Swarnadeep", ""], ["Nie", "Yixin", ""], ["Bansal", "Mohit", ""]]}, {"id": "2010.10439", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, William W.\n  Cohen", "title": "Open Question Answering over Tables and Text", "comments": "Accepted to ICLR 2021. Main paper has 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open question answering (QA), the answer to a question is produced by\nretrieving and then analyzing documents that might contain answers to the\nquestion. Most open QA systems have considered only retrieving information from\nunstructured text. Here we consider for the first time open QA over both\ntabular and textual data and present a new large-scale dataset Open\nTable-and-Text Question Answering (OTT-QA) to evaluate performance on this\ntask. Most questions in OTT-QA require multi-hop inference across tabular data\nand unstructured text, and the evidence required to answer a question can be\ndistributed in different ways over these two types of input, making evidence\nretrieval challenging -- our baseline model using an iterative retriever and\nBERT-based reader achieves an exact match score less than 10%. We then propose\ntwo novel techniques to address the challenge of retrieving and aggregating\nevidence for OTT-QA. The first technique is to use \"early fusion\" to group\nmultiple highly relevant tabular and textual units into a fused block, which\nprovides more context for the retriever to search for. The second technique is\nto use a cross-block reader to model the cross-dependency between multiple\nretrieved evidence with global-local sparse attention. Combining these two\ntechniques improves the score significantly, to above 27%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:48:14 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 08:21:18 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Chen", "Wenhu", ""], ["Chang", "Ming-Wei", ""], ["Schlinger", "Eva", ""], ["Wang", "William", ""], ["Cohen", "William W.", ""]]}, {"id": "2010.10453", "submitter": "Maria Leonor Pacheco", "authors": "Maria Leonor Pacheco and Dan Goldwasser", "title": "Modeling Content and Context with Deep Relational Learning", "comments": "TACL pre-MIT Press version", "journal-ref": "Transactions of the Association for Computational Linguistics,\n  2021", "doi": "10.1162/tacl_a_00357", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building models for realistic natural language tasks requires dealing with\nlong texts and accounting for complicated structural dependencies.\nNeural-symbolic representations have emerged as a way to combine the reasoning\ncapabilities of symbolic methods, with the expressiveness of neural networks.\nHowever, most of the existing frameworks for combining neural and symbolic\nrepresentations have been designed for classic relational learning tasks that\nwork over a universe of symbolic entities and relations. In this paper, we\npresent DRaiL, an open-source declarative framework for specifying deep\nrelational models, designed to support a variety of NLP scenarios. Our\nframework supports easy integration with expressive language encoders, and\nprovides an interface to study the interactions between representation,\ninference and learning.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:09:35 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Pacheco", "Maria Leonor", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2010.10472", "submitter": "Yiyuan Li", "authors": "Yiyuan Li, Antonios Anastasopoulos, Alan W Black", "title": "Comparison of Interactive Knowledge Base Spelling Correction Models for\n  Low-Resource Languages", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spelling normalization for low resource languages is a challenging task\nbecause the patterns are hard to predict and large corpora are usually required\nto collect enough examples. This work shows a comparison of a neural model and\ncharacter language models with varying amounts on target language data. Our\nusage scenario is interactive correction with nearly zero amounts of training\nexamples, improving models as more data is collected, for example within a chat\napp. Such models are designed to be incrementally improved as feedback is given\nfrom users. In this work, we design a knowledge-base and prediction model\nembedded system for spelling correction in low-resource languages. Experimental\nresults on multiple languages show that the model could become effective with a\nsmall amount of data. We perform experiments on both natural and synthetic\ndata, as well as on data from two endangered languages (Ainu and Griko). Last,\nwe built a prototype system that was used for a small case study on Hinglish,\nwhich further demonstrated the suitability of our approach in real world\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:31:07 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Li", "Yiyuan", ""], ["Anastasopoulos", "Antonios", ""], ["Black", "Alan W", ""]]}, {"id": "2010.10499", "submitter": "Adrian de Wynter", "authors": "Adrian de Wynter and Daniel J. Perry", "title": "Optimal Subarchitecture Extraction For BERT", "comments": "Preprint. Under review. Corrected typos on v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We extract an optimal subset of architectural parameters for the BERT\narchitecture from Devlin et al. (2018) by applying recent breakthroughs in\nalgorithms for neural architecture search. This optimal subset, which we refer\nto as \"Bort\", is demonstrably smaller, having an effective (that is, not\ncounting the embedding layer) size of $5.5\\%$ the original BERT-large\narchitecture, and $16\\%$ of the net size. Bort is also able to be pretrained in\n$288$ GPU hours, which is $1.2\\%$ of the time required to pretrain the\nhighest-performing BERT parametric architectural variant, RoBERTa-large (Liu et\nal., 2019), and about $33\\%$ of that of the world-record, in GPU hours,\nrequired to train BERT-large on the same hardware. It is also $7.9$x faster on\na CPU, as well as being better performing than other compressed variants of the\narchitecture, and some of the non-compressed variants: it obtains performance\nimprovements of between $0.3\\%$ and $31\\%$, absolute, with respect to\nBERT-large, on multiple public natural language understanding (NLU) benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:53:01 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 23:09:00 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["de Wynter", "Adrian", ""], ["Perry", "Daniel J.", ""]]}, {"id": "2010.10501", "submitter": "William Gantt", "authors": "William Gantt, Benjamin Kane, Aaron Steven White", "title": "Natural Language Inference with Mixed Effects", "comments": null, "journal-ref": "The Ninth Joint Conference on Lexical and Computational Semantics\n  (*SEM2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing evidence that the prevalence of disagreement in the raw\nannotations used to construct natural language inference datasets makes the\ncommon practice of aggregating those annotations to a single label problematic.\nWe propose a generic method that allows one to skip the aggregation step and\ntrain on the raw annotations directly without subjecting the model to unwanted\nnoise that can arise from annotator response biases. We demonstrate that this\nmethod, which generalizes the notion of a \\textit{mixed effects model} by\nincorporating \\textit{annotator random effects} into any existing neural model,\nimproves performance over models that do not incorporate such effects.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:54:16 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Gantt", "William", ""], ["Kane", "Benjamin", ""], ["White", "Aaron Steven", ""]]}, {"id": "2010.10556", "submitter": "Peidong Wang", "authors": "Peidong Wang, Zhuo Chen, DeLiang Wang, Jinyu Li, Yifan Gong", "title": "Speaker Separation Using Speaker Inventories and Estimated Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose speaker separation using speaker inventories and estimated speech\n(SSUSIES), a framework leveraging speaker profiles and estimated speech for\nspeaker separation. SSUSIES contains two methods, speaker separation using\nspeaker inventories (SSUSI) and speaker separation using estimated speech\n(SSUES). SSUSI performs speaker separation with the help of speaker inventory.\nBy combining the advantages of permutation invariant training (PIT) and speech\nextraction, SSUSI significantly outperforms conventional approaches. SSUES is a\nwidely applicable technique that can substantially improve speaker separation\nperformance using the output of first-pass separation. We evaluate the models\non both speaker separation and speech recognition metrics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:15:45 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wang", "Peidong", ""], ["Chen", "Zhuo", ""], ["Wang", "DeLiang", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2010.10563", "submitter": "Pablo Messina", "authors": "Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia Besa,\n  Sergio Uribe, Marcelo and\\'ia, Cristian Tejos, Claudia Prieto and Daniel\n  Capurro", "title": "A Survey on Deep Learning and Explainability for Automatic Image-based\n  Medical Report Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year physicians face an increasing demand of image-based diagnosis from\npatients, a problem that can be addressed with recent artificial intelligence\nmethods. In this context, we survey works in the area of automatic report\ngeneration from medical images, with emphasis on methods using deep neural\nnetworks, with respect to: (1) Datasets, (2) Architecture Design, (3)\nExplainability and (4) Evaluation Metrics. Our survey identifies interesting\ndevelopments, but also remaining challenges. Among them, the current evaluation\nof generated reports is especially weak, since it mostly relies on traditional\nNatural Language Processing (NLP) metrics, which do not accurately capture\nmedical correctness.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:48:37 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Messina", "Pablo", ""], ["Pino", "Pablo", ""], ["Parra", "Denis", ""], ["Soto", "Alvaro", ""], ["Besa", "Cecilia", ""], ["Uribe", "Sergio", ""], ["and\u00eda", "Marcelo", ""], ["Tejos", "Cristian", ""], ["Prieto", "Claudia", ""], ["Capurro", "Daniel", ""]]}, {"id": "2010.10566", "submitter": "Fei Liu", "authors": "Sangwoo Cho and Kaiqiang Song and Chen Li and Dong Yu and Hassan\n  Foroosh and Fei Liu", "title": "Better Highlighting: Creating Sub-Sentence Summary Highlights", "comments": "EMNLP 2020 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amongst the best means to summarize is highlighting. In this paper, we aim to\ngenerate summary highlights to be overlaid on the original documents to make it\neasier for readers to sift through a large amount of text. The method allows\nsummaries to be understood in context to prevent a summarizer from distorting\nthe original meaning, of which abstractive summarizers usually fall short. In\nparticular, we present a new method to produce self-contained highlights that\nare understandable on their own to avoid confusion. Our method combines\ndeterminantal point processes and deep contextualized representations to\nidentify an optimal set of sub-sentence segments that are both important and\nnon-redundant to form summary highlights. To demonstrate the flexibility and\nmodeling power of our method, we conduct extensive experiments on summarization\ndatasets. Our analysis provides evidence that highlighting is a promising\navenue of research towards future summarization.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:57:42 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Cho", "Sangwoo", ""], ["Song", "Kaiqiang", ""], ["Li", "Chen", ""], ["Yu", "Dong", ""], ["Foroosh", "Hassan", ""], ["Liu", "Fei", ""]]}, {"id": "2010.10573", "submitter": "Hoang Nguyen Hung Van", "authors": "Hoang Van, David Kauchak, Gondy Leroy", "title": "AutoMeTS: The Autocomplete for Medical Text Simplification", "comments": "9 pages, 3 figures, and 8 tables, Accpeted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of text simplification (TS) is to transform difficult text into a\nversion that is easier to understand and more broadly accessible to a wide\nvariety of readers. In some domains, such as healthcare, fully automated\napproaches cannot be used since information must be accurately preserved.\nInstead, semi-automated approaches can be used that assist a human writer in\nsimplifying text faster and at a higher quality. In this paper, we examine the\napplication of autocomplete to text simplification in the medical domain. We\nintroduce a new parallel medical data set consisting of aligned English\nWikipedia with Simple English Wikipedia sentences and examine the application\nof pretrained neural language models (PNLMs) on this dataset. We compare four\nPNLMs(BERT, RoBERTa, XLNet, and GPT-2), and show how the additional context of\nthe sentence to be simplified can be incorporated to achieve better results\n(6.17% absolute improvement over the best individual model). We also introduce\nan ensemble model that combines the four PNLMs and outperforms the best\nindividual model by 2.1%, resulting in an overall word prediction accuracy of\n64.52%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:20:29 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Van", "Hoang", ""], ["Kauchak", "David", ""], ["Leroy", "Gondy", ""]]}, {"id": "2010.10597", "submitter": "Aditya Kalyanpur", "authors": "Clifton McFate, Aditya Kalyanpur, Dave Ferrucci, Andrea Bradshaw,\n  Ariel Diertani, David Melville, Lori Moon", "title": "SKATE: A Natural Language Interface for Encoding Structured Knowledge", "comments": "Accepted at IAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language (NL) applications, there is often a mismatch between what\nthe NL interface is capable of interpreting and what a lay user knows how to\nexpress. This work describes a novel natural language interface that reduces\nthis mismatch by refining natural language input through successive,\nautomatically generated semi-structured templates. In this paper we describe\nhow our approach, called SKATE, uses a neural semantic parser to parse NL input\nand suggest semi-structured templates, which are recursively filled to produce\nfully structured interpretations. We also show how SKATE integrates with a\nneural rule-generation model to interactively suggest and acquire commonsense\nknowledge. We provide a preliminary coverage analysis of SKATE for the task of\nstory understanding, and then describe a current business use-case of the tool\nin a specific domain: COVID-19 policy design.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:13:09 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 01:01:45 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["McFate", "Clifton", ""], ["Kalyanpur", "Aditya", ""], ["Ferrucci", "Dave", ""], ["Bradshaw", "Andrea", ""], ["Diertani", "Ariel", ""], ["Melville", "David", ""], ["Moon", "Lori", ""]]}, {"id": "2010.10648", "submitter": "Elman Mansimov", "authors": "Elman Mansimov, Mitchell Stern, Mia Chen, Orhan Firat, Jakob\n  Uszkoreit, Puneet Jain", "title": "Towards End-to-End In-Image Neural Machine Translation", "comments": "Accepted as an oral presentation at EMNLP, NLP Beyond Text workshop,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we offer a preliminary investigation into the task of in-image\nmachine translation: transforming an image containing text in one language into\nan image containing the same text in another language. We propose an end-to-end\nneural model for this task inspired by recent approaches to neural machine\ntranslation, and demonstrate promising initial results based purely on\npixel-level supervision. We then offer a quantitative and qualitative\nevaluation of our system outputs and discuss some common failure modes.\nFinally, we conclude with directions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:20:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mansimov", "Elman", ""], ["Stern", "Mitchell", ""], ["Chen", "Mia", ""], ["Firat", "Orhan", ""], ["Uszkoreit", "Jakob", ""], ["Jain", "Puneet", ""]]}, {"id": "2010.10649", "submitter": "Wei-Fan Chen", "authors": "Wei-Fan Chen, Khalid Al-Khatib, Benno Stein and Henning Wachsmuth", "title": "Detecting Media Bias in News Articles using Gaussian Bias Distributions", "comments": null, "journal-ref": "EMNLP 2020 Findings", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media plays an important role in shaping public opinion. Biased media can\ninfluence people in undesirable directions and hence should be unmasked as\nsuch. We observe that featurebased and neural text classification approaches\nwhich rely only on the distribution of low-level lexical information fail to\ndetect media bias. This weakness becomes most noticeable for articles on new\nevents, where words appear in new contexts and hence their \"bias\npredictiveness\" is unclear. In this paper, we therefore study how second-order\ninformation about biased statements in an article helps to improve detection\neffectiveness. In particular, we utilize the probability distributions of the\nfrequency, positions, and sequential order of lexical and informational\nsentence-level bias in a Gaussian Mixture Model. On an existing media bias\ndataset, we find that the frequency and positions of biased statements strongly\nimpact article-level bias, whereas their exact sequential order is secondary.\nUsing a standard model for sentence-level bias detection, we provide empirical\nevidence that article-level bias detectors that use second-order information\nclearly outperform those without.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:20:49 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chen", "Wei-Fan", ""], ["Al-Khatib", "Khalid", ""], ["Stein", "Benno", ""], ["Wachsmuth", "Henning", ""]]}, {"id": "2010.10652", "submitter": "Wei-Fan Chen", "authors": "Wei-Fan Chen, Khalid Al-Khatib, Henning Wachsmuth and Benno Stein", "title": "Analyzing Political Bias and Unfairness in News Articles at Different\n  Levels of Granularity", "comments": null, "journal-ref": "NLP+CSS 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media organizations bear great reponsibility because of their considerable\ninfluence on shaping beliefs and positions of our society. Any form of media\ncan contain overly biased content, e.g., by reporting on political events in a\nselective or incomplete manner. A relevant question hence is whether and how\nsuch form of imbalanced news coverage can be exposed. The research presented in\nthis paper addresses not only the automatic detection of bias but goes one step\nfurther in that it explores how political bias and unfairness are manifested\nlinguistically. In this regard we utilize a new corpus of 6964 news articles\nwith labels derived from adfontesmedia.com and develop a neural model for bias\nassessment. By analyzing this model on article excerpts, we find insightful\nbias patterns at different levels of text granularity, from single words to the\nwhole article discourse.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:25:00 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chen", "Wei-Fan", ""], ["Al-Khatib", "Khalid", ""], ["Wachsmuth", "Henning", ""], ["Stein", "Benno", ""]]}, {"id": "2010.10669", "submitter": "Ram\\'on Fernandez Astudillo", "authors": "Ramon Fernandez Astudillo, Miguel Ballesteros, Tahira Naseem, Austin\n  Blodgett, Radu Florian", "title": "Transition-based Parsing with Stack-Transformers", "comments": "Accepted to Findings of EMNLP2020, open review\n  https://openreview.net/forum?id=b36spsuUAde, code\n  https://github.com/IBM/transition-amr-parser", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the parser state is key to good performance in transition-based\nparsing. Recurrent Neural Networks considerably improved the performance of\ntransition-based systems by modelling the global state, e.g. stack-LSTM\nparsers, or local state modeling of contextualized features, e.g. Bi-LSTM\nparsers. Given the success of Transformer architectures in recent parsing\nsystems, this work explores modifications of the sequence-to-sequence\nTransformer architecture to model either global or local parser states in\ntransition-based parsing. We show that modifications of the cross attention\nmechanism of the Transformer considerably strengthen performance both on\ndependency and Abstract Meaning Representation (AMR) parsing tasks,\nparticularly for smaller models or limited training data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 23:20:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Astudillo", "Ramon Fernandez", ""], ["Ballesteros", "Miguel", ""], ["Naseem", "Tahira", ""], ["Blodgett", "Austin", ""], ["Florian", "Radu", ""]]}, {"id": "2010.10673", "submitter": "Ram\\'on Fernandez Astudillo", "authors": "Young-Suk Lee, Ramon Fernandez Astudillo, Tahira Naseem, Revanth Gangi\n  Reddy, Radu Florian, Salim Roukos", "title": "Pushing the Limits of AMR Parsing with Self-Learning", "comments": "Accepted to Findings of EMNLP2020, open review\n  https://openreview.net/forum?id=4q5-oJgLiO, code\n  https://github.com/IBM/transition-amr-parser", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Meaning Representation (AMR) parsing has experienced a notable\ngrowth in performance in the last two years, due both to the impact of transfer\nlearning and the development of novel architectures specific to AMR. At the\nsame time, self-learning techniques have helped push the performance boundaries\nof other natural language processing applications, such as machine translation\nor question answering. In this paper, we explore different ways in which\ntrained models can be applied to improve AMR parsing performance, including\ngeneration of synthetic text and AMR annotations as well as refinement of\nactions oracle. We show that, without any additional human annotations, these\ntechniques improve an already performant parser and achieve state-of-the-art\nresults on AMR 1.0 and AMR 2.0.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 23:45:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Lee", "Young-Suk", ""], ["Astudillo", "Ramon Fernandez", ""], ["Naseem", "Tahira", ""], ["Reddy", "Revanth Gangi", ""], ["Florian", "Radu", ""], ["Roukos", "Salim", ""]]}, {"id": "2010.10694", "submitter": "Erica Cooper", "authors": "Antoine Perquin, Erica Cooper, Junichi Yamagishi", "title": "An Investigation of the Relation Between Grapheme Embeddings and\n  Pronunciation for Tacotron-based Systems", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models, particularly Tacotron-based ones, are currently a popular\nsolution for text-to-speech synthesis. They allow the production of\nhigh-quality synthesized speech with little to no text preprocessing. Indeed,\nthey can be trained using either graphemes or phonemes as input directly.\nHowever, in the case of grapheme inputs, little is known concerning the\nrelation between the underlying representations learned by the model and word\npronunciations. This work investigates this relation in the case of a Tacotron\nmodel trained on French graphemes. Our analysis shows that grapheme embeddings\nare related to phoneme information despite no such information being present\nduring training. Thanks to this property, we show that grapheme embeddings\nlearned by Tacotron models can be useful for tasks such as grapheme-to-phoneme\nconversion and control of the pronunciation in synthetic speech.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 00:58:29 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:48:56 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Perquin", "Antoine", ""], ["Cooper", "Erica", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "2010.10699", "submitter": "Zhao Xinyan", "authors": "Xinyan Zhao, Liangwei Chen, Huanhuan Chen", "title": "A Weighted Heterogeneous Graph Based Dialogue System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge based dialogue systems have attracted increasing research interest\nin diverse applications. However, for disease diagnosis, the widely used\nknowledge graph is hard to represent the symptom-symptom relations and\nsymptom-disease relations since the edges of traditional knowledge graph are\nunweighted. Most research on disease diagnosis dialogue systems highly rely on\ndata-driven methods and statistical features, lacking profound comprehension of\nsymptom-disease relations and symptom-symptom relations. To tackle this issue,\nthis work presents a weighted heterogeneous graph based dialogue system for\ndisease diagnosis. Specifically, we build a weighted heterogeneous graph based\non symptom co-occurrence and a proposed symptom frequency-inverse disease\nfrequency. Then this work proposes a graph based deep Q-network (Graph-DQN) for\ndialogue management. By combining Graph Convolutional Network (GCN) with DQN to\nlearn the embeddings of diseases and symptoms from both the structural and\nattribute information in the weighted heterogeneous graph, Graph-DQN could\ncapture the symptom-disease relations and symptom-symptom relations better.\nExperimental results show that the proposed dialogue system rivals the\nstate-of-the-art models. More importantly, the proposed dialogue system can\ncomplete the task with less dialogue turns and possess a better distinguishing\ncapability on diseases with similar symptoms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 01:22:37 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 02:09:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhao", "Xinyan", ""], ["Chen", "Liangwei", ""], ["Chen", "Huanhuan", ""]]}, {"id": "2010.10743", "submitter": "Jianhao Yan", "authors": "Jianhao Yan, Fandong Meng, Jie Zhou", "title": "Multi-Unit Transformers for Neural Machine Translation", "comments": "Accepted as a main conference paper in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models achieve remarkable success in Neural Machine Translation.\nMany efforts have been devoted to deepening the Transformer by stacking several\nunits (i.e., a combination of Multihead Attentions and FFN) in a cascade, while\nthe investigation over multiple parallel units draws little attention. In this\npaper, we propose the Multi-Unit Transformers (MUTE), which aim to promote the\nexpressiveness of the Transformer by introducing diverse and complementary\nunits. Specifically, we use several parallel units and show that modeling with\nmultiple units improves model performance and introduces diversity. Further, to\nbetter leverage the advantage of the multi-unit setting, we design biased\nmodule and sequential dependency that guide and encourage complementariness\namong different units. Experimental results on three machine translation tasks,\nthe NIST Chinese-to-English, WMT'14 English-to-German and WMT'18\nChinese-to-English, show that the MUTE models significantly outperform the\nTransformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild\ndrop in inference speed (about 3.1%). In addition, our methods also surpass the\nTransformer-Big model, with only 54\\% of its parameters. These results\ndemonstrate the effectiveness of the MUTE, as well as its efficiency in both\nthe inference process and parameter usage.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 03:41:49 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 11:33:45 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yan", "Jianhao", ""], ["Meng", "Fandong", ""], ["Zhou", "Jie", ""]]}, {"id": "2010.10755", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Ying Sheng, Nguyen Vo, Sandeep Tata", "title": "FreeDOM: A Transferable Neural Architecture for Structured Information\n  Extraction on Web Documents", "comments": "in Proc. of KDD 2020 (Research Track). Figure 5 updated", "journal-ref": null, "doi": "10.1145/3394486.3403153", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured data from HTML documents is a long-studied problem with\na broad range of applications like augmenting knowledge bases, supporting\nfaceted search, and providing domain-specific experiences for key verticals\nlike shopping and movies. Previous approaches have either required a small\nnumber of examples for each target site or relied on carefully handcrafted\nheuristics built over visual renderings of websites. In this paper, we present\na novel two-stage neural approach, named FreeDOM, which overcomes both these\nlimitations. The first stage learns a representation for each DOM node in the\npage by combining both the text and markup information. The second stage\ncaptures longer range distance and semantic relatedness using a relational\nneural network. By combining these stages, FreeDOM is able to generalize to\nunseen sites after training on a small number of seed sites from that vertical\nwithout requiring expensive hand-crafted features over visual renderings of the\npage. Through experiments on a public dataset with 8 different verticals, we\nshow that FreeDOM beats the previous state of the art by nearly 3.7 F1 points\non average without requiring features over rendered pages or expensive\nhand-crafted features.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:20:13 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Sheng", "Ying", ""], ["Vo", "Nguyen", ""], ["Tata", "Sandeep", ""]]}, {"id": "2010.10757", "submitter": "Srinivasan Iyer", "authors": "Srinivasan Iyer, Sewon Min, Yashar Mehdad, Wen-tau Yih", "title": "RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open\n  Domain Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain\nQuestion Answering (QA) are typically trained for span selection using\ndistantly supervised positive examples and heuristically retrieved negative\nexamples. This training scheme possibly explains empirical observations that\nthese models achieve a high recall amongst their top few predictions, but a low\noverall accuracy, motivating the need for answer re-ranking. We develop a\nsimple and effective re-ranking approach (RECONSIDER) for span-extraction\ntasks, that improves upon the performance of large pre-trained MRC models.\nRECONSIDER is trained on positive and negative examples extracted from high\nconfidence predictions of MRC models, and uses in-passage span annotations to\nperform span-focused re-ranking over a smaller candidate set. As a result,\nRECONSIDER learns to eliminate close false positive passages, and achieves a\nnew state of the art on four QA tasks, including 45.5% Exact Match accuracy on\nNatural Questions with real user questions, and 61.7% on TriviaQA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:28:42 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Iyer", "Srinivasan", ""], ["Min", "Sewon", ""], ["Mehdad", "Yashar", ""], ["Yih", "Wen-tau", ""]]}, {"id": "2010.10759", "submitter": "Yangyang Shi", "authors": "Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching-Feng Yeh, Julian\n  Chan, Frank Zhang, Duc Le, Mike Seltzer", "title": "Emformer: Efficient Memory Transformer Based Acoustic Model For Low\n  Latency Streaming Speech Recognition", "comments": "5 pages, 2 figures, submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient memory transformer Emformer for low latency\nstreaming speech recognition. In Emformer, the long-range history context is\ndistilled into an augmented memory bank to reduce self-attention's computation\ncomplexity. A cache mechanism saves the computation for the key and value in\nself-attention for the left context. Emformer applies a parallelized block\nprocessing in training to support low latency models. We carry out experiments\non benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets\nWER $2.50\\%$ on test-clean and $5.62\\%$ on test-other. Comparing with a strong\nbaseline augmented memory transformer (AM-TRF), Emformer gets $4.6$ folds\ntraining speedup and $18\\%$ relative real-time factor (RTF) reduction in\ndecoding with relative WER reduction $17\\%$ on test-clean and $9\\%$ on\ntest-other. For a low latency scenario with an average latency of 80 ms,\nEmformer achieves WER $3.01\\%$ on test-clean and $7.09\\%$ on test-other.\nComparing with the LSTM baseline with the same latency and model size, Emformer\ngets relative WER reduction $9\\%$ and $16\\%$ on test-clean and test-other,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:38:09 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 19:59:08 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 14:55:59 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 07:07:35 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Shi", "Yangyang", ""], ["Wang", "Yongqiang", ""], ["Wu", "Chunyang", ""], ["Yeh", "Ching-Feng", ""], ["Chan", "Julian", ""], ["Zhang", "Frank", ""], ["Le", "Duc", ""], ["Seltzer", "Mike", ""]]}, {"id": "2010.10789", "submitter": "Weizhen Qi", "authors": "Weizhen Qi, Yeyun Gong, Yu Yan, Jian Jiao, Bo Shao, Ruofei Zhang,\n  Houqiang Li, Nan Duan, Ming Zhou", "title": "ProphetNet-Ads: A Looking Ahead Strategy for Generative Retrieval Models\n  in Sponsored Search Engine", "comments": "Accepted to NLPCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sponsored search engine, generative retrieval models are recently\nproposed to mine relevant advertisement keywords for users' input queries.\nGenerative retrieval models generate outputs token by token on a path of the\ntarget library prefix tree (Trie), which guarantees all of the generated\noutputs are legal and covered by the target library. In actual use, we found\nseveral typical problems caused by Trie-constrained searching length. In this\npaper, we analyze these problems and propose a looking ahead strategy for\ngenerative retrieval models named ProphetNet-Ads. ProphetNet-Ads improves the\nretrieval ability by directly optimizing the Trie-constrained searching space.\nWe build a dataset from a real-word sponsored search engine and carry out\nexperiments to analyze different generative retrieval models. Compared with\nTrie-based LSTM generative retrieval model proposed recently, our single model\nresult and integrated result improve the recall by 15.58\\% and 18.8\\%\nrespectively with beam size 5. Case studies further demonstrate how these\nproblems are alleviated by ProphetNet-Ads clearly.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:03:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Qi", "Weizhen", ""], ["Gong", "Yeyun", ""], ["Yan", "Yu", ""], ["Jiao", "Jian", ""], ["Shao", "Bo", ""], ["Zhang", "Ruofei", ""], ["Li", "Houqiang", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""]]}, {"id": "2010.10801", "submitter": "Arthur Jacobs M", "authors": "Arthur M. Jacobs and Annette Kinder", "title": "Quasi Error-free Text Classification and Authorship Recognition in a\n  large Corpus of English Literature based on a Novel Feature Set", "comments": "18 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gutenberg Literary English Corpus (GLEC) provides a rich source of\ntextual data for research in digital humanities, computational linguistics or\nneurocognitive poetics. However, so far only a small subcorpus, the Gutenberg\nEnglish Poetry Corpus, has been submitted to quantitative text analyses\nproviding predictions for scientific studies of literature. Here we show that\nin the entire GLEC quasi error-free text classification and authorship\nrecognition is possible with a method using the same set of five style and five\ncontent features, computed via style and sentiment analysis, in both tasks. Our\nresults identify two standard and two novel features (i.e., type-token ratio,\nfrequency, sonority score, surprise) as most diagnostic in these tasks. By\nproviding a simple tool applicable to both short poems and long novels\ngenerating quantitative predictions about features that co-determe the\ncognitive and affective processing of specific text categories or authors, our\ndata pave the way for many future computational and empirical studies of\nliterature or experiments in reading psychology.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:39:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jacobs", "Arthur M.", ""], ["Kinder", "Annette", ""]]}, {"id": "2010.10811", "submitter": "Puhai Yang", "authors": "Puhai Yang, Heyan Huang, Xianling Mao", "title": "STN4DST: A Scalable Dialogue State Tracking based on Slot Tagging\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability for handling unknown slot values is a important problem in\ndialogue state tracking (DST). As far as we know, previous scalable DST\napproaches generally rely on either the candidate generation from slot tagging\noutput or the span extraction in dialogue context. However, the candidate\ngeneration based DST often suffers from error propagation due to its pipelined\ntwo-stage process; meanwhile span extraction based DST has the risk of\ngenerating invalid spans in the lack of semantic constraints between start and\nend position pointers. To tackle the above drawbacks, in this paper, we propose\na novel scalable dialogue state tracking method based on slot tagging\nnavigation, which implements an end-to-end single-step pointer to locate and\nextract slot value quickly and accurately by the joint learning of slot tagging\nand slot value position prediction in the dialogue context, especially for\nunknown slot values. Extensive experiments over several benchmark datasets show\nthat the proposed model performs better than state-of-the-art baselines\ngreatly.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:09:20 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 11:00:12 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yang", "Puhai", ""], ["Huang", "Heyan", ""], ["Mao", "Xianling", ""]]}, {"id": "2010.10813", "submitter": "Jinman Zhao", "authors": "Zhao Jinman, Shawn Zhong, Xiaomin Zhang, Yingyu Liang", "title": "PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding", "comments": "16 pages including 4 pages of appendix. Accepted to Findings of EMNLP\n  2020 and SustaiNLP 2020. Code can be found at\n  [https://github.com/jmzhao/pbos]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look into the task of \\emph{generalizing} word embeddings: given a set of\npre-trained word vectors over a finite vocabulary, the goal is to predict\nembedding vectors for out-of-vocabulary words, \\emph{without} extra contextual\ninformation. We rely solely on the spellings of words and propose a model,\nalong with an efficient algorithm, that simultaneously models subword\nsegmentation and computes subword-based compositional word embedding. We call\nthe model probabilistic bag-of-subwords (PBoS), as it applies bag-of-subwords\nfor all possible segmentations based on their likelihood. Inspections and affix\nprediction experiment show that PBoS is able to produce meaningful subword\nsegmentations and subword rankings without any source of explicit morphological\nknowledge. Word similarity and POS tagging experiments show clear advantages of\nPBoS over previous subword-level models in the quality of generated word\nembeddings across languages.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:11:08 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jinman", "Zhao", ""], ["Zhong", "Shawn", ""], ["Zhang", "Xiaomin", ""], ["Liang", "Yingyu", ""]]}, {"id": "2010.10817", "submitter": "Chengzhi Zhang", "authors": "Yuzhuo Wang, Chengzhi Zhang", "title": "Using the Full-text Content of Academic Articles to Identify and\n  Evaluate Algorithm Entities in the Domain of Natural Language Processing", "comments": null, "journal-ref": "Journal of Informetrics,2020", "doi": "10.1016/j.joi.2020.101091", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, the advancement, improvement, and application of\nalgorithms in academic research have played an important role in promoting the\ndevelopment of different disciplines. Academic papers in various disciplines,\nespecially computer science, contain a large number of algorithms. Identifying\nthe algorithms from the full-text content of papers can determine popular or\nclassical algorithms in a specific field and help scholars gain a comprehensive\nunderstanding of the algorithms and even the field. To this end, this article\ntakes the field of natural language processing (NLP) as an example and\nidentifies algorithms from academic papers in the field. A dictionary of\nalgorithms is constructed by manually annotating the contents of papers, and\nsentences containing algorithms in the dictionary are extracted through\ndictionary-based matching. The number of articles mentioning an algorithm is\nused as an indicator to analyze the influence of that algorithm. Our results\nreveal the algorithm with the highest influence in NLP papers and show that\nclassification algorithms represent the largest proportion among the\nhigh-impact algorithms. In addition, the evolution of the influence of\nalgorithms reflects the changes in research tasks and topics in the field, and\nthe changes in the influence of different algorithms show different trends. As\na preliminary exploration, this paper conducts an analysis of the impact of\nalgorithms mentioned in the academic text, and the results can be used as\ntraining data for the automatic extraction of large-scale algorithms in the\nfuture. The methodology in this paper is domain-independent and can be applied\nto other domains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:24:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wang", "Yuzhuo", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2010.10820", "submitter": "Chan Young Park", "authors": "Chan Young Park, Xinru Yan, Anjalie Field, Yulia Tsvetkov", "title": "Multilingual Contextual Affective Analysis of LGBT People Portrayals in\n  Wikipedia", "comments": "ICWSM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Specific lexical choices in narrative text reflect both the writer's\nattitudes towards people in the narrative and influence the audience's\nreactions. Prior work has examined descriptions of people in English using\ncontextual affective analysis, a natural language processing (NLP) technique\nthat seeks to analyze how people are portrayed along dimensions of power,\nagency, and sentiment. Our work presents an extension of this methodology to\nmultilingual settings, which is enabled by a new corpus that we collect and a\nnew multilingual model. We additionally show how word connotations differ\nacross languages and cultures, highlighting the difficulty of generalizing\nexisting English datasets and methods. We then demonstrate the usefulness of\nour method by analyzing Wikipedia biography pages of members of the LGBT\ncommunity across three languages: English, Russian, and Spanish. Our results\nshow systematic differences in how the LGBT community is portrayed across\nlanguages, surfacing cultural differences in narratives and signs of social\nbiases. Practically, this model can be used to identify Wikipedia articles for\nfurther manual analysis -- articles that might contain content gaps or an\nimbalanced representation of particular social groups.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:27:36 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 08:20:12 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Park", "Chan Young", ""], ["Yan", "Xinru", ""], ["Field", "Anjalie", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2010.10833", "submitter": "Xinyu Zuo", "authors": "Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao", "title": "KnowDis: Knowledge Enhanced Data Augmentation for Event Causality\n  Detection via Distant Supervision", "comments": "Accepted to COLING2020", "journal-ref": "COLING2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern models of event causality detection (ECD) are mainly based on\nsupervised learning from small hand-labeled corpora. However, hand-labeled\ntraining data is expensive to produce, low coverage of causal expressions and\nlimited in size, which makes supervised methods hard to detect causal relations\nbetween events. To solve this data lacking problem, we investigate a data\naugmentation framework for ECD, dubbed as Knowledge Enhanced Distant Data\nAugmentation (KnowDis). Experimental results on two benchmark datasets\nEventStoryLine corpus and Causal-TimeBank show that 1) KnowDis can augment\navailable training data assisted with the lexical and causal commonsense\nknowledge for ECD via distant supervision, and 2) our method outperforms\nprevious methods by a large margin assisted with automatically labeled training\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:44:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zuo", "Xinyu", ""], ["Chen", "Yubo", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""]]}, {"id": "2010.10836", "submitter": "Deepak P", "authors": "Soumya Suvra Ghosal, Deepak P, Anna Jurek-Loughrey", "title": "ReSCo-CC: Unsupervised Identification of Key Disinformation Sentences", "comments": "The 22nd International Conference on Information Integration and\n  Web-based Applications & Services (iiWAS '20), Chiang Mai, Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disinformation is often presented in long textual articles, especially when\nit relates to domains such as health, often seen in relation to COVID-19. These\narticles are typically observed to have a number of trustworthy sentences among\nwhich core disinformation sentences are scattered. In this paper, we propose a\nnovel unsupervised task of identifying sentences containing key disinformation\nwithin a document that is known to be untrustworthy. We design a three-phase\nstatistical NLP solution for the task which starts with embedding sentences\nwithin a bespoke feature space designed for the task. Sentences represented\nusing those features are then clustered, following which the key sentences are\nidentified through proximity scoring. We also curate a new dataset with\nsentence level disinformation scorings to aid evaluation for this task; the\ndataset is being made publicly available to facilitate further research. Based\non a comprehensive empirical evaluation against techniques from related tasks\nsuch as claim detection and summarization, as well as against simplified\nvariants of our proposed approach, we illustrate that our method is able to\nidentify core disinformation effectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:53:36 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ghosal", "Soumya Suvra", ""], ["P", "Deepak", ""], ["Jurek-Loughrey", "Anna", ""]]}, {"id": "2010.10839", "submitter": "Wubo Li", "authors": "Wubo Li, Dongwei Jiang, Wei Zou, Xiangang Li", "title": "TMT: A Transformer-based Modal Translator for Improving Multimodal\n  Sequence Representations in Audio Visual Scene-aware Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses when\ndiscussing about a given video. The previous state-of-the-art model shows\nsuperior performance for this task using Transformer-based architecture.\nHowever, there remain some limitations in learning better representation of\nmodalities. Inspired by Neural Machine Translation (NMT), we propose the\nTransformer-based Modal Translator (TMT) to learn the representations of the\nsource modal sequence by translating the source modal sequence to the related\ntarget modal sequence in a supervised manner. Based on Multimodal Transformer\nNetworks (MTN), we apply TMT to video and dialog, proposing MTN-TMT for the\nvideo-grounded dialog system. On the AVSD track of the Dialog System Technology\nChallenge 7, MTN-TMT outperforms the MTN and other submission models in both\nVideo and Text task and Text Only task. Compared with MTN, MTN-TMT improves all\nmetrics, especially, achieving relative improvement up to 14.1% on CIDEr. Index\nTerms: multimodal learning, audio-visual scene-aware dialog, neural machine\ntranslation, multi-task learning\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:02:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Wubo", ""], ["Jiang", "Dongwei", ""], ["Zou", "Wei", ""], ["Li", "Xiangang", ""]]}, {"id": "2010.10852", "submitter": "Huy Quoc To", "authors": "Huy Quoc To, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen, Anh Gia-Tuan\n  Nguyen", "title": "Gender Prediction Based on Vietnamese Names with Machine Learning\n  Techniques", "comments": "6 pages, 6 figures. NLPIR 2020: 4th International Conference on\n  Natural Language Processing and Information Retrieval", "journal-ref": null, "doi": "10.1145/3443279.3443309", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As biological gender is one of the aspects of presenting individual human,\nmuch work has been done on gender classification based on people names. The\nproposals for English and Chinese languages are tremendous; still, there have\nbeen few works done for Vietnamese so far. We propose a new dataset for gender\nprediction based on Vietnamese names. This dataset comprises over 26,000 full\nnames annotated with genders. This dataset is available on our website for\nresearch purposes. In addition, this paper describes six machine learning\nalgorithms (Support Vector Machine, Multinomial Naive Bayes, Bernoulli Naive\nBayes, Decision Tree, Random Forrest and Logistic Regression) and a deep\nlearning model (LSTM) with fastText word embedding for gender prediction on\nVietnamese names. We create a dataset and investigate the impact of each name\ncomponent on detecting gender. As a result, the best F1-score that we have\nachieved is up to 96% on LSTM model and we generate a web API based on our\ntrained model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:25:48 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 02:21:32 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 01:29:35 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 07:25:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["To", "Huy Quoc", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""], ["Nguyen", "Anh Gia-Tuan", ""]]}, {"id": "2010.10866", "submitter": "Cl\\'ement Rebuffel", "authors": "Cl\\'ement Rebuffel, Laure Soulier, Geoffrey Scoutheeten, Patrick\n  Gallinari", "title": "PARENTing via Model-Agnostic Reinforcement Learning to Correct\n  Pathological Behaviors in Data-to-Text Generation", "comments": "Accepted at the 13th International Conference on Natural Language\n  Generation (INLG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In language generation models conditioned by structured data, the classical\ntraining via maximum likelihood almost always leads models to pick up on\ndataset divergence (i.e., hallucinations or omissions), and to incorporate them\nerroneously in their own generations at inference. In this work, we build ontop\nof previous Reinforcement Learning based approaches and show that a\nmodel-agnostic framework relying on the recently introduced PARENT metric is\nefficient at reducing both hallucinations and omissions. Evaluations on the\nwidely used WikiBIO and WebNLG benchmarks demonstrate the effectiveness of this\nframework compared to state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:49:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 13:00:20 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Rebuffel", "Cl\u00e9ment", ""], ["Soulier", "Laure", ""], ["Scoutheeten", "Geoffrey", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2010.10873", "submitter": "Milad Moradi", "authors": "Milad Moradi, Matthias Samwald", "title": "Explaining black-box text classifiers for disease-treatment information\n  extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and other intricate Artificial Intelligence (AI) models\nhave reached high levels of accuracy on many biomedical natural language\nprocessing tasks. However, their applicability in real-world use cases may be\nlimited due to their vague inner working and decision logic. A post-hoc\nexplanation method can approximate the behavior of a black-box AI model by\nextracting relationships between feature values and outcomes. In this paper, we\nintroduce a post-hoc explanation method that utilizes confident itemsets to\napproximate the behavior of black-box classifiers for medical information\nextraction. Incorporating medical concepts and semantics into the explanation\nprocess, our explanator finds semantic relations between inputs and outputs in\ndifferent parts of the decision space of a black-box classifier. The\nexperimental results show that our explanation method can outperform\nperturbation and decision set based explanators in terms of fidelity and\ninterpretability of explanations produced for predictions on a\ndisease-treatment information extraction task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:58:00 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Moradi", "Milad", ""], ["Samwald", "Matthias", ""]]}, {"id": "2010.10874", "submitter": "Erik Ekstedt", "authors": "Erik Ekstedt and Gabriel Skantze", "title": "TurnGPT: a Transformer-based Language Model for Predicting Turn-taking\n  in Spoken Dialog", "comments": "Accepted to Findings of ACL: EMNLP 2020", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.268", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic and pragmatic completeness is known to be important for turn-taking\nprediction, but so far machine learning models of turn-taking have used such\nlinguistic information in a limited way. In this paper, we introduce TurnGPT, a\ntransformer-based language model for predicting turn-shifts in spoken dialog.\nThe model has been trained and evaluated on a variety of written and spoken\ndialog datasets. We show that the model outperforms two baselines used in prior\nwork. We also report on an ablation study, as well as attention and gradient\nanalyses, which show that the model is able to utilize the dialog context and\npragmatic completeness for turn-taking prediction. Finally, we explore the\nmodel's potential in not only detecting, but also projecting, turn-completions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:58:39 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ekstedt", "Erik", ""], ["Skantze", "Gabriel", ""]]}, {"id": "2010.10892", "submitter": "Yang Jiao", "authors": "Yang Jiao", "title": "BERT for Joint Multichannel Speech Dereverberation with Spatial-aware\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for joint multichannel speech dereverberation with two\nspatial-aware tasks: direction-of-arrival (DOA) estimation and speech\nseparation. The proposed method addresses involved tasks as a sequence to\nsequence mapping problem, which is general enough for a variety of front-end\nspeech enhancement tasks. The proposed method is inspired by the excellent\nsequence modeling capability of bidirectional encoder representation from\ntransformers (BERT). Instead of utilizing explicit representations from\npretraining in a self-supervised manner, we utilizes transformer encoded hidden\nrepresentations in a supervised manner. Both multichannel spectral magnitude\nand spectral phase information of varying length utterances are encoded.\nExperimental result demonstrates the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:05:17 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 02:41:39 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Jiao", "Yang", ""]]}, {"id": "2010.10894", "submitter": "Junwei Bao Ph.D.", "authors": "Yingyao Wang, Junwei Bao, Guangyi Liu, Youzheng Wu, Xiaodong He, Bowen\n  Zhou and Tiejun Zhao", "title": "Learning to Decouple Relations: Few-Shot Relation Classification with\n  Entity-Guided Attention and Confusion-Aware Training", "comments": "11 pages, 5 figures, accepted by COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to enhance the few-shot relation classification especially\nfor sentences that jointly describe multiple relations. Due to the fact that\nsome relations usually keep high co-occurrence in the same context, previous\nfew-shot relation classifiers struggle to distinguish them with few annotated\ninstances. To alleviate the above relation confusion problem, we propose CTEG,\na model equipped with two mechanisms to learn to decouple these easily-confused\nrelations. On the one hand, an Entity-Guided Attention (EGA) mechanism, which\nleverages the syntactic relations and relative positions between each word and\nthe specified entity pair, is introduced to guide the attention to filter out\ninformation causing confusion. On the other hand, a Confusion-Aware Training\n(CAT) method is proposed to explicitly learn to distinguish relations by\nplaying a pushing-away game between classifying a sentence into a true relation\nand its confusing relation. Extensive experiments are conducted on the FewRel\ndataset, and the results show that our proposed model achieves comparable and\neven much better results to strong baselines in terms of accuracy. Furthermore,\nthe ablation test and case study verify the effectiveness of our proposed EGA\nand CAT, especially in addressing the relation confusion problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:07:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wang", "Yingyao", ""], ["Bao", "Junwei", ""], ["Liu", "Guangyi", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2010.10900", "submitter": "Tommaso Soru", "authors": "Anand Panchbhai and Tommaso Soru and Edgard Marx", "title": "Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition", "comments": "Proceedings of the First Indo-American Knowledge Graph and Semantic\n  Web Conference (KGSWC-India 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A booming amount of information is continuously added to the Internet as\nstructured and unstructured data, feeding knowledge bases such as DBpedia and\nWikidata with billions of statements describing millions of entities. The aim\nof Question Answering systems is to allow lay users to access such data using\nnatural language without needing to write formal queries. However, users often\nsubmit questions that are complex and require a certain level of abstraction\nand reasoning to decompose them into basic graph patterns. In this short paper,\nwe explore the use of architectures based on Neural Machine Translation called\nNeural SPARQL Machines to learn pattern compositions. We show that\nsequence-to-sequence models are a viable and promising option to transform long\nutterances into complex SPARQL queries.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:12:01 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Panchbhai", "Anand", ""], ["Soru", "Tommaso", ""], ["Marx", "Edgard", ""]]}, {"id": "2010.10906", "submitter": "Branden Chan", "authors": "Branden Chan, Stefan Schweter, Timo M\\\"oller", "title": "German's Next Language Model", "comments": "Accepted by COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present the experiments which lead to the creation of our\nBERT and ELECTRA based German language models, GBERT and GELECTRA. By varying\nthe input training data, model size, and the presence of Whole Word Masking\n(WWM) we were able to attain SoTA performance across a set of document\nclassification and named entity recognition (NER) tasks for both models of base\nand large size. We adopt an evaluation driven approach in training these models\nand our results indicate that both adding more data and utilizing WWM improve\nmodel performance. By benchmarking against existing German models, we show that\nthese models are the best German models to date. Our trained models will be\nmade publicly available to the research community.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:28:23 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:39:30 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 17:21:23 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 11:02:43 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chan", "Branden", ""], ["Schweter", "Stefan", ""], ["M\u00f6ller", "Timo", ""]]}, {"id": "2010.10907", "submitter": "Elena Voita", "authors": "Elena Voita, Rico Sennrich, Ivan Titov", "title": "Analyzing the Source and Target Contributions to Predictions in Neural\n  Machine Translation", "comments": "ACL 2021 (more accurate results with the improved LRP code)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Neural Machine Translation (and, more generally, conditional language\nmodeling), the generation of a target token is influenced by two types of\ncontext: the source and the prefix of the target sequence. While many attempts\nto understand the internal workings of NMT models have been made, none of them\nexplicitly evaluates relative source and target contributions to a generation\ndecision. We argue that this relative contribution can be evaluated by adopting\na variant of Layerwise Relevance Propagation (LRP). Its underlying\n'conservation principle' makes relevance propagation unique: differently from\nother methods, it evaluates not an abstract quantity reflecting token\nimportance, but the proportion of each token's influence. We extend LRP to the\nTransformer and conduct an analysis of NMT models which explicitly evaluates\nthe source and target relative contributions to the generation process. We\nanalyze changes in these contributions when conditioning on different types of\nprefixes, when varying the training objective or the amount of training data,\nand during the training process. We find that models trained with more data\ntend to rely on source information more and to have more sharp token\ncontributions; the training process is non-monotonic with several stages of\ndifferent nature.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:37:27 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 10:30:46 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 14:32:12 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Voita", "Elena", ""], ["Sennrich", "Rico", ""], ["Titov", "Ivan", ""]]}, {"id": "2010.10910", "submitter": "Mali Jin", "authors": "Mali Jin and Nikolaos Aletras", "title": "Complaint Identification in Social Media with Transformer Networks", "comments": "Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complaining is a speech act extensively used by humans to communicate a\nnegative inconsistency between reality and expectations. Previous work on\nautomatically identifying complaints in social media has focused on using\nfeature-based and task-specific neural network models. Adapting\nstate-of-the-art pre-trained neural language models and their combinations with\nother linguistic information from topics or sentiment for complaint prediction\nhas yet to be explored. In this paper, we evaluate a battery of neural models\nunderpinned by transformer networks which we subsequently combine with\nlinguistic information. Experiments on a publicly available data set of\ncomplaints demonstrate that our models outperform previous state-of-the-art\nmethods by a large margin achieving a macro F1 up to 87.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:44:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jin", "Mali", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2010.10921", "submitter": "Aibek Makazhanov", "authors": "Aibek Makazhanov, Sharon Goldwater, Adam Lopez", "title": "LemMED: Fast and Effective Neural Morphological Analysis with Short\n  Context Windows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LemMED, a character-level encoder-decoder for contextual\nmorphological analysis (combined lemmatization and tagging). LemMED extends and\nis named after two other attention-based models, namely Lematus, a contextual\nlemmatizer, and MED, a morphological (re)inflection model. Our approach does\nnot require training separate lemmatization and tagging models, nor does it\nneed additional resources and tools, such as morphological dictionaries or\ntransducers. Moreover, LemMED relies solely on character-level representations\nand on local context. Although the model can, in principle, account for global\ncontext on sentence level, our experiments show that using just a single word\nof context around each target word is not only more computationally feasible,\nbut yields better results as well. We evaluate LemMED in the framework of the\nSIMGMORPHON-2019 shared task on combined lemmatization and tagging. In terms of\naverage performance LemMED ranks 5th among 13 systems and is bested only by the\nsubmissions that use contextualized embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:08:02 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Makazhanov", "Aibek", ""], ["Goldwater", "Sharon", ""], ["Lopez", "Adam", ""]]}, {"id": "2010.10932", "submitter": "Sungchul Choi", "authors": "Jaewoong Choi, Sion Jang, Jaeyoung Kim, Jiho Lee, Janghyeok Yoona,\n  Sungchul Choi", "title": "Deep learning-based citation recommendation system for patents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we address the challenges in developing a deep learning-based\nautomatic patent citation recommendation system. Although deep learning-based\nrecommendation systems have exhibited outstanding performance in various\ndomains (such as movies, products, and paper citations), their validity in\npatent citations has not been investigated, owing to the lack of a freely\navailable high-quality dataset and relevant benchmark model. To solve these\nproblems, we present a novel dataset called PatentNet that includes textual\ninformation and metadata for approximately 110,000 patents from the Google Big\nQuery service. Further, we propose strong benchmark models considering the\nsimilarity of textual information and metadata (such as cooperative patent\nclassification code). Compared with existing recommendation methods, the\nproposed benchmark method achieved a mean reciprocal rank of 0.2377 on the test\nset, whereas the existing state-of-the-art recommendation method achieved\n0.2073.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:18:21 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Choi", "Jaewoong", ""], ["Jang", "Sion", ""], ["Kim", "Jaeyoung", ""], ["Lee", "Jiho", ""], ["Yoona", "Janghyeok", ""], ["Choi", "Sungchul", ""]]}, {"id": "2010.10938", "submitter": "Chi-Liang Liu", "authors": "Chi-Liang Liu and Tsung-Yuan Hsu and Yung-Sung Chuang and Hung-yi Lee", "title": "What makes multilingual BERT multilingual?", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.09205", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multilingual BERT works remarkably well on cross-lingual transfer\ntasks, superior to static non-contextualized word embeddings. In this work, we\nprovide an in-depth experimental study to supplement the existing literature of\ncross-lingual ability. We compare the cross-lingual ability of\nnon-contextualized and contextualized representation model with the same data.\nWe found that datasize and context window size are crucial factors to the\ntransferability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 05:41:56 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Liu", "Chi-Liang", ""], ["Hsu", "Tsung-Yuan", ""], ["Chuang", "Yung-Sung", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2010.10998", "submitter": "Aditya Kalyanpur", "authors": "Aditya Kalyanpur, Or Biran, Tom Breloff, Jennifer Chu-Carroll, Ariel\n  Diertani, Owen Rambow, Mark Sammons", "title": "Open-Domain Frame Semantic Parsing Using Transformers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frame semantic parsing is a complex problem which includes multiple\nunderlying subtasks. Recent approaches have employed joint learning of subtasks\n(such as predicate and argument detection), and multi-task learning of related\ntasks (such as syntactic and semantic parsing). In this paper, we explore\nmulti-task learning of all subtasks with transformer-based models. We show that\na purely generative encoder-decoder architecture handily beats the previous\nstate of the art in FrameNet 1.7 parsing, and that a mixed decoding multi-task\napproach achieves even better performance. Finally, we show that the multi-task\nmodel also outperforms recent state of the art systems for PropBank SRL parsing\non the CoNLL 2012 benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:38:04 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 23:37:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kalyanpur", "Aditya", ""], ["Biran", "Or", ""], ["Breloff", "Tom", ""], ["Chu-Carroll", "Jennifer", ""], ["Diertani", "Ariel", ""], ["Rambow", "Owen", ""], ["Sammons", "Mark", ""]]}, {"id": "2010.10999", "submitter": "Sohee Yang", "authors": "Sohee Yang, Minjoon Seo", "title": "Is Retriever Merely an Approximator of Reader?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art in open-domain question answering (QA) relies on an\nefficient retriever that drastically reduces the search space for the expensive\nreader. A rather overlooked question in the community is the relationship\nbetween the retriever and the reader, and in particular, if the whole purpose\nof the retriever is just a fast approximation for the reader. Our empirical\nevidence indicates that the answer is no, and that the reader and the retriever\nare complementary to each other even in terms of accuracy only. We make a\ncareful conjecture that the architectural constraint of the retriever, which\nhas been originally intended for enabling approximate search, seems to also\nmake the model more robust in large-scale search. We then propose to distill\nthe reader into the retriever so that the retriever absorbs the strength of the\nreader while keeping its own benefit. Experimental results show that our method\ncan enhance the document recall rate as well as the end-to-end QA accuracy of\noff-the-shelf retrievers in open-domain QA tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:40:15 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Yang", "Sohee", ""], ["Seo", "Minjoon", ""]]}, {"id": "2010.11003", "submitter": "Chi-Liang Liu", "authors": "Chi-Liang Liu and Hung-yi Lee", "title": "Unsupervised Deep Learning based Multiple Choices Question Answering:\n  Start Learning from Basic Knowledge", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the possibility of almost unsupervised Multiple\nChoices Question Answering (MCQA). Starting from very basic knowledge, MCQA\nmodel knows that some choices have higher probabilities of being correct than\nthe others. The information, though very noisy, guides the training of an MCQA\nmodel. The proposed method is shown to outperform the baseline approaches on\nRACE and even comparable with some supervised learning approaches on MC500.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:44:35 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Liu", "Chi-Liang", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2010.11004", "submitter": "Mounica Maddela", "authors": "Mounica Maddela, Fernando Alva-Manchego, Wei Xu", "title": "Controllable Text Simplification with Explicit Paraphrasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Text Simplification improves the readability of sentences through several\nrewriting transformations, such as lexical paraphrasing, deletion, and\nsplitting. Current simplification systems are predominantly\nsequence-to-sequence models that are trained end-to-end to perform all these\noperations simultaneously. However, such systems limit themselves to mostly\ndeleting words and cannot easily adapt to the requirements of different target\naudiences. In this paper, we propose a novel hybrid approach that leverages\nlinguistically-motivated rules for splitting and deletion, and couples them\nwith a neural paraphrasing model to produce varied rewriting styles. We\nintroduce a new data augmentation method to improve the paraphrasing capability\nof our model. Through automatic and manual evaluations, we show that our\nproposed model establishes a new state-of-the-art for the task, paraphrasing\nmore often than the existing systems, and can control the degree of each\nsimplification operation applied to the input texts.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:44:40 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 00:44:57 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 23:57:07 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Maddela", "Mounica", ""], ["Alva-Manchego", "Fernando", ""], ["Xu", "Wei", ""]]}, {"id": "2010.11018", "submitter": "Huaao Zhang", "authors": "Huaao Zhang, Shigui Qiu, Xiangyu Duan, Min Zhang", "title": "Token Drop mechanism for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural machine translation with millions of parameters is vulnerable to\nunfamiliar inputs. We propose Token Drop to improve generalization and avoid\noverfitting for the NMT model. Similar to word dropout, whereas we replace\ndropped token with a special token instead of setting zero to words. We further\nintroduce two self-supervised objectives: Replaced Token Detection and Dropped\nToken Prediction. Our method aims to force model generating target translation\nwith less information, in this way the model can learn textual representation\nbetter. Experiments on Chinese-English and English-Romanian benchmark\ndemonstrate the effectiveness of our approach and our model achieves\nsignificant improvements over a strong Transformer baseline.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:02:27 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhang", "Huaao", ""], ["Qiu", "Shigui", ""], ["Duan", "Xiangyu", ""], ["Zhang", "Min", ""]]}, {"id": "2010.11019", "submitter": "Pranaydeep Singh", "authors": "Pranaydeep Singh and Els Lefever", "title": "LT3 at SemEval-2020 Task 9: Cross-lingual Embeddings for Sentiment\n  Analysis of Hinglish Social Media Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our contribution to the SemEval-2020 Task 9 on Sentiment\nAnalysis for Code-mixed Social Media Text. We investigated two approaches to\nsolve the task of Hinglish sentiment analysis. The first approach uses\ncross-lingual embeddings resulting from projecting Hinglish and pre-trained\nEnglish FastText word embeddings in the same space. The second approach\nincorporates pre-trained English embeddings that are incrementally retrained\nwith a set of Hinglish tweets. The results show that the second approach\nperforms best, with an F1-score of 70.52% on the held-out test data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:03:16 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Singh", "Pranaydeep", ""], ["Lefever", "Els", ""]]}, {"id": "2010.11032", "submitter": "Leshem Choshen", "authors": "Leshem Choshen, Dmitry Nikolaev, Yevgeni Berzak, Omri Abend", "title": "Classifying Syntactic Errors in Learner Language", "comments": "CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for classifying syntactic errors in learner language,\nnamely errors whose correction alters the morphosyntactic structure of a\nsentence.\n  The methodology builds on the established Universal Dependencies syntactic\nrepresentation scheme, and provides complementary information to other\nerror-classification systems.\n  Unlike existing error classification methods, our method is applicable across\nlanguages, which we showcase by producing a detailed picture of syntactic\nerrors in learner English and learner Russian. We further demonstrate the\nutility of the methodology for analyzing the outputs of leading Grammatical\nError Correction (GEC) systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:28:22 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 14:58:14 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Choshen", "Leshem", ""], ["Nikolaev", "Dmitry", ""], ["Berzak", "Yevgeni", ""], ["Abend", "Omri", ""]]}, {"id": "2010.11054", "submitter": "Jiaming Luo", "authors": "Jiaming Luo, Frederik Hartmann, Enrico Santus, Yuan Cao, Regina\n  Barzilay", "title": "Deciphering Undersegmented Ancient Scripts Using Phonetic Prior", "comments": "TACL 2020, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most undeciphered lost languages exhibit two characteristics that pose\nsignificant decipherment challenges: (1) the scripts are not fully segmented\ninto words; (2) the closest known language is not determined. We propose a\ndecipherment model that handles both of these challenges by building on rich\nlinguistic constraints reflecting consistent patterns in historical sound\nchange. We capture the natural phonological geometry by learning character\nembeddings based on the International Phonetic Alphabet (IPA). The resulting\ngenerative framework jointly models word segmentation and cognate alignment,\ninformed by phonological constraints. We evaluate the model on both deciphered\nlanguages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments\nshow that incorporating phonetic geometry leads to clear and consistent gains.\nAdditionally, we propose a measure for language closeness which correctly\nidentifies related languages for Gothic and Ugaritic. For Iberian, the method\ndoes not show strong evidence supporting Basque as a related language,\nconcurring with the favored position by the current scholarship.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:03:52 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Luo", "Jiaming", ""], ["Hartmann", "Frederik", ""], ["Santus", "Enrico", ""], ["Cao", "Yuan", ""], ["Barzilay", "Regina", ""]]}, {"id": "2010.11066", "submitter": "Chenyu You", "authors": "Chenyu You, Nuo Chen, Yuexian Zou", "title": "Contextualized Attention-based Knowledge Transfer for Spoken\n  Conversational Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:17:18 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 02:17:26 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 20:04:11 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 16:32:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["You", "Chenyu", ""], ["Chen", "Nuo", ""], ["Zou", "Yuexian", ""]]}, {"id": "2010.11067", "submitter": "Chenyu You", "authors": "Chenyu You, Nuo Chen, Yuexian Zou", "title": "Knowledge Distillation for Improved Accuracy in Spoken Question\n  Answering", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken question answering (SQA) is a challenging task that requires the\nmachine to fully understand the complex spoken documents. Automatic speech\nrecognition (ASR) plays a significant role in the development of QA systems.\nHowever, the recent work shows that ASR systems generate highly noisy\ntranscripts, which critically limit the capability of machine comprehension on\nthe SQA task. To address the issue, we present a novel distillation framework.\nSpecifically, we devise a training strategy to perform knowledge distillation\n(KD) from spoken documents and written counterparts. Our work makes a step\ntowards distilling knowledge from the language model as a supervision signal to\nlead to better student accuracy by reducing the misalignment between automatic\nand manual transcriptions. Experiments demonstrate that our approach\noutperforms several state-of-the-art language models on the Spoken-SQuAD\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:18:01 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 02:16:42 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 02:26:27 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["You", "Chenyu", ""], ["Chen", "Nuo", ""], ["Zou", "Yuexian", ""]]}, {"id": "2010.11075", "submitter": "Nils Barlaug", "authors": "Nils Barlaug, Jon Atle Gulla", "title": "Neural Networks for Entity Matching: A Survey", "comments": "Published in ACM Transactions on Knowledge Discovery from Data (TKDD)", "journal-ref": "ACM Transactions on Knowledge Discovery from Data, Volume 15,\n  Issue 3, April 2021", "doi": "10.1145/3442200", "report-no": null, "categories": "cs.DB cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity matching is the problem of identifying which records refer to the same\nreal-world entity. It has been actively researched for decades, and a variety\nof different approaches have been developed. Even today, it remains a\nchallenging problem, and there is still generous room for improvement. In\nrecent years we have seen new methods based upon deep learning techniques for\nnatural language processing emerge.\n  In this survey, we present how neural networks have been used for entity\nmatching. Specifically, we identify which steps of the entity matching process\nexisting work have targeted using neural networks, and provide an overview of\nthe different techniques used at each step. We also discuss contributions from\ndeep learning in entity matching compared to traditional methods, and propose a\ntaxonomy of deep neural networks for entity matching.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:36:03 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 21:51:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Barlaug", "Nils", ""], ["Gulla", "Jon Atle", ""]]}, {"id": "2010.11080", "submitter": "Tao Yu", "authors": "Tao Yu, Shafiq Joty", "title": "Online Conversation Disentanglement with Pointer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge amounts of textual conversations occur online every day, where multiple\nconversations take place concurrently. Interleaved conversations lead to\ndifficulties in not only following the ongoing discussions but also extracting\nrelevant information from simultaneous messages. Conversation disentanglement\naims to separate intermingled messages into detached conversations. However,\nexisting disentanglement methods rely mostly on handcrafted features that are\ndataset specific, which hinders generalization and adaptability. In this work,\nwe propose an end-to-end online framework for conversation disentanglement that\navoids time-consuming domain-specific feature engineering. We design a novel\nway to embed the whole utterance that comprises timestamp, speaker, and message\ntext, and proposes a custom attention mechanism that models disentanglement as\na pointing problem while effectively capturing inter-utterance interactions in\nan end-to-end fashion. We also introduce a joint-learning objective to better\ncapture contextual information. Our experiments on the Ubuntu IRC dataset show\nthat our method achieves state-of-the-art performance in both link and\nconversation prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:43:07 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Yu", "Tao", ""], ["Joty", "Shafiq", ""]]}, {"id": "2010.11085", "submitter": "Sai Muralidhar Jayanthi", "authors": "Sai Muralidhar Jayanthi, Danish Pruthi, Graham Neubig", "title": "NeuSpell: A Neural Spelling Correction Toolkit", "comments": "Accepted at EMNLP 2020 (system demonstrations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce NeuSpell, an open-source toolkit for spelling correction in\nEnglish. Our toolkit comprises ten different models, and benchmarks them on\nnaturally occurring misspellings from multiple sources. We find that many\nsystems do not adequately leverage the context around the misspelt token. To\nremedy this, (i) we train neural models using spelling errors in context,\nsynthetically constructed by reverse engineering isolated misspellings; and\n(ii) use contextual representations. By training on our synthetic examples,\ncorrection rates improve by 9% (absolute) compared to the case when models are\ntrained on randomly sampled character perturbations. Using richer contextual\nrepresentations boosts the correction rate by another 3%. Our toolkit enables\npractitioners to use our proposed and existing spelling correction systems,\nboth via a unified command line, as well as a web interface. Among many\npotential applications, we demonstrate the utility of our spell-checkers in\ncombating adversarial misspellings. The toolkit can be accessed at\nneuspell.github.io. Code and pretrained models are available at\nhttp://github.com/neuspell/neuspell.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:53:29 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jayanthi", "Sai Muralidhar", ""], ["Pruthi", "Danish", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.11089", "submitter": "U\\u{g}ur Merto\\u{g}lu", "authors": "U\\u{g}ur Merto\\u{g}lu, Burkay Gen\\c{c}", "title": "Lexicon generation for detecting fake news", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the digitization of media, an immense amount of news data has been\ngenerated by online sources, including mainstream media outlets as well as\nsocial networks. However, the ease of production and distribution resulted in\ncirculation of fake news as well as credible, authentic news. The pervasive\ndissemination of fake news has extreme negative impacts on individuals and\nsociety. Therefore, fake news detection has recently become an emerging topic\nas an interdisciplinary research field that is attracting significant attention\nfrom many research disciplines, including social sciences and linguistics. In\nthis study, we propose a method primarily based on lexicons including a scoring\nsystem to facilitate the detection of the fake news in Turkish. We contribute\nto the literature by collecting a novel, large scale, and credible dataset of\nTurkish news, and by constructing the first fake news detection lexicon for\nTurkish.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 20:39:57 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Merto\u011flu", "U\u011fur", ""], ["Gen\u00e7", "Burkay", ""]]}, {"id": "2010.11091", "submitter": "Mohiuddin Md Abdul Qudar", "authors": "Mohiuddin Md Abdul Qudar, Vijay Mago", "title": "TweetBERT: A Pretrained Language Representation Model for Twitter Text\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is a well-known microblogging social site where users express their\nviews and opinions in real-time. As a result, tweets tend to contain valuable\ninformation. With the advancements of deep learning in the domain of natural\nlanguage processing, extracting meaningful information from tweets has become a\ngrowing interest among natural language researchers. Applying existing language\nrepresentation models to extract information from Twitter does not often\nproduce good results. Moreover, there is no existing language representation\nmodels for text analysis specific to the social media domain. Hence, in this\narticle, we introduce two TweetBERT models, which are domain specific language\npresentation models, pre-trained on millions of tweets. We show that the\nTweetBERT models significantly outperform the traditional BERT models in\nTwitter text mining tasks by more than 7% on each Twitter dataset. We also\nprovide an extensive analysis by evaluating seven BERT models on 31 different\ndatasets. Our results validate our hypothesis that continuously training\nlanguage models on twitter corpus help performance with Twitter.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:45:02 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Qudar", "Mohiuddin Md Abdul", ""], ["Mago", "Vijay", ""]]}, {"id": "2010.11092", "submitter": "Rian Adam Rajagede", "authors": "Rian Adam Rajagede and Rochana Prih Hastuti", "title": "Stacking Neural Network Models for Automatic Short Answer Scoring", "comments": "submitted to The 5th International Conference on Information\n  Technology and Digital Applications 2020", "journal-ref": null, "doi": "10.1088/1757-899X/1077/1/012013", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic short answer scoring is one of the text classification problems to\nassess students' answers during exams automatically. Several challenges can\narise in making an automatic short answer scoring system, one of which is the\nquantity and quality of the data. The data labeling process is not easy because\nit requires a human annotator who is an expert in their field. Further, the\ndata imbalance process is also a challenge because the number of labels for\ncorrect answers is always much less than the wrong answers. In this paper, we\npropose the use of a stacking model based on neural network and XGBoost for\nclassification process with sentence embedding feature. We also propose to use\ndata upsampling method to handle imbalance classes and hyperparameters\noptimization algorithm to find a robust model automatically. We use Ukara 1.0\nChallenge dataset and our best model obtained an F1-score of 0.821 exceeding\nthe previous work at the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:00:09 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rajagede", "Rian Adam", ""], ["Hastuti", "Rochana Prih", ""]]}, {"id": "2010.11119", "submitter": "Torsten Scholak", "authors": "Torsten Scholak, Raymond Li, Dzmitry Bahdanau, Harm de Vries, Chris\n  Pal", "title": "DuoRAT: Towards Simpler Text-to-SQL Models", "comments": "Code is available at https://github.com/ElementAI/duorat", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that neural text-to-SQL models can effectively\ntranslate natural language questions into corresponding SQL queries on unseen\ndatabases. Working mostly on the Spider dataset, researchers have been\nproposing increasingly sophisticated modelling approaches to the problem.\nContrary to this trend, in this paper we identify the aspects in which\ntext-to-SQL models can be simplified. We begin by building DuoRAT, a\nre-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is\nusing only relation-aware or vanilla transformers as the building blocks. We\nperform several ablation experiments using DuoRAT as the baseline model. Our\nexperiments confirm the usefulness of some of the techniques and point out the\nredundancy of others, including structural SQL features and features that link\nthe question with the schema.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:27:49 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Scholak", "Torsten", ""], ["Li", "Raymond", ""], ["Bahdanau", "Dzmitry", ""], ["de Vries", "Harm", ""], ["Pal", "Chris", ""]]}, {"id": "2010.11123", "submitter": "Daniel Ajisafe", "authors": "Daniel Ajisafe, Oluwabukola Adegboro, Esther Oduntan, Tayo Arulogun", "title": "Towards End-to-End Training of Automatic Speech Recognition for Nigerian\n  Pidgin", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nigerian Pidgin remains one of the most popular languages in West Africa.\nWith at least 75 million speakers along the West African coast, the language\nhas spread to diasporic communities through Nigerian immigrants in England,\nCanada, and America, amongst others. In contrast, the language remains an\nunder-resourced one in the field of natural language processing, particularly\non speech recognition and translation tasks. In this work, we present the first\nparallel (speech-to-text) data on Nigerian pidgin. We also trained the first\nend-to-end speech recognition system (QuartzNet and Jasper model) on this\nlanguage which were both optimized using Connectionist Temporal Classification\n(CTC) loss. With baseline results, we were able to achieve a low word error\nrate (WER) of 0.77% using a greedy decoder on our dataset. Finally, we\nopen-source the data and code along with this publication in order to encourage\nfuture research in this direction.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:32:58 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ajisafe", "Daniel", ""], ["Adegboro", "Oluwabukola", ""], ["Oduntan", "Esther", ""], ["Arulogun", "Tayo", ""]]}, {"id": "2010.11125", "submitter": "Angela Fan", "authors": "Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\n  Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav\n  Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov,\n  Edouard Grave, Michael Auli, Armand Joulin", "title": "Beyond English-Centric Multilingual Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work in translation demonstrated the potential of massively\nmultilingual machine translation by training a single model able to translate\nbetween any pair of languages. However, much of this work is English-Centric by\ntraining only on data which was translated from or to English. While this is\nsupported by large sources of training data, it does not reflect translation\nneeds worldwide. In this work, we create a true Many-to-Many multilingual\ntranslation model that can translate directly between any pair of 100\nlanguages. We build and open source a training dataset that covers thousands of\nlanguage directions with supervised data, created through large-scale mining.\nThen, we explore how to effectively increase model capacity through a\ncombination of dense scaling and language-specific sparse parameters to create\nhigh quality models. Our focus on non-English-Centric models brings gains of\nmore than 10 BLEU when directly translating between non-English directions\nwhile performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and\nfinal M2M-100 model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:01:23 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Fan", "Angela", ""], ["Bhosale", "Shruti", ""], ["Schwenk", "Holger", ""], ["Ma", "Zhiyi", ""], ["El-Kishky", "Ahmed", ""], ["Goyal", "Siddharth", ""], ["Baines", "Mandeep", ""], ["Celebi", "Onur", ""], ["Wenzek", "Guillaume", ""], ["Chaudhary", "Vishrav", ""], ["Goyal", "Naman", ""], ["Birch", "Tom", ""], ["Liptchinsky", "Vitaliy", ""], ["Edunov", "Sergey", ""], ["Grave", "Edouard", ""], ["Auli", "Michael", ""], ["Joulin", "Armand", ""]]}, {"id": "2010.11132", "submitter": "Te I", "authors": "Daniel Li, Te I, Naveen Arivazhagan, Colin Cherry, Dirk Padfield", "title": "Sentence Boundary Augmentation For Neural Machine Translation Robustness", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) models have demonstrated strong state of the\nart performance on translation tasks where well-formed training and evaluation\ndata are provided, but they remain sensitive to inputs that include errors of\nvarious types. Specifically, in the context of long-form speech translation\nsystems, where the input transcripts come from Automatic Speech Recognition\n(ASR), the NMT models have to handle errors including phoneme substitutions,\ngrammatical structure, and sentence boundaries, all of which pose challenges to\nNMT robustness. Through in-depth error analysis, we show that sentence boundary\nsegmentation has the largest impact on quality, and we develop a simple data\naugmentation strategy to improve segmentation robustness.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:44:48 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Daniel", ""], ["I", "Te", ""], ["Arivazhagan", "Naveen", ""], ["Cherry", "Colin", ""], ["Padfield", "Dirk", ""]]}, {"id": "2010.11137", "submitter": "Yan Zeng", "authors": "Yan Zeng and Jian-Yun Nie", "title": "Multi-Domain Dialogue State Tracking based on State Graph", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of multi-domain Dialogue State Tracking (DST) with\nopen vocabulary, which aims to extract the state from the dialogue. Existing\napproaches usually concatenate previous dialogue state with dialogue history as\nthe input to a bi-directional Transformer encoder. They rely on the\nself-attention mechanism of Transformer to connect tokens in them. However,\nattention may be paid to spurious connections, leading to wrong inference. In\nthis paper, we propose to construct a dialogue state graph in which domains,\nslots and values from the previous dialogue state are connected properly.\nThrough training, the graph node and edge embeddings can encode co-occurrence\nrelations between domain-domain, slot-slot and domain-slot, reflecting the\nstrong transition paths in general dialogue. The state graph, encoded with\nrelational-GCN, is fused into the Transformer encoder. Experimental results\nshow that our approach achieves a new state of the art on the task while\nremaining efficient. It outperforms existing open-vocabulary DST approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:55:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zeng", "Yan", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2010.11140", "submitter": "Yan Zeng", "authors": "Yan Zeng and Jian-Yun Nie", "title": "A Simple and Efficient Multi-Task Learning Approach for Conditioned\n  Dialogue Generation", "comments": "Accepted as NAACL 2021 Long Paper (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conditioned dialogue generation suffers from the scarcity of labeled\nresponses. In this work, we exploit labeled non-dialogue text data related to\nthe condition, which are much easier to collect. We propose a multi-task\nlearning approach to leverage both labeled dialogue and text data. The 3 tasks\njointly optimize the same pre-trained Transformer -- conditioned dialogue\ngeneration task on the labeled dialogue data, conditioned language encoding\ntask and conditioned language generation task on the labeled text data.\nExperimental results show that our approach outperforms the state-of-the-art\nmodels by leveraging the labeled texts, and it also obtains larger improvement\nin performance comparing to the previous methods to leverage text data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:56:49 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 14:51:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zeng", "Yan", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2010.11148", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang, Tara N. Sainath,\n  Yanzhang He, Arun Narayanan, Wei Han, Anmol Gulati, Yonghui Wu, Ruoming Pang", "title": "FastEmit: Low-latency Streaming ASR with Sequence-level Emission\n  Regularization", "comments": "Accepted in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming automatic speech recognition (ASR) aims to emit each hypothesized\nword as quickly and accurately as possible. However, emitting fast without\ndegrading quality, as measured by word error rate (WER), is highly challenging.\nExisting approaches including Early and Late Penalties and Constrained\nAlignments penalize emission delay by manipulating per-token or per-frame\nprobability prediction in sequence transducer models. While being successful in\nreducing delay, these approaches suffer from significant accuracy regression\nand also require additional word alignment information from an existing model.\nIn this work, we propose a sequence-level emission regularization method, named\nFastEmit, that applies latency regularization directly on per-sequence\nprobability in training transducer models, and does not require any alignment.\nWe demonstrate that FastEmit is more suitable to the sequence-level\noptimization of transducer models for streaming ASR by applying it on various\nend-to-end streaming ASR networks including RNN-Transducer,\nTransformer-Transducer, ConvNet-Transducer and Conformer-Transducer. We achieve\n150-300 ms latency reduction with significantly better accuracy over previous\ntechniques on a Voice Search test set. FastEmit also improves streaming ASR\naccuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile\nlatency from 210 ms to only 30 ms on LibriSpeech.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:05:01 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:59:05 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yu", "Jiahui", ""], ["Chiu", "Chung-Cheng", ""], ["Li", "Bo", ""], ["Chang", "Shuo-yiin", ""], ["Sainath", "Tara N.", ""], ["He", "Yanzhang", ""], ["Narayanan", "Arun", ""], ["Han", "Wei", ""], ["Gulati", "Anmol", ""], ["Wu", "Yonghui", ""], ["Pang", "Ruoming", ""]]}, {"id": "2010.11153", "submitter": "Tsz Kin Lam", "authors": "Tsz Kin Lam, Shigehiko Schamoni, Stefan Riezler", "title": "Cascaded Models With Cyclic Feedback For Direct Speech Translation", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct speech translation describes a scenario where only speech inputs and\ncorresponding translations are available. Such data are notoriously limited. We\npresent a technique that allows cascades of automatic speech recognition (ASR)\nand machine translation (MT) to exploit in-domain direct speech translation\ndata in addition to out-of-domain MT and ASR data. After pre-training MT and\nASR, we use a feedback cycle where the downstream performance of the MT system\nis used as a signal to improve the ASR system by self-training, and the MT\ncomponent is fine-tuned on multiple ASR outputs, making it more tolerant\ntowards spelling variations. A comparison to end-to-end speech translation\nusing components of identical architecture and the same data shows gains of up\nto 3.8 BLEU points on LibriVoxDeEn and up to 5.1 BLEU points on CoVoST for\nGerman-to-English speech translation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:18:51 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 16:52:33 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Lam", "Tsz Kin", ""], ["Schamoni", "Shigehiko", ""], ["Riezler", "Stefan", ""]]}, {"id": "2010.11170", "submitter": "Ozan \\.Irsoy", "authors": "Tianze Shi, Igor Malioutov, Ozan \\.Irsoy", "title": "Semantic Role Labeling as Syntactic Dependency Parsing", "comments": "Appeared in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce the task of (span-based) PropBank-style semantic role labeling\n(SRL) to syntactic dependency parsing. Our approach is motivated by our\nempirical analysis that shows three common syntactic patterns account for over\n98% of the SRL annotations for both English and Chinese data. Based on this\nobservation, we present a conversion scheme that packs SRL annotations into\ndependency tree representations through joint labels that permit highly\naccurate recovery back to the original format. This representation allows us to\ntrain statistical dependency parsers to tackle SRL and achieve competitive\nperformance with the current state of the art. Our findings show the promise of\nsyntactic dependency trees in encoding semantic role relations within their\nsyntactic domain of locality, and point to potential further integration of\nsyntactic methods into semantic role labeling in the future.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:46:11 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Shi", "Tianze", ""], ["Malioutov", "Igor", ""], ["\u0130rsoy", "Ozan", ""]]}, {"id": "2010.11230", "submitter": "Mohammad Kachuee Mr.", "authors": "Mohammad Kachuee, Hao Yuan, Young-Bum Kim, Sungjin Lee", "title": "Self-Supervised Contrastive Learning for Efficient User Satisfaction\n  Prediction in Conversational Agents", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turn-level user satisfaction is one of the most important performance metrics\nfor conversational agents. It can be used to monitor the agent's performance\nand provide insights about defective user experiences. Moreover, a powerful\nsatisfaction model can be used as an objective function that a conversational\nagent continuously optimizes for. While end-to-end deep learning has shown\npromising results, having access to a large number of reliable annotated\nsamples required by these methods remains challenging. In a large-scale\nconversational system, there is a growing number of newly developed skills,\nmaking the traditional data collection, annotation, and modeling process\nimpractical due to the required annotation costs as well as the turnaround\ntimes. In this paper, we suggest a self-supervised contrastive learning\napproach that leverages the pool of unlabeled data to learn user-agent\ninteractions. We show that the pre-trained models using the self-supervised\nobjective are transferable to the user satisfaction prediction. In addition, we\npropose a novel few-shot transfer learning approach that ensures better\ntransferability for very small sample sizes. The suggested few-shot method does\nnot require any inner loop optimization process and is scalable to very large\ndatasets and complex models. Based on our experiments using real-world data\nfrom a large-scale commercial system, the suggested approach is able to\nsignificantly reduce the required number of annotations, while improving the\ngeneralization on unseen out-of-domain skills.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 18:10:58 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 16:44:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kachuee", "Mohammad", ""], ["Yuan", "Hao", ""], ["Kim", "Young-Bum", ""], ["Lee", "Sungjin", ""]]}, {"id": "2010.11238", "submitter": "Sirigireddy Dhana Laxmi", "authors": "Sirigireddy Dhanalaxmi, Rohit Agarwal, Aman Sinha", "title": "Detection of COVID-19 informative tweets using RoBERTa", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social media such as Twitter is a hotspot of user-generated information. In\nthis ongoing Covid-19 pandemic, there has been an abundance of data on social\nmedia which can be classified as informative and uninformative content. In this\npaper, we present our work to detect informative Covid-19 English tweets using\nRoBERTa model as a part of the W-NUT workshop 2020. We show the efficacy of our\nmodel on a public dataset with an F1-score of 0.89 on the validation dataset\nand 0.87 on the leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 18:43:13 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Dhanalaxmi", "Sirigireddy", ""], ["Agarwal", "Rohit", ""], ["Sinha", "Aman", ""]]}, {"id": "2010.11246", "submitter": "Tianze Shi", "authors": "Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daum\\'e III and Lillian\n  Lee", "title": "On the Potential of Lexico-logical Alignments for Semantic Parsing to\n  SQL Queries", "comments": "Findings of ACL: EMNLP 2020", "journal-ref": "Findings of ACL: EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale semantic parsing datasets annotated with logical forms have\nenabled major advances in supervised approaches. But can richer supervision\nhelp even more? To explore the utility of fine-grained, lexical-level\nsupervision, we introduce Squall, a dataset that enriches 11,276\nWikiTableQuestions English-language questions with manually created SQL\nequivalents plus alignments between SQL and question fragments. Our annotation\nenables new training possibilities for encoder-decoder models, including\napproaches from machine translation previously precluded by the absence of\nalignments. We propose and test two methods: (1) supervised attention; (2)\nadopting an auxiliary objective of disambiguating references in the input\nqueries to table columns. In 5-fold cross validation, these strategies improve\nover strong baselines by 4.4% execution accuracy. Oracle experiments suggest\nthat annotated alignments can support further accuracy gains of up to 23.9%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 19:01:00 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Shi", "Tianze", ""], ["Zhao", "Chen", ""], ["Boyd-Graber", "Jordan", ""], ["Daum\u00e9", "Hal", "III"], ["Lee", "Lillian", ""]]}, {"id": "2010.11247", "submitter": "Renjie Zheng", "authors": "Junkun Chen, Renjie Zheng, Atsuhito Kita, Mingbo Ma, Liang Huang", "title": "Improving Simultaneous Translation with Pseudo References", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous translation is vastly different from full-sentence translation,\nin the sense that it starts translation before the source sentence ends, with\nonly a few words delay. However, due to the lack of large scale and publicly\navailable simultaneous translation datasets, most simultaneous translation\nsystems still train with ordinary full-sentence parallel corpora which are not\nsuitable for the simultaneous scenario due to the existence of unnecessary\nlong-distance reorderings. Instead of expensive, time-consuming annotation, we\npropose a novel method that rewrites the target side of existing full-sentence\ncorpus into simultaneous-style translation. Experiments on Chinese-to-English\ntranslation demonstrate about +2.7 BLEU improvements with the addition of newly\ngenerated pseudo references.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 19:03:06 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Junkun", ""], ["Zheng", "Renjie", ""], ["Kita", "Atsuhito", ""], ["Ma", "Mingbo", ""], ["Huang", "Liang", ""]]}, {"id": "2010.11253", "submitter": "Rico Angell", "authors": "Rico Angell, Nicholas Monath, Sunil Mohan, Nishant Yadav and Andrew\n  McCallum", "title": "Clustering-based Inference for Biomedical Entity Linking", "comments": "NAACL 2021 Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to large number of entities in biomedical knowledge bases, only a small\nfraction of entities have corresponding labelled training data. This\nnecessitates entity linking models which are able to link mentions of unseen\nentities using learned representations of entities. Previous approaches link\neach mention independently, ignoring the relationships within and across\ndocuments between the entity mentions. These relations can be very useful for\nlinking mentions in biomedical text where linking decisions are often difficult\ndue mentions having a generic or a highly specialized form. In this paper, we\nintroduce a model in which linking decisions can be made not merely by linking\nto a knowledge base entity but also by grouping multiple mentions together via\nclustering and jointly making linking predictions. In experiments on the\nlargest publicly available biomedical dataset, we improve the best independent\nprediction for entity linking by 3.0 points of accuracy, and our\nclustering-based inference model further improves entity linking by 2.3 points.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 19:16:27 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 19:21:58 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Angell", "Rico", ""], ["Monath", "Nicholas", ""], ["Mohan", "Sunil", ""], ["Yadav", "Nishant", ""], ["McCallum", "Andrew", ""]]}, {"id": "2010.11304", "submitter": "Wenxuan Zhou", "authors": "Wenxuan Zhou, Kevin Huang, Tengyu Ma, Jing Huang", "title": "Document-Level Relation Extraction with Adaptive Thresholding and\n  Localized Context Pooling", "comments": "Accepted by AAAI 2021. Code available at\n  https://github.com/wzhouad/ATLOP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction (RE) poses new challenges compared to its\nsentence-level counterpart. One document commonly contains multiple entity\npairs, and one entity pair occurs multiple times in the document associated\nwith multiple possible relations. In this paper, we propose two novel\ntechniques, adaptive thresholding and localized context pooling, to solve the\nmulti-label and multi-entity problems. The adaptive thresholding replaces the\nglobal threshold for multi-label classification in the prior work with a\nlearnable entities-dependent threshold. The localized context pooling directly\ntransfers attention from pre-trained language models to locate relevant context\nthat is useful to decide the relation. We experiment on three document-level RE\nbenchmark datasets: DocRED, a recently released large-scale RE dataset, and two\ndatasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding\nand Localized cOntext Pooling) model achieves an F1 score of 63.4, and also\nsignificantly outperforms existing models on both CDR and GDA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 20:41:23 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:18:24 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 02:25:49 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zhou", "Wenxuan", ""], ["Huang", "Kevin", ""], ["Ma", "Tengyu", ""], ["Huang", "Jing", ""]]}, {"id": "2010.11322", "submitter": "Jonathan Pilault", "authors": "Jaehong Park, Jonathan Pilault and Christopher Pal", "title": "Learning to Summarize Long Texts with Memory Compression and Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent\nneural network based encoder decoder architectures and we explore its use for\nabstractive document summarization. Mem2Mem transfers \"memories\" via\nreadable/writable external memory modules that augment both the encoder and\ndecoder. Our memory regularization compresses an encoded input article into a\nmore compact set of sentence representations. Most importantly, the memory\ncompression step performs implicit extraction without labels, sidestepping\nissues with suboptimal ground-truth data and exposure bias of hybrid\nextractive-abstractive summarization techniques. By allowing the decoder to\nread/write over the encoded input memory, the model learns to read salient\ninformation about the input article while keeping track of what has been\ngenerated. Our Mem2Mem approach yields results that are competitive with state\nof the art transformer based summarization methods, but with 16 times fewer\nparameters\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 21:45:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Park", "Jaehong", ""], ["Pilault", "Jonathan", ""], ["Pal", "Christopher", ""]]}, {"id": "2010.11325", "submitter": "Rui Feng", "authors": "Rui Feng, Jie Yuan, Chao Zhang", "title": "Probing and Fine-tuning Reading Comprehension Models for Few-shot Event\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of event extraction from text data, which requires both\ndetecting target event types and their arguments. Typically, both the event\ndetection and argument detection subtasks are formulated as supervised sequence\nlabeling problems. We argue that the event extraction models so trained are\ninherently label-hungry, and can generalize poorly across domains and text\ngenres.We propose a reading comprehension framework for event\nextraction.Specifically, we formulate event detection as a textual entailment\nprediction problem, and argument detection as a question answer-ing problem. By\nconstructing proper query templates, our approach can effectively distill rich\nknowledge about tasks and label semantics from pretrained reading comprehension\nmodels. Moreover, our model can be fine-tuned with a small amount of data to\nboost its performance. Our experiment results show that our method performs\nstrongly for zero-shot and few-shot event extraction, and it achieves\nstate-of-the-art performance on the ACE 2005 benchmark when trained with full\nsupervision.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 21:48:39 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Feng", "Rui", ""], ["Yuan", "Jie", ""], ["Zhang", "Chao", ""]]}, {"id": "2010.11333", "submitter": "Yogarshi Vyas", "authors": "Yogarshi Vyas, Miguel Ballesteros", "title": "Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In entity linking, mentions of named entities in raw text are disambiguated\nagainst a knowledge base (KB). This work focuses on linking to unseen KBs that\ndo not have training data and whose schema is unknown during training. Our\napproach relies on methods to flexibly convert entities from arbitrary KBs with\nseveral attribute-value pairs into flat strings, which we use in conjunction\nwith state-of-the-art models for zero-shot linking. To improve the\ngeneralization of our model, we use two regularization schemes based on\nshuffling of entity attributes and handling of unseen attributes. Experiments\non English datasets where models are trained on the CoNLL dataset, and tested\non the TAC-KBP 2010 dataset show that our models outperform baseline models by\nover 12 points of accuracy. Unlike prior work, our approach also allows for\nseamlessly combining multiple training datasets. We test this ability by adding\nboth a completely different dataset (Wikia), as well as increasing amount of\ntraining data from the TAC-KBP 2010 training set. Our models perform favorably\nacross the board.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:07:31 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Vyas", "Yogarshi", ""], ["Ballesteros", "Miguel", ""]]}, {"id": "2010.11334", "submitter": "Chiyu Zhang", "authors": "Muhammad Abdul-Mageed, Chiyu Zhang, Houda Bouamor and Nizar Habash", "title": "NADI 2020: The First Nuanced Arabic Dialect Identification Shared Task", "comments": "Accepted in The Fifth Arabic Natural Language Processing Workshop\n  (WANLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the results and findings of the First Nuanced Arabic Dialect\nIdentification Shared Task (NADI). This Shared Task includes two subtasks:\ncountry-level dialect identification (Subtask 1) and province-level sub-dialect\nidentification (Subtask 2). The data for the shared task covers a total of 100\nprovinces from 21 Arab countries and are collected from the Twitter domain. As\nsuch, NADI is the first shared task to target naturally-occurring fine-grained\ndialectal text at the sub-country level. A total of 61 teams from 25 countries\nregistered to participate in the tasks, thus reflecting the interest of the\ncommunity in this area. We received 47 submissions for Subtask 1 from 18 teams\nand 9 submissions for Subtask 2 from 9 teams.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:14:28 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 04:53:19 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 19:18:33 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Zhang", "Chiyu", ""], ["Bouamor", "Houda", ""], ["Habash", "Nizar", ""]]}, {"id": "2010.11338", "submitter": "Yun Tang", "authors": "Yun Tang, Juan Pino, Changhan Wang, Xutai Ma, Dmitriy Genzel", "title": "A General Multi-Task Learning Framework to Leverage Text Data for Speech\n  to Text Tasks", "comments": "Accepted for ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based sequence-to-sequence modeling provides a powerful and elegant\nsolution for applications that need to map one sequence to a different\nsequence. Its success heavily relies on the availability of large amounts of\ntraining data. This presents a challenge for speech applications where labelled\nspeech data is very expensive to obtain, such as automatic speech recognition\n(ASR) and speech translation (ST). In this study, we propose a general\nmulti-task learning framework to leverage text data for ASR and ST tasks. Two\nauxiliary tasks, a denoising autoencoder task and machine translation task, are\nproposed to be co-trained with ASR and ST tasks respectively. We demonstrate\nthat representing text input as phoneme sequences can reduce the difference\nbetween speech and text inputs, and enhance the knowledge transfer from text\ncorpora to the speech to text tasks. Our experiments show that the proposed\nmethod achieves a relative 10~15% word error rate reduction on the English\nLibrispeech task compared with our baseline, and improves the speech\ntranslation quality on the MuST-C tasks by 3.6~9.2 BLEU.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:40:43 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:08:25 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Tang", "Yun", ""], ["Pino", "Juan", ""], ["Wang", "Changhan", ""], ["Ma", "Xutai", ""], ["Genzel", "Dmitriy", ""]]}, {"id": "2010.11349", "submitter": "Xie Chen", "authors": "Xie Chen, Sarangarajan Parthasarathy, William Gale, Shuangyu Chang,\n  Michael Zeng", "title": "LSTM-LM with Long-Term History for First-Pass Decoding in Conversational\n  Speech Recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTM language models (LSTM-LMs) have been proven to be powerful and yielded\nsignificant performance improvements over count based n-gram LMs in modern\nspeech recognition systems. Due to its infinite history states and\ncomputational load, most previous studies focus on applying LSTM-LMs in the\nsecond-pass for rescoring purpose. Recent work shows that it is feasible and\ncomputationally affordable to adopt the LSTM-LMs in the first-pass decoding\nwithin a dynamic (or tree based) decoder framework. In this work, the LSTM-LM\nis composed with a WFST decoder on-the-fly for the first-pass decoding.\nFurthermore, motivated by the long-term history nature of LSTM-LMs, the use of\ncontext beyond the current utterance is explored for the first-pass decoding in\nconversational speech recognition. The context information is captured by the\nhidden states of LSTM-LMs across utterance and can be used to guide the\nfirst-pass search effectively. The experimental results in our internal meeting\ntranscription system show that significant performance improvements can be\nobtained by incorporating the contextual information with LSTM-LMs in the\nfirst-pass decoding, compared to applying the contextual information in the\nsecond-pass rescoring.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 23:40:26 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Xie", ""], ["Parthasarathy", "Sarangarajan", ""], ["Gale", "William", ""], ["Chang", "Shuangyu", ""], ["Zeng", "Michael", ""]]}, {"id": "2010.11351", "submitter": "Minghan Li", "authors": "M. Li, H. Bai, L. Tan, K. Xiong, M. Li, J. Lin", "title": "Latte-Mix: Measuring Sentence Semantic Similarity with Latent\n  Categorical Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring sentence semantic similarity using pre-trained language models such\nas BERT generally yields unsatisfactory zero-shot performance, and one main\nreason is ineffective token aggregation methods such as mean pooling. In this\npaper, we demonstrate under a Bayesian framework that distance between\nprimitive statistics such as the mean of word embeddings are fundamentally\nflawed for capturing sentence-level semantic similarity. To remedy this issue,\nwe propose to learn a categorical variational autoencoder (VAE) based on\noff-the-shelf pre-trained language models. We theoretically prove that\nmeasuring the distance between the latent categorical mixtures, namely\nLatte-Mix, can better reflect the true sentence semantic similarity. In\naddition, our Bayesian framework provides explanations for why models finetuned\non labelled sentence pairs have better zero-shot performance. We also\nempirically demonstrate that these finetuned models could be further improved\nby Latte-Mix. Our method not only yields the state-of-the-art zero-shot\nperformance on semantic similarity datasets such as STS, but also enjoy the\nbenefits of fast training and having small memory footprints.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 23:45:18 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Li", "M.", ""], ["Bai", "H.", ""], ["Tan", "L.", ""], ["Xiong", "K.", ""], ["Li", "M.", ""], ["Lin", "J.", ""]]}, {"id": "2010.11358", "submitter": "Aaron Baier-Reinio", "authors": "Aaron Baier-Reinio and Hans De Sterck", "title": "N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using\n  Neural Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use neural ordinary differential equations to formulate a variant of the\nTransformer that is depth-adaptive in the sense that an input-dependent number\nof time steps is taken by the ordinary differential equation solver. Our goal\nin proposing the N-ODE Transformer is to investigate whether its\ndepth-adaptivity may aid in overcoming some specific known theoretical\nlimitations of the Transformer in handling nonlocal effects. Specifically, we\nconsider the simple problem of determining the parity of a binary sequence, for\nwhich the standard Transformer has known limitations that can only be overcome\nby using a sufficiently large number of layers or attention heads. We find,\nhowever, that the depth-adaptivity of the N-ODE Transformer does not provide a\nremedy for the inherently nonlocal nature of the parity problem, and provide\nexplanations for why this is so. Next, we pursue regularization of the N-ODE\nTransformer by penalizing the arclength of the ODE trajectories, but find that\nthis fails to improve the accuracy or efficiency of the N-ODE Transformer on\nthe challenging parity problem. We suggest future avenues of research for\nmodifications and extensions of the N-ODE Transformer that may lead to improved\naccuracy and efficiency for sequence modelling tasks such as neural machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 00:48:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Baier-Reinio", "Aaron", ""], ["De Sterck", "Hans", ""]]}, {"id": "2010.11362", "submitter": "Rithesh Kumar", "authors": "Rithesh Kumar, Kundan Kumar, Vicki Anand, Yoshua Bengio, Aaron\n  Courville", "title": "NU-GAN: High resolution neural upsampling with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose NU-GAN, a new method for resampling audio from\nlower to higher sampling rates (upsampling). Audio upsampling is an important\nproblem since productionizing generative speech technology requires operating\nat high sampling rates. Such applications use audio at a resolution of 44.1 kHz\nor 48 kHz, whereas current speech synthesis methods are equipped to handle a\nmaximum of 24 kHz resolution. NU-GAN takes a leap towards solving audio\nupsampling as a separate component in the text-to-speech (TTS) pipeline by\nleveraging techniques for audio generation using GANs. ABX preference tests\nindicate that our NU-GAN resampler is capable of resampling 22 kHz to 44.1 kHz\naudio that is distinguishable from original audio only 7.4% higher than random\nchance for single speaker dataset, and 10.8% higher than chance for\nmulti-speaker dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:00:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kumar", "Rithesh", ""], ["Kumar", "Kundan", ""], ["Anand", "Vicki", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "2010.11374", "submitter": "Devendra Singh Sachan", "authors": "Devendra Singh Sachan and Lingfei Wu and Mrinmaya Sachan and William\n  Hamilton", "title": "Stronger Transformers for Neural Multi-Hop Question Generation", "comments": "Code will be made available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on automated question generation has almost exclusively focused on\ngenerating simple questions whose answers can be extracted from a single\ndocument. However, there is an increasing interest in developing systems that\nare capable of more complex multi-hop question generation, where answering the\nquestions requires reasoning over multiple documents. In this work, we\nintroduce a series of strong transformer models for multi-hop question\ngeneration, including a graph-augmented transformer that leverages relations\nbetween entities in the text. While prior work has emphasized the importance of\ngraph-based models, we show that we can substantially outperform the\nstate-of-the-art by 5 BLEU points using a standard transformer architecture. We\nfurther demonstrate that graph-based augmentations can provide complimentary\nimprovements on top of this foundation. Interestingly, we find that several\nimportant factors--such as the inclusion of an auxiliary contrastive objective\nand data filtering could have larger impacts on performance. We hope that our\nstronger baselines and analysis provide a constructive foundation for future\nwork in this area.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:51:09 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sachan", "Devendra Singh", ""], ["Wu", "Lingfei", ""], ["Sachan", "Mrinmaya", ""], ["Hamilton", "William", ""]]}, {"id": "2010.11383", "submitter": "Li Wanli", "authors": "Wanli Li and Tieyun Qian", "title": "Exploit Multiple Reference Graphs for Semi-supervised Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual annotation of the labeled data for relation extraction is\ntime-consuming and labor-intensive. Semi-supervised methods can offer helping\nhands for this problem and have aroused great research interests. Existing work\nfocuses on mapping the unlabeled samples to the classes to augment the labeled\ndataset. However, it is hard to find an overall good mapping function,\nespecially for the samples with complicated syntactic components in one\nsentence.\n  To tackle this limitation, we propose to build the connection between the\nunlabeled data and the labeled ones rather than directly mapping the unlabeled\nsamples to the classes. Specifically, we first use three kinds of information\nto construct reference graphs, including entity reference, verb reference, and\nsemantics reference. The goal is to semantically or lexically connect the\nunlabeled sample(s) to the labeled one(s). Then, we develop a Multiple\nReference Graph (MRefG) model to exploit the reference information for better\nrecognizing high-quality unlabeled samples. The effectiveness of our method is\ndemonstrated by extensive comparison experiments with the state-of-the-art\nbaselines on two public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:14:27 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Li", "Wanli", ""], ["Qian", "Tieyun", ""]]}, {"id": "2010.11384", "submitter": "Gabriele Pergola", "authors": "Gabriele Pergola, Lin Gui, Yulan He", "title": "A Disentangled Adversarial Neural Topic Model for Separating Opinions\n  from Plots in User Reviews", "comments": "Proceedings of the 2021 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies,\n  NAACL 2021", "journal-ref": null, "doi": "10.18653/v1/2021.naacl-main.228", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexibility of the inference process in Variational Autoencoders (VAEs)\nhas recently led to revising traditional probabilistic topic models giving rise\nto Neural Topic Models (NTMs). Although these approaches have achieved\nsignificant results, surprisingly very little work has been done on how to\ndisentangle the latent topics. Existing topic models when applied to reviews\nmay extract topics associated with writers' subjective opinions mixed with\nthose related to factual descriptions such as plot summaries in movie and book\nreviews. It is thus desirable to automatically separate opinion topics from\nplot/neutral ones enabling a better interpretability. In this paper, we propose\na neural topic model combined with adversarial training to disentangle opinion\ntopics from plot and neutral ones. We conduct an extensive experimental\nassessment introducing a new collection of movie and book reviews paired with\ntheir plots, namely MOBO dataset, showing an improved coherence and variety of\ntopics, a consistent disentanglement rate, and sentiment classification\nperformance superior to other supervised topic models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:15:13 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 14:34:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pergola", "Gabriele", ""], ["Gui", "Lin", ""], ["He", "Yulan", ""]]}, {"id": "2010.11386", "submitter": "Jheng-Hong Yang", "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin", "title": "Distilling Dense Representations for Ranking using Tightly-Coupled\n  Teachers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to ranking with dense representations that applies\nknowledge distillation to improve the recently proposed late-interaction\nColBERT model. Specifically, we distill the knowledge from ColBERT's expressive\nMaxSim operator for computing relevance scores into a simple dot product, thus\nenabling single-step ANN search. Our key insight is that during distillation,\ntight coupling between the teacher model and the student model enables more\nflexible distillation strategies and yields better learned representations. We\nempirically show that our approach improves query latency and greatly reduces\nthe onerous storage requirements of ColBERT, while only making modest\nsacrifices in terms of effectiveness. By combining our dense representations\nwith sparse representations derived from document expansion, we are able to\napproach the effectiveness of a standard cross-encoder reranker using BERT that\nis orders of magnitude slower.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:26:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Lin", "Jimmy", ""]]}, {"id": "2010.11387", "submitter": "George Boateng", "authors": "George Boateng", "title": "Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses", "comments": "6 pages. Accepted and presented at NeurIPS 2020 workshop (Black in\n  AI) and AIED 2021 (international conference on AI in Education)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Introductory hands-on courses such as our smartphone-based coding course,\nSuaCode require a lot of support for students to accomplish learning goals.\nOnline environments make it even more difficult to get assistance especially\nmore recently because of COVID-19. Given the multilingual context of SuaCode\nstudents - learners across 42 African countries that are mostly Anglophone or\nFrancophone - in this work, we developed a bilingual Artificial Intelligence\n(AI) Teaching Assistant (TA) - Kwame - that provides answers to students'\ncoding questions from SuaCode courses in English and French. Kwame is a\nSentence-BERT (SBERT)-based question-answering (QA) system that we trained and\nevaluated offline using question-answer pairs created from the course's\nquizzes, lesson notes and students' questions in past cohorts. Kwame finds the\nparagraph most semantically similar to the question via cosine similarity. We\ncompared the system with TF-IDF and Universal Sentence Encoder. Our results\nshowed that fine-tuning on the course data and returning the top 3 and 5\nanswers improved the accuracy results. Kwame will make it easy for students to\nget quick and accurate answers to questions in SuaCode courses.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:26:12 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 00:12:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Boateng", "George", ""]]}, {"id": "2010.11395", "submitter": "Xie Chen", "authors": "Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, Jinyu Li", "title": "Developing Real-time Streaming Transformer Transducer for Speech\n  Recognition on Large-scale Dataset", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Transformer based end-to-end models have achieved great success in\nmany areas including speech recognition. However, compared to LSTM models, the\nheavy computational cost of the Transformer during inference is a key issue to\nprevent their applications. In this work, we explored the potential of\nTransformer Transducer (T-T) models for the fist pass decoding with low latency\nand fast speed on a large-scale dataset. We combine the idea of Transformer-XL\nand chunk-wise streaming processing to design a streamable Transformer\nTransducer model. We demonstrate that T-T outperforms the hybrid model, RNN\nTransducer (RNN-T), and streamable Transformer attention-based encoder-decoder\nmodel in the streaming scenario. Furthermore, the runtime cost and latency can\nbe optimized with a relatively small look-ahead.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:01:21 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:57:02 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 08:02:29 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Xie", ""], ["Wu", "Yu", ""], ["Wang", "Zhenghao", ""], ["Liu", "Shujie", ""], ["Li", "Jinyu", ""]]}, {"id": "2010.11428", "submitter": "Qiujia Li", "authors": "Qiujia Li, David Qiu, Yu Zhang, Bo Li, Yanzhang He, Philip C.\n  Woodland, Liangliang Cao, Trevor Strohman", "title": "Confidence Estimation for Attention-based Sequence-to-sequence Models\n  for Speech Recognition", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For various speech-related tasks, confidence scores from a speech recogniser\nare a useful measure to assess the quality of transcriptions. In traditional\nhidden Markov model-based automatic speech recognition (ASR) systems,\nconfidence scores can be reliably obtained from word posteriors in decoding\nlattices. However, for an ASR system with an auto-regressive decoder, such as\nan attention-based sequence-to-sequence model, computing word posteriors is\ndifficult. An obvious alternative is to use the decoder softmax probability as\nthe model confidence. In this paper, we first examine how some commonly used\nregularisation methods influence the softmax-based confidence scores and study\nthe overconfident behaviour of end-to-end models. Then we propose a lightweight\nand effective approach named confidence estimation module (CEM) on top of an\nexisting end-to-end ASR model. Experiments on LibriSpeech show that CEM can\nmitigate the overconfidence problem and can produce more reliable confidence\nscores with and without shallow fusion of a language model. Further analysis\nshows that CEM generalises well to speech from a moderately mismatched domain\nand can potentially improve downstream tasks such as semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 04:02:27 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 18:49:07 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Qiujia", ""], ["Qiu", "David", ""], ["Zhang", "Yu", ""], ["Li", "Bo", ""], ["He", "Yanzhang", ""], ["Woodland", "Philip C.", ""], ["Cao", "Liangliang", ""], ["Strohman", "Trevor", ""]]}, {"id": "2010.11445", "submitter": "Mingbo Ma", "authors": "Junkun Chen, Mingbo Ma, Renjie Zheng, Liang Huang", "title": "MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end Speech-to-text Translation (E2E-ST), which directly translates\nsource language speech to target language text, is widely useful in practice,\nbut traditional cascaded approaches (ASR+MT) often suffer from error\npropagation in the pipeline. On the other hand, existing end-to-end solutions\nheavily depend on the source language transcriptions for pre-training or\nmulti-task training with Automatic Speech Recognition (ASR). We instead propose\na simple technique to learn a robust speech encoder in a self-supervised\nfashion only on the speech side, which can utilize speech data without\ntranscription. This technique termed Masked Acoustic Modeling (MAM), not only\nprovides an alternative solution to improving E2E-ST, but also can perform\npre-training on any acoustic signals (including non-speech ones) without\nannotation. We conduct our experiments over 8 different translation directions.\nIn the setting without using any transcriptions, our technique achieves an\naverage improvement of +1.1 BLEU, and +2.3 BLEU with MAM pre-training.\nPre-training of MAM with arbitrary acoustic signals also has an average\nimprovement with +1.6 BLEU for those languages. Compared with ASR multi-task\nlearning solution, which replies on transcription during training, our\npre-trained MAM model, which does not use transcription, achieves similar\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 05:02:06 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 20:36:39 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Chen", "Junkun", ""], ["Ma", "Mingbo", ""], ["Zheng", "Renjie", ""], ["Huang", "Liang", ""]]}, {"id": "2010.11478", "submitter": "Minho Ryu", "authors": "Minho Ryu and Kichun Lee", "title": "Knowledge Distillation for BERT Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pre-trained language model, BERT, has brought significant performance\nimprovements across a range of natural language processing tasks. Since the\nmodel is trained on a large corpus of diverse topics, it shows robust\nperformance for domain shift problems in which data distributions at training\n(source data) and testing (target data) differ while sharing similarities.\nDespite its great improvements compared to previous models, it still suffers\nfrom performance degradation due to domain shifts. To mitigate such problems,\nwe propose a simple but effective unsupervised domain adaptation method,\nadversarial adaptation with distillation (AAD), which combines the adversarial\ndiscriminative domain adaptation (ADDA) framework with knowledge distillation.\nWe evaluate our approach in the task of cross-domain sentiment classification\non 30 domain pairs, advancing the state-of-the-art performance for unsupervised\ndomain adaptation in text sentiment classification.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:51:24 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 02:12:06 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ryu", "Minho", ""], ["Lee", "Kichun", ""]]}, {"id": "2010.11481", "submitter": "Yu-An Chung", "authors": "Yu-An Chung and Yonatan Belinkov and James Glass", "title": "Similarity Analysis of Self-Supervised Speech Representations", "comments": "Accepted to ICASSP 2021. Supplementary materials available at\n  https://github.com/iamyuanchung/ICASSP21-Similarity-Supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised speech representation learning has recently been a prosperous\nresearch topic. Many algorithms have been proposed for learning useful\nrepresentations from large-scale unlabeled data, and their applications to a\nwide range of speech tasks have also been investigated. However, there has been\nlittle research focusing on understanding the properties of existing\napproaches. In this work, we aim to provide a comparative study of some of the\nmost representative self-supervised algorithms. Specifically, we quantify the\nsimilarities between different self-supervised representations using existing\nsimilarity measures. We also design probing tasks to study the correlation\nbetween the models' pre-training loss and the amount of specific speech\ninformation contained in their learned representations. In addition to showing\nhow various self-supervised models behave differently given the same input, our\nstudy also finds that the training objective has a higher impact on\nrepresentation similarity than architectural choices such as building blocks\n(RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also\nsuggest that there exists a strong correlation between pre-training loss and\ndownstream performance for some self-supervised algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:02:21 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 14:42:51 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chung", "Yu-An", ""], ["Belinkov", "Yonatan", ""], ["Glass", "James", ""]]}, {"id": "2010.11490", "submitter": "Christophe Cerisara", "authors": "Christophe Cerisara (SYNALP), Pavel Kral, Ladislav Lenc", "title": "On the Effects of Using word2vec Representations in Neural Networks for\n  Dialogue Act Recognition", "comments": null, "journal-ref": "Computer Speech and Language, Elsevier, 2018, 47, pp.175 - 193", "doi": "10.1016/j.csl.2017.07.009", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue act recognition is an important component of a large number of\nnatural language processing pipelines. Many research works have been carried\nout in this area, but relatively few investigate deep neural networks and word\nembeddings. This is surprising, given that both of these techniques have proven\nexceptionally good in most other language-related domains. We propose in this\nwork a new deep neural network that explores recurrent models to capture word\nsequences within sentences, and further study the impact of pretrained word\nembeddings. We validate this model on three languages: English, French and\nCzech. The performance of the proposed approach is consistent across these\nlanguages and it is comparable to the state-of-the-art results in English. More\nimportantly, we confirm that deep neural networks indeed outperform a Maximum\nEntropy classifier, which was expected. However , and this is more surprising,\nwe also found that standard word2vec em-beddings do not seem to bring valuable\ninformation for this task and the proposed model, whatever the size of the\ntraining corpus is. We thus further analyse the resulting embeddings and\nconclude that a possible explanation may be related to the mismatch between the\ntype of lexical-semantic information captured by the word2vec embeddings, and\nthe kind of relations between words that is the most useful for the dialogue\nact recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:21:17 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Cerisara", "Christophe", "", "SYNALP"], ["Kral", "Pavel", ""], ["Lenc", "Ladislav", ""]]}, {"id": "2010.11506", "submitter": "Lingkai Kong", "authors": "Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, Chao\n  Zhang", "title": "Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution\n  Data", "comments": "EMNLP2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuned pre-trained language models can suffer from severe miscalibration\nfor both in-distribution and out-of-distribution (OOD) data due to\nover-parameterization. To mitigate this issue, we propose a regularized\nfine-tuning method. Our method introduces two types of regularization for\nbetter calibration: (1) On-manifold regularization, which generates pseudo\non-manifold samples through interpolation within the data manifold. Augmented\ntraining with these pseudo samples imposes a smoothness regularization to\nimprove in-distribution calibration. (2) Off-manifold regularization, which\nencourages the model to output uniform distributions for pseudo off-manifold\nsamples to address the over-confidence issue for OOD data. Our experiments\ndemonstrate that the proposed method outperforms existing calibration methods\nfor text classification in terms of expectation calibration error,\nmisclassification detection, and OOD detection on six datasets. Our code can be\nfound at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:48:38 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kong", "Lingkai", ""], ["Jiang", "Haoming", ""], ["Zhuang", "Yuchen", ""], ["Lyu", "Jie", ""], ["Zhao", "Tuo", ""], ["Zhang", "Chao", ""]]}, {"id": "2010.11522", "submitter": "Jiaoyan Chen", "authors": "Ziheng Zhang and Jiaoyan Chen and Xi Chen and Hualuo Liu and Yuejia\n  Xiang and Bo Liu and Yefeng Zheng", "title": "An Industry Evaluation of Embedding-based Entity Alignment", "comments": "accepted by COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding-based entity alignment has been widely investigated in recent\nyears, but most proposed methods still rely on an ideal supervised learning\nsetting with a large number of unbiased seed mappings for training and\nvalidation, which significantly limits their usage. In this study, we evaluate\nthose state-of-the-art methods in an industrial context, where the impact of\nseed mappings with different sizes and different biases is explored. Besides\nthe popular benchmarks from DBpedia and Wikidata, we contribute and evaluate a\nnew industrial benchmark that is extracted from two heterogeneous knowledge\ngraphs (KGs) under deployment for medical applications. The experimental\nresults enable the analysis of the advantages and disadvantages of these\nalignment methods and the further discussion of suitable strategies for their\nindustrial deployment.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:33:58 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 12:25:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhang", "Ziheng", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Xi", ""], ["Liu", "Hualuo", ""], ["Xiang", "Yuejia", ""], ["Liu", "Bo", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2010.11524", "submitter": "Tatiana Likhomanenko", "authors": "Tatiana Likhomanenko, Qiantong Xu, Jacob Kahn, Gabriel Synnaeve, Ronan\n  Collobert", "title": "SlimIPL: Language-Model-Free Iterative Pseudo-Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:36:33 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 20:42:24 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 05:08:56 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 08:11:52 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Likhomanenko", "Tatiana", ""], ["Xu", "Qiantong", ""], ["Kahn", "Jacob", ""], ["Synnaeve", "Gabriel", ""], ["Collobert", "Ronan", ""]]}, {"id": "2010.11539", "submitter": "Changzhen Ji", "authors": "Changzhen Ji, Xin Zhou, Yating Zhang, Xiaozhong Liu, Changlong Sun,\n  Conghui Zhu and Tiejun Zhao", "title": "Cross Copy Network for Dialogue Generation", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, audiences from different fields witness the\nachievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer\nGenerator Networks, and Transformer) to enhance dialogue content generation.\nWhile content fluency and accuracy often serve as the major indicators for\nmodel training, dialogue logics, carrying critical information for some\nparticular domains, are often ignored. Take customer service and court debate\ndialogue as examples, compatible logics can be observed across different\ndialogue instances, and this information can provide vital evidence for\nutterance generation. In this paper, we propose a novel network architecture -\nCross Copy Networks(CCN) to explore the current dialog context and similar\ndialogue instances' logical structure simultaneously. Experiments with two\ntasks, court debate and customer service content generation, proved that the\nproposed algorithm is superior to existing state-of-art content generation\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:03:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ji", "Changzhen", ""], ["Zhou", "Xin", ""], ["Zhang", "Yating", ""], ["Liu", "Xiaozhong", ""], ["Sun", "Changlong", ""], ["Zhu", "Conghui", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2010.11543", "submitter": "Jee-Weon Jung", "authors": "Jee-weon Jung, Hee-Soo Heo, Ha-Jin Yu, Joon Son Chung", "title": "Graph Attention Networks for Speaker Verification", "comments": "5 pages, 1 figure, 2 tables, accepted for presentation at ICASSP 2021\n  as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel back-end framework for speaker verification using\ngraph attention networks. Segment-wise speaker embeddings extracted from\nmultiple crops within an utterance are interpreted as node representations of a\ngraph. The proposed framework inputs segment-wise speaker embeddings from an\nenrollment and a test utterance and directly outputs a similarity score. We\nfirst construct a graph using segment-wise speaker embeddings and then input\nthese to graph attention networks. After a few graph attention layers with\nresidual connections, each node is projected into a one-dimensional space using\naffine transform, followed by a readout operation resulting in a scalar\nsimilarity score. To enable successful adaptation for speaker verification, we\npropose techniques such as separating trainable weights for attention map\ncalculations between segment-wise speaker embeddings from different utterances.\nThe effectiveness of the proposed framework is validated using three different\nspeaker embedding extractors trained with different architectures and objective\nfunctions. Experimental results demonstrate consistent improvement over various\nbaseline back-end classifiers, with an average equal error rate improvement of\n20% over the cosine similarity back-end without test time augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:08:02 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 08:12:17 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jung", "Jee-weon", ""], ["Heo", "Hee-Soo", ""], ["Yu", "Ha-Jin", ""], ["Chung", "Joon Son", ""]]}, {"id": "2010.11548", "submitter": "Artem Kramov", "authors": "S.D. Pogorilyy, A.A. Kramov", "title": "Method of noun phrase detection in Ukrainian texts", "comments": "25 pages, in Ukrainian, 5 figures, 2 tables", "journal-ref": "Control Systems and Computers. 2019. Issue 5. P. 48-59", "doi": "10.15407/csc.2019.05.048", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction. The area of natural language processing considers AI-complete\ntasks that cannot be solved using traditional algorithmic actions. Such tasks\nare commonly implemented with the usage of machine learning methodology and\nmeans of computer linguistics. One of the preprocessing tasks of a text is the\nsearch of noun phrases. The accuracy of this task has implications for the\neffectiveness of many other tasks in the area of natural language processing.\nIn spite of the active development of research in the area of natural language\nprocessing, the investigation of the search for noun phrases within Ukrainian\ntexts are still at an early stage. Results. The different methods of noun\nphrases detection have been analyzed. The expediency of the representation of\nsentences as a tree structure has been justified. The key disadvantage of many\nmethods of noun phrase detection is the severe dependence of the effectiveness\nof their detection from the features of a certain language. Taking into account\nthe unified format of sentence processing and the availability of the trained\nmodel for the building of sentence trees for Ukrainian texts, the Universal\nDependency model has been chosen. The complex method of noun phrases detection\nin Ukrainian texts utilizing Universal Dependencies means and named-entity\nrecognition model has been suggested. Experimental verification of the\neffectiveness of the suggested method on the corpus of Ukrainian news has been\nperformed. Different metrics of method accuracy have been calculated.\nConclusions. The results obtained can indicate that the suggested method can be\nused to find noun phrases in Ukrainian texts. An accuracy increase of the\nmethod can be made with the usage of appropriate named-entity recognition\nmodels according to a subject area.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:20:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Pogorilyy", "S. D.", ""], ["Kramov", "A. A.", ""]]}, {"id": "2010.11553", "submitter": "Hrituraj Singh", "authors": "Hrituraj Singh, Gaurav Verma, Balaji Vasan Srinivasan", "title": "Incorporating Stylistic Lexical Preferences in Generative Language\n  Models", "comments": "To Appear in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent advances in language modeling have resulted in powerful\ngeneration models, their generation style remains implicitly dependent on the\ntraining data and can not emulate a specific target style. Leveraging the\ngenerative capabilities of a transformer-based language models, we present an\napproach to induce certain target-author attributes by incorporating continuous\nmulti-dimensional lexical preferences of an author into generative language\nmodels. We introduce rewarding strategies in a reinforcement learning framework\nthat encourages the use of words across multiple categorical dimensions, to\nvarying extents. Our experiments demonstrate that the proposed approach can\ngenerate text that distinctively aligns with a given target author's lexical\nstyle. We conduct quantitative and qualitative comparisons with competitive and\nrelevant baselines to illustrate the benefits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:24:05 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Singh", "Hrituraj", ""], ["Verma", "Gaurav", ""], ["Srinivasan", "Balaji Vasan", ""]]}, {"id": "2010.11562", "submitter": "Amit Gajbhiye", "authors": "Amit Gajbhiye, Thomas Winterbottom, Noura Al Moubayed, and Steven\n  Bradley", "title": "Bilinear Fusion of Commonsense Knowledge with Attention-Based NLI Models", "comments": "Published in Lecture Notes in Computer Science, Springer\n  International Publishing", "journal-ref": null, "doi": "10.1007/978-3-030-61609-0_50", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of incorporating real-world commonsense knowledge into\ndeep Natural Language Inference (NLI) models. Existing external knowledge\nincorporation methods are limited to lexical level knowledge and lack\ngeneralization across NLI models, datasets, and commonsense knowledge sources.\nTo address these issues, we propose a novel NLI model-independent neural\nframework, BiCAM. BiCAM incorporates real-world commonsense knowledge into NLI\nmodels. Combined with convolutional feature detectors and bilinear feature\nfusion, BiCAM provides a conceptually simple mechanism that generalizes well.\nQuantitative evaluations with two state-of-the-art NLI baselines on SNLI and\nSciTail datasets in conjunction with ConceptNet and Aristo Tuple KGs show that\nBiCAM considerably improves the accuracy the incorporated NLI baselines. For\nexample, our BiECAM model, an instance of BiCAM, on the challenging SciTail\ndataset, improves the accuracy of incorporated baselines by 7.0% with\nConceptNet, and 8.0% with Aristo Tuple KG.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:38:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Gajbhiye", "Amit", ""], ["Winterbottom", "Thomas", ""], ["Moubayed", "Noura Al", ""], ["Bradley", "Steven", ""]]}, {"id": "2010.11574", "submitter": "Jan Christian Blaise Cruz", "authors": "Jan Christian Blaise Cruz, Jose Kristian Resabal, James Lin, Dan John\n  Velasco and Charibeth Cheng", "title": "Exploiting News Article Structure for Automatic Corpus Generation", "comments": "Formerly titled \"Investigating the True Performance of Transformers\n  in Low-Resource Languages: A Case Study in Automatic Corpus Creation.\" Code\n  and data available at\n  https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 10:09:10 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 06:27:07 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cruz", "Jan Christian Blaise", ""], ["Resabal", "Jose Kristian", ""], ["Lin", "James", ""], ["Velasco", "Dan John", ""], ["Cheng", "Charibeth", ""]]}, {"id": "2010.11578", "submitter": "Navita Goyal", "authors": "Navita Goyal, Balaji Vasan Srinivasan, Anandhavelu Natarajan,\n  Abhilasha Sancheti", "title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": "Proceedings of the 2021 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies,\n  pages 3500\u00e2\u0080\u00933510", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer has been widely explored in natural language generation with\nnon-parallel corpus by directly or indirectly extracting a notion of style from\nsource and target domain corpus. A common shortcoming of existing approaches is\nthe prerequisite of joint annotations across all the stylistic dimensions under\nconsideration. Availability of such dataset across a combination of styles\nlimits the extension of these setups to multiple style dimensions. While\ncascading single-dimensional models across multiple styles is a possibility, it\nsuffers from content loss, especially when the style dimensions are not\ncompletely independent of each other. In our work, we relax this requirement of\njointly annotated data across multiple styles by using independently acquired\ndata across different style dimensions without any additional annotations. We\ninitialize an encoder-decoder setup with transformer-based language model\npre-trained on a generic corpus and enhance its re-writing capability to\nmultiple target style dimensions by employing multiple style-aware language\nmodels as discriminators. Through quantitative and qualitative evaluation, we\nshow the ability of our model to control styles across multiple style\ndimensions while preserving content of the input text. We compare it against\nbaselines involving cascaded state-of-the-art uni-dimensional style transfer\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 10:16:29 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 09:39:42 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Goyal", "Navita", ""], ["Srinivasan", "Balaji Vasan", ""], ["Natarajan", "Anandhavelu", ""], ["Sancheti", "Abhilasha", ""]]}, {"id": "2010.11593", "submitter": "Hari Krishna Vydana Mr", "authors": "Hari Krishna Vydana, Lukas Burget, Jan Cernocky", "title": "A Technical Report: BUT Speech Translation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the BUT's speech translation systems. The systems are\nEnglish$\\longrightarrow$German offline speech translation systems. The systems\nare based on our previous works \\cite{Jointly_trained_transformers}. Though\nEnd-to-End and cascade~(ASR-MT) spoken language translation~(SLT) systems are\nreaching comparable performances, a large degradation is observed when\ntranslating ASR hypothesis compared to the oracle input text. To reduce this\nperformance degradation, we have jointly-trained ASR and MT modules with ASR\nobjective as an auxiliary loss. Both the networks are connected through the\nneural hidden representations. This model has an End-to-End differentiable path\nwith respect to the final objective function and also utilizes the ASR\nobjective for better optimization. During the inference both the modules(i.e.,\nASR and MT) are connected through the hidden representations corresponding to\nthe n-best hypotheses. Ensembling with independently trained ASR and MT models\nhave further improved the performance of the system.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 10:52:31 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Vydana", "Hari Krishna", ""], ["Burget", "Lukas", ""], ["Cernocky", "Jan", ""]]}, {"id": "2010.11604", "submitter": "Changzhen Ji", "authors": "Changzhen Ji, Xin Zhou, Conghui Zhu and Tiejun Zhao", "title": "AI-lead Court Debate Case Investigation", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The multi-role judicial debate composed of the plaintiff, defendant, and\njudge is an important part of the judicial trial. Different from other types of\ndialogue, questions are raised by the judge, The plaintiff, plaintiff's agent\ndefendant, and defendant's agent would be to debating so that the trial can\nproceed in an orderly manner. Question generation is an important task in\nNatural Language Generation. In the judicial trial, it can help the judge raise\nefficient questions so that the judge has a clearer understanding of the case.\nIn this work, we propose an innovative end-to-end question generation\nmodel-Trial Brain Model (TBM) to build a Trial Brain, it can generate the\nquestions the judge wants to ask through the historical dialogue between the\nplaintiff and the defendant. Unlike prior efforts in natural language\ngeneration, our model can learn the judge's questioning intention through\npredefined knowledge. We do experiments on real-world datasets, the\nexperimental results show that our model can provide a more accurate question\nin the multi-role court debate scene.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:05:14 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 03:47:33 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ji", "Changzhen", ""], ["Zhou", "Xin", ""], ["Zhu", "Conghui", ""], ["Zhao", "Tiejun", ""]]}, {"id": "2010.11639", "submitter": "Li-Hsin Chang", "authors": "Li-Hsin Chang, Sampo Pyysalo, Jenna Kanerva, Filip Ginter", "title": "Towards Fully Bilingual Deep Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models based on deep neural networks have facilitated great advances\nin natural language processing and understanding tasks in recent years. While\nmodels covering a large number of languages have been introduced, their\nmultilinguality has come at a cost in terms of monolingual performance, and the\nbest-performing models at most tasks not involving cross-lingual transfer\nremain monolingual. In this paper, we consider the question of whether it is\npossible to pre-train a bilingual model for two remotely related languages\nwithout compromising performance at either language. We collect pre-training\ndata, create a Finnish-English bilingual BERT model and evaluate its\nperformance on datasets used to evaluate the corresponding monolingual models.\nOur bilingual model performs on par with Google's original English BERT on GLUE\nand nearly matches the performance of monolingual Finnish BERT on a range of\nFinnish NLP tasks, clearly outperforming multilingual BERT. We find that when\nthe model vocabulary size is increased, the BERT-Base architecture has\nsufficient capacity to learn two remotely related languages to a level where it\nachieves comparable performance with monolingual models, demonstrating the\nfeasibility of training fully bilingual deep language models. The model and all\ntools involved in its creation are freely available at\nhttps://github.com/TurkuNLP/biBERT\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:22:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chang", "Li-Hsin", ""], ["Pyysalo", "Sampo", ""], ["Kanerva", "Jenna", ""], ["Ginter", "Filip", ""]]}, {"id": "2010.11657", "submitter": "Renyu Wang", "authors": "Renyu Wang, Ruilin Tong, Yu Ting Yeung, Xiao Chen", "title": "The HUAWEI Speaker Diarisation System for the VoxCeleb Speaker\n  Diarisation Challenge", "comments": "5 pages, 2 figures, A report about our diarisation system for\n  VoxCeleb Challenge, Interspeech conference workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes system setup of our submission to speaker diarisation\ntrack (Track 4) of VoxCeleb Speaker Recognition Challenge 2020. Our diarisation\nsystem consists of a well-trained neural network based speech enhancement model\nas pre-processing front-end of input speech signals. We replace conventional\nenergy-based voice activity detection (VAD) with a neural network based VAD.\nThe neural network based VAD provides more accurate annotation of speech\nsegments containing only background music, noise, and other interference, which\nis crucial to diarisation performance. We apply agglomerative hierarchical\nclustering (AHC) of x-vectors and variational Bayesian hidden Markov model\n(VB-HMM) based iterative clustering for speaker clustering. Experimental\nresults demonstrate that our proposed system achieves substantial improvements\nover the baseline system, yielding diarisation error rate (DER) of 10.45%, and\nJacard error rate (JER) of 22.46% on the evaluation set.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:42:07 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 07:45:47 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wang", "Renyu", ""], ["Tong", "Ruilin", ""], ["Yeung", "Yu Ting", ""], ["Chen", "Xiao", ""]]}, {"id": "2010.11666", "submitter": "Pavel Kalaidin", "authors": "Nadezhda Zueva, Madina Kabirova, Pavel Kalaidin", "title": "Reducing Unintended Identity Bias in Russian Hate Speech Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxicity has become a grave problem for many online communities and has been\ngrowing across many languages, including Russian. Hate speech creates an\nenvironment of intimidation, discrimination, and may even incite some\nreal-world violence. Both researchers and social platforms have been focused on\ndeveloping models to detect toxicity in online communication for a while now. A\ncommon problem of these models is the presence of bias towards some words (e.g.\nwoman, black, jew) that are not toxic, but serve as triggers for the classifier\ndue to model caveats. In this paper, we describe our efforts towards\nclassifying hate speech in Russian, and propose simple techniques of reducing\nunintended bias, such as generating training data with language models using\nterms and words related to protected identities as context and applying word\ndropout to such words.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:54:14 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zueva", "Nadezhda", ""], ["Kabirova", "Madina", ""], ["Kalaidin", "Pavel", ""]]}, {"id": "2010.11683", "submitter": "Xiang Dai", "authors": "Xiang Dai and Heike Adel", "title": "An Analysis of Simple Data Augmentation for Named Entity Recognition", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simple yet effective data augmentation techniques have been proposed for\nsentence-level and sentence-pair natural language processing tasks. Inspired by\nthese efforts, we design and compare data augmentation for named entity\nrecognition, which is usually modeled as a token-level sequence labeling\nproblem. Through experiments on two data sets from the biomedical and materials\nscience domains (i2b2-2010 and MaSciP), we show that simple augmentation can\nboost performance for both recurrent and transformer-based models, especially\nfor small training sets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 13:21:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Dai", "Xiang", ""], ["Adel", "Heike", ""]]}, {"id": "2010.11701", "submitter": "Philipp Sadler", "authors": "Philipp Sadler", "title": "Spatial Attention as an Interface for Image Captioning Models", "comments": "A thesis submitted in fulfillment of the requirements for the degree\n  Master of Science in Cognitive Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal workings of modern deep learning models stay often unclear to an\nexternal observer, although spatial attention mechanisms are involved. The idea\nof this work is to translate these spatial attentions into natural language to\nprovide a simpler access to the model's function. Thus, I took a neural image\ncaptioning model and measured the reactions to external modification in its\nspatial attention for three different interface methods: a fixation over the\nwhole generation process, a fixation for the first time-steps and an addition\nto the generator's attention. The experimental results for bounding box based\nspatial attention vectors have shown that the captioning model reacts to method\ndependent changes in up to 52.65% and includes in 9.00% of the cases object\ncategories, which were otherwise unmentioned. Afterwards, I established such a\nlink to a hierarchical co-attention network for visual question answering by\nextraction of its word, phrase and question level spatial attentions. Here,\ngenerated captions for the word level included details of the question-answer\npairs in up to 55.20% of the cases. This work indicates that spatial attention\nseen as an external interface for image caption generators is an useful method\nto access visual functions in natural language.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:04:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sadler", "Philipp", ""]]}, {"id": "2010.11731", "submitter": "Akbar Karimi", "authors": "Akbar Karimi, Leonardo Rossi, Andrea Prati", "title": "Improving BERT Performance for Aspect-Based Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-Based Sentiment Analysis (ABSA) studies the consumer opinion on the\nmarket products. It involves examining the type of sentiments as well as\nsentiment targets expressed in product reviews. Analyzing the language used in\na review is a difficult task that requires a deep understanding of the\nlanguage. In recent years, deep language models, such as BERT\n\\cite{devlin2019bert}, have shown great progress in this regard. In this work,\nwe propose two simple modules called Parallel Aggregation and Hierarchical\nAggregation to be utilized on top of BERT for two main ABSA tasks namely Aspect\nExtraction (AE) and Aspect Sentiment Classification (ASC) in order to improve\nthe model's performance. We show that applying the proposed models eliminates\nthe need for further training of the BERT model. The source code is available\non the Web for further research and reproduction of the results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 13:52:18 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 10:23:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Karimi", "Akbar", ""], ["Rossi", "Leonardo", ""], ["Prati", "Andrea", ""]]}, {"id": "2010.11745", "submitter": "Tatiana Likhomanenko", "authors": "Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello,\n  Jacob Kahn, Gilad Avidov, Ronan Collobert, Gabriel Synnaeve", "title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is pushing numbers on a single benchmark valuable in automatic speech\nrecognition? Research results in acoustic modeling are typically evaluated\nbased on performance on a single dataset. While the research community has\ncoalesced around various benchmarks, we set out to understand generalization\nperformance in acoustic modeling across datasets - in particular, if models\ntrained on a single dataset transfer to other (possibly out-of-domain)\ndatasets. We show that, in general, reverberative and additive noise\naugmentation improves generalization performance across domains. Further, we\ndemonstrate that when a large enough set of benchmarks is used, average word\nerror rate (WER) performance over them provides a good proxy for performance on\nreal-world noisy data. Finally, we show that training a single acoustic model\non the most widely-used datasets - combined - reaches competitive performance\non both research and real-world benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:01:32 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 06:19:01 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 18:30:52 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Likhomanenko", "Tatiana", ""], ["Xu", "Qiantong", ""], ["Pratap", "Vineel", ""], ["Tomasello", "Paden", ""], ["Kahn", "Jacob", ""], ["Avidov", "Gilad", ""], ["Collobert", "Ronan", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "2010.11747", "submitter": "Ivana Kvapilikova", "authors": "Ivana Kvapil\\'ikov\\'a, Tom Kocmi, Ond\\v{r}ej Bojar", "title": "CUNI Systems for the Unsupervised and Very Low Resource Translation Task\n  in WMT20", "comments": "WMT20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a description of CUNI systems submitted to the WMT20 task\non unsupervised and very low-resource supervised machine translation between\nGerman and Upper Sorbian. We experimented with training on synthetic data and\npre-training on a related language pair. In the fully unsupervised scenario, we\nachieved 25.5 and 23.7 BLEU translating from and into Upper Sorbian,\nrespectively. Our low-resource systems relied on transfer learning from\nGerman-Czech parallel data and achieved 57.4 BLEU and 56.1 BLEU, which is an\nimprovement of 10 BLEU points over the baseline trained only on the available\nsmall German-Upper Sorbian parallel corpus.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:04:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kvapil\u00edkov\u00e1", "Ivana", ""], ["Kocmi", "Tom", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2010.11764", "submitter": "Aman Madaan", "authors": "Aman Madaan, Dheeraj Rajagopal, Yiming Yang, Abhilasha Ravichander,\n  Eduard Hovy, Shrimai Prabhumoye", "title": "EIGEN: Event Influence GENeration using Pre-trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning about events and tracking their influences is fundamental to\nunderstanding processes. In this paper, we present EIGEN - a method to leverage\npre-trained language models to generate event influences conditioned on a\ncontext, nature of their influence, and the distance in a reasoning chain. We\nalso derive a new dataset for research and evaluation of methods for event\ninfluence generation. EIGEN outperforms strong baselines both in terms of\nautomated evaluation metrics (by 10 ROUGE points) and human judgments on\ncloseness to reference and relevance of generations. Furthermore, we show that\nthe event influences generated by EIGEN improve the performance on a \"what-if\"\nQuestion Answering (WIQA) benchmark (over 3% F1), especially for questions that\nrequire background knowledge and multi-hop reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:36:04 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Madaan", "Aman", ""], ["Rajagopal", "Dheeraj", ""], ["Yang", "Yiming", ""], ["Ravichander", "Abhilasha", ""], ["Hovy", "Eduard", ""], ["Prabhumoye", "Shrimai", ""]]}, {"id": "2010.11784", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel\n  Collier", "title": "Self-Alignment Pretraining for Biomedical Entity Representations", "comments": "NAACL 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread success of self-supervised learning via masked\nlanguage models (MLM), accurately capturing fine-grained semantic relationships\nin the biomedical domain remains a challenge. This is of paramount importance\nfor entity-level tasks such as entity linking where the ability to model entity\nrelations (especially synonymy) is pivotal. To address this challenge, we\npropose SapBERT, a pretraining scheme that self-aligns the representation space\nof biomedical entities. We design a scalable metric learning framework that can\nleverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts.\nIn contrast with previous pipeline-based hybrid systems, SapBERT offers an\nelegant one-model-for-all solution to the problem of medical entity linking\n(MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking\ndatasets. In the scientific domain, we achieve SOTA even without task-specific\nsupervision. With substantial improvement over various domain-specific\npretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining\nscheme proves to be both effective and robust.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:59:57 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 11:01:50 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Fangyu", ""], ["Shareghi", "Ehsan", ""], ["Meng", "Zaiqiao", ""], ["Basaldella", "Marco", ""], ["Collier", "Nigel", ""]]}, {"id": "2010.11791", "submitter": "Ivan Vuli\\'c", "authors": "Matthew Henderson and Ivan Vuli\\'c", "title": "ConVEx: Data-Efficient and Few-Shot Slot Labeling", "comments": "NAACL 2021 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose ConVEx (Conversational Value Extractor), an efficient pretraining\nand fine-tuning neural approach for slot-labeling dialog tasks. Instead of\nrelying on more general pretraining objectives from prior work (e.g., language\nmodeling, response selection), ConVEx's pretraining objective, a novel pairwise\ncloze task using Reddit data, is well aligned with its intended usage on\nsequence labeling tasks. This enables learning domain-specific slot labelers by\nsimply fine-tuning decoding layers of the pretrained general-purpose sequence\nlabeling model, while the majority of the pretrained model's parameters are\nkept frozen. We report state-of-the-art performance of ConVEx across a range of\ndiverse domains and data sets for dialog slot-labeling, with the largest gains\nin the most challenging, few-shot setups. We believe that ConVEx's reduced\npretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its\nefficient fine-tuning and strong performance, promise wider portability and\nscalability for data-efficient sequence-labeling tasks in general.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:13:35 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 16:28:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Henderson", "Matthew", ""], ["Vuli\u0107", "Ivan", ""]]}, {"id": "2010.11803", "submitter": "Zeqian Li", "authors": "Zeqian Li, Jacob Whitehill", "title": "Compositional embedding models for speaker identification and\n  diarization with simultaneous speech from 2+ speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for speaker diarization that can handle overlapping\nspeech with 2+ people. Our method is based on compositional embeddings [1]:\nLike standard speaker embedding methods such as x-vector [2], compositional\nembedding models contain a function f that separates speech from different\nspeakers. In addition, they include a composition function g to compute\nset-union operations in the embedding space so as to infer the set of speakers\nwithin the input audio. In an experiment on multi-person speaker identification\nusing synthesized LibriSpeech data, the proposed method outperforms traditional\nembedding methods that are only trained to separate single speakers (not\nspeaker sets). In a speaker diarization experiment on the AMI Headset Mix\ncorpus, we achieve state-of-the-art accuracy (DER=22.93%), slightly higher than\nthe previous best result (23.82% from [3]).\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:33:36 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 15:47:18 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Li", "Zeqian", ""], ["Whitehill", "Jacob", ""]]}, {"id": "2010.11818", "submitter": "Hao Zheng", "authors": "Hao Zheng and Mirella Lapata", "title": "Compositional Generalization via Semantic Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural sequence-to-sequence models have been successfully applied to\nsemantic parsing, they struggle to perform well on query-based data splits that\nrequire \\emph{composition generalization}, an ability of systematically\ngeneralizing to unseen composition of seen components. Motivated by the\nexplicitly built-in compositionality in traditional statistical semantic\nparsing, we propose a new decoding framework that preserves the expressivity\nand generality of sequence-to-sequence models while featuring explicit\nlexicon-style alignments and disentangled information processing. Specifically,\nwe decompose decoding into two phases where an input utterance is first tagged\nwith semantic symbols representing the meanings of its individual words, and\nthen a sequence-to-sequence model is used to predict the final meaning\nrepresentation conditioning on the utterance and the predicted tag sequence.\nExperimental results on three semantic parsing datasets with query-based splits\nshow that the proposed approach consistently improves compositional\ngeneralization of sequence-to-sequence models across different model\narchitectures, domains and semantic formalisms.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:55:15 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zheng", "Hao", ""], ["Lapata", "Mirella", ""]]}, {"id": "2010.11853", "submitter": "Johannes E. M. Mosig", "authors": "Johannes E. M. Mosig, Shikib Mehri, Thomas Kober", "title": "STAR: A Schema-Guided Dialog Dataset for Transfer Learning", "comments": "Equal contribution: Johannes E. M. Mosig, Shikib Mehri", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present STAR, a schema-guided task-oriented dialog dataset consisting of\n127,833 utterances and knowledge base queries across 5,820 task-oriented\ndialogs in 13 domains that is especially designed to facilitate task and domain\ntransfer learning in task-oriented dialog. Furthermore, we propose a scalable\ncrowd-sourcing paradigm to collect arbitrarily large datasets of the same\nquality as STAR. Moreover, we introduce novel schema-guided dialog models that\nuse an explicit description of the task(s) to generalize from known to unknown\ntasks. We demonstrate the effectiveness of these models, particularly for\nzero-shot generalization across tasks and domains.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:45:00 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mosig", "Johannes E. M.", ""], ["Mehri", "Shikib", ""], ["Kober", "Thomas", ""]]}, {"id": "2010.11855", "submitter": "Michael Wick", "authors": "Michael L. Wick, Kate Silverstein, Jean-Baptiste Tristan, Adam Pocock,\n  Mark Johnson", "title": "Detecting and Exorcising Statistical Demons from Language Models with\n  Anti-Models of Negative Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It's been said that \"Language Models are Unsupervised Multitask Learners.\"\nIndeed, self-supervised language models trained on \"positive\" examples of\nEnglish text generalize in desirable ways to many natural language tasks. But\nif such models can stray so far from an initial self-supervision objective, a\nwayward model might generalize in undesirable ways too, say to nonsensical\n\"negative\" examples of unnatural language. A key question in this work is: do\nlanguage models trained on (positive) training data also generalize to\n(negative) test data? We use this question as a contrivance to assess the\nextent to which language models learn undesirable properties of text, such as\nn-grams, that might interfere with the learning of more desirable properties of\ntext, such as syntax. We find that within a model family, as the number of\nparameters, training epochs, and data set size increase, so does a model's\nability to generalize to negative n-gram data, indicating standard\nself-supervision generalizes too far. We propose a form of inductive bias that\nattenuates such undesirable signals with negative data distributions\nautomatically learned from positive data. We apply the method to remove n-gram\nsignals from LSTMs and find that doing so causes them to favor syntactic\nsignals, as demonstrated by large error reductions (up to 46% on the hardest\ncases) on a syntactic subject-verb agreement task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:45:32 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wick", "Michael L.", ""], ["Silverstein", "Kate", ""], ["Tristan", "Jean-Baptiste", ""], ["Pocock", "Adam", ""], ["Johnson", "Mark", ""]]}, {"id": "2010.11856", "submitter": "Akari Asai", "authors": "Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi\n  and Hannaneh Hajishirzi", "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering", "comments": "Published as a conference paper at NAACL-HLT 2021 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual question answering tasks typically assume answers exist in the\nsame language as the question. Yet in practice, many languages face both\ninformation scarcity -- where languages have few reference articles -- and\ninformation asymmetry -- where questions reference concepts from other\ncultures. This work extends open-retrieval question answering to a\ncross-lingual setting enabling questions from one language to be answered via\nanswer content from another language. We construct a large-scale dataset built\non questions from TyDi QA lacking same-language answers. Our task formulation,\ncalled Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k\ninformation-seeking questions from across 7 diverse non-English languages.\nBased on this dataset, we introduce three new tasks that involve cross-lingual\ndocument retrieval using multi-lingual and English resources. We establish\nbaselines with state-of-the-art machine translation systems and cross-lingual\npretrained models. Experimental results suggest that XOR QA is a challenging\ntask that will facilitate the development of novel techniques for multilingual\nquestion answering. Our data and code are available at\nhttps://nlp.cs.washington.edu/xorqa.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:47:17 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 10:00:22 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 05:22:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Asai", "Akari", ""], ["Kasai", "Jungo", ""], ["Clark", "Jonathan H.", ""], ["Lee", "Kenton", ""], ["Choi", "Eunsol", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2010.11859", "submitter": "Nikolay Bogoychev Dr", "authors": "Nikolay Bogoychev", "title": "Not all parameters are born equal: Attention is mostly what you need", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are widely used in state-of-the-art machine translation, but the\nkey to their success is still unknown. To gain insight into this, we consider\nthree groups of parameters: embeddings, attention, and feed forward neural\nnetwork (FFN) layers. We examine the relative importance of each by performing\nan ablation study where we initialise them at random and freeze them, so that\ntheir weights do not change over the course of the training. Through this, we\nshow that the attention and FFN are equally important and fulfil the same\nfunctionality in a model. We show that the decision about whether a component\nis frozen or allowed to train is at least as important for the final model\nperformance as its number of parameters. At the same time, the number of\nparameters alone is not indicative of a component's importance. Finally, while\nthe embedding layer is the least essential for machine translation tasks, it is\nthe most important component for language modelling tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:49:18 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Bogoychev", "Nikolay", ""]]}, {"id": "2010.11869", "submitter": "Lei Xu", "authors": "Lei Xu, Ivan Ramirez, Kalyan Veeramachaneni", "title": "Rewriting Meaningful Sentences via Conditional BERT Sampling and an\n  application on fooling text classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most adversarial attack methods that are designed to deceive a text\nclassifier change the text classifier's prediction by modifying a few words or\ncharacters. Few try to attack classifiers by rewriting a whole sentence, due to\nthe difficulties inherent in sentence-level rephrasing as well as the problem\nof setting the criteria for legitimate rewriting.\n  In this paper, we explore the problem of creating adversarial examples with\nsentence-level rewriting. We design a new sampling method, named\nParaphraseSampler, to efficiently rewrite the original sentence in multiple\nways. Then we propose a new criteria for modification, called a sentence-level\nthreaten model. This criteria allows for both word- and sentence-level changes,\nand can be adjusted independently in two dimensions: semantic similarity and\ngrammatical quality. Experimental results show that many of these rewritten\nsentences are misclassified by the classifier. On all 6 datasets, our\nParaphraseSampler achieves a better attack success rate than our baseline.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:03:13 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Xu", "Lei", ""], ["Ramirez", "Ivan", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "2010.11915", "submitter": "Akari Asai", "authors": "Akari Asai and Eunsol Choi", "title": "Challenges in Information-Seeking QA: Unanswerable Questions and\n  Paragraph Retrieval", "comments": "Published as a conference paper at ACL 2021 (long). Our code and\n  annotated data are publicly available at\n  https://github.com/AkariAsai/unanswerable_qa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent pretrained language models \"solved\" many reading comprehension\nbenchmarks, where questions are written with access to the evidence document.\nHowever, datasets containing information-seeking queries where evidence\ndocuments are provided after the queries are written independently remain\nchallenging. We analyze why answering information-seeking queries is more\nchallenging and where their prevalent unanswerabilities arise, on Natural\nQuestions and TyDi QA. Our controlled experiments suggest two headrooms --\nparagraph selection and answerability prediction, i.e. whether the paired\nevidence document contains the answer to the query or not. When provided with a\ngold paragraph and knowing when to abstain from answering, existing models\neasily outperform a human annotator. However, predicting answerability itself\nremains challenging. We manually annotate 800 unanswerable examples across six\nlanguages on what makes them challenging to answer. With this new data, we\nconduct per-category answerability prediction, revealing issues in the current\ndataset collection as well as task formulation. Together, our study points to\navenues for future research in information-seeking question answering, both for\ndataset creation and model development.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:48:17 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 19:13:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Asai", "Akari", ""], ["Choi", "Eunsol", ""]]}, {"id": "2010.11918", "submitter": "Andreas R\\\"uckl\\'e", "authors": "Andreas R\\\"uckl\\'e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas\n  Pfeiffer, Nils Reimers, Iryna Gurevych", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively pre-trained transformer models are computationally expensive to\nfine-tune, slow for inference, and have large storage requirements. Recent\napproaches tackle these shortcomings by training smaller models, dynamically\nreducing the model size, and by training light-weight adapters. In this paper,\nwe propose AdapterDrop, removing adapters from lower transformer layers during\ntraining and inference, which incorporates concepts from all three directions.\nWe show that AdapterDrop can dynamically reduce the computational overhead when\nperforming inference over multiple tasks simultaneously, with minimal decrease\nin task performances. We further prune adapters from AdapterFusion, which\nimproves the inference efficiency while maintaining the task performances\nentirely.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:49:42 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["R\u00fcckl\u00e9", "Andreas", ""], ["Geigle", "Gregor", ""], ["Glockner", "Max", ""], ["Beck", "Tilman", ""], ["Pfeiffer", "Jonas", ""], ["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.11930", "submitter": "Rodrigo Nogueira", "authors": "Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin", "title": "Scientific Claim Verification with VERT5ERINI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the adaptation of a pretrained sequence-to-sequence model\nto the task of scientific claim verification in the biomedical domain. We\npropose VERT5ERINI that exploits T5 for abstract retrieval, sentence selection\nand label prediction, which are three critical sub-tasks of claim verification.\nWe evaluate our pipeline on SCIFACT, a newly curated dataset that requires\nmodels to not just predict the veracity of claims but also provide relevant\nsentences from a corpus of scientific literature that support this decision.\nEmpirically, our pipeline outperforms a strong baseline in each of the three\nsteps. Finally, we show VERT5ERINI's ability to generalize to two new datasets\nof COVID-19 claims using evidence from the ever-expanding CORD-19 corpus.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:56:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Pradeep", "Ronak", ""], ["Ma", "Xueguang", ""], ["Nogueira", "Rodrigo", ""], ["Lin", "Jimmy", ""]]}, {"id": "2010.11934", "submitter": "Colin Raffel", "authors": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou,\n  Aditya Siddhant, Aditya Barua, Colin Raffel", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:58:14 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:25:28 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 18:45:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xue", "Linting", ""], ["Constant", "Noah", ""], ["Roberts", "Adam", ""], ["Kale", "Mihir", ""], ["Al-Rfou", "Rami", ""], ["Siddhant", "Aditya", ""], ["Barua", "Aditya", ""], ["Raffel", "Colin", ""]]}, {"id": "2010.11936", "submitter": "Tomasz Stanis{\\l}awek", "authors": "Rafal Powalski and Tomasz Stanislawek", "title": "UniCase -- Rethinking Casing in Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new approach to dealing with the problem of\ncase-sensitiveness in Language Modelling (LM). We propose simple architecture\nmodification to the RoBERTa language model, accompanied by a new tokenization\nstrategy, which we named Unified Case LM (UniCase). We tested our solution on\nthe GLUE benchmark, which led to increased performance by 0.42 points.\nMoreover, we prove that the UniCase model works much better when we have to\ndeal with text data, where all tokens are uppercased (+5.88 point).\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:58:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Powalski", "Rafal", ""], ["Stanislawek", "Tomasz", ""]]}, {"id": "2010.11939", "submitter": "Chu-Cheng Lin", "authors": "Chu-Cheng Lin and Aaron Jaech and Xin Li and Matthew R. Gormley and\n  Jason Eisner", "title": "Limitations of Autoregressive Models and Their Alternatives", "comments": "NAACL 2021 (same content, more relaxed layout)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Standard autoregressive language models perform only polynomial-time\ncomputation to compute the probability of the next symbol. While this is\nattractive, it means they cannot model distributions whose next-symbol\nprobability is hard to compute. Indeed, they cannot even model them well enough\nto solve associated easy decision problems for which an engineer might want to\nconsult a language model. These limitations apply no matter how much\ncomputation and data are used to train the model, unless the model is given\naccess to oracle parameters that grow superpolynomially in sequence length.\n  Thus, simply training larger autoregressive language models is not a panacea\nfor NLP. Alternatives include energy-based models (which give up efficient\nsampling) and latent-variable autoregressive models (which give up efficient\nscoring of a given string). Both are powerful enough to escape the above\nlimitations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:59:09 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 14:24:32 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 02:09:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lin", "Chu-Cheng", ""], ["Jaech", "Aaron", ""], ["Li", "Xin", ""], ["Gormley", "Matthew R.", ""], ["Eisner", "Jason", ""]]}, {"id": "2010.11947", "submitter": "Zekun Xu", "authors": "Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, Nathanael Teissier", "title": "A Differentially Private Text Perturbation Method Using a Regularized\n  Mahalanobis Metric", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing the privacy-utility tradeoff is a crucial requirement of many\npractical machine learning systems that deal with sensitive customer data. A\npopular approach for privacy-preserving text analysis is noise injection, in\nwhich text data is first mapped into a continuous embedding space, perturbed by\nsampling a spherical noise from an appropriate distribution, and then projected\nback to the discrete vocabulary space. While this allows the perturbation to\nadmit the required metric differential privacy, often the utility of downstream\ntasks modeled on this perturbed data is low because the spherical noise does\nnot account for the variability in the density around different words in the\nembedding space. In particular, words in a sparse region are likely unchanged\neven when the noise scale is large. %Using the global sensitivity of the\nmechanism can potentially add too much noise to the words in the dense regions\nof the embedding space, causing a high utility loss, whereas using local\nsensitivity can leak information through the scale of the noise added.\n  In this paper, we propose a text perturbation mechanism based on a carefully\ndesigned regularized variant of the Mahalanobis metric to overcome this\nproblem. For any given noise scale, this metric adds an elliptical noise to\naccount for the covariance structure in the embedding space. This heterogeneity\nin the noise scale along different directions helps ensure that the words in\nthe sparse region have sufficient likelihood of replacement without sacrificing\nthe overall utility. We provide a text-perturbation algorithm based on this\nmetric and formally prove its privacy guarantees. Additionally, we empirically\nshow that our mechanism improves the privacy statistics to achieve the same\nlevel of utility as compared to the state-of-the-art Laplace mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 23:06:44 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Xu", "Zekun", ""], ["Aggarwal", "Abhinav", ""], ["Feyisetan", "Oluwaseyi", ""], ["Teissier", "Nathanael", ""]]}, {"id": "2010.11966", "submitter": "David Lowell", "authors": "David Lowell, Brian E. Howard, Zachary C. Lipton, Byron C. Wallace", "title": "Unsupervised Data Augmentation with Naive Augmentation and without\n  Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Data Augmentation (UDA) is a semi-supervised technique that\napplies a consistency loss to penalize differences between a model's\npredictions on (a) observed (unlabeled) examples; and (b) corresponding\n'noised' examples produced via data augmentation. While UDA has gained\npopularity for text classification, open questions linger over which design\ndecisions are necessary and over how to extend the method to sequence labeling\ntasks. This method has recently gained traction for text classification. In\nthis paper, we re-examine UDA and demonstrate its efficacy on several\nsequential tasks. Our main contribution is an empirical study of UDA to\nestablish which components of the algorithm confer benefits in NLP. Notably,\nalthough prior work has emphasized the use of clever augmentation techniques\nincluding back-translation, we find that enforcing consistency between\npredictions assigned to observed and randomly substituted words often yields\ncomparable (or greater) benefits compared to these complex perturbation models.\nFurthermore, we find that applying its consistency loss affords meaningful\ngains without any unlabeled data at all, i.e., in a standard supervised\nsetting. In short: UDA need not be unsupervised, and does not require complex\ndata augmentation to be effective.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:01:51 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lowell", "David", ""], ["Howard", "Brian E.", ""], ["Lipton", "Zachary C.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2010.11967", "submitter": "Chenguang Wang", "authors": "Chenguang Wang, Xiao Liu, Dawn Song", "title": "Language Models are Open Knowledge Graphs", "comments": "30 pages, 32 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to construct knowledge graphs (KGs) from pre-trained\nlanguage models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs\n(e.g, Wikidata, NELL) are built in either a supervised or semi-supervised\nmanner, requiring humans to create knowledge. Recent deep language models\nautomatically acquire knowledge from large-scale corpora via pre-training. The\nstored knowledge has enabled the language models to improve downstream NLP\ntasks, e.g., answering questions, and writing code and articles. In this paper,\nwe propose an unsupervised method to cast the knowledge contained within\nlanguage models into KGs. We show that KGs are constructed with a single\nforward pass of the pre-trained language models (without fine-tuning) over the\ncorpora. We demonstrate the quality of the constructed KGs by comparing to two\nKGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual\nknowledge that is new in the existing KGs. Our code and KGs will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:01:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wang", "Chenguang", ""], ["Liu", "Xiao", ""], ["Song", "Dawn", ""]]}, {"id": "2010.11973", "submitter": "Badr M. Abdullah", "authors": "Badr M. Abdullah, Jacek Kudera, Tania Avgustinova, Bernd M\\\"obius,\n  Dietrich Klakow", "title": "Rediscovering the Slavic Continuum in Representations Emerging from\n  Neural Models of Spoken Language Identification", "comments": "Accepted in VarDial 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have been employed for various spoken language\nrecognition tasks, including tasks that are multilingual by definition such as\nspoken language identification. In this paper, we present a neural model for\nSlavic language identification in speech signals and analyze its emergent\nrepresentations to investigate whether they reflect objective measures of\nlanguage relatedness and/or non-linguists' perception of language similarity.\nWhile our analysis shows that the language representation space indeed captures\nlanguage relatedness to a great extent, we find perceptual confusability\nbetween languages in our study to be the best predictor of the language\nrepresentation similarity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:18:19 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Abdullah", "Badr M.", ""], ["Kudera", "Jacek", ""], ["Avgustinova", "Tania", ""], ["M\u00f6bius", "Bernd", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.11980", "submitter": "Tuan Manh Lai", "authors": "Tuan Manh Lai, Trung Bui, Doo Soon Kim, Quan Hung Tran", "title": "A Joint Learning Approach based on Self-Distillation for Keyphrase\n  Extraction from Scientific Documents", "comments": "Accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase extraction is the task of extracting a small set of phrases that\nbest describe a document. Most existing benchmark datasets for the task\ntypically have limited numbers of annotated documents, making it challenging to\ntrain increasingly complex neural networks. In contrast, digital libraries\nstore millions of scientific articles online, covering a wide range of topics.\nWhile a significant portion of these articles contain keyphrases provided by\ntheir authors, most other articles lack such kind of annotations. Therefore, to\neffectively utilize these large amounts of unlabeled articles, we propose a\nsimple and efficient joint learning approach based on the idea of\nself-distillation. Experimental results show that our approach consistently\nimproves the performance of baseline models for keyphrase extraction.\nFurthermore, our best models outperform previous methods for the task,\nachieving new state-of-the-art results on two public benchmarks: Inspec and\nSemEval-2017.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:36:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lai", "Tuan Manh", ""], ["Bui", "Trung", ""], ["Kim", "Doo Soon", ""], ["Tran", "Quan Hung", ""]]}, {"id": "2010.11982", "submitter": "Avia Efrat", "authors": "Avia Efrat and Omer Levy", "title": "The Turking Test: Can Language Models Understand Instructions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning provides the learner with a set of input-output\nexamples of the target task. Humans, however, can also learn to perform new\ntasks from instructions in natural language. Can machines learn to understand\ninstructions as well? We present the Turking Test, which examines a model's\nability to follow natural language instructions of varying complexity. These\nrange from simple tasks, like retrieving the nth word of a sentence, to ones\nthat require creativity, such as generating examples for SNLI and SQuAD in\nplace of human intelligence workers (\"turkers\"). Despite our lenient evaluation\nmethodology, we observe that a large pretrained language model performs poorly\nacross all tasks. Analyzing the model's error patterns reveals that the model\ntends to ignore explicit instructions and often generates outputs that cannot\nbe construed as an attempt to solve the task. While it is not yet clear whether\ninstruction understanding can be captured by traditional language models, the\nsheer expressivity of instruction understanding makes it an appealing\nalternative to the rising few-shot inference paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:44:16 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Efrat", "Avia", ""], ["Levy", "Omer", ""]]}, {"id": "2010.11985", "submitter": "Jianing Yang", "authors": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir\n  Zadeh, Soujanya Poria, Louis-Philippe Morency", "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal\n  Language Sequences", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication is multimodal in nature; it is through multiple\nmodalities such as language, voice, and facial expressions, that opinions and\nemotions are expressed. Data in this domain exhibits complex multi-relational\nand temporal interactions. Learning from this data is a fundamentally\nchallenging research problem. In this paper, we propose Modal-Temporal\nAttention Graph (MTAG). MTAG is an interpretable graph-based neural model that\nprovides a suitable framework for analyzing multimodal sequential data. We\nfirst introduce a procedure to convert unaligned multimodal sequence data into\na graph with heterogeneous nodes and edges that captures the rich interactions\nacross modalities and through time. Then, a novel graph fusion operation,\ncalled MTAG fusion, along with a dynamic pruning and read-out technique, is\ndesigned to efficiently process this modal-temporal graph and capture various\ninteractions. By learning to focus only on the important interactions within\nthe graph, MTAG achieves state-of-the-art performance on multimodal sentiment\nanalysis and emotion recognition benchmarks, while utilizing significantly\nfewer model parameters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:58:50 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 18:44:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yang", "Jianing", ""], ["Wang", "Yongxin", ""], ["Yi", "Ruitao", ""], ["Zhu", "Yuying", ""], ["Rehman", "Azaan", ""], ["Zadeh", "Amir", ""], ["Poria", "Soujanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2010.11988", "submitter": "Bailin Wang", "authors": "Bailin Wang, Mirella Lapata and Ivan Titov", "title": "Meta-Learning for Domain Generalization in Semantic Parsing", "comments": "NAACL2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of building semantic parsers which can be applied to new\ndomains and generate programs unseen at training has long been acknowledged,\nand datasets testing out-of-domain performance are becoming increasingly\navailable. However, little or no attention has been devoted to learning\nalgorithms or objectives which promote domain generalization, with virtually\nall existing approaches relying on standard supervised learning. In this work,\nwe use a meta-learning framework which targets zero-shot domain generalization\nfor semantic parsing. We apply a model-agnostic training algorithm that\nsimulates zero-shot parsing by constructing virtual train and test sets from\ndisjoint domains. The learning objective capitalizes on the intuition that\ngradient steps that improve source-domain performance should also improve\ntarget-domain performance, thus encouraging a parser to generalize to unseen\ntarget domains. Experimental results on the (English) Spider and Chinese Spider\ndatasets show that the meta-learning objective significantly boosts the\nperformance of a baseline parser.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:00:36 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 20:40:38 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Bailin", ""], ["Lapata", "Mirella", ""], ["Titov", "Ivan", ""]]}, {"id": "2010.11997", "submitter": "Zhanwen Chen", "authors": "Zhanwen Chen, Shiyao Li, Roxanne Rashedi, Xiaoman Zi, Morgan\n  Elrod-Erickson, Bryan Hollis, Angela Maliakal, Xinyu Shen, Simeng Zhao,\n  Maithilee Kunda", "title": "Characterizing Datasets for Social Visual Question Answering, and the\n  New TinySocial Dataset", "comments": "To appear in the Joint IEEE International Conference on Development\n  and Learning and on Epigenetic Robotics (ICDL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social intelligence includes the ability to watch videos and answer\nquestions about social and theory-of-mind-related content, e.g., for a scene in\nHarry Potter, \"Is the father really upset about the boys flying the car?\"\nSocial visual question answering (social VQA) is emerging as a valuable\nmethodology for studying social reasoning in both humans (e.g., children with\nautism) and AI agents. However, this problem space spans enormous variations in\nboth videos and questions. We discuss methods for creating and characterizing\nsocial VQA datasets, including 1) crowdsourcing versus in-house authoring,\nincluding sample comparisons of two new datasets that we created\n(TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ\ndataset; 2) a new rubric for characterizing the difficulty and content of a\ngiven video; and 3) a new rubric for characterizing question types. We close by\ndescribing how having well-characterized social VQA datasets will enhance the\nexplainability of AI agents and can also inform assessments and educational\ninterventions for people.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 03:20:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chen", "Zhanwen", ""], ["Li", "Shiyao", ""], ["Rashedi", "Roxanne", ""], ["Zi", "Xiaoman", ""], ["Elrod-Erickson", "Morgan", ""], ["Hollis", "Bryan", ""], ["Maliakal", "Angela", ""], ["Shen", "Xinyu", ""], ["Zhao", "Simeng", ""], ["Kunda", "Maithilee", ""]]}, {"id": "2010.12008", "submitter": "Siamak Shakeri", "authors": "Siamak Shakeri, Noah Constant, Mihir Sanjay Kale, Linting Xue", "title": "Towards Zero-Shot Multilingual Synthetic Question and Answer Generation\n  for Cross-Lingual Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method to generate multilingual question and answer pairs\non a large scale through the use of a single generative model. These synthetic\nsamples can be used to improve the zero-shot performance of multilingual QA\nmodels on target languages. Our proposed multi-task training of the generative\nmodel only requires the labeled training samples in English, thus removing the\nneed for such samples in the target languages, making it applicable to far more\nlanguages than those with labeled data. Human evaluations indicate the majority\nof such samples are grammatically correct and sensible. Experimental results\nshow our proposed approach can achieve large gains on the XQuAD dataset,\nreducing the gap between zero-shot and supervised performance of smaller QA\nmodels on various languages.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:59:37 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 21:24:02 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 21:07:33 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shakeri", "Siamak", ""], ["Constant", "Noah", ""], ["Kale", "Mihir Sanjay", ""], ["Xue", "Linting", ""]]}, {"id": "2010.12077", "submitter": "Daiki Shirafuji", "authors": "Daiki Shirafuji, Hiromichi Kameya, Rafal Rzepka and Kenji Araki", "title": "Summarizing Utterances from Japanese Assembly Minutes using Political\n  Sentence-BERT-based Method for QA Lab-PoliInfo-2 Task of NTCIR-15", "comments": "8 pages, 1 figure, 8 tables, NTCIR-15 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many discussions held during political meetings, and a large number\nof utterances for various topics is included in their transcripts. We need to\nread all of them if we want to follow speakers\\' intentions or opinions about a\ngiven topic. To avoid such a costly and time-consuming process to grasp often\nlongish discussions, NLP researchers work on generating concise summaries of\nutterances. Summarization subtask in QA Lab-PoliInfo-2 task of the NTCIR-15\naddresses this problem for Japanese utterances in assembly minutes, and our\nteam (SKRA) participated in this subtask. As a first step for summarizing\nutterances, we created a new pre-trained sentence embedding model, i.e. the\nJapanese Political Sentence-BERT. With this model, we summarize utterances\nwithout labelled data. This paper describes our approach to solving the task\nand discusses its results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:37:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Shirafuji", "Daiki", ""], ["Kameya", "Hiromichi", ""], ["Rzepka", "Rafal", ""], ["Araki", "Kenji", ""]]}, {"id": "2010.12083", "submitter": "Simon Stepputtis", "authors": "Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee,\n  Chitta Baral, Heni Ben Amor", "title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020), Vancouver, Canada as spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning is a popular approach for teaching motor skills to robots.\nHowever, most approaches focus on extracting policy parameters from execution\ntraces alone (i.e., motion trajectories and perceptual data). No adequate\ncommunication channel exists between the human expert and the robot to describe\ncritical aspects of the task, such as the properties of the target object or\nthe intended shape of the motion. Motivated by insights into the human teaching\nprocess, we introduce a method for incorporating unstructured natural language\ninto imitation learning. At training time, the expert can provide\ndemonstrations along with verbal descriptions in order to describe the\nunderlying intent (e.g., \"go to the large green bowl\"). The training process\nthen interrelates these two modalities to encode the correlations between\nlanguage, perception, and motion. The resulting language-conditioned visuomotor\npolicies can be conditioned at runtime on new human commands and instructions,\nwhich allows for more fine-grained control over the trained policies while also\nreducing situational ambiguity. We demonstrate in a set of simulation\nexperiments how our approach can learn language-conditioned manipulation\npolicies for a seven-degree-of-freedom robot arm and compare the results to a\nvariety of alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:49:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Stepputtis", "Simon", ""], ["Campbell", "Joseph", ""], ["Phielipp", "Mariano", ""], ["Lee", "Stefan", ""], ["Baral", "Chitta", ""], ["Amor", "Heni Ben", ""]]}, {"id": "2010.12096", "submitter": "Thibault Doutre", "authors": "Thibault Doutre, Wei Han, Min Ma, Zhiyun Lu, Chung-Cheng Chiu, Ruoming\n  Pang, Arun Narayanan, Ananya Misra, Yu Zhang, Liangliang Cao", "title": "Improving Streaming Automatic Speech Recognition With Non-Streaming\n  Model Distillation On Unsupervised Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming end-to-end automatic speech recognition (ASR) models are widely\nused on smart speakers and on-device applications. Since these models are\nexpected to transcribe speech with minimal latency, they are constrained to be\ncausal with no future context, compared to their non-streaming counterparts.\nConsequently, streaming models usually perform worse than non-streaming models.\nWe propose a novel and effective learning method by leveraging a non-streaming\nASR model as a teacher to generate transcripts on an arbitrarily large data\nset, which is then used to distill knowledge into streaming ASR models. This\nway, we scale the training of streaming models to up to 3 million hours of\nYouTube audio. Experiments show that our approach can significantly reduce the\nword error rate (WER) of RNNT models not only on LibriSpeech but also on\nYouTube data in four languages. For example, in French, we are able to reduce\nthe WER by 16.4% relatively to a baseline streaming model by leveraging a\nnon-streaming teacher model trained on the same amount of labeled data as the\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:41:33 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 21:56:06 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Doutre", "Thibault", ""], ["Han", "Wei", ""], ["Ma", "Min", ""], ["Lu", "Zhiyun", ""], ["Chiu", "Chung-Cheng", ""], ["Pang", "Ruoming", ""], ["Narayanan", "Arun", ""], ["Misra", "Ananya", ""], ["Zhang", "Yu", ""], ["Cao", "Liangliang", ""]]}, {"id": "2010.12104", "submitter": "Siyuan Feng", "authors": "Siyuan Feng, Piotr \\.Zelasko, Laureano Moro-Vel\\'azquez, Ali\n  Abavisani, Mark Hasegawa-Johnson, Odette Scharenborg, Najim Dehak", "title": "How Phonotactics Affect Multilingual and Zero-shot ASR Performance", "comments": "Accepted for publication in IEEE ICASSP 2021. The first 2 authors\n  contributed equally to this work", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9414478", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of combining multiple languages' recordings to train a single\nautomatic speech recognition (ASR) model brings the promise of the emergence of\nuniversal speech representation. Recently, a Transformer encoder-decoder model\nhas been shown to leverage multilingual data well in IPA transcriptions of\nlanguages presented during training. However, the representations it learned\nwere not successful in zero-shot transfer to unseen languages. Because that\nmodel lacks an explicit factorization of the acoustic model (AM) and language\nmodel (LM), it is unclear to what degree the performance suffered from\ndifferences in pronunciation or the mismatch in phonotactics. To gain more\ninsight into the factors limiting zero-shot ASR transfer, we replace the\nencoder-decoder with a hybrid ASR system consisting of a separate AM and LM.\nThen, we perform an extensive evaluation of monolingual, multilingual, and\ncrosslingual (zero-shot) acoustic and language models on a set of 13\nphonetically diverse languages. We show that the gain from modeling\ncrosslingual phonotactics is limited, and imposing a too strong model can hurt\nthe zero-shot transfer. Furthermore, we find that a multilingual LM hurts a\nmultilingual ASR system's performance, and retaining only the target language's\nphonotactic data in LM training is preferable.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 23:07:24 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:53:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Feng", "Siyuan", ""], ["\u017belasko", "Piotr", ""], ["Moro-Vel\u00e1zquez", "Laureano", ""], ["Abavisani", "Ali", ""], ["Hasegawa-Johnson", "Mark", ""], ["Scharenborg", "Odette", ""], ["Dehak", "Najim", ""]]}, {"id": "2010.12121", "submitter": "Feiliang Ren", "authors": "Feiliang Ren, Juchen Li, Huihui Zhang, Shilei Liu, Bochao Li, Ruicheng\n  Ming, Yujia Bai", "title": "Knowledge Graph Embedding with Atrous Convolution and Residual Learning", "comments": "Accepted by COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding is an important task and it will benefit lots of\ndownstream applications. Currently, deep neural networks based methods achieve\nstate-of-the-art performance. However, most of these existing methods are very\ncomplex and need much time for training and inference. To address this issue,\nwe propose a simple but effective atrous convolution based knowledge graph\nembedding method. Compared with existing state-of-the-art methods, our method\nhas following main characteristics. First, it effectively increases feature\ninteractions by using atrous convolutions. Second, to address the original\ninformation forgotten issue and vanishing/exploding gradient issue, it uses the\nresidual learning method. Third, it has simpler structure but much higher\nparameter efficiency. We evaluate our method on six benchmark datasets with\ndifferent evaluation metrics. Extensive experiments show that our model is very\neffective. On these diverse datasets, it achieves better results than the\ncompared state-of-the-art methods on most of evaluation metrics. The source\ncodes of our model could be found at https://github.com/neukg/AcrE.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 00:57:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 06:07:38 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ren", "Feiliang", ""], ["Li", "Juchen", ""], ["Zhang", "Huihui", ""], ["Liu", "Shilei", ""], ["Li", "Bochao", ""], ["Ming", "Ruicheng", ""], ["Bai", "Yujia", ""]]}, {"id": "2010.12136", "submitter": "Bowen Li", "authors": "Bowen Li, Xiaojuan Qi, Philip H. S. Torr, Thomas Lukasiewicz", "title": "Lightweight Generative Adversarial Networks for Text-Guided Image\n  Manipulation", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel lightweight generative adversarial network for efficient\nimage manipulation using natural language descriptions. To achieve this, a new\nword-level discriminator is proposed, which provides the generator with\nfine-grained training feedback at word-level, to facilitate training a\nlightweight generator that has a small number of parameters, but can still\ncorrectly focus on specific visual attributes of an image, and then edit them\nwithout affecting other contents that are not described in the text.\nFurthermore, thanks to the explicit training signal related to each word, the\ndiscriminator can also be simplified to have a lightweight structure. Compared\nwith the state of the art, our method has a much smaller number of parameters,\nbut still achieves a competitive manipulation performance. Extensive\nexperimental results demonstrate that our method can better disentangle\ndifferent visual attributes, then correctly map them to corresponding semantic\nwords, and thus achieve a more accurate image modification using natural\nlanguage descriptions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 02:43:02 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Li", "Bowen", ""], ["Qi", "Xiaojuan", ""], ["Torr", "Philip H. S.", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2010.12148", "submitter": "Yu-Kun Li", "authors": "Dongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun, Hao Tian, Hua Wu and\n  Haifeng Wang", "title": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling\n  for Natural Language Understanding", "comments": "Accepted by NAACL-HLT 2021. Codes will be released at\n  https://github.com/PaddlePaddle/ERNIE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse-grained linguistic information, such as named entities or phrases,\nfacilitates adequately representation learning in pre-training. Previous works\nmainly focus on extending the objective of BERT's Masked Language Modeling\n(MLM) from masking individual tokens to contiguous sequences of n tokens. We\nargue that such contiguously masking method neglects to model the\nintra-dependencies and inter-relation of coarse-grained linguistic information.\nAs an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method\nto enhance the integration of coarse-grained information into pre-training. In\nERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram\nidentities rather than contiguous sequences of n tokens. Furthermore,\nERNIE-Gram employs a generator model to sample plausible n-gram identities as\noptional n-gram masks and predict them in both coarse-grained and fine-grained\nmanners to enable comprehensive n-gram prediction and relation modeling. We\npre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19\ndownstream tasks. Experimental results show that ERNIE-Gram outperforms\nprevious pre-training models like XLNet and RoBERTa by a large margin, and\nachieves comparable results with state-of-the-art methods. The source codes and\npre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 03:42:20 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 07:14:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xiao", "Dongling", ""], ["Li", "Yu-Kun", ""], ["Zhang", "Han", ""], ["Sun", "Yu", ""], ["Tian", "Hao", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2010.12155", "submitter": "Menglong Xu", "authors": "Menglong Xu, Shengqiang Li, Xiao-Lei Zhang", "title": "Transformer-based End-to-End Speech Recognition with Local Dense\n  Synthesizer Attention", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9414353", "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several studies reported that dot-product selfattention (SA) may\nnot be indispensable to the state-of-theart Transformer models. Motivated by\nthe fact that dense synthesizer attention (DSA), which dispenses with dot\nproducts and pairwise interactions, achieved competitive results in many\nlanguage processing tasks, in this paper, we first propose a DSA-based speech\nrecognition, as an alternative to SA. To reduce the computational complexity\nand improve the performance, we further propose local DSA (LDSA) to restrict\nthe attention scope of DSA to a local range around the current central frame\nfor speech recognition. Finally, we combine LDSA with SA to extract the local\nand global information simultaneously. Experimental results on the Ai-shell1\nMandarine speech recognition corpus show that the proposed LDSA-Transformer\nachieves a character error rate (CER) of 6.49%, which is slightly better than\nthat of the SA-Transformer. Meanwhile, the LDSA-Transformer requires less\ncomputation than the SATransformer. The proposed combination method not only\nachieves a CER of 6.18%, which significantly outperforms the SA-Transformer,\nbut also has roughly the same number of parameters and computational complexity\nas the latter. The implementation of the multi-head LDSA is available at\nhttps://github.com/mlxu995/multihead-LDSA.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 04:13:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 02:38:06 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 03:52:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Menglong", ""], ["Li", "Shengqiang", ""], ["Zhang", "Xiao-Lei", ""]]}, {"id": "2010.12156", "submitter": "Fei Zhao", "authors": "Fei Zhao, Zhen Wu, Xinyu Dai", "title": "Attention Transfer Network for Aspect-level Sentiment Classification", "comments": "Accept to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-level sentiment classification (ASC) aims to detect the sentiment\npolarity of a given opinion target in a sentence. In neural network-based\nmethods for ASC, most works employ the attention mechanism to capture the\ncorresponding sentiment words of the opinion target, then aggregate them as\nevidence to infer the sentiment of the target. However, aspect-level datasets\nare all relatively small-scale due to the complexity of annotation. Data\nscarcity causes the attention mechanism sometimes to fail to focus on the\ncorresponding sentiment words of the target, which finally weakens the\nperformance of neural models. To address the issue, we propose a novel\nAttention Transfer Network (ATN) in this paper, which can successfully exploit\nattention knowledge from resource-rich document-level sentiment classification\ndatasets to improve the attention capability of the aspect-level sentiment\nclassification task. In the ATN model, we design two different methods to\ntransfer attention knowledge and conduct experiments on two ASC benchmark\ndatasets. Extensive experimental results show that our methods consistently\noutperform state-of-the-art works. Further analysis also validates the\neffectiveness of ATN.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 04:26:33 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Zhao", "Fei", ""], ["Wu", "Zhen", ""], ["Dai", "Xinyu", ""]]}, {"id": "2010.12174", "submitter": "Rubungo Andre Niyongabo", "authors": "Rubungo Andre Niyongabo and Hong Qu and Julia Kreutzer and Li Huang", "title": "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for\n  Kinyarwanda and Kirundi", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in text classification has been focused on high-resource\nlanguages such as English and Chinese. For low-resource languages, amongst them\nmost African languages, the lack of well-annotated data and effective\npreprocessing, is hindering the progress and the transfer of successful\nmethods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS)\nfor multi-class classification of news articles in Kinyarwanda and Kirundi, two\nlow-resource African languages. The two languages are mutually intelligible,\nbut while Kinyarwanda has been studied in Natural Language Processing (NLP) to\nsome extent, this work constitutes the first study on Kirundi. Along with the\ndatasets, we provide statistics, guidelines for preprocessing, and monolingual\nand cross-lingual baseline models. Our experiments show that training\nembeddings on the relatively higher-resourced Kinyarwanda yields successful\ncross-lingual transfer to Kirundi. In addition, the design of the created\ndatasets allows for a wider use in NLP beyond text classification in future\nstudies, such as representation learning, cross-lingual learning with more\ndistant languages, or as base for new annotations for tasks such as parsing,\nPOS tagging, and NER. The datasets, stopwords, and pre-trained embeddings are\npublicly available at https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus .\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 05:37:42 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Niyongabo", "Rubungo Andre", ""], ["Qu", "Hong", ""], ["Kreutzer", "Julia", ""], ["Huang", "Li", ""]]}, {"id": "2010.12180", "submitter": "Sanyuan Chen", "authors": "Sanyuan Chen, Yu Wu, Zhuo Chen, Takuya Yoshioka, Shujie Liu, Jinyu Li", "title": "Don't shoot butterfly with rifles: Multi-channel Continuous Speech\n  Separation with Early Exit Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With its strong modeling capacity that comes from a multi-head and\nmulti-layer structure, Transformer is a very powerful model for learning a\nsequential representation and has been successfully applied to speech\nseparation recently. However, multi-channel speech separation sometimes does\nnot necessarily need such a heavy structure for all time frames especially when\nthe cross-talker challenge happens only occasionally. For example, in\nconversation scenarios, most regions contain only a single active speaker,\nwhere the separation task downgrades to a single speaker enhancement problem.\nIt turns out that using a very deep network structure for dealing with signals\nwith a low overlap ratio not only negatively affects the inference efficiency\nbut also hurts the separation performance. To deal with this problem, we\npropose an early exit mechanism, which enables the Transformer model to handle\ndifferent cases with adaptive depth. Experimental results indicate that not\nonly does the early exit mechanism accelerate the inference, but it also\nimproves the accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 06:21:11 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chen", "Sanyuan", ""], ["Wu", "Yu", ""], ["Chen", "Zhuo", ""], ["Yoshioka", "Takuya", ""], ["Liu", "Shujie", ""], ["Li", "Jinyu", ""]]}, {"id": "2010.12183", "submitter": "Weizhe Lin", "authors": "Zhilin Wang, Weizhe Lin, Xiaodong Wu", "title": "Learning Similarity between Movie Characters and Its Potential\n  Implications on Understanding Human Experiences", "comments": "To appear in Proceedings of the 2021 NAACL Workshop WNU: 3rd Workshop\n  on Narrative Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many different aspects of human experiences have been studied by the\nNLP community, none has captured its full richness. We propose a new task to\ncapture this richness based on an unlikely setting: movie characters. We sought\nto capture theme-level similarities between movie characters that were\ncommunity-curated into 20,000 themes. By introducing a two-step approach that\nbalances performance and efficiency, we managed to achieve 9-27\\% improvement\nover recent paragraph-embedding based methods. Finally, we demonstrate how the\nthematic information learnt from movie characters can potentially be used to\nunderstand themes in the experience of people, as indicated on Reddit posts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 06:28:25 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:35:18 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Zhilin", ""], ["Lin", "Weizhe", ""], ["Wu", "Xiaodong", ""]]}, {"id": "2010.12198", "submitter": "Abhinav Ramesh Kashyap", "authors": "Abhinav Ramesh Kashyap, Devamanyu Hazarika, Min-Yen Kan, Roger\n  Zimmermann", "title": "Domain Divergences: a Survey and Empirical Analysis", "comments": "Accepted for publication in 2021 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain divergence plays a significant role in estimating the performance of a\nmodel in new domains. While there is a significant literature on divergence\nmeasures, researchers find it hard to choose an appropriate divergence for a\ngiven NLP application. We address this shortcoming by both surveying the\nliterature and through an empirical study. We develop a taxonomy of divergence\nmeasures consisting of three classes -- Information-theoretic, Geometric, and\nHigher-order measures and identify the relationships between them. Further, to\nunderstand the common use-cases of these measures, we recognise three novel\napplications -- 1) Data Selection, 2) Learning Representation, and 3) Decisions\nin the Wild -- and use it to organise our literature. From this, we identify\nthat Information-theoretic measures are prevalent for 1) and 3), and\nHigher-order measures are more common for 2). To further help researchers\nchoose appropriate measures to predict drop in performance -- an important\naspect of Decisions in the Wild, we perform correlation analysis spanning 130\ndomain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures\nidentified from our survey. To calculate these divergences, we consider the\ncurrent contextual word representations (CWR) and contrast with the older\ndistributed representations. We find that traditional measures over word\ndistributions still serve as strong baselines, while higher-order measures with\nCWR are effective.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 07:12:52 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 06:47:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kashyap", "Abhinav Ramesh", ""], ["Hazarika", "Devamanyu", ""], ["Kan", "Min-Yen", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2010.12223", "submitter": "Richard Moot", "authors": "Richard Moot (TEXTE, LIRMM, CNRS)", "title": "Proof-theoretic aspects of NL$\\lambda$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof-theoretic analysis of the logic NL$\\lambda$ (Barker \\&\nShan 2014, Barker 2019). We notably introduce a novel calculus of proof nets\nand prove it is sound and complete with respect to the sequent calculus for the\nlogic. We study decidability and complexity of the logic using this new\ncalculus, proving a new upper bound for complexity of the logic (showing it is\nin NP) and a new lower bound for the class of formal language generated by the\nformalism (mildly context-sensitive languages extended with a permutation\nclosure operation). Finally, thanks to this new calculus, we present a novel\ncomparison between NL$\\lambda$ and the hybrid type-logical grammars of Kubota\n\\& Levine (2020). We show there is an unexpected convergence of the natural\nlanguage analyses proposed in the two formalism. In addition to studying the\nproof-theoretic properties of NL$\\lambda$, we greatly extends its linguistic\ncoverage.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:13:39 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Moot", "Richard", "", "TEXTE, LIRMM, CNRS"]]}, {"id": "2010.12231", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Yi-Chiao Wu, Tomoki Hayashi, Tomoki Toda", "title": "Any-to-One Sequence-to-Sequence Voice Conversion using Self-Supervised\n  Discrete Speech Representations", "comments": "Submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to any-to-one (A2O) voice conversion (VC) in a\nsequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker,\nincluding those unseen during training, to a fixed target speaker. We utilize\nvq-wav2vec (VQW2V), a discretized self-supervised speech representation that\nwas learned from massive unlabeled data, which is assumed to be\nspeaker-independent and well corresponds to underlying linguistic contents.\nGiven a training dataset of the target speaker, we extract VQW2V and acoustic\nfeatures to estimate a seq2seq mapping function from the former to the latter.\nWith the help of a pretraining method and a newly designed postprocessing\ntechnique, our model can be generalized to only 5 min of data, even\noutperforming the same model trained with parallel data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:34:52 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Wu", "Yi-Chiao", ""], ["Hayashi", "Tomoki", ""], ["Toda", "Tomoki", ""]]}, {"id": "2010.12251", "submitter": "Sunghyun Park", "authors": "Sunghyun Park, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee,\n  Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya", "title": "A Scalable Framework for Learning From Implicit User Feedback to Improve\n  Natural Language Understanding in Large-Scale Conversational AI Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Understanding (NLU) is an established component within a\nconversational AI or digital assistant system, and it is responsible for\nproducing semantic understanding of a user request. We propose a scalable and\nautomatic approach for improving NLU in a large-scale conversational AI system\nby leveraging implicit user feedback, with an insight that user interaction\ndata and dialog context have rich information embedded from which user\nsatisfaction and intention can be inferred. In particular, we propose a general\ndomain-agnostic framework for curating new supervision data for improving NLU\nfrom live production traffic. With an extensive set of experiments, we show the\nresults of applying the framework and improving NLU for a large-scale\nproduction system and show its impact across 10 domains.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:23:44 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Park", "Sunghyun", ""], ["Li", "Han", ""], ["Patel", "Ameen", ""], ["Mudgal", "Sidharth", ""], ["Lee", "Sungjin", ""], ["Kim", "Young-Bum", ""], ["Matsoukas", "Spyros", ""], ["Sarikaya", "Ruhi", ""]]}, {"id": "2010.12267", "submitter": "Xinsheng Wang", "authors": "Xinsheng Wang, Siyuan Feng, Jihua Zhu, Mark Hasegawa-Johnson, Odette\n  Scharenborg", "title": "Show and Speak: Directly Synthesize Spoken Description of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new model, referred to as the show and speak (SAS)\nmodel that, for the first time, is able to directly synthesize spoken\ndescriptions of images, bypassing the need for any text or phonemes. The basic\nstructure of SAS is an encoder-decoder architecture that takes an image as\ninput and predicts the spectrogram of speech that describes this image. The\nfinal speech audio is obtained from the predicted spectrogram via WaveNet.\nExtensive experiments on the public benchmark database Flickr8k demonstrate\nthat the proposed SAS is able to synthesize natural spoken descriptions for\nimages, indicating that synthesizing spoken descriptions for images while\nbypassing text and phonemes is feasible.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:53:01 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:58:19 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Xinsheng", ""], ["Feng", "Siyuan", ""], ["Zhu", "Jihua", ""], ["Hasegawa-Johnson", "Mark", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2010.12272", "submitter": "Zhen Ke", "authors": "Zhen Ke, Liang Shi, Songtao Sun, Erli Meng, Bin Wang, Xipeng Qiu", "title": "Pre-training with Meta Learning for Chinese Word Segmentation", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches show that pre-trained models (PTMs) are beneficial to\nChinese Word Segmentation (CWS). However, PTMs used in previous works usually\nadopt language modeling as pre-training tasks, lacking task-specific prior\nsegmentation knowledge and ignoring the discrepancy between pre-training tasks\nand downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained\nmodel METASEG, which employs a unified architecture and incorporates meta\nlearning algorithm into a multi-criteria pre-training task. Empirical results\nshow that METASEG could utilize common prior segmentation knowledge from\ndifferent existing criteria and alleviate the discrepancy between pre-trained\nmodels and downstream CWS tasks. Besides, METASEG can achieve new\nstate-of-the-art performance on twelve widely-used CWS datasets and\nsignificantly improve model performance in low-resource settings.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 10:00:46 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 07:15:20 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ke", "Zhen", ""], ["Shi", "Liang", ""], ["Sun", "Songtao", ""], ["Meng", "Erli", ""], ["Wang", "Bin", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2010.12283", "submitter": "Minjeong Kim", "authors": "Minjeong Kim, Gyuwan Kim, Sang-Woo Lee, Jung-Woo Ha", "title": "ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken\n  Language Understanding", "comments": "ICASSP 2021; 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language model pre-training has shown promising results in various downstream\ntasks. In this context, we introduce a cross-modal pre-trained language model,\ncalled Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language\nunderstanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text\nas an input, ST-BERT learns a contextualized cross-modal alignment via our two\nproposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and\nCross-modal Conditioned Language Modeling (CM-CLM). Experimental results on\nthree benchmarks present that our approach is effective for various SLU\ndatasets and shows a surprisingly marginal performance degradation even when 1%\nof the training data are available. Also, our method shows further SLU\nperformance gain via domain-adaptive pre-training with domain-specific\nspeech-text pair data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 10:28:20 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 13:52:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kim", "Minjeong", ""], ["Kim", "Gyuwan", ""], ["Lee", "Sang-Woo", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "2010.12305", "submitter": "Lukas Lange", "authors": "Lukas Lange, Heike Adel, Jannik Str\\\"otgen, Dietrich Klakow", "title": "Adversarial Learning of Feature-based Meta-Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain embedding types outperform others in different scenarios, e.g.,\nsubword-based embeddings can model rare words well and domain-specific\nembeddings can better represent in-domain terms. Therefore, recent works\nconsider attention-based meta-embeddings to combine different embedding types.\nWe demonstrate that these methods have two shortcomings: First, the attention\nweights are calculated without knowledge of word properties. Second, the\ndifferent embedding types can form clusters in the common embedding space,\npreventing the computation of a meaningful average of different embeddings and\nthus, reducing performance. We propose to solve these problems by using\nfeature-based meta-embeddings learned with adversarial training. Our\nexperiments and analysis on sentence classification and sequence tagging tasks\nshow that our approach is effective. We set the new state of the art on various\ndatasets across languages and domains.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:16:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lange", "Lukas", ""], ["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.12309", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Str\\\"otgen,\n  Dietrich Klakow", "title": "A Survey on Recent Approaches for Natural Language Processing in\n  Low-Resource Scenarios", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and huge language models are becoming omnipresent in\nnatural language applications. As they are known for requiring large amounts of\ntraining data, there is a growing body of work to improve the performance in\nlow-resource settings. Motivated by the recent fundamental changes towards\nneural models and the popular pre-train and fine-tune paradigm, we survey\npromising approaches for low-resource natural language processing. After a\ndiscussion about the different dimensions of data availability, we give a\nstructured overview of methods that enable learning when training data is\nsparse. This includes mechanisms to create additional labeled data like data\naugmentation and distant supervision as well as transfer learning settings that\nreduce the need for target supervision. A goal of our survey is to explain how\nthese methods differ in their requirements as understanding them is essential\nfor choosing a technique suited for a specific low-resource setting. Further\nkey aspects of this work are to highlight open issues and to outline promising\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:22:01 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:11:47 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 13:48:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Lange", "Lukas", ""], ["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.12321", "submitter": "Antoine Tixier", "authors": "Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis", "title": "BARThez: a Skilled Pretrained French Sequence-to-Sequence Model", "comments": "More experiments and results, human evaluation, reorganization of\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive transfer learning has taken the entire NLP field by storm, with\nmodels such as BERT and BART setting new state of the art on countless NLU\ntasks. However, most of the available models and research have been conducted\nfor English. In this work, we introduce BARThez, the first large-scale\npretrained seq2seq model for French. Being based on BART, BARThez is\nparticularly well-suited for generative tasks. We evaluate BARThez on five\ndiscriminative tasks from the FLUE benchmark and two generative tasks from a\nnovel summarization dataset, OrangeSum, that we created for this research. We\nshow BARThez to be very competitive with state-of-the-art BERT-based French\nlanguage models such as CamemBERT and FlauBERT. We also continue the\npretraining of a multilingual BART on BARThez' corpus, and show our resulting\nmodel, mBARThez, to significantly boost BARThez' generative performance. Code,\ndata and models are publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:57:33 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 09:31:57 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Eddine", "Moussa Kamal", ""], ["Tixier", "Antoine J. -P.", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2010.12322", "submitter": "Lukas Lange", "authors": "Lukas Lange, Xiang Dai, Heike Adel, Jannik Str\\\"otgen", "title": "NLNDE at CANTEMIST: Neural Sequence Labeling and Parsing Approaches for\n  Clinical Concept Extraction", "comments": "IberLEF 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition and normalization of clinical information, such as tumor\nmorphology mentions, is an important, but complex process consisting of\nmultiple subtasks. In this paper, we describe our system for the CANTEMIST\nshared task, which is able to extract, normalize and rank ICD codes from\nSpanish electronic health records using neural sequence labeling and parsing\napproaches with context-aware embeddings. Our best system achieves 85.3 F1,\n76.7 F1, and 77.0 MAP for the three tasks, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:59:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lange", "Lukas", ""], ["Dai", "Xiang", ""], ["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""]]}, {"id": "2010.12401", "submitter": "Gaurish Thakkar Mr", "authors": "Gaurish Thakkar, Marcis Pinnis", "title": "Pretraining and Fine-Tuning Strategies for Sentiment Analysis of Latvian\n  Tweets", "comments": null, "journal-ref": null, "doi": "10.3233/FAIA200602", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present various pre-training strategies that aid in\nim-proving the accuracy of the sentiment classification task. We, at first,\npre-trainlanguage representation models using these strategies and then\nfine-tune them onthe downstream task. Experimental results on a time-balanced\ntweet evaluation setshow the improvement over the previous technique. We\nachieve 76% accuracy forsentiment analysis on Latvian tweets, which is a\nsubstantial improvement over pre-vious work\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:45:33 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Thakkar", "Gaurish", ""], ["Pinnis", "Marcis", ""]]}, {"id": "2010.12405", "submitter": "Xin Li", "authors": "Xin Li, Lidong Bing, Wenxuan Zhang, Zheng Li, Wai Lam", "title": "Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual adaptation with multilingual pre-trained language models\n(mPTLMs) mainly consists of two lines of works: zero-shot approach and\ntranslation-based approach, which have been studied extensively on the\nsequence-level tasks. We further verify the efficacy of these cross-lingual\nadaptation approaches by evaluating their performances on more fine-grained\nsequence tagging tasks. After re-examining their strengths and drawbacks, we\npropose a novel framework to consolidate the zero-shot approach and the\ntranslation-based approach for better adaptation performance. Instead of simply\naugmenting the source data with the machine-translated data, we tailor-make a\nwarm-up mechanism to quickly update the mPTLMs with the gradients estimated on\na few translated data. Then, the adaptation approach is applied to the refined\nparameters and the cross-lingual transfer is performed in a warm-start way. The\nexperimental results on nine target languages demonstrate that our method is\nbeneficial to the cross-lingual adaptation of various sequence tagging tasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:47:01 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:52:09 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 13:52:14 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Li", "Xin", ""], ["Bing", "Lidong", ""], ["Zhang", "Wenxuan", ""], ["Li", "Zheng", ""], ["Lam", "Wai", ""]]}, {"id": "2010.12406", "submitter": "Gaurish Thakkar Mr", "authors": "Diego Alves, Tin Kuculo, Gabriel Amaral, Gaurish Thakkar, and Marko\n  Tadic", "title": "UNER: Universal Named-Entity RecognitionFramework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Universal Named-Entity Recognition (UNER)framework, a\n4-level classification hierarchy, and the methodology that isbeing adopted to\ncreate the first multilingual UNER corpus: the SETimesparallel corpus annotated\nfor named-entities. First, the English SETimescorpus will be annotated using\nexisting tools and knowledge bases. Afterevaluating the resulting annotations\nthrough crowdsourcing campaigns,they will be propagated automatically to other\nlanguages within the SE-Times corpora. Finally, as an extrinsic evaluation, the\nUNER multilin-gual dataset will be used to train and test available NER tools.\nAs part offuture research directions, we aim to increase the number of\nlanguages inthe UNER corpus and to investigate possible ways of integrating\nUNERwith available knowledge graphs to improve named-entity recognition.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:53:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Alves", "Diego", ""], ["Kuculo", "Tin", ""], ["Amaral", "Gabriel", ""], ["Thakkar", "Gaurish", ""], ["Tadic", "Marko", ""]]}, {"id": "2010.12412", "submitter": "Ohad Rubin", "authors": "Ohad Rubin and Jonathan Berant", "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing", "comments": "Accepted to NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The de-facto standard decoding method for semantic parsing in recent years\nhas been to autoregressively decode the abstract syntax tree of the target\nprogram using a top-down depth-first traversal. In this work, we propose an\nalternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that\nconstructs at decoding step $t$ the top-$K$ sub-trees of height $\\leq t$. Our\nparser enjoys several benefits compared to top-down autoregressive parsing.\nFrom an efficiency perspective, bottom-up parsing allows to decode all\nsub-trees of a certain height in parallel, leading to logarithmic runtime\ncomplexity rather than linear. From a modeling perspective, a bottom-up parser\nlearns representations for meaningful semantic sub-programs at each step,\nrather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a\nchallenging zero-shot semantic parsing benchmark, and show that SmBoP leads to\na 2.2x speed-up in decoding time and a $\\sim$5x speed-up in training time,\ncompared to a semantic parser that uses autoregressive decoding. SmBoP obtains\n71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and\n69.5 exact match, comparable to the 69.6 exact match of the autoregressive\nRAT-SQL+GraPPa.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:02:32 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:37:59 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Rubin", "Ohad", ""], ["Berant", "Jonathan", ""]]}, {"id": "2010.12418", "submitter": "Ahmed Al-Ali", "authors": "Ahmed Ghanim Al-Ali, Robert Phaal, Donald Sull", "title": "Deep Learning Framework for Measuring the Digital Strategy of Companies\n  from Earnings Calls", "comments": "Proceedings of The 28th International Conference on Computational\n  Linguistics, 9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Companies today are racing to leverage the latest digital technologies, such\nas artificial intelligence, blockchain, and cloud computing. However, many\ncompanies report that their strategies did not achieve the anticipated business\nresults. This study is the first to apply state of the art NLP models on\nunstructured data to understand the different clusters of digital strategy\npatterns that companies are Adopting. We achieve this by analyzing earnings\ncalls from Fortune Global 500 companies between 2015 and 2019. We use\nTransformer based architecture for text classification which show a better\nunderstanding of the conversation context. We then investigate digital strategy\npatterns by applying clustering analysis. Our findings suggest that Fortune 500\ncompanies use four distinct strategies which are product led, customer\nexperience led, service led, and efficiency led. This work provides an\nempirical baseline for companies and researchers to enhance our understanding\nof the field.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:07:12 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 04:36:18 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Al-Ali", "Ahmed Ghanim", ""], ["Phaal", "Robert", ""], ["Sull", "Donald", ""]]}, {"id": "2010.12421", "submitter": "Jose Camacho-Collados", "authors": "Francesco Barbieri and Jose Camacho-Collados and Leonardo Neves and\n  Luis Espinosa-Anke", "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet\n  Classification", "comments": "Findings of EMNLP 2020. TweetEval benchmark available at\n  https://github.com/cardiffnlp/tweeteval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental landscape in natural language processing for social media is\ntoo fragmented. Each year, new shared tasks and datasets are proposed, ranging\nfrom classics like sentiment analysis to irony detection or emoji prediction.\nTherefore, it is unclear what the current state of the art is, as there is no\nstandardized evaluation protocol, neither a strong set of baselines trained on\nsuch domain-specific data. In this paper, we propose a new evaluation framework\n(TweetEval) consisting of seven heterogeneous Twitter-specific classification\ntasks. We also provide a strong set of baselines as starting point, and compare\ndifferent language modeling pre-training strategies. Our initial experiments\nshow the effectiveness of starting off with existing pre-trained generic\nlanguage models, and continue training them on Twitter corpora.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:11:04 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 09:14:54 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Barbieri", "Francesco", ""], ["Camacho-Collados", "Jose", ""], ["Neves", "Leonardo", ""], ["Espinosa-Anke", "Luis", ""]]}, {"id": "2010.12428", "submitter": "Gaurish Thakkar Mr", "authors": "Diego Alves, Gaurish Thakkar, Marko Tadi\\'c", "title": "Evaluating Language Tools for Fifteen EU-official Under-resourced\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the results of the evaluation campaign of language\ntools available for fifteen EU-official under-resourced languages. The\nevaluation was conducted within the MSC ITN CLEOPATRA action that aims at\nbuilding the cross-lingual event-centric knowledge processing on top of the\napplication of linguistic processing chains (LPCs) for at least 24 EU-official\nlanguages. In this campaign, we concentrated on three existing NLP platforms\n(Stanford CoreNLP, NLP Cube, UDPipe) that all provide models for\nunder-resourced languages and in this first run we covered 15 under-resourced\nlanguages for which the models were available. We present the design of the\nevaluation campaign and present the results as well as discuss them. We\nconsidered the difference between reported and our tested results within a\nsingle percentage point as being within the limits of acceptable tolerance and\nthus consider this result as reproducible. However, for a number of languages,\nthe results are below what was reported in the literature, and in some cases,\nour testing results are even better than the ones reported previously.\nParticularly problematic was the evaluation of NERC systems. One of the reasons\nis the absence of universally or cross-lingually applicable named entities\nclassification scheme that would serve the NERC task in different languages\nanalogous to the Universal Dependency scheme in parsing task. To build such a\nscheme has become one of our the future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:21:03 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Alves", "Diego", ""], ["Thakkar", "Gaurish", ""], ["Tadi\u0107", "Marko", ""]]}, {"id": "2010.12433", "submitter": "Gaurish Thakkar Mr", "authors": "Diego Alves, Gaurish Thakkar, Marko Tadi\\'c", "title": "Natural Language Processing Chains Inside a Cross-lingual Event-Centric\n  Knowledge Pipeline for European Union Under-resourced Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the strategy for developing a platform containing\nLanguage Processing Chains for European Union languages, consisting of\nTokenization to Parsing, also including Named Entity recognition andwith\naddition ofSentiment Analysis. These chains are part of the first step of an\nevent-centric knowledge processing pipeline whose aim is to process\nmultilingual media information about major events that can cause an impactin\nEurope and the rest of the world. Due to the differences in terms of\navailability of language resources for each language, we have built this\nstrategy in three steps, starting with processing chains for the well-resourced\nlanguages and finishing with the development of new modules for the\nunder-resourced ones. In order to classify all European Union official\nlanguages in terms of resources, we have analysed the size of annotated corpora\nas well as the existence of pre-trained models in mainstream Language\nProcessing tools, and we have combined this information with the proposed\nclassification published at META-NETwhitepaper series.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:26:30 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Alves", "Diego", ""], ["Thakkar", "Gaurish", ""], ["Tadi\u0107", "Marko", ""]]}, {"id": "2010.12472", "submitter": "Tommaso Caselli", "authors": "Tommaso Caselli, Valerio Basile, Jelena Mitrovi\\'c, Michael Granitzer", "title": "HateBERT: Retraining BERT for Abusive Language Detection in English", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce HateBERT, a re-trained BERT model for abusive\nlanguage detection in English. The model was trained on RAL-E, a large-scale\ndataset of Reddit comments in English from communities banned for being\noffensive, abusive, or hateful that we have collected and made available to the\npublic. We present the results of a detailed comparison between a general\npre-trained language model and the abuse-inclined version obtained by\nretraining with posts from the banned communities on three English datasets for\noffensive, abusive language and hate speech detection tasks. In all datasets,\nHateBERT outperforms the corresponding general BERT model. We also discuss a\nbattery of experiments comparing the portability of the generic pre-trained\nlanguage model and its corresponding abusive language-inclined counterpart\nacross the datasets, indicating that portability is affected by compatibility\nof the annotated phenomena.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:14:14 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 10:00:07 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Caselli", "Tommaso", ""], ["Basile", "Valerio", ""], ["Mitrovi\u0107", "Jelena", ""], ["Granitzer", "Michael", ""]]}, {"id": "2010.12473", "submitter": "Henning Wachsmuth", "authors": "Henning Wachsmuth and Till Werner", "title": "Intrinsic Quality Assessment of Arguments", "comments": "Accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several quality dimensions of natural language arguments have been\ninvestigated. Some are likely to be reflected in linguistic features (e.g., an\nargument's arrangement), whereas others depend on context (e.g., relevance) or\ntopic knowledge (e.g., acceptability). In this paper, we study the intrinsic\ncomputational assessment of 15 dimensions, i.e., only learning from an\nargument's text. In systematic experiments with eight feature types on an\nexisting corpus, we observe moderate but significant learning success for most\ndimensions. Rhetorical quality seems hardest to assess, and subjectivity\nfeatures turn out strong, although length bias in the corpus impedes full\nvalidity. We also find that human assessors differ more clearly to each other\nthan to our approach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:16:10 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wachsmuth", "Henning", ""], ["Werner", "Till", ""]]}, {"id": "2010.12487", "submitter": "Damien Garreau", "authors": "Dina Mardaoui and Damien Garreau", "title": "An Analysis of LIME for Text Data", "comments": "29 pages, 17 figures, accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas \"black-boxes.\" Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:40:13 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 08:43:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Mardaoui", "Dina", ""], ["Garreau", "Damien", ""]]}, {"id": "2010.12495", "submitter": "Daniel Deutsch", "authors": "Daniel Deutsch, Dan Roth", "title": "Understanding the Extent to which Summarization Evaluation Metrics\n  Measure the Information Quality of Summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference-based metrics such as ROUGE or BERTScore evaluate the content\nquality of a summary by comparing the summary to a reference. Ideally, this\ncomparison should measure the summary's information quality by calculating how\nmuch information the summaries have in common. In this work, we analyze the\ntoken alignments used by ROUGE and BERTScore to compare summaries and argue\nthat their scores largely cannot be interpreted as measuring information\noverlap, but rather the extent to which they discuss the same topics. Further,\nwe provide evidence that this result holds true for many other summarization\nevaluation metrics. The consequence of this result is that it means the\nsummarization community has not yet found a reliable automatic metric that\naligns with its research goal, to generate summaries with high-quality\ninformation. Then, we propose a simple and interpretable method of evaluating\nsummaries which does directly measure information overlap and demonstrate how\nit can be used to gain insights into model behavior that could not be provided\nby other methods alone.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:55:15 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Deutsch", "Daniel", ""], ["Roth", "Dan", ""]]}, {"id": "2010.12497", "submitter": "Omid Ghahabi", "authors": "Omid Ghahabi, Volker Fischer", "title": "EML System Description for VoxCeleb Speaker Diarization Challenge 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report describes the EML submission to the first VoxCeleb\nspeaker diarization challenge. Although the aim of the challenge has been the\noffline processing of the signals, the submitted system is basically the EML\nonline algorithm which decides about the speaker labels in runtime\napproximately every 1.2 sec. For the first phase of the challenge, only\nVoxCeleb2 dev dataset was used for training. The results on the provided\nVoxConverse dev set show much better accuracy in terms of both DER and JER\ncompared to the offline baseline provided in the challenge. The real-time\nfactor of the whole diarization process is about 0.01 using a single CPU\nmachine.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:01:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ghahabi", "Omid", ""], ["Fischer", "Volker", ""]]}, {"id": "2010.12505", "submitter": "Tim Draws", "authors": "Tim Draws, Jody Liu, Nava Tintarev", "title": "Helping users discover perspectives: Enhancing opinion mining with joint\n  topic models", "comments": "Accepted at the SENTIRE workshop at ICDM 2020:\n  https://sentic.net/sentire/#2020", "journal-ref": "2020 International Conference on Data Mining Workshops (ICDMW)", "doi": "10.1109/ICDMW51313.2020.00013", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support or opposition concerning a debated claim such as abortion should be\nlegal can have different underlying reasons, which we call perspectives. This\npaper explores how opinion mining can be enhanced with joint topic modeling, to\nidentify distinct perspectives within the topic, providing an informative\noverview from unstructured text. We evaluate four joint topic models (TAM, JST,\nVODUM, and LAM) in a user study assessing human understandability of the\nextracted perspectives. Based on the results, we conclude that joint topic\nmodels such as TAM can discover perspectives that align with human judgments.\nMoreover, our results suggest that users are not influenced by their\npre-existing stance on the topic of abortion when interpreting the output of\ntopic models.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:13:06 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 20:28:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Draws", "Tim", ""], ["Liu", "Jody", ""], ["Tintarev", "Nava", ""]]}, {"id": "2010.12510", "submitter": "Nafise Sadat Moosavi", "authors": "Nafise Sadat Moosavi, Marcel de Boer, Prasetya Ajie Utama, Iryna\n  Gurevych", "title": "Improving Robustness by Augmenting Training Sentences with\n  Predicate-Argument Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing NLP datasets contain various biases, and models tend to quickly\nlearn those biases, which in turn limits their robustness. Existing approaches\nto improve robustness against dataset biases mostly focus on changing the\ntraining objective so that models learn less from biased examples. Besides,\nthey mostly focus on addressing a specific bias, and while they improve the\nperformance on adversarial evaluation sets of the targeted bias, they may bias\nthe model in other ways, and therefore, hurt the overall robustness. In this\npaper, we propose to augment the input sentences in the training data with\ntheir corresponding predicate-argument structures, which provide a higher-level\nabstraction over different realizations of the same meaning and help the model\nto recognize important parts of sentences. We show that without targeting a\nspecific bias, our sentence augmentation improves the robustness of transformer\nmodels against multiple biases. In addition, we show that models can still be\nvulnerable to the lexical overlap bias, even when the training data does not\ncontain this bias, and that the sentence augmentation also improves the\nrobustness in this scenario. We will release our adversarial datasets to\nevaluate bias in such a scenario as well as our augmentation scripts at\nhttps://github.com/UKPLab/data-augmentation-for-robustness.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:22:05 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Moosavi", "Nafise Sadat", ""], ["de Boer", "Marcel", ""], ["Utama", "Prasetya Ajie", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.12512", "submitter": "Linyi Yang", "authors": "Linyi Yang, Eoin M. Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and\n  Ruihai Dong", "title": "Generating Plausible Counterfactual Explanations for Deep Transformers\n  in Financial Text Classification", "comments": "Accepted by COLING-20 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Corporate mergers and acquisitions (M&A) account for billions of dollars of\ninvestment globally every year, and offer an interesting and challenging domain\nfor artificial intelligence. However, in these highly sensitive domains, it is\ncrucial to not only have a highly robust and accurate model, but be able to\ngenerate useful explanations to garner a user's trust in the automated system.\nRegrettably, the recent research regarding eXplainable AI (XAI) in financial\ntext classification has received little to no attention, and many current\nmethods for generating textual-based explanations result in highly implausible\nexplanations, which damage a user's trust in the system. To address these\nissues, this paper proposes a novel methodology for producing plausible\ncounterfactual explanations, whilst exploring the regularization benefits of\nadversarial training on language models in the domain of FinTech. Exhaustive\nquantitative experiments demonstrate that not only does this approach improve\nthe model accuracy when compared to the current state-of-the-art and human\nperformance, but it also generates counterfactual explanations which are\nsignificantly more plausible based on human trials.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:29:26 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yang", "Linyi", ""], ["Kenny", "Eoin M.", ""], ["Ng", "Tin Lok James", ""], ["Yang", "Yi", ""], ["Smyth", "Barry", ""], ["Dong", "Ruihai", ""]]}, {"id": "2010.12523", "submitter": "Yinfei Yang", "authors": "Jing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni, Yinfei Yang", "title": "Neural Passage Retrieval with Improved Negative Contrast", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the effects of negative sampling in dual encoder\nmodels used to retrieve passages for automatic question answering. We explore\nfour negative sampling strategies that complement the straightforward random\nsampling of negatives, typically used to train dual encoder models. Out of the\nfour strategies, three are based on retrieval and one on heuristics. Our\nretrieval-based strategies are based on the semantic similarity and the lexical\noverlap between questions and passages. We train the dual encoder models in two\nstages: pre-training with synthetic data and fine tuning with domain-specific\ndata. We apply negative sampling to both stages. The approach is evaluated in\ntwo passage retrieval tasks. Even though it is not evident that there is one\nsingle sampling strategy that works best in all the tasks, it is clear that our\nstrategies contribute to improving the contrast between the response and all\nthe other passages. Furthermore, mixing the negatives from different strategies\nachieve performance on par with the best performing strategy in all tasks. Our\nresults establish a new state-of-the-art level of performance on two of the\nopen-domain question answering datasets that we evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:45:06 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Lu", "Jing", ""], ["Abrego", "Gustavo Hernandez", ""], ["Ma", "Ji", ""], ["Ni", "Jianmo", ""], ["Yang", "Yinfei", ""]]}, {"id": "2010.12527", "submitter": "Peng Qi", "authors": "Peng Qi, Haejun Lee, Oghenetegiri \"TG\" Sido, Christopher D. Manning", "title": "Retrieve, Read, Rerank, then Iterate: Answering Open-Domain Questions of\n  Varying Reasoning Steps from Text", "comments": "Peng Qi, Haejun Lee, and TG Sido contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified system to answer directly from text open-domain\nquestions that may require a varying number of retrieval steps. We employ a\nsingle multi-task transformer model to perform all the necessary subtasks --\nretrieving supporting facts, reranking them, and predicting the answer from all\nretrieved documents -- in an iterative fashion. We avoid making crucial\nassumptions as previous work that do not transfer well to real-world settings,\nincluding exploiting knowledge of the fixed number of retrieval steps required\nto answer each question or using structured metadata like knowledge bases or\nweb links that have limited availability. Instead, we design a system that\nwould answer open-domain questions on any text collection without prior\nknowledge of reasoning complexity. To emulate this setting, we construct a new\nbenchmark by combining existing one- and two-step datasets with a new\ncollection of 203 questions that require three Wikipedia pages to answer,\nunifying Wikipedia corpora versions in the process. We show that our model\ndemonstrates competitive performance on both existing benchmarks and this new\nbenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:51:09 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 16:39:47 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 16:48:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Qi", "Peng", ""], ["Lee", "Haejun", ""], ["Sido", "Oghenetegiri \"TG\"", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2010.12532", "submitter": "Nicole Peinelt", "authors": "Nicole Peinelt, Marek Rei and Maria Liakata", "title": "GiBERT: Introducing Linguistic Knowledge into BERT through a Lightweight\n  Gated Injection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models such as BERT have been the driving force\nbehind recent improvements across many NLP tasks. However, BERT is only trained\nto predict missing words - either behind masks or in the next sentence - and\nhas no knowledge of lexical, syntactic or semantic information beyond what it\npicks up through unsupervised pre-training. We propose a novel method to\nexplicitly inject linguistic knowledge in the form of word embeddings into any\nlayer of a pre-trained BERT. Our performance improvements on multiple semantic\nsimilarity datasets when injecting dependency-based and counter-fitted\nembeddings indicate that such information is beneficial and currently missing\nfrom the original model. Our qualitative analysis shows that counter-fitted\nembedding injection particularly helps with cases involving synonym pairs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:00:26 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Peinelt", "Nicole", ""], ["Rei", "Marek", ""], ["Liakata", "Maria", ""]]}, {"id": "2010.12547", "submitter": "Lin Pan", "authors": "Lin Pan, Chung-Wei Hang, Haode Qi, Abhishek Shah, Saloni Potdar, Mo Yu", "title": "Multilingual BERT Post-Pretraining Alignment", "comments": "Accepted at NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method to align multilingual contextual embeddings as a\npost-pretraining step for improved zero-shot cross-lingual transferability of\nthe pretrained models. Using parallel data, our method aligns embeddings on the\nword level through the recently proposed Translation Language Modeling\nobjective as well as on the sentence level via contrastive learning and random\ninput shuffling. We also perform sentence-level code-switching with English\nwhen finetuning on downstream tasks. On XNLI, our best model (initialized from\nmBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves\ncomparable result to XLM for translate-train while using less than 18% of the\nsame parallel data and 31% less model parameters. On MLQA, our model\noutperforms XLM-R_Base that has 57% more parameters than ours.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:14:41 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 15:24:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Pan", "Lin", ""], ["Hang", "Chung-Wei", ""], ["Qi", "Haode", ""], ["Shah", "Abhishek", ""], ["Potdar", "Saloni", ""], ["Yu", "Mo", ""]]}, {"id": "2010.12562", "submitter": "Xiaotao Gu", "authors": "Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, Jiawei Han", "title": "On the Transformer Growth for Progressive BERT Training", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the excessive cost of large-scale language model pre-training,\nconsiderable efforts have been made to train BERT progressively -- start from\nan inferior but low-cost model and gradually grow the model to increase the\ncomputational complexity. Our objective is to advance the understanding of\nTransformer growth and discover principles that guide progressive training.\nFirst, we find that similar to network architecture search, Transformer growth\nalso favors compound scaling. Specifically, while existing methods only conduct\nnetwork growth in a single dimension, we observe that it is beneficial to use\ncompound growth operators and balance multiple dimensions (e.g., depth, width,\nand input length of the model). Moreover, we explore alternative growth\noperators in each dimension via controlled comparison to give operator\nselection practical guidance. In light of our analyses, the proposed method\nspeeds up BERT pre-training by 73.6% and 82.2% for the base and large models\nrespectively, while achieving comparable performances\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:44:59 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 22:32:35 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 06:42:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gu", "Xiaotao", ""], ["Liu", "Liyuan", ""], ["Yu", "Hongkun", ""], ["Li", "Jing", ""], ["Chen", "Chen", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.12563", "submitter": "Eric Wallace", "authors": "Eric Wallace, Tony Z. Zhao, Shi Feng, Sameer Singh", "title": "Concealed Data Poisoning Attacks on NLP Models", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks alter NLP model predictions by perturbing test-time\ninputs. However, it is much less understood whether, and how, predictions can\nbe manipulated with small, concealed changes to the training data. In this\nwork, we develop a new data poisoning attack that allows an adversary to\ncontrol model predictions whenever a desired trigger phrase is present in the\ninput. For instance, we insert 50 poison examples into a sentiment model's\ntraining set that causes the model to frequently predict Positive whenever the\ninput contains \"James Bond\". Crucially, we craft these poison examples using a\ngradient-based procedure so that they do not mention the trigger phrase. We\nalso apply our poison attack to language modeling (\"Apple iPhone\" triggers\nnegative generations) and machine translation (\"iced coffee\" mistranslated as\n\"hot coffee\"). We conclude by proposing three defenses that can mitigate our\nattack at some cost in prediction accuracy or extra human annotation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:47:06 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 09:10:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wallace", "Eric", ""], ["Zhao", "Tony Z.", ""], ["Feng", "Shi", ""], ["Singh", "Sameer", ""]]}, {"id": "2010.12566", "submitter": "Aditi Chaudhary", "authors": "Aditi Chaudhary, Karthik Raman, Krishna Srinivasan, Jiecao Chen", "title": "DICT-MLM: Improved Multilingual Pre-Training using Bilingual\n  Dictionaries", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained multilingual language models such as mBERT have shown immense\ngains for several natural language processing (NLP) tasks, especially in the\nzero-shot cross-lingual setting. Most, if not all, of these pre-trained models\nrely on the masked-language modeling (MLM) objective as the key language\nlearning objective. The principle behind these approaches is that predicting\nthe masked words with the help of the surrounding text helps learn potent\ncontextualized representations. Despite the strong representation learning\ncapability enabled by MLM, we demonstrate an inherent limitation of MLM for\nmultilingual representation learning. In particular, by requiring the model to\npredict the language-specific token, the MLM objective disincentivizes learning\na language-agnostic representation -- which is a key goal of multilingual\npre-training. Therefore to encourage better cross-lingual representation\nlearning we propose the DICT-MLM method. DICT-MLM works by incentivizing the\nmodel to be able to predict not just the original masked word, but potentially\nany of its cross-lingual synonyms as well. Our empirical analysis on multiple\ndownstream tasks spanning 30+ languages, demonstrates the efficacy of the\nproposed approach and its ability to learn better multilingual representations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:53:11 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chaudhary", "Aditi", ""], ["Raman", "Karthik", ""], ["Srinivasan", "Krishna", ""], ["Chen", "Jiecao", ""]]}, {"id": "2010.12613", "submitter": "Julia Siekiera", "authors": "Julia Siekiera, Marius K\\\"oppel, Edwin Simpson, Kevin Stowe, Iryna\n  Gurevych, Stefan Kramer", "title": "Ranking Creative Language Characteristics in Small Data Scenarios", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to rank creative natural language provides an important general\ntool for downstream language understanding and generation. However, current\ndeep ranking models require substantial amounts of labeled data that are\ndifficult and expensive to obtain for different domains, languages and creative\ncharacteristics. A recent neural approach, the DirectRanker, promises to reduce\nthe amount of training data needed but its application to text isn't fully\nexplored. We therefore adapt the DirectRanker to provide a new deep model for\nranking creative language with small data. We compare DirectRanker with a\nBayesian approach, Gaussian process preference learning (GPPL), which has\npreviously been shown to work well with sparse data. Our experiments with\nsparse training data show that while the performance of standard neural ranking\napproaches collapses with small training datasets, DirectRanker remains\neffective. We find that combining DirectRanker with GPPL increases performance\nacross different settings by leveraging the complementary benefits of both\nmodels. Our combined approach outperforms the previous state-of-the-art on\nhumor and metaphor novelty tasks, increasing Spearman's $\\rho$ by 14% and 16%\non average.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:57:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Siekiera", "Julia", ""], ["K\u00f6ppel", "Marius", ""], ["Simpson", "Edwin", ""], ["Stowe", "Kevin", ""], ["Gurevych", "Iryna", ""], ["Kramer", "Stefan", ""]]}, {"id": "2010.12623", "submitter": "Liangming Pan", "authors": "Liangming Pan, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, William Yang\n  Wang", "title": "Unsupervised Multi-hop Question Answering by Question Generation", "comments": "NAACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining training data for multi-hop question answering (QA) is\ntime-consuming and resource-intensive. We explore the possibility to train a\nwell-performed multi-hop QA model without referencing any human-labeled\nmulti-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose\nMQA-QG, an unsupervised framework that can generate human-like multi-hop\ntraining data from both homogeneous and heterogeneous data sources. MQA-QG\ngenerates questions by first selecting/generating relevant information from\neach data source and then integrating the multiple information to form a\nmulti-hop question. Using only generated training data, we can train a\ncompetent multi-hop QA which achieves 61% and 83% of the supervised learning\nperformance for the HybridQA and the HotpotQA dataset, respectively. We also\nshow that pretraining the QA system with the generated data would greatly\nreduce the demand for human-annotated training data. Our codes are publicly\navailable at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:13:47 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 01:48:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Pan", "Liangming", ""], ["Chen", "Wenhu", ""], ["Xiong", "Wenhan", ""], ["Kan", "Min-Yen", ""], ["Wang", "William Yang", ""]]}, {"id": "2010.12626", "submitter": "Laure Thompson", "authors": "Laure Thompson, David Mimno", "title": "Topic Modeling with Contextualized Word Representation Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering token-level contextualized word representations produces output\nthat shares many similarities with topic models for English text collections.\nUnlike clusterings of vocabulary-level word embeddings, the resulting models\nmore naturally capture polysemy and can be used as a way of organizing\ndocuments. We evaluate token clusterings trained from several different output\nlayers of popular contextualized language models. We find that BERT and GPT-2\nproduce high quality clusterings, but RoBERTa does not. These cluster models\nare simple, reliable, and can perform as well as, if not better than, LDA topic\nmodels, maintaining high topic quality even when the number of topics is large\nrelative to the size of the local collection.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:16:59 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Thompson", "Laure", ""], ["Mimno", "David", ""]]}, {"id": "2010.12627", "submitter": "Tobias Eder", "authors": "Tobias Eder, Viktor Hangya, Alexander Fraser", "title": "Anchor-based Bilingual Word Embeddings for Low-Resource Languages", "comments": "The Joint Conference of the 59th Annual Meeting of the Association\n  for Computational Linguistics and the 10th International Joint Conference on\n  Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good quality monolingual word embeddings (MWEs) can be built for languages\nwhich have large amounts of unlabeled text. MWEs can be aligned to bilingual\nspaces using only a few thousand word translation pairs. For low resource\nlanguages training MWEs monolingually results in MWEs of poor quality, and thus\npoor bilingual word embeddings (BWEs) as well. This paper proposes a new\napproach for building BWEs in which the vector space of the high resource\nsource language is used as a starting point for training an embedding space for\nthe low resource target language. By using the source vectors as anchors the\nvector spaces are automatically aligned during training. We experiment on\nEnglish-German, English-Hiligaynon and English-Macedonian. We show that our\napproach results not only in improved BWEs and bilingual lexicon induction\nperformance, but also in improved target language MWE quality as measured using\nmonolingual word similarity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:17:00 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 11:06:05 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Eder", "Tobias", ""], ["Hangya", "Viktor", ""], ["Fraser", "Alexander", ""]]}, {"id": "2010.12634", "submitter": "Yusen Zhang", "authors": "Yusen Zhang, Xiangyu Dong, Shuaichen Chang, Tao Yu, Peng Shi and Rui\n  Zhang", "title": "Did You Ask a Good Question? A Cross-Domain Question Intention\n  Classification Benchmark for Text-to-SQL", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have achieved significant results on the text-to-SQL task, in\nwhich most current work assumes all the input questions are legal and generates\na SQL query for any input. However, in the real scenario, users can input any\ntext that may not be able to be answered by a SQL query. In this work, we\npropose TriageSQL, the first cross-domain text-to-SQL question intention\nclassification benchmark that requires models to distinguish four types of\nunanswerable questions from answerable questions. The baseline RoBERTa model\nachieves a 60% F1 score on the test set, demonstrating the need for further\nimprovement on this task. Our dataset is available at\nhttps://github.com/chatc/TriageSQL.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:36:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Yusen", ""], ["Dong", "Xiangyu", ""], ["Chang", "Shuaichen", ""], ["Yu", "Tao", ""], ["Shi", "Peng", ""], ["Zhang", "Rui", ""]]}, {"id": "2010.12637", "submitter": "Dhivya Chandrasekaran", "authors": "Dhivya Chandrasekaran and Vijay Mago", "title": "Comparative analysis of word embeddings in assessing semantic similarity\n  of complex sentences", "comments": "14 pages, 6 figures, submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:55:11 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 03:44:56 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 21:15:24 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chandrasekaran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2010.12638", "submitter": "Hao Cheng", "authors": "Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, Jianfeng Gao", "title": "Posterior Differential Regularization with f-divergence for Improving\n  Model Robustness", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of enhancing model robustness through regularization.\nSpecifically, we focus on methods that regularize the model posterior\ndifference between clean and noisy inputs. Theoretically, we provide a\nconnection of two recent methods, Jacobian Regularization and Virtual\nAdversarial Training, under this framework. Additionally, we generalize the\nposterior differential regularization to the family of $f$-divergences and\ncharacterize the overall regularization framework in terms of Jacobian matrix.\nEmpirically, we systematically compare those regularizations and standard BERT\ntraining on a diverse set of tasks to provide a comprehensive profile of their\neffect on model in-domain and out-of-domain generalization. For both fully\nsupervised and semi-supervised settings, our experiments show that regularizing\nthe posterior differential with $f$-divergence can result in well-improved\nmodel robustness. In particular, with a proper $f$-divergence, a BERT-base\nmodel can achieve comparable generalization as its BERT-large counterpart for\nin-domain, adversarial and domain shift scenarios, indicating the great\npotential of the proposed framework for boosting model generalization for NLP\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:58:01 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:22:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cheng", "Hao", ""], ["Liu", "Xiaodong", ""], ["Pereira", "Lis", ""], ["Yu", "Yaoliang", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2010.12639", "submitter": "Jesse Thomason", "authors": "Shurjo Banerjee, Jesse Thomason, Jason J. Corso", "title": "The RobotSlang Benchmark: Dialog-guided Robot Localization and\n  Navigation", "comments": "Conference on Robot Learning 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robot systems for applications from search and rescue to assistive\nguidance should be able to engage in natural language dialog with people. To\nstudy such cooperative communication, we introduce Robot Simultaneous\nLocalization and Mapping with Natural Language (RobotSlang), a benchmark of 169\nnatural language dialogs between a human Driver controlling a robot and a human\nCommander providing guidance towards navigation goals. In each trial, the pair\nfirst cooperates to localize the robot on a global map visible to the\nCommander, then the Driver follows Commander instructions to move the robot to\na sequence of target objects. We introduce a Localization from Dialog History\n(LDH) and a Navigation from Dialog History (NDH) task where a learned agent is\ngiven dialog and visual observations from the robot platform as input and must\nlocalize in the global map or navigate towards the next target object,\nrespectively. RobotSlang is comprised of nearly 5k utterances and over 1k\nminutes of robot camera and control streams. We present an initial model for\nthe NDH task, and show that an agent trained in simulation can follow the\nRobotSlang dialog-based navigation instructions for controlling a physical\nrobot platform. Code and data are available at https://umrobotslang.github.io/.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:58:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Banerjee", "Shurjo", ""], ["Thomason", "Jesse", ""], ["Corso", "Jason J.", ""]]}, {"id": "2010.12643", "submitter": "Jacopo Staiano", "authors": "Arij Riabi, Thomas Scialom, Rachel Keraron, Beno\\^it Sagot, Djam\\'e\n  Seddah, Jacopo Staiano", "title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question\n  Answering", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coupled with the availability of large scale datasets, deep learning\narchitectures have enabled rapid progress on the Question Answering task.\nHowever, most of those datasets are in English, and the performances of\nstate-of-the-art multilingual models are significantly lower when evaluated on\nnon-English data. Due to high data collection costs, it is not realistic to\nobtain annotated data for each language one desires to support.\n  We propose a method to improve the Cross-lingual Question Answering\nperformance without requiring additional annotated data, leveraging Question\nGeneration models to produce synthetic samples in a cross-lingual fashion. We\nshow that the proposed method allows to significantly outperform the baselines\ntrained on English data only. We report a new state-of-the-art on four\nmultilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:09:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Riabi", "Arij", ""], ["Scialom", "Thomas", ""], ["Keraron", "Rachel", ""], ["Sagot", "Beno\u00eet", ""], ["Seddah", "Djam\u00e9", ""], ["Staiano", "Jacopo", ""]]}, {"id": "2010.12652", "submitter": "Orhan Firat", "authors": "Mahdis Mahdieh, Mia Xu Chen, Yuan Cao, Orhan Firat", "title": "Rapid Domain Adaptation for Machine Translation with Monolingual Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge of machine translation is how to quickly adapt to unseen\ndomains in face of surging events like COVID-19, in which case timely and\naccurate translation of in-domain information into multiple languages is\ncritical but little parallel data is available yet. In this paper, we propose\nan approach that enables rapid domain adaptation from the perspective of\nunsupervised translation. Our proposed approach only requires in-domain\nmonolingual data and can be quickly applied to a preexisting translation system\ntrained on general domain, reaching significant gains on in-domain translation\nquality with little or no drop on general-domain. We also propose an effective\nprocedure of simultaneous adaptation for multiple domains and languages. To the\nbest of our knowledge, this is the first attempt that aims to address\nunsupervised multilingual domain adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:31:37 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mahdieh", "Mahdis", ""], ["Chen", "Mia Xu", ""], ["Cao", "Yuan", ""], ["Firat", "Orhan", ""]]}, {"id": "2010.12658", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Yicheng Sun, Hejia Chen, Jie Wang", "title": "Generating Adequate Distractors for Multiple-Choice Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to automatic generation of adequate\ndistractors for a given question-answer pair (QAP) generated from a given\narticle to form an adequate multiple-choice question (MCQ). Our method is a\ncombination of part-of-speech tagging, named-entity tagging, semantic-role\nlabeling, regular expressions, domain knowledge bases, word embeddings, word\nedit distance, WordNet, and other algorithms. We use the US SAT (Scholastic\nAssessment Test) practice reading tests as a dataset to produce QAPs and\ngenerate three distractors for each QAP to form an MCQ. We show that, via\nexperiments and evaluations by human judges, each MCQ has at least one adequate\ndistractor and 84\\% of MCQs have three adequate distractors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:47:58 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Cheng", ""], ["Sun", "Yicheng", ""], ["Chen", "Hejia", ""], ["Wang", "Jie", ""]]}, {"id": "2010.12673", "submitter": "Liang Lu", "authors": "Liang Lu, Zhong Meng, Naoyuki Kanda, Jinyu Li, and Yifan Gong", "title": "On Minimum Word Error Rate Training of the Hybrid Autoregressive\n  Transducer", "comments": "5 pages, 1 figure. Accepted to ICASSP 2021, but we withdrawn due to a\n  bug in code. We updated the results after the bug fix, and submitted the\n  paper to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Autoregressive Transducer (HAT) is a recently proposed end-to-end\nacoustic model that extends the standard Recurrent Neural Network Transducer\n(RNN-T) for the purpose of the external language model (LM) fusion. In HAT, the\nblank probability and the label probability are estimated using two separate\nprobability distributions, which provides a more accurate solution for internal\nLM score estimation, and thus works better when combining with an external LM.\nPrevious work mainly focuses on HAT model training with the negative\nlog-likelihood loss, while in this paper, we study the minimum word error rate\n(MWER) training of HAT -- a criterion that is closer to the evaluation metric\nfor speech recognition, and has been successfully applied to other types of\nend-to-end models such as sequence-to-sequence (S2S) and RNN-T models. From\nexperiments with around 30,000 hours of training data, we show that MWER\ntraining can improve the accuracy of HAT models, while at the same time,\nimproving the robustness of the model against the decoding hyper-parameters\nsuch as length normalization and decoding beam during inference.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:16:30 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:12:23 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 17:35:00 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lu", "Liang", ""], ["Meng", "Zhong", ""], ["Kanda", "Naoyuki", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2010.12675", "submitter": "David Gaddy", "authors": "David Gaddy, Alex Kouzemtchenko, Pavan Kumar Reddy, Prateek Kolhar,\n  and Rushin Shah", "title": "Overcoming Conflicting Data for Model Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore how to use a small amount of new data to update a\nmodel when the desired output for some examples has changed. When making\nupdates in this way, one potential problem that arises is the presence of\nconflicting data, or out-of-date labels in the original training set. To\nevaluate the impact of this problem, we propose an experimental setup for\nsimulating changes to a neural semantic parser. We show that the presence of\nconflicting data greatly hinders learning of an update, then explore several\nmethods to mitigate its effect. Our methods lead to large improvements in model\naccuracy compared to a naive mixing strategy, and our best method closes 86% of\nthe accuracy gap between this baseline and an oracle upper bound.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:19:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gaddy", "David", ""], ["Kouzemtchenko", "Alex", ""], ["Reddy", "Pavan Kumar", ""], ["Kolhar", "Prateek", ""], ["Shah", "Rushin", ""]]}, {"id": "2010.12676", "submitter": "Chunchuan Lyu Mr.", "authors": "Chunchuan Lyu, Shay B. Cohen, Ivan Titov", "title": "A Differentiable Relaxation of Graph Segmentation and Alignment for AMR\n  Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Meaning Representations (AMR) are a broad-coverage semantic\nformalism which represents sentence meaning as a directed acyclic graph. To\ntrain most AMR parsers, one needs to segment the graph into subgraphs and align\neach such subgraph to a word in a sentence; this is normally done at\npreprocessing, relying on hand-crafted rules. In contrast, we treat both\nalignment and segmentation as latent variables in our model and induce them as\npart of end-to-end training.\n  As marginalizing over the structured latent variables is infeasible, we use\nthe variational autoencoding framework.\n  To ensure end-to-end differentiable optimization, we introduce a continuous\ndifferentiable relaxation of the segmentation and alignment problems. We\nobserve that inducing segmentation yields substantial gains over using a\n`greedy' segmentation heuristic. The performance of our method also approaches\nthat of a model that relies on \\citet{Lyu2018AMRPA}'s segmentation rules, which\nwere hand-crafted to handle individual AMR constructions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:22:50 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lyu", "Chunchuan", ""], ["Cohen", "Shay B.", ""], ["Titov", "Ivan", ""]]}, {"id": "2010.12681", "submitter": "Armineh Nourbakhsh", "authors": "Natraj Raman, Armineh Nourbakhsh, Sameena Shah, Manuela Veloso", "title": "Robust Document Representations using Latent Topics and Metadata", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task specific fine-tuning of a pre-trained neural language model using a\ncustom softmax output layer is the de facto approach of late when dealing with\ndocument classification problems. This technique is not adequate when labeled\nexamples are not available at training time and when the metadata artifacts in\na document must be exploited. We address these challenges by generating\ndocument representations that capture both text and metadata artifacts in a\ntask agnostic manner. Instead of traditional auto-regressive or auto-encoding\nbased training, our novel self-supervised approach learns a soft-partition of\nthe input space when generating text embeddings. Specifically, we employ a\npre-learned topic model distribution as surrogate labels and construct a loss\nfunction based on KL divergence. Our solution also incorporates metadata\nexplicitly rather than just augmenting them with text. The generated document\nembeddings exhibit compositional characteristics and are directly used by\ndownstream classification tasks to create decision boundaries from a small\nnumber of labeled examples, thereby eschewing complicated recognition methods.\nWe demonstrate through extensive evaluation that our proposed cross-model\nfusion solution outperforms several competitive baselines on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:52:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Raman", "Natraj", ""], ["Nourbakhsh", "Armineh", ""], ["Shah", "Sameena", ""], ["Veloso", "Manuela", ""]]}, {"id": "2010.12683", "submitter": "Jyun-Yu Jiang", "authors": "Jyun-Yu Jiang, Chenyan Xiong, Chia-Jung Lee and Wei Wang", "title": "Long Document Ranking with Query-Directed Sparse Transformer", "comments": "Accepted by EMNLP 2020, 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computing cost of transformer self-attention often necessitates breaking\nlong documents to fit in pretrained models in document ranking tasks. In this\npaper, we design Query-Directed Sparse attention that induces IR-axiomatic\nstructures in transformer self-attention. Our model, QDS-Transformer, enforces\nthe principle properties desired in ranking: local contextualization,\nhierarchical representation, and query-oriented proximity matching, while it\nalso enjoys efficiency from sparsity. Experiments on one fully supervised and\nthree few-shot TREC document ranking benchmarks demonstrate the consistent and\nrobust advantage of QDS-Transformer over previous approaches, as they either\nretrofit long documents into BERT or use sparse attention without emphasizing\nIR principles. We further quantify the computing complexity and demonstrates\nthat our sparse attention with TVM implementation is twice more efficient than\nthe fully-connected self-attention. All source codes, trained model, and\npredictions of this work are available at\nhttps://github.com/hallogameboy/QDS-Transformer.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:57:56 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jiang", "Jyun-Yu", ""], ["Xiong", "Chenyan", ""], ["Lee", "Chia-Jung", ""], ["Wang", "Wei", ""]]}, {"id": "2010.12684", "submitter": "Valentin Hofmann", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\\\"utze", "title": "Dynamic Contextualized Word Embeddings", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static word embeddings that represent words by a single vector cannot capture\nthe variability of word meaning in different linguistic and extralinguistic\ncontexts. Building on prior work on contextualized and dynamic word embeddings,\nwe introduce dynamic contextualized word embeddings that represent words as a\nfunction of both linguistic and extralinguistic context. Based on a pretrained\nlanguage model (PLM), dynamic contextualized word embeddings model time and\nsocial space jointly, which makes them attractive for a range of NLP tasks\ninvolving semantic variability. We highlight potential application scenarios by\nmeans of qualitative and quantitative analyses on four English datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:02:40 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 16:34:30 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 13:08:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hofmann", "Valentin", ""], ["Pierrehumbert", "Janet B.", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2010.12688", "submitter": "Oshin Agarwal", "authors": "Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou", "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced\n  Language Model Pre-training", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on Data-To-Text Generation, the task of converting knowledge graph\n(KG) triples into natural text, focused on domain-specific benchmark datasets.\nIn this paper, however, we verbalize the entire English Wikidata KG, and\ndiscuss the unique challenges associated with a broad, open-domain, large-scale\nverbalization. We further show that verbalizing a comprehensive, encyclopedic\nKG like Wikidata can be used to integrate structured KGs and natural language\ncorpora. In contrast to the many architectures that have been developed to\nintegrate these two sources, our approach converts the KG into natural text,\nallowing it to be seamlessly integrated into existing language models. It\ncarries the further advantages of improved factual accuracy and reduced\ntoxicity in the resulting language model. We evaluate this approach by\naugmenting the retrieval corpus in a retrieval language model and showing\nsignificant improvements on the knowledge intensive tasks of open domain QA and\nthe LAMA knowledge probe.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:14:50 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 18:25:01 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Agarwal", "Oshin", ""], ["Ge", "Heming", ""], ["Shakeri", "Siamak", ""], ["Al-Rfou", "Rami", ""]]}, {"id": "2010.12693", "submitter": "Nadezhda Chirkova", "authors": "Nadezhda Chirkova", "title": "On the Embeddings of Variables in Recurrent Neural Networks for Source\n  Code", "comments": "Published at the 2021 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code processing heavily relies on the methods widely used in natural\nlanguage processing (NLP), but involves specifics that need to be taken into\naccount to achieve higher quality. An example of this specificity is that the\nsemantics of a variable is defined not only by its name but also by the\ncontexts in which the variable occurs. In this work, we develop dynamic\nembeddings, a recurrent mechanism that adjusts the learned semantics of the\nvariable when it obtains more information about the variable's role in the\nprogram. We show that using the proposed dynamic embeddings significantly\nimproves the performance of the recurrent neural network, in code completion\nand bug fixing tasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:32:11 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 16:05:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chirkova", "Nadezhda", ""]]}, {"id": "2010.12694", "submitter": "Sayali Kulkarni", "authors": "Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, Eugene Ie", "title": "AQuaMuSe: Automatically Generating Datasets for Query-Based\n  Multi-Document Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Summarization is the task of compressing source document(s) into coherent and\nsuccinct passages. This is a valuable tool to present users with concise and\naccurate sketch of the top ranked documents related to their queries.\nQuery-based multi-document summarization (qMDS) addresses this pervasive need,\nbut the research is severely limited due to lack of training and evaluation\ndatasets as existing single-document and multi-document summarization datasets\nare inadequate in form and scale. We propose a scalable approach called\nAQuaMuSe to automatically mine qMDS examples from question answering datasets\nand large document corpora. Our approach is unique in the sense that it can\ngeneral a dual dataset -- for extractive and abstractive summaries both. We\npublicly release a specific instance of an AQuaMuSe dataset with 5,519\nquery-based summaries, each associated with an average of 6 input documents\nselected from an index of 355M documents from Common Crawl. Extensive\nevaluation of the dataset along with baseline summarization model experiments\nare provided.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:38:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kulkarni", "Sayali", ""], ["Chammas", "Sheide", ""], ["Zhu", "Wan", ""], ["Sha", "Fei", ""], ["Ie", "Eugene", ""]]}, {"id": "2010.12699", "submitter": "Stefan Gr\\\"unewald", "authors": "Stefan Gr\\\"unewald, Annemarie Friedrich, Jonas Kuhn", "title": "Applying Occam's Razor to Transformer-Based Dependency Parsing: What\n  Works, What Doesn't, and What is Really Necessary", "comments": "14 pages, 1 figure; camera-ready version for IWPT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of pre-trained transformer-based contextualized word\nembeddings has led to considerable improvements in the accuracy of graph-based\nparsers for frameworks such as Universal Dependencies (UD). However, previous\nworks differ in various dimensions, including their choice of pre-trained\nlanguage models and whether they use LSTM layers. With the aims of\ndisentangling the effects of these choices and identifying a simple yet widely\napplicable architecture, we introduce STEPS, a new modular graph-based\ndependency parser. Using STEPS, we perform a series of analyses on the UD\ncorpora of a diverse set of languages. We find that the choice of pre-trained\nembeddings has by far the greatest impact on parser performance and identify\nXLM-R as a robust choice across the languages in our study. Adding LSTM layers\nprovides no benefits when using transformer-based embeddings. A multi-task\ntraining setup outputting additional UD features may contort results. Taking\nthese insights together, we propose a simple but widely applicable parser\narchitecture and configuration, achieving new state-of-the-art results (in\nterms of LAS) for 10 out of 12 diverse languages.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:58:26 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 12:44:18 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 12:30:13 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Gr\u00fcnewald", "Stefan", ""], ["Friedrich", "Annemarie", ""], ["Kuhn", "Jonas", ""]]}, {"id": "2010.12707", "submitter": "Dorottya Demszky", "authors": "Dorottya Demszky, Devyani Sharma, Jonathan H. Clark, Vinodkumar\n  Prabhakaran, Jacob Eisenstein", "title": "Learning to Recognize Dialect Features", "comments": "NAACL camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building NLP systems that serve everyone requires accounting for dialect\ndifferences. But dialects are not monolithic entities: rather, distinctions\nbetween and within dialects are captured by the presence, absence, and\nfrequency of dozens of dialect features in speech and text, such as the\ndeletion of the copula in \"He {} running\". In this paper, we introduce the task\nof dialect feature detection, and present two multitask learning approaches,\nboth based on pretrained transformers. For most dialects, large-scale annotated\ncorpora for these features are unavailable, making it difficult to train\nrecognizers. We train our models on a small number of minimal pairs, building\non how linguists typically define dialect features. Evaluation on a test set of\n22 dialect features of Indian English demonstrates that these models learn to\nrecognize many features with high accuracy, and that a few minimal pairs can be\nas effective for training as thousands of labeled examples. We also demonstrate\nthe downstream applicability of dialect feature detection both as a measure of\ndialect density and as a dialect classifier.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 23:25:00 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 05:23:13 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 22:27:50 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Demszky", "Dorottya", ""], ["Sharma", "Devyani", ""], ["Clark", "Jonathan H.", ""], ["Prabhakaran", "Vinodkumar", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "2010.12710", "submitter": "Maria Phillips", "authors": "Debajyoti Datta, Maria Phillips, Jennifer Chiu, Ginger S. Watson,\n  James P. Bywater, Laura Barnes, and Donald Brown", "title": "Improving Classification through Weak Supervision in Context-specific\n  Conversational Agent Development for Teacher Education", "comments": "Preprint: Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques applied to the Natural Language Processing (NLP)\ncomponent of conversational agent development show promising results for\nimproved accuracy and quality of feedback that a conversational agent can\nprovide. The effort required to develop an educational scenario specific\nconversational agent is time consuming as it requires domain experts to label\nand annotate noisy data sources such as classroom videos. Previous approaches\nto modeling annotations have relied on labeling thousands of examples and\ncalculating inter-annotator agreement and majority votes in order to model the\nnecessary scenarios. This method, while proven successful, ignores individual\nannotator strengths in labeling a data point and under-utilizes examples that\ndo not have a majority vote for labeling. We propose using a multi-task weak\nsupervision method combined with active learning to address these concerns.\nThis approach requires less labeling than traditional methods and shows\nsignificant improvements in precision, efficiency, and time-requirements than\nthe majority vote method (Ratner 2019). We demonstrate the validity of this\nmethod on the Google Jigsaw data set and then propose a scenario to apply this\nmethod using the Instructional Quality Assessment(IQA) to define the categories\nfor labeling. We propose using probabilistic modeling of annotator labeling to\ngenerate active learning examples to further label the data. Active learning is\nable to iteratively improve the training performance and accuracy of the\noriginal classification model. This approach combines state-of-the art labeling\ntechniques of weak supervision and active learning to optimize results in the\neducational domain and could be further used to lessen the data requirements\nfor expanded scenarios within the education domain through transfer learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 23:39:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Datta", "Debajyoti", ""], ["Phillips", "Maria", ""], ["Chiu", "Jennifer", ""], ["Watson", "Ginger S.", ""], ["Bywater", "James P.", ""], ["Barnes", "Laura", ""], ["Brown", "Donald", ""]]}, {"id": "2010.12712", "submitter": "Shuguang Chen", "authors": "Shuguang Chen, Gustavo Aguilar, Leonardo Neves, Thamar Solorio", "title": "A Caption Is Worth A Thousand Images: Investigating Image Captions for\n  Multimodal Named Entity Recognition", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal named entity recognition (MNER) requires to bridge the gap between\nlanguage understanding and visual context. Due to advances in natural language\nprocessing (NLP) and computer vision (CV), many neural techniques have been\nproposed to incorporate images into the NER task. In this work, we conduct a\ndetailed analysis of current state-of-the-art fusion techniques for MNER and\ndescribe scenarios where adding information from the image does not always\nresult in boosts in performance. We also study the use of captions as a way to\nenrich the context for MNER. We provide extensive empirical analysis and an\nablation study on three datasets from popular social platforms to expose the\nsituations where the approach is beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 23:41:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Shuguang", ""], ["Aguilar", "Gustavo", ""], ["Neves", "Leonardo", ""], ["Solorio", "Thamar", ""]]}, {"id": "2010.12719", "submitter": "Falcon Dai", "authors": "Falcon Z. Dai", "title": "Word2vec Conjecture and A Limitative Result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being inspired by the success of \\texttt{word2vec}\n\\citep{mikolov2013distributed} in capturing analogies, we study the conjecture\nthat analogical relations can be represented by vector spaces. Unlike many\nprevious works that focus on the distributional semantic aspect of\n\\texttt{word2vec}, we study the purely \\emph{representational} question: can\n\\emph{all} semantic word-word relations be represented by differences (or\ndirections) of vectors? We call this the word2vec conjecture and point out some\nof its desirable implications. However, we will exhibit a class of relations\nthat cannot be represented in this way, thus falsifying the conjecture and\nestablishing a limitative result for the representability of semantic relations\nby vector spaces over fields of characteristic 0, e.g., real or complex\nnumbers.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 00:14:04 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Dai", "Falcon Z.", ""]]}, {"id": "2010.12723", "submitter": "Yuning Mao", "authors": "Yuning Mao, Xiang Ren, Heng Ji, Jiawei Han", "title": "Constrained Abstractive Summarization: Preserving Factual Consistency\n  with Constrained Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summaries generated by abstractive summarization are supposed to only contain\nstatements entailed by the source documents. However, state-of-the-art\nabstractive methods are still prone to hallucinate content inconsistent with\nthe source documents. In this paper, we propose constrained abstractive\nsummarization (CAS), a general setup that preserves the factual consistency of\nabstractive summarization by specifying tokens as constraints that must be\npresent in the summary. We explore the feasibility of using lexically\nconstrained decoding, a technique applicable to any abstractive method with\nbeam search decoding, to fulfill CAS and conduct experiments in two scenarios:\n(1) Standard summarization without human involvement, where keyphrase\nextraction is used to extract constraints from source documents; (2)\nInteractive summarization with human feedback, which is simulated by taking\nmissing tokens in the reference summaries as constraints. Automatic and human\nevaluations on two benchmark datasets demonstrate that CAS improves the quality\nof abstractive summaries, especially on factual consistency. In particular, we\nobserve up to 11.2 ROUGE-2 gains when several ground-truth tokens are used as\nconstraints in the interactive summarization scenario.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 00:27:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mao", "Yuning", ""], ["Ren", "Xiang", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.12725", "submitter": "Peter Shaw", "authors": "Peter Shaw, Ming-Wei Chang, Panupong Pasupat, Kristina Toutanova", "title": "Compositional Generalization and Natural Language Variation: Can a\n  Semantic Parsing Approach Handle Both?", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models excel at handling natural language variation, but\nhave been shown to struggle with out-of-distribution compositional\ngeneralization. This has motivated new specialized architectures with stronger\ncompositional biases, but most of these approaches have only been evaluated on\nsynthetically-generated datasets, which are not representative of natural\nlanguage variation. In this work we ask: can we develop a semantic parsing\napproach that handles both natural language variation and compositional\ngeneralization? To better assess this capability, we propose new train and test\nsplits of non-synthetic datasets. We demonstrate that strong existing\napproaches do not perform well across a broad set of evaluations. We also\npropose NQG-T5, a hybrid model that combines a high-precision grammar-based\napproach with a pre-trained sequence-to-sequence model. It outperforms existing\napproaches across several compositional generalization challenges on\nnon-synthetic data, while also being competitive with the state-of-the-art on\nstandard evaluations. While still far from solving this problem, our study\nhighlights the importance of diverse evaluations and the open challenge of\nhandling both compositional generalization and natural language variation in\nsemantic parsing.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 00:38:27 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 21:25:04 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Shaw", "Peter", ""], ["Chang", "Ming-Wei", ""], ["Pasupat", "Panupong", ""], ["Toutanova", "Kristina", ""]]}, {"id": "2010.12729", "submitter": "Adina Williams", "authors": "Adina Williams, Tristan Thrush, Douwe Kiela", "title": "ANLIzing the Adversarial Natural Language Inference Dataset", "comments": "33 pages, 1 figure, 24 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform an in-depth error analysis of Adversarial NLI (ANLI), a recently\nintroduced large-scale human-and-model-in-the-loop natural language inference\ndataset collected over multiple rounds. We propose a fine-grained annotation\nscheme of the different aspects of inference that are responsible for the gold\nclassification labels, and use it to hand-code all three of the ANLI\ndevelopment sets. We use these annotations to answer a variety of interesting\nquestions: which inference types are most common, which models have the highest\nperformance on each reasoning type, and which types are the most challenging\nfor state of-the-art models? We hope that our annotations will enable more\nfine-grained evaluation of models trained on ANLI, provide us with a deeper\nunderstanding of where models fail and succeed, and help us determine how to\ntrain better models in future.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 01:03:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Williams", "Adina", ""], ["Thrush", "Tristan", ""], ["Kiela", "Douwe", ""]]}, {"id": "2010.12730", "submitter": "Gustavo Aguilar", "authors": "Gustavo Aguilar, Bryan McCann, Tong Niu, Nazneen Rajani, Nitish\n  Keskar, Thamar Solorio", "title": "Char2Subword: Extending the Subword Embedding Space Using Robust\n  Character Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword\ntokenization process of language models. BPE provides multiple benefits, such\nas handling the out-of-vocabulary problem and reducing vocabulary sparsity.\nHowever, this process is defined from the pre-training data statistics, making\nthe tokenization on different domains susceptible to infrequent spelling\nsequences (e.g., misspellings as in social media or character-level adversarial\nattacks). On the other hand, though robust to misspellings, pure\ncharacter-level models often lead to unreasonably large sequences and make it\nharder for the model to learn meaningful contiguous characters. We propose a\ncharacter-based subword module (char2subword) that learns the subword embedding\ntable in pre-trained models like BERT to alleviate these challenges. Our\nchar2subword module builds representations from characters out of the subword\nvocabulary, and it can be used as a drop-in replacement of the subword\nembedding table. The module is robust to character-level alterations such as\nmisspellings, word inflection, casing, and punctuation. We integrate it further\nwith BERT through pre-training while keeping BERT transformer parameters fixed.\nWe show our method's effectiveness by outperforming mBERT on the linguistic\ncode-switching evaluation (LinCE) benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 01:08:28 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 17:17:23 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Aguilar", "Gustavo", ""], ["McCann", "Bryan", ""], ["Niu", "Tong", ""], ["Rajani", "Nazneen", ""], ["Keskar", "Nitish", ""], ["Solorio", "Thamar", ""]]}, {"id": "2010.12741", "submitter": "Jo\\~ao Sedoc", "authors": "Seolhwa Lee, Heuiseok Lim, Jo\\~ao Sedoc", "title": "An Evaluation Protocol for Generative Conversational Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a multitude of novel generative models for open-domain\nconversational systems; however, there is no systematic evaluation of different\nsystems. Systematic comparisons require consistency in experimental design,\nevaluation sets, conversational systems and their outputs, and statistical\nanalysis. We lay out a protocol for the evaluation of conversational models\nusing head-to-head pairwise comparison. We analyze ten recent models that claim\nstate-of-the-art performance using a paired head-to-head performance\n(win-loss-tie) on five evaluation datasets. Our findings show that DialoGPT and\nBlender are superior systems using Bradley-Terry model and TrueSkill ranking\nmethods. These findings demonstrate the feasibility of our protocol to evaluate\nconversational agents and evaluation sets. Finally, we make all code and\nevaluations publicly available for researchers to compare their model to other\nstate-of-the-art dialog models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 01:59:49 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lee", "Seolhwa", ""], ["Lim", "Heuiseok", ""], ["Sedoc", "Jo\u00e3o", ""]]}, {"id": "2010.12742", "submitter": "Zhiqiang Hu", "authors": "Zhiqiang Hu, Roy Ka-Wei Lee, Charu C. Aggarwal, Aston Zhang", "title": "Text Style Transfer: A Review and Experimental Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stylistic properties of text have intrigued computational linguistics\nresearchers in recent years. Specifically, researchers have investigated the\nText Style Transfer (TST) task, which aims to change the stylistic properties\nof the text while retaining its style independent content. Over the last few\nyears, many novel TST algorithms have been developed, while the industry has\nleveraged these algorithms to enable exciting TST applications. The field of\nTST research has burgeoned because of this symbiosis. This article aims to\nprovide a comprehensive review of recent research efforts on text style\ntransfer. More concretely, we create a taxonomy to organize the TST models and\nprovide a comprehensive summary of the state of the art. We review the existing\nevaluation methodologies for TST tasks and conduct a large-scale\nreproducibility study where we experimentally benchmark 19 state-of-the-art TST\nalgorithms on two publicly available datasets. Finally, we expand on current\ntrends and provide new perspectives on the new and exciting developments in the\nTST field.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 02:02:58 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 14:48:37 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Zhiqiang", ""], ["Lee", "Roy Ka-Wei", ""], ["Aggarwal", "Charu C.", ""], ["Zhang", "Aston", ""]]}, {"id": "2010.12753", "submitter": "Ben Zhou", "authors": "Ben Zhou and Kyle Richardson and Qiang Ning and Tushar Khot and Ashish\n  Sabharwal and Dan Roth", "title": "Temporal Reasoning on Implicit Events from Distant Supervision", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose TRACIE, a novel temporal reasoning dataset that evaluates the\ndegree to which systems understand implicit events -- events that are not\nmentioned explicitly in natural language text but can be inferred from it. This\nintroduces a new challenge in temporal reasoning research, where prior work has\nfocused on explicitly mentioned events. Human readers can infer implicit events\nvia commonsense reasoning, resulting in a more comprehensive understanding of\nthe situation and, consequently, better reasoning about time. We find, however,\nthat state-of-the-art models struggle when predicting temporal relationships\nbetween implicit and explicit events. To address this, we propose a\nneuro-symbolic temporal reasoning model, SYMTIME, which exploits distant\nsupervision signals from large-scale text and uses temporal rules to combine\nstart times and durations to infer end times. SYMTIME outperforms strong\nbaseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training\nsetting. Our approach also generalizes to other temporal reasoning tasks, as\nevidenced by a gain of 1%-9% on MATRES, an explicit event benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:12:27 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 21:07:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhou", "Ben", ""], ["Richardson", "Kyle", ""], ["Ning", "Qiang", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Roth", "Dan", ""]]}, {"id": "2010.12755", "submitter": "Xinyu Zhao", "authors": "Xinyu Zhao, Shih-ting Lin, Greg Durrett", "title": "Effective Distant Supervision for Temporal Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A principal barrier to training temporal relation extraction models in new\ndomains is the lack of varied, high quality examples and the challenge of\ncollecting more. We present a method of automatically collecting\ndistantly-supervised examples of temporal relations. We scrape and\nautomatically label event pairs where the temporal relations are made explicit\nin text, then mask out those explicit cues, forcing a model trained on this\ndata to learn other signals. We demonstrate that a pre-trained Transformer\nmodel is able to transfer from the weakly labeled examples to human-annotated\nbenchmarks in both zero-shot and few-shot settings, and that the masking scheme\nis important in improving generalization.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:17:31 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 00:56:48 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhao", "Xinyu", ""], ["Lin", "Shih-ting", ""], ["Durrett", "Greg", ""]]}, {"id": "2010.12757", "submitter": "Kai Sun", "authors": "Kai Sun, Seungwhan Moon, Paul Crook, Stephen Roller, Becka Silvert,\n  Bing Liu, Zhiguang Wang, Honglei Liu, Eunjoon Cho, Claire Cardie", "title": "Adding Chit-Chat to Enhance Task-Oriented Dialogues", "comments": "To appear in NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dialogue corpora and models are typically designed under two\ndisjoint motives: while task-oriented systems focus on achieving functional\ngoals (e.g., booking hotels), open-domain chatbots aim at making socially\nengaging conversations. In this work, we propose to integrate both types of\nsystems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with\nthe goal of making virtual assistant conversations more engaging and\ninteractive. Specifically, we propose a Human <-> AI collaborative data\ncollection approach for generating diverse chit-chat responses to augment\ntask-oriented dialogues with minimal annotation effort. We then present our new\nchit-chat-based annotations to 23.8K dialogues from two popular task-oriented\ndatasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their\nadvantage over the originals via human evaluation. Lastly, we propose three new\nmodels for adding chit-chat to task-oriented dialogues, explicitly trained to\npredict user goals and to generate contextually relevant chit-chat responses.\nAutomatic and human evaluations show that, compared with the state-of-the-art\ntask-oriented baseline, our models can code-switch between task and chit-chat\nto be more engaging, interesting, knowledgeable, and humanlike, while\nmaintaining competitive task performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:22:43 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 19:12:41 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sun", "Kai", ""], ["Moon", "Seungwhan", ""], ["Crook", "Paul", ""], ["Roller", "Stephen", ""], ["Silvert", "Becka", ""], ["Liu", "Bing", ""], ["Wang", "Zhiguang", ""], ["Liu", "Honglei", ""], ["Cho", "Eunjoon", ""], ["Cardie", "Claire", ""]]}, {"id": "2010.12758", "submitter": "Zhiyu Chen", "authors": "Zhiyu Chen, Honglei Liu, Hu Xu, Seungwhan Moon, Hao Zhou, Bing Liu", "title": "NUANCED: Natural Utterance Annotation for Nuanced Conversation with\n  Estimated Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing conversational systems are mostly agent-centric, which assumes the\nuser utterances would closely follow the system ontology (for NLU or dialogue\nstate tracking). However, in real-world scenarios, it is highly desirable that\nthe users can speak freely in their own way. It is extremely hard, if not\nimpossible, for the users to adapt to the unknown system ontology. In this\nwork, we attempt to build a user-centric dialogue system. As there is no clean\nmapping for a user's free form utterance to an ontology, we first model the\nuser preferences as estimated distributions over the system ontology and map\nthe users' utterances to such distributions. Learning such a mapping poses new\nchallenges on reasoning over existing knowledge, ranging from factoid\nknowledge, commonsense knowledge to the users' own situations. To this end, we\nbuild a new dataset named NUANCED that focuses on such realistic settings for\nconversational recommendation. Collected via dialogue simulation and\nparaphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user\nresponses. We conduct experiments, showing both the usefulness and challenges\nof our problem setting. We believe NUANCED can serve as a valuable resource to\npush existing research from the agent-centric system to the user-centric\nsystem. The code and data will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:23:14 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Zhiyu", ""], ["Liu", "Honglei", ""], ["Xu", "Hu", ""], ["Moon", "Seungwhan", ""], ["Zhou", "Hao", ""], ["Liu", "Bing", ""]]}, {"id": "2010.12762", "submitter": "Sarah Wiegreffe", "authors": "Sarah Wiegreffe, Ana Marasovi\\'c, Noah A. Smith", "title": "Measuring Association Between Labels and Free-Text Rationales", "comments": "9 pages main, 7 pages appendix. v2: updated results in Section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In interpretable NLP, we require faithful rationales that reflect the model's\ndecision-making process for an explained instance. While prior work focuses on\nextractive rationales (a subset of the input words), we investigate their\nless-studied counterpart: free-text natural language rationales. We demonstrate\nthat pipelines, existing models for faithful extractive rationalization on\ninformation-extraction style tasks, do not extend as reliably to \"reasoning\"\ntasks requiring free-text rationales. We turn to models that jointly predict\nand rationalize, a class of widely used high-performance models for free-text\nrationalization whose faithfulness is not yet established. We define\nlabel-rationale association as a necessary property for faithfulness: the\ninternal mechanisms of the model producing the label and the rationale must be\nmeaningfully correlated. We propose two measurements to test this property:\nrobustness equivalence and feature importance agreement. We find that\nstate-of-the-art T5-based joint models exhibit both properties for\nrationalizing commonsense question-answering and natural language inference,\nindicating their potential for producing faithful free-text rationales.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:40:56 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 03:45:38 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wiegreffe", "Sarah", ""], ["Marasovi\u0107", "Ana", ""], ["Smith", "Noah A.", ""]]}, {"id": "2010.12764", "submitter": "Rodolfo Corona", "authors": "Rodolfo Corona, Daniel Fried, Coline Devin, Dan Klein, Trevor Darrell", "title": "Modular Networks for Compositional Instruction Following", "comments": "Published in NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard architectures used in instruction following often struggle on novel\ncompositions of subgoals (e.g. navigating to landmarks or picking up objects)\nobserved during training. We propose a modular architecture for following\nnatural language instructions that describe sequences of diverse subgoals. In\nour approach, subgoal modules each carry out natural language instructions for\na specific subgoal type. A sequence of modules to execute is chosen by learning\nto segment the instructions and predicting a subgoal type for each segment.\nWhen compared to standard, non-modular sequence-to-sequence approaches on\nALFRED, a challenging instruction following benchmark, we find that\nmodularization improves generalization to novel subgoal compositions, as well\nas to environments unseen in training.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:48:45 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 05:34:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Corona", "Rodolfo", ""], ["Fried", "Daniel", ""], ["Devin", "Coline", ""], ["Klein", "Dan", ""], ["Darrell", "Trevor", ""]]}, {"id": "2010.12770", "submitter": "Jianpeng Cheng J", "authors": "Jianpeng Cheng, Devang Agrawal, Hector Martinez Alonso, Shruti\n  Bhargava, Joris Driesen, Federico Flego, Shaona Ghosh, Dain Kaplan, Dimitri\n  Kartsaklis, Lin Li, Dhivya Piraviperumal, Jason D Williams, Hong Yu, Diarmuid\n  O Seaghdha, Anders Johannsen", "title": "Conversational Semantic Parsing for Dialog State Tracking", "comments": "Publish as a conference paper at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new perspective on dialog state tracking (DST), the task of\nestimating a user's goal through the course of a dialog. By formulating DST as\na semantic parsing task over hierarchical representations, we can incorporate\nsemantic compositionality, cross-domain knowledge sharing and co-reference. We\npresent TreeDST, a dataset of 27k conversations annotated with tree-structured\ndialog states and system acts. We describe an encoder-decoder framework for DST\nwith hierarchical representations, which leads to 20% improvement over\nstate-of-the-art DST approaches that operate on a flat meaning space of\nslot-value pairs.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:10:32 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 04:33:00 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 18:02:43 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Agrawal", "Devang", ""], ["Alonso", "Hector Martinez", ""], ["Bhargava", "Shruti", ""], ["Driesen", "Joris", ""], ["Flego", "Federico", ""], ["Ghosh", "Shaona", ""], ["Kaplan", "Dain", ""], ["Kartsaklis", "Dimitri", ""], ["Li", "Lin", ""], ["Piraviperumal", "Dhivya", ""], ["Williams", "Jason D", ""], ["Yu", "Hong", ""], ["Seaghdha", "Diarmuid O", ""], ["Johannsen", "Anders", ""]]}, {"id": "2010.12771", "submitter": "Yixin Liu", "authors": "Yixin Liu, Graham Neubig, John Wieting", "title": "On Learning Text Style Transfer with Direct Rewards", "comments": "Published as a long paper at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most cases, the lack of parallel corpora makes it impossible to directly\ntrain supervised models for the text style transfer task. In this paper, we\nexplore training algorithms that instead optimize reward functions that\nexplicitly consider different aspects of the style-transferred outputs. In\nparticular, we leverage semantic similarity metrics originally used for\nfine-tuning neural machine translation models to explicitly assess the\npreservation of content between system outputs and input texts. We also\ninvestigate the potential weaknesses of the existing automatic metrics and\npropose efficient strategies of using these metrics for training. The\nexperimental results show that our model provides significant gains in both\nautomatic and human evaluation over strong baselines, indicating the\neffectiveness of our proposed methods and training strategies.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:30:02 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:00:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Liu", "Yixin", ""], ["Neubig", "Graham", ""], ["Wieting", "John", ""]]}, {"id": "2010.12773", "submitter": "Xiang Deng", "authors": "Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr\n  Polozov, Huan Sun, Matthew Richardson", "title": "Structure-Grounded Pretraining for Text-to-SQL", "comments": "Accepted to NAACL 2021. Please contact the first author for questions\n  regarding the spider-realistic dataset", "journal-ref": null, "doi": "10.18653/v1/2021.naacl-main.105", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to capture text-table alignment is essential for tasks like\ntext-to-SQL. A model needs to correctly recognize natural language references\nto columns and values and to ground them in the given database schema. In this\npaper, we present a novel weakly supervised Structure-Grounded pretraining\nframework (StruG) for text-to-SQL that can effectively learn to capture\ntext-table alignment based on a parallel text-table corpus. We identify a set\nof novel prediction tasks: column grounding, value grounding and column-value\nmapping, and leverage them to pretrain a text-table encoder. Additionally, to\nevaluate different methods under more realistic text-table alignment settings,\nwe create a new evaluation set Spider-Realistic based on Spider dev set with\nexplicit mentions of column names removed, and adopt eight existing text-to-SQL\ndatasets for cross-database evaluation. STRUG brings significant improvement\nover BERT-LARGE in all settings. Compared with existing pretraining methods\nsuch as GRAPPA, STRUG achieves similar performance on Spider, and outperforms\nall baselines on more realistic sets. All the code and data used in this work\nis public available at https://aka.ms/strug.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:35:35 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 21:12:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Deng", "Xiang", ""], ["Awadallah", "Ahmed Hassan", ""], ["Meek", "Christopher", ""], ["Polozov", "Oleksandr", ""], ["Sun", "Huan", ""], ["Richardson", "Matthew", ""]]}, {"id": "2010.12776", "submitter": "Yanda Chen", "authors": "Yanda Chen (1), Md Arafat Sultan (2), Vittorio Castelli (2) ((1)\n  Department of Computer Science, Columbia University, (2) IBM Research AI,\n  T.J. Watson Research Center, New York, USA)", "title": "Improved Synthetic Training for Reading Comprehension", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generated synthetic training examples have been shown to\nimprove performance in machine reading comprehension (MRC). Compared to human\nannotated gold standard data, synthetic training data has unique properties,\nsuch as high availability at the possible expense of quality. In view of such\ndifferences, in this paper, we explore novel applications of synthetic examples\nto MRC. Our proposed pre-training and knowledge distillation strategies show\nsignificant improvements over existing methods. In a particularly surprising\ndiscovery, we observe that synthetic distillation often yields students that\ncan outperform the teacher model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:41:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Yanda", ""], ["Sultan", "Md Arafat", ""], ["Castelli", "Vittorio", ""]]}, {"id": "2010.12777", "submitter": "Hyung Won Chung", "authors": "Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, Jason Riesa", "title": "Improving Multilingual Models with Language-Clustered Vocabularies", "comments": "Published in the main conference of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art multilingual models depend on vocabularies that cover all of\nthe languages the model will expect to see at inference time, but the standard\nmethods for generating those vocabularies are not ideal for massively\nmultilingual applications. In this work, we introduce a novel procedure for\nmultilingual vocabulary generation that combines the separately trained\nvocabularies of several automatically derived language clusters, thus balancing\nthe trade-off between cross-lingual subword sharing and language-specific\nvocabularies. Our experiments show improvements across languages on key\nmultilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1\\%), and WikiAnn NER\n(+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without\nincreasing the size of the model or data.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:49:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chung", "Hyung Won", ""], ["Garrette", "Dan", ""], ["Tan", "Kiat Chuan", ""], ["Riesa", "Jason", ""]]}, {"id": "2010.12779", "submitter": "Aida Mostafazadeh Davani", "authors": "Aida Mostafazadeh Davani, Ali Omrani, Brendan Kennedy, Mohammad Atari,\n  Xiang Ren, Morteza Dehghani", "title": "Fair Hate Speech Detection through Evaluation of Social Group\n  Counterfactuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches for mitigating bias in supervised models are designed to reduce\nmodels' dependence on specific sensitive features of the input data, e.g.,\nmentioned social groups. However, in the case of hate speech detection, it is\nnot always desirable to equalize the effects of social groups because of their\nessential role in distinguishing outgroup-derogatory hate, such that particular\ntypes of hateful rhetoric carry the intended meaning only when contextualized\naround certain social group tokens. Counterfactual token fairness for a\nmentioned social group evaluates the model's predictions as to whether they are\nthe same for (a) the actual sentence and (b) a counterfactual instance, which\nis generated by changing the mentioned social group in the sentence. Our\napproach assures robust model predictions for counterfactuals that imply\nsimilar meaning as the actual sentence. To quantify the similarity of a\nsentence and its counterfactual, we compare their likelihood score calculated\nby generative language models. By equalizing model behaviors on each sentence\nand its counterfactuals, we mitigate bias in the proposed model while\npreserving the overall classification performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:51:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Davani", "Aida Mostafazadeh", ""], ["Omrani", "Ali", ""], ["Kennedy", "Brendan", ""], ["Atari", "Mohammad", ""], ["Ren", "Xiang", ""], ["Dehghani", "Morteza", ""]]}, {"id": "2010.12780", "submitter": "Yan Zeng", "authors": "Yan Zeng and Jian-Yun Nie", "title": "Open-Domain Dialogue Generation Based on Pre-trained Language Models", "comments": "[v0], 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models have been successfully used in response\ngeneration for open-domain dialogue. Four main frameworks have been proposed:\n(1) Transformer-ED using Transformer encoder and decoder separately for source\nand target sentences; (2) Transformer-Dec using Transformer decoder for both\nsource and target sentences; (3) Transformer-MLM using Transformer decoder that\napplies bi-directional attention on the source side and left-to-right attention\non the target side with masked language model objective; and (4) Transformer-AR\nthat uses auto-regressive objective instead. In this study, we compare these\nframeworks on 3 datasets, and our comparison reveals that the best framework\nuses bidirectional attention on the source side and does not separate encoder\nand decoder. We also examine model discrepancy, and our experiments confirm\nthat the performance of a model is directly impacted by the underlying\ndiscrepancies. We then propose two correction methods to reduce the\ndiscrepancies, and both improve the model performance. These results show that\ndiscrepancies is an important factor to consider when we use a pre-trained\nmodel, and a reduction in discrepancies can lead to improved performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:52:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zeng", "Yan", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2010.12784", "submitter": "Vikram Gupta", "authors": "Vikram Gupta, Haoyue Shi, Kevin Gimpel, Mrinmaya Sachan", "title": "Clustering Contextualized Representations of Text for Unsupervised\n  Syntax Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore clustering of contextualized text representations for two\nunsupervised syntax induction tasks: part of speech induction (POSI) and\nconstituency labelling (CoLab). We propose a deep embedded clustering approach\nwhich jointly transforms these representations into a lower dimension cluster\nfriendly space and clusters them. We further enhance these representations by\naugmenting them with task-specific representations. We also explore the\neffectiveness of multilingual representations for different tasks and\nlanguages. With this work, we establish the first strong baselines for\nunsupervised syntax induction using contextualized text representations. We\nreport competitive performance on 45-tag POSI, state-of-the-art performance on\n12-tag POSI across 10 languages, and competitive results on CoLab.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 05:06:29 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gupta", "Vikram", ""], ["Shi", "Haoyue", ""], ["Gimpel", "Kevin", ""], ["Sachan", "Mrinmaya", ""]]}, {"id": "2010.12786", "submitter": "Huda Khayrallah", "authors": "Huda Khayrallah, Jo\\~ao Sedoc", "title": "Measuring the `I don't know' Problem through the Lens of Gricean\n  Quantity", "comments": "to appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the intrinsic evaluation of neural generative dialog models\nthrough the lens of Grice's Maxims of Conversation (1975). Based on the maxim\nof Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to\ndiagnose the `I don't know' problem, in which a dialog system produces generic\nresponses. The linguistically motivated RUQ diagnostic compares the model score\nof a generic response to that of the reference response. We find that for\nreasonable baseline models, `I don't know' is preferred over the reference the\nmajority of the time, but this can be reduced to less than 5% with\nhyperparameter tuning. RUQ allows for the direct analysis of the `I don't know'\nproblem, which has been addressed but not analyzed by prior work.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 05:16:36 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 18:55:37 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Khayrallah", "Huda", ""], ["Sedoc", "Jo\u00e3o", ""]]}, {"id": "2010.12787", "submitter": "Kung-Hsiang Huang", "authors": "Kung-Hsiang Huang, Nanyun Peng", "title": "Document-level Event Extraction with Efficient End-to-end Learning of\n  Cross-event Dependencies", "comments": "To appear at NAACL 2021 WNU workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully understanding narratives often requires identifying events in the\ncontext of whole documents and modeling the event relations. However,\ndocument-level event extraction is a challenging task as it requires the\nextraction of event and entity coreference, and capturing arguments that span\nacross different sentences. Existing works on event extraction usually confine\non extracting events from single sentences, which fail to capture the\nrelationships between the event mentions at the scale of a document, as well as\nthe event arguments that appear in a different sentence than the event trigger.\nIn this paper, we propose an end-to-end model leveraging Deep Value Networks\n(DVN), a structured prediction algorithm, to efficiently capture cross-event\ndependencies for document-level event extraction. Experimental results show\nthat our approach achieves comparable performance to CRF-based models on ACE05,\nwhile enjoys significantly higher computational efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 05:28:16 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 19:58:32 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 22:47:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Kung-Hsiang", ""], ["Peng", "Nanyun", ""]]}, {"id": "2010.12789", "submitter": "Limin Zhang", "authors": "Limin Zhang", "title": "New Approaches for Natural Language Understanding based on the Idea that\n  Natural Language encodes both Information and its Processing Procedures", "comments": "15 pages, 9 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We must recognize that natural language is a way of information encoding, and\nit encodes not only the information but also the procedures for how information\nis processed. To understand natural language, the same as we conceive and\ndesign computer languages, the first step is to separate information (or data)\nand the processing procedures of information (or data). In natural language,\nsome processing procedures of data are encoded directly as the structure chunk\nand the pointer chunk (this paper has reclassified lexical chunks as the data\nchunk, structure chunk, and the pointer chunk); some processing procedures of\ndata imply in sentences structures; some requests of processing procedures are\nexpressed by information senders and processed by information receivers. For\nthe data parts, the classification encoding system of attribute information and\nthe information organization architecture (including constitutional structures\nof information sets and the hierarchy between the information sets) were\ndiscussed. In section 2, the theoretical part elaborated in section 2 has been\nverified in examples and proofed that the studies in this paper have achieved\nthe goal of enabling machines to understand the information conveyed in the\ndialogue. In section 4, the author summarizes the basic conditions of\n\"Understanding\", rethinks what \"Understanding\" is and how to proceed. The study\nin this paper provides a practical, theoretical basis and research methods for\nNLU. It also can be applied in large-scale and multi-type information\nprocessing in the artificial intelligence (AI) area.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 05:40:47 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:45:38 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 16:50:32 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhang", "Limin", ""]]}, {"id": "2010.12794", "submitter": "Zihan Wang", "authors": "Zihan Wang and Dheeraj Mekala and Jingbo Shang", "title": "X-Class: Text Classification with Extremely Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore to conduct text classification with extremely weak\nsupervision, i.e., only relying on the surface text of class names. This is a\nmore challenging setting than the seed-driven weak supervision, which allows a\nfew seed words per class. We opt to attack this problem from a representation\nlearning perspective -- ideal document representations should lead to very\nclose results between clustering and the desired classification. In particular,\none can classify the same corpus differently (e.g., based on topics and\nlocations), so document representations must be adaptive to the given class\nnames. We propose a novel framework X-Class to realize it. Specifically, we\nfirst estimate comprehensive class representations by incrementally adding the\nmost similar word to each class until inconsistency appears. Following a\ntailored mixture of class attention mechanisms, we obtain the document\nrepresentation via a weighted average of contextualized token representations.\nWe then cluster and align the documents to classes with the prior of each\ndocument assigned to its nearest class. Finally, we pick the most confident\ndocuments from each cluster to train a text classifier. Extensive experiments\ndemonstrate that X-Class can rival and even outperform seed-driven weakly\nsupervised methods on 7 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:09:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Zihan", ""], ["Mekala", "Dheeraj", ""], ["Shang", "Jingbo", ""]]}, {"id": "2010.12795", "submitter": "Navita Goyal", "authors": "Navita Goyal, Roodram Paneri, Ayush Agarwal, Udit Kalani, Abhilasha\n  Sancheti, Niyati Chhaya", "title": "CaM-Gen:Causally-aware Metric-guided Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content is created for a well-defined purpose, often described by a metric or\na signal represented in the form of structured information. The relationship\nbetween the metrics or the goal of a target content and the content itself are\nnon-trivial. While large scale language models show promising text generation\ncapabilities, guiding and informing the generated text with external metrics is\nchallenging. These metrics and the content tend to have inherent relationships\nand not all of them may directly impact the content. We introduce a CaM-Gen:\nCausally-aware Generative Networks guided by user-defined input metrics\nincorporating the causal relationships between the metric and the content\nfeatures. We leverage causal inference techniques to identify the causally\nsignificant aspects of text that leads to the target metric and then explicitly\nguide the generative model towards these by a feedback mechanism. We propose\nthis mechanism for variational autoencoder-based and transformer-based\ngenerative models. The proposed models beat baselines in terms of the target\nmetric accuracy while maintaining the fluency and the language quality of the\ngenerated text. To the best of our knowledge, this is one of the early attempts\nat incorporating a metric-guide using causal inference towards controlled\ngeneration.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:17:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Goyal", "Navita", ""], ["Paneri", "Roodram", ""], ["Agarwal", "Ayush", ""], ["Kalani", "Udit", ""], ["Sancheti", "Abhilasha", ""], ["Chhaya", "Niyati", ""]]}, {"id": "2010.12800", "submitter": "Xinliang (Frederick) Zhang", "authors": "Xinliang Frederick Zhang, Heming Sun, Xiang Yue, Emmett Jesrani, Simon\n  Lin, Huan Sun", "title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSpecifically, similar to a standard FAQ dataset, COUGH consists of three parts:\nFAQ Bank, User Query Bank and Annotated Relevance Set. FAQ Bank contains ~16K\nFAQ items scraped from 55 credible websites (e.g., CDC and WHO). For\nevaluation, we introduce User Query Bank and Annotated Relevance Set, where the\nformer contains 1201 human-paraphrased queries while the latter contains ~32\nhuman-annotated FAQ items for each query. We analyze COUGH by testing different\nFAQ retrieval models built on top of BM25 and BERT, among which the best model\nachieves 0.29 under P@5, indicating that the dataset presents a great challenge\nfor future research. Our dataset is freely available at\nhttps://github.com/sunlab-osu/covid-faq.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:30:59 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Xinliang Frederick", ""], ["Sun", "Heming", ""], ["Yue", "Xiang", ""], ["Jesrani", "Emmett", ""], ["Lin", "Simon", ""], ["Sun", "Huan", ""]]}, {"id": "2010.12808", "submitter": "Xiaodong Yu", "authors": "Xiaodong Yu, Wenpeng Yin, Dan Roth", "title": "Paired Representation Learning for Event and Entity Coreference", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-reference of Events and of Entities are commonly formulated as binary\nclassification problems, given a pair of events or entities as input. Earlier\nwork addressed the main challenge in these problems -- the representation of\neach element in the input pair by: (i) modelling the representation of one\nelement (event or entity) without considering the other element in the pair;\n(ii) encoding all attributes of one element (e.g., arguments of an event) into\na single non-interpretable vector, thus losing the ability to compare\ncross-element attributes. In this work we propose paired representation\nlearning (PairedRL) for coreference resolution. Given a pair of elements\n(Events or Entities) our model treats the pair's sentences as a single sequence\nso that each element in the pair learns its representation by encoding its own\ncontext as well the other element's context. In addition, when representing\nevents, PairedRL is structured in that it represents the event's arguments to\nfacilitate their individual contribution to the final prediction. As we show,\nin both (within-document & cross-document) event and entity coreference\nbenchmarks, our unified approach, PairedRL, outperforms prior state of the art\nsystems with a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:55:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yu", "Xiaodong", ""], ["Yin", "Wenpeng", ""], ["Roth", "Dan", ""]]}, {"id": "2010.12812", "submitter": "Zexuan Zhong", "authors": "Zexuan Zhong and Danqi Chen", "title": "A Frustratingly Easy Approach for Entity and Relation Extraction", "comments": "NAACL 2021. Our code and models are available at\n  https://github.com/princeton-nlp/PURE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end relation extraction aims to identify named entities and extract\nrelations between them. Most recent work models these two subtasks jointly,\neither by casting them in one structured prediction framework, or performing\nmulti-task learning through shared representations. In this work, we present a\nsimple pipelined approach for entity and relation extraction, and establish the\nnew state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC),\nobtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint\nmodels with the same pre-trained encoders. Our approach essentially builds on\ntwo independent encoders and merely uses the entity model to construct the\ninput for the relation model. Through a series of careful examinations, we\nvalidate the importance of learning distinct contextual representations for\nentities and relations, fusing entity information early in the relation model,\nand incorporating global context. Finally, we also present an efficient\napproximation to our approach which requires only one pass of both entity and\nrelation encoders at inference time, achieving an 8-16$\\times$ speedup with a\nslight reduction in accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:14:01 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:48:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Zhong", "Zexuan", ""], ["Chen", "Danqi", ""]]}, {"id": "2010.12813", "submitter": "Kevin Lin", "authors": "Catherine Chen, Kevin Lin, Dan Klein", "title": "Constructing Taxonomies from Pretrained Language Models", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for constructing taxonomic trees (e.g., WordNet) using\npretrained language models. Our approach is composed of two modules, one that\npredicts parenthood relations and another that reconciles those predictions\ninto trees. The parenthood prediction module produces likelihood scores for\neach potential parent-child pair, creating a graph of parent-child relation\nscores. The tree reconciliation module treats the task as a graph optimization\nproblem and outputs the maximum spanning tree of this graph. We train our model\non subtrees sampled from WordNet, and test on non-overlapping WordNet subtrees.\nWe show that incorporating web-retrieved glosses can further improve\nperformance. On the task of constructing subtrees of English WordNet, the model\nachieves 66.7 ancestor F1, a 20.0% relative increase over the previous best\npublished result on this task. In addition, we convert the original English\ndataset into nine other languages using Open Multilingual WordNet and extend\nour results across these languages.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:16:21 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 02:37:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Catherine", ""], ["Lin", "Kevin", ""], ["Klein", "Dan", ""]]}, {"id": "2010.12820", "submitter": "Emily Sheng", "authors": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng", "title": "\"Nice Try, Kiddo\": Investigating Ad Hominems in Dialogue Responses", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad hominem attacks are those that target some feature of a person's character\ninstead of the position the person is maintaining. These attacks are harmful\nbecause they propagate implicit biases and diminish a person's credibility.\nSince dialogue systems respond directly to user input, it is important to study\nad hominems in dialogue responses. To this end, we propose categories of ad\nhominems, compose an annotated dataset, and build a classifier to analyze human\nand dialogue system responses to English Twitter posts. We specifically compare\nresponses to Twitter topics about marginalized communities (#BlackLivesMatter,\n#MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad\nhominems could further amplify the skew of power away from marginalized\npopulations. Furthermore, we propose a constrained decoding technique that uses\nsalient $n$-gram similarity as a soft constraint for top-$k$ sampling to reduce\nthe amount of ad hominems generated. Our results indicate that 1) responses\nfrom both humans and DialoGPT contain more ad hominems for discussions around\nmarginalized communities, 2) different quantities of ad hominems in the\ntraining data can influence the likelihood of generating ad hominems, and 3) we\ncan use constrained decoding techniques to reduce ad hominems in generated\ndialogue responses.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:37:49 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:22:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sheng", "Emily", ""], ["Chang", "Kai-Wei", ""], ["Natarajan", "Premkumar", ""], ["Peng", "Nanyun", ""]]}, {"id": "2010.12821", "submitter": "Hyung Won Chung", "authors": "Hyung Won Chung, Thibault F\\'evry, Henry Tsai, Melvin Johnson,\n  Sebastian Ruder", "title": "Rethinking embedding coupling in pre-trained language models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We re-evaluate the standard practice of sharing weights between input and\noutput embeddings in state-of-the-art pre-trained language models. We show that\ndecoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input\nembedding of multilingual models. By reallocating the input embedding\nparameters in the Transformer layers, we achieve dramatically better\nperformance on standard natural language understanding tasks with the same\nnumber of parameters during fine-tuning. We also show that allocating\nadditional capacity to the output embedding provides benefits to the model that\npersist through the fine-tuning stage even though the output embedding is\ndiscarded after pre-training. Our analysis shows that larger output embeddings\nprevent the model's last layers from overspecializing to the pre-training task\nand encourage Transformer representations to be more general and more\ntransferable to other tasks and languages. Harnessing these findings, we are\nable to train models that achieve strong performance on the XTREME benchmark\nwithout increasing the number of parameters at the fine-tuning stage.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:43:00 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chung", "Hyung Won", ""], ["F\u00e9vry", "Thibault", ""], ["Tsai", "Henry", ""], ["Johnson", "Melvin", ""], ["Ruder", "Sebastian", ""]]}, {"id": "2010.12825", "submitter": "Rochelle Choenni", "authors": "Rochelle Choenni, Ekaterina Shutova", "title": "Cross-neutralising: Probing for joint encoding of linguistic information\n  in multilingual models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual sentence encoders are widely used to transfer NLP models across\nlanguages. The success of this transfer is, however, dependent on the model's\nability to encode the patterns of cross-lingual similarity and variation. Yet,\nlittle is known as to how these models are able to do this. We propose a simple\nmethod to study how relationships between languages are encoded in two\nstate-of-the-art multilingual models (i.e. M-BERT and XLM-R). The results\nprovide insight into their information sharing mechanisms and suggest that\nlinguistic properties are encoded jointly across typologically-similar\nlanguages in these models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:55:32 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 16:21:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Choenni", "Rochelle", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2010.12826", "submitter": "Felix Faltings", "authors": "Felix Faltings and Michel Galley and Gerold Hintz and Chris Brockett\n  and Chris Quirk and Jianfeng Gao and Bill Dolan", "title": "Text Editing by Command", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prevailing paradigm in neural text generation is one-shot generation, where\ntext is produced in a single step. The one-shot setting is inadequate, however,\nwhen the constraints the user wishes to impose on the generated text are\ndynamic, especially when authoring longer documents. We address this limitation\nwith an interactive text generation setting in which the user interacts with\nthe system by issuing commands to edit existing text. To this end, we propose a\nnovel text editing task, and introduce WikiDocEdits, a dataset of\nsingle-sentence edits crawled from Wikipedia. We show that our Interactive\nEditor, a transformer-based model trained on this dataset, outperforms\nbaselines and obtains positive results in both automatic and human evaluations.\nWe present empirical and qualitative analyses of this model's performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:00:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Faltings", "Felix", ""], ["Galley", "Michel", ""], ["Hintz", "Gerold", ""], ["Brockett", "Chris", ""], ["Quirk", "Chris", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "2010.12827", "submitter": "Amane Sugiyama", "authors": "Amane Sugiyama and Naoki Yoshinaga", "title": "Context-aware Decoder for Neural Machine Translation using a Target-side\n  Document-Level Language Model", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many context-aware neural machine translation models have been\nproposed to incorporate contexts in translation, most of those models are\ntrained end-to-end on parallel documents aligned in sentence-level. Because\nonly a few domains (and language pairs) have such document-level parallel data,\nwe cannot perform accurate context-aware translation in most domains. We\ntherefore present a simple method to turn a sentence-level translation model\ninto a context-aware model by incorporating a document-level language model\ninto the decoder. Our context-aware decoder is built upon only a sentence-level\nparallel corpora and monolingual corpora; thus no document-level parallel data\nis needed. In a theoretical viewpoint, the core part of this work is the novel\nrepresentation of contextual information using point-wise mutual information\nbetween context and the current sentence. We show the effectiveness of our\napproach in three language pairs, English to French, English to Russian, and\nJapanese to English, by evaluation in \\textsc{bleu} and contrastive tests for\ncontext-aware translation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:06:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Sugiyama", "Amane", ""], ["Yoshinaga", "Naoki", ""]]}, {"id": "2010.12828", "submitter": "Haoyu Zhang", "authors": "Haoyu Zhang, Dingkun Long, Guangwei Xu, Pengjun Xie, Fei Huang, Ji\n  Wang", "title": "Keyphrase Extraction with Dynamic Graph Convolutional Networks and\n  Diversified Inference", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase extraction (KE) aims to summarize a set of phrases that accurately\nexpress a concept or a topic covered in a given document. Recently,\nSequence-to-Sequence (Seq2Seq) based generative framework is widely used in KE\ntask, and it has obtained competitive performance on various benchmarks. The\nmain challenges of Seq2Seq methods lie in acquiring informative latent document\nrepresentation and better modeling the compositionality of the target\nkeyphrases set, which will directly affect the quality of generated keyphrases.\nIn this paper, we propose to adopt the Dynamic Graph Convolutional Networks\n(DGCN) to solve the above two problems simultaneously. Concretely, we explore\nto integrate dependency trees with GCN for latent representation learning.\nMoreover, the graph structure in our model is dynamically modified during the\nlearning process according to the generated keyphrases. To this end, our\napproach is able to explicitly learn the relations within the keyphrases\ncollection and guarantee the information interchange between encoder and\ndecoder in both directions. Extensive experiments on various KE benchmark\ndatasets demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:11:23 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Haoyu", ""], ["Long", "Dingkun", ""], ["Xu", "Guangwei", ""], ["Xie", "Pengjun", ""], ["Huang", "Fei", ""], ["Wang", "Ji", ""]]}, {"id": "2010.12829", "submitter": "Xian Li", "authors": "Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino,\n  Alexei Baevski, Alexis Conneau, Michael Auli", "title": "Multilingual Speech Translation with Efficient Finetuning of Pretrained\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective approach to build multilingual\nspeech-to-text (ST) translation by efficient transfer learning from pretrained\nspeech encoder and text decoder. Our key finding is that a minimalistic LNA\n(LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and\ncross-modality transfer ability by only finetuning less than 10% of the\npretrained parameters. This enables effectively leveraging large pretrained\nmodels with low training cost. Using wav2vec 2.0 for acoustic modeling, and\nmBART for multilingual text generation, our approach advanced the new\nstate-of-the-art for 34 translation directions (and surpassing cascaded ST for\n23 of them) on large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on\naverage across 15 En-X directions and +5.1 BLEU on average across 19 X-En\ndirections). Our approach demonstrates strong zero-shot performance in a\nmany-to-many multilingual model (+5.7 BLEU on average across 18 non-English\ndirections), making it an appealing approach for attaining high-quality speech\ntranslation with improved parameter and data efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:15:08 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 07:25:28 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 05:48:49 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2021 08:16:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Xian", ""], ["Wang", "Changhan", ""], ["Tang", "Yun", ""], ["Tran", "Chau", ""], ["Tang", "Yuqing", ""], ["Pino", "Juan", ""], ["Baevski", "Alexei", ""], ["Conneau", "Alexis", ""], ["Auli", "Michael", ""]]}, {"id": "2010.12831", "submitter": "Liunian Harold Li", "authors": "Liunian Harold Li, Haoxuan You, Zhecan Wang, Alireza Zareian, Shih-Fu\n  Chang, Kai-Wei Chang", "title": "Unsupervised Vision-and-Language Pre-training Without Parallel Images\n  and Captions", "comments": "NAACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained contextual vision-and-language (V&L) models have achieved\nimpressive performance on various benchmarks. However, existing models require\na large amount of parallel image-caption data for pre-training. Such data are\ncostly to collect and require cumbersome curation. Inspired by unsupervised\nmachine translation, we investigate if a strong V&L representation model can be\nlearned through unsupervised pre-training without image-caption corpora. In\nparticular, we propose to conduct ``mask-and-predict'' pre-training on\ntext-only and image-only corpora and introduce the object tags detected by an\nobject recognition model as anchor points to bridge two modalities. We find\nthat such a simple approach achieves performance close to a model pre-trained\nwith aligned data, on four English V&L benchmarks. Our work challenges the\nwidely held notion that aligned data is necessary for V&L pre-training, while\nsignificantly reducing the amount of supervision needed for V&L models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:17:54 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 23:54:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Liunian Harold", ""], ["You", "Haoxuan", ""], ["Wang", "Zhecan", ""], ["Zareian", "Alireza", ""], ["Chang", "Shih-Fu", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2010.12834", "submitter": "Saadia Gabriel", "authors": "Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao", "title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization", "comments": "ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural language models can generate text with remarkable fluency and\ncoherence, controlling for factual correctness in generation remains an open\nresearch question. This major discrepancy between the surface-level fluency and\nthe content-level correctness of neural generation has motivated a new line of\nresearch that seeks automatic metrics for evaluating the factuality of machine\ntext. In this paper, we introduce GO FIGURE, a meta-evaluation framework for\nevaluating factuality evaluation metrics. We propose five necessary and\nintuitive conditions to evaluate factuality metrics on diagnostic factuality\ndata across three different summarization tasks. Our benchmark analysis on ten\nfactuality metrics reveals that our meta-evaluation framework provides a robust\nand efficient evaluation that is extensible to multiple types of factual\nconsistency and standard generation metrics, including QA metrics. It also\nreveals that while QA metrics generally improve over standard metrics that\nmeasure factuality across domains, performance is highly dependent on the way\nin which questions are generated.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:30:20 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 18:21:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gabriel", "Saadia", ""], ["Celikyilmaz", "Asli", ""], ["Jha", "Rahul", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2010.12836", "submitter": "Alexander Fabbri", "authors": "Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan\n  Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad", "title": "Improving Zero and Few-Shot Abstractive Summarization with Intermediate\n  Fine-tuning and Data Augmentation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models pretrained with self-supervised objectives on large text corpora\nachieve state-of-the-art performance on English text summarization tasks.\nHowever, these models are typically fine-tuned on hundreds of thousands of data\npoints, an infeasible requirement when applying summarization to new, niche\ndomains. In this work, we introduce a novel and generalizable method, called\nWikiTransfer, for fine-tuning pretrained models for summarization in an\nunsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained\nmodels on pseudo-summaries, produced from generic Wikipedia data, which contain\ncharacteristics of the target dataset, such as the length and level of\nabstraction of the desired summaries. WikiTransfer models achieve\nstate-of-the-art, zero-shot abstractive summarization performance on the\nCNN-DailyMail dataset and demonstrate the effectiveness of our approach on\nthree additional diverse datasets. These models are more robust to noisy data\nand also achieve better or comparable few-shot performance using 10 and 100\ntraining examples when compared to few-shot transfer from other summarization\ndatasets. To further boost performance, we employ data augmentation via\nround-trip translation as well as introduce a regularization term for improved\nfew-shot transfer. To understand the role of dataset aspects in transfer\nperformance and the quality of the resulting output summaries, we further study\nthe effect of the components of our unsupervised fine-tuning data and analyze\nfew-shot performance using both automatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:36:49 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 13:04:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Fabbri", "Alexander R.", ""], ["Han", "Simeng", ""], ["Li", "Haoyuan", ""], ["Li", "Haoran", ""], ["Ghazvininejad", "Marjan", ""], ["Joty", "Shafiq", ""], ["Radev", "Dragomir", ""], ["Mehdad", "Yashar", ""]]}, {"id": "2010.12844", "submitter": "Sahisnu Mazumder", "authors": "Sahisnu Mazumder, Oriana Riva", "title": "FLIN: A Flexible Natural Language Interface for Web Navigation", "comments": "Accepted to NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI assistants can now carry out tasks for users by directly interacting with\nwebsite UIs. Current semantic parsing and slot-filling techniques cannot\nflexibly adapt to many different websites without being constantly re-trained.\nWe propose FLIN, a natural language interface for web navigation that maps user\ncommands to concept-level actions (rather than low-level UI actions), thus\nbeing able to flexibly adapt to different websites and handle their transient\nnature. We frame this as a ranking problem: given a user command and a webpage,\nFLIN learns to score the most relevant navigation instruction (involving action\nand parameter values). To train and evaluate FLIN, we collect a dataset using\nnine popular websites from three domains. Our results show that FLIN was able\nto adapt to new websites in a given domain.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 09:11:26 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 23:39:18 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mazumder", "Sahisnu", ""], ["Riva", "Oriana", ""]]}, {"id": "2010.12850", "submitter": "Semih Yavuz", "authors": "Shiyang Li, Semih Yavuz, Kazuma Hashimoto, Jia Li, Tong Niu, Nazneen\n  Rajani, Xifeng Yan, Yingbo Zhou and Caiming Xiong", "title": "CoCo: Controllable Counterfactuals for Evaluating Dialogue State\n  Trackers", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue state trackers have made significant progress on benchmark datasets,\nbut their generalization capability to novel and realistic scenarios beyond the\nheld-out conversations is less understood. We propose controllable\ncounterfactuals (CoCo) to bridge this gap and evaluate dialogue state tracking\n(DST) models on novel scenarios, i.e., would the system successfully tackle the\nrequest if the user responded differently but still consistently with the\ndialogue flow? CoCo leverages turn-level belief states as counterfactual\nconditionals to produce novel conversation scenarios in two steps: (i)\ncounterfactual goal generation at turn-level by dropping and adding slots\nfollowed by replacing slot values, (ii) counterfactual conversation generation\nthat is conditioned on (i) and consistent with the dialogue flow. Evaluating\nstate-of-the-art DST models on MultiWOZ dataset with CoCo-generated\ncounterfactuals results in a significant performance drop of up to 30.8% (from\n49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used\ntechniques like paraphrasing only affect the accuracy by at most 2%. Human\nevaluations show that COCO-generated conversations perfectly reflect the\nunderlying user goal with more than 95% accuracy and are as human-like as the\noriginal conversations, further strengthening its reliability and promise to be\nadopted as part of the robustness evaluation of DST models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 09:39:35 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 08:22:41 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 06:35:21 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Shiyang", ""], ["Yavuz", "Semih", ""], ["Hashimoto", "Kazuma", ""], ["Li", "Jia", ""], ["Niu", "Tong", ""], ["Rajani", "Nazneen", ""], ["Yan", "Xifeng", ""], ["Zhou", "Yingbo", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.12854", "submitter": "Tushar Khot", "authors": "Shih-Ting Lin and Ashish Sabharwal and Tushar Khot", "title": "ReadOnce Transformers: Reusable Representations of Text for Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large-scale language models are extremely effective when directly\nfine-tuned on many end-tasks, such models learn to extract information and\nsolve the task simultaneously from end-task supervision. This is wasteful, as\nthe general problem of gathering information from a document is mostly\ntask-independent and need not be re-learned from scratch each time. Moreover,\nonce the information has been captured in a computable representation, it can\nnow be re-used across examples, leading to faster training and evaluation of\nmodels. We present a transformer-based approach, ReadOnce Transformers, that is\ntrained to build such information-capturing representations of text. Our model\ncompresses the document into a variable-length task-independent representation\nthat can now be re-used in different examples and tasks, thereby requiring a\ndocument to only be read once. Additionally, we extend standard text-to-text\nmodels to consume our ReadOnce Representations along with text to solve\nmultiple downstream tasks. We show our task-independent representations can be\nused for multi-hop QA, abstractive QA, and summarization. We observe 2x-5x\nspeedups compared to standard text-to-text models, while also being able to\nhandle long documents that would normally exceed the length limit of current\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 09:53:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lin", "Shih-Ting", ""], ["Sabharwal", "Ashish", ""], ["Khot", "Tushar", ""]]}, {"id": "2010.12858", "submitter": "Benjamin Muller", "authors": "Benjamin Muller and Antonis Anastasopoulos and Beno\\^it Sagot and\n  Djam\\'e Seddah", "title": "When Being Unseen from mBERT is just the Beginning: Handling New\n  Languages With Multilingual Language Models", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Transfer learning based on pretraining language models on a large amount of\nraw data has become a new norm to reach state-of-the-art performance in NLP.\nStill, it remains unclear how this approach should be applied for unseen\nlanguages that are not covered by any available large-scale multilingual\nlanguage model and for which only a small amount of raw data is generally\navailable. In this work, by comparing multilingual and monolingual models, we\nshow that such models behave in multiple ways on unseen languages. Some\nlanguages greatly benefit from transfer learning and behave similarly to\nclosely related high resource languages whereas others apparently do not.\nFocusing on the latter, we show that this failure to transfer is largely\nrelated to the impact of the script used to write such languages.\nTransliterating those languages improves very significantly the ability of\nlarge-scale multilingual language models on downstream tasks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:15:03 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 09:56:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Muller", "Benjamin", ""], ["Anastasopoulos", "Antonis", ""], ["Sagot", "Beno\u00eet", ""], ["Seddah", "Djam\u00e9", ""]]}, {"id": "2010.12864", "submitter": "Xisen Jin", "authors": "Xisen Jin, Francesco Barbieri, Brendan Kennedy, Aida Mostafazadeh\n  Davani, Leonardo Neves, Xiang Ren", "title": "On Transferability of Bias Mitigation Effects in Language Model\n  Fine-Tuning", "comments": "14 pages; Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuned language models have been shown to exhibit biases against\nprotected groups in a host of modeling tasks such as text classification and\ncoreference resolution. Previous works focus on detecting these biases,\nreducing bias in data representations, and using auxiliary training objectives\nto mitigate bias during fine-tuning. Although these techniques achieve bias\nreduction for the task and domain at hand, the effects of bias mitigation may\nnot directly transfer to new tasks, requiring additional data collection and\ncustomized annotation of sensitive attributes, and re-evaluation of appropriate\nfairness metrics. We explore the feasibility and benefits of upstream bias\nmitigation (UBM) for reducing bias on downstream tasks, by first applying bias\nmitigation to an upstream model through fine-tuning and subsequently using it\nfor downstream fine-tuning. We find, in extensive experiments across hate\nspeech detection, toxicity detection, occupation prediction, and coreference\nresolution tasks over various bias factors, that the effects of UBM are indeed\ntransferable to new downstream tasks or domains via fine-tuning, creating less\nbiased downstream models than directly fine-tuning on the downstream task or\ntransferring from a vanilla upstream model. Though challenges remain, we show\nthat UBM promises more efficient and accessible bias mitigation in LM\nfine-tuning.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:36:11 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 23:34:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jin", "Xisen", ""], ["Barbieri", "Francesco", ""], ["Kennedy", "Brendan", ""], ["Davani", "Aida Mostafazadeh", ""], ["Neves", "Leonardo", ""], ["Ren", "Xiang", ""]]}, {"id": "2010.12868", "submitter": "Yongchang Hao", "authors": "Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu and\n  Xing Wang", "title": "Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine\n  Translation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-Autoregressive machine Translation (NAT) models have demonstrated\nsignificant inference speedup but suffer from inferior translation accuracy.\nThe common practice to tackle the problem is transferring the Autoregressive\nmachine Translation (AT) knowledge to NAT models, e.g., with knowledge\ndistillation. In this work, we hypothesize and empirically verify that AT and\nNAT encoders capture different linguistic properties of source sentences.\nTherefore, we propose to adopt Multi-Task learning to transfer the AT knowledge\nto NAT models through encoder sharing. Specifically, we take the AT model as an\nauxiliary task to enhance NAT model performance. Experimental results on WMT14\nEnglish-German and WMT16 English-Romanian datasets show that the proposed\nMulti-Task NAT achieves significant improvements over the baseline NAT models.\nFurthermore, the performance on large-scale WMT19 and WMT20 English-German\ndatasets confirm the consistency of our proposed method. In addition,\nexperimental results demonstrate that our Multi-Task NAT is complementary to\nknowledge distillation, the standard knowledge transfer method for NAT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:00:58 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 07:24:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hao", "Yongchang", ""], ["He", "Shilin", ""], ["Jiao", "Wenxiang", ""], ["Tu", "Zhaopeng", ""], ["Lyu", "Michael", ""], ["Wang", "Xing", ""]]}, {"id": "2010.12871", "submitter": "Zein Shaheen", "authors": "Zein Shaheen, Gerhard Wohlgenannt, Erwin Filtz", "title": "Large Scale Legal Text Classification Using Transformer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large multi-label text classification is a challenging Natural Language\nProcessing (NLP) problem that is concerned with text classification for\ndatasets with thousands of labels. We tackle this problem in the legal domain,\nwhere datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc\nvocabulary were created within the legal information systems of the European\nUnion. The EuroVoc taxonomy includes around 7000 concepts. In this work, we\nstudy the performance of various recent transformer-based models in combination\nwith strategies such as generative pretraining, gradual unfreezing and\ndiscriminative learning rates in order to reach competitive classification\nperformance, and present new state-of-the-art results of 0.661 (F1) for\nJRC-Acquis and 0.754 for EURLEX57K. Furthermore, we quantify the impact of\nindividual steps, such as language model fine-tuning or gradual unfreezing in\nan ablation study, and provide reference dataset splits created with an\niterative stratification algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:03:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Shaheen", "Zein", ""], ["Wohlgenannt", "Gerhard", ""], ["Filtz", "Erwin", ""]]}, {"id": "2010.12872", "submitter": "Aaron Chan", "authors": "Mrigank Raman, Aaron Chan, Siddhant Agarwal, Peifeng Wang, Hansen\n  Wang, Sungchul Kim, Ryan Rossi, Handong Zhao, Nedim Lipka, Xiang Ren", "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted\n  Perturbation", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) have helped neural models improve performance on\nvarious knowledge-intensive tasks, like question answering and item\nrecommendation. By using attention over the KG, such KG-augmented models can\nalso \"explain\" which KG information was most relevant for making a given\nprediction. In this paper, we question whether these models are really behaving\nas we expect. We show that, through a reinforcement learning policy (or even\nsimple heuristics), one can produce deceptively perturbed KGs, which maintain\nthe downstream performance of the original KG while significantly deviating\nfrom the original KG's semantics and structure. Our findings raise doubts about\nKG-augmented models' ability to reason about KG information and give sensible\nexplanations.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:04:45 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 21:56:00 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 10:43:59 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 07:40:13 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 05:50:57 GMT"}, {"version": "v6", "created": "Mon, 3 May 2021 18:38:15 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Raman", "Mrigank", ""], ["Chan", "Aaron", ""], ["Agarwal", "Siddhant", ""], ["Wang", "Peifeng", ""], ["Wang", "Hansen", ""], ["Kim", "Sungchul", ""], ["Rossi", "Ryan", ""], ["Zhao", "Handong", ""], ["Lipka", "Nedim", ""], ["Ren", "Xiang", ""]]}, {"id": "2010.12873", "submitter": "Jun Yan", "authors": "Jun Yan, Mrigank Raman, Aaron Chan, Tianyu Zhang, Ryan Rossi, Handong\n  Zhao, Sungchul Kim, Nedim Lipka, Xiang Ren", "title": "Learning Contextualized Knowledge Structures for Commonsense Reasoning", "comments": "Accepted to Findings of ACL-IJCNLP 2021. Code and data:\n  https://github.com/INK-USC/HGN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, knowledge graph (KG) augmented models have achieved noteworthy\nsuccess on various commonsense reasoning tasks. However, KG edge (fact)\nsparsity and noisy edge extraction/generation often hinder models from\nobtaining useful knowledge to reason over. To address these issues, we propose\na new KG-augmented model: Hybrid Graph Network (HGN). Unlike prior methods, HGN\nlearns to jointly contextualize extracted and generated knowledge by reasoning\nover both within a unified graph structure. Given the task input context and an\nextracted KG subgraph, HGN is trained to generate embeddings for the subgraph's\nmissing edges to form a \"hybrid\" graph, then reason over the hybrid graph while\nfiltering out context-irrelevant edges. We demonstrate HGN's effectiveness\nthrough considerable performance gains across four commonsense reasoning\nbenchmarks, plus a user study on edge validness and helpfulness.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:09:16 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 11:59:06 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 08:16:35 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yan", "Jun", ""], ["Raman", "Mrigank", ""], ["Chan", "Aaron", ""], ["Zhang", "Tianyu", ""], ["Rossi", "Ryan", ""], ["Zhao", "Handong", ""], ["Kim", "Sungchul", ""], ["Lipka", "Nedim", ""], ["Ren", "Xiang", ""]]}, {"id": "2010.12881", "submitter": "Arturo Oncevay", "authors": "Arturo Oncevay and Kervy Rivas Rojas", "title": "Revisiting Neural Language Modelling with Syllables", "comments": "5 pages (main paper), 4 pages of Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language modelling is regularly analysed at word, subword or character units,\nbut syllables are seldom used. Syllables provide shorter sequences than\ncharacters, they can be extracted with rules, and their segmentation typically\nrequires less specialised effort than identifying morphemes. We reconsider\nsyllables for an open-vocabulary generation task in 20 languages. We use\nrule-based syllabification methods for five languages and address the rest with\na hyphenation tool, which behaviour as syllable proxy is validated. With a\ncomparable perplexity, we show that syllables outperform characters, annotated\nmorphemes and unsupervised subwords. Finally, we also study the overlapping of\nsyllables concerning other subword pieces and discuss some limitations and\nopportunities.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:44:41 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Oncevay", "Arturo", ""], ["Rojas", "Kervy Rivas", ""]]}, {"id": "2010.12882", "submitter": "Mingyang Chen", "authors": "Mingyang Chen, Wen Zhang, Zonggang Yuan, Yantao Jia, Huajun Chen", "title": "FedE: Embedding Knowledge Graphs in Federated Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) consisting of triples are always incomplete, so it's\nimportant to do Knowledge Graph Completion (KGC) by predicting missing triples.\nMulti-Source KG is a common situation in real KG applications which can be\nviewed as a set of related individual KGs where different KGs contains\nrelations of different aspects of entities. It's intuitive that, for each\nindividual KG, its completion could be greatly contributed by the triples\ndefined and labeled in other ones. However, because of the data privacy and\nsensitivity, a set of relevant knowledge graphs cannot complement each other's\nKGC by just collecting data from different knowledge graphs together.\nTherefore, in this paper, we introduce federated setting to keep their privacy\nwithout triple transferring between KGs and apply it in embedding knowledge\ngraph, a typical method which have proven effective for KGC in the past decade.\nWe propose a Federated Knowledge Graph Embedding framework FedE, focusing on\nlearning knowledge graph embeddings by aggregating locally-computed updates.\nFinally, we conduct extensive experiments on datasets derived from KGE\nbenchmark datasets and results show the effectiveness of our proposed FedE.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:52:05 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Mingyang", ""], ["Zhang", "Wen", ""], ["Yuan", "Zonggang", ""], ["Jia", "Yantao", ""], ["Chen", "Huajun", ""]]}, {"id": "2010.12884", "submitter": "Ximing Lu", "authors": "Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra\n  Bhagavatula, Yejin Choi", "title": "NeuroLogic Decoding: (Un)supervised Neural Text Generation with\n  Predicate Logic Constraints", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional text generation often requires lexical constraints, i.e., which\nwords should or shouldn't be included in the output text. While the dominant\nrecipe for conditional text generation has been large-scale pretrained language\nmodels that are finetuned on the task-specific training data, such models do\nnot learn to follow the underlying constraints reliably, even when supervised\nwith large amounts of task-specific examples.\n  We propose NeuroLogic Decoding, a simple yet effective algorithm that enables\nneural language models -- supervised or not -- to generate fluent text while\nsatisfying complex lexical constraints. Our approach is powerful yet efficient.\nIt handles any set of lexical constraints that is expressible under predicate\nlogic, while its asymptotic runtime is equivalent to conventional beam search.\n  Empirical results on four benchmarks show that NeuroLogic Decoding\noutperforms previous approaches, including algorithms that handle a subset of\nour constraints. Moreover, we find that unsupervised models with NeuroLogic\nDecoding often outperform supervised models with conventional decoding, even\nwhen the latter is based on considerably larger networks. Our results suggest\nthe limit of large-scale neural networks for fine-grained controllable\ngeneration and the promise of inference-time algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:55:22 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 18:59:28 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lu", "Ximing", ""], ["West", "Peter", ""], ["Zellers", "Rowan", ""], ["Bras", "Ronan Le", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.12885", "submitter": "Tong Niu", "authors": "Tong Niu, Semih Yavuz, Yingbo Zhou, Huan Wang, Nitish Shirish Keskar,\n  Caiming Xiong", "title": "Unsupervised Paraphrase Generation via Dynamic Blocking", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dynamic Blocking, a decoding algorithm which enables large-scale\npretrained autoregressive models (such as BART, T5, GPT-2 and XLNet) to\ngenerate high-quality paraphrases in an unsupervised setting. In order to\nobtain an alternative surface form, whenever the language model emits a token\nthat is present in the source sequence, we prevent the model from generating\nthe subsequent source token for the next time step. We show that our approach\nachieves state-of-the-art results on benchmark datasets when compared to\nprevious unsupervised approaches, and is even comparable with strong\nsupervised, in-domain models. We also propose a new automatic metric based on\nself-BLEU and BERTscore which not only discourages the model from copying the\ninput through, but also evaluates text similarity based on distributed\nrepresentations, hence avoiding reliance on exact keyword matching. In\naddition, we demonstrate that our model generalizes across languages without\nany additional training.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:55:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Niu", "Tong", ""], ["Yavuz", "Semih", ""], ["Zhou", "Yingbo", ""], ["Wang", "Huan", ""], ["Keskar", "Nitish Shirish", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.12912", "submitter": "Camilo Thorne", "authors": "Camilo Thorne and Saber Akhondi", "title": "Word Embeddings for Chemical Patent Natural Language Processing", "comments": "Extended version of an extended abstract presented (and reviewed) at\n  the Latinx Workshop at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We evaluate chemical patent word embeddings against known biomedical\nembeddings and show that they outperform the latter extrinsically and\nintrinsically. We also show that using contextualized embeddings can induce\npredictive models of reasonable performance for this domain over a relatively\nsmall gold standard.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:03:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Thorne", "Camilo", ""], ["Akhondi", "Saber", ""]]}, {"id": "2010.12919", "submitter": "Reid Pryzant", "authors": "Reid Pryzant, Dallas Card, Dan Jurafsky, Victor Veitch, Dhanya Sridhar", "title": "Causal Effects of Linguistic Properties", "comments": "To appear at NAACL 2021 (Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using observational data to estimate the causal\neffects of linguistic properties. For example, does writing a complaint\npolitely lead to a faster response time? How much will a positive product\nreview increase sales? This paper addresses two technical challenges related to\nthe problem before developing a practical method. First, we formalize the\ncausal quantity of interest as the effect of a writer's intent, and establish\nthe assumptions necessary to identify this from observational data. Second, in\npractice, we only have access to noisy proxies for the linguistic properties of\ninterest -- e.g., predictions from classifiers and lexicons. We propose an\nestimator for this setting and prove that its bias is bounded when we perform\nan adjustment for the text. Based on these results, we introduce TextCause, an\nalgorithm for estimating causal effects of linguistic properties. The method\nleverages (1) distant supervision to improve the quality of noisy proxies, and\n(2) a pre-trained language model (BERT) to adjust for the text. We show that\nthe proposed method outperforms related approaches when estimating the effect\nof Amazon review sentiment on semi-simulated sales figures. Finally, we present\nan applied case study investigating the effects of complaint politeness on\nbureaucratic response times.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 15:43:37 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:30:35 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 17:01:13 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 20:10:34 GMT"}, {"version": "v5", "created": "Mon, 14 Jun 2021 14:10:05 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Pryzant", "Reid", ""], ["Card", "Dallas", ""], ["Jurafsky", "Dan", ""], ["Veitch", "Victor", ""], ["Sridhar", "Dhanya", ""]]}, {"id": "2010.12925", "submitter": "Camilo Thorne", "authors": "Dhruba Pujary and Camilo Thorne and Wilker Aziz", "title": "Disease Normalization with Graph Embeddings", "comments": "This is a pre-print of a paper to appear in the proceedings of the\n  IntelliSys 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The detection and normalization of diseases in biomedical texts are key\nbiomedical natural language processing tasks. Disease names need not only be\nidentified, but also normalized or linked to clinical taxonomies describing\ndiseases such as MeSH. In this paper we describe deep learning methods that\ntackle both tasks. We train and test our methods on the known NCBI disease\nbenchmark corpus. We propose to represent disease names by leveraging MeSH's\ngraphical structure together with the lexical information available in the\ntaxonomy using graph embeddings. We also show that combining neural named\nentity recognition models with our graph-based entity linking methods via\nmultitask learning leads to improved disease recognition in the NCBI corpus.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 16:25:05 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Pujary", "Dhruba", ""], ["Thorne", "Camilo", ""], ["Aziz", "Wilker", ""]]}, {"id": "2010.12937", "submitter": "Arun Kumar Singh", "authors": "Arun Kumar Singh, Sushant Dave, Dr. Prathosh A. P., Prof. Brejesh Lall\n  and Shresth Mehta", "title": "A Benchmark Corpus and Neural Approach for Sanskrit Derivative Nouns\n  Analysis", "comments": "6 pages, 2 figures, EACL 2021 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents first benchmark corpus of Sanskrit Pratyaya (suffix) and\ninflectional words (padas) formed due to suffixes along with neural network\nbased approaches to process the formation and splitting of inflectional words.\nInflectional words spans the primary and secondary derivative nouns as the\nscope of current work. Pratyayas are an important dimension of morphological\nanalysis of Sanskrit texts. There have been Sanskrit Computational Linguistics\ntools for processing and analyzing Sanskrit texts. Unfortunately there has not\nbeen any work to standardize & validate these tools specifically for derivative\nnouns analysis. In this work, we prepared a Sanskrit suffix benchmark corpus\ncalled Pratyaya-Kosh to evaluate the performance of tools. We also present our\nown neural approach for derivative nouns analysis while evaluating the same on\nmost prominent Sanskrit Morphological Analysis tools. This benchmark will be\nfreely dedicated and available to researchers worldwide and we hope it will\nmotivate all to improve morphological analysis in Sanskrit Language.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 17:22:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Singh", "Arun Kumar", ""], ["Dave", "Sushant", ""], ["P.", "Dr. Prathosh A.", ""], ["Lall", "Prof. Brejesh", ""], ["Mehta", "Shresth", ""]]}, {"id": "2010.12940", "submitter": "Arun Kumar Singh", "authors": "Sushant Dave, Arun Kumar Singh, Dr. Prathosh A. P. and Prof. Brejesh\n  Lall", "title": "Neural Compound-Word (Sandhi) Generation and Splitting in Sanskrit\n  Language", "comments": "6 pages, 3 figures, CODS-COMAD 2021, IIIT Bangalore, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes neural network based approaches to the process of the\nformation and splitting of word-compounding, respectively known as the Sandhi\nand Vichchhed, in Sanskrit language. Sandhi is an important idea essential to\nmorphological analysis of Sanskrit texts. Sandhi leads to word transformations\nat word boundaries. The rules of Sandhi formation are well defined but complex,\nsometimes optional and in some cases, require knowledge about the nature of the\nwords being compounded. Sandhi split or Vichchhed is an even more difficult\ntask given its non uniqueness and context dependence. In this work, we propose\nthe route of formulating the problem as a sequence to sequence prediction task,\nusing modern deep learning techniques. Being the first fully data driven\ntechnique, we demonstrate that our model has an accuracy better than the\nexisting methods on multiple standard datasets, despite not using any\nadditional lexical or morphological resources. The code is being made available\nat https://github.com/IITD-DataScience/Sandhi_Prakarana\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 18:02:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Dave", "Sushant", ""], ["Singh", "Arun Kumar", ""], ["P.", "Dr. Prathosh A.", ""], ["Lall", "Prof. Brejesh", ""]]}, {"id": "2010.12973", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Ruoming Pang, Yu Zhang, Shigeki Karita", "title": "Unsupervised Learning of Disentangled Speech Content and Style\n  Representation", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for unsupervised learning of speech representation\ndisentangling contents and styles. Our model consists of: (1) a local encoder\nthat captures per-frame information; (2) a global encoder that captures\nper-utterance information; and (3) a conditional decoder that reconstructs\nspeech given local and global latent variables. Our experiments show that (1)\nthe local latent variables encode speech contents, as reconstructed speech can\nbe recognized by ASR with low word error rates (WER), even with a different\nglobal encoding; (2) the global latent variables encode speaker style, as\nreconstructed speech shares speaker identity with the source utterance of the\nglobal encoding. Additionally, we demonstrate an useful application from our\npre-trained model, where we can train a speaker recognition model from the\nglobal latent variables and achieve high accuracy by fine-tuning with as few\ndata as one label per speaker.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 20:16:03 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 04:01:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tjandra", "Andros", ""], ["Pang", "Ruoming", ""], ["Zhang", "Yu", ""], ["Karita", "Shigeki", ""]]}, {"id": "2010.13002", "submitter": "Sam Shleifer", "authors": "Sam Shleifer and Alexander M. Rush", "title": "Pre-trained Summarization Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent state-of-the-art approaches to summarization utilize large pre-trained\nTransformer models. Distilling these models to smaller student models has\nbecome critically important for practical use; however there are many different\ndistillation methods proposed by the NLP literature. Recent work on distilling\nBERT for classification and regression tasks shows strong performance using\ndirect knowledge distillation. Alternatively, machine translation practitioners\ndistill using pseudo-labeling, where a small model is trained on the\ntranslations of a larger model. A third, simpler approach is to 'shrink and\nfine-tune' (SFT), which avoids any explicit distillation by copying parameters\nto a smaller student model and then fine-tuning. We compare these three\napproaches for distillation of Pegasus and BART, the current and former state\nof the art, pre-trained summarization models, and find that SFT outperforms\nknowledge distillation and pseudo-labeling on the CNN/DailyMail dataset, but\nunder-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch\nCode and checkpoints of different sizes are available through Hugging Face\ntransformers here http://tiny.cc/4iy0tz.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 23:15:43 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 04:47:59 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Shleifer", "Sam", ""], ["Rush", "Alexander M.", ""]]}, {"id": "2010.13009", "submitter": "Jianguo Zhang", "authors": "Jian-Guo Zhang, Kazuma Hashimoto, Wenhao Liu, Chien-Sheng Wu, Yao Wan,\n  Philip S. Yu, Richard Socher, Caiming Xiong", "title": "Discriminative Nearest Neighbor Few-Shot Intent Detection by\n  Transferring Natural Language Inference", "comments": "19 pages, accepted by EMNLP 2020 main conference as a long paper.\n  Code will be available at https://github.com/salesforce/DNNC-few-shot-intent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent detection is one of the core components of goal-oriented dialog\nsystems, and detecting out-of-scope (OOS) intents is also a practically\nimportant skill. Few-shot learning is attracting much attention to mitigate\ndata scarcity, but OOS detection becomes even more challenging. In this paper,\nwe present a simple yet effective approach, discriminative nearest neighbor\nclassification with deep self-attention. Unlike softmax classifiers, we\nleverage BERT-style pairwise encoding to train a binary classifier that\nestimates the best matched training example for a user input. We propose to\nboost the discriminative ability by transferring a natural language inference\n(NLI) model. Our extensive experiments on a large-scale multi-domain intent\ndetection task show that our method achieves more stable and accurate in-domain\nand OOS detection accuracy than RoBERTa-based classifiers and embedding-based\nnearest neighbor approaches. More notably, the NLI transfer enables our 10-shot\nmodel to perform competitively with 50-shot or even full-shot classifiers,\nwhile we can keep the inference time constant by leveraging a faster embedding\nretrieval model.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 00:39:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Jian-Guo", ""], ["Hashimoto", "Kazuma", ""], ["Liu", "Wenhao", ""], ["Wu", "Chien-Sheng", ""], ["Wan", "Yao", ""], ["Yu", "Philip S.", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.13028", "submitter": "Sayyed Zahiri", "authors": "Sayyed M. Zahiri and Ali Ahmadvand", "title": "CRAB: Class Representation Attentive BERT for Hate Speech Identification\n  in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, social media platforms have hosted an explosion of hate\nspeech and objectionable content. The urgent need for effective automatic hate\nspeech detection models have drawn remarkable investment from companies and\nresearchers. Social media posts are generally short and their semantics could\ndrastically be altered by even a single token. Thus, it is crucial for this\ntask to learn context-aware input representations, and consider relevancy\nscores between input embeddings and class representations as an additional\nsignal. To accommodate these needs, this paper introduces CRAB (Class\nRepresentation Attentive BERT), a neural model for detecting hate speech in\nsocial media. The model benefits from two semantic representations: (i)\ntrainable token-wise and sentence-wise class representations, and (ii)\ncontextualized input embeddings from state-of-the-art BERT encoder. To\ninvestigate effectiveness of CRAB, we train our model on Twitter data and\ncompare it against strong baselines. Our results show that CRAB achieves 1.89%\nrelative improved Macro-averaged F1 over state-of-the-art baseline. The results\nof this research open an opportunity for the future research on automated\nabusive behavior detection in social media\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 04:11:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zahiri", "Sayyed M.", ""], ["Ahmadvand", "Ali", ""]]}, {"id": "2010.13031", "submitter": "Jian Du", "authors": "Xiaoying Li, Suyuan Peng, Jian Du", "title": "Towards Medical Knowmetrics: Representing and Computing Medical\n  Knowledge using Semantic Predications as the Knowledge Unit and the\n  Uncertainty as the Knowledge Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In China, Prof. Hongzhou Zhao and Zeyuan Liu are the pioneers of the concept\n\"knowledge unit\" and \"knowmetrics\" for measuring knowledge. However, the\ndefinition of \"computable knowledge object\" remains controversial so far in\ndifferent fields. For example, it is defined as 1) quantitative scientific\nconcept in natural science and engineering, 2) knowledge point in the field of\neducation research, and 3) semantic predications, i.e.,\nSubject-Predicate-Object (SPO) triples in biomedical fields. The Semantic\nMEDLINE Database (SemMedDB), a high-quality public repository of SPO triples\nextracted from medical literature, provides a basic data infrastructure for\nmeasuring medical knowledge. In general, the study of extracting SPO triples as\ncomputable knowledge unit from unstructured scientific text has been\noverwhelmingly focusing on scientific knowledge per se. Since the SPO triples\nwould be possibly extracted from hypothetical, speculative statements or even\nconflicting and contradictory assertions, the knowledge status (i.e., the\nuncertainty), which serves as an integral and critical part of scientific\nknowledge has been largely overlooked. This article aims to put forward a\nframework for Medical Knowmetrics using the SPO triples as the knowledge unit\nand the uncertainty as the knowledge context. The lung cancer publications\ndataset is used to validate the proposed framework. The uncertainty of medical\nknowledge and how its status evolves over time indirectly reflect the strength\nof competing knowledge claims, and the probability of certainty for a given SPO\ntriple. We try to discuss the new insights using the uncertainty-centric\napproaches to detect research fronts, and identify knowledge claims with high\ncertainty level, in order to improve the efficacy of knowledge-driven decision\nsupport.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 04:27:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Xiaoying", ""], ["Peng", "Suyuan", ""], ["Du", "Jian", ""]]}, {"id": "2010.13040", "submitter": "Zhaoning Li", "authors": "Zhaoning Li and Jiangtao Ren", "title": "Fine-tuning ERNIE for chest abnormal imaging signs extraction", "comments": "30 pages, 5 figures, 8 tables", "journal-ref": "Journal of Biomedical Informatics, Volume 108, 2020, 103492", "doi": "10.1016/j.jbi.2020.103492", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest imaging reports describe the results of chest radiography procedures.\nAutomatic extraction of abnormal imaging signs from chest imaging reports has a\npivotal role in clinical research and a wide range of downstream medical tasks.\nHowever, there are few studies on information extraction from Chinese chest\nimaging reports. In this paper, we formulate chest abnormal imaging sign\nextraction as a sequence tagging and matching problem. On this basis, we\npropose a transferred abnormal imaging signs extractor with pretrained ERNIE as\nthe backbone, named EASON (fine-tuning ERNIE with CRF for Abnormal Signs\nExtractiON), which can address the problem of data insufficiency. In addition,\nto assign the attributes (the body part and degree) to corresponding abnormal\nimaging signs from the results of the sequence tagging model, we design a\nsimple but effective tag2relation algorithm based on the nature of chest\nimaging report text. We evaluate our method on the corpus provided by a medical\nbig data company, and the experimental results demonstrate that our method\nachieves significant and consistent improvement compared to other baselines.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 05:18:14 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 13:34:24 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Zhaoning", ""], ["Ren", "Jiangtao", ""]]}, {"id": "2010.13047", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Yosuke Higuchi, Kevin Duh, Tatsuya Kawahara, Shinji\n  Watanabe", "title": "Orthros: Non-autoregressive End-to-end Speech Translation with\n  Dual-decoder", "comments": "Accepted at IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast inference speed is an important goal towards real-world deployment of\nspeech translation (ST) systems. End-to-end (E2E) models based on the\nencoder-decoder architecture are more suitable for this goal than traditional\ncascaded systems, but their effectiveness regarding decoding speed has not been\nexplored so far. Inspired by recent progress in non-autoregressive (NAR)\nmethods in text-based translation, which generates target tokens in parallel by\neliminating conditional dependencies, we study the problem of NAR decoding for\nE2E-ST. We propose a novel NAR E2E-ST framework, Orthros, in which both NAR and\nautoregressive (AR) decoders are jointly trained on the shared speech encoder.\nThe latter is used for selecting better translation among various length\ncandidates generated from the former, which dramatically improves the\neffectiveness of a large length beam with negligible overhead. We further\ninvestigate effective length prediction methods from speech inputs and the\nimpact of vocabulary sizes. Experiments on four benchmarks show the\neffectiveness of the proposed method in improving inference speed while\nmaintaining competitive translation quality compared to state-of-the-art AR\nE2E-ST systems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 06:35:30 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 19:06:58 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 15:01:11 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Higuchi", "Yosuke", ""], ["Duh", "Kevin", ""], ["Kawahara", "Tatsuya", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2010.13049", "submitter": "Gongqi Lin", "authors": "Gongqi Lin, Yuan Miao, Xiaoyong Yang, Wenwu Ou, Lizhen Cui, Wei Guo,\n  Chunyan Miao", "title": "Commonsense knowledge adversarial dataset that challenges ELECTRA", "comments": "To appear in ICARCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge is critical in human reading comprehension. While\nmachine comprehension has made significant progress in recent years, the\nability in handling commonsense knowledge remains limited. Synonyms are one of\nthe most widely used commonsense knowledge. Constructing adversarial dataset is\nan important approach to find weak points of machine comprehension models and\nsupport the design of solutions. To investigate machine comprehension models'\nability in handling the commonsense knowledge, we created a Question and Answer\nDataset with common knowledge of Synonyms (QADS). QADS are questions generated\nbased on SQuAD 2.0 by applying commonsense knowledge of synonyms. The synonyms\nare extracted from WordNet. Words often have multiple meanings and synonyms. We\nused an enhanced Lesk algorithm to perform word sense disambiguation to\nidentify synonyms for the context. ELECTRA achieves the state-of-art result on\nthe SQuAD 2.0 dataset in 2019. With scale, ELECTRA can achieve similar\nperformance as BERT does. However, QADS shows that ELECTRA has little ability\nto handle commonsense knowledge of synonyms. In our experiment, ELECTRA-small\ncan achieve 70% accuracy on SQuAD 2.0, but only 20% on QADS. ELECTRA-large did\nnot perform much better. Its accuracy on SQuAD 2.0 is 88% but dropped\nsignificantly to 26% on QADS. In our earlier experiments, BERT, although also\nfailed badly on QADS, was not as bad as ELECTRA. The result shows that even\ntop-performing NLP models have little ability to handle commonsense knowledge\nwhich is essential in reading comprehension.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 07:17:45 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lin", "Gongqi", ""], ["Miao", "Yuan", ""], ["Yang", "Xiaoyong", ""], ["Ou", "Wenwu", ""], ["Cui", "Lizhen", ""], ["Guo", "Wei", ""], ["Miao", "Chunyan", ""]]}, {"id": "2010.13057", "submitter": "Sathvik Nair", "authors": "Sathvik Nair, Mahesh Srinivasan, Stephan Meylan", "title": "Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense\n  Knowledge", "comments": "To appear in proceedings of the Cognitive Aspects of the Lexicon\n  Workshop at the 28th International Conference on Computational Linguistics\n  (COLING)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding context-dependent variation in word meanings is a key aspect of\nhuman language comprehension supported by the lexicon. Lexicographic resources\n(e.g., WordNet) capture only some of this context-dependent variation; for\nexample, they often do not encode how closely senses, or discretized word\nmeanings, are related to one another. Our work investigates whether recent\nadvances in NLP, specifically contextualized word embeddings, capture\nhuman-like distinctions between English word senses, such as polysemy and\nhomonymy. We collect data from a behavioral, web-based experiment, in which\nparticipants provide judgments of the relatedness of multiple WordNet senses of\na word in a two-dimensional spatial arrangement task. We find that\nparticipants' judgments of the relatedness between senses are correlated with\ndistances between senses in the BERT embedding space. Homonymous senses (e.g.,\nbat as mammal vs. bat as sports equipment) are reliably more distant from one\nanother in the embedding space than polysemous ones (e.g., chicken as animal\nvs. chicken as meat). Our findings point towards the potential utility of\ncontinuous-space representations of sense meanings.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 07:56:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nair", "Sathvik", ""], ["Srinivasan", "Mahesh", ""], ["Meylan", "Stephan", ""]]}, {"id": "2010.13062", "submitter": "Zhixiang Li", "authors": "Mengzhe Li, Yudan Wang, Ying Zhao and Zhixiang Li", "title": "Transgender Community Sentiment Analysis from Social Media Data: A\n  Natural Language Processing Approach", "comments": "5 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Transgender community is experiencing a huge disparity in mental health\nconditions compared with the general population. Interpreting the social medial\ndata posted by transgender people may help us understand the sentiments of\nthese sexual minority groups better and apply early interventions. In this\nstudy, we manually categorize 300 social media comments posted by transgender\npeople to the sentiment of negative, positive, and neutral. 5 machine learning\nalgorithms and 2 deep neural networks are adopted to build sentiment analysis\nclassifiers based on the annotated data. Results show that our annotations are\nreliable with a high Cohen's Kappa score over 0.8 across all three classes.\nLSTM model yields an optimal performance of accuracy over 0.85 and AUC of\n0.876. Our next step will focus on using advanced natural language processing\nalgorithms on a larger annotated dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 08:13:34 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Mengzhe", ""], ["Wang", "Yudan", ""], ["Zhao", "Ying", ""], ["Li", "Zhixiang", ""]]}, {"id": "2010.13094", "submitter": "Masahiro Kaneko", "authors": "Masahiro Kaneko and Danushka Bollegala", "title": "Autoencoding Improves Pre-trained Word Embeddings", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work investigating the geometry of pre-trained word embeddings have\nshown that word embeddings to be distributed in a narrow cone and by centering\nand projecting using principal component vectors one can increase the accuracy\nof a given set of pre-trained word embeddings. However, theoretically, this\npost-processing step is equivalent to applying a linear autoencoder to minimise\nthe squared l2 reconstruction error. This result contradicts prior work (Mu and\nViswanath, 2018) that proposed to remove the top principal components from\npre-trained embeddings. We experimentally verify our theoretical claims and\nshow that retaining the top principal components is indeed useful for improving\npre-trained word embeddings, without requiring access to additional linguistic\nresources or labelled data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 11:30:05 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 07:51:34 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kaneko", "Masahiro", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2010.13105", "submitter": "Gyuwan Kim", "authors": "Seongbin Kim, Gyuwan Kim, Seongjin Shin, Sangmin Lee", "title": "Two-stage Textual Knowledge Distillation for End-to-End Spoken Language\n  Understanding", "comments": "ICASSP 2021; 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches open a new way for more accurate and efficient spoken\nlanguage understanding (SLU) systems by alleviating the drawbacks of\ntraditional pipeline systems. Previous works exploit textual information for an\nSLU model via pre-training with automatic speech recognition or fine-tuning\nwith knowledge distillation. To utilize textual information more effectively,\nthis work proposes a two-stage textual knowledge distillation method that\nmatches utterance-level representations and predicted logits of two modalities\nduring pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a\nspeech encoder because it captures general and rich features. Furthermore, we\nimprove the performance, especially in a low-resource scenario, with data\naugmentation methods by randomly masking spans of discrete audio tokens and\ncontextualized hidden representations. Consequently, we push the\nstate-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy\nin the full dataset setting and 99.5% in the 10% subset setting. Throughout the\nablation studies, we empirically verify that all used methods are crucial to\nthe final performance, providing the best practice for spoken language\nunderstanding. Code is available at https://github.com/clovaai/textual-kd-slu.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:36:05 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 11:09:38 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Kim", "Seongbin", ""], ["Kim", "Gyuwan", ""], ["Shin", "Seongjin", ""], ["Lee", "Sangmin", ""]]}, {"id": "2010.13128", "submitter": "Mokanarangan Thayaparan", "authors": "Mokanarangan Thayaparan, Marco Valentino, Andr\\'e Freitas", "title": "ExplanationLP: Abductive Reasoning for Explainable Science Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for answering and explaining multiple-choice\nscience questions by reasoning on grounding and abstract inference chains. This\npaper frames question answering as an abductive reasoning problem, constructing\nplausible explanations for each choice and then selecting the candidate with\nthe best explanation as the final answer. Our system, ExplanationLP, elicits\nexplanations by constructing a weighted graph of relevant facts for each\ncandidate answer and extracting the facts that satisfy certain structural and\nsemantic constraints. To extract the explanations, we employ a linear\nprogramming formalism designed to select the optimal subgraph. The graphs'\nweighting function is composed of a set of parameters, which we fine-tune to\noptimize answer selection performance. We carry out our experiments on the\nWorldTree and ARC-Challenge corpus to empirically demonstrate the following\nconclusions: (1) Grounding-Abstract inference chains provides the semantic\ncontrol to perform explainable abductive reasoning (2) Efficiency and\nrobustness in learning with a fewer number of parameters by outperforming\ncontemporary explainable and transformer-based approaches in a similar setting\n(3) Generalisability by outperforming SOTA explainable approaches on general\nscience question sets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 14:49:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2010.13168", "submitter": "Tenzin Singhay Bhotia", "authors": "Vaibhav Kumar, Tenzin Singhay Bhotia, Vaibhav Kumar", "title": "Fair Embedding Engine: A Library for Analyzing and Mitigating Gender\n  Bias in Word Embeddings", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-contextual word embedding models have been shown to inherit human-like\nstereotypical biases of gender, race and religion from the training corpora. To\ncounter this issue, a large body of research has emerged which aims to mitigate\nthese biases while keeping the syntactic and semantic utility of embeddings\nintact. This paper describes Fair Embedding Engine (FEE), a library for\nanalysing and mitigating gender bias in word embeddings. FEE combines various\nstate of the art techniques for quantifying, visualising and mitigating gender\nbias in word embeddings under a standard abstraction. FEE will aid\npractitioners in fast track analysis of existing debiasing methods on their\nembedding models. Further, it will allow rapid prototyping of new methods by\nevaluating their performance on a suite of standard metrics.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 17:31:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kumar", "Vaibhav", ""], ["Bhotia", "Tenzin Singhay", ""], ["Kumar", "Vaibhav", ""]]}, {"id": "2010.13192", "submitter": "Alexandra Chronopoulou", "authors": "Alexandra Chronopoulou, Dario Stojanovski, Viktor Hangya, Alexander\n  Fraser", "title": "The LMU Munich System for the WMT 2020 Unsupervised Machine Translation\n  Shared Task", "comments": "WMT Unsupervised Shared Task 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the submission of LMU Munich to the WMT 2020\nunsupervised shared task, in two language directions, German<->Upper Sorbian.\nOur core unsupervised neural machine translation (UNMT) system follows the\nstrategy of Chronopoulou et al. (2020), using a monolingual pretrained language\ngeneration model (on German) and fine-tuning it on both German and Upper\nSorbian, before initializing a UNMT model, which is trained with online\nbacktranslation. Pseudo-parallel data obtained from an unsupervised statistical\nmachine translation (USMT) system is used to fine-tune the UNMT model. We also\napply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more\nrobust system. We additionally experiment with residual adapters and find them\nuseful in the Upper Sorbian->German direction. We explore sampling during\nbacktranslation and curriculum learning to use SMT translations in a more\nprincipled way. Finally, we ensemble our best-performing systems and reach a\nBLEU score of 32.4 on German->Upper Sorbian and 35.2 on Upper Sorbian->German.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 19:04:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chronopoulou", "Alexandra", ""], ["Stojanovski", "Dario", ""], ["Hangya", "Viktor", ""], ["Fraser", "Alexander", ""]]}, {"id": "2010.13270", "submitter": "Yosuke Higuchi", "authors": "Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe, Tetsuji Ogawa,\n  Tetsunori Kobayashi", "title": "Improved Mask-CTC for Non-Autoregressive End-to-End ASR", "comments": "Accepted to ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real-world deployment of automatic speech recognition (ASR), the system\nis desired to be capable of fast inference while relieving the requirement of\ncomputational resources. The recently proposed end-to-end ASR system based on\nmask-predict with connectionist temporal classification (CTC), Mask-CTC,\nfulfills this demand by generating tokens in a non-autoregressive fashion.\nWhile Mask-CTC achieves remarkably fast inference speed, its recognition\nperformance falls behind that of conventional autoregressive (AR) systems. To\nboost the performance of Mask-CTC, we first propose to enhance the encoder\nnetwork architecture by employing a recently proposed architecture called\nConformer. Next, we propose new training and decoding methods by introducing\nauxiliary objective to predict the length of a partial target sequence, which\nallows the model to delete or insert tokens during inference. Experimental\nresults on different ASR tasks show that the proposed approaches improve\nMask-CTC significantly, outperforming a standard CTC model (15.5% $\\rightarrow$\n9.1% WER on WSJ). Moreover, Mask-CTC now achieves competitive results to AR\nmodels with no degradation of inference speed ($<$ 0.1 RTF using CPU). We also\nshow a potential application of Mask-CTC to end-to-end speech translation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 01:22:35 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 05:46:18 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Higuchi", "Yosuke", ""], ["Inaguma", "Hirofumi", ""], ["Watanabe", "Shinji", ""], ["Ogawa", "Tetsuji", ""], ["Kobayashi", "Tetsunori", ""]]}, {"id": "2010.13374", "submitter": "Bruce W. Lee", "authors": "Bruce W. Lee and Jason Lee", "title": "LXPER Index 2.0: Improving Text Readability Assessment Model for L2\n  English Students in Korea", "comments": "NLP-TEA 2020, Association for Computational Linguistics", "journal-ref": "Proceedings of the 6th Workshop on Natural Language Processing\n  Techniques for Educational Applications, 2020", "doi": null, "report-no": "2020.nlptea-1.3", "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing a text readability assessment model specifically for texts in a\nforeign English Language Training (ELT) curriculum has never had much attention\nin the field of Natural Language Processing. Hence, most developed models show\nextremely low accuracy for L2 English texts, up to the point where not many\neven serve as a fair comparison. In this paper, we investigate a text\nreadability assessment model for L2 English learners in Korea. In accordance,\nwe improve and expand the Text Corpus of the Korean ELT curriculum\n(CoKEC-text). Each text is labeled with its target grade level. We train our\nmodel with CoKEC-text and significantly improve the accuracy of readability\nassessment for texts in the Korean ELT curriculum.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:03:14 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 17:04:03 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 18:37:49 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 11:04:49 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Lee", "Bruce W.", ""], ["Lee", "Jason", ""]]}, {"id": "2010.13378", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Nasim Nouri, Franck Dernoncourt, Dejing Dou,\n  Thien Huu Nguyen", "title": "Introducing Syntactic Structures into Target Opinion Word Extraction\n  with Deep Learning", "comments": "accepted at EMNLP 2020 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted opinion word extraction (TOWE) is a sub-task of aspect based\nsentiment analysis (ABSA) which aims to find the opinion words for a given\naspect-term in a sentence. Despite their success for TOWE, the current deep\nlearning models fail to exploit the syntactic information of the sentences that\nhave been proved to be useful for TOWE in the prior research. In this work, we\npropose to incorporate the syntactic structures of the sentences into the deep\nlearning models for TOWE, leveraging the syntax-based opinion possibility\nscores and the syntactic connections between the words. We also introduce a\nnovel regularization technique to improve the performance of the deep learning\nmodels based on the representation distinctions between the words in TOWE. The\nproposed model is extensively analyzed and achieves the state-of-the-art\nperformance on four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:13:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Nouri", "Nasim", ""], ["Dernoncourt", "Franck", ""], ["Dou", "Dejing", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2010.13382", "submitter": "Young Jin Kim", "authors": "Young Jin Kim and Hany Hassan Awadalla", "title": "FastFormers: Highly Efficient Transformer Models for Natural Language\n  Understanding", "comments": "Accepted to SustaiNLP 2020 at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models are the state-of-the-art for Natural Language\nUnderstanding (NLU) applications. Models are getting bigger and better on\nvarious tasks. However, Transformer models remain computationally challenging\nsince they are not efficient at inference-time compared to traditional\napproaches. In this paper, we present FastFormers, a set of recipes to achieve\nefficient inference-time performance for Transformer-based models on various\nNLU tasks. We show how carefully utilizing knowledge distillation, structured\npruning and numerical optimization can lead to drastic improvements on\ninference efficiency. We provide effective recipes that can guide practitioners\nto choose the best settings for various NLU tasks and pretrained models.\nApplying the proposed recipes to the SuperGLUE benchmark, we achieve from 9.8x\nup to 233.9x speed-up compared to out-of-the-box models on CPU. On GPU, we also\nachieve up to 12.4x speed-up with the presented methods. We show that\nFastFormers can drastically reduce cost of serving 100 million requests from\n4,223 USD to just 18 USD on an Azure F16s_v2 instance. This translates to a\nsustainable runtime by reducing energy consumption 6.9x - 125.8x according to\nthe metrics used in the SustaiNLP 2020 shared task.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:25:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kim", "Young Jin", ""], ["Awadalla", "Hany Hassan", ""]]}, {"id": "2010.13389", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Nasim Nour, Franck Dernoncourt, Quan Hung\n  Tran, Dejing Dou, Thien Huu Nguyen", "title": "Improving Aspect-based Sentiment Analysis with Gated Graph Convolutional\n  Networks and Syntax-based Regulation", "comments": "accepted at EMNLP 2020 findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based Sentiment Analysis (ABSA) seeks to predict the sentiment\npolarity of a sentence toward a specific aspect. Recently, it has been shown\nthat dependency trees can be integrated into deep learning models to produce\nthe state-of-the-art performance for ABSA. However, these models tend to\ncompute the hidden/representation vectors without considering the aspect terms\nand fail to benefit from the overall contextual importance scores of the words\nthat can be obtained from the dependency tree for ABSA. In this work, we\npropose a novel graph-based deep learning model to overcome these two issues of\nthe prior work on ABSA. In our model, gate vectors are generated from the\nrepresentation vectors of the aspect terms to customize the hidden vectors of\nthe graph-based models toward the aspect terms. In addition, we propose a\nmechanism to obtain the importance scores for each word in the sentences based\non the dependency trees that are then injected into the model to improve the\nrepresentation vectors for ABSA. The proposed model achieves the\nstate-of-the-art performance on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:36:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Nour", "Nasim", ""], ["Dernoncourt", "Franck", ""], ["Tran", "Quan Hung", ""], ["Dou", "Dejing", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2010.13391", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Tuan Ngo Nguyen, Thien Huu Nguyen", "title": "Graph Transformer Networks with Syntactic and Semantic Structures for\n  Event Argument Extraction", "comments": "accepted at EMNLP 2020 findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of Event Argument Extraction (EAE) is to find the role of each\nentity mention for a given event trigger word. It has been shown in the\nprevious works that the syntactic structures of the sentences are helpful for\nthe deep learning models for EAE. However, a major problem in such prior works\nis that they fail to exploit the semantic structures of the sentences to induce\neffective representations for EAE. Consequently, in this work, we propose a\nnovel model for EAE that exploits both syntactic and semantic structures of the\nsentences with the Graph Transformer Networks (GTNs) to learn more effective\nsentence structures for EAE. In addition, we introduce a novel inductive bias\nbased on information bottleneck to improve generalization of the EAE models.\nExtensive experiments are performed to demonstrate the benefits of the proposed\nmodel, leading to state-of-the-art performance for EAE on standard datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:41:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Nguyen", "Tuan Ngo", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2010.13404", "submitter": "Rifat Rahman", "authors": "Rifat Rahman", "title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model", "comments": "6 pages, 7 figures, 2020 23rd International Conference on Computer\n  and Information Technology (ICCIT),\n  https://ieeexplore.ieee.org/abstract/document/9392738", "journal-ref": null, "doi": "10.1109/ICCIT51783.2020.9392738", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:00:48 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 13:51:27 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 20:58:27 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Rahman", "Rifat", ""]]}, {"id": "2010.13415", "submitter": "Yucheng Wang", "authors": "Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu and\n  Limin Sun", "title": "TPLinker: Single-stage Joint Extraction of Entities and Relations\n  Through Token Pair Linking", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting entities and relations from unstructured text has attracted\nincreasing attention in recent years but remains challenging, due to the\nintrinsic difficulty in identifying overlapping relations with shared entities.\nPrior works show that joint learning can result in a noticeable performance\ngain. However, they usually involve sequential interrelated steps and suffer\nfrom the problem of exposure bias. At training time, they predict with the\nground truth conditions while at inference it has to make extraction from\nscratch. This discrepancy leads to error accumulation. To mitigate the issue,\nwe propose in this paper a one-stage joint extraction model, namely, TPLinker,\nwhich is capable of discovering overlapping relations sharing one or both\nentities while immune from the exposure bias. TPLinker formulates joint\nextraction as a token pair linking problem and introduces a novel handshaking\ntagging scheme that aligns the boundary tokens of entity pairs under each\nrelation type. Experiment results show that TPLinker performs significantly\nbetter on overlapping and multiple relation extraction, and achieves\nstate-of-the-art performance on two public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:35:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Yucheng", ""], ["Yu", "Bowen", ""], ["Zhang", "Yueyang", ""], ["Liu", "Tingwen", ""], ["Zhu", "Hongsong", ""], ["Sun", "Limin", ""]]}, {"id": "2010.13457", "submitter": "Henry Turner", "authors": "Henry Turner, Giulio Lovisotto and Ivan Martinovic", "title": "Speaker Anonymization with Distribution-Preserving X-Vector Generation\n  for the VoicePrivacy Challenge 2020", "comments": "5 pages Replacement: A small processing bug led to slightly incorrect\n  results. Conclusions remain the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Distribution-Preserving Voice Anonymization\ntechnique, as our submission to the VoicePrivacy Challenge 2020. We observe\nthat the challenge baseline system generates fake X-vectors which are very\nsimilar to each other, significantly more so than those extracted from organic\nspeakers. This difference arises from averaging many X-vectors from a pool of\nspeakers in the anonymization process, causing a loss of information. We\npropose a new method to generate fake X-vectors which overcomes these\nlimitations by preserving the distributional properties of X-vectors and their\nintra-similarity. We use population data to learn the properties of the\nX-vector space, before fitting a generative model which we use to sample fake\nX-vectors. We show how this approach generates X-vectors that more closely\nfollow the expected intra-similarity distribution of organic speaker X-vectors.\nOur method can be easily integrated with others as the anonymization component\nof the system and removes the need to distribute a pool of speakers to use\nduring the anonymization. Our approach leads to an increase in EER of up to\n$19.4\\%$ in males and $11.1\\%$ in females in scenarios where enrollment and\ntrial utterances are anonymized versus the baseline solution, demonstrating the\ndiversity of our generated voices.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:53:56 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 16:11:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Turner", "Henry", ""], ["Lovisotto", "Giulio", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2010.13515", "submitter": "Andrea Asperti", "authors": "Andrea Asperti and Stefano Dal Bianco", "title": "Syllabification of the Divine Comedy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a syllabification algorithm for the Divine Comedy using techniques\nfrom probabilistic and constraint programming. We particularly focus on the\nsynalephe, addressed in terms of the \"propensity\" of a word to take part in a\nsynalephe with adjacent words. We jointly provide an online vocabulary\ncontaining, for each word, information about its syllabification, the location\nof the tonic accent, and the aforementioned synalephe propensity, on the left\nand right sides. The algorithm is intrinsically nondeterministic, producing\ndifferent possible syllabifications for each verse, with different likelihoods;\nmetric constraints relative to accents on the 10th, 4th and 6th syllables are\nused to further reduce the solution space. The most likely syllabification is\nhence returned as output. We believe that this work could be a major milestone\nfor a lot of different investigations. From the point of view of digital\nhumanities it opens new perspectives on computer assisted analysis of digital\nsources, comprising automated detection of anomalous and problematic cases,\nmetric clustering of verses and their categorization, or more foundational\ninvestigations addressing e.g. the phonetic roles of consonants and vowels.\nFrom the point of view of text processing and deep learning, information about\nsyllabification and the location of accents opens a wide range of exciting\nperspectives, from the possibility of automatic learning syllabification of\nwords and verses, to the improvement of generative models, aware of metric\nissues, and more respectful of the expected musicality.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:14:14 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Asperti", "Andrea", ""], ["Bianco", "Stefano Dal", ""]]}, {"id": "2010.13544", "submitter": "Zhenzhen Li", "authors": "Zhenzhen Li, Jian-Yun Nie, Benyou Wang, Pan Du, Yuhan Zhang, Lixin\n  Zou, and Dongsheng Li", "title": "Meta-Learning for Neural Relation Classification with Distant\n  Supervision", "comments": "10 pages, 7 figures; corrected one encoding error in CIKM pdf", "journal-ref": "In Proceedings of CIKM, pp. 815-824. 2020", "doi": "10.1145/3340531.3412039", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision provides a means to create a large number of weakly\nlabeled data at low cost for relation classification. However, the resulting\nlabeled instances are very noisy, containing data with wrong labels. Many\napproaches have been proposed to select a subset of reliable instances for\nneural model training, but they still suffer from noisy labeling problem or\nunderutilization of the weakly-labeled data. To better select more reliable\ntraining instances, we introduce a small amount of manually labeled data as\nreference to guide the selection process. In this paper, we propose a\nmeta-learning based approach, which learns to reweight noisy training data\nunder the guidance of reference data. As the clean reference data is usually\nvery small, we propose to augment it by dynamically distilling the most\nreliable elite instances from the noisy data. Experiments on several datasets\ndemonstrate that the reference data can effectively guide the selection of\ntraining data, and our augmented approach consistently improves the performance\nof relation classification comparing to the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:52:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Zhenzhen", ""], ["Nie", "Jian-Yun", ""], ["Wang", "Benyou", ""], ["Du", "Pan", ""], ["Zhang", "Yuhan", ""], ["Zou", "Lixin", ""], ["Li", "Dongsheng", ""]]}, {"id": "2010.13556", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xiusi Chen, Yu Meng, Jiawei Han", "title": "Hierarchical Metadata-Aware Document Categorization under Weak\n  Supervision", "comments": "9 pages; Accepted to WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorizing documents into a given label hierarchy is intuitively appealing\ndue to the ubiquity of hierarchical topic structures in massive text corpora.\nAlthough related studies have achieved satisfying performance in fully\nsupervised hierarchical document classification, they usually require massive\nhuman-annotated training data and only utilize text information. However, in\nmany domains, (1) annotations are quite expensive where very few training\nsamples can be acquired; (2) documents are accompanied by metadata information.\nHence, this paper studies how to integrate the label hierarchy, metadata, and\ntext signals for document categorization under weak supervision. We develop\nHiMeCat, an embedding-based generative framework for our task. Specifically, we\npropose a novel joint representation learning module that allows simultaneous\nmodeling of category dependencies, metadata information and textual semantics,\nand we introduce a data augmentation module that hierarchically synthesizes\ntraining documents to complement the original, small-scale training set. Our\nexperiments demonstrate a consistent improvement of HiMeCat over competitive\nbaselines and validate the contribution of our representation learning and data\naugmentation modules.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 13:07:56 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 02:02:39 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Yu", ""], ["Chen", "Xiusi", ""], ["Meng", "Yu", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.13585", "submitter": "Reza Marzban", "authors": "Reza Marzban, Christopher John Crick", "title": "Interpreting convolutional networks trained on textual data", "comments": "9 pages, 6 figures, 5 tables", "journal-ref": null, "doi": "10.5220/0010205901960203", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many advances in the artificial intelligence field due to the\nemergence of deep learning. In almost all sub-fields, artificial neural\nnetworks have reached or exceeded human-level performance. However, most of the\nmodels are not interpretable. As a result, it is hard to trust their decisions,\nespecially in life and death scenarios. In recent years, there has been a\nmovement toward creating explainable artificial intelligence, but most work to\ndate has concentrated on image processing models, as it is easier for humans to\nperceive visual patterns. There has been little work in other fields like\nnatural language processing. In this paper, we train a convolutional model on\ntextual data and analyze the global logic of the model by studying its filter\nvalues. In the end, we find the most important words in our corpus to our\nmodels logic and remove the rest (95%). New models trained on just the 5% most\nimportant words can achieve the same performance as the original model while\nreducing training time by more than half. Approaches such as this will help us\nto understand NLP models, explain their decisions according to their word\nchoices, and improve them by finding blind spots and biases.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:12:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Marzban", "Reza", ""], ["Crick", "Christopher John", ""]]}, {"id": "2010.13588", "submitter": "Ozan Caglayan", "authors": "Ozan Caglayan, Pranava Madhyastha, Lucia Specia", "title": "Curious Case of Language Generation Evaluation Metrics: A Cautionary\n  Tale", "comments": "7 pages, accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluation of language generation systems is a well-studied problem\nin Natural Language Processing. While novel metrics are proposed every year, a\nfew popular metrics remain as the de facto metrics to evaluate tasks such as\nimage captioning and machine translation, despite their known limitations. This\nis partly due to ease of use, and partly because researchers expect to see them\nand know how to interpret them. In this paper, we urge the community for more\ncareful consideration of how they automatically evaluate their models by\ndemonstrating important failure cases on multiple datasets, language pairs and\ntasks. Our experiments show that metrics (i) usually prefer system outputs to\nhuman-authored texts, (ii) can be insensitive to correct translations of rare\nwords, (iii) can yield surprisingly high scores when given a single sentence as\nsystem output for the entire test set.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 13:57:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Caglayan", "Ozan", ""], ["Madhyastha", "Pranava", ""], ["Specia", "Lucia", ""]]}, {"id": "2010.13609", "submitter": "Dumitru-Clementin Cercel", "authors": "Mircea-Adrian Tanase, Dumitru-Clementin Cercel and Costin-Gabriel\n  Chiru", "title": "UPB at SemEval-2020 Task 12: Multilingual Offensive Language Detection\n  on Social Media by Fine-tuning a Variety of BERT-based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offensive language detection is one of the most challenging problem in the\nnatural language processing field, being imposed by the rising presence of this\nphenomenon in online social media. This paper describes our Transformer-based\nsolutions for identifying offensive language on Twitter in five languages\n(i.e., English, Arabic, Danish, Greek, and Turkish), which was employed in\nSubtask A of the Offenseval 2020 shared task. Several neural architectures\n(i.e., BERT, mBERT, Roberta, XLM-Roberta, and ALBERT), pre-trained using both\nsingle-language and multilingual corpora, were fine-tuned and compared using\nmultiple combinations of datasets. Finally, the highest-scoring models were\nused for our submissions in the competition, which ranked our team 21st of 85,\n28th of 53, 19th of 39, 16th of 37, and 10th of 46 for English, Arabic, Danish,\nGreek, and Turkish, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:28:29 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 09:21:21 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tanase", "Mircea-Adrian", ""], ["Cercel", "Dumitru-Clementin", ""], ["Chiru", "Costin-Gabriel", ""]]}, {"id": "2010.13637", "submitter": "Peng Gao", "authors": "Peng Gao, Fei Shao, Xiaoyuan Liu, Xusheng Xiao, Zheng Qin, Fengyuan\n  Xu, Prateek Mittal, Sanjeev R. Kulkarni, Dawn Song", "title": "Enabling Efficient Cyber Threat Hunting With Cyber Threat Intelligence", "comments": "Accepted paper at ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-based cyber threat hunting has emerged as an important solution to\ncounter sophisticated attacks. However, existing approaches require non-trivial\nefforts of manual query construction and have overlooked the rich external\nthreat knowledge provided by open-source Cyber Threat Intelligence (OSCTI). To\nbridge the gap, we propose ThreatRaptor, a system that facilitates threat\nhunting in computer systems using OSCTI. Built upon system auditing frameworks,\nThreatRaptor provides (1) an unsupervised, light-weight, and accurate NLP\npipeline that extracts structured threat behaviors from unstructured OSCTI\ntext, (2) a concise and expressive domain-specific query language, TBQL, to\nhunt for malicious system activities, (3) a query synthesis mechanism that\nautomatically synthesizes a TBQL query for hunting, and (4) an efficient query\nexecution engine to search the big audit logging data. Evaluations on a broad\nset of attack cases demonstrate the accuracy and efficiency of ThreatRaptor in\npractical threat hunting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:54:01 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 06:20:46 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gao", "Peng", ""], ["Shao", "Fei", ""], ["Liu", "Xiaoyuan", ""], ["Xiao", "Xusheng", ""], ["Qin", "Zheng", ""], ["Xu", "Fengyuan", ""], ["Mittal", "Prateek", ""], ["Kulkarni", "Sanjeev R.", ""], ["Song", "Dawn", ""]]}, {"id": "2010.13641", "submitter": "Timo Schick", "authors": "Timo Schick, Helmut Schmid, Hinrich Sch\\\"utze", "title": "Automatically Identifying Words That Can Serve as Labels for Few-Shot\n  Text Classification", "comments": "To appear at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent approach for few-shot text classification is to convert textual\ninputs to cloze questions that contain some form of task description, process\nthem with a pretrained language model and map the predicted words to labels.\nManually defining this mapping between words and labels requires both domain\nexpertise and an understanding of the language model's abilities. To mitigate\nthis issue, we devise an approach that automatically finds such a mapping given\nsmall amounts of training data. For a number of tasks, the mapping found by our\napproach performs almost as well as hand-crafted label-to-word mappings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:56:22 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Schick", "Timo", ""], ["Schmid", "Helmut", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2010.13652", "submitter": "Thomas Winters", "authors": "Thomas Winters, Pieter Delobelle", "title": "Dutch Humor Detection by Generating Negative Examples", "comments": "Accepted at the Proceedings of the 32st Benelux Conference on\n  Artificial Intelligence (BNAIC 2020) and the 29th Belgian Dutch Conference on\n  Machine Learning (Benelearn 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting if a text is humorous is a hard task to do computationally, as it\nusually requires linguistic and common sense insights. In machine learning,\nhumor detection is usually modeled as a binary classification task, trained to\npredict if the given text is a joke or another type of text. Rather than using\ncompletely different non-humorous texts, we propose using text generation\nalgorithms for imitating the original joke dataset to increase the difficulty\nfor the learning algorithm. We constructed several different joke and non-joke\ndatasets to test the humor detection abilities of different language\ntechnologies. In particular, we compare the humor detection capabilities of\nclassic neural network approaches with the state-of-the-art Dutch language\nmodel RobBERT. In doing so, we create and compare the first Dutch humor\ndetection systems. We found that while other language models perform well when\nthe non-jokes came from completely different domains, RobBERT was the only one\nthat was able to distinguish jokes from generated negative examples. This\nperformance illustrates the usefulness of using text generation to create\nnegative datasets for humor recognition, and also shows that transformer models\nare a large step forward in humor detection.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:15:10 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Winters", "Thomas", ""], ["Delobelle", "Pieter", ""]]}, {"id": "2010.13658", "submitter": "Baosong Yang", "authors": "Tianchi Bi and Liang Yao and Baosong Yang and Haibo Zhang and Weihua\n  Luo and Boxing Chen", "title": "Constraint Translation Candidates: A Bridge between Neural Query\n  Translation and Cross-lingual Information Retrieval", "comments": "SIGIR eCom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query translation (QT) is a key component in cross-lingual information\nretrieval system (CLIR). With the help of deep learning, neural machine\ntranslation (NMT) has shown promising results on various tasks. However, NMT is\ngenerally trained with large-scale out-of-domain data rather than in-domain\nquery translation pairs. Besides, the translation model lacks a mechanism at\nthe inference time to guarantee the generated words to match the search index.\nThe two shortages of QT result in readable texts for human but inadequate\ncandidates for the downstream retrieval task. In this paper, we propose a novel\napproach to alleviate these problems by limiting the open target vocabulary\nsearch space of QT to a set of important words mined from search index\ndatabase. The constraint translation candidates are employed at both of\ntraining and inference time, thus guiding the translation model to learn and\ngenerate well performing target queries. The proposed methods are exploited and\nexamined in a real-word CLIR system--Aliexpress e-Commerce search engine.\nExperimental results demonstrate that our approach yields better performance on\nboth translation quality and retrieval accuracy than the strong NMT baseline.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:27:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bi", "Tianchi", ""], ["Yao", "Liang", ""], ["Yang", "Baosong", ""], ["Zhang", "Haibo", ""], ["Luo", "Weihua", ""], ["Chen", "Boxing", ""]]}, {"id": "2010.13659", "submitter": "Baosong Yang", "authors": "Liang Yao and Baosong Yang and Haibo Zhang and Weihua Luo and Boxing\n  Chen", "title": "Exploiting Neural Query Translation into Cross Lingual Information\n  Retrieval", "comments": "SIGIR eCom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial role in cross-language information retrieval (CLIR), query\ntranslation has three main challenges: 1) the adequacy of translation; 2) the\nlack of in-domain parallel training data; and 3) the requisite of low latency.\nTo this end, existing CLIR systems mainly exploit statistical-based machine\ntranslation (SMT) rather than the advanced neural machine translation (NMT),\nlimiting the further improvements on both translation and retrieval quality. In\nthis paper, we investigate how to exploit neural query translation model into\nCLIR system. Specifically, we propose a novel data augmentation method that\nextracts query translation pairs according to user clickthrough data, thus to\nalleviate the problem of domain-adaptation in NMT. Then, we introduce an\nasynchronous strategy which is able to leverage the advantages of the real-time\nin SMT and the veracity in NMT. Experimental results reveal that the proposed\napproach yields better retrieval quality than strong baselines and can be well\napplied into a real-world CLIR system, i.e. Aliexpress e-Commerce search\nengine. Readers can examine and test their cases on our website:\nhttps://aliexpress.com .\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:28:19 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yao", "Liang", ""], ["Yang", "Baosong", ""], ["Zhang", "Haibo", ""], ["Luo", "Weihua", ""], ["Chen", "Boxing", ""]]}, {"id": "2010.13674", "submitter": "Christina Niklaus", "authors": "Thiemo Wambsganss, Christina Niklaus, Matthias S\\\"ollner, Siegfried\n  Handschuh, Jan Marco Leimeister", "title": "A Corpus for Argumentative Writing Support in German", "comments": "to be published in The 28th International Conference on Computational\n  Linguistics (COLING 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel annotation approach to capture claims and\npremises of arguments and their relations in student-written persuasive peer\nreviews on business models in German language. We propose an annotation scheme\nbased on annotation guidelines that allows to model claims and premises as well\nas support and attack relations for capturing the structure of argumentative\ndiscourse in student-written peer reviews. We conduct an annotation study with\nthree annotators on 50 persuasive essays to evaluate our annotation scheme. The\nobtained inter-rater agreement of $\\alpha=0.57$ for argument components and\n$\\alpha=0.49$ for argumentative relations indicates that the proposed\nannotation scheme successfully guides annotators to moderate agreement.\nFinally, we present our freely available corpus of 1,000 persuasive\nstudent-written peer reviews on business models and our annotation guidelines\nto encourage future research on the design and development of argumentative\nwriting support systems for students.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:52:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wambsganss", "Thiemo", ""], ["Niklaus", "Christina", ""], ["S\u00f6llner", "Matthias", ""], ["Handschuh", "Siegfried", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2010.13688", "submitter": "Alexander Kalinowski", "authors": "Alexander Kalinowski, Yuan An", "title": "A Survey of Embedding Space Alignment Methods for Language and Knowledge\n  Graphs", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural embedding approaches have become a staple in the fields of computer\nvision, natural language processing, and more recently, graph analytics. Given\nthe pervasive nature of these algorithms, the natural question becomes how to\nexploit the embedding spaces to map, or align, embeddings of different data\nsources. To this end, we survey the current research landscape on word,\nsentence and knowledge graph embedding algorithms. We provide a classification\nof the relevant alignment techniques and discuss benchmark datasets used in\nthis field of research. By gathering these diverse approaches into a singular\nsurvey, we hope to further motivate research into alignment of embedding spaces\nof varied data types and sources.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:08:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kalinowski", "Alexander", ""], ["An", "Yuan", ""]]}, {"id": "2010.13814", "submitter": "Constantin Orasan", "authors": "Hadeel Saadany, Constantin Orasan", "title": "Is it Great or Terrible? Preserving Sentiment in Neural Machine\n  Translation of Arabic Reviews", "comments": null, "journal-ref": "Proceedings of the Fifth Arabic Natural Language Processing\n  Workshop WANLP 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the advent of Neural Machine Translation (NMT) approaches there has\nbeen a tremendous improvement in the quality of automatic translation. However,\nNMT output still lacks accuracy in some low-resource languages and sometimes\nmakes major errors that need extensive post-editing. This is particularly\nnoticeable with texts that do not follow common lexico-grammatical standards,\nsuch as user generated content (UGC). In this paper we investigate the\nchallenges involved in translating book reviews from Arabic into English, with\nparticular focus on the errors that lead to incorrect translation of sentiment\npolarity. Our study points to the special characteristics of Arabic UGC,\nexamines the sentiment transfer errors made by Google Translate of Arabic UGC\nto English, analyzes why the problem occurs, and proposes an error typology\nspecific of the translation of Arabic UGC. Our analysis shows that the output\nof online translation tools of Arabic UGC can either fail to transfer the\nsentiment at all by producing a neutral target text, or completely flips the\nsentiment polarity of the target word or phrase and hence delivers a wrong\naffect message. We address this problem by fine-tuning an NMT model with\nrespect to sentiment polarity showing that this approach can significantly help\nwith correcting sentiment errors detected in the online translation of Arabic\nUGC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:01:52 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Saadany", "Hadeel", ""], ["Orasan", "Constantin", ""]]}, {"id": "2010.13816", "submitter": "Maarten Sap", "authors": "Xinyao Ma, Maarten Sap, Hannah Rashkin, Yejin Choi", "title": "PowerTransformer: Unsupervised Controllable Revision for Biased Language\n  Correction", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconscious biases continue to be prevalent in modern text and media, calling\nfor algorithms that can assist writers with bias correction. For example, a\nfemale character in a story is often portrayed as passive and powerless (\"She\ndaydreams about being a doctor\") while a man is portrayed as more proactive and\npowerful (\"He pursues his dream of being a doctor\").\n  We formulate *Controllable Debiasing*, a new revision task that aims to\nrewrite a given text to correct the implicit and potentially undesirable bias\nin character portrayals. We then introduce PowerTransformer as an approach that\ndebiases text through the lens of connotation frames (Sap et al., 2017), which\nencode pragmatic knowledge of implied power dynamics with respect to verb\npredicates. One key challenge of our task is the lack of parallel corpora. To\naddress this challenge, we adopt an unsupervised approach using auxiliary\nsupervision with related tasks such as paraphrasing and self-supervision based\non a reconstruction loss, building on pretrained language models.\n  Through comprehensive experiments based on automatic and human evaluations,\nwe demonstrate that our approach outperforms ablations and existing methods\nfrom related tasks. Furthermore, we demonstrate the use of PowerTransformer as\na step toward mitigating the well-documented gender bias in character portrayal\nin movie scripts.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:05:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ma", "Xinyao", ""], ["Sap", "Maarten", ""], ["Rashkin", "Hannah", ""], ["Choi", "Yejin", ""]]}, {"id": "2010.13826", "submitter": "Cheng-I Lai", "authors": "Cheng-I Lai, Yung-Sung Chuang, Hung-Yi Lee, Shang-Wen Li, James Glass", "title": "Semi-Supervised Spoken Language Understanding via Self-Supervised Speech\n  and Language Model Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent work on Spoken Language Understanding (SLU) is limited in at\nleast one of three ways: models were trained on oracle text input and neglected\nASR errors, models were trained to predict only intents without the slot\nvalues, or models were trained on a large amount of in-house data. In this\npaper, we propose a clean and general framework to learn semantics directly\nfrom speech with semi-supervision from transcribed or untranscribed speech to\naddress these issues. Our framework is built upon pretrained end-to-end (E2E)\nASR and self-supervised language models, such as BERT, and fine-tuned on a\nlimited amount of target SLU data. We study two semi-supervised settings for\nthe ASR component: supervised pretraining on transcribed speech, and\nunsupervised pretraining by replacing the ASR encoder with self-supervised\nspeech representations, such as wav2vec. In parallel, we identify two essential\ncriteria for evaluating SLU models: environmental noise-robustness and E2E\nsemantics evaluation. Experiments on ATIS show that our SLU framework with\nspeech as input can perform on par with those using oracle text as input in\nsemantics understanding, even though environmental noise is present and a\nlimited amount of labeled semantics data is available for training.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:21:27 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lai", "Cheng-I", ""], ["Chuang", "Yung-Sung", ""], ["Lee", "Hung-Yi", ""], ["Li", "Shang-Wen", ""], ["Glass", "James", ""]]}, {"id": "2010.13839", "submitter": "Subhajit Chaudhury", "authors": "Thomas Carta, Subhajit Chaudhury, Kartik Talamadupula and Michiaki\n  Tatsubori", "title": "VisualHints: A Visual-Lingual Environment for Multimodal Reinforcement\n  Learning", "comments": "Code is available at http://ibm.biz/VisualHints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VisualHints, a novel environment for multimodal reinforcement\nlearning (RL) involving text-based interactions along with visual hints\n(obtained from the environment). Real-life problems often demand that agents\ninteract with the environment using both natural language information and\nvisual perception towards solving a goal. However, most traditional RL\nenvironments either solve pure vision-based tasks like Atari games or\nvideo-based robotic manipulation; or entirely use natural language as a mode of\ninteraction, like Text-based games and dialog systems. In this work, we aim to\nbridge this gap and unify these two approaches in a single environment for\nmultimodal RL. We introduce an extension of the TextWorld cooking environment\nwith the addition of visual clues interspersed throughout the environment. The\ngoal is to force an RL agent to use both text and visual features to predict\nnatural language action commands for solving the final task of cooking a meal.\nWe enable variations and difficulties in our environment to emulate various\ninteractive real-world scenarios. We present a baseline multimodal agent for\nsolving such problems using CNN-based feature extraction from visual hints and\nLSTMs for textual feature extraction. We believe that our proposed\nvisual-lingual environment will facilitate novel problem settings for the RL\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:51:02 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Carta", "Thomas", ""], ["Chaudhury", "Subhajit", ""], ["Talamadupula", "Kartik", ""], ["Tatsubori", "Michiaki", ""]]}, {"id": "2010.13856", "submitter": "Ciprian Chelba", "authors": "Ciprian Chelba, Junpei Zhou, Yuezhang (Music) Li, Hideto Kazawa, Jeff\n  Klingner, Mengmeng Niu", "title": "Data Troubles in Sentence Level Confidence Estimation for Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates the feasibility of confidence estimation for neural\nmachine translation models operating at the high end of the performance\nspectrum. As a side product of the data annotation process necessary for\nbuilding such models we propose sentence level accuracy $SACC$ as a simple,\nself-explanatory evaluation metric for quality of translation.\n  Experiments on two different annotator pools, one comprised of non-expert\n(crowd-sourced) and one of expert (professional) translators show that $SACC$\ncan vary greatly depending on the translation proficiency of the annotators,\ndespite the fact that both pools are about equally reliable according to\nKrippendorff's alpha metric; the relatively low values of inter-annotator\nagreement confirm the expectation that sentence-level binary labeling $good$ /\n$needs\\ work$ for translation out of context is very hard.\n  For an English-Spanish translation model operating at $SACC = 0.89$ according\nto a non-expert annotator pool we can derive a confidence estimate that labels\n0.5-0.6 of the $good$ translations in an \"in-domain\" test set with 0.95\nPrecision. Switching to an expert annotator pool decreases $SACC$ dramatically:\n$0.61$ for English-Spanish, measured on the exact same data as above. This\nforces us to lower the CE model operating point to 0.9 Precision while labeling\ncorrectly about 0.20-0.25 of the $good$ translations in the data.\n  We find surprising the extent to which CE depends on the level of proficiency\nof the annotator pool used for labeling the data. This leads to an important\nrecommendation we wish to make when tackling CE modeling in practice: it is\ncritical to match the end-user expectation for translation quality in the\ndesired domain with the demands of annotators assigning binary quality labels\nto CE training data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 19:20:29 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chelba", "Ciprian", "", "Music"], ["Zhou", "Junpei", "", "Music"], ["Yuezhang", "", "", "Music"], ["Li", "", ""], ["Kazawa", "Hideto", ""], ["Klingner", "Jeff", ""], ["Niu", "Mengmeng", ""]]}, {"id": "2010.13870", "submitter": "Leon Bergen", "authors": "Charles Yu, Ryan Sie, Nico Tedeschi, Leon Bergen", "title": "Word Frequency Does Not Predict Grammatical Knowledge in Language Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural language models learn, to varying degrees of accuracy, the grammatical\nproperties of natural languages. In this work, we investigate whether there are\nsystematic sources of variation in the language models' accuracy. Focusing on\nsubject-verb agreement and reflexive anaphora, we find that certain nouns are\nsystematically understood better than others, an effect which is robust across\ngrammatical tasks and different language models. Surprisingly, we find that\nacross four orders of magnitude, corpus frequency is unrelated to a noun's\nperformance on grammatical tasks. Finally, we find that a novel noun's\ngrammatical properties can be few-shot learned from various types of training\ndata. The results present a paradox: there should be less variation in\ngrammatical performance than is actually observed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 19:51:36 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yu", "Charles", ""], ["Sie", "Ryan", ""], ["Tedeschi", "Nico", ""], ["Bergen", "Leon", ""]]}, {"id": "2010.13878", "submitter": "Suyoun Kim", "authors": "Suyoun Kim, Yuan Shangguan, Jay Mahadeokar, Antoine Bruguier,\n  Christian Fuegen, Michael L. Seltzer, Duc Le", "title": "Improved Neural Language Model Fusion for Streaming Recurrent Neural\n  Network Transducer", "comments": "submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network Transducer (RNN-T), like most end-to-end speech\nrecognition model architectures, has an implicit neural network language model\n(NNLM) and cannot easily leverage unpaired text data during training. Previous\nwork has proposed various fusion methods to incorporate external NNLMs into\nend-to-end ASR to address this weakness. In this paper, we propose extensions\nto these techniques that allow RNN-T to exploit external NNLMs during both\ntraining and inference time, resulting in 13-18% relative Word Error Rate\nimprovement on Librispeech compared to strong baselines. Furthermore, our\nmethods do not incur extra algorithmic latency and allow for flexible\nplug-and-play of different NNLMs without re-training. We also share in-depth\nanalysis to better understand the benefits of the different NNLM fusion\nmethods. Our work provides a reliable technique for leveraging unpaired text\ndata to significantly improve RNN-T while keeping the system streamable,\nflexible, and lightweight.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 20:10:12 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kim", "Suyoun", ""], ["Shangguan", "Yuan", ""], ["Mahadeokar", "Jay", ""], ["Bruguier", "Antoine", ""], ["Fuegen", "Christian", ""], ["Seltzer", "Michael L.", ""], ["Le", "Duc", ""]]}, {"id": "2010.13912", "submitter": "Chien-Sheng Wu", "authors": "Chien-Sheng Wu and Caiming Xiong", "title": "Probing Task-Oriented Dialogue Representation from Language Models", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates pre-trained language models to find out which model\nintrinsically carries the most informative representation for task-oriented\ndialogue tasks. We approach the problem from two aspects: supervised classifier\nprobe and unsupervised mutual information probe. We fine-tune a feed-forward\nlayer as the classifier probe on top of a fixed pre-trained language model with\nannotated labels in a supervised way. Meanwhile, we propose an unsupervised\nmutual information probe to evaluate the mutual dependence between a real\nclustering and a representation clustering. The goals of this empirical paper\nare to 1) investigate probing techniques, especially from the unsupervised\nmutual information aspect, 2) provide guidelines of pre-trained language model\nselection for the dialogue research community, 3) find insights of pre-training\nfactors for dialogue application that may be the key to success.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:34:39 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wu", "Chien-Sheng", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.13920", "submitter": "Chien-Sheng Wu", "authors": "Chien-Sheng Wu and Steven Hoi and Caiming Xiong", "title": "Improving Limited Labeled Dialogue State Tracking with Self-Supervision", "comments": "EMNLP 2020 (findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dialogue state tracking (DST) models require plenty of labeled data.\nHowever, collecting high-quality labels is costly, especially when the number\nof domains increases. In this paper, we address a practical DST problem that is\nrarely discussed, i.e., learning efficiently with limited labeled data. We\npresent and investigate two self-supervised objectives: preserving latent\nconsistency and modeling conversational behavior. We encourage a DST model to\nhave consistent latent distributions given a perturbed input, making it more\nrobust to an unseen scenario. We also add an auxiliary utterance generation\ntask, modeling a potential correlation between conversational behavior and\ndialogue states. The experimental results show that our proposed\nself-supervised signals can improve joint goal accuracy by 8.95\\% when only 1\\%\nlabeled data is used on the MultiWOZ dataset. We can achieve an additional\n1.76\\% improvement if some unlabeled data is jointly trained as semi-supervised\nlearning. We analyze and visualize how our proposed self-supervised signals\nhelp the DST task and hope to stimulate future data-efficient DST research.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:57:42 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Wu", "Chien-Sheng", ""], ["Hoi", "Steven", ""], ["Xiong", "Caiming", ""]]}, {"id": "2010.13944", "submitter": "Khyathi Raghavi Chandu", "authors": "Khyathi Raghavi Chandu, Ruo-Ping Dong, Alan Black", "title": "Reading Between the Lines: Exploring Infilling in Visual Narratives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating long form narratives such as stories and procedures from multiple\nmodalities has been a long standing dream for artificial intelligence. In this\nregard, there is often crucial subtext that is derived from the surrounding\ncontexts. The general seq2seq training methods render the models shorthanded\nwhile attempting to bridge the gap between these neighbouring contexts. In this\npaper, we tackle this problem by using \\textit{infilling} techniques involving\nprediction of missing steps in a narrative while generating textual\ndescriptions from a sequence of images. We also present a new large scale\n\\textit{visual procedure telling} (ViPT) dataset with a total of 46,200\nprocedures and around 340k pairwise images and textual descriptions that is\nrich in such contextual dependencies. Generating steps using infilling\ntechnique demonstrates the effectiveness in visual procedures with more\ncoherent texts. We conclusively show a METEOR score of 27.51 on procedures\nwhich is higher than the state-of-the-art on visual storytelling. We also\ndemonstrate the effects of interposing new text with missing images during\ninference. The code and the dataset will be publicly available at\nhttps://visual-narratives.github.io/Visual-Narratives/.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:09:09 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chandu", "Khyathi Raghavi", ""], ["Dong", "Ruo-Ping", ""], ["Black", "Alan", ""]]}, {"id": "2010.13982", "submitter": "Hung-Ting Chen", "authors": "Hung-Ting Chen, Yu-Chieh Chao, Ta-Hsuan Chao, Wei-Yun Ma", "title": "Predict and Use Latent Patterns for Short-Text Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neural network models nowadays have achieved promising performances in\nChit-chat settings. The majority of them rely on an encoder for understanding\nthe post and a decoder for generating the response. Without given assigned\nsemantics, the models lack the fine-grained control over responses as the\nsemantic mapping between posts and responses is hidden on the fly within the\nend-to-end manners. Some previous works utilize sampled latent words as a\ncontrollable semantic form to drive the generated response around the work, but\nfew works attempt to use more complex semantic patterns to guide the\ngeneration. In this paper, we propose to use more detailed semantic forms,\nincluding latent responses and part-of-speech sequences sampled from the\ncorresponding distributions, as the controllable semantics to guide the\ngeneration. Our results show that the richer semantics are not only able to\nprovide informative and diverse responses, but also increase the overall\nperformance of response quality, including fluency and coherence.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 01:31:42 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 03:14:27 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Chen", "Hung-Ting", ""], ["Chao", "Yu-Chieh", ""], ["Chao", "Ta-Hsuan", ""], ["Ma", "Wei-Yun", ""]]}, {"id": "2010.13984", "submitter": "Siwon Kim", "authors": "Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon", "title": "Interpretation of NLP models through input marginalization", "comments": "10 pages, 5 figures, to be published in the 2020 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To demystify the \"black box\" property of deep neural networks for natural\nlanguage processing (NLP), several methods have been proposed to interpret\ntheir predictions by measuring the change in prediction probability after\nerasing each token of an input. Since existing methods replace each token with\na predefined value (i.e., zero), the resulting sentence lies out of the\ntraining data distribution, yielding misleading interpretations. In this study,\nwe raise the out-of-distribution problem induced by the existing interpretation\nmethods and present a remedy; we propose to marginalize each token out. We\ninterpret various NLP models trained for sentiment analysis and natural\nlanguage inference using the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 01:40:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kim", "Siwon", ""], ["Yi", "Jihun", ""], ["Kim", "Eunji", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2010.13991", "submitter": "Wei Zou", "authors": "Dongwei Jiang, Wubo Li, Miao Cao, Wei Zou, Xiangang Li", "title": "Speech SIMCLR: Combining Contrastive and Reconstruction Objective for\n  Self-supervised Speech Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised visual pretraining has shown significant progress recently.\nAmong those methods, SimCLR greatly advanced the state of the art in\nself-supervised and semi-supervised learning on ImageNet. The input feature\nrepresentations for speech and visual tasks are both continuous, so it is\nnatural to consider applying similar objective on speech representation\nlearning. In this paper, we propose Speech SimCLR, a new self-supervised\nobjective for speech representation learning. During training, Speech SimCLR\napplies augmentation on raw speech and its spectrogram. Its objective is the\ncombination of contrastive loss that maximizes agreement between differently\naugmented samples in the latent space and reconstruction loss of input\nrepresentation. The proposed method achieved competitive results on speech\nemotion recognition and speech recognition.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 02:09:06 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 03:13:25 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Jiang", "Dongwei", ""], ["Li", "Wubo", ""], ["Cao", "Miao", ""], ["Zou", "Wei", ""], ["Li", "Xiangang", ""]]}, {"id": "2010.14029", "submitter": "Runxin Xu", "authors": "Runxin Xu, Zhuo Zhi, Jun Cao, Mingxuan Wang, Lei Li", "title": "Volctrans Parallel Corpus Filtering System for WMT 2020", "comments": "WMT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our submissions to the WMT20 shared task on\nparallel corpus filtering and alignment for low-resource conditions. The task\nrequires the participants to align potential parallel sentence pairs out of the\ngiven document pairs, and score them so that low-quality pairs can be filtered.\nOur system, Volctrans, is made of two modules, i.e., a mining module and a\nscoring module. Based on the word alignment model, the mining module adopts an\niterative mining strategy to extract latent parallel sentences. In the scoring\nmodule, an XLM-based scorer provides scores, followed by reranking mechanisms\nand ensemble. Our submissions outperform the baseline by 3.x/2.x and 2.x/2.x\nfor km-en and ps-en on From Scratch/Fine-Tune conditions, which is the highest\namong all submissions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:20:04 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Xu", "Runxin", ""], ["Zhi", "Zhuo", ""], ["Cao", "Jun", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2010.14042", "submitter": "Kasturi Bhattacharjee", "authors": "Kasturi Bhattacharjee, Miguel Ballesteros, Rishita Anubhai, Smaranda\n  Muresan, Jie Ma, Faisal Ladhak, Yaser Al-Onaizan", "title": "To BERT or Not to BERT: Comparing Task-specific and Task-agnostic\n  Semi-Supervised Approaches for Sequence Tagging", "comments": "Accepted in the Proceedings of 2020 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP\n  2020)(https://2020.emnlp.org/papers/main)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging large amounts of unlabeled data using Transformer-like\narchitectures, like BERT, has gained popularity in recent times owing to their\neffectiveness in learning general representations that can then be further\nfine-tuned for downstream tasks to much success. However, training these models\ncan be costly both from an economic and environmental standpoint. In this work,\nwe investigate how to effectively use unlabeled data: by exploring the\ntask-specific semi-supervised approach, Cross-View Training (CVT) and comparing\nit with task-agnostic BERT in multiple settings that include domain and task\nrelevant English data. CVT uses a much lighter model architecture and we show\nthat it achieves similar performance to BERT on a set of sequence tagging\ntasks, with lesser financial and environmental impact.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 04:03:47 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bhattacharjee", "Kasturi", ""], ["Ballesteros", "Miguel", ""], ["Anubhai", "Rishita", ""], ["Muresan", "Smaranda", ""], ["Ma", "Jie", ""], ["Ladhak", "Faisal", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "2010.14061", "submitter": "Yan Zeng", "authors": "Yan Zeng and Jian-Yun Nie", "title": "Jointly Optimizing State Operation Prediction and Value Generation for\n  Dialogue State Tracking", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of multi-domain Dialogue State Tracking (DST) with\nopen vocabulary. Existing approaches exploit BERT encoder and copy-based RNN\ndecoder, where the encoder predicts the state operation, and the decoder\ngenerates new slot values. However, in such a stacked encoder-decoder\nstructure, the operation prediction objective only affects the BERT encoder and\nthe value generation objective mainly affects the RNN decoder. In this paper,\nwe propose a purely Transformer-based framework, where a single BERT works as\nboth the encoder and the decoder. In so doing, the operation prediction\nobjective and the value generation objective can jointly optimize this BERT for\nDST. At the decoding step, we re-use the hidden states of the encoder in the\nself-attention mechanism of the corresponding decoder layers to construct a\nflat encoder-decoder architecture for effective parameter updating.\nExperimental results show that our approach substantially outperforms the\nexisting state-of-the-art framework, and it also achieves very competitive\nperformance to the best ontology-based approaches.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 04:54:52 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 02:04:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zeng", "Yan", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2010.14102", "submitter": "Wen Wu", "authors": "Wen Wu, Chao Zhang, Philip C. Woodland", "title": "Emotion recognition by fusing time synchronous and time asynchronous\n  representations", "comments": null, "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), 2021, pp. 6269-6273", "doi": "10.1109/ICASSP39728.2021.9414880", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 07:14:31 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 11:46:06 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wu", "Wen", ""], ["Zhang", "Chao", ""], ["Woodland", "Philip C.", ""]]}, {"id": "2010.14104", "submitter": "Bj\\\"orn Bebensee", "authors": "Bj\\\"orn Bebensee, Byoung-Tak Zhang", "title": "Co-attentional Transformers for Story-Based Video Understanding", "comments": "10 pages, 2 figures, submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent trends in vision and language learning, we explore\napplications of attention mechanisms for visio-lingual fusion within an\napplication to story-based video understanding. Like other video-based QA\ntasks, video story understanding requires agents to grasp complex temporal\ndependencies. However, as it focuses on the narrative aspect of video it also\nrequires understanding of the interactions between different characters, as\nwell as their actions and their motivations. We propose a novel co-attentional\ntransformer model to better capture long-term dependencies seen in visual\nstories such as dramas and measure its performance on the video question\nanswering task. We evaluate our approach on the recently introduced DramaQA\ndataset which features character-centered video story understanding questions.\nOur model outperforms the baseline model by 8 percentage points overall, at\nleast 4.95 and up to 12.8 percentage points on all difficulty levels and\nmanages to beat the winner of the DramaQA challenge.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 07:17:09 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bebensee", "Bj\u00f6rn", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2010.14123", "submitter": "Viet Lai", "authors": "Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen", "title": "Event Detection: Gate Diversity and Syntactic Importance Scoresfor Graph\n  Convolution Neural Networks", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent studies on event detection (ED) haveshown that the syntactic\ndependency graph canbe employed in graph convolution neural net-works (GCN) to\nachieve state-of-the-art per-formance. However, the computation of thehidden\nvectors in such graph-based models isagnostic to the trigger candidate words,\npo-tentially leaving irrelevant information for thetrigger candidate for event\nprediction. In addi-tion, the current models for ED fail to exploitthe overall\ncontextual importance scores of thewords, which can be obtained via the\ndepen-dency tree, to boost the performance. In thisstudy, we propose a novel\ngating mechanismto filter noisy information in the hidden vec-tors of the GCN\nmodels for ED based on theinformation from the trigger candidate. Wealso\nintroduce novel mechanisms to achievethe contextual diversity for the gates and\ntheimportance score consistency for the graphsand models in ED. The experiments\nshow thatthe proposed model achieves state-of-the-artperformance on two ED\ndatasets\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:28:28 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lai", "Viet Dac", ""], ["Nguyen", "Tuan Ngo", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2010.14233", "submitter": "Ethan Chi", "authors": "Ethan A. Chi, Julian Salazar, and Katrin Kirchhoff", "title": "Align-Refine: Non-Autoregressive Speech Recognition via Iterative\n  Realignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive models greatly improve decoding speed over typical\nsequence-to-sequence models, but suffer from degraded performance. Infilling\nand iterative refinement models make up some of this gap by editing the outputs\nof a non-autoregressive model, but are constrained in the edits that they can\nmake. We propose iterative realignment, where refinements occur over latent\nalignments rather than output sequence space. We demonstrate this in speech\nrecognition with Align-Refine, an end-to-end Transformer-based model which\nrefines connectionist temporal classification (CTC) alignments to allow\nlength-changing insertions and deletions. Align-Refine outperforms Imputer and\nMask-CTC, matching an autoregressive baseline on WSJ at 1/14th the real-time\nfactor and attaining a LibriSpeech test-other WER of 9.0% without an LM. Our\nmodel is strong even in one iteration with a shallower decoder.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 09:35:37 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chi", "Ethan A.", ""], ["Salazar", "Julian", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "2010.14234", "submitter": "Muvazima Mansoor", "authors": "Muvazima Mansoor, Kirthika Gurumurthy, Anantharam R U, V R Badri\n  Prasad", "title": "Global Sentiment Analysis Of COVID-19 Tweets Over Time", "comments": "7 pages, 20 figures, Submitted to International journal of Data\n  Science and Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus pandemic has affected the normal course of life. People\naround the world have taken to social media to express their opinions and\ngeneral emotions regarding this phenomenon that has taken over the world by\nstorm. The social networking site, Twitter showed an unprecedented increase in\ntweets related to the novel Coronavirus in a very short span of time. This\npaper presents the global sentiment analysis of tweets related to Coronavirus\nand how the sentiment of people in different countries has changed over time.\nFurthermore, to determine the impact of Coronavirus on daily aspects of life,\ntweets related to Work From Home (WFH) and Online Learning were scraped and the\nchange in sentiment over time was observed. In addition, various Machine\nLearning models such as Long Short Term Memory (LSTM) and Artificial Neural\nNetworks (ANN) were implemented for sentiment classification and their\naccuracies were determined. Exploratory data analysis was also performed for a\ndataset providing information about the number of confirmed cases on a per-day\nbasis in a few of the worst-hit countries to provide a comparison between the\nchange in sentiment with the change in cases since the start of this pandemic\ntill June 2020.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 12:10:10 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 08:24:09 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mansoor", "Muvazima", ""], ["Gurumurthy", "Kirthika", ""], ["U", "Anantharam R", ""], ["Prasad", "V R Badri", ""]]}, {"id": "2010.14235", "submitter": "Yao Lu", "authors": "Yao Lu, Yue Dong, Laurent Charlin", "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document\n  Summarization of Scientific Articles", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-document summarization is a challenging task for which there exists\nlittle large-scale datasets. We propose Multi-XScience, a large-scale\nmulti-document summarization dataset created from scientific articles.\nMulti-XScience introduces a challenging multi-document summarization task:\nwriting the related-work section of a paper based on its abstract and the\narticles it references. Our work is inspired by extreme summarization, a\ndataset construction protocol that favours abstractive modeling approaches.\nDescriptive statistics and empirical results---using several state-of-the-art\nmodels trained on the Multi-XScience dataset---reveal that Multi-XScience is\nwell suited for abstractive models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 12:10:19 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lu", "Yao", ""], ["Dong", "Yue", ""], ["Charlin", "Laurent", ""]]}, {"id": "2010.14255", "submitter": "Jianing Wang", "authors": "Jianing Wang", "title": "RH-Net: Improving Neural Relation Extraction via Reinforcement Learning\n  and Hierarchical Relational Searching", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision (DS) aims to generate large-scale heuristic labeling\ncorpus, which is widely used for neural relation extraction currently. However,\nit heavily suffers from noisy labeling and long-tail distributions problem.\nMany advanced approaches usually separately address two problems, which ignore\ntheir mutual interactions. In this paper, we propose a novel framework named\nRH-Net, which utilizes Reinforcement learning and Hierarchical relational\nsearching module to improve relation extraction. We leverage reinforcement\nlearning to instruct the model to select high-quality instances. We then\npropose the hierarchical relational searching module to share the semantics\nfrom correlative instances between data-rich and data-poor classes. During the\niterative process, the two modules keep interacting to alleviate the noisy and\nlong-tail problem simultaneously. Extensive experiments on widely used NYT data\nset clearly show that our method significant improvements over state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 12:50:27 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 07:24:01 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wang", "Jianing", ""]]}, {"id": "2010.14271", "submitter": "Ming Gong", "authors": "Junhao Liu, Linjun Shou, Jian Pei, Ming Gong, Min Yang, Daxin Jiang", "title": "Cross-lingual Machine Reading Comprehension with Language Branch\n  Knowledge Distillation", "comments": "Accepted as long paper in COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual Machine Reading Comprehension (CLMRC) remains a challenging\nproblem due to the lack of large-scale annotated datasets in low-source\nlanguages, such as Arabic, Hindi, and Vietnamese. Many previous approaches use\ntranslation data by translating from a rich-source language, such as English,\nto low-source languages as auxiliary supervision. However, how to effectively\nleverage translation data and reduce the impact of noise introduced by\ntranslation remains onerous. In this paper, we tackle this challenge and\nenhance the cross-lingual transferring performance by a novel augmentation\napproach named Language Branch Machine Reading Comprehension (LBMRC). A\nlanguage branch is a group of passages in one single language paired with\nquestions in all target languages. We train multiple machine reading\ncomprehension (MRC) models proficient in individual language based on LBMRC.\nThen, we devise a multilingual distillation approach to amalgamate knowledge\nfrom multiple language branch models to a single model for all target\nlanguages. Combining the LBMRC and multilingual distillation can be more robust\nto the data noises, therefore, improving the model's cross-lingual ability.\nMeanwhile, the produced single multilingual model is applicable to all target\nlanguages, which saves the cost of training, inference, and maintenance for\nmultiple models. Extensive experiments on two CLMRC benchmarks clearly show the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:12:17 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liu", "Junhao", ""], ["Shou", "Linjun", ""], ["Pei", "Jian", ""], ["Gong", "Ming", ""], ["Yang", "Min", ""], ["Jiang", "Daxin", ""]]}, {"id": "2010.14318", "submitter": "Peidong Wang", "authors": "Peidong Wang, Tara N. Sainath, Ron J. Weiss", "title": "Multitask Training with Text Data for End-to-End Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multitask training method for attention-based end-to-end speech\nrecognition models. We regularize the decoder in a listen, attend, and spell\nmodel by multitask training it on both audio-text and text-only data. Trained\non the 100-hour subset of LibriSpeech, the proposed method, without requiring\nan additional language model, leads to an 11% relative performance improvement\nover the baseline and approaches the performance of language model shallow\nfusion on the test-clean evaluation set. We observe a similar trend on the\nwhole 960-hour LibriSpeech training set. Analyses of different types of errors\nand sample output sentences demonstrate that the proposed method can\nincorporate language level information, suggesting its effectiveness in\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:29:28 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 01:13:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Peidong", ""], ["Sainath", "Tara N.", ""], ["Weiss", "Ron J.", ""]]}, {"id": "2010.14342", "submitter": "Guanyi Chen", "authors": "Guanyi Chen, Yinhe Zheng, Yupei Du", "title": "Listener's Social Identity Matters in Personalised Response Generation", "comments": "Long paper accepted at INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalised response generation enables generating human-like responses by\nmeans of assigning the generator a social identity. However, pragmatics theory\nsuggests that human beings adjust the way of speaking based on not only who\nthey are but also whom they are talking to. In other words, when modelling\npersonalised dialogues, it might be favourable if we also take the listener's\nsocial identity into consideration. To validate this idea, we use gender as a\ntypical example of a social variable to investigate how the listener's identity\ninfluences the language used in Chinese dialogues on social media. Also, we\nbuild personalised generators. The experiment results demonstrate that the\nlistener's identity indeed matters in the language use of responses and that\nthe response generator can capture such differences in language use. More\ninterestingly, by additionally modelling the listener's identity, the\npersonalised response generator performs better in its own identity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:57:21 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chen", "Guanyi", ""], ["Zheng", "Yinhe", ""], ["Du", "Yupei", ""]]}, {"id": "2010.14439", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang\n  Ren, William W. Cohen", "title": "Differentiable Open-Ended Commonsense Reasoning", "comments": "Accepted to NAACL 2021. Project website: https://open-csr.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current commonsense reasoning research focuses on developing models that use\ncommonsense knowledge to answer multiple-choice questions. However, systems\ndesigned to answer multiple-choice questions may not be useful in applications\nthat do not provide a small list of candidate answers to choose from. As a step\ntowards making commonsense reasoning research more realistic, we propose to\nstudy open-ended commonsense reasoning (OpenCSR) -- the task of answering a\ncommonsense question without any pre-defined choices -- using as a resource\nonly a corpus of commonsense facts written in natural language. OpenCSR is\nchallenging due to a large decision space, and because many questions require\nimplicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an\nefficient Differentiable model for multi-hop Reasoning over knowledge Facts. To\nevaluate OpenCSR methods, we adapt several popular commonsense reasoning\nbenchmarks, and collect multiple new answers for each test question via\ncrowd-sourcing. Experiments show that DrFact outperforms strong baseline\nmethods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:07:00 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 20:20:27 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Sun", "Haitian", ""], ["Dhingra", "Bhuwan", ""], ["Zaheer", "Manzil", ""], ["Ren", "Xiang", ""], ["Cohen", "William W.", ""]]}, {"id": "2010.14448", "submitter": "Xavier Ferrer Aran", "authors": "Xavier Ferrer-Aran, Tom van Nuenen, Natalia Criado, Jose M. Such", "title": "Discovering and Interpreting Conceptual Biases in Online Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language carries implicit human biases, functioning both as a reflection and\na perpetuation of stereotypes that people carry with them. Recently, ML-based\nNLP methods such as word embeddings have been shown to learn such language\nbiases with striking accuracy. This capability of word embeddings has been\nsuccessfully exploited as a tool to quantify and study human biases. However,\nprevious studies only consider a predefined set of conceptual biases to attest\n(e.g., whether gender is more or less associated with particular jobs), or just\ndiscover biased words without helping to understand their meaning at the\nconceptual level. As such, these approaches are either unable to find\nconceptual biases that have not been defined in advance, or the biases they\nfind are difficult to interpret and study. This makes existing approaches\nunsuitable to discover and interpret biases in online communities, as such\ncommunities may carry different biases than those in mainstream culture. This\npaper proposes a general, data-driven approach to automatically discover and\nhelp interpret conceptual biases encoded in word embeddings. We apply this\napproach to study the conceptual biases present in the language used in online\ncommunities and experimentally show the validity and stability of our method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:07:12 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ferrer-Aran", "Xavier", ""], ["van Nuenen", "Tom", ""], ["Criado", "Natalia", ""], ["Such", "Jose M.", ""]]}, {"id": "2010.14464", "submitter": "Lukasz Borchmann", "authors": "{\\L}ukasz Borchmann, Dawid Jurkiewicz, Filip Grali\\'nski, Tomasz\n  G\\'orecki", "title": "Dynamic Boundary Time Warping for Sub-sequence Matching with Few\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel method of finding a fragment in a long temporal\nsequence similar to the set of shorter sequences. We are the first to propose\nan algorithm for such a search that does not rely on computing the average\nsequence from query examples. Instead, we use query examples as is, utilizing\nall of them simultaneously. The introduced method based on the Dynamic Time\nWarping (DTW) technique is suited explicitly for few-shot query-by-example\nretrieval tasks. We evaluate it on two different few-shot problems from the\nfield of Natural Language Processing. The results show it either outperforms\nbaselines and previous approaches or achieves comparable results when a low\nnumber of examples is available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:23:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Borchmann", "\u0141ukasz", ""], ["Jurkiewicz", "Dawid", ""], ["Grali\u0144ski", "Filip", ""], ["G\u00f3recki", "Tomasz", ""]]}, {"id": "2010.14465", "submitter": "Marta R. Costa-juss\\`a", "authors": "Marta R. Costa-juss\\`a and Christine Basta and Gerard I. G\\'allego", "title": "Evaluating Gender Bias in Speech Translation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n  This paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:24:27 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:22:49 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 10:10:40 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Costa-juss\u00e0", "Marta R.", ""], ["Basta", "Christine", ""], ["G\u00e1llego", "Gerard I.", ""]]}, {"id": "2010.14479", "submitter": "Sugat Chaturvedi", "authors": "Rochana Chaturvedi, Sugat Chaturvedi", "title": "It's All in the Name: A Character Based Approach To Infer Religion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic inference from text has received a surge of attention in the\nfield of natural language processing in the last decade. In this paper, we use\npersonal names to infer religion in South Asia - where religion is a salient\nsocial division, and yet, disaggregated data on it remains scarce. Existing\nwork predicts religion using dictionary based method, and therefore, can not\nclassify unseen names. We use character based models which learn character\npatterns and, therefore, can classify unseen names as well with high accuracy.\nThese models are also much faster and can easily be scaled to large data sets.\nWe improve our classifier by combining the name of an individual with that of\ntheir parent/spouse and achieve remarkably high accuracy. Finally, we trace the\nclassification decisions of a convolutional neural network model using\nlayer-wise relevance propagation which can explain the predictions of complex\nnon-linear classifiers and circumvent their purported black box nature. We show\nhow character patterns learned by the classifier are rooted in the linguistic\norigins of names.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:38:11 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chaturvedi", "Rochana", ""], ["Chaturvedi", "Sugat", ""]]}, {"id": "2010.14481", "submitter": "Biao Zhang", "authors": "Biao Zhang, Ivan Titov, Rico Sennrich", "title": "Fast Interleaved Bidirectional Sequence Generation", "comments": "WMT2020, source code is at\n  https://github.com/bzhangGo/zero/tree/master/docs/interleaved_bidirectional_transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independence assumptions during sequence generation can speed up inference,\nbut parallel generation of highly inter-dependent tokens comes at a cost in\nquality. Instead of assuming independence between neighbouring tokens\n(semi-autoregressive decoding, SA), we take inspiration from bidirectional\nsequence generation and introduce a decoder that generates target words from\nthe left-to-right and right-to-left directions simultaneously. We show that we\ncan easily convert a standard architecture for unidirectional decoding into a\nbidirectional decoder by simply interleaving the two directions and adapting\nthe word positions and self-attention masks. Our interleaved bidirectional\ndecoder (IBDecoder) retains the model simplicity and training efficiency of the\nstandard Transformer, and on five machine translation tasks and two document\nsummarization tasks, achieves a decoding speedup of ~2X compared to\nautoregressive decoding with comparable quality. Notably, it outperforms\nleft-to-right SA because the independence assumptions in IBDecoder are more\nfelicitous. To achieve even higher speedups, we explore hybrid models where we\neither simultaneously predict multiple neighbouring tokens per direction, or\nperform multi-directional decoding by partitioning the target sequence. These\nmethods achieve speedups to 4X-11X across different tasks at the cost of <1\nBLEU or <0.5 ROUGE (on average). Source code is released at\nhttps://github.com/bzhangGo/zero.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:38:51 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhang", "Biao", ""], ["Titov", "Ivan", ""], ["Sennrich", "Rico", ""]]}, {"id": "2010.14534", "submitter": "Marion Bartl", "authors": "Marion Bartl and Malvina Nissim and Albert Gatt", "title": "Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender\n  Bias", "comments": "10 pages, 4 figures, to appear in Proceedings of the 2nd Workshop on\n  Gender Bias in Natural Language Processing at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contextualized word embeddings have been replacing standard embeddings as the\nrepresentational knowledge source of choice in NLP systems. Since a variety of\nbiases have previously been found in standard word embeddings, it is crucial to\nassess biases encoded in their replacements as well. Focusing on BERT (Devlin\net al., 2018), we measure gender bias by studying associations between\ngender-denoting target words and names of professions in English and German,\ncomparing the findings with real-world workforce statistics. We mitigate bias\nby fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying\nCounterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that\nour method of measuring bias is appropriate for languages such as English, but\nnot for languages with a rich morphology and gender-marking, such as German.\nOur results highlight the importance of investigating bias and mitigation\ntechniques cross-linguistically, especially in view of the current emphasis on\nlarge-scale, multilingual language models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:06:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Bartl", "Marion", ""], ["Nissim", "Malvina", ""], ["Gatt", "Albert", ""]]}, {"id": "2010.14557", "submitter": "Ruizhe Li", "authors": "Xiao Li, Guanyi Chen, Chenghua Lin, Ruizhe Li", "title": "DGST: a Dual-Generator Network for Text Style Transfer", "comments": "Accepted by EMNLP 2020, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DGST, a novel and simple Dual-Generator network architecture for\ntext Style Transfer. Our model employs two generators only, and does not rely\non any discriminators or parallel corpus for training. Both quantitative and\nqualitative experiments on the Yelp and IMDb datasets show that our model gives\ncompetitive performance compared to several strong baselines with more\ncomplicated architecture designs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:54:51 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Li", "Xiao", ""], ["Chen", "Guanyi", ""], ["Lin", "Chenghua", ""], ["Li", "Ruizhe", ""]]}, {"id": "2010.14568", "submitter": "Kaiyu Yang", "authors": "Kaiyu Yang, Jia Deng", "title": "Strongly Incremental Constituency Parsing with Graph Neural Networks", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing sentences into syntax trees can benefit downstream applications in\nNLP. Transition-based parsers build trees by executing actions in a state\ntransition system. They are computationally efficient, and can leverage machine\nlearning to predict actions based on partial trees. However, existing\ntransition-based parsers are predominantly based on the shift-reduce transition\nsystem, which does not align with how humans are known to parse sentences.\nPsycholinguistic research suggests that human parsing is strongly incremental:\nhumans grow a single parse tree by adding exactly one token at each step. In\nthis paper, we propose a novel transition system called attach-juxtapose. It is\nstrongly incremental; it represents a partial sentence using a single tree;\neach action adds exactly one token into the partial tree. Based on our\ntransition system, we develop a strongly incremental parser. At each step, it\nencodes the partial tree using a graph neural network and predicts an action.\nWe evaluate our parser on Penn Treebank (PTB) and Chinese Treebank (CTB). On\nPTB, it outperforms existing parsers trained with only constituency trees; and\nit performs on par with state-of-the-art parsers that use dependency trees as\nadditional training data. On CTB, our parser establishes a new state of the\nart. Code is available at\nhttps://github.com/princeton-vl/attach-juxtapose-parser.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:19:38 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Yang", "Kaiyu", ""], ["Deng", "Jia", ""]]}, {"id": "2010.14571", "submitter": "Isaac Caswell", "authors": "Isaac Caswell, Theresa Breiner, Daan van Esch, Ankur Bapna", "title": "Language ID in the Wild: Unexpected Challenges on the Path to a\n  Thousand-Language Web Text Corpus", "comments": "Accepted to COLING 2020. 9 pages with 8 page abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large text corpora are increasingly important for a wide variety of Natural\nLanguage Processing (NLP) tasks, and automatic language identification (LangID)\nis a core technology needed to collect such datasets in a multilingual context.\nLangID is largely treated as solved in the literature, with models reported\nthat achieve over 90% average F1 on as many as 1,366 languages. We train LangID\nmodels on up to 1,629 languages with comparable quality on held-out test sets,\nbut find that human-judged LangID accuracy for web-crawl text corpora created\nusing these models is only around 5% for many lower-resource languages,\nsuggesting a need for more robust evaluation. Further analysis revealed a\nvariety of error modes, arising from domain mismatch, class imbalance, language\nsimilarity, and insufficiently expressive models. We propose two classes of\ntechniques to mitigate these errors: wordlist-based tunable-precision filters\n(for which we release curated lists in about 500 languages) and\ntransformer-based semi-supervised LangID models, which increase median dataset\nprecision from 5.5% to 71.2%. These techniques enable us to create an initial\ndata set covering 100K or more relatively clean sentences in each of 500+\nlanguages, paving the way towards a 1,000-language web text corpus.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:29:17 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 15:18:35 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Caswell", "Isaac", ""], ["Breiner", "Theresa", ""], ["van Esch", "Daan", ""], ["Bapna", "Ankur", ""]]}, {"id": "2010.14576", "submitter": "Jeniya Tabassum", "authors": "Jeniya Tabassum, Sydney Lee, Wei Xu, Alan Ritter", "title": "WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet\n  Lab Protocols", "comments": "to appear in EMNLP 2020 (WNUT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results of the wet lab information extraction task at\nWNUT 2020. This task consisted of two sub tasks: (1) a Named Entity Recognition\n(NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2\nparticipants. We outline the task, data annotation process, corpus statistics,\nand provide a high-level overview of the participating systems for each sub\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:34:53 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 09:34:36 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 03:06:32 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tabassum", "Jeniya", ""], ["Lee", "Sydney", ""], ["Xu", "Wei", ""], ["Ritter", "Alan", ""]]}, {"id": "2010.14584", "submitter": "Aleksandra Edwards Mrs", "authors": "Aleksandra Edwards, David Rogers, Jose Camacho-Collados, H\\'el\\`ene de\n  Ribaupierre, Alun Preece", "title": "Predicting Themes within Complex Unstructured Texts: A Case Study on\n  Safeguarding Reports", "comments": "10 pages, 5 figures, workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of text and sentence classification is associated with the need for\nlarge amounts of labelled training data. The acquisition of high volumes of\nlabelled datasets can be expensive or unfeasible, especially for\nhighly-specialised domains for which documents are hard to obtain. Research on\nthe application of supervised classification based on small amounts of training\ndata is limited. In this paper, we address the combination of state-of-the-art\ndeep learning and classification methods and provide an insight into what\ncombination of methods fit the needs of small, domain-specific, and\nterminologically-rich corpora. We focus on a real-world scenario related to a\ncollection of safeguarding reports comprising learning experiences and\nreflections on tackling serious incidents involving children and vulnerable\nadults. The relatively small volume of available reports and their use of\nhighly domain-specific terminology makes the application of automated\napproaches difficult. We focus on the problem of automatically identifying the\nmain themes in a safeguarding report using supervised classification\napproaches. Our results show the potential of deep learning models to simulate\nsubject-expert behaviour even for complex tasks with limited labelled data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:48:23 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 09:15:14 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 17:33:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Edwards", "Aleksandra", ""], ["Rogers", "David", ""], ["Camacho-Collados", "Jose", ""], ["de Ribaupierre", "H\u00e9l\u00e8ne", ""], ["Preece", "Alun", ""]]}, {"id": "2010.14587", "submitter": "Jean-Baptiste Lamare", "authors": "Jean-Baptiste Lamare, Tobi Olatunji, Li Yao", "title": "On the diminishing return of labeling clinical reports", "comments": "Accepted at the EMNLP 2020 Clinical NLP workshop, 9 pages + 2 for\n  references, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ample evidence suggests that better machine learning models may be steadily\nobtained by training on increasingly larger datasets on natural language\nprocessing (NLP) problems from non-medical domains. Whether the same holds true\nfor medical NLP has by far not been thoroughly investigated. This work shows\nthat this is indeed not always the case. We reveal the somehow\ncounter-intuitive observation that performant medical NLP models may be\nobtained with small amount of labeled data, quite the opposite to the common\nbelief, most likely due to the domain specificity of the problem. We show\nquantitatively the effect of training data size on a fixed test set composed of\ntwo of the largest public chest x-ray radiology report datasets on the task of\nabnormality classification. The trained models not only make use of the\ntraining data efficiently, but also outperform the current state-of-the-art\nrule-based systems by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:51:04 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Lamare", "Jean-Baptiste", ""], ["Olatunji", "Tobi", ""], ["Yao", "Li", ""]]}, {"id": "2010.14588", "submitter": "Robert Leaman", "authors": "Robert Leaman and Zhiyong Lu", "title": "A Comprehensive Dictionary and Term Variation Analysis for COVID-19 and\n  SARS-CoV-2", "comments": "Accepted EMNLP NLP-COVID Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of unique terms in the scientific literature used to refer to\neither SARS-CoV-2 or COVID-19 is remarkably large and has continued to increase\nrapidly despite well-established standardized terms. This high degree of term\nvariation makes high recall identification of these important entities\ndifficult. In this manuscript we present an extensive dictionary of terms used\nin the literature to refer to SARS-CoV-2 and COVID-19. We use a rule-based\napproach to iteratively generate new term variants, then locate these variants\nin a large text corpus. We compare our dictionary to an extensive collection of\nterminological resources, demonstrating that our resource provides a\nsubstantial number of additional terms. We use our dictionary to analyze the\nusage of SARS-CoV-2 and COVID-19 terms over time and show that the number of\nunique terms continues to grow rapidly. Our dictionary is freely available at\nhttps://github.com/ncbi-nlp/CovidTermVar.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:51:53 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Leaman", "Robert", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2010.14606", "submitter": "Arun Narayanan", "authors": "Arun Narayanan, Tara N. Sainath, Ruoming Pang, Jiahui Yu, Chung-Cheng\n  Chiu, Rohit Prabhavalkar, Ehsan Variani, Trevor Strohman", "title": "Cascaded encoders for unifying streaming and non-streaming ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) automatic speech recognition (ASR) models, by now, have\nshown competitive performance on several benchmarks. These models are\nstructured to either operate in streaming or non-streaming mode. This work\npresents cascaded encoders for building a single E2E ASR model that can operate\nin both these modes simultaneously. The proposed model consists of streaming\nand non-streaming encoders. Input features are first processed by the streaming\nencoder; the non-streaming encoder operates exclusively on the output of the\nstreaming encoder. A single decoder then learns to decode either using the\noutput of the streaming or the non-streaming encoder. Results show that this\nmodel achieves similar word error rates (WER) as a standalone streaming model\nwhen operating in streaming mode, and obtains 10% -- 27% relative improvement\nwhen operating in non-streaming mode. Our results also show that the proposed\napproach outperforms existing E2E two-pass models, especially on long-form\nspeech.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:59:50 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Narayanan", "Arun", ""], ["Sainath", "Tara N.", ""], ["Pang", "Ruoming", ""], ["Yu", "Jiahui", ""], ["Chiu", "Chung-Cheng", ""], ["Prabhavalkar", "Rohit", ""], ["Variani", "Ehsan", ""], ["Strohman", "Trevor", ""]]}, {"id": "2010.14649", "submitter": "Takashi Wada", "authors": "Takashi Wada, Tomoharu Iwata, Yuji Matsumoto, Timothy Baldwin, Jey Han\n  Lau", "title": "Learning Contextualised Cross-lingual Word Embeddings for Extremely\n  Low-Resource Languages Using Parallel Corpora", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for learning contextualised cross-lingual word\nembeddings based only on a small parallel corpus (e.g. a few hundred sentence\npairs). Our method obtains word embeddings via an LSTM-based encoder-decoder\nmodel that performs bidirectional translation and reconstruction of the input\nsentence. Through sharing model parameters among different languages, our model\njointly trains the word embeddings in a common multilingual space. We also\npropose a simple method to combine word and subword embeddings to make use of\northographic similarities across different languages. We base our experiments\non real-world data from endangered languages, namely Yongning Na,\nShipibo-Konibo and Griko. Our experiments on bilingual lexicon induction and\nword alignment tasks show that our model outperforms existing methods by a\nlarge margin for most language pairs. These results demonstrate that, contrary\nto common belief, an encoder-decoder translation model is beneficial for\nlearning cross-lingual representations, even in extremely low-resource\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 22:24:01 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wada", "Takashi", ""], ["Iwata", "Tomoharu", ""], ["Matsumoto", "Yuji", ""], ["Baldwin", "Timothy", ""], ["Lau", "Jey Han", ""]]}, {"id": "2010.14660", "submitter": "Pierre Dognin", "authors": "Pierre L. Dognin, Igor Melnyk, Inkit Padhi, Cicero Nogueira dos\n  Santos, Payel Das", "title": "DualTKB: A Dual Learning Bridge between Text and Knowledge Base", "comments": "Equal Contributions of Authors Pierre L. Dognin, Igor Melnyk, and\n  Inkit Padhi. Accepted at EMNLP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a dual learning approach for unsupervised text to\npath and path to text transfers in Commonsense Knowledge Bases (KBs). We\ninvestigate the impact of weak supervision by creating a weakly supervised\ndataset and show that even a slight amount of supervision can significantly\nimprove the model performance and enable better-quality transfers. We examine\ndifferent model architectures, and evaluation metrics, proposing a novel\nCommonsense KB completion metric tailored for generative models. Extensive\nexperimental results show that the proposed method compares very favorably to\nthe existing baselines. This approach is a viable step towards a more advanced\nsystem for automatic KB construction/expansion and the reverse operation of KB\nconversion to coherent textual descriptions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 22:56:18 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Dognin", "Pierre L.", ""], ["Melnyk", "Igor", ""], ["Padhi", "Inkit", ""], ["Santos", "Cicero Nogueira dos", ""], ["Das", "Payel", ""]]}, {"id": "2010.14665", "submitter": "Frank Zhang", "authors": "Yongqiang Wang, Yangyang Shi, Frank Zhang, Chunyang Wu, Julian Chan,\n  Ching-Feng Yeh, Alex Xiao", "title": "Transformer in action: a comparative study of transformer-based acoustic\n  models for large scale speech recognition applications", "comments": "submitted to ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we summarize the application of transformer and its streamable\nvariant, Emformer based acoustic model for large scale speech recognition\napplications. We compare the transformer based acoustic models with their LSTM\ncounterparts on industrial scale tasks. Specifically, we compare Emformer with\nlatency-controlled BLSTM (LCBLSTM) on medium latency tasks and LSTM on low\nlatency tasks. On a low latency voice assistant task, Emformer gets 24% to 26%\nrelative word error rate reductions (WERRs). For medium latency scenarios,\ncomparing with LCBLSTM with similar model size and latency, Emformer gets\nsignificant WERR across four languages in video captioning datasets with 2-3\ntimes inference real-time factors reduction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 23:04:21 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:24:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wang", "Yongqiang", ""], ["Shi", "Yangyang", ""], ["Zhang", "Frank", ""], ["Wu", "Chunyang", ""], ["Chan", "Julian", ""], ["Yeh", "Ching-Feng", ""], ["Xiao", "Alex", ""]]}, {"id": "2010.14678", "submitter": "Amir Pouran Ben Veyseh", "authors": "Amir Pouran Ben Veyseh, Franck Dernoncourt, Quan Hung Tran, Thien Huu\n  Nguyen", "title": "What Does This Acronym Mean? Introducing a New Dataset for Acronym\n  Identification and Disambiguation", "comments": "accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acronyms are the short forms of phrases that facilitate conveying lengthy\nsentences in documents and serve as one of the mainstays of writing. Due to\ntheir importance, identifying acronyms and corresponding phrases (i.e., acronym\nidentification (AI)) and finding the correct meaning of each acronym (i.e.,\nacronym disambiguation (AD)) are crucial for text understanding. Despite the\nrecent progress on this task, there are some limitations in the existing\ndatasets which hinder further improvement. More specifically, limited size of\nmanually annotated AI datasets or noises in the automatically created acronym\nidentification datasets obstruct designing advanced high-performing acronym\nidentification models. Moreover, the existing datasets are mostly limited to\nthe medical domain and ignore other domains. In order to address these two\nlimitations, we first create a manually annotated large AI dataset for\nscientific domain. This dataset contains 17,506 sentences which is\nsubstantially larger than previous scientific AI datasets. Next, we prepare an\nAD dataset for scientific domain with 62,441 samples which is significantly\nlarger than the previous scientific AD dataset. Our experiments show that the\nexisting state-of-the-art models fall far behind human-level performance on\nboth datasets proposed by this work. In addition, we propose a new deep\nlearning model that utilizes the syntactical structure of the sentence to\nexpand an ambiguous acronym in a sentence. The proposed model outperforms the\nstate-of-the-art models on the new AD dataset, providing a strong baseline for\nfuture research on this dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 00:12:36 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Veyseh", "Amir Pouran Ben", ""], ["Dernoncourt", "Franck", ""], ["Tran", "Quan Hung", ""], ["Nguyen", "Thien Huu", ""]]}, {"id": "2010.14697", "submitter": "Claire Bowern", "authors": "Luke Lindemann and Claire Bowern", "title": "Character Entropy in Modern and Historical Texts: Comparison Metrics for\n  an Undeciphered Manuscript", "comments": "Updated following updates to corpus files (see paper for details)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper outlines the creation of three corpora for multilingual comparison\nand analysis of the Voynich manuscript: a corpus of Voynich texts partitioned\nby Currier language, scribal hand, and transcription system, a corpus of 294\nlanguage samples compiled from Wikipedia, and a corpus of eighteen transcribed\nhistorical texts in eight languages. These corpora will be utilized in\nsubsequent work by the Voynich Working Group at Yale University.\n  We demonstrate the utility of these corpora for studying characteristics of\nthe Voynich script and language, with an analysis of conditional character\nentropy in Voynichese. We discuss the interaction between character entropy and\nlanguage, script size and type, glyph compositionality, scribal conventions and\nabbreviations, positional character variants, and bigram frequency.\n  This analysis characterizes the interaction between script compositionality,\ncharacter size, and predictability. We show that substantial manipulations of\nglyph composition are not sufficient to align conditional entropy levels with\nnatural languages. The unusually predictable nature of the Voynichese script is\nnot attributable to a particular script or transcription system, underlying\nlanguage, or substitution cipher. Voynichese is distinct from every comparison\ntext in our corpora because character placement is highly constrained within\nthe word, and this may indicate the loss of phonemic distinctions from the\nunderlying language.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 01:53:59 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 23:33:40 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lindemann", "Luke", ""], ["Bowern", "Claire", ""]]}, {"id": "2010.14701", "submitter": "Samuel McCandlish", "authors": "Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse,\n  Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris\n  Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M.\n  Ziegler, John Schulman, Dario Amodei, Sam McCandlish", "title": "Scaling Laws for Autoregressive Generative Modeling", "comments": "20+17 pages, 33 figures; added appendix with additional language\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify empirical scaling laws for the cross-entropy loss in four\ndomains: generative image modeling, video modeling, multimodal\nimage$\\leftrightarrow$text models, and mathematical problem solving. In all\ncases autoregressive Transformers smoothly improve in performance as model size\nand compute budgets increase, following a power-law plus constant scaling law.\nThe optimal model size also depends on the compute budget through a power-law,\nwith exponents that are nearly universal across all data domains.\n  The cross-entropy loss has an information theoretic interpretation as\n$S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws\nsuggest a prediction for both the true data distribution's entropy and the KL\ndivergence between the true and model distributions. With this interpretation,\nbillion-parameter Transformers are nearly perfect models of the YFCC100M image\ndistribution downsampled to an $8\\times 8$ resolution, and we can forecast the\nmodel size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in\nnats/image for other resolutions.\n  We find a number of additional scaling laws in specific domains: (a) we\nidentify a scaling relation for the mutual information between captions and\nimages in multimodal models, and show how to answer the question \"Is a picture\nworth a thousand words?\"; (b) in the case of mathematical problem solving, we\nidentify scaling laws for model performance when extrapolating beyond the\ntraining distribution; (c) we finetune generative image models for ImageNet\nclassification and find smooth scaling of the classification loss and error\nrate, even as the generative loss levels off. Taken together, these results\nstrengthen the case that scaling laws have important implications for neural\nnetwork performance, including on downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:17:24 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 04:16:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Henighan", "Tom", ""], ["Kaplan", "Jared", ""], ["Katz", "Mor", ""], ["Chen", "Mark", ""], ["Hesse", "Christopher", ""], ["Jackson", "Jacob", ""], ["Jun", "Heewoo", ""], ["Brown", "Tom B.", ""], ["Dhariwal", "Prafulla", ""], ["Gray", "Scott", ""], ["Hallacy", "Chris", ""], ["Mann", "Benjamin", ""], ["Radford", "Alec", ""], ["Ramesh", "Aditya", ""], ["Ryder", "Nick", ""], ["Ziegler", "Daniel M.", ""], ["Schulman", "John", ""], ["Amodei", "Dario", ""], ["McCandlish", "Sam", ""]]}, {"id": "2010.14707", "submitter": "Yang Qian", "authors": "Yang Qian, Yuanchun Jiang, Yidong Chai, Yezheng Liu, Jiansha Sun", "title": "TopicModel4J: A Java Package for Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models provide a flexible and principled framework for exploring hidden\nstructure in high-dimensional co-occurrence data and are commonly used natural\nlanguage processing (NLP) of text. In this paper, we design and implement a\nJava package, TopicModel4J, which contains 13 kinds of representative\nalgorithms for fitting topic models. The TopicModel4J in the Java programming\nenvironment provides an easy-to-use interface for data analysts to run the\nalgorithms, and allow to easily input and output data. In addition, this\npackage provides a few unstructured text preprocessing techniques, such as\nsplitting textual data into words, lowercasing the words, preforming\nlemmatization and removing the useless characters, URLs and stop words.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:33:41 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Qian", "Yang", ""], ["Jiang", "Yuanchun", ""], ["Chai", "Yidong", ""], ["Liu", "Yezheng", ""], ["Sun", "Jiansha", ""]]}, {"id": "2010.14720", "submitter": "Songlin Yang", "authors": "Songlin Yang, Yong Jiang, Wenjuan Han, Kewei Tu", "title": "Second-Order Unsupervised Neural Dependency Parsing", "comments": "COLING 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the unsupervised dependency parsers are based on first-order\nprobabilistic generative models that only consider local parent-child\ninformation. Inspired by second-order supervised dependency parsing, we\nproposed a second-order extension of unsupervised neural dependency models that\nincorporate grandparent-child or sibling information. We also propose a novel\ndesign of the neural parameterization and optimization methods of the\ndependency models. In second-order models, the number of grammar rules grows\ncubically with the increase of vocabulary size, making it difficult to train\nlexicalized models that may contain thousands of words. To circumvent this\nproblem while still benefiting from both second-order parsing and\nlexicalization, we use the agreement-based learning framework to jointly train\na second-order unlexicalized model and a first-order lexicalized model.\nExperiments on multiple datasets show the effectiveness of our second-order\nmodels compared with recent state-of-the-art methods. Our joint model achieves\na 10% improvement over the previous state-of-the-art parser on the full WSJ\ntest set\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:01:33 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Yang", "Songlin", ""], ["Jiang", "Yong", ""], ["Han", "Wenjuan", ""], ["Tu", "Kewei", ""]]}, {"id": "2010.14725", "submitter": "Ruchao Fan", "authors": "Ruchao Fan, Wei Chu, Peng Chang, Jing Xiao", "title": "CASS-NAT: CTC Alignment-based Single Step Non-autoregressive Transformer\n  for Speech Recognition", "comments": "Accepted to ICASSP2021, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a CTC alignment-based single step non-autoregressive transformer\n(CASS-NAT) for speech recognition. Specifically, the CTC alignment contains the\ninformation of (a) the number of tokens for decoder input, and (b) the time\nspan of acoustics for each token. The information are used to extract acoustic\nrepresentation for each token in parallel, referred to as token-level acoustic\nembedding which substitutes the word embedding in autoregressive transformer\n(AT) to achieve parallel generation in decoder. During inference, an\nerror-based alignment sampling method is proposed to be applied to the CTC\noutput space, reducing the WER and retaining the parallelism as well.\nExperimental results show that the proposed method achieves WERs of 3.8%/9.1%\non Librispeech test clean/other dataset without an external LM, and a CER of\n5.8% on Aishell1 Mandarin corpus, respectively1. Compared to the AT baseline,\nthe CASS-NAT has a performance reduction on WER, but is 51.2x faster in terms\nof RTF. When decoding with an oracle CTC alignment, the lower bound of WER\nwithout LM reaches 2.3% on the test-clean set, indicating the potential of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:14:05 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 22:40:07 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Fan", "Ruchao", ""], ["Chu", "Wei", ""], ["Chang", "Peng", ""], ["Xiao", "Jing", ""]]}, {"id": "2010.14730", "submitter": "Xiaoyu Kou", "authors": "Xiaoyu Kou, Yankai Lin, Yuntao Li, Jiahao Xu, Peng Li, Jie Zhou, Yan\n  Zhang", "title": "DisenE: Disentangling Knowledge Graph Embeddings", "comments": "There are some mistakes in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding (KGE), aiming to embed entities and relations into\nlow-dimensional vectors, has attracted wide attention recently. However, the\nexisting research is mainly based on the black-box neural models, which makes\nit difficult to interpret the learned representation. In this paper, we\nintroduce DisenE, an end-to-end framework to learn disentangled knowledge graph\nembeddings. Specially, we introduce an attention-based mechanism that enables\nthe model to explicitly focus on relevant components of entity embeddings\naccording to a given relation. Furthermore, we introduce two novel regularizers\nto encourage each component of the entity representation to independently\nreflect an isolated semantic aspect. Experimental results demonstrate that our\nproposed DisenE investigates a perspective to address the interpretability of\nKGE and is proved to be an effective way to improve the performance of link\nprediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:45:19 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 12:53:03 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kou", "Xiaoyu", ""], ["Lin", "Yankai", ""], ["Li", "Yuntao", ""], ["Xu", "Jiahao", ""], ["Li", "Peng", ""], ["Zhou", "Jie", ""], ["Zhang", "Yan", ""]]}, {"id": "2010.14759", "submitter": "Yufang Hou", "authors": "Yufang Hou", "title": "Fine-grained Information Status Classification Using Discourse\n  Context-Aware BERT", "comments": "accepted at COLING2020. arXiv admin note: substantial text overlap\n  with arXiv:1908.04755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the\nproblem as a subtask of learning fine-grained information status (IS). However,\nthese systems heavily depend on many hand-crafted linguistic features. In this\npaper, we propose a simple discourse context-aware BERT model for fine-grained\nIS classification. On the ISNotes corpus (Markert et al., 2012), our model\nachieves new state-of-the-art performance on fine-grained IS classification,\nobtaining a 4.8 absolute overall accuracy improvement compared to Hou et al.\n(2013a). More importantly, we also show an improvement of 10.5 F1 points for\nbridging anaphora recognition without using any complex hand-crafted semantic\nfeatures designed for capturing the bridging phenomenon. We further analyze the\ntrained model and find that the most attended signals for each IS category\ncorrespond well to linguistic notions of information status.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 22:30:17 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 14:36:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hou", "Yufang", ""]]}, {"id": "2010.14784", "submitter": "Yuanhao Zhuo", "authors": "Yuanhao Zhuo", "title": "A Chinese Text Classification Method With Low Hardware Requirement Based\n  on Improved Model Concatenation", "comments": "5 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the accuracy performance of Chinese text classification\nmodels with low hardware requirements, an improved concatenation-based model is\ndesigned in this paper, which is a concatenation of 5 different sub-models,\nincluding TextCNN, LSTM, and Bi-LSTM. Compared with the existing ensemble\nlearning method, for a text classification mission, this model's accuracy is 2%\nhigher. Meanwhile, the hardware requirements of this model are much lower than\nthe BERT-based model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 06:32:41 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhuo", "Yuanhao", ""]]}, {"id": "2010.14794", "submitter": "Kun Zhou", "authors": "Kun Zhou, Berrak Sisman, Rui Liu and Haizhou Li", "title": "Seen and Unseen emotional style transfer for voice conversion with a new\n  emotional speech dataset", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional voice conversion aims to transform emotional prosody in speech\nwhile preserving the linguistic content and speaker identity. Prior studies\nshow that it is possible to disentangle emotional prosody using an\nencoder-decoder network conditioned on discrete representation, such as one-hot\nemotion labels. Such networks learn to remember a fixed set of emotional\nstyles. In this paper, we propose a novel framework based on variational\nauto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes\nuse of a pre-trained speech emotion recognition (SER) model to transfer\nemotional style during training and at run-time inference. In this way, the\nnetwork is able to transfer both seen and unseen emotional style to a new\nutterance. We show that the proposed framework achieves remarkable performance\nby consistently outperforming the baseline framework. This paper also marks the\nrelease of an emotional speech dataset (ESD) for voice conversion, which has\nmultiple speakers and languages.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 07:16:18 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 02:30:45 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zhou", "Kun", ""], ["Sisman", "Berrak", ""], ["Liu", "Rui", ""], ["Li", "Haizhou", ""]]}, {"id": "2010.14798", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Ye Bai, Jianhua Tao, Zhengqi\n  wen", "title": "Decoupling Pronunciation and Language for End-to-end Code-switching\n  Automatic Speech Recognition", "comments": "5 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent significant advances witnessed in end-to-end (E2E) ASR\nsystem for code-switching, hunger for audio-text paired data limits the further\nimprovement of the models' performance. In this paper, we propose a decoupled\ntransformer model to use monolingual paired data and unpaired text data to\nalleviate the problem of code-switching data shortage. The model is decoupled\ninto two parts: audio-to-phoneme (A2P) network and phoneme-to-text (P2T)\nnetwork. The A2P network can learn acoustic pattern scenarios using large-scale\nmonolingual paired data. Meanwhile, it generates multiple phoneme sequence\ncandidates for single audio data in real-time during the training process. Then\nthe generated phoneme-text paired data is used to train the P2T network. This\nnetwork can be pre-trained with large amounts of external unpaired text data.\nBy using monolingual data and unpaired text data, the decoupled transformer\nmodel reduces the high dependency on code-switching paired training data of E2E\nmodel to a certain extent. Finally, the two networks are optimized jointly\nthrough attention fusion. We evaluate the proposed method on the public\nMandarin-English code-switching dataset. Compared with our transformer\nbaseline, the proposed method achieves 18.14% relative mix error rate\nreduction.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 07:46:15 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Zhang", "Shuai", ""], ["Yi", "Jiangyan", ""], ["Tian", "Zhengkun", ""], ["Bai", "Ye", ""], ["Tao", "Jianhua", ""], ["wen", "Zhengqi", ""]]}, {"id": "2010.14804", "submitter": "Benlai Tang", "authors": "Zhonghao Li, Benlai Tang, Xiang Yin, Yuan Wan, Ling Xu, Chen Shen,\n  Zejun Ma", "title": "PPG-based singing voice conversion with adversarial representation\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing voice conversion (SVC) aims to convert the voice of one singer to\nthat of other singers while keeping the singing content and melody. On top of\nrecent voice conversion works, we propose a novel model to steadily convert\nsongs while keeping their naturalness and intonation. We build an end-to-end\narchitecture, taking phonetic posteriorgrams (PPGs) as inputs and generating\nmel spectrograms. Specifically, we implement two separate encoders: one encodes\nPPGs as content, and the other compresses mel spectrograms to supply acoustic\nand musical information. To improve the performance on timbre and melody, an\nadversarial singer confusion module and a mel-regressive representation\nlearning module are designed for the model. Objective and subjective\nexperiments are conducted on our private Chinese singing corpus. Comparing with\nthe baselines, our methods can significantly improve the conversion performance\nin terms of naturalness, melody, and voice similarity. Moreover, our PPG-based\nmethod is proved to be robust for noisy sources.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:03:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Li", "Zhonghao", ""], ["Tang", "Benlai", ""], ["Yin", "Xiang", ""], ["Wan", "Yuan", ""], ["Xu", "Ling", ""], ["Shen", "Chen", ""], ["Ma", "Zejun", ""]]}, {"id": "2010.14806", "submitter": "Xiao Pan", "authors": "Liwei Wu, Xiao Pan, Zehui Lin, Yaoming Zhu, Mingxuan Wang, Lei Li", "title": "The Volctrans Machine Translation System for WMT20", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our VolcTrans system on WMT20 shared news translation\ntask. We participated in 8 translation directions. Our basic systems are based\non Transformer, with several variants (wider or deeper Transformers, dynamic\nconvolutions). The final system includes text pre-process, data selection,\nsynthetic data generation, advanced model ensemble, and multilingual\npre-training.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 08:08:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 10:29:53 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wu", "Liwei", ""], ["Pan", "Xiao", ""], ["Lin", "Zehui", ""], ["Zhu", "Yaoming", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2010.14841", "submitter": "Chengyu Wang", "authors": "Yiwu Yao, Yuchao Li, Chengyu Wang, Tianhang Yu, Houjiang Chen,\n  Xiaotang Jiang, Jun Yang, Jun Huang, Wei Lin, Hui Shu, Chengfei Lv", "title": "INT8 Winograd Acceleration for Conv1D Equipped ASR Models Deployed on\n  Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensive computation of Automatic Speech Recognition (ASR) models\nobstructs them from being deployed on mobile devices. In this paper, we present\na novel quantized Winograd optimization pipeline, which combines the\nquantization and fast convolution to achieve efficient inference acceleration\non mobile devices for ASR models. To avoid the information loss due to the\ncombination of quantization and Winograd convolution, a Range-Scaled\nQuantization (RSQ) training method is proposed to expand the quantized\nnumerical range and to distill knowledge from high-precision values. Moreover,\nan improved Conv1D equipped DFSMN (ConvDFSMN) model is designed for mobile\ndeployment. We conduct extensive experiments on both ConvDFSMN and Wav2letter\nmodels. Results demonstrate the models can be effectively optimized with the\nproposed pipeline. Especially, Wav2letter achieves 1.48* speedup with an\napproximate 0.07% WER decrease on ARMv7-based mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:25:49 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Yao", "Yiwu", ""], ["Li", "Yuchao", ""], ["Wang", "Chengyu", ""], ["Yu", "Tianhang", ""], ["Chen", "Houjiang", ""], ["Jiang", "Xiaotang", ""], ["Yang", "Jun", ""], ["Huang", "Jun", ""], ["Lin", "Wei", ""], ["Shu", "Hui", ""], ["Lv", "Chengfei", ""]]}, {"id": "2010.14872", "submitter": "Kristian Miok", "authors": "Kristian Miok, Gregor Pirs and Marko Robnik-Sikonja", "title": "Bayesian Methods for Semi-supervised Text Annotation", "comments": "Accepted for COLING 2020, The 14th Linguistic Annotation Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human annotations are an important source of information in the development\nof natural language understanding approaches. As under the pressure of\nproductivity annotators can assign different labels to a given text, the\nquality of produced annotations frequently varies. This is especially the case\nif decisions are difficult, with high cognitive load, requires awareness of\nbroader context, or careful consideration of background knowledge. To alleviate\nthe problem, we propose two semi-supervised methods to guide the annotation\nprocess: a Bayesian deep learning model and a Bayesian ensemble method. Using a\nBayesian deep learning method, we can discover annotations that cannot be\ntrusted and might require reannotation. A recently proposed Bayesian ensemble\nmethod helps us to combine the annotators' labels with predictions of trained\nmodels. According to the results obtained from three hate speech detection\nexperiments, the proposed Bayesian methods can improve the annotations and\nprediction performance of BERT models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 10:42:04 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Miok", "Kristian", ""], ["Pirs", "Gregor", ""], ["Robnik-Sikonja", "Marko", ""]]}, {"id": "2010.14891", "submitter": "Mayuko Kori", "authors": "Mayuko Kori, Takeshi Tsukada and Naoki Kobayashi", "title": "A Cyclic Proof System for HFLN", "comments": "27 pages", "journal-ref": null, "doi": "10.4230/LIPIcs.CSL.2021.29", "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cyclic proof system allows us to perform inductive reasoning without\nexplicit inductions. We propose a cyclic proof system for HFLN, which is a\nhigher-order predicate logic with natural numbers and alternating fixed-points.\nOurs is the first cyclic proof system for a higher-order logic, to our\nknowledge. Due to the presence of higher-order predicates and alternating\nfixed-points, our cyclic proof system requires a more delicate global condition\non cyclic proofs than the original system of Brotherston and Simpson. We prove\nthe decidability of checking the global condition and soundness of this system,\nand also prove a restricted form of standard completeness for an infinitary\nvariant of our cyclic proof system. A potential application of our cyclic proof\nsystem is semi-automated verification of higher-order programs, based on\nKobayashi et al.'s recent work on reductions from program verification to HFLN\nvalidity checking.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 11:19:53 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:17:08 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kori", "Mayuko", ""], ["Tsukada", "Takeshi", ""], ["Kobayashi", "Naoki", ""]]}, {"id": "2010.14920", "submitter": "Yuchen Liu", "authors": "Yuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing Zong", "title": "Bridging the Modality Gap for Speech-to-Text Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end speech translation aims to translate speech in one language into\ntext in another language via an end-to-end way. Most existing methods employ an\nencoder-decoder structure with a single encoder to learn acoustic\nrepresentation and semantic information simultaneously, which ignores the\nspeech-and-text modality differences and makes the encoder overloaded, leading\nto great difficulty in learning such a model. To address these issues, we\npropose a Speech-to-Text Adaptation for Speech Translation (STAST) model which\naims to improve the end-to-end model performance by bridging the modality gap\nbetween speech and text. Specifically, we decouple the speech translation\nencoder into three parts and introduce a shrink mechanism to match the length\nof speech representation with that of the corresponding text transcription. To\nobtain better semantic representation, we completely integrate a text-based\ntranslation model into the STAST so that two tasks can be trained in the same\nlatent space. Furthermore, we introduce a cross-modal adaptation method to\nclose the distance between speech and text representation. Experimental results\non English-French and English-German speech translation corpora have shown that\nour model significantly outperforms strong baselines, and achieves the new\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:33:04 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Liu", "Yuchen", ""], ["Zhu", "Junnan", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "2010.14952", "submitter": "Isar Nejadgholi", "authors": "Svetlana Kiritchenko and Isar Nejadgholi", "title": "Towards Ethics by Design in Online Abusive Content Detection", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support safety and inclusion in online communications, significant efforts\nin NLP research have been put towards addressing the problem of abusive content\ndetection, commonly defined as a supervised classification task. The research\neffort has spread out across several closely related sub-areas, such as\ndetection of hate speech, toxicity, cyberbullying, etc. There is a pressing\nneed to consolidate the field under a common framework for task formulation,\ndataset design and performance evaluation. Further, despite current\ntechnologies achieving high classification accuracies, several ethical issues\nhave been revealed. We bring ethical issues to forefront and propose a unified\nframework as a two-step process. First, online content is categorized around\npersonal and identity-related subject matters. Second, severity of abuse is\nidentified through comparative annotation within each category. The novel\nframework is guided by the Ethics by Design principle and is a step towards\nbuilding more accurate and trusted models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 13:10:24 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kiritchenko", "Svetlana", ""], ["Nejadgholi", "Isar", ""]]}, {"id": "2010.15025", "submitter": "Xingchen Song", "authors": "Xingchen Song, Zhiyong Wu, Yiheng Huang, Chao Weng, Dan Su, Helen Meng", "title": "Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input", "comments": "Accepted to ICASSP 2021, final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive (NAR) transformer models have achieved significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive (AR) models in automatic speech recognition (ASR). Most of the\nNAR transformers take a fixed-length sequence filled with MASK tokens or a\nredundant sequence copied from encoder states as decoder input, they cannot\nprovide efficient target-side information thus leading to accuracy degradation.\nTo address this problem, we propose a CTC-enhanced NAR transformer, which\ngenerates target sequence by refining predictions of the CTC module.\nExperimental results show that our method outperforms all previous NAR\ncounterparts and achieves 50x faster decoding speed than a strong AR baseline\nwith only 0.0 ~ 0.3 absolute CER degradation on Aishell-1 and Aishell-2\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 15:00:09 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 03:42:42 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Song", "Xingchen", ""], ["Wu", "Zhiyong", ""], ["Huang", "Yiheng", ""], ["Weng", "Chao", ""], ["Su", "Dan", ""], ["Meng", "Helen", ""]]}, {"id": "2010.15036", "submitter": "Usman Naseem", "authors": "Usman Naseem, Imran Razzak, Shah Khalid Khan, Mukesh Prasad", "title": "A Comprehensive Survey on Word Representation Models: From Classical to\n  State-Of-The-Art Word Representation Language Models", "comments": null, "journal-ref": "ACM Transactions on Asian and Low-Resource Language Information\n  Processing (TALLIP) 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word representation has always been an important research area in the history\nof natural language processing (NLP). Understanding such complex text data is\nimperative, given that it is rich in information and can be used widely across\nvarious applications. In this survey, we explore different word representation\nmodels and its power of expression, from the classical to modern-day\nstate-of-the-art word representation language models (LMS). We describe a\nvariety of text representation methods, and model designs have blossomed in the\ncontext of NLP, including SOTA LMs. These models can transform large volumes of\ntext into effective vector representations capturing the same semantic\ninformation. Further, such representations can be utilized by various machine\nlearning (ML) algorithms for a variety of NLP related tasks. In the end, this\nsurvey briefly discusses the commonly used ML and DL based classifiers,\nevaluation metrics and the applications of these word embeddings in different\nNLP tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 15:15:13 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Naseem", "Usman", ""], ["Razzak", "Imran", ""], ["Khan", "Shah Khalid", ""], ["Prasad", "Mukesh", ""]]}, {"id": "2010.15058", "submitter": "Tomek Korbak", "authors": "Tomasz Korbak and Julian Zubek and Joanna R\\k{a}czaszek-Leonardi", "title": "Measuring non-trivial compositionality in emergent communication", "comments": "4th Workshop on Emergent Communication, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositionality is an important explanatory target in emergent communication\nand language evolution. The vast majority of computational models of\ncommunication account for the emergence of only a very basic form of\ncompositionality: trivial compositionality. A compositional protocol is\ntrivially compositional if the meaning of a complex signal (e.g. blue circle)\nboils down to the intersection of meanings of its constituents (e.g. the\nintersection of the set of blue objects and the set of circles). A protocol is\nnon-trivially compositional (NTC) if the meaning of a complex signal (e.g.\nbiggest apple) is a more complex function of the meanings of their\nconstituents. In this paper, we review several metrics of compositionality used\nin emergent communication and experimentally show that most of them fail to\ndetect NTC - i.e. they treat non-trivial compositionality as a failure of\ncompositionality. The one exception is tree reconstruction error, a metric\nmotivated by formal accounts of compositionality. These results emphasise\nimportant limitations of emergent communication research that could hamper\nprogress on modelling the emergence of NTC.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:11:07 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:22:44 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Korbak", "Tomasz", ""], ["Zubek", "Julian", ""], ["R\u0105czaszek-Leonardi", "Joanna", ""]]}, {"id": "2010.15065", "submitter": "Amir Shanehsazzadeh", "authors": "Amir Shanehsazzadeh, David Belanger, David Dohan", "title": "Fixed-Length Protein Embeddings using Contextual Lenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Basic Local Alignment Search Tool (BLAST) is currently the most popular\nmethod for searching databases of biological sequences. BLAST compares\nsequences via similarity defined by a weighted edit distance, which results in\nit being computationally expensive. As opposed to working with edit distance, a\nvector similarity approach can be accelerated substantially using modern\nhardware or hashing techniques. Such an approach would require fixed-length\nembeddings for biological sequences. There has been recent interest in learning\nfixed-length protein embeddings using deep learning models under the hypothesis\nthat the hidden layers of supervised or semi-supervised models could produce\npotentially useful vector embeddings. We consider transformer (BERT) protein\nlanguage models that are pretrained on the TrEMBL data set and learn\nfixed-length embeddings on top of them with contextual lenses. The embeddings\nare trained to predict the family a protein belongs to for sequences in the\nPfam database. We show that for nearest-neighbor family classification,\npretraining offers a noticeable boost in performance and that the corresponding\nlearned embeddings are competitive with BLAST. Furthermore, we show that the\nraw transformer embeddings, obtained via static pooling, do not perform well on\nnearest-neighbor family classification, which suggests that learning embeddings\nin a supervised manner via contextual lenses may be a compute-efficient\nalternative to fine-tuning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:54:55 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Shanehsazzadeh", "Amir", ""], ["Belanger", "David", ""], ["Dohan", "David", ""]]}, {"id": "2010.15067", "submitter": "Muhammed Tarik Altuncu", "authors": "M. Tarik Altuncu, Sophia N. Yaliraki, Mauricio Barahona", "title": "Graph-based Topic Extraction from Vector Embeddings of Text Documents:\n  Application to a Corpus of News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production of news content is growing at an astonishing rate. To help manage\nand monitor the sheer amount of text, there is an increasing need to develop\nefficient methods that can provide insights into emerging content areas, and\nstratify unstructured corpora of text into `topics' that stem intrinsically\nfrom content similarity. Here we present an unsupervised framework that brings\ntogether powerful vector embeddings from natural language processing with tools\nfrom multiscale graph partitioning that can reveal natural partitions at\ndifferent resolutions without making a priori assumptions about the number of\nclusters in the corpus. We show the advantages of graph-based clustering\nthrough end-to-end comparisons with other popular clustering and topic\nmodelling methods, and also evaluate different text vector embeddings, from\nclassic Bag-of-Words to Doc2Vec to the recent transformers based model Bert.\nThis comparative work is showcased through an analysis of a corpus of US news\ncoverage during the presidential election year of 2016.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:20:05 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Altuncu", "M. Tarik", ""], ["Yaliraki", "Sophia N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "2010.15090", "submitter": "Vishal Sunder", "authors": "Vishal Sunder and Eric Fosler-Lussier", "title": "Handling Class Imbalance in Low-Resource Dialogue Systems by Combining\n  Few-Shot Classification and Interpolation", "comments": "5 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utterance classification performance in low-resource dialogue systems is\nconstrained by an inevitably high degree of data imbalance in class labels. We\npresent a new end-to-end pairwise learning framework that is designed\nspecifically to tackle this phenomenon by inducing a few-shot classification\ncapability in the utterance representations and augmenting data through an\ninterpolation of utterance representations. Our approach is a general purpose\ntraining methodology, agnostic to the neural architecture used for encoding\nutterances. We show significant improvements in macro-F1 score over standard\ncross-entropy training for three different neural architectures, demonstrating\nimprovements on a Virtual Patient dialogue dataset as well as a low-resourced\nemulation of the Switchboard dialogue act classification dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 17:05:24 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Sunder", "Vishal", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "2010.15114", "submitter": "Kyle Aitken", "authors": "Kyle Aitken, Vinay V. Ramasesh, Ankush Garg, Yuan Cao, David Sussillo,\n  Niru Maheswaranathan", "title": "The geometry of integration in text classification RNNs", "comments": "9+19 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread application of recurrent neural networks (RNNs) across\na variety of tasks, a unified understanding of how RNNs solve these tasks\nremains elusive. In particular, it is unclear what dynamical patterns arise in\ntrained RNNs, and how those patterns depend on the training dataset or task.\nThis work addresses these questions in the context of a specific natural\nlanguage processing task: text classification. Using tools from dynamical\nsystems analysis, we study recurrent networks trained on a battery of both\nnatural and synthetic text classification tasks. We find the dynamics of these\ntrained RNNs to be both interpretable and low-dimensional. Specifically, across\narchitectures and datasets, RNNs accumulate evidence for each class as they\nprocess the text, using a low-dimensional attractor manifold as the underlying\nmechanism. Moreover, the dimensionality and geometry of the attractor manifold\nare determined by the structure of the training dataset; in particular, we\ndescribe how simple word-count statistics computed on the training dataset can\nbe used to predict these properties. Our observations span multiple\narchitectures and datasets, reflecting a common mechanism RNNs employ to\nperform text classification. To the degree that integration of evidence towards\na decision is a common computational primitive, this work lays the foundation\nfor using dynamical systems techniques to study the inner workings of RNNs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 17:58:53 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Aitken", "Kyle", ""], ["Ramasesh", "Vinay V.", ""], ["Garg", "Ankush", ""], ["Cao", "Yuan", ""], ["Sussillo", "David", ""], ["Maheswaranathan", "Niru", ""]]}, {"id": "2010.15149", "submitter": "Yiwei Luo", "authors": "Yiwei Luo, Dallas Card, Dan Jurafsky", "title": "Detecting Stance in Media on Global Warming", "comments": "9 pages, 6 figures", "journal-ref": "Findings of ACL: EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citing opinions is a powerful yet understudied strategy in argumentation. For\nexample, an environmental activist might say, \"Leading scientists agree that\nglobal warming is a serious concern,\" framing a clause which affirms their own\nstance (\"that global warming is serious\") as an opinion endorsed (\"[scientists]\nagree\") by a reputable source (\"leading\"). In contrast, a global warming denier\nmight frame the same clause as the opinion of an untrustworthy source with a\npredicate connoting doubt: \"Mistaken scientists claim [...].\" Our work studies\nopinion-framing in the global warming (GW) debate, an increasingly partisan\nissue that has received little attention in NLP. We introduce Global Warming\nStance Dataset (GWSD), a dataset of stance-labeled GW sentences, and train a\nBERT classifier to study novel aspects of argumentation in how different sides\nof a debate represent their own and each other's opinions. From 56K news\narticles, we find that similar linguistic devices for self-affirming and\nopponent-doubting discourse are used across GW-accepting and skeptic media,\nthough GW-skeptical media shows more opponent-doubt. We also find that authors\noften characterize sources as hypocritical, by ascribing opinions expressing\nthe author's own view to source entities known to publicly endorse the opposing\nview. We release our stance dataset, model, and lexicons of framing devices for\nfuture work on opinion-framing and the automatic detection of GW stance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:01:02 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 23:40:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Luo", "Yiwei", ""], ["Card", "Dallas", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2010.15225", "submitter": "Dylan Ebert", "authors": "Dylan Ebert, Ellie Pavlick", "title": "A Visuospatial Dataset for Naturalistic Verb Learning", "comments": "9 pages, 3 figures, starsem 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset for training and evaluating grounded language\nmodels. Our data is collected within a virtual reality environment and is\ndesigned to emulate the quality of language data to which a pre-verbal child is\nlikely to have access: That is, naturalistic, spontaneous speech paired with\nrichly grounded visuospatial context. We use the collected data to compare\nseveral distributional semantics models for verb learning. We evaluate neural\nmodels based on 2D (pixel) features as well as feature-engineered models based\non 3D (symbolic, spatial) features, and show that neither modeling approach\nachieves satisfactory performance. Our results are consistent with evidence\nfrom child language acquisition that emphasizes the difficulty of learning\nverbs from naive distributional data. We discuss avenues for future work on\ncognitively-inspired grounded language learning, and release our corpus with\nthe intent of facilitating research on the topic.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:47:13 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ebert", "Dylan", ""], ["Pavlick", "Ellie", ""]]}, {"id": "2010.15251", "submitter": "Marimuthu Kalimuthu", "authors": "Marimuthu Kalimuthu, Aditya Mogadala, Marius Mosbach, Dietrich Klakow", "title": "Fusion Models for Improved Visual Captioning", "comments": "Accepted at \"Multi-Modal Deep Learning: Challenges and Applications\"\n  (MMDLCA), International Conference on Pattern Recognition (ICPR)-2020,\n  Milano, Italia", "journal-ref": "Springer LNCS, volume 12666, 2021", "doi": "10.1007/978-3-030-68780-9_32", "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual captioning aims to generate textual descriptions given images or\nvideos. Traditionally, image captioning models are trained on human annotated\ndatasets such as Flickr30k and MS-COCO, which are limited in size and\ndiversity. This limitation hinders the generalization capabilities of these\nmodels while also rendering them liable to making mistakes. Language models\ncan, however, be trained on vast amounts of freely available unlabelled data\nand have recently emerged as successful language encoders and coherent text\ngenerators. Meanwhile, several unimodal and multimodal fusion techniques have\nbeen proven to work well for natural language generation and automatic speech\nrecognition. Building on these recent developments, and with the aim of\nimproving the quality of generated captions, the contribution of our work in\nthis paper is two-fold: First, we propose a generic multimodal model fusion\nframework for caption generation as well as emendation where we utilize\ndifferent fusion strategies to integrate a pretrained Auxiliary Language Model\n(AuxLM) within the traditional encoder-decoder visual captioning frameworks.\nNext, we employ the same fusion strategies to integrate a pretrained Masked\nLanguage Model (MLM), namely BERT, with a visual captioning model, viz. Show,\nAttend, and Tell, for emending both syntactic and semantic errors in captions.\nOur caption emendation experiments on three benchmark image captioning\ndatasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the\nbaseline, indicating the usefulness of our proposed multimodal fusion\nstrategies. Further, we perform a preliminary qualitative analysis on the\nemended captions and identify error categories based on the type of\ncorrections.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 21:55:25 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 04:01:02 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kalimuthu", "Marimuthu", ""], ["Mogadala", "Aditya", ""], ["Mosbach", "Marius", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2010.15266", "submitter": "Abhinav Singh", "authors": "Abhinav Singh, Patrick Xia, Guanghui Qin, Mahsa Yarmohammadi, Benjamin\n  Van Durme", "title": "CopyNext: Explicit Span Copying and Alignment in Sequence to Sequence\n  Models", "comments": "4th Workshop on Structured Prediction for NLP (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy mechanisms are employed in sequence to sequence models (seq2seq) to\ngenerate reproductions of words from the input to the output. These frameworks,\noperating at the lexical type level, fail to provide an explicit alignment that\nrecords where each token was copied from. Further, they require contiguous\ntoken sequences from the input (spans) to be copied individually. We present a\nmodel with an explicit token-level copy operation and extend it to copying\nentire spans. Our model provides hard alignments between spans in the input and\noutput, allowing for nontraditional applications of seq2seq, like information\nextraction. We demonstrate the approach on Nested Named Entity Recognition,\nachieving near state-of-the-art accuracy with an order of magnitude increase in\ndecoding speed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 22:45:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Singh", "Abhinav", ""], ["Xia", "Patrick", ""], ["Qin", "Guanghui", ""], ["Yarmohammadi", "Mahsa", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2010.15300", "submitter": "Emaad Manzoor", "authors": "Emaad Manzoor, Nihar B. Shah", "title": "Uncovering Latent Biases in Text: Method and Application to Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying systematic disparities in numerical quantities such as employment\nrates and wages between population subgroups provides compelling evidence for\nthe existence of societal biases. However, biases in the text written for\nmembers of different subgroups (such as in recommendation letters for male and\nnon-male candidates), though widely reported anecdotally, remain challenging to\nquantify. In this work, we introduce a novel framework to quantify bias in text\ncaused by the visibility of subgroup membership indicators. We develop a\nnonparametric estimation and inference procedure to estimate this bias. We then\nformalize an identification strategy to causally link the estimated bias to the\nvisibility of subgroup membership indicators, provided observations from time\nperiods both before and after an identity-hiding policy change. We identify an\napplication wherein \"ground truth\" bias can be inferred to evaluate our\nframework, instead of relying on synthetic or secondary data. Specifically, we\napply our framework to quantify biases in the text of peer reviews from a\nreputed machine learning conference before and after the conference adopted a\ndouble-blind reviewing policy. We show evidence of biases in the review ratings\nthat serves as \"ground truth\", and show that our proposed framework accurately\ndetects these biases from the review text without having access to the review\nratings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 01:24:19 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Manzoor", "Emaad", ""], ["Shah", "Nihar B.", ""]]}, {"id": "2010.15313", "submitter": "Keen You", "authors": "Keen You and Dan Goldwasser", "title": "\"where is this relationship going?\": Understanding Relationship\n  Trajectories in Narrative Text", "comments": "Accepted to *Sem 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a new commonsense reasoning task: given a narrative describing a\nsocial interaction that centers on two protagonists, systems make inferences\nabout the underlying relationship trajectory. Specifically, we propose two\nevaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction\nMCQ. In Relationship Outlook Prediction, a system maps an interaction to a\nrelationship outlook that captures how the interaction is expected to change\nthe relationship. In Resolution Prediction, a system attributes a given\nrelationship outlook to a particular resolution that explains the outcome.\nThese two tasks parallel two real-life questions that people frequently ponder\nupon as they navigate different social situations: \"where is this relationship\ngoing?\" and \"how did we end up here?\". To facilitate the investigation of human\nsocial relationships through these two tasks, we construct a new dataset,\nSocial Narrative Tree, which consists of 1250 stories documenting a variety of\ndaily social interactions. The narratives encode a multitude of social elements\nthat interweave to give rise to rich commonsense knowledge of how relationships\nevolve with respect to social interactions. We establish baseline performances\nusing language models and the accuracies are significantly lower than human\nperformance. The results demonstrate that models need to look beyond syntactic\nand semantic signals to comprehend complex human relationships.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 02:07:05 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["You", "Keen", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2010.15316", "submitter": "Michal Malyska", "authors": "Alister D Costa, Stefan Denkovski, Michal Malyska, Sae Young Moon,\n  Brandon Rufino, Zhen Yang, Taylor Killian, Marzyeh Ghassemi", "title": "Multiple Sclerosis Severity Classification From Clinical Text", "comments": "EMNLP 2020 Clinical NLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Sclerosis (MS) is a chronic, inflammatory and degenerative\nneurological disease, which is monitored by a specialist using the Expanded\nDisability Status Scale (EDSS) and recorded in unstructured text in the form of\na neurology consult note. An EDSS measurement contains an overall \"EDSS\" score\nand several functional subscores. Typically, expert knowledge is required to\ninterpret consult notes and generate these scores. Previous approaches used\nlimited context length Word2Vec embeddings and keyword searches to predict\nscores given a consult note, but often failed when scores were not explicitly\nstated. In this work, we present MS-BERT, the first publicly available\ntransformer model trained on real clinical data other than MIMIC. Next, we\npresent MSBC, a classifier that applies MS-BERT to generate embeddings and\npredict EDSS and functional subscores. Lastly, we explore combining MSBC with\nother models through the use of Snorkel to generate scores for unlabelled\nconsult notes. MSBC achieves state-of-the-art performance on all metrics and\nprediction tasks and outperforms the models generated from the Snorkel\nensemble. We improve Macro-F1 by 0.12 (to 0.88) for predicting EDSS and on\naverage by 0.29 (to 0.63) for predicting functional subscores over previous\nWord2Vec CNN and rule-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 02:15:23 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Costa", "Alister D", ""], ["Denkovski", "Stefan", ""], ["Malyska", "Michal", ""], ["Moon", "Sae Young", ""], ["Rufino", "Brandon", ""], ["Yang", "Zhen", ""], ["Killian", "Taylor", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "2010.15360", "submitter": "Shaolei Wang", "authors": "Shaolei Wang, Zhongyuan Wang, Wanxiang Che, Ting Liu", "title": "Combining Self-Training and Self-Supervised Learning for Unsupervised\n  Disfluency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to disfluency detection heavily rely on\nhuman-annotated corpora, which is expensive to obtain in practice. There have\nbeen several proposals to alleviate this issue with, for instance,\nself-supervised learning techniques, but they still require human-annotated\ncorpora. In this work, we explore the unsupervised learning paradigm which can\npotentially work with unlabeled text corpora that are cheaper and easier to\nobtain. Our model builds upon the recent work on Noisy Student Training, a\nsemi-supervised learning approach that extends the idea of self-training.\nExperimental results on the commonly used English Switchboard test set show\nthat our approach achieves competitive performance compared to the previous\nstate-of-the-art supervised systems using contextualized word embeddings (e.g.\nBERT and ELECTRA).\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 05:29:26 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wang", "Shaolei", ""], ["Wang", "Zhongyuan", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2010.15366", "submitter": "Sung-Feng Huang", "authors": "Sung-Feng Huang, Shun-Po Chuang, Da-Rong Liu, Yi-Chen Chen, Gene-Ping\n  Yang, Hung-yi Lee", "title": "Stabilizing Label Assignment for Speech Separation by Self-supervised\n  Pre-training", "comments": "Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech separation has been well developed, with the very successful\npermutation invariant training (PIT) approach, although the frequent label\nassignment switching happening during PIT training remains to be a problem when\nbetter convergence speed and achievable performance are desired. In this paper,\nwe propose to perform self-supervised pre-training to stabilize the label\nassignment in training the speech separation model. Experiments over several\ntypes of self-supervised approaches, several typical speech separation models\nand two different datasets showed that very good improvements are achievable if\na proper self-supervised approach is chosen.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:07:01 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 15:31:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Huang", "Sung-Feng", ""], ["Chuang", "Shun-Po", ""], ["Liu", "Da-Rong", ""], ["Chen", "Yi-Chen", ""], ["Yang", "Gene-Ping", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2010.15411", "submitter": "Milan Gritta", "authors": "Milan Gritta, Gerasimos Lampouras and Ignacio Iacobacci", "title": "Conversation Graph: Data Augmentation, Training and Evaluation for\n  Non-Deterministic Dialogue Management", "comments": "Accepted at Transactions of Association of Computational Linguistics\n  (to be presented at ACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialogue systems typically rely on large amounts of\nhigh-quality training data or require complex handcrafted rules. However,\nexisting datasets are often limited in size considering the complexity of the\ndialogues. Additionally, conventional training signal inference is not suitable\nfor non-deterministic agent behaviour, i.e. considering multiple actions as\nvalid in identical dialogue states. We propose the Conversation Graph\n(ConvGraph), a graph-based representation of dialogues that can be exploited\nfor data augmentation, multi-reference training and evaluation of\nnon-deterministic agents. ConvGraph generates novel dialogue paths to augment\ndata volume and diversity. Intrinsic and extrinsic evaluation across three\ndatasets shows that data augmentation and/or multi-reference training with\nConvGraph can improve dialogue success rates by up to 6.4%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 08:23:24 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 14:22:50 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Gritta", "Milan", ""], ["Lampouras", "Gerasimos", ""], ["Iacobacci", "Ignacio", ""]]}, {"id": "2010.15423", "submitter": "M\\=arcis Pinnis", "authors": "Rihards Kri\\v{s}lauks, M\\=arcis Pinnis", "title": "Tilde at WMT 2020: News Task Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes Tilde's submission to the WMT2020 shared task on news\ntranslation for both directions of the English-Polish language pair in both the\nconstrained and the unconstrained tracks. We follow our submissions from the\nprevious years and build our baseline systems to be morphologically motivated\nsub-word unit-based Transformer base models that we train using the Marian\nmachine translation toolkit. Additionally, we experiment with different\nparallel and monolingual data selection schemes, as well as sampled\nback-translation. Our final models are ensembles of Transformer base and\nTransformer big models that feature right-to-left re-ranking.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 08:59:37 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Kri\u0161lauks", "Rihards", ""], ["Pinnis", "M\u0101rcis", ""]]}, {"id": "2010.15437", "submitter": "Mana Ihori", "authors": "Mana Ihori, Ryo Masumura, Naoki Makishima, Tomohiro Tanaka, Akihiko\n  Takashima, Shota Orihashi", "title": "Memory Attentive Fusion: External Language Model Integration for\n  Transformer-based Sequence-to-Sequence Model", "comments": "Accepted as a short paper at INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel fusion method for integrating an external\nlanguage model (LM) into the Transformer based sequence-to-sequence (seq2seq)\nmodel. While paired data are basically required to train the seq2seq model, the\nexternal LM can be trained with only unpaired data. Thus, it is important to\nleverage memorized knowledge in the external LM for building the seq2seq model,\nsince it is hard to prepare a large amount of paired data. However, the\nexisting fusion methods assume that the LM is integrated with recurrent neural\nnetwork-based seq2seq models instead of the Transformer. Therefore, this paper\nproposes a fusion method that can explicitly utilize network structures in the\nTransformer. The proposed method, called {\\bf memory attentive fusion},\nleverages the Transformer-style attention mechanism that repeats source-target\nattention in a multi-hop manner for reading the memorized knowledge in the LM.\nOur experiments on two text-style conversion tasks demonstrate that the\nproposed method performs better than conventional fusion methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:16:23 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ihori", "Mana", ""], ["Masumura", "Ryo", ""], ["Makishima", "Naoki", ""], ["Tanaka", "Tomohiro", ""], ["Takashima", "Akihiko", ""], ["Orihashi", "Shota", ""]]}, {"id": "2010.15458", "submitter": "Yuyang Nie", "authors": "Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, and Bo Dai", "title": "Named Entity Recognition for Social Media Texts with Semantic\n  Augmentation", "comments": "Natural Language Processing. 9 pages, 3 figures. EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for named entity recognition suffer from data sparsity\nproblems when conducted on short and informal texts, especially user-generated\nsocial media content. Semantic augmentation is a potential way to alleviate\nthis problem. Given that rich semantic information is implicitly preserved in\npre-trained word embeddings, they are potential ideal resources for semantic\naugmentation. In this paper, we propose a neural-based approach to NER for\nsocial media texts where both local (from running text) and augmented semantics\nare taken into account. In particular, we obtain the augmented semantic\ninformation from a large-scale corpus, and propose an attentive semantic\naugmentation module and a gate module to encode and aggregate such information,\nrespectively. Extensive experiments are performed on three benchmark datasets\ncollected from English and Chinese social media platforms, where the results\ndemonstrate the superiority of our approach to previous studies across all\nthree datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 10:06:46 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Nie", "Yuyang", ""], ["Tian", "Yuanhe", ""], ["Wan", "Xiang", ""], ["Song", "Yan", ""], ["Dai", "Bo", ""]]}, {"id": "2010.15466", "submitter": "Yuyang Nie", "authors": "Yuyang Nie, Yuanhe Tian, Yan Song, Xiang Ao, and Xiang Wan", "title": "Improving Named Entity Recognition with Attentive Ensemble of Syntactic\n  Information", "comments": "Natural Language Processing. 15 pages, 3 figures, Findings of\n  EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is highly sensitive to sentential syntactic\nand semantic properties where entities may be extracted according to how they\nare used and placed in the running text. To model such properties, one could\nrely on existing resources to providing helpful knowledge to the NER task; some\nexisting studies proved the effectiveness of doing so, and yet are limited in\nappropriately leveraging the knowledge such as distinguishing the important\nones for particular context. In this paper, we improve NER by leveraging\ndifferent types of syntactic information through attentive ensemble, which\nfunctionalizes by the proposed key-value memory networks, syntax attention, and\nthe gate mechanism for encoding, weighting and aggregating such syntactic\ninformation, respectively. Experimental results on six English and Chinese\nbenchmark datasets suggest the effectiveness of the proposed model and show\nthat it outperforms previous studies on all experiment datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 10:25:17 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Nie", "Yuyang", ""], ["Tian", "Yuanhe", ""], ["Song", "Yan", ""], ["Ao", "Xiang", ""], ["Wan", "Xiang", ""]]}, {"id": "2010.15535", "submitter": "Craig Stewart", "authors": "Ricardo Rei, Craig Stewart, Catarina Farinha, Alon Lavie", "title": "Unbabel's Participation in the WMT20 Metrics Shared Task", "comments": "WMT Metrics Shared Task 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the contribution of the Unbabel team to the WMT 2020 Shared Task\non Metrics. We intend to participate on the segment-level, document-level and\nsystem-level tracks on all language pairs, as well as the 'QE as a Metric'\ntrack. Accordingly, we illustrate results of our models in these tracks with\nreference to test sets from the previous year. Our submissions build upon the\nrecently proposed COMET framework: We train several estimator models to regress\non different human-generated quality scores and a novel ranking model trained\non relative ranks obtained from Direct Assessments. We also propose a simple\ntechnique for converting segment-level predictions into a document-level score.\nOverall, our systems achieve strong results for all language pairs on previous\ntest sets and in many cases set a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:59:44 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Rei", "Ricardo", ""], ["Stewart", "Craig", ""], ["Farinha", "Catarina", ""], ["Lavie", "Alon", ""]]}, {"id": "2010.15598", "submitter": "Micaela Kaplan", "authors": "Micaela Kaplan", "title": "May I Ask Who's Calling? Named Entity Recognition on Call Center\n  Transcripts for Privacy Law Compliance", "comments": "The 6th Workshop on Noisy User-generated Text (W-NUT) 2020 at EMNLP", "journal-ref": "Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop\n  on Noisy User-generated Text (2020) 1-6", "doi": null, "report-no": "https://www.aclweb.org/anthology/2020.wnut-1.1", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate using Named Entity Recognition on a new type of user-generated\ntext: a call center conversation. These conversations combine problems from\nspontaneous speech with problems novel to conversational Automated Speech\nRecognition, including incorrect recognition, alongside other common problems\nfrom noisy user-generated text. Using our own corpus with new annotations,\ntraining custom contextual string embeddings, and applying a BiLSTM-CRF, we\nmatch state-of-the-art results on our novel task.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:53:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kaplan", "Micaela", ""]]}, {"id": "2010.15600", "submitter": "Ciro Garcia Mr", "authors": "Ciro Ivan Garcia Lopez", "title": "Three computational models and its equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.CL cs.GL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The study of computability has its origin in Hilbert's conference of 1900,\nwhere an adjacent question, to the ones he asked, is to give a precise\ndescription of the notion of algorithm. In the search for a good definition\narose three independent theories: Turing and the Turing machines, G\\\"odel and\nthe recursive functions, Church and the Lambda Calculus.\n  Later there were established by Kleene that the classic models of computation\nare equivalent. This fact is widely accepted by many textbooks and the proof is\nomitted since the proof is tedious and unreadable. We intend to fill this gap\npresenting the proof in a modern way, without forgetting the mathematical\ndetails.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 05:55:19 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Lopez", "Ciro Ivan Garcia", ""]]}, {"id": "2010.15602", "submitter": "Junhua Liu", "authors": "Nachamma Sockalingam and Junhua Liu", "title": "Designing learning experiences for online teaching and learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching is about constantly innovating strategies, ways and means to engage\ndiverse students in active and meaningful learning. In line with this, SUTD\nadopts various student-centric teaching and learning teaching methods and\napproaches. This means that our graduate/undergraduate instructors have to be\nready to teach using these student student-centric teaching and learning\npedagogies. In this article, I share my experiences of redesigning this\nteaching course that is typically conducted face-to-face to a synchronous\nonline course and also invite one of the participant in this course to reflect\non his experience as a student.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:03:49 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Sockalingam", "Nachamma", ""], ["Liu", "Junhua", ""]]}, {"id": "2010.15653", "submitter": "Niko Moritz", "authors": "Niko Moritz, Takaaki Hori, Jonathan Le Roux", "title": "Semi-Supervised Speech Recognition via Graph-based Temporal\n  Classification", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning has demonstrated promising results in automatic\nspeech recognition (ASR) by self-training using a seed ASR model with\npseudo-labels generated for unlabeled data. The effectiveness of this approach\nlargely relies on the pseudo-label accuracy, for which typically only the\n1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best\nlist can provide more accurate labels for an unlabeled speech utterance and\nalso reflect uncertainties of the seed ASR model. In this paper, we propose a\ngeneralized form of the connectionist temporal classification (CTC) objective\nthat accepts a graph representation of the training labels. The newly proposed\ngraph-based temporal classification (GTC) objective is applied for\nself-training with WFST-based supervision, which is generated from an N-best\nlist of pseudo-labels. In this setup, GTC is used to learn not only a temporal\nalignment, similarly to CTC, but also a label alignment to obtain the optimal\npseudo-label sequence from the weighted graph. Results show that this approach\ncan effectively exploit an N-best list of pseudo-labels with associated scores,\nconsiderably outperforming standard pseudo-labeling, with ASR results\napproaching an oracle experiment in which the best hypotheses of the N-best\nlists are selected manually.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 14:56:56 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 16:51:50 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Moritz", "Niko", ""], ["Hori", "Takaaki", ""], ["Roux", "Jonathan Le", ""]]}, {"id": "2010.15728", "submitter": "Hang Dong", "authors": "Hang Dong, V\\'ictor Su\\'arez-Paniagua, William Whiteley, Honghan Wu", "title": "Explainable Automated Coding of Clinical Notes using Hierarchical\n  Label-wise Attention Networks and Label Embedding Initialisation", "comments": "Accepted to Journal of Biomedical Informatics, structured abstract in\n  full text, 21 pages, 5 figures, 4 supplementary materials (4 extra pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Diagnostic or procedural coding of clinical notes aims to derive a coded\nsummary of disease-related information about patients. Such coding is usually\ndone manually in hospitals but could potentially be automated to improve the\nefficiency and accuracy of medical coding. Recent studies on deep learning for\nautomated medical coding achieved promising performances. However, the\nexplainability of these models is usually poor, preventing them to be used\nconfidently in supporting clinical practice. Another limitation is that these\nmodels mostly assume independence among labels, ignoring the complex\ncorrelation among medical codes which can potentially be exploited to improve\nthe performance. We propose a Hierarchical Label-wise Attention Network (HLAN),\nwhich aimed to interpret the model by quantifying importance (as attention\nweights) of words and sentences related to each of the labels. Secondly, we\npropose to enhance the major deep learning models with a label embedding (LE)\ninitialisation approach, which learns a dense, continuous vector representation\nand then injects the representation into the final layers and the label-wise\nattention layers in the models. We evaluated the methods using three settings\non the MIMIC-III discharge summaries: full codes, top-50 codes, and the UK NHS\nCOVID-19 shielding codes. Experiments were conducted to compare HLAN and LE\ninitialisation to the state-of-the-art neural network based methods. HLAN\nachieved the best Micro-level AUC and $F_1$ on the top-50 code prediction and\ncomparable results on the NHS COVID-19 shielding code prediction to other\nmodels. By highlighting the most salient words and sentences for each label,\nHLAN showed more meaningful and comprehensive model interpretation compared to\nits downgraded baselines and the CNN-based models. LE initialisation\nconsistently boosted most deep learning models for automated medical coding.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 16:21:26 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 11:01:34 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 11:10:48 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 19:23:00 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Dong", "Hang", ""], ["Su\u00e1rez-Paniagua", "V\u00edctor", ""], ["Whiteley", "William", ""], ["Wu", "Honghan", ""]]}, {"id": "2010.15778", "submitter": "Timo Denk", "authors": "Timo I. Denk and Ana Peleteiro Ramallo", "title": "Contextual BERT: Conditioning the Language Model Using a Global State", "comments": "Accepted at the TextGraphs-14 workshop at COLING'2020 - The 28th\n  International Conference on Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT is a popular language model whose main pre-training task is to fill in\nthe blank, i.e., predicting a word that was masked out of a sentence, based on\nthe remaining words. In some applications, however, having an additional\ncontext can help the model make the right prediction, e.g., by taking the\ndomain or the time of writing into account. This motivates us to advance the\nBERT architecture by adding a global state for conditioning on a fixed-sized\ncontext. We present our two novel approaches and apply them to an industry\nuse-case, where we complete fashion outfits with missing articles, conditioned\non a specific customer. An experimental comparison to other methods from the\nliterature shows that our methods improve personalization significantly.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:25:20 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Denk", "Timo I.", ""], ["Ramallo", "Ana Peleteiro", ""]]}, {"id": "2010.15875", "submitter": "Yunchengh Hua", "authors": "Yuncheng Hua, Yuan-Fang Li, Gholamreza Haffari, Guilin Qi and Wei Wu", "title": "Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via\n  Alternate Meta-learning", "comments": "8 pages, 2 figures, published in IJCAI 2020", "journal-ref": "IJCAI 2020: 3679-3686", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compelling approach to complex question answering is to convert the\nquestion to a sequence of actions, which can then be executed on the knowledge\nbase to yield the answer, aka the programmer-interpreter approach. Use similar\ntraining questions to the test question, meta-learning enables the programmer\nto adapt to unseen questions to tackle potential distributional biases quickly.\nHowever, this comes at the cost of manually labeling similar questions to learn\na retrieval model, which is tedious and expensive. In this paper, we present a\nnovel method that automatically learns a retrieval model alternately with the\nprogrammer from weak supervision, i.e., the system's performance with respect\nto the produced answers. To the best of our knowledge, this is the first\nattempt to train the retrieval model with the programmer jointly. Our system\nleads to state-of-the-art performance on a large-scale task for complex\nquestion answering over knowledge bases. We have released our code at\nhttps://github.com/DevinJake/MARL.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:28:16 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Hua", "Yuncheng", ""], ["Li", "Yuan-Fang", ""], ["Haffari", "Gholamreza", ""], ["Qi", "Guilin", ""], ["Wu", "Wei", ""]]}, {"id": "2010.15877", "submitter": "Yunchengh Hua", "authors": "Yuncheng Hua, Yuan-Fang Li, Gholamreza Haffari, Guilin Qi and Tongtong\n  Wu", "title": "Few-Shot Complex Knowledge Base Question Answering via Meta\n  Reinforcement Learning", "comments": "11 pages, 1 figure, accepted in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex question-answering (CQA) involves answering complex natural-language\nquestions on a knowledge base (KB). However, the conventional neural program\ninduction (NPI) approach exhibits uneven performance when the questions have\ndifferent types, harboring inherently different characteristics, e.g.,\ndifficulty level. This paper proposes a meta-reinforcement learning approach to\nprogram induction in CQA to tackle the potential distributional bias in\nquestions. Our method quickly and effectively adapts the meta-learned\nprogrammer to new questions based on the most similar questions retrieved from\nthe training data. The meta-learned policy is then used to learn a good\nprogramming policy, utilizing the trial trajectories and their rewards for\nsimilar questions in the support set. Our method achieves state-of-the-art\nperformance on the CQA dataset (Saha et al., 2018) while using only five trial\ntrajectories for the top-5 retrieved questions in each support set, and\nmetatraining on tasks constructed from only 1% of the training set. We have\nreleased our code at https://github.com/DevinJake/MRL-CQA.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:34:55 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Hua", "Yuncheng", ""], ["Li", "Yuan-Fang", ""], ["Haffari", "Gholamreza", ""], ["Qi", "Guilin", ""], ["Wu", "Tongtong", ""]]}, {"id": "2010.15881", "submitter": "Yunchengh Hua", "authors": "Yuncheng Hua, Yuan-Fang Li, Guilin Qi, Wei Wu, Jingyao Zhang, Daiqing\n  Qi", "title": "Less is More: Data-Efficient Complex Question Answering over Knowledge\n  Bases", "comments": "18 pages, 4 figures, published in JWS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering is an effective method for obtaining information from\nknowledge bases (KB). In this paper, we propose the Neural-Symbolic Complex\nQuestion Answering (NS-CQA) model, a data-efficient reinforcement learning\nframework for complex question answering by using only a modest number of\ntraining samples. Our framework consists of a neural generator and a symbolic\nexecutor that, respectively, transforms a natural-language question into a\nsequence of primitive actions, and executes them over the knowledge base to\ncompute the answer. We carefully formulate a set of primitive symbolic actions\nthat allows us to not only simplify our neural network design but also\naccelerate model convergence. To reduce search space, we employ the copy and\nmasking mechanisms in our encoder-decoder architecture to drastically reduce\nthe decoder output vocabulary and improve model generalizability. We equip our\nmodel with a memory buffer that stores high-reward promising programs. Besides,\nwe propose an adaptive reward function. By comparing the generated trial with\nthe trials stored in the memory buffer, we derive the curriculum-guided reward\nbonus, i.e., the proximity and the novelty. To mitigate the sparse reward\nproblem, we combine the adaptive reward and the reward bonus, reshaping the\nsparse reward into dense feedback. Also, we encourage the model to generate new\ntrials to avoid imitating the spurious trials while making the model remember\nthe past high-reward trials to improve data efficiency. Our NS-CQA model is\nevaluated on two datasets: CQA, a recent large-scale complex question answering\ndataset, and WebQuestionsSP, a multi-hop question answering dataset. On both\ndatasets, our model outperforms the state-of-the-art models. Notably, on CQA,\nNS-CQA performs well on questions with higher complexity, while only using\napproximately 1% of the total training samples.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:42:44 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Hua", "Yuncheng", ""], ["Li", "Yuan-Fang", ""], ["Qi", "Guilin", ""], ["Wu", "Wei", ""], ["Zhang", "Jingyao", ""], ["Qi", "Daiqing", ""]]}, {"id": "2010.15884", "submitter": "Hongbo Rong", "authors": "Hongbo Rong, Xiaochen Hao, Yun Liang, Lidong Xu, Hong H Jiang, Pradeep\n  Dubey", "title": "Systolic Computing on GPUs for Productive Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a language and compiler to productively build high-performance\n{\\it software systolic arrays} that run on GPUs. Based on a rigorous\nmathematical foundation (uniform recurrence equations and space-time\ntransform), our language has a high abstraction level and covers a wide range\nof applications. A programmer {\\it specifies} a projection of a dataflow\ncompute onto a linear systolic array, while leaving the detailed implementation\nof the projection to a compiler; the compiler implements the specified\nprojection and maps the linear systolic array to the SIMD execution units and\nvector registers of GPUs. In this way, both productivity and performance are\nachieved in the same time. This approach neatly combines loop transformations,\ndata shuffling, and vector register allocation into a single framework.\nMeanwhile, many other optimizations can be applied as well; the compiler\ncomposes the optimizations together to generate efficient code.\n  We implemented the approach on Intel GPUs. This is the first system that\nallows productive construction of systolic arrays on GPUs. We allow multiple\nprojections, arbitrary projection directions and linear schedules, which can\nexpress most, if not all, systolic arrays in practice. Experiments with 1- and\n2-D convolution on an Intel GEN9.5 GPU have demonstrated the generality of the\napproach, and its productivity in expressing various systolic designs for\nfinding the best candidate. Although our systolic arrays are purely software\nrunning on generic SIMD hardware, compared with the GPU's specialized, hardware\nsamplers that perform the same convolutions, some of our best designs are up to\n59\\% faster. Overall, this approach holds promise for productive\nhigh-performance computing on GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:49:54 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Rong", "Hongbo", ""], ["Hao", "Xiaochen", ""], ["Liang", "Yun", ""], ["Xu", "Lidong", ""], ["Jiang", "Hong H", ""], ["Dubey", "Pradeep", ""]]}, {"id": "2010.15909", "submitter": "Lasha Abzianidze", "authors": "Lasha Abzianidze", "title": "Learning as Abduction: Trainable Natural Logic Theorem Prover for\n  Natural Language Inference", "comments": "Presented at *SEM, see the official link\n  https://www.aclweb.org/anthology/2020.starsem-1.3 The code available at\n  https://github.com/kovvalsky/LangPro", "journal-ref": "Proceedings of the Ninth Joint Conference on Lexical and\n  Computational Semantics (*SEM). ACL. pp. 20-31. 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tackling Natural Language Inference with a logic-based method is becoming\nless and less common. While this might have been counterintuitive several\ndecades ago, nowadays it seems pretty obvious. The main reasons for such a\nconception are that (a) logic-based methods are usually brittle when it comes\nto processing wide-coverage texts, and (b) instead of automatically learning\nfrom data, they require much of manual effort for development. We make a step\ntowards to overcome such shortcomings by modeling learning from data as\nabduction: reversing a theorem-proving procedure to abduce semantic relations\nthat serve as the best explanation for the gold label of an inference problem.\nIn other words, instead of proving sentence-level inference relations with the\nhelp of lexical relations, the lexical relations are proved taking into account\nthe sentence-level inference relations. We implement the learning method in a\ntableau theorem prover for natural language and show that it improves the\nperformance of the theorem prover on the SICK dataset by 1.4% while still\nmaintaining high precision (>94%). The obtained results are competitive with\nthe state of the art among logic-based systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 19:49:17 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:02:44 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Abzianidze", "Lasha", ""]]}, {"id": "2010.15924", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Ond\\v{r}ej Bojar", "title": "How Many Pages? Paper Length Prediction from the Metadata", "comments": "5 pages, 6 tables. Published in proceedings of NLPIR 2020, the 4th\n  International Conference on Natural Language Processing and Information\n  Retrieval, Seoul, Korea", "journal-ref": null, "doi": "10.1145/3443279.3443305", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict the length of a scientific paper may be helpful in\nnumerous situations. This work defines the paper length prediction task as a\nregression problem and reports several experimental results using popular\nmachine learning models. We also create a huge dataset of publication metadata\nand the respective lengths in number of pages. The dataset will be freely\navailable and is intended to foster research in this domain. As future work, we\nwould like to explore more advanced regressors based on neural networks and big\npretrained language models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:28:24 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:21:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2010.15925", "submitter": "Ekaterina Artemova", "authors": "Tatiana Shavrina and Alena Fenogenova and Anton Emelyanov and Denis\n  Shevelev and Ekaterina Artemova and Valentin Malykh and Vladislav Mikhailov\n  and Maria Tikhonova and Andrey Chertok and Andrey Evlampiev", "title": "RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark", "comments": "to appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an advanced Russian general language\nunderstanding evaluation benchmark -- RussianGLUE. Recent advances in the field\nof universal language models and transformers require the development of a\nmethodology for their broad diagnostics and testing for general intellectual\nskills - detection of natural language inference, commonsense reasoning,\nability to perform simple logical operations regardless of text subject or\nlexicon. For the first time, a benchmark of nine tasks, collected and organized\nanalogically to the SuperGLUE methodology, was developed from scratch for the\nRussian language. We provide baselines, human level evaluation, an open-source\nframework for evaluating models\n(https://github.com/RussianNLP/RussianSuperGLUE), and an overall leaderboard of\ntransformer models for the Russian language. Besides, we present the first\nresults of comparing multilingual models in the adapted diagnostic test set and\noffer the first steps to further expanding or assessing state-of-the-art models\nindependently of language.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:31:39 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 11:02:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Shavrina", "Tatiana", ""], ["Fenogenova", "Alena", ""], ["Emelyanov", "Anton", ""], ["Shevelev", "Denis", ""], ["Artemova", "Ekaterina", ""], ["Malykh", "Valentin", ""], ["Mikhailov", "Vladislav", ""], ["Tikhonova", "Maria", ""], ["Chertok", "Andrey", ""], ["Evlampiev", "Andrey", ""]]}, {"id": "2010.15939", "submitter": "Ekaterina Artemova", "authors": "Vitaly Ivanin and Ekaterina Artemova and Tatiana Batura and Vladimir\n  Ivanov and Veronika Sarkisyan and Elena Tutubalina and Ivan Smurov", "title": "RuREBus: a Case Study of Joint Named Entity Recognition and Relation\n  Extraction from e-Government Domain", "comments": "to appear in AIST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show-case an application of information extraction methods, such as named\nentity recognition (NER) and relation extraction (RE) to a novel corpus,\nconsisting of documents, issued by a state agency. The main challenges of this\ncorpus are: 1) the annotation scheme differs greatly from the one used for the\ngeneral domain corpora, and 2) the documents are written in a language other\nthan English. Unlike expectations, the state-of-the-art transformer-based\nmodels show modest performance for both tasks, either when approached\nsequentially, or in an end-to-end fashion. Our experiments have demonstrated\nthat fine-tuning on a large unlabeled corpora does not automatically yield\nsignificant improvement and thus we may conclude that more sophisticated\nstrategies of leveraging unlabelled texts are demanded. In this paper, we\ndescribe the whole developed pipeline, starting from text annotation, baseline\ndevelopment, and designing a shared task in hopes of improving the baseline.\nEventually, we realize that the current NER and RE technologies are far from\nbeing mature and do not overcome so far challenges like ours.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:56:15 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ivanin", "Vitaly", ""], ["Artemova", "Ekaterina", ""], ["Batura", "Tatiana", ""], ["Ivanov", "Vladimir", ""], ["Sarkisyan", "Veronika", ""], ["Tutubalina", "Elena", ""], ["Smurov", "Ivan", ""]]}, {"id": "2010.15980", "submitter": "Robert Logan", "authors": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer\n  Singh", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically\n  Generated Prompts", "comments": "v2: Fixed error in Figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable success of pretrained language models has motivated the study\nof what kinds of knowledge these models learn during pretraining. Reformulating\ntasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach\nfor gauging such knowledge, however, its usage is limited by the manual effort\nand guesswork required to write suitable prompts. To address this, we develop\nAutoPrompt, an automated method to create prompts for a diverse set of tasks,\nbased on a gradient-guided search. Using AutoPrompt, we show that masked\nlanguage models (MLMs) have an inherent capability to perform sentiment\nanalysis and natural language inference without additional parameters or\nfinetuning, sometimes achieving performance on par with recent state-of-the-art\nsupervised models. We also show that our prompts elicit more accurate factual\nknowledge from MLMs than the manually created prompts on the LAMA benchmark,\nand that MLMs can be used as relation extractors more effectively than\nsupervised relation extraction models. These results demonstrate that\nautomatically generated prompts are a viable parameter-free alternative to\nexisting probing methods, and as pretrained LMs become more sophisticated and\ncapable, potentially a replacement for finetuning.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:54:00 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 05:33:35 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shin", "Taylor", ""], ["Razeghi", "Yasaman", ""], ["Logan", "Robert L.", "IV"], ["Wallace", "Eric", ""], ["Singh", "Sameer", ""]]}, {"id": "2010.16021", "submitter": "Xiang Yue", "authors": "Xiang Yue and Xinliang Frederick Zhang and Ziyu Yao and Simon Lin and\n  Huan Sun", "title": "CliniQG4QA: Generating Diverse Questions for Domain Adaptation of\n  Clinical Question Answering", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical question answering (QA) aims to automatically answer questions from\nmedical professionals based on clinical texts. Studies show that neural QA\nmodels trained on one corpus may not generalize well to new clinical texts from\na different institute or a different patient group, where large-scale QA pairs\nare not readily available for retraining. To address this challenge, we propose\na simple yet effective framework, CliniQG4QA, which leverages question\ngeneration (QG) to synthesize QA pairs on new clinical contexts and boosts QA\nmodels without requiring manual annotations. In order to generate diverse types\nof questions that are essential for training QA models, we further introduce a\nseq2seq-based question phrase prediction (QPP) module that can be used together\nwith most existing QG models to diversify their generation. Our comprehensive\nexperiment results show that the QA corpus generated by our framework is\nhelpful to improve QA models on the new contexts (up to 8% absolute gain in\nterms of Exact Match), and that the QPP module plays a crucial role in\nachieving the gain.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:06:10 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 01:11:23 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yue", "Xiang", ""], ["Zhang", "Xinliang Frederick", ""], ["Yao", "Ziyu", ""], ["Lin", "Simon", ""], ["Sun", "Huan", ""]]}, {"id": "2010.16046", "submitter": "Fuli Luo", "authors": "Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei\n  Huang, Luo Si", "title": "VECO: Variable and Flexible Cross-lingual Pre-training for Language\n  Understanding and Generation", "comments": "Accepted by ACL 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work in multilingual pretraining has demonstrated the potential of\ncross-lingual transferability by training a unified Transformer encoder for\nmultiple languages. However, much of this work only relies on the shared\nvocabulary and bilingual contexts to encourage the correlation across\nlanguages, which is loose and implicit for aligning the contextual\nrepresentations between languages. In this paper, we plug a cross-attention\nmodule into the Transformer encoder to explicitly build the interdependence\nbetween languages. It can effectively avoid the degeneration of predicting\nmasked words only conditioned on the context in its own language. More\nimportantly, when fine-tuning on downstream tasks, the cross-attention module\ncan be plugged in or out on-demand, thus naturally benefiting a wider range of\ncross-lingual tasks, from language understanding to generation.\n  As a result, the proposed cross-lingual model delivers new state-of-the-art\nresults on various cross-lingual understanding tasks of the XTREME benchmark,\ncovering text classification, sequence labeling, question answering, and\nsentence retrieval. For cross-lingual generation tasks, it also outperforms all\nexisting cross-lingual models and state-of-the-art Transformer variants on\nWMT14 English-to-German and English-to-French translation datasets, with gains\nof up to 1~2 BLEU.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:41:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:15:11 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Luo", "Fuli", ""], ["Wang", "Wei", ""], ["Liu", "Jiahao", ""], ["Liu", "Yijia", ""], ["Bi", "Bin", ""], ["Huang", "Songfang", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""]]}, {"id": "2010.16056", "submitter": "Zhihong Chen", "authors": "Zhihong Chen, Yan Song, Tsung-Hui Chang, Xiang Wan", "title": "Generating Radiology Reports via Memory-driven Transformer", "comments": "Natural Language Processing. 11 pages, 6 figures. EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is frequently used in clinical practice and trials for\ndiagnosis and treatment. Writing imaging reports is time-consuming and can be\nerror-prone for inexperienced radiologists. Therefore, automatically generating\nradiology reports is highly desired to lighten the workload of radiologists and\naccordingly promote clinical automation, which is an essential task to apply\nartificial intelligence to the medical domain. In this paper, we propose to\ngenerate radiology reports with memory-driven Transformer, where a relational\nmemory is designed to record key information of the generation process and a\nmemory-driven conditional layer normalization is applied to incorporating the\nmemory into the decoder of Transformer. Experimental results on two prevailing\nradiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed\napproach outperforms previous models with respect to both language generation\nmetrics and clinical evaluations. Particularly, this is the first work\nreporting the generation results on MIMIC-CXR to the best of our knowledge.\nFurther analyses also demonstrate that our approach is able to generate long\nreports with necessary medical terms as well as meaningful image-text attention\nmappings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 04:08:03 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Chen", "Zhihong", ""], ["Song", "Yan", ""], ["Chang", "Tsung-Hui", ""], ["Wan", "Xiang", ""]]}, {"id": "2010.16059", "submitter": "Ningyu Zhang", "authors": "Haiyang Yu, Ningyu Zhang, Shumin Deng, Hongbin Ye, Wei Zhang, Huajun\n  Chen", "title": "Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot\n  Relational Triple Extraction", "comments": "accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current supervised relational triple extraction approaches require huge\namounts of labeled data and thus suffer from poor performance in few-shot\nsettings. However, people can grasp new knowledge by learning a few instances.\nTo this end, we take the first step to study the few-shot relational triple\nextraction, which has not been well understood. Unlike previous single-task\nfew-shot problems, relational triple extraction is more challenging as the\nentities and relations have implicit correlations. In this paper, We propose a\nnovel multi-prototype embedding network model to jointly extract the\ncomposition of relational triples, namely, entity pairs and corresponding\nrelations. To be specific, we design a hybrid prototypical learning mechanism\nthat bridges text and knowledge concerning both entities and relations. Thus,\nimplicit correlations between entities and relations are injected.\nAdditionally, we propose a prototype-aware regularization to learn more\nrepresentative prototypes. Experimental results demonstrate that the proposed\nmethod can improve the performance of the few-shot triple extraction.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 04:18:39 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Yu", "Haiyang", ""], ["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Ye", "Hongbin", ""], ["Zhang", "Wei", ""], ["Chen", "Huajun", ""]]}, {"id": "2010.16068", "submitter": "Ningyu Zhang", "authors": "Juan Li, Ruoxu Wang, Ningyu Zhang, Wen Zhang, Fan Yang, Huajun Chen", "title": "Logic-guided Semantic Representation Learning for Zero-Shot Relation\n  Classification", "comments": "Accepted for COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification aims to extract semantic relations between entity\npairs from the sentences. However, most existing methods can only identify seen\nrelation classes that occurred during training. To recognize unseen relations\nat test time, we explore the problem of zero-shot relation classification.\nPrevious work regards the problem as reading comprehension or textual\nentailment, which have to rely on artificial descriptive information to improve\nthe understandability of relation types. Thus, rich semantic knowledge of the\nrelation labels is ignored. In this paper, we propose a novel logic-guided\nsemantic representation learning model for zero-shot relation classification.\nOur approach builds connections between seen and unseen relations via implicit\nand explicit semantic representations with knowledge graph embeddings and logic\nrules. Extensive experimental results demonstrate that our method can\ngeneralize to unseen relation types and achieve promising improvements.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 04:30:09 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Li", "Juan", ""], ["Wang", "Ruoxu", ""], ["Zhang", "Ningyu", ""], ["Zhang", "Wen", ""], ["Yang", "Fan", ""], ["Chen", "Huajun", ""]]}, {"id": "2010.16071", "submitter": "Yanpei Shi", "authors": "Yanpei Shi, Mingjie Chen, Qiang Huang, Thomas Hain", "title": "T-vectors: Weakly Supervised Speaker Identification Using Hierarchical\n  Transformer Model", "comments": "Submitted to ICASSP2021. arXiv admin note: text overlap with\n  arXiv:2005.07817", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying multiple speakers without knowing where a speaker's voice is in a\nrecording is a challenging task. This paper proposes a hierarchical network\nwith transformer encoders and memory mechanism to address this problem. The\nproposed model contains a frame-level encoder and segment-level encoder, both\nof them make use of the transformer encoder block. The multi-head attention\nmechanism in the transformer structure could better capture different speaker\nproperties when the input utterance contains multiple speakers. The memory\nmechanism used in the frame-level encoders can build a recurrent connection\nthat better capture long-term speaker features. The experiments are conducted\non artificial datasets based on the Switchboard Cellular part1 (SWBC) and\nVoxceleb1 datasets. In different data construction scenarios (Concat and\nOverlap), the proposed model shows better performance comparaing with four\nstrong baselines, reaching 13.3% and 10.5% relative improvement compared with\nH-vectors and S-vectors. The use of memory mechanism could reach 10.6% and 7.7%\nrelative improvement compared with not using memory mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:38:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Shi", "Yanpei", ""], ["Chen", "Mingjie", ""], ["Huang", "Qiang", ""], ["Hain", "Thomas", ""]]}, {"id": "2010.16088", "submitter": "Xiang Chen", "authors": "Tian Li, Xiang Chen, Shanghang Zhang, Zhen Dong, Kurt Keutzer", "title": "Cross-Domain Sentiment Classification with Contrastive Learning and\n  Mutual Information Maximization", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning (CL) has been successful as a powerful representation\nlearning method. In this work we propose CLIM: Contrastive Learning with mutual\nInformation Maximization, to explore the potential of CL on cross-domain\nsentiment classification. To the best of our knowledge, CLIM is the first to\nadopt contrastive learning for natural language processing (NLP) tasks across\ndomains. Due to scarcity of labels on the target domain, we introduce mutual\ninformation maximization (MIM) apart from CL to exploit the features that best\nsupport the final prediction. Furthermore, MIM is able to maintain a relatively\nbalanced distribution of the model's prediction, and enlarges the margin\nbetween classes on the target domain. The larger margin increases our model's\nrobustness and enables the same classifier to be optimal across domains.\nConsequently, we achieve new state-of-the-art results on the Amazon-review\ndataset as well as the airlines dataset, showing the efficacy of our proposed\nmethod CLIM.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 06:12:01 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 02:52:20 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Li", "Tian", ""], ["Chen", "Xiang", ""], ["Zhang", "Shanghang", ""], ["Dong", "Zhen", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2010.16097", "submitter": "Haonan Li", "authors": "Haonan Li, Maria Vasardani, Martin Tomko, Timothy Baldwin", "title": "Target Word Masking for Location Metonymy Resolution", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing metonymy resolution approaches rely on features extracted from\nexternal resources like dictionaries and hand-crafted lexical resources. In\nthis paper, we propose an end-to-end word-level classification approach based\nonly on BERT, without dependencies on taggers, parsers, curated dictionaries of\nplace names, or other external resources. We show that our approach achieves\nthe state-of-the-art on 5 datasets, surpassing conventional BERT models and\nbenchmarks by a large margin. We also show that our approach generalises well\nto unseen data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 06:34:44 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Li", "Haonan", ""], ["Vasardani", "Maria", ""], ["Tomko", "Martin", ""], ["Baldwin", "Timothy", ""]]}, {"id": "2010.16131", "submitter": "Rachid Riad", "authors": "Rachid Riad and Hadrien Titeux and Laurie Lemoine and Justine\n  Montillot and Agnes Sliwinski and Jennifer Hamet Bagnou and Xuan Nga Cao and\n  Anne-Catherine Bachoud-L\\'evi and Emmanuel Dupoux", "title": "Comparison of Speaker Role Recognition and Speaker Enrollment Protocol\n  for conversational Clinical Interviews", "comments": "Submitted to ICASSP 2021,1 pages of supplementary material appear\n  only in the arxiv version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conversations between a clinician and a patient, in natural conditions, are\nvaluable sources of information for medical follow-up. The automatic analysis\nof these dialogues could help extract new language markers and speed-up the\nclinicians' reports. Yet, it is not clear which speech processing pipeline is\nthe most performing to detect and identify the speaker turns, especially for\nindividuals with speech and language disorders. Here, we proposed a split of\nthe data that allows conducting a comparative evaluation of speaker role\nrecognition and speaker enrollment methods to solve this task. We trained\nend-to-end neural network architectures to adapt to each task and evaluate each\napproach under the same metric. Experimental results are reported on\nnaturalistic clinical conversations between Neuropsychologist and Interviewees,\nat different stages of Huntington's disease. We found that our Speaker Role\nRecognition model gave the best performances. In addition, our study underlined\nthe importance of retraining models with in-domain data. Finally, we observed\nthat results do not depend on the demographics of the Interviewee, highlighting\nthe clinical relevance of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:07:37 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 08:09:11 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Riad", "Rachid", ""], ["Titeux", "Hadrien", ""], ["Lemoine", "Laurie", ""], ["Montillot", "Justine", ""], ["Sliwinski", "Agnes", ""], ["Bagnou", "Jennifer Hamet", ""], ["Cao", "Xuan Nga", ""], ["Bachoud-L\u00e9vi", "Anne-Catherine", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2010.16143", "submitter": "Yudong Zhu", "authors": "Yudong Zhu, Di Zhou, Jinghui Xiao, Xin Jiang, Xiao Chen, Qun Liu", "title": "HyperText: Endowing FastText with Hyperbolic Geometry", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language data exhibit tree-like hierarchical structures such as the\nhypernym-hyponym relations in WordNet. FastText, as the state-of-the-art text\nclassifier based on shallow neural network in Euclidean space, may not model\nsuch hierarchies precisely with limited representation capacity. Considering\nthat hyperbolic space is naturally suitable for modeling tree-like hierarchical\ndata, we propose a new model named HyperText for efficient text classification\nby endowing FastText with hyperbolic geometry. Empirically, we show that\nHyperText outperforms FastText on a range of text classification tasks with\nmuch reduced parameters.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:30:54 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 08:58:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhu", "Yudong", ""], ["Zhou", "Di", ""], ["Xiao", "Jinghui", ""], ["Jiang", "Xin", ""], ["Chen", "Xiao", ""], ["Liu", "Qun", ""]]}, {"id": "2010.16218", "submitter": "Claudia Schulz", "authors": "Claudia Schulz and Josh Levy-Kramer and Camille Van Assel and Miklos\n  Kepes and Nils Hammerla", "title": "Biomedical Concept Relatedness -- A large EHR-based benchmark", "comments": "Accepted for publication at the 28th International Conference on\n  Computational Linguistics (COLING 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising application of AI to healthcare is the retrieval of information\nfrom electronic health records (EHRs), e.g. to aid clinicians in finding\nrelevant information for a consultation or to recruit suitable patients for a\nstudy. This requires search capabilities far beyond simple string matching,\nincluding the retrieval of concepts (diagnoses, symptoms, medications, etc.)\nrelated to the one in question. The suitability of AI methods for such\napplications is tested by predicting the relatedness of concepts with known\nrelatedness scores. However, all existing biomedical concept relatedness\ndatasets are notoriously small and consist of hand-picked concept pairs. We\nopen-source a novel concept relatedness benchmark overcoming these issues: it\nis six times larger than existing datasets and concept pairs are chosen based\non co-occurrence in EHRs, ensuring their relevance for the application of\ninterest. We present an in-depth analysis of our new dataset and compare it to\nexisting ones, highlighting that it is not only larger but also complements\nexisting datasets in terms of the types of concepts included. Initial\nexperiments with state-of-the-art embedding methods show that our dataset is a\nchallenging new benchmark for testing concept relatedness models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:20:18 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Schulz", "Claudia", ""], ["Levy-Kramer", "Josh", ""], ["Van Assel", "Camille", ""], ["Kepes", "Miklos", ""], ["Hammerla", "Nils", ""]]}, {"id": "2010.16228", "submitter": "Gerasimos Spanakis", "authors": "Thalea Schlender and Gerasimos Spanakis", "title": "\"Thy algorithm shalt not bear false witness\": An Evaluation of\n  Multiclass Debiasing Methods on Word Embeddings", "comments": "15 pages, presented at BNAIC/BENELEARN 2020, data/code at\n  https://github.com/thaleaschlender/An-Evaluation-of-Multiclass-Debiasing-Methods-on-Word-Embeddings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the vast development and employment of artificial intelligence\napplications, research into the fairness of these algorithms has been\nincreased. Specifically, in the natural language processing domain, it has been\nshown that social biases persist in word embeddings and are thus in danger of\namplifying these biases when used. As an example of social bias, religious\nbiases are shown to persist in word embeddings and the need for its removal is\nhighlighted. This paper investigates the state-of-the-art multiclass debiasing\ntechniques: Hard debiasing, SoftWEAT debiasing and Conceptor debiasing. It\nevaluates their performance when removing religious bias on a common basis by\nquantifying bias removal via the Word Embedding Association Test (WEAT), Mean\nAverage Cosine Similarity (MAC) and the Relative Negative Sentiment Bias\n(RNSB). By investigating the religious bias removal on three widely used word\nembeddings, namely: Word2Vec, GloVe, and ConceptNet, it is shown that the\npreferred method is ConceptorDebiasing. Specifically, this technique manages to\ndecrease the measured religious bias on average by 82,42%, 96,78% and 54,76%\nfor the three word embedding sets respectively.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:49:39 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 09:24:21 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Schlender", "Thalea", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "2010.16249", "submitter": "Haejun Lee", "authors": "Haejun Lee, Drew A. Hudson, Kangwook Lee and Christopher D. Manning", "title": "SLM: Learning a Discourse Language Representation with Sentence\n  Unshuffling", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Sentence-level Language Modeling, a new pre-training objective\nfor learning a discourse language representation in a fully self-supervised\nmanner. Recent pre-training methods in NLP focus on learning either bottom or\ntop-level language representations: contextualized word representations derived\nfrom language model objectives at one extreme and a whole sequence\nrepresentation learned by order classification of two given textual segments at\nthe other. However, these models are not directly encouraged to capture\nrepresentations of intermediate-size structures that exist in natural languages\nsuch as sentences and the relationships among them. To that end, we propose a\nnew approach to encourage learning of a contextualized sentence-level\nrepresentation by shuffling the sequence of input sentences and training a\nhierarchical transformer model to reconstruct the original ordering. Through\nexperiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show\nthat this feature of our model improves the performance of the original BERT by\nlarge margins.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:33:41 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Lee", "Haejun", ""], ["Hudson", "Drew A.", ""], ["Lee", "Kangwook", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2010.16275", "submitter": "Tong Zhu", "authors": "Tong Zhu, Haitao Wang, Junjie Yu, Xiabing Zhou, Wenliang Chen, Wei\n  Zhang, Min Zhang", "title": "Towards Accurate and Consistent Evaluation: A Dataset for\n  Distantly-Supervised Relation Extraction", "comments": "This paper has been accepted for publication in COLING2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, distantly-supervised relation extraction has achieved a\ncertain success by using deep neural networks. Distant Supervision (DS) can\nautomatically generate large-scale annotated data by aligning entity pairs from\nKnowledge Bases (KB) to sentences. However, these DS-generated datasets\ninevitably have wrong labels that result in incorrect evaluation scores during\ntesting, which may mislead the researchers. To solve this problem, we build a\nnew dataset NYTH, where we use the DS-generated data as training data and hire\nannotators to label test data. Compared with the previous datasets, NYT-H has a\nmuch larger test set and then we can perform more accurate and consistent\nevaluation. Finally, we present the experimental results of several widely used\nsystems on NYT-H. The experimental results show that the ranking lists of the\ncomparison systems on the DS-labelled test data and human-annotated test data\nare different. This indicates that our human-annotated data is necessary for\nevaluation of distantly-supervised relation extraction.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:52:52 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Zhu", "Tong", ""], ["Wang", "Haitao", ""], ["Yu", "Junjie", ""], ["Zhou", "Xiabing", ""], ["Chen", "Wenliang", ""], ["Zhang", "Wei", ""], ["Zhang", "Min", ""]]}, {"id": "2010.16314", "submitter": "Max Berrendorf", "authors": "Max Berrendorf and Ludwig Wacker and Evgeniy Faerman", "title": "A Critical Assessment of State-of-the-Art in Entity Alignment", "comments": "updated acknowledgement and fixed typo", "journal-ref": null, "doi": "10.1007/978-3-030-72240-1_2", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we perform an extensive investigation of two state-of-the-art\n(SotA) methods for the task of Entity Alignment in Knowledge Graphs. Therefore,\nwe first carefully examine the benchmarking process and identify several\nshortcomings, which make the results reported in the original works not always\ncomparable. Furthermore, we suspect that it is a common practice in the\ncommunity to make the hyperparameter optimization directly on a test set,\nreducing the informative value of reported performance. Thus, we select a\nrepresentative sample of benchmarking datasets and describe their properties.\nWe also examine different initializations for entity representations since they\nare a decisive factor for model performance. Furthermore, we use a shared\ntrain/validation/test split for a fair evaluation setting in which we evaluate\nall methods on all datasets. In our evaluation, we make several interesting\nfindings. While we observe that most of the time SotA approaches perform better\nthan baselines, they have difficulties when the dataset contains noise, which\nis the case in most real-life applications. Moreover, we find out in our\nablation study that often different features of SotA methods are crucial for\ngood performance than previously assumed. The code is available at\nhttps://github.com/mberr/ea-sota-comparison.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:09:19 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:54:05 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Berrendorf", "Max", ""], ["Wacker", "Ludwig", ""], ["Faerman", "Evgeniy", ""]]}, {"id": "2010.16324", "submitter": "Ahmadreza Mosallanezhad", "authors": "Ahmadreza Mosallanezhad, Kai Shu, Huan Liu", "title": "Topic-Preserving Synthetic News Generation: An Adversarial Deep\n  Reinforcement Learning Approach", "comments": "10 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there exist powerful language models such as OpenAI's GPT-2 that\ncan generate readable text and can be fine-tuned to generate text for a\nspecific domain. Considering GPT-2, it cannot directly generate synthetic news\nwith respect to a given topic and the output of the language model cannot be\nexplicitly controlled. In this paper, we study the novel problem of\ntopic-preserving synthetic news generation. We propose a novel deep\nreinforcement learning-based method to control the output of GPT-2 with respect\nto a given news topic. When generating text using GPT-2, by default, the most\nprobable word is selected from the vocabulary. Instead of selecting the best\nword each time from GPT-2's output, an RL agent tries to select words that\noptimize the matching of a given topic. In addition, using a fake news detector\nas an adversary, we investigate generating realistic news using our proposed\nmethod. In this paper, we consider realistic news as news that cannot be easily\ndetected by a fake news classifier. Experimental results demonstrate the\neffectiveness of the proposed framework on generating topic-preserving news\ncontent than state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:29:16 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Mosallanezhad", "Ahmadreza", ""], ["Shu", "Kai", ""], ["Liu", "Huan", ""]]}, {"id": "2010.16336", "submitter": "Ari Kobren", "authors": "Naveen Jafer Nizar, Ari Kobren", "title": "Leveraging Extracted Model Adversaries for Improved Black Box Attacks", "comments": null, "journal-ref": "Analyzing and interpreting neural networks for NLP, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for adversarial input generation against black box models\nfor reading comprehension based question answering. Our approach is composed of\ntwo steps. First, we approximate a victim black box model via model extraction\n(Krishna et al., 2020). Second, we use our own white box method to generate\ninput perturbations that cause the approximate model to fail. These perturbed\ninputs are used against the victim. In experiments we find that our method\nimproves on the efficacy of the AddAny---a white box attack---performed on the\napproximate model by 25% F1, and the AddSent attack---a black box attack---by\n11% F1 (Jia and Liang, 2017).\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:53:50 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:38:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Nizar", "Naveen Jafer", ""], ["Kobren", "Ari", ""]]}, {"id": "2010.16357", "submitter": "Tavpritesh Sethi", "authors": "Ridam Pal, Rohan Pandey, Vaibhav Gautam, Kanav Bhagat, Tavpritesh\n  Sethi", "title": "A Cross-lingual Natural Language Processing Framework for Infodemic\n  Management", "comments": "8 Pages, 2 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has put immense pressure on health systems which are\nfurther strained due to the misinformation surrounding it. Under such a\nsituation, providing the right information at the right time is crucial. There\nis a growing demand for the management of information spread using Artificial\nIntelligence. Hence, we have exploited the potential of Natural Language\nProcessing for identifying relevant information that needs to be disseminated\namongst the masses. In this work, we present a novel Cross-lingual Natural\nLanguage Processing framework to provide relevant information by matching daily\nnews with trusted guidelines from the World Health Organization. The proposed\npipeline deploys various techniques of NLP such as summarizers, word\nembeddings, and similarity metrics to provide users with news articles along\nwith a corresponding healthcare guideline. A total of 36 models were evaluated\nand a combination of LexRank based summarizer on Word2Vec embedding with Word\nMover distance metric outperformed all other models. This novel open-source\napproach can be used as a template for proactive dissemination of relevant\nhealthcare information in the midst of misinformation spread associated with\nepidemics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:26:35 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Pal", "Ridam", ""], ["Pandey", "Rohan", ""], ["Gautam", "Vaibhav", ""], ["Bhagat", "Kanav", ""], ["Sethi", "Tavpritesh", ""]]}, {"id": "2010.16363", "submitter": "Gregory Yauney", "authors": "Gregory Yauney, Jack Hessel, David Mimno", "title": "Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents", "comments": null, "journal-ref": "Published in EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can give us insights into the contextual meanings of words, but\ncurrent image-text grounding approaches require detailed annotations. Such\ngranular annotation is rare, expensive, and unavailable in most domain-specific\ncontexts. In contrast, unlabeled multi-image, multi-sentence documents are\nabundant. Can lexical grounding be learned from such documents, even though\nthey have significant lexical and visual overlap? Working with a case study\ndataset of real estate listings, we demonstrate the challenge of distinguishing\nhighly correlated grounded terms, such as \"kitchen\" and \"bedroom\", and\nintroduce metrics to assess this document similarity. We present a simple\nunsupervised clustering-based method that increases precision and recall beyond\nobject detection and image tagging baselines when evaluated on labeled subsets\nof the dataset. The proposed method is particularly effective for local\ncontextual meanings of a word, for example associating \"granite\" with\ncountertops in the real estate dataset and with rocky landscapes in a Wikipedia\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:39:49 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Yauney", "Gregory", ""], ["Hessel", "Jack", ""], ["Mimno", "David", ""]]}, {"id": "2010.16368", "submitter": "Wei Zhou", "authors": "Wei Zhou and Simon Berger and Ralf Schl\\\"uter and Hermann Ney", "title": "Phoneme Based Neural Transducer for Large Vocabulary Speech Recognition", "comments": "accepted at ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To join the advantages of classical and end-to-end approaches for speech\nrecognition, we present a simple, novel and competitive approach for\nphoneme-based neural transducer modeling. Different alignment label topologies\nare compared and word-end-based phoneme label augmentation is proposed to\nimprove performance. Utilizing the local dependency of phonemes, we adopt a\nsimplified neural network structure and a straightforward integration with the\nexternal word-level language model to preserve the consistency of seq-to-seq\nmodeling. We also present a simple, stable and efficient training procedure\nusing frame-wise cross-entropy loss. A phonetic context size of one is shown to\nbe sufficient for the best performance. A simplified scheduled sampling\napproach is applied for further improvement and different decoding approaches\nare briefly compared. The overall performance of our best model is comparable\nto state-of-the-art (SOTA) results for the TED-LIUM Release 2 and Switchboard\ncorpora.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:53:29 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 11:48:58 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 11:13:53 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 13:05:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhou", "Wei", ""], ["Berger", "Simon", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2010.16407", "submitter": "Yatin Chaudhary", "authors": "Yatin Chaudhary, Pankaj Gupta, Khushbu Saxena, Vivek Kulkarni, Thomas\n  Runkler, Hinrich Sch\\\"utze", "title": "TopicBERT for Energy Efficient Document Classification", "comments": "EMNLP2020 (Findings): 9 pages, 5 figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior research notes that BERT's computational cost grows quadratically with\nsequence length thus leading to longer training times, higher GPU memory\nconstraints and carbon emissions. While recent work seeks to address these\nscalability issues at pre-training, these issues are also prominent in\nfine-tuning especially for long sequence tasks like document classification.\nOur work thus focuses on optimizing the computational cost of fine-tuning for\ndocument classification. We achieve this by complementary learning of both\ntopic and language models in a unified framework, named TopicBERT. This\nsignificantly reduces the number of self-attention operations - a main\nperformance bottleneck. Consequently, our model achieves a 1.4x ($\\sim40\\%$)\nspeedup with $\\sim40\\%$ reduction in $CO_2$ emission while retaining $99.9\\%$\nperformance over 5 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 00:56:54 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Chaudhary", "Yatin", ""], ["Gupta", "Pankaj", ""], ["Saxena", "Khushbu", ""], ["Kulkarni", "Vivek", ""], ["Runkler", "Thomas", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2010.16408", "submitter": "Khawar Iqbal Malik", "authors": "Irfan Qutab, Khawar Iqbal Malik, Hira Arooj", "title": "Sentiment Analysis for Roman Urdu Text over Social Media, a Comparative\n  Study", "comments": "8 Pages, 12 Figures. International Journal of Computer Science and\n  Network - 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In present century, data volume is increasing enormously. The data could be\nin form for image, text, voice, and video. One factor in this huge growth of\ndata is usage of social media where everyone is posting data on daily basis\nduring chatting, exchanging information, and uploading their personal and\nofficial credential. Research of sentiments seeks to uncover abstract knowledge\nin Published texts in which users communicate their emotions and thoughts about\nshared content, including blogs, news and social networks. Roman Urdu is the\none of most dominant language on social networks in Pakistan and India. Roman\nUrdu is among the varieties of the world's third largest Urdu language but yet\nnot sufficient work has been done in this language. In this article we\naddressed the prior concepts and strategies used to examine the sentiment of\nthe roman Urdu text and reported their results as well.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:19:00 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Qutab", "Irfan", ""], ["Malik", "Khawar Iqbal", ""], ["Arooj", "Hira", ""]]}, {"id": "2010.16410", "submitter": "Xuming Hu", "authors": "Xuming Hu, Fukun Ma, Chenyao Liu, Chenwei Zhang, Lijie Wen, Philip S.\n  Yu", "title": "Semi-supervised Relation Extraction via Incremental Meta Self-Training", "comments": "Code and data available at https://github.com/THU-BPM/MetaSRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate human efforts from obtaining large-scale annotations,\nSemi-Supervised Relation Extraction methods aim to leverage unlabeled data in\naddition to learning from limited samples. Existing self-training methods\nsuffer from the gradual drift problem, where noisy pseudo labels on unlabeled\ndata are incorporated during training. To alleviate the noise in pseudo labels,\nwe propose a method called MetaSRE, where a Relation Label Generation Network\ngenerates accurate quality assessment on pseudo labels by (meta) learning from\nthe successful and failed attempts on Relation Classification as an additional\nmeta-objective. To reduce the influence of noisy pseudo labels, MetaSRE adopts\na pseudo label selection and exploitation scheme which assesses pseudo label\nquality on unlabeled samples and only exploits high-quality pseudo labels in a\nself-training fashion to incrementally augment labeled samples for both\nrobustness and accuracy. Experimental results on two public datasets\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:54:11 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Hu", "Xuming", ""], ["Ma", "Fukun", ""], ["Liu", "Chenyao", ""], ["Zhang", "Chenwei", ""], ["Wen", "Lijie", ""], ["Yu", "Philip S.", ""]]}, {"id": "2010.16411", "submitter": "Sai Krishna Rallabandi", "authors": "Akshat Gupta, Sai Krishna Rallabandi and Alan W Black", "title": "Mere account mein kitna balance hai? -- On building voice enabled\n  Banking Services for Multilingual Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous progress in speech and language processing has brought language\ntechnologies closer to daily human life. Voice technology has the potential to\nact as a horizontal enabling layer across all aspects of digitization. It is\nespecially beneficial to rural communities in scenarios like a pandemic. In\nthis work we present our initial exploratory work towards one such direction --\nbuilding voice enabled banking services for multilingual societies. Speech\ninteraction for typical banking transactions in multilingual communities\ninvolves the presence of filled pauses and is characterized by Code Mixing.\nCode Mixing is a phenomenon where lexical items from one language are embedded\nin the utterance of another. Therefore speech systems deployed for banking\napplications should be able to process such content. In our work we investigate\nvarious training strategies for building speech based intent recognition\nsystems. We present our results using a Naive Bayes classifier on approximate\nacoustic phone units using the Allosaurus library.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 01:20:09 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Gupta", "Akshat", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan W", ""]]}, {"id": "2010.16413", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Robert Leaman, Alexis Allot, Ling Luo, Chih-Hsuan Wei,\n  Shankai Yan, Zhiyong Lu", "title": "Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic\n  with Natural Language Processing (NLP)", "comments": "51 pages, 3 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:10:43 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 18:22:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Qingyu", ""], ["Leaman", "Robert", ""], ["Allot", "Alexis", ""], ["Luo", "Ling", ""], ["Wei", "Chih-Hsuan", ""], ["Yan", "Shankai", ""], ["Lu", "Zhiyong", ""]]}]