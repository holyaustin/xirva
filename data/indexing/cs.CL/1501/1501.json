[{"id": "1501.00311", "submitter": "Jun Ping Ng", "authors": "Jun-Ping Ng and Min-Yen Kan", "title": "QANUS: An Open-source Question-Answering Platform", "comments": "6 pages, 3 figures, demo paper describing QANUS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we motivate the need for a publicly available, generic\nsoftware framework for question-answering (QA) systems. We present an\nopen-source QA framework QANUS which researchers can leverage on to build new\nQA systems easily and rapidly. The framework implements much of the code that\nwill otherwise have been repeated across different QA systems. To demonstrate\nthe utility and practicality of the framework, we further present a fully\nfunctioning factoid QA system QA-SYS built on top of QANUS.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 20:51:25 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Ng", "Jun-Ping", ""], ["Kan", "Min-Yen", ""]]}, {"id": "1501.00657", "submitter": "Scott A. Hale", "authors": "Scott A. Hale", "title": "Cross-language Wikipedia Editing of Okinawa, Japan", "comments": "In Proceedings of the SIGCHI Conference on Human Factors in Computing\n  Systems, CHI 2015. ACM", "journal-ref": null, "doi": "10.1145/2702123.2702346", "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article analyzes users who edit Wikipedia articles about Okinawa, Japan,\nin English and Japanese. It finds these users are among the most active and\ndedicated users in their primary languages, where they make many large,\nhigh-quality edits. However, when these users edit in their non-primary\nlanguages, they tend to make edits of a different type that are overall smaller\nin size and more often restricted to the narrow set of articles that exist in\nboth languages. Design changes to motivate wider contributions from users in\ntheir non-primary languages and to encourage multilingual users to transfer\nmore information across language divides are presented.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 10:10:32 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 16:19:48 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Hale", "Scott A.", ""]]}, {"id": "1501.00841", "submitter": "Gerard Lynch", "authors": "Gerard Lynch and Carl Vogel", "title": "Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama\n  in translation", "comments": "6 pages", "journal-ref": "Digital Humanities 2009 Proceedings, University of Maryland,\n  College Park, MD, USA, pages 192-195", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into the stylistic properties of translations is an issue which has\nreceived some attention in computational stylistics. Previous work by Rybicki\n(2006) on the distinguishing of character idiolects in the work of Polish\nauthor Henryk Sienkiewicz and two corresponding English translations using\nBurrow's Delta method concluded that idiolectal differences could be observed\nin the source texts and this variation was preserved to a large degree in both\ntranslations. This study also found that the two translations were also highly\ndistinguishable from one another. Burrows (2002) examined English translations\nof Juvenal also using the Delta method, results of this work suggest that some\ntranslators are more adept at concealing their own style when translating the\nworks of another author whereas other authors tend to imprint their own style\nto a greater extent on the work they translate. Our work examines the writing\nof a single author, Norwegian playwright Henrik Ibsen, and these writings\ntranslated into both German and English from Norwegian, in an attempt to\ninvestigate the preservation of characterization, defined here as the\ndistinctiveness of textual contributions of characters.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 12:55:03 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Lynch", "Gerard", ""], ["Vogel", "Carl", ""]]}, {"id": "1501.00960", "submitter": "Peter Sheridan Dodds", "authors": "Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds", "title": "Characterizing the Google Books corpus: Strong limits to inferences of\n  socio-cultural and linguistic evolution", "comments": "13 pages, 16 figures", "journal-ref": "PLoS ONE, 10, e0137041, 2015", "doi": "10.1371/journal.pone.0137041", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is tempting to treat frequency trends from the Google Books data sets as\nindicators of the \"true\" popularity of various words and phrases. Doing so\nallows us to draw quantitatively strong conclusions about the evolution of\ncultural perception of a given topic, such as time or gender. However, the\nGoogle Books corpus suffers from a number of limitations which make it an\nobscure mask of cultural popularity. A primary issue is that the corpus is in\neffect a library, containing one of each book. A single, prolific author is\nthereby able to noticeably insert new phrases into the Google Books lexicon,\nwhether the author is widely read or not. With this understood, the Google\nBooks corpus remains an important data set to be considered more lexicon-like\nthan text-like. Here, we show that a distinct problematic feature arises from\nthe inclusion of scientific texts, which have become an increasingly\nsubstantive portion of the corpus throughout the 1900s. The result is a surge\nof phrases typical to academic articles but less common in general, such as\nreferences to time in the form of citations. We highlight these dynamics by\nexamining and comparing major contributions to the statistical divergence of\nEnglish data sets between decades in the period 1800--2000. We find that only\nthe English Fiction data set from the second version of the corpus is not\nheavily affected by professional texts, in clear contrast to the first version\nof the fiction data set and both unfiltered English data sets. Our findings\nemphasize the need to fully characterize the dynamics of the Google Books\ncorpus before using these data sets to draw broad conclusions about cultural\nand linguistic evolution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 20:09:59 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 10:11:46 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 11:46:25 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 13:37:06 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Pechenick", "Eitan Adam", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1501.01243", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno, Javier Ramirez, Iria da Cunha", "title": "Un r\\'esumeur \\`a base de graphes, ind\\'ep\\'endant de la langue", "comments": "8 pages, in French, 2 figures; International Workshop on African\n  Human Language Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present REG, a graph-based approach for study a fundamental\nproblem of Natural Language Processing (NLP): the automatic text summarization.\nThe algorithm maps a document as a graph, then it computes the weight of their\nsentences. We have applied this approach to summarize documents in three\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:27:40 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Ramirez", "Javier", ""], ["da Cunha", "Iria", ""]]}, {"id": "1501.01252", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Mayeul Mathias, Assema Moussa, Fen Zhou, Juan-Manuel Torres-Moreno,\n  Marie-Sylvie Poli, Didier Josselin, Marc El-B\\`eze, Andr\\'ea Carneiro\n  Linhares, Francoise Rigat", "title": "Optimisation using Natural Language Processing: Personalized Tour\n  Recommendation for Museums", "comments": "8 pages, 4 figures; Proceedings of the 2014 Federated Conference on\n  Computer Science and Information Systems pp. 439-446", "journal-ref": null, "doi": "10.15439/2014F336", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method to provide personalized tour recommendation\nfor museum visits. It combines an optimization of preference criteria of\nvisitors with an automatic extraction of artwork importance from museum\ninformation based on Natural Language Processing using textual energy. This\nproject includes researchers from computer and social sciences. Some results\nare obtained with numerical experiments. They show that our model clearly\nimproves the satisfaction of the visitor who follows the proposed tour. This\nwork foreshadows some interesting outcomes and applications about on-demand\npersonalized visit of museums in a very near future.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:58:43 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Mathias", "Mayeul", ""], ["Moussa", "Assema", ""], ["Zhou", "Fen", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Poli", "Marie-Sylvie", ""], ["Josselin", "Didier", ""], ["El-B\u00e8ze", "Marc", ""], ["Linhares", "Andr\u00e9a Carneiro", ""], ["Rigat", "Francoise", ""]]}, {"id": "1501.01254", "submitter": "Manoj Jayaweera", "authors": "A.J.P.M.P. Jayaweera and N.G.J. Dias", "title": "Unknown Words Analysis in POS tagging of Sinhala Language", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part of Speech (POS) is a very vital topic in Natural Language Processing\n(NLP) task in any language, which involves analysing the construction of the\nlanguage, behaviours and the dynamics of the language, the knowledge that could\nbe utilized in computational linguistics analysis and automation applications.\nIn this context, dealing with unknown words (words do not appear in the lexicon\nreferred as unknown words) is also an important task, since growing NLP systems\nare used in more and more new applications. One aid of predicting lexical\ncategories of unknown words is the use of syntactical knowledge of the\nlanguage. The distinction between open class words and closed class words\ntogether with syntactical features of the language used in this research to\npredict lexical categories of unknown words in the tagging process. An\nexperiment is performed to investigate the ability of the approach to parse\nunknown words using syntactical knowledge without human intervention. This\nexperiment shows that the performance of the tagging process is enhanced when\nword class distinction is used together with syntactic rules to parse sentences\ncontaining unknown words in Sinhala language.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 18:03:13 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Jayaweera", "A. J. P. M. P.", ""], ["Dias", "N. G. J.", ""]]}, {"id": "1501.01318", "submitter": "Aymen Abu-Errub Ph.D.", "authors": "Ashraf Odeh, Aymen Abu-Errub, Qusai Shambour and Nidal Turab", "title": "Arabic Text Categorization Algorithm using Vector Evaluation Method", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 6, No 6, December 2014", "doi": "10.5121/ijcsit.2014.6606", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text categorization is the process of grouping documents into categories\nbased on their contents. This process is important to make information\nretrieval easier, and it became more important due to the huge textual\ninformation available online. The main problem in text categorization is how to\nimprove the classification accuracy. Although Arabic text categorization is a\nnew promising field, there are a few researches in this field. This paper\nproposes a new method for Arabic text categorization using vector evaluation.\nThe proposed method uses a categorized Arabic documents corpus, and then the\nweights of the tested document's words are calculated to determine the document\nkeywords which will be compared with the keywords of the corpus categorizes to\ndetermine the tested document's best category.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 21:10:26 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Odeh", "Ashraf", ""], ["Abu-Errub", "Aymen", ""], ["Shambour", "Qusai", ""], ["Turab", "Nidal", ""]]}, {"id": "1501.01386", "submitter": "Rafiullah Khan", "authors": "Misbah Daud, Rafiullah Khan, Mohibullah and Aitazaz Daud", "title": "Roman Urdu Opinion Mining System (RUOMiS)", "comments": "8 pages, 2 figures, 4 tables, Computer Science & Engineering: An\n  International Journal (CSEIJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convincing a customer is always considered as a challenging task in every\nbusiness. But when it comes to online business, this task becomes even more\ndifficult. Online retailers try everything possible to gain the trust of the\ncustomer. One of the solutions is to provide an area for existing users to\nleave their comments. This service can effectively develop the trust of the\ncustomer however normally the customer comments about the product in their\nnative language using Roman script. If there are hundreds of comments this\nmakes difficulty even for the native customers to make a buying decision. This\nresearch proposes a system which extracts the comments posted in Roman Urdu,\ntranslate them, find their polarity and then gives us the rating of the\nproduct. This rating will help the native and non-native customers to make\nbuying decision efficiently from the comments posted in Roman Urdu.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 08:04:32 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Daud", "Misbah", ""], ["Khan", "Rafiullah", ""], ["Mohibullah", "", ""], ["Daud", "Aitazaz", ""]]}, {"id": "1501.01866", "submitter": "Dirk Roorda", "authors": "Dirk Roorda", "title": "The Hebrew Bible as Data: Laboratory - Sharing - Experiences", "comments": "12 pages, 5 figures, follow up on the workshop Biblical Scholarship\n  and Humanities Computing: Data Types, Text, Language and Interpretation, held\n  at the Lorentz Centre Leiden from 6 Feb 2012 through 10 Feb 2012,\n  http://www.lorentzcenter.nl/lc/web/2012/480/report.php3?wsid=480&venue=Oort", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The systematic study of ancient texts including their production,\ntransmission and interpretation is greatly aided by the digital methods that\nstarted taking off in the 1970s. But how is that research in turn transmitted\nto new generations of researchers? We tell a story of Bible and computer across\nthe decades and then point out the current challenges: (1) finding a stable\ndata representation for changing methods of computation; (2) sharing results in\ninter- and intra-disciplinary ways, for reproducibility and\ncross-fertilization. We report recent developments in meeting these challenges.\nThe scene is the text database of the Hebrew Bible, constructed by the Eep\nTalstra Centre for Bible and Computer (ETCBC), which is still growing in detail\nand sophistication. We show how a subtle mix of computational ingredients\nenable scholars to research the transmission and interpretation of the Hebrew\nBible in new ways: (1) a standard data format, Linguistic Annotation Framework\n(LAF); (2) the methods of scientific computing, made accessible by\n(interactive) Python and its associated ecosystem. Additionally, we show how\nthese efforts have culminated in the construction of a new, publicly accessible\nsearch engine SHEBANQ, where the text of the Hebrew Bible and its underlying\ndata can be queried in a simple, yet powerful query language MQL, and where\nthose queries can be saved and shared.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 14:21:39 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Roorda", "Dirk", ""]]}, {"id": "1501.01894", "submitter": "Vinodh Rajan", "authors": "Vinodh Rajan", "title": "Quantifying Scripts: Defining metrics of characters for quantitative and\n  descriptive analysis", "comments": "Manuscript submitted to Literary and Linguistic Computing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of scripts plays an important role in paleography and in\nquantitative linguistics. Especially in the field of digital paleography\nquantitative features are much needed to differentiate glyphs. We describe an\nelaborate set of metrics that quantify qualitative information contained in\ncharacters and hence indirectly also quantify the scribal features. We broadly\ndivide the metrics into several categories and describe each individual metric\nwith its underlying qualitative significance. The metrics are largely derived\nfrom the related area of gesture design and recognition. We also propose\nseveral novel metrics. The proposed metrics are soundly grounded on the\nprinciples of handwriting production and handwriting analysis. These computed\nmetrics could serve as descriptors for scripts and also be used for comparing\nand analyzing scripts. We illustrate some quantitative analysis based on the\nproposed metrics by applying it to the paleographic evolution of the medieval\nTamil script from Brahmi. We also outline future work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 16:12:34 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Rajan", "Vinodh", ""]]}, {"id": "1501.02527", "submitter": "Nicholas Locascio", "authors": "Harini Suresh, Nicholas Locascio", "title": "Autodetection and Classification of Hidden Cultural City Districts from\n  Yelp Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are a way to discover underlying themes in an otherwise\nunstructured collection of documents. In this study, we specifically used the\nLatent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to\nclassify restaurants based off of their reviews. Furthermore, we hypothesize\nthat within a city, restaurants can be grouped into similar \"clusters\" based on\nboth location and similarity. We used several different clustering methods,\nincluding K-means Clustering and a Probabilistic Mixture Model, in order to\nuncover and classify districts, both well-known and hidden (i.e. cultural areas\nlike Chinatown or hearsay like \"the best street for Italian restaurants\")\nwithin a city. We use these models to display and label different clusters on a\nmap. We also introduce a topic similarity heatmap that displays the similarity\ndistribution in a city to a new restaurant.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 03:10:01 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Suresh", "Harini", ""], ["Locascio", "Nicholas", ""]]}, {"id": "1501.02530", "submitter": "Anna Senina", "authors": "Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele", "title": "A Dataset for Movie Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive video service (DVS) provides linguistic descriptions of movies\nand allows visually impaired people to follow a movie along with their peers.\nSuch descriptions are by design mainly visual and thus naturally form an\ninteresting data source for computer vision and computational linguistics. In\nthis work we propose a novel dataset which contains transcribed DVS, which is\ntemporally aligned to full length HD movies. In addition we also collected the\naligned movie scripts which have been used in prior work and compare the two\ndifferent sources of descriptions. In total the Movie Description dataset\ncontains a parallel corpus of over 54,000 sentences and video snippets from 72\nHD movies. We characterize the dataset by benchmarking different approaches for\ngenerating video descriptions. Comparing DVS to scripts, we find that DVS is\nfar more visual and describes precisely what is shown rather than what should\nhappen according to the scripts created prior to movie production.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 03:31:33 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Tandon", "Niket", ""], ["Schiele", "Bernt", ""]]}, {"id": "1501.02598", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham, Marco Baroni", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "comments": "accepted at NAACL 2015, camera ready version, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual\ninformation into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)\nbuild vector-based word representations by learning to predict linguistic\ncontexts in text corpora. However, for a restricted set of words, the models\nare also exposed to visual representations of the objects they denote\n(extracted from natural images), and must predict linguistic and visual\nfeatures jointly. The MMSKIP-GRAM models achieve good performance on a variety\nof semantic benchmarks. Moreover, since they propagate visual information to\nall words, we use them to improve image labeling and retrieval in the zero-shot\nsetup, where the test concepts are never seen during model training. Finally,\nthe MMSKIP-GRAM models discover intriguing visual properties of abstract words,\npaving the way to realistic implementations of embodied theories of meaning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 10:48:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 09:37:08 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 09:47:33 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1501.02670", "submitter": "Amaru Cuba Gyllensten", "authors": "Amaru Cuba Gyllensten and Magnus Sahlgren", "title": "Navigating the Semantic Horizon using Relative Neighborhood Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with nearest neighbor search in distributional\nsemantic models. A normal nearest neighbor search only returns a ranked list of\nneighbors, with no information about the structure or topology of the local\nneighborhood. This is a potentially serious shortcoming of the mode of querying\na distributional semantic model, since a ranked list of neighbors may conflate\nseveral different senses. We argue that the topology of neighborhoods in\nsemantic space provides important information about the different senses of\nterms, and that such topological structures can be used for word-sense\ninduction. We also argue that the topology of the neighborhoods in semantic\nspace can be used to determine the semantic horizon of a point, which we define\nas the set of neighbors that have a direct connection to the point. We\nintroduce relative neighborhood graphs as method to uncover the topological\nproperties of neighborhoods in semantic models. We also provide examples of\nrelative neighborhood graphs for three well-known semantic models; the PMI\nmodel, the GloVe model, and the skipgram model.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 14:48:54 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Gyllensten", "Amaru Cuba", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "1501.02714", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Georgiana Dinu, Adam Liska, Marco Baroni", "title": "From Visual Attributes to Adjectives through Decompositional\n  Distributional Semantics", "comments": "accepted at Transactions of the Association for Computational\n  Linguistics (TACL), 3/2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automated image analysis progresses, there is increasing interest in\nricher linguistic annotation of pictures, with attributes of objects (e.g.,\nfurry, brown...) attracting most attention. By building on the recent\n\"zero-shot learning\" approach, and paying attention to the linguistic nature of\nattributes as noun modifiers, and specifically adjectives, we show that it is\npossible to tag images with attribute-denoting adjectives even when no training\ndata containing the relevant annotation are available. Our approach relies on\ntwo key observations. First, objects can be seen as bundles of attributes,\ntypically expressed as adjectival modifiers (a dog is something furry, brown,\netc.), and thus a function trained to map visual representations of objects to\nnominal labels can implicitly learn to map attributes to adjectives. Second,\nobjects and attributes come together in pictures (the same thing is a dog and\nit is brown). We can thus achieve better attribute (and object) label retrieval\nby treating images as \"visual phrases\", and decomposing their linguistic\nrepresentation into an attribute-denoting adjective and an object-denoting\nnoun. Our approach performs comparably to a method exploiting manual attribute\nannotation, it outperforms various competitive alternatives in both attribute\nand object annotation, and it automatically constructs attribute-centric\nrepresentations that significantly improve performance in supervised object\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 16:48:19 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 12:32:05 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Dinu", "Georgiana", ""], ["Liska", "Adam", ""], ["Baroni", "Marco", ""]]}, {"id": "1501.03191", "submitter": "Michael Bloodgood", "authors": "Benjamin S. Mericli and Michael Bloodgood", "title": "Annotating Cognates and Etymological Origin in Turkic Languages", "comments": "5 pages, 8 tables; appeared in Proceedings of the First Workshop on\n  Language Resources and Technologies for Turkic Languages at the Eighth\n  International Conference on Language Resources and Evaluation (LREC'12),\n  pages 47-51, Istanbul, Turkey, May 2012. European Language Resources\n  Association", "journal-ref": "In Proceedings of the First Workshop on Language Resources and\n  Technologies for Turkic Languages at LREC'12, pages 47-51, Istanbul, Turkey,\n  May 2012. European Language Resources Association", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turkic languages exhibit extensive and diverse etymological relationships\namong lexical items. These relationships make the Turkic languages promising\nfor exploring automated translation lexicon induction by leveraging cognate and\nother etymological information. However, due to the extent and diversity of the\ntypes of relationships between words, it is not clear how to annotate such\ninformation. In this paper, we present a methodology for annotating cognates\nand etymological origin in Turkic languages. Our method strives to balance the\namount of research effort the annotator expends with the utility of the\nannotations for supporting research on improving automated translation lexicon\ninduction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 22:14:57 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Mericli", "Benjamin S.", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1501.03210", "submitter": "Piyush Bansal", "authors": "Piyush Bansal, Romil Bansal and Vasudeva Varma", "title": "Towards Deep Semantic Analysis Of Hashtags", "comments": "To Appear in 37th European Conference on Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashtags are semantico-syntactic constructs used across various social\nnetworking and microblogging platforms to enable users to start a topic\nspecific discussion or classify a post into a desired category. Segmenting and\nlinking the entities present within the hashtags could therefore help in better\nunderstanding and extraction of information shared across the social media.\nHowever, due to lack of space delimiters in the hashtags (e.g #nsavssnowden),\nthe segmentation of hashtags into constituent entities (\"NSA\" and \"Edward\nSnowden\" in this case) is not a trivial task. Most of the current\nstate-of-the-art social media analytics systems like Sentiment Analysis and\nEntity Linking tend to either ignore hashtags, or treat them as a single word.\nIn this paper, we present a context aware approach to segment and link entities\nin the hashtags to a knowledge base (KB) entry, based on the context within the\ntweet. Our approach segments and links the entities in hashtags such that the\ncoherence between hashtag semantics and the tweet is maximized. To the best of\nour knowledge, no existing study addresses the issue of linking entities in\nhashtags for extracting semantic information. We evaluate our method on two\ndifferent datasets, and demonstrate the effectiveness of our technique in\nimproving the overall entity linking in tweets via additional semantic\ninformation provided by segmenting and linking entities in a hashtag.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 23:51:29 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Bansal", "Piyush", ""], ["Bansal", "Romil", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1501.03214", "submitter": "Roger Bilisoly", "authors": "Roger Bilisoly", "title": "Quantifying Prosodic Variability in Middle English Alliterative Poetry", "comments": "12 pages, 8 figures. Based on a presentation given at the Joint\n  Statistical Meetings, Section on Statistical Learning and Data Mining, which\n  took place August, 2014, in Boston, Massachusetts, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in the mathematical structure of poetry dates back to at least the\n19th century: after retiring from his mathematics position, J. J. Sylvester\nwrote a book on prosody called $\\textit{The Laws of Verse}$. Today there is\ninterest in the computer analysis of poems, and this paper discusses how a\nstatistical approach can be applied to this task. Starting with the definition\nof what Middle English alliteration is, $\\textit{Sir Gawain and the Green\nKnight}$ and William Langland's $\\textit{Piers Plowman}$ are used to illustrate\nthe methodology. Theory first developed for analyzing data from a Riemannian\nmanifold turns out to be applicable to strings allowing one to compute a\ngeneralized mean and variance for textual data, which is applied to the poems\nabove. The ratio of these two variances produces the analogue of the F test,\nand resampling allows p-values to be estimated. Consequently, this methodology\nprovides a way to compare prosodic variability between two texts.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 00:06:15 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Bilisoly", "Roger", ""]]}, {"id": "1501.03302", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images", "comments": "Presented in AAAI-15 Workshop: Beyond the Turing Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in language and image understanding by machines has sparkled the\ninterest of the research community in more open-ended, holistic tasks, and\nrefueled an old AI dream of building intelligent machines. We discuss a few\nprominent challenges that characterize such holistic tasks and argue for\n\"question answering about images\" as a particular appealing instance of such a\nholistic task. In particular, we point out that it is a version of a Turing\nTest that is likely to be more robust to over-interpretations and contrast it\nwith tasks like grounding and generation of descriptions. Finally, we discuss\ntools to measure progress in this field.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 10:38:43 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 10:18:54 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1501.04324", "submitter": "Jia Xu Dr.", "authors": "Jia Xu and Geliang Chen", "title": "Phrase Based Language Model For Statistical Machine Translation", "comments": "5 pages. This version of the paper was submitted for review to EMNLP\n  2013. The title, the idea and the content of this paper was presented by the\n  first author in the machine translation group meeting at the MSRA-NLC lab\n  (Microsoft Research Asia, Natural Language Computing) on July 16, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider phrase based Language Models (LM), which generalize the commonly\nused word level models. Similar concept on phrase based LMs appears in speech\nrecognition, which is rather specialized and thus less suitable for machine\ntranslation (MT). In contrast to the dependency LM, we first introduce the\nexhaustive phrase-based LMs tailored for MT use. Preliminary experimental\nresults show that our approach outperform word based LMs with the respect to\nperplexity and translation quality.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 16:37:53 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Xu", "Jia", ""], ["Chen", "Geliang", ""]]}, {"id": "1501.04325", "submitter": "Lars Maaloe", "authors": "Lars Maaloe and Morten Arngren and Ole Winther", "title": "Deep Belief Nets for Topic Modeling", "comments": "Accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning\n  for Text Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying traditional collaborative filtering to digital publishing is\nchallenging because user data is very sparse due to the high volume of\ndocuments relative to the number of users. Content based approaches, on the\nother hand, is attractive because textual content is often very informative. In\nthis paper we describe large-scale content based collaborative filtering for\ndigital publishing. To solve the digital publishing recommender problem we\ncompare two approaches: latent Dirichlet allocation (LDA) and deep belief nets\n(DBN) that both find low-dimensional latent representations for documents.\nEfficient retrieval can be carried out in the latent representation. We work\nboth on public benchmarks and digital media content provided by Issuu, an\nonline publishing platform. This article also comes with a newly developed deep\nbelief nets toolbox for topic modeling tailored towards performance evaluation\nof the DBN model and comparisons to the LDA model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 17:12:59 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Maaloe", "Lars", ""], ["Arngren", "Morten", ""], ["Winther", "Ole", ""]]}, {"id": "1501.04346", "submitter": "Divyanshu Vats", "authors": "Andrew S. Lan and Divyanshu Vats and Andrew E. Waters and Richard G.\n  Baraniuk", "title": "Mathematical Language Processing: Automatic Grading and Feedback for\n  Open Response Mathematical Questions", "comments": "ACM Conference on Learning at Scale, March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While computer and communication technologies have provided effective means\nto scale up many aspects of education, the submission and grading of\nassessments such as homework assignments and tests remains a weak link. In this\npaper, we study the problem of automatically grading the kinds of open response\nmathematical questions that figure prominently in STEM (science, technology,\nengineering, and mathematics) courses. Our data-driven framework for\nmathematical language processing (MLP) leverages solution data from a large\nnumber of learners to evaluate the correctness of their solutions, assign\npartial-credit scores, and provide feedback to each learner on the likely\nlocations of any errors. MLP takes inspiration from the success of natural\nlanguage processing for text data and comprises three main steps. First, we\nconvert each solution to an open response mathematical question into a series\nof numerical features. Second, we cluster the features from several solutions\nto uncover the structures of correct, partially correct, and incorrect\nsolutions. We develop two different clustering approaches, one that leverages\ngeneric clustering algorithms and one based on Bayesian nonparametrics. Third,\nwe automatically grade the remaining (potentially large number of) solutions\nbased on their assigned cluster and one instructor-provided grade per cluster.\nAs a bonus, we can track the cluster assignment of each step of a multistep\nsolution and determine when it departs from a cluster of correct solutions,\nwhich enables us to indicate the likely locations of errors to learners. We\ntest and validate MLP on real-world MOOC data to demonstrate how it can\nsubstantially reduce the human effort required in large-scale educational\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 20:50:39 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Vats", "Divyanshu", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1501.04920", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Gerardo Sierra, Juan-Manuel Torres-Moreno, Alejandro Molina", "title": "Regroupement s\\'emantique de d\\'efinitions en espagnol", "comments": "11 pages, in French, 5 figures. Workshop Evaluation des m\\'ethodes\n  d'Extraction de Connaissances dans les Donn\\'ees EvalECD EGC'10, 2010 Tunis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the description and evaluation of a new unsupervised\nlearning method of clustering of definitions in Spanish according to their\nsemantic. Textual Energy was used as a clustering measure, and we study an\nadaptation of the Precision and Recall to evaluate our method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 19:01:20 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Sierra", "Gerardo", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Molina", "Alejandro", ""]]}, {"id": "1501.05203", "submitter": "Geliang Chen", "authors": "Geliang Chen", "title": "Phrase Based Language Model for Statistical Machine Translation:\n  Empirical Study", "comments": "supplementary material of http://arxiv.org/abs/1501.04324. This\n  version is identical to the Bachelor thesis of Geliang Chen archived on the\n  20th June 2013 in Peking University. Thesis advisor: Professor Jia Xu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reordering is a challenge to machine translation (MT) systems. In MT, the\nwidely used approach is to apply word based language model (LM) which considers\nthe constituent units of a sentence as words. In speech recognition (SR), some\nphrase based LM have been proposed. However, those LMs are not necessarily\nsuitable or optimal for reordering. We propose two phrase based LMs which\nconsiders the constituent units of a sentence as phrases. Experiments show that\nour phrase based LMs outperform the word based LM with the respect of\nperplexity and n-best list re-ranking.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 15:48:28 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 11:40:46 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 07:55:39 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Chen", "Geliang", ""]]}, {"id": "1501.05396", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Etienne Marcheret, Vaibhava Goel", "title": "Deep Multimodal Learning for Audio-Visual Speech Recognition", "comments": "ICASSP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present methods in deep multimodal learning for fusing\nspeech and visual modalities for Audio-Visual Automatic Speech Recognition\n(AV-ASR). First, we study an approach where uni-modal deep networks are trained\nseparately and their final hidden layers fused to obtain a joint feature space\nin which another deep network is built. While the audio network alone achieves\na phone error rate (PER) of $41\\%$ under clean condition on the IBM large\nvocabulary audio-visual studio dataset, this fusion model achieves a PER of\n$35.83\\%$ demonstrating the tremendous value of the visual channel in phone\nclassification even in audio with high signal to noise ratio. Second, we\npresent a new deep network architecture that uses a bilinear softmax layer to\naccount for class specific correlations between modalities. We show that\ncombining the posteriors from the bilinear networks with those from the fused\nmodel mentioned above results in a further significant phone error rate\nreduction, yielding a final PER of $34.03\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 05:25:33 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Mroueh", "Youssef", ""], ["Marcheret", "Etienne", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1501.05940", "submitter": "Taoufik Rachad", "authors": "T. Rachad, J. Boutahar and S. El ghazi", "title": "A New Efficient Method for Calculating Similarity Between Web Services", "comments": "7 pages, 4 figures, 8 tables, International Journal of Advanced\n  Computer Science and Applications (IJACSA),Vol. 5, No. 8, 2014", "journal-ref": null, "doi": "10.14569/IJACSA.2014.050809", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services allow communication between heterogeneous systems in a\ndistributed environment. Their enormous success and their increased use led to\nthe fact that thousands of Web services are present on the Internet. This\nsignificant number of Web services which not cease to increase has led to\nproblems of the difficulty in locating and classifying web services, these\nproblems are encountered mainly during the operations of web services discovery\nand substitution. Traditional ways of search based on keywords are not\nsuccessful in this context, their results do not support the structure of Web\nservices and they consider in their search only the identifiers of the web\nservice description language (WSDL) interface elements. The methods based on\nsemantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web\nservice with a semantic description allow raising partially this problem, but\ntheir complexity and difficulty delays their adoption in real cases. Measuring\nthe similarity between the web services interfaces is the most suitable\nsolution for this kind of problems, it will classify available web services so\nas to know those that best match the searched profile and those that do not\nmatch. Thus, the main goal of this work is to study the degree of similarity\nbetween any two web services by offering a new method that is more effective\nthan existing works.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:18:45 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Rachad", "T.", ""], ["Boutahar", "J.", ""], ["ghazi", "S. El", ""]]}, {"id": "1501.06587", "submitter": "Daniel Lemire", "authors": "Xiaodan Zhu, Peter Turney, Daniel Lemire, Andr\\'e Vellino", "title": "Measuring academic influence: Not all citations are equal", "comments": null, "journal-ref": "Journal of the Association for Information Science and Technology,\n  66: 408-427", "doi": "10.1002/asi.23179", "report-no": null, "categories": "cs.DL cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The importance of a research article is routinely measured by counting how\nmany times it has been cited. However, treating all citations with equal weight\nignores the wide variety of functions that citations perform. We want to\nautomatically identify the subset of references in a bibliography that have a\ncentral academic influence on the citing paper. For this purpose, we examine\nthe effectiveness of a variety of features for determining the academic\ninfluence of a citation. By asking authors to identify the key references in\ntheir own work, we created a data set in which citations were labeled according\nto their academic influence. Using automatic feature selection with supervised\nmachine learning, we found a model for predicting academic influence that\nachieves good performance on this data set using only four features. The best\nfeatures, among those we evaluated, were those based on the number of times a\nreference is mentioned in the body of a citing paper. The performance of these\nfeatures inspired us to design an influence-primed h-index (the hip-index).\nUnlike the conventional h-index, it weights citations by how many times a\nreference is mentioned. According to our experiments, the hip-index is a better\nindicator of researcher performance than the conventional h-index.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 21:06:02 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Zhu", "Xiaodan", ""], ["Turney", "Peter", ""], ["Lemire", "Daniel", ""], ["Vellino", "Andr\u00e9", ""]]}, {"id": "1501.07005", "submitter": "Monika Makwana", "authors": "Monika T. Makwana, Deepak C. Vegda", "title": "Survey:Natural Language Parsing For Indian Languages", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic parsing is a necessary task which is required for NLP applications\nincluding machine translation. It is a challenging task to develop a\nqualitative parser for morphological rich and agglutinative languages.\nSyntactic analysis is used to understand the grammatical structure of a natural\nlanguage sentence. It outputs all the grammatical information of each word and\nits constituent. Also issues related to it help us to understand the language\nin a more detailed way. This literature survey is groundwork to understand the\ndifferent parser development for Indian languages and various approaches that\nare used to develop such tools and techniques. This paper provides a survey of\nresearch papers from well known journals and conferences.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 07:04:18 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Makwana", "Monika T.", ""], ["Vegda", "Deepak C.", ""]]}, {"id": "1501.07496", "submitter": "Helio de Oliveira M.", "authors": "E.L.F. Da Silva and H.M. de Oliveira", "title": "Implementation of an Automatic Syllabic Division Algorithm from Speech\n  Files in Portuguese Language", "comments": "9 pages, 7 figures, 4 tables, conference: XIX Congresso Brasileiro de\n  Automatica CBA, Campina Grande, Setembro, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.DS eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm for voice automatic syllabic splitting in the Portuguese\nlanguage is proposed, which is based on the envelope of the speech signal of\nthe input audio file. A computational implementation in MatlabTM is presented\nand made available at the URL\nhttp://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its\nstraightforwardness, the proposed method is very attractive for embedded\nsystems (e.g. i-phones). It can also be used as a screen to assist more\nsophisticated methods. Voice excerpts containing more than one syllable and\nidentified by the same envelope are named as super-syllables and they are\nsubsequently separated. The results indicate which samples corresponds to the\nbeginning and end of each detected syllable. Preliminary tests were performed\nto fifty words at an identification rate circa 70% (further improvements may be\nincorporated to treat particular phonemes). This algorithm is also useful in\nvoice command systems, as a tool in the teaching of Portuguese language or even\nfor patients with speech pathology.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 16:09:17 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Da Silva", "E. L. F.", ""], ["de Oliveira", "H. M.", ""]]}, {"id": "1501.07676", "submitter": "Issa Atoum", "authors": "Issa Atoum, Chih How Bong, Narayanan Kulathuramaiyer", "title": "Towards Resolving Software Quality-in-Use Measurement Challenges", "comments": "9 pages, 4 figures, Journal of Emerging Trends in Computing and\n  Information Sciences, Vol. 5, No. 11, November 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software quality-in-use comprehends the quality from user's perspectives. It\nhas gained its importance in e-learning applications, mobile service based\napplications and project management tools. User's decisions on software\nacquisitions are often ad hoc or based on preference due to difficulty in\nquantitatively measure software quality-in-use. However, why quality-in-use\nmeasurement is difficult? Although there are many software quality models to\nour knowledge, no works surveys the challenges related to software\nquality-in-use measurement. This paper has two main contributions; 1) presents\nmajor issues and challenges in measuring software quality-in-use in the context\nof the ISO SQuaRE series and related software quality models, 2) Presents a\nnovel framework that can be used to predict software quality-in-use, and 3)\npresents preliminary results of quality-in-use topic prediction. Concisely, the\nissues are related to the complexity of the current standard models and the\nlimitations and incompleteness of the customized software quality models. The\nproposed framework employs sentiment analysis techniques to predict software\nquality-in-use.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 06:51:42 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Atoum", "Issa", ""], ["Bong", "Chih How", ""], ["Kulathuramaiyer", "Narayanan", ""]]}]