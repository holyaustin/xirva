[{"id": "2102.00086", "submitter": "Xuhui Zhou", "authors": "Xuhui Zhou, Maarten Sap, Swabha Swayamdipta, Noah A. Smith, Yejin Choi", "title": "Challenges in Automated Debiasing for Toxic Language Detection", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biased associations have been a challenge in the development of classifiers\nfor detecting toxic language, hindering both fairness and accuracy. As\npotential solutions, we investigate recently introduced debiasing methods for\ntext classification datasets and models, as applied to toxic language\ndetection. Our focus is on lexical (e.g., swear words, slurs, identity\nmentions) and dialectal markers (specifically African American English). Our\ncomprehensive experiments establish that existing methods are limited in their\nability to prevent biased behavior in current toxicity detectors. We then\npropose an automatic, dialect-aware data correction method, as a\nproof-of-concept. Despite the use of synthetic labels, this method reduces\ndialectal associations with toxicity. Overall, our findings show that debiasing\na model trained on biased toxic language data is not as effective as simply\nrelabeling the data to remove existing biases.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 22:03:17 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhou", "Xuhui", ""], ["Sap", "Maarten", ""], ["Swayamdipta", "Swabha", ""], ["Smith", "Noah A.", ""], ["Choi", "Yejin", ""]]}, {"id": "2102.00176", "submitter": "Weizhe Yuan", "authors": "Weizhe Yuan and Pengfei Liu and Graham Neubig", "title": "Can We Automate Scientific Reviewing?", "comments": "TLDR: This paper proposes to use NLP models to generate first-pass\n  peer reviews for scientific papers . (Generated by our system.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The rapid development of science and technology has been accompanied by an\nexponential growth in peer-reviewed scientific publications. At the same time,\nthe review of each paper is a laborious process that must be carried out by\nsubject matter experts. Thus, providing high-quality reviews of this growing\nnumber of papers is a significant challenge. In this work, we ask the question\n\"can we automate scientific reviewing?\", discussing the possibility of using\nstate-of-the-art natural language processing (NLP) models to generate\nfirst-pass peer reviews for scientific papers. Arguably the most difficult part\nof this is defining what a \"good\" review is in the first place, so we first\ndiscuss possible evaluation measures for such reviews. We then collect a\ndataset of papers in the machine learning domain, annotate them with different\naspects of content covered in each review, and train targeted summarization\nmodels that take in papers to generate reviews. Comprehensive experimental\nresults show that system-generated reviews tend to touch upon more aspects of\nthe paper than human-written reviews, but the generated text can suffer from\nlower constructiveness for all aspects except the explanation of the core ideas\nof the papers, which are largely factually correct. We finally summarize eight\nchallenges in the pursuit of a good review generation system together with\npotential solutions, which, hopefully, will inspire more future research on\nthis subject. We make all code, and the dataset publicly available:\nhttps://github.com/neulab/ReviewAdvisor, as well as a ReviewAdvisor system:\nhttp://review.nlpedia.ai/.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 07:16:53 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Yuan", "Weizhe", ""], ["Liu", "Pengfei", ""], ["Neubig", "Graham", ""]]}, {"id": "2102.00214", "submitter": "Nikita Desai Prof", "authors": "Nikita P. Desai, Prof.(Dr.) Vipul K. Dabhi", "title": "Taxonomic survey of Hindi Language NLP systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Natural Language processing (NLP) represents the task of automatic handling\nof natural human language by machines.There is large spectrum of possible\napplications of NLP which help in automating tasks like translating text from\none language to other, retrieving and summarizing data from very huge\nrepositories, spam email filtering, identifying fake news in digital media,\nfind sentiment and feedback of people, find political opinions and views of\npeople on various government policies, provide effective medical assistance\nbased on past history records of patient etc. Hindi is the official language of\nIndia with nearly 691 million users in India and 366 million in rest of world.\nAt present, a number of government and private sector projects and researchers\nin India and abroad, are working towards developing NLP applications and\nresources for Indian languages. This survey gives a report of the resources and\napplications available for Hindi language NLP.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 11:53:56 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Desai", "Nikita P.", "", "Dr."], ["Prof.", "", "", "Dr."], ["Dabhi", "Vipul K.", ""]]}, {"id": "2102.00225", "submitter": "Tong Guo", "authors": "Tong Guo", "title": "Learning From How Human Correct", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 13:13:50 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 02:19:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Guo", "Tong", ""]]}, {"id": "2102.00238", "submitter": "Raviraj Joshi", "authors": "Rutuja Taware, Shraddha Varat, Gaurav Salunke, Chaitanya Gawande,\n  Geetanjali Kale, Rahul Khengare, Raviraj Joshi", "title": "ShufText: A Simple Black Box Approach to Evaluate the Fragility of Text\n  Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is the most basic natural language processing task. It\nhas a wide range of applications ranging from sentiment analysis to topic\nclassification. Recently, deep learning approaches based on CNN, LSTM, and\nTransformers have been the de facto approach for text classification. In this\nwork, we highlight a common issue associated with these approaches. We show\nthat these systems are over-reliant on the important words present in the text\nthat are useful for classification. With limited training data and\ndiscriminative training strategy, these approaches tend to ignore the semantic\nmeaning of the sentence and rather just focus on keywords or important n-grams.\nWe propose a simple black box technique ShutText to present the shortcomings of\nthe model and identify the over-reliance of the model on keywords. This\ninvolves randomly shuffling the words in a sentence and evaluating the\nclassification accuracy. We see that on common text classification datasets\nthere is very little effect of shuffling and with high probability these models\npredict the original class. We also evaluate the effect of language model\npretraining on these models and try to answer questions around model robustness\nto out of domain sentences. We show that simple models based on CNN or LSTM as\nwell as complex models like BERT are questionable in terms of their syntactic\nand semantic understanding.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:18:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Taware", "Rutuja", ""], ["Varat", "Shraddha", ""], ["Salunke", "Gaurav", ""], ["Gawande", "Chaitanya", ""], ["Kale", "Geetanjali", ""], ["Khengare", "Rahul", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2102.00247", "submitter": "Shilun Lin", "authors": "Shilun Lin, Fenglong Xie, Li Meng, Xinhui Li, Li Lu", "title": "Triple M: A Practical Text-to-speech Synthesis System With\n  Multi-guidance Attention And Multi-band Multi-time LPCNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a robust and efficient text-to-speech (TTS) synthesis system\nnamed Triple M is proposed for large-scale online application. The key\ncomponents of Triple M are: 1) A sequence-to-sequence model adopts a novel\nmulti-guidance attention to transfer complementary advantages from guiding\nattention mechanisms to the basic attention mechanism without in-domain\nperformance loss and online service modification. Compared with single\nattention mechanism, multi-guidance attention not only brings better\nnaturalness to long sentence synthesis, but also reduces the word error rate by\n26.8%. 2) A new efficient multi-band multi-time vocoder framework, which\nreduces the computational complexity from 2.8 to 1.0 GFLOP and speeds up LPCNet\nby 2.75x on a single CPU.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 15:38:36 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 13:26:29 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 07:04:35 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 14:54:13 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lin", "Shilun", ""], ["Xie", "Fenglong", ""], ["Meng", "Li", ""], ["Li", "Xinhui", ""], ["Lu", "Li", ""]]}, {"id": "2102.00272", "submitter": "Bhanu Prakash Reddy Guda", "authors": "Bhanu Prakash Reddy Guda, Aparna Garimella and Niyati Chhaya", "title": "EmpathBERT: A BERT-based Framework for Demographic-aware Empathy\n  Prediction", "comments": "Accepted in EACL 2019, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect preferences vary with user demographics, and tapping into demographic\ninformation provides important cues about the users' language preferences. In\nthis paper, we utilize the user demographics, and propose EmpathBERT, a\ndemographic-aware framework for empathy prediction based on BERT. Through\nseveral comparative experiments, we show that EmpathBERT surpasses traditional\nmachine learning and deep learning models, and illustrate the importance of\nuser demographics to predict empathy and distress in user responses to\nstimulative news articles. We also highlight the importance of affect\ninformation in the responses by developing affect-aware models to predict user\ndemographic attributes.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 16:57:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Guda", "Bhanu Prakash Reddy", ""], ["Garimella", "Aparna", ""], ["Chhaya", "Niyati", ""]]}, {"id": "2102.00287", "submitter": "Eva Vanmassenhove", "authors": "Eva Vanmassenhove, Dimitar Shterionov, Matthew Gwilliam", "title": "Machine Translationese: Effects of Algorithmic Bias on Linguistic\n  Complexity in Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies in the field of Machine Translation (MT) and Natural Language\nProcessing (NLP) have shown that existing models amplify biases observed in the\ntraining data. The amplification of biases in language technology has mainly\nbeen examined with respect to specific phenomena, such as gender bias. In this\nwork, we go beyond the study of gender in MT and investigate how bias\namplification might affect language in a broader sense. We hypothesize that the\n'algorithmic bias', i.e. an exacerbation of frequently observed patterns in\ncombination with a loss of less frequent ones, not only exacerbates societal\nbiases present in current datasets but could also lead to an artificially\nimpoverished language: 'machine translationese'. We assess the linguistic\nrichness (on a lexical and morphological level) of translations created by\ndifferent data-driven MT paradigms - phrase-based statistical (PB-SMT) and\nneural MT (NMT). Our experiments show that there is a loss of lexical and\nmorphological richness in the translations produced by all investigated MT\nparadigms for two language pairs (EN<=>FR and EN<=>ES).\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 18:49:11 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Vanmassenhove", "Eva", ""], ["Shterionov", "Dimitar", ""], ["Gwilliam", "Matthew", ""]]}, {"id": "2102.00290", "submitter": "Maur\\'icio Gruppi", "authors": "Maur\\'icio Gruppi, Sibel Adal{\\i}, Pin-Yu Chen", "title": "Fake it Till You Make it: Self-Supervised Semantic Shifts for\n  Monolingual Word Embedding Tasks", "comments": "Published at AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of language is subject to variation over time as well as across\nsocial groups and knowledge domains, leading to differences even in the\nmonolingual scenario. Such variation in word usage is often called lexical\nsemantic change (LSC). The goal of LSC is to characterize and quantify language\nvariations with respect to word meaning, to measure how distinct two language\nsources are (that is, people or language models). Because there is hardly any\ndata available for such a task, most solutions involve unsupervised methods to\nalign two embeddings and predict semantic change with respect to a distance\nmeasure. To that end, we propose a self-supervised approach to model lexical\nsemantic change by generating training samples by introducing perturbations of\nword vectors in the input corpora. We show that our method can be used for the\ndetection of semantic change with any alignment method. Furthermore, it can be\nused to choose the landmark words to use in alignment and can lead to\nsubstantial improvements over the existing techniques for alignment.\n  We illustrate the utility of our techniques using experimental results on\nthree different datasets, involving words with the same or different meanings.\nOur methods not only provide significant improvements but also can lead to\nnovel findings for the LSC problem.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 18:59:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gruppi", "Maur\u00edcio", ""], ["Adal\u0131", "Sibel", ""], ["Chen", "Pin-Yu", ""]]}, {"id": "2102.00291", "submitter": "Wen-Chin Huang", "authors": "Wen-Chin Huang, Chia-Hua Wu, Shang-Bao Luo, Kuan-Yu Chen, Hsin-Min\n  Wang, Tomoki Toda", "title": "Speech Recognition by Simply Fine-tuning BERT", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple method for automatic speech recognition (ASR) by\nfine-tuning BERT, which is a language model (LM) trained on large-scale\nunlabeled text data and can generate rich contextual representations. Our\nassumption is that given a history context sequence, a powerful LM can narrow\nthe range of possible choices and the speech signal can be used as a simple\nclue. Hence, comparing to conventional ASR systems that train a powerful\nacoustic model (AM) from scratch, we believe that speech recognition is\npossible by simply fine-tuning a BERT model. As an initial study, we\ndemonstrate the effectiveness of the proposed idea on the AISHELL dataset and\nshow that stacking a very simple AM on top of BERT can yield reasonable\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 19:06:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Huang", "Wen-Chin", ""], ["Wu", "Chia-Hua", ""], ["Luo", "Shang-Bao", ""], ["Chen", "Kuan-Yu", ""], ["Wang", "Hsin-Min", ""], ["Toda", "Tomoki", ""]]}, {"id": "2102.00299", "submitter": "Jeremy Barnes", "authors": "Jeremy Barnes, Lilja {\\O}vrelid, Erik Velldal", "title": "If you've got it, flaunt it: Making the most of fine-grained sentiment\n  annotations", "comments": "To appear in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-grained sentiment analysis attempts to extract sentiment holders,\ntargets and polar expressions and resolve the relationship between them, but\nprogress has been hampered by the difficulty of annotation. Targeted sentiment\nanalysis, on the other hand, is a more narrow task, focusing on extracting\nsentiment targets and classifying their polarity.In this paper, we explore\nwhether incorporating holder and expression information can improve target\nextraction and classification and perform experiments on eight English\ndatasets. We conclude that jointly predicting target and polarity BIO labels\nimproves target extraction, and that augmenting the input text with gold\nexpressions generally improves targeted polarity classification. This\nhighlights the potential importance of annotating expressions for fine-grained\nsentiment datasets. At the same time, our results show that performance of\ncurrent models for predicting polar expressions is poor, hampering the benefit\nof this information in practice.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 19:47:58 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Barnes", "Jeremy", ""], ["\u00d8vrelid", "Lilja", ""], ["Velldal", "Erik", ""]]}, {"id": "2102.00385", "submitter": "Guangsheng Bao", "authors": "Guangsheng Bao and Yue Zhang", "title": "Contextualized Rewriting for Text Summarization", "comments": null, "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive summarization suffers from irrelevance, redundancy and\nincoherence. Existing work shows that abstractive rewriting for extractive\nsummaries can improve the conciseness and readability. These rewriting systems\nconsider extracted summaries as the only input, which is relatively focused but\ncan lose important background knowledge. In this paper, we investigate\ncontextualized rewriting, which ingests the entire original document. We\nformalize contextualized rewriting as a seq2seq problem with group alignments,\nintroducing group tag as a solution to model the alignments, identifying\nextracted summaries through content-based addressing. Results show that our\napproach significantly outperforms non-contextualized rewriting systems without\nrequiring reinforcement learning, achieving strong improvements on ROUGE scores\nupon multiple extractive summarizers.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 05:35:57 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 06:29:16 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bao", "Guangsheng", ""], ["Zhang", "Yue", ""]]}, {"id": "2102.00395", "submitter": "Behrooz Janfada", "authors": "Majid Asgari-Bidhendi, Behrooz Janfada, Amir Havangi, Sayyed Ali\n  Hossayni, Behrouz Minaei-Bidgoli", "title": "An Unsupervised Language-Independent Entity Disambiguation Method and\n  its Evaluation on the English and Persian Languages", "comments": "arXiv admin note: text overlap with arXiv:2004.10816", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Entity Linking is one of the essential tasks of information extraction and\nnatural language understanding. Entity linking mainly consists of two tasks:\nrecognition and disambiguation of named entities. Most studies address these\ntwo tasks separately or focus only on one of them. Moreover, most of the\nstate-of-the -art entity linking algorithms are either supervised, which have\npoor performance in the absence of annotated corpora or language-dependent,\nwhich are not appropriate for multi-lingual applications. In this paper, we\nintroduce an Unsupervised Language-Independent Entity Disambiguation (ULIED),\nwhich utilizes a novel approach to disambiguate and link named entities.\nEvaluation of ULIED on different English entity linking datasets as well as the\nonly available Persian dataset illustrates that ULIED in most of the cases\noutperforms the state-of-the-art unsupervised multi-lingual approaches.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 06:41:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Asgari-Bidhendi", "Majid", ""], ["Janfada", "Behrooz", ""], ["Havangi", "Amir", ""], ["Hossayni", "Sayyed Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2102.00405", "submitter": "Sagor Sarker", "authors": "Sagor Sarker", "title": "BNLP: Natural language processing toolkit for Bengali language", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BNLP is an open source language processing toolkit for Bengali language\nconsisting with tokenization, word embedding, POS tagging, NER tagging\nfacilities. BNLP provides pre-trained model with high accuracy to do model\nbased tokenization, embedding, POS tagging, NER tagging task for Bengali\nlanguage. BNLP pre-trained model achieves significant results in Bengali text\ntokenization, word embedding, POS tagging and NER tagging task. BNLP is using\nwidely in the Bengali research communities with 16K downloads, 119 stars and 31\nforks. BNLP is available at https://github.com/sagorbrur/bnlp.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 07:56:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sarker", "Sagor", ""]]}, {"id": "2102.00420", "submitter": "Teja Kanchinadam", "authors": "Teja Kanchinadam, Zihang Meng, Joseph Bockhorst, Vikas Singh, Glenn\n  Fung", "title": "Graph Neural Networks to Predict Customer Satisfaction Following\n  Interactions with a Corporate Call Center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer satisfaction is an important factor in creating and maintaining\nlong-term relationships with customers. Near real-time identification of\npotentially dissatisfied customers following phone calls can provide\norganizations the opportunity to take meaningful interventions and to foster\nongoing customer satisfaction and loyalty. This work describes a fully\noperational system we have developed at a large US company for predicting\ncustomer satisfaction following incoming phone calls. The system takes as an\ninput speech-to-text transcriptions of calls and predicts call satisfaction\nreported by customers on post-call surveys (scale from 1 to 10). Because of its\nordinal, subjective, and often highly-skewed nature, predicting survey scores\nis not a trivial task and presents several modeling challenges. We introduce a\ngraph neural network (GNN) approach that takes into account the comparative\nnature of the problem by considering the relative scores among batches, instead\nof only pairs of calls when training. This approach produces more accurate\npredictions than previous approaches including standard regression and\nclassification models that directly fit the survey scores with call data. Our\nproposed approach can be easily generalized to other customer satisfaction\nprediction problems.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:13:57 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kanchinadam", "Teja", ""], ["Meng", "Zihang", ""], ["Bockhorst", "Joseph", ""], ["Singh", "Vikas", ""], ["Fung", "Glenn", ""]]}, {"id": "2102.00424", "submitter": "Alessandro Suglia", "authors": "Alessandro Suglia, Yonatan Bisk, Ioannis Konstas, Antonio Vergari,\n  Emanuele Bastianelli, Andrea Vanzo, Oliver Lemon", "title": "An Empirical Study on the Generalization Power of Neural Representations\n  Learned via Visual Guessing Games", "comments": "Accepted paper for the 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guessing games are a prototypical instance of the \"learning by interacting\"\nparadigm. This work investigates how well an artificial agent can benefit from\nplaying guessing games when later asked to perform on novel NLP downstream\ntasks such as Visual Question Answering (VQA). We propose two ways to exploit\nplaying guessing games: 1) a supervised learning scenario in which the agent\nlearns to mimic successful guessing games and 2) a novel way for an agent to\nplay by itself, called Self-play via Iterated Experience Learning (SPIEL).\n  We evaluate the ability of both procedures to generalize: an in-domain\nevaluation shows an increased accuracy (+7.79) compared with competitors on the\nevaluation suite CompGuessWhat?!; a transfer evaluation shows improved\nperformance for VQA on the TDIUC dataset in terms of harmonic average accuracy\n(+5.31) thanks to more fine-grained object representations learned via SPIEL.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:30:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Suglia", "Alessandro", ""], ["Bisk", "Yonatan", ""], ["Konstas", "Ioannis", ""], ["Vergari", "Antonio", ""], ["Bastianelli", "Emanuele", ""], ["Vanzo", "Andrea", ""], ["Lemon", "Oliver", ""]]}, {"id": "2102.00425", "submitter": "Mark Standke", "authors": "Mark Standke, Abdullah Kiwan, Annalena Lange, Dr. Silvan Berg", "title": "Introduction of a novel word embedding approach based on technology\n  labels extracted from patent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Diversity in patent language is growing and makes finding synonyms for\nconducting patent searches more and more challenging. In addition to that, most\napproaches for dealing with diverse patent language are based on manual search\nand human intuition. In this paper, a word embedding approach using statistical\nanalysis of human labeled data to produce accurate and language independent\nword vectors for technical terms is introduced. This paper focuses on the\nexplanation of the idea behind the statistical analysis and shows first\nqualitative results. The resulting algorithm is a development of the former\nEQMania UG (eqmania.com) and can be tested under eqalice.com until April 2021.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:37:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Standke", "Mark", ""], ["Kiwan", "Abdullah", ""], ["Lange", "Annalena", ""], ["Berg", "Dr. Silvan", ""]]}, {"id": "2102.00461", "submitter": "Mariana Almeida", "authors": "Bruno Jardim and Ricardo Rei and Mariana S. C. Almeida", "title": "Multilingual Email Zoning", "comments": "Accepted at EACL 2021 SRW\n  (https://sites.google.com/view/eaclsrw2021/home); 6 pages with 2 Figures and\n  8 Tables, plus references; Cleverly Multilingual Zoning Corpus available at\n  https://github.com/cleverly-ai/multilingual-email-zoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The segmentation of emails into functional zones (also dubbed email zoning)\nis a relevant preprocessing step for most NLP tasks that deal with emails.\nHowever, despite the multilingual character of emails and their applications,\nprevious literature regarding email zoning corpora and systems was developed\nessentially for English.\n  In this paper, we analyse the existing email zoning corpora and propose a new\nmultilingual benchmark composed of 625 emails in Portuguese, Spanish and\nFrench. Moreover, we introduce OKAPI, the first multilingual email segmentation\nmodel based on a language agnostic sentence encoder. Besides generalizing well\nfor unseen languages, our model is competitive with current English benchmarks,\nand reached new state-of-the-art performances for domain adaptation tasks in\nEnglish.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 14:32:20 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 19:37:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jardim", "Bruno", ""], ["Rei", "Ricardo", ""], ["Almeida", "Mariana S. C.", ""]]}, {"id": "2102.00466", "submitter": "Matthew McDermott", "authors": "Matthew B. A. McDermott, Brendan Yap, Harry Hsu, Di Jin, Peter\n  Szolovits", "title": "Adversarial Contrastive Pre-training for Protein Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in Natural Language Processing (NLP) demonstrate that\nlarge-scale, self-supervised pre-training can be extremely beneficial for\ndownstream tasks. These ideas have been adapted to other domains, including the\nanalysis of the amino acid sequences of proteins. However, to date most\nattempts on protein sequences rely on direct masked language model style\npre-training. In this work, we design a new, adversarial pre-training method\nfor proteins, extending and specializing similar advances in NLP. We show\ncompelling results in comparison to traditional MLM pre-training, though\nfurther development is needed to ensure the gains are worth the significant\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 15:06:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["McDermott", "Matthew B. A.", ""], ["Yap", "Brendan", ""], ["Hsu", "Harry", ""], ["Jin", "Di", ""], ["Szolovits", "Peter", ""]]}, {"id": "2102.00467", "submitter": "Yuan Wu", "authors": "Yuan Wu, Diana Inkpen, Ahmed El-Roby", "title": "Mixup Regularized Adversarial Networks for Multi-Domain Text\n  Classification", "comments": "5 pages, 1 figure, ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the shared-private paradigm and adversarial training has significantly\nimproved the performances of multi-domain text classification (MDTC) models.\nHowever, there are two issues for the existing methods. First, instances from\nthe multiple domains are not sufficient for domain-invariant feature\nextraction. Second, aligning on the marginal distributions may lead to fatal\nmismatching. In this paper, we propose a mixup regularized adversarial network\n(MRAN) to address these two issues. More specifically, the domain and category\nmixup regularizations are introduced to enrich the intrinsic features in the\nshared latent space and enforce consistent predictions in-between training\ninstances such that the learned features can be more domain-invariant and\ndiscriminative. We conduct experiments on two benchmarks: The Amazon review\ndataset and the FDU-MTL dataset. Our approach on these two datasets yields\naverage accuracies of 87.64\\% and 89.0\\% respectively, outperforming all\nrelevant baselines.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 15:24:05 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wu", "Yuan", ""], ["Inkpen", "Diana", ""], ["El-Roby", "Ahmed", ""]]}, {"id": "2102.00472", "submitter": "Boshko Koloski", "authors": "Boshko Koloski and Senja Pollak and Bla\\v{z} \\v{S}krlj and Matej\n  Martinc", "title": "Extending Neural Keyword Extraction with TF-IDF tagset matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyword extraction is the task of identifying words (or multi-word\nexpressions) that best describe a given document and serve in news portals to\nlink articles of similar topics. In this work we develop and evaluate our\nmethods on four novel data sets covering less represented, morphologically-rich\nlanguages in European news media industry (Croatian, Estonian, Latvian and\nRussian). First, we perform evaluation of two supervised neural\ntransformer-based methods (TNT-KID and BERT+BiLSTM CRF) and compare them to a\nbaseline TF-IDF based unsupervised approach. Next, we show that by combining\nthe keywords retrieved by both neural transformer based methods and extending\nthe final set of keywords with an unsupervised TF-IDF based technique, we can\ndrastically improve the recall of the system, making it appropriate to be used\nas a recommendation system in the media house environment.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 15:39:17 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Koloski", "Boshko", ""], ["Pollak", "Senja", ""], ["\u0160krlj", "Bla\u017e", ""], ["Martinc", "Matej", ""]]}, {"id": "2102.00509", "submitter": "Swaprava Nath", "authors": "Ankur Gupta, Yash Varun, Prarthana Das, Nithya Muttineni, Parth\n  Srivastava, Hamim Zafar, Tanmoy Chakraborty, Swaprava Nath", "title": "TruthBot: An Automated Conversational Tool for Intent Learning, Curated\n  Information Presenting, and Fake News Alerting", "comments": "17 pages, 11 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present TruthBot, an all-in-one multilingual conversational chatbot\ndesigned for seeking truth (trustworthy and verified information) on specific\ntopics. It helps users to obtain information specific to certain topics,\nfact-check information, and get recent news. The chatbot learns the intent of a\nquery by training a deep neural network from the data of the previous intents\nand responds appropriately when it classifies the intent in one of the classes\nabove. Each class is implemented as a separate module that uses either its own\ncurated knowledge-base or searches the web to obtain the correct information.\nThe topic of the chatbot is currently set to COVID-19. However, the bot can be\neasily customized to any topic-specific responses. Our experimental results\nshow that each module performs significantly better than its closest\ncompetitor, which is verified both quantitatively and through several\nuser-based surveys in multiple languages. TruthBot has been deployed in June\n2020 and is currently running.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 18:23:05 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gupta", "Ankur", ""], ["Varun", "Yash", ""], ["Das", "Prarthana", ""], ["Muttineni", "Nithya", ""], ["Srivastava", "Parth", ""], ["Zafar", "Hamim", ""], ["Chakraborty", "Tanmoy", ""], ["Nath", "Swaprava", ""]]}, {"id": "2102.00529", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste\n  Alayrac, Aida Nematzadeh", "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal\n  Transformers", "comments": "pre-print of MIT Press Publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently multimodal transformer models have gained popularity because their\nperformance on language and vision tasks suggest they learn rich\nvisual-linguistic representations. Focusing on zero-shot image retrieval tasks,\nwe study three important factors which can impact the quality of learned\nrepresentations: pretraining data, the attention mechanism, and loss functions.\nBy pretraining models on six datasets, we observe that dataset noise and\nlanguage similarity to our downstream task are important indicators of model\nperformance. Through architectural analysis, we learn that models with a\nmultimodal attention mechanism can outperform deeper models with modality\nspecific attention mechanisms. Finally, we show that successful contrastive\nlosses used in the self-supervised learning literature do not yield similar\nperformance gains when used in multimodal transformers\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 20:36:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Mellor", "John", ""], ["Schneider", "Rosalia", ""], ["Alayrac", "Jean-Baptiste", ""], ["Nematzadeh", "Aida", ""]]}, {"id": "2102.00541", "submitter": "Leonid Pugachev", "authors": "Leonid Pugachev, Mikhail Burtsev", "title": "Short Text Clustering with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent techniques for the task of short text clustering often rely on word\nembeddings as a transfer learning component. This paper shows that sentence\nvector representations from Transformers in conjunction with different\nclustering methods can be successfully applied to address the task.\nFurthermore, we demonstrate that the algorithm of enhancement of clustering via\niterative classification can further improve initial clustering performance\nwith different classifiers, including those based on pre-trained Transformer\nlanguage models.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 21:31:11 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Pugachev", "Leonid", ""], ["Burtsev", "Mikhail", ""]]}, {"id": "2102.00583", "submitter": "Besnik Fetahu", "authors": "Lijun Lyu, Maria Koutraki, Martin Krickl, Besnik Fetahu", "title": "Neural OCR Post-Hoc Correction of Historical Corpora", "comments": "To appear at TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Optical character recognition (OCR) is crucial for a deeper access to\nhistorical collections. OCR needs to account for orthographic variations,\ntypefaces, or language evolution (i.e., new letters, word spellings), as the\nmain source of character, word, or word segmentation transcription errors. For\ndigital corpora of historical prints, the errors are further exacerbated due to\nlow scan quality and lack of language standardization.\n  For the task of OCR post-hoc correction, we propose a neural approach based\non a combination of recurrent (RNN) and deep convolutional network (ConvNet) to\ncorrect OCR transcription errors. At character level we flexibly capture\nerrors, and decode the corrected output based on a novel attention mechanism.\nAccounting for the input and output similarity, we propose a new loss function\nthat rewards the model's correcting behavior.\n  Evaluation on a historical book corpus in German language shows that our\nmodels are robust in capturing diverse OCR transcription errors and reduce the\nword error rate of 32.3% by more than 89%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 01:35:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lyu", "Lijun", ""], ["Koutraki", "Maria", ""], ["Krickl", "Martin", ""], ["Fetahu", "Besnik", ""]]}, {"id": "2102.00610", "submitter": "Nathan White", "authors": "Nathan M. White and Timothy Henry-Rodriguez", "title": "The Harrington Yowlumne Narrative Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Minority languages continue to lack adequate resources for their development,\nespecially in the technological domain. Likewise, the J.P. Harrington Papers\ncollection at the Smithsonian Institution are difficult to access in practical\nterms for community members and researchers due to its handwritten and\ndisorganized format. Our current work seeks to make a portion of this\npublicly-available yet problematic material practically accessible for natural\nlanguage processing use. Here, we present the Harrington Yowlumne Narrative\nCorpus, a corpus of 20 narrative texts that derive from the Tejone\\~no Yowlumne\ncommunity of the Tinliw rancheria in Kern County, California between 1910 and\n1925. We digitally transcribe the texts and provide gold-standard aligned\nlexeme-based normalized text with these texts. Altogether, the text contains\n67,835 transcribed characters aligned with 10,721 gold standard text-normalized\nwords.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 03:16:24 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["White", "Nathan M.", ""], ["Henry-Rodriguez", "Timothy", ""]]}, {"id": "2102.00621", "submitter": "Yi Shi", "authors": "Yi Shi and Congyi Wang and Yu Chen and Bin Wang", "title": "Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of Chinese characters are monophonic, while a special group of\ncharacters, called polyphonic characters, have multiple pronunciations. As a\nprerequisite of performing speech-related generative tasks, the correct\npronunciation must be identified among several candidates. This process is\ncalled Polyphone Disambiguation. Although the problem has been well explored\nwith both knowledge-based and learning-based approaches, it remains challenging\ndue to the lack of publicly available labeled datasets and the irregular nature\nof polyphone in Mandarin Chinese. In this paper, we propose a novel\nsemi-supervised learning (SSL) framework for Mandarin Chinese polyphone\ndisambiguation that can potentially leverage unlimited unlabeled text data. We\nexplore the effect of various proxy labeling strategies including\nentropy-thresholding and lexicon-based labeling. Qualitative and quantitative\nexperiments demonstrate that our method achieves state-of-the-art performance.\nIn addition, we publish a novel dataset specifically for the polyphone\ndisambiguation task to promote further researches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 03:47:59 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 06:52:47 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Shi", "Yi", ""], ["Wang", "Congyi", ""], ["Chen", "Yu", ""], ["Wang", "Bin", ""]]}, {"id": "2102.00651", "submitter": "Zhicheng Liang", "authors": "Zhicheng Liang and Deborah L. McGuinness", "title": "Commonsense Knowledge Mining from Term Definitions", "comments": "In the Commonsense Knowledge Graphs (CSKGs) Workshop of the 35th AAAI\n  Conference on Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge has proven to be beneficial to a variety of application\nareas, including question answering and natural language understanding.\nPrevious work explored collecting commonsense knowledge triples automatically\nfrom text to increase the coverage of current commonsense knowledge graphs. We\ninvestigate a few machine learning approaches to mining commonsense knowledge\ntriples using dictionary term definitions as inputs and provide some initial\nevaluation of the results. We start from extracting candidate triples using\npart-of-speech tag patterns from text, and then compare the performance of\nthree existing models for triple scoring. Our experiments show that term\ndefinitions contain some valid and novel commonsense knowledge triples for some\nsemantic relations, and also indicate some challenges with using existing\ntriple scoring models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 05:54:02 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Liang", "Zhicheng", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "2102.00677", "submitter": "Hang Gao", "authors": "Hang Gao, Mengting Hu, Renhong Cheng, Tiegang Gao", "title": "Hierarchical Ranking for Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer selection is a task to choose the positive answers from a pool of\ncandidate answers for a given question. In this paper, we propose a novel\nstrategy for answer selection, called hierarchical ranking. We introduce three\nlevels of ranking: point-level ranking, pair-level ranking, and list-level\nranking. They formulate their optimization objectives by employing supervisory\ninformation from different perspectives to achieve the same goal of ranking\ncandidate answers. Therefore, the three levels of ranking are related and they\ncan promote each other. We take the well-performed compare-aggregate model as\nthe backbone and explore three schemes to implement the idea of applying the\nhierarchical rankings jointly: the scheme under the Multi-Task Learning (MTL)\nstrategy, the Ranking Integration (RI) scheme, and the Progressive Ranking\nIntegration (PRI) scheme. Experimental results on two public datasets, WikiQA\nand TREC-QA, demonstrate that the proposed hierarchical ranking is effective.\nOur method achieves state-of-the-art (non-BERT) performance on both TREC-QA and\nWikiQA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 07:35:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gao", "Hang", ""], ["Hu", "Mengting", ""], ["Cheng", "Renhong", ""], ["Gao", "Tiegang", ""]]}, {"id": "2102.00769", "submitter": "Yukai Shi", "authors": "Yukai Shi, Sen Zhang, Chenxing Zhou, Xiaodan Liang, Xiaojun Yang,\n  Liang Lin", "title": "GTAE: Graph-Transformer based Auto-Encoders for Linguistic-Constrained\n  Text Style Transfer", "comments": "The first two authors share equal-authorship;\n  Code:https://github.com/SenZHANG-GitHub/graph-text-style-transfer ;\n  benchmark: https://github.com/ykshi/text-style-transfer-benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parallel text style transfer has attracted increasing research interests\nin recent years. Despite successes in transferring the style based on the\nencoder-decoder framework, current approaches still lack the ability to\npreserve the content and even logic of original sentences, mainly due to the\nlarge unconstrained model space or too simplified assumptions on latent\nembedding space. Since language itself is an intelligent product of humans with\ncertain grammars and has a limited rule-based model space by its nature,\nrelieving this problem requires reconciling the model capacity of deep neural\nnetworks with the intrinsic model constraints from human linguistic rules. To\nthis end, we propose a method called Graph Transformer based Auto Encoder\n(GTAE), which models a sentence as a linguistic graph and performs feature\nextraction and style transfer at the graph level, to maximally retain the\ncontent and the linguistic structure of original sentences. Quantitative\nexperiment results on three non-parallel text style transfer tasks show that\nour model outperforms state-of-the-art methods in content preservation, while\nachieving comparable performance on transfer accuracy and sentence naturalness.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 11:08:45 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shi", "Yukai", ""], ["Zhang", "Sen", ""], ["Zhou", "Chenxing", ""], ["Liang", "Xiaodan", ""], ["Yang", "Xiaojun", ""], ["Lin", "Liang", ""]]}, {"id": "2102.00781", "submitter": "Sandeep Mathias", "authors": "Rahul Kumar, Sandeep Mathias, Sriparna Saha, Pushpak Bhattacharyya", "title": "Many Hands Make Light Work: Using Essay Traits to Automatically Score\n  Essays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research in the area of automatic essay grading (AEG) is geared towards\nscoring the essay holistically while there has also been some work done on\nscoring individual essay traits. In this paper, we describe a way to score\nessays holistically using a multi-task learning (MTL) approach, where scoring\nthe essay holistically is the primary task, and scoring the essay traits is the\nauxiliary task. We compare our results with a single-task learning (STL)\napproach, using both LSTMs and BiLSTMs. We also compare our results of the\nauxiliary task with such tasks done in other AEG systems. To find out which\ntraits work best for different types of essays, we conduct ablation tests for\neach of the essay traits. We also report the runtime and number of training\nparameters for each system. We find that MTL-based BiLSTM system gives the best\nresults for scoring the essay holistically, as well as performing well on\nscoring the essay traits.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 11:31:09 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kumar", "Rahul", ""], ["Mathias", "Sandeep", ""], ["Saha", "Sriparna", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2102.00804", "submitter": "Ayush Kumar", "authors": "Mukuntha Narayanan Sundararaman, Ayush Kumar, Jithendra Vepa", "title": "Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR\n  Transcript", "comments": "Accepted to Interspeech 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed significant improvement in ASR systems to\nrecognize spoken utterances. However, it is still a challenging task for noisy\nand out-of-domain data, where substitution and deletion errors are prevalent in\nthe transcribed text. These errors significantly degrade the performance of\ndownstream tasks. In this work, we propose a BERT-style language model,\nreferred to as PhonemeBERT, that learns a joint language model with phoneme\nsequence and ASR transcript to learn phonetic-aware representations that are\nrobust to ASR errors. We show that PhonemeBERT can be used on downstream tasks\nusing phoneme sequences as additional features, and also in low-resource setup\nwhere we only have ASR-transcripts for the downstream tasks with no phoneme\ninformation available. We evaluate our approach extensively by generating noisy\ndata for three benchmark datasets - Stanford Sentiment Treebank, TREC and ATIS\nfor sentiment, question and intent classification tasks respectively. The\nresults of the proposed approach beats the state-of-the-art baselines\ncomprehensively on each dataset.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 12:45:15 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 18:19:02 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Sundararaman", "Mukuntha Narayanan", ""], ["Kumar", "Ayush", ""], ["Vepa", "Jithendra", ""]]}, {"id": "2102.00816", "submitter": "Mingxi Cheng", "authors": "Mingxi Cheng, Shahin Nazarian, Paul Bogdan", "title": "VRoC: Variational Autoencoder-aided Multi-task Rumor Classifier Based on\n  Text", "comments": "Proceedings of The Web Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media became popular and percolated almost all aspects of our daily\nlives. While online posting proves very convenient for individual users, it\nalso fosters fast-spreading of various rumors. The rapid and wide percolation\nof rumors can cause persistent adverse or detrimental impacts. Therefore,\nresearchers invest great efforts on reducing the negative impacts of rumors.\nTowards this end, the rumor classification system aims to detect, track, and\nverify rumors in social media. Such systems typically include four components:\n(i) a rumor detector, (ii) a rumor tracker, (iii) a stance classifier, and (iv)\na veracity classifier. In order to improve the state-of-the-art in rumor\ndetection, tracking, and verification, we propose VRoC, a tweet-level\nvariational autoencoder-based rumor classification system. VRoC consists of a\nco-train engine that trains variational autoencoders (VAEs) and rumor\nclassification components. The co-train engine helps the VAEs to tune their\nlatent representations to be classifier-friendly. We also show that VRoC is\nable to classify unseen rumors with high levels of accuracy. For the PHEME\ndataset, VRoC consistently outperforms several state-of-the-art techniques, on\nboth observed and unobserved rumors, by up to 26.9%, in terms of macro-F1\nscores.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:26:57 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Cheng", "Mingxi", ""], ["Nazarian", "Shahin", ""], ["Bogdan", "Paul", ""]]}, {"id": "2102.00819", "submitter": "Lya Hulliyyatus Suadaa", "authors": "Lya Hulliyyatus Suadaa, Hidetaka Kamigaito, Manabu Okumura, Hiroya\n  Takamura", "title": "Metric-Type Identification for Multi-Level Header Numerical Tables in\n  Scientific Papers", "comments": "To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerical tables are widely used to present experimental results in\nscientific papers. For table understanding, a metric-type is essential to\ndiscriminate numbers in the tables. We introduce a new information extraction\ntask, metric-type identification from multi-level header numerical tables, and\nprovide a dataset extracted from scientific papers consisting of header tables,\ncaptions, and metric-types. We then propose two joint-learning neural\nclassification and generation schemes featuring pointer-generator-based and\nBERT-based models. Our results show that the joint models can handle both\nin-header and out-of-header metric-type identification problems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:09:36 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Suadaa", "Lya Hulliyyatus", ""], ["Kamigaito", "Hidetaka", ""], ["Okumura", "Manabu", ""], ["Takamura", "Hiroya", ""]]}, {"id": "2102.00838", "submitter": "Rafael Angarita", "authors": "Shufan Jiang (CRESTIC, ISEP), Rafael Angarita (ISEP), Stephane Cormier\n  (CRESTIC), Francis Rousseaux (CRESTIC)", "title": "Fine-tuning BERT-based models for Plant Health Bulletin Classification", "comments": null, "journal-ref": "Technology and Environment Workshop'21, Jan 2021, Montpellier,\n  France", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of digitization, different actors in agriculture produce numerous\ndata. Such data contains already latent historical knowledge in the domain.\nThis knowledge enables us to precisely study natural hazards within global or\nlocal aspects, and then improve the risk prevention tasks and augment the\nyield, which helps to tackle the challenge of growing population and changing\nalimentary habits. In particular, French Plants Health Bulletins (BSV, for its\nname in French Bulletin de Sant{\\'e} du V{\\'e}g{\\'e}tal) give information about\nthe development stages of phytosanitary risks in agricultural production.\nHowever, they are written in natural language, thus, machines and human cannot\nexploit them as efficiently as it could be. Natural language processing (NLP)\ntechnologies aim to automatically process and analyze large amounts of natural\nlanguage data. Since the 2010s, with the increases in computational power and\nparallelization, representation learning and deep learning methods became\nwidespread in NLP. Recent advancements Bidirectional Encoder Representations\nfrom Transformers (BERT) inspire us to rethink of knowledge representation and\nnatural language understanding in plant health management domain. The goal in\nthis work is to propose a BERT-based approach to automatically classify the BSV\nto make their data easily indexable. We sampled 200 BSV to finetune the\npretrained BERT language models and classify them as pest or/and disease and we\nshow preliminary results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:14:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jiang", "Shufan", "", "CRESTIC, ISEP"], ["Angarita", "Rafael", "", "ISEP"], ["Cormier", "Stephane", "", "CRESTIC"], ["Rousseaux", "Francis", "", "CRESTIC"]]}, {"id": "2102.00845", "submitter": "Takashi Oya", "authors": "Takashi Oya and Shigeo Morishima", "title": "LSTM-SAKT: LSTM-Encoded SAKT-like Transformer for Knowledge Tracing", "comments": "4 pages, 3 figures, the paper at AAAI 2021 Workshop on AI Education\n  https://sites.google.com/view/tipce-2021/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the 2nd place solution for the Riiid! Answer\nCorrectness Prediction in Kaggle, the world's largest data science competition\nwebsite. This competition was held from October 16, 2020, to January 7, 2021,\nwith 3395 teams and 4387 competitors. The main insights and contributions of\nthis paper are as follows. (i) We pointed out existing Transformer-based models\nare suffering from a problem that the information which their query/key/value\ncan contain is limited. To solve this problem, we proposed a method that uses\nLSTM to obtain query/key/value and verified its effectiveness. (ii) We pointed\nout 'inter-container' leakage problem, which happens in datasets where\nquestions are sometimes served together. To solve this problem, we showed\nspecial indexing/masking techniques that are useful when using RNN-variants and\nTransformer. (iii) We found additional hand-crafted features are effective to\novercome the limits of Transformer, which can never consider the samples older\nthan the sequence length.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:21:46 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:46:53 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Oya", "Takashi", ""], ["Morishima", "Shigeo", ""]]}, {"id": "2102.00875", "submitter": "Agrin Aram Hilmkil", "authors": "Agrin Hilmkil and Sebastian Callh and Matteo Barbieri and Leon Ren\\'e\n  S\\\"utfeld and Edvin Listo Zec and Olof Mogren", "title": "Scaling Federated Learning for Fine-tuning of Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising approach to distributed compute, as\nwell as distributed data, and provides a level of privacy and compliance to\nlegal frameworks. This makes FL attractive for both consumer and healthcare\napplications. While the area is actively being explored, few studies have\nexamined FL in the context of larger language models and there is a lack of\ncomprehensive reviews of robustness across tasks, architectures, numbers of\nclients, and other relevant factors. In this paper, we explore the fine-tuning\nof Transformer-based language models in a federated learning setting. We\nevaluate three popular BERT-variants of different sizes (BERT, ALBERT, and\nDistilBERT) on a number of text classification tasks such as sentiment analysis\nand author identification. We perform an extensive sweep over the number of\nclients, ranging up to 32, to evaluate the impact of distributed compute on\ntask performance in the federated averaging setting. While our findings suggest\nthat the large sizes of the evaluated models are not generally prohibitive to\nfederated training, we found that the different models handle federated\naveraging to a varying degree. Most notably, DistilBERT converges significantly\nslower with larger numbers of clients, and under some circumstances, even\ncollapses to chance level performance. Investigating this issue presents an\ninteresting perspective for future research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 14:31:39 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hilmkil", "Agrin", ""], ["Callh", "Sebastian", ""], ["Barbieri", "Matteo", ""], ["S\u00fctfeld", "Leon Ren\u00e9", ""], ["Zec", "Edvin Listo", ""], ["Mogren", "Olof", ""]]}, {"id": "2102.00881", "submitter": "G\\\"ul\\c{s}en Eryi\\u{g}it", "authors": "G\\\"ul\\c{s}en Eryi\\u{g}it, Ali \\c{S}enta\\c{s}, Johanna Monti", "title": "Gamified Crowdsourcing for Idiom Corpora Construction", "comments": "25 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning idiomatic expressions is seen as one of the most challenging stages\nin second language learning because of their unpredictable meaning. A similar\nsituation holds for their identification within natural language processing\napplications such as machine translation and parsing. The lack of high-quality\nusage samples exacerbates this challenge not only for humans but also for\nartificial intelligence systems. This article introduces a gamified\ncrowdsourcing approach for collecting language learning materials for idiomatic\nexpressions; a messaging bot is designed as an asynchronous multiplayer game\nfor native speakers who compete with each other while providing idiomatic and\nnonidiomatic usage examples and rating other players' entries. As opposed to\nclassical crowdprocessing annotation efforts in the field, for the first time\nin the literature, a crowdcreating & crowdrating approach is implemented and\ntested for idiom corpora construction. The approach is language independent and\nevaluated on two languages in comparison to traditional data preparation\ntechniques in the field. The reaction of the crowd is monitored under different\nmotivational means (namely, gamification affordances and monetary rewards). The\nresults reveal that the proposed approach is powerful in collecting the\ntargeted materials, and although being an explicit crowdsourcing approach, it\nis found entertaining and useful by the crowd. The approach has been shown to\nhave the potential to speed up the construction of idiom corpora for different\nnatural languages to be used as second language learning material, training\ndata for supervised idiom identification systems, or samples for lexicographic\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 14:44:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Eryi\u011fit", "G\u00fcl\u015fen", ""], ["\u015eenta\u015f", "Ali", ""], ["Monti", "Johanna", ""]]}, {"id": "2102.00894", "submitter": "Nora Kassner", "authors": "Nora Kassner, Philipp Dufter, Hinrich Sch\\\"utze", "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained\n  Language Models", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, it has been found that monolingual English language models can be\nused as knowledge bases. Instead of structural knowledge base queries, masked\nsentences such as \"Paris is the capital of [MASK]\" are used as probes. We\ntranslate the established benchmarks TREx and GoogleRE into 53 languages.\nWorking with mBERT, we investigate three questions. (i) Can mBERT be used as a\nmultilingual knowledge base? Most prior work only considers English. Extending\nresearch to multiple languages is important for diversity and accessibility.\n(ii) Is mBERT's performance as knowledge base language-independent or does it\nvary from language to language? (iii) A multilingual model is trained on more\ntext, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for\nbetter performance? We find that using mBERT as a knowledge base yields varying\nperformance across languages and pooling predictions across languages improves\nperformance. Conversely, mBERT exhibits a language bias; e.g., when queried in\nItalian, it tends to predict Italy as the country of origin.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:07:06 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kassner", "Nora", ""], ["Dufter", "Philipp", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2102.00904", "submitter": "Augusto Camargo", "authors": "Augusto Camargo, Wesley Carvalho, Felipe Peressim, Alan Barzilay,\n  Marcelo Finger", "title": "Text-to-hashtag Generation using Seq2seq Learning", "comments": null, "journal-ref": null, "doi": "10.5753/bresci.2021.15797", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we studied whether models based on BiLSTM and BERT can predict\nhashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a\nsizable financial impact on Ecommerce. We processed a corpus of Ecommerce\nreviews as inputs, and predicted hashtags as outputs. We evaluated the results\nusing four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A\nword cloud was used as a qualitative metric. While all computer-generated\nmetrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results\nproduced amazing scores. We concluded that the texts predicted by the neural\nnetworks are very promising for use as hashtags for products on Ecommerce\nwebsites. The code for this work is available at\nhttps://github.com/augustocamargo/text-to-hashtag.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:28:27 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 21:48:23 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Camargo", "Augusto", ""], ["Carvalho", "Wesley", ""], ["Peressim", "Felipe", ""], ["Barzilay", "Alan", ""], ["Finger", "Marcelo", ""]]}, {"id": "2102.00912", "submitter": "Zahraa Abdallah Dr", "authors": "Sean-Kelly Palicki, Shereen Fouad, Mariam Adedoyin-Olowe, Zahraa S.\n  Abdallah", "title": "Transfer Learning Approach for Detecting Psychological Distress in\n  Brexit Tweets", "comments": "SAC 2021, MLA - Machine Learning and its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2016, United Kingdom (UK) citizens voted to leave the European Union (EU),\nwhich was officially implemented in 2020. During this period, UK residents\nexperienced a great deal of uncertainty around the UK's continued relationship\nwith the EU. Many people have used social media platforms to express their\nemotions about this critical event. Sentiment analysis has been recently\nconsidered as an important tool for detecting mental well-being in Twitter\ncontents. However, detecting the psychological distress status in\npolitical-related tweets is a challenging task due to the lack of explicit\nsentences describing the depressive or anxiety status. To address this problem,\nthis paper leverages a transfer learning approach for sentiment analysis to\nmeasure the non-clinical psychological distress status in Brexit tweets. The\nframework transfers the knowledge learnt from self-reported psychological\ndistress tweets (source domain) to detect the distress status in Brexit tweets\n(target domain). The framework applies a domain adaptation technique to\ndecrease the impact of negative transfer between source and target domains. The\npaper also introduces a Brexit distress index that can be used to detect levels\nof psychological distress of individuals in Brexit tweets. We design an\nexperiment that includes data from both domains. The proposed model is able to\ndetect the non-clinical psychological distress status in Brexit tweets with an\naccuracy of 66% and 62% on the source and target domains, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:54:37 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Palicki", "Sean-Kelly", ""], ["Fouad", "Shereen", ""], ["Adedoyin-Olowe", "Mariam", ""], ["Abdallah", "Zahraa S.", ""]]}, {"id": "2102.00917", "submitter": "L Nathan Perkins", "authors": "Tommy Leung, L. Nathan Perkins", "title": "Counting Protests in News Articles: A Dataset and Semi-Automated Data\n  Collection Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Between January 2017 and January 2021, thousands of local news sources in the\nUnited States reported on over 42,000 protests about topics such as civil\nrights, immigration, guns, and the environment. Given the vast number of local\njournalists that report on protests daily, extracting these events as\nstructured data to understand temporal and geographic trends can empower civic\ndecision-making. However, the task of extracting events from news articles\npresents well known challenges to the NLP community in the fields of domain\ndetection, slot filling, and coreference resolution.\n  To help improve the resources available for extracting structured data from\nnews stories, our contribution is three-fold. We 1) release a manually labeled\ndataset of news article URLs, dates, locations, crowd size estimates, and 494\ndiscrete descriptive tags corresponding to 42,347 reported protest events in\nthe United States between January 2017 and January 2021; 2) describe the\nsemi-automated data collection pipeline used to discover, sort, and review the\n144,568 English articles that comprise the dataset; and 3) benchmark a\nlong-short term memory (LSTM) low dimensional classifier that demonstrates the\nutility of processing news articles based on syntactic structures, such as\nparagraphs and sentences, to count the number of reported protest events.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:35:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Leung", "Tommy", ""], ["Perkins", "L. Nathan", ""]]}, {"id": "2102.00924", "submitter": "Yida Xin", "authors": "Yida Xin, Henry Lieberman and Peter Chin", "title": "Revisiting the Prepositional-Phrase Attachment Problem Using Explicit\n  Commonsense Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the challenging problem of resolving prepositional-phrase (PP)\nattachment ambiguity. To date, proposed solutions are either rule-based, where\nexplicit grammar rules direct how to resolve ambiguities; or statistical, where\nthe decision is learned from a corpus of labeled examples. We argue that\nexplicit commonsense knowledge bases can provide an essential ingredient for\nmaking good attachment decisions. We implemented a module, named Patch-Comm,\nthat can be used by a variety of conventional parsers, to make attachment\ndecisions. Where the commonsense KB does not provide direct answers, we fall\nback on a more general system that infers \"out-of-knowledge-base\" assertions in\na manner similar to the way some NLP systems handle out-of-vocabulary words.\nOur results suggest that the commonsense knowledge-based approach can provide\nthe best of both worlds, integrating rule-based and statistical techniques. As\nthe field is increasingly coming to recognize the importance of explainability\nin AI, a commonsense approach can enable NLP developers to better understand\nthe behavior of systems, and facilitate natural dialogues with end users.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:48:36 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:51:30 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Xin", "Yida", ""], ["Lieberman", "Henry", ""], ["Chin", "Peter", ""]]}, {"id": "2102.01013", "submitter": "Valentin Pelloin", "authors": "Valentin Pelloin, Nathalie Camelin, Antoine Laurent, Renato De Mori,\n  Antoine Caubri\\`ere, Yannick Est\\`eve, Sylvain Meignier", "title": "End2End Acoustic to Semantic Transduction", "comments": "Accepted at IEEE ICASSP 2021", "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP39728.2021.9413581", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end sequence-to-sequence spoken\nlanguage understanding model using an attention mechanism. It reliably selects\ncontextual acoustic features in order to hypothesize semantic contents. An\ninitial architecture capable of extracting all pronounced words and concepts\nfrom acoustic spans is designed and tested. With a shallow fusion language\nmodel, this system reaches a 13.6 concept error rate (CER) and an 18.5 concept\nvalue error rate (CVER) on the French MEDIA corpus, achieving an absolute 2.8\npoints reduction compared to the state-of-the-art. Then, an original model is\nproposed for hypothesizing concepts and their values. This transduction reaches\na 15.4 CER and a 21.6 CVER without any new type of context.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 17:42:59 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Pelloin", "Valentin", ""], ["Camelin", "Nathalie", ""], ["Laurent", "Antoine", ""], ["De Mori", "Renato", ""], ["Caubri\u00e8re", "Antoine", ""], ["Est\u00e8ve", "Yannick", ""], ["Meignier", "Sylvain", ""]]}, {"id": "2102.01017", "submitter": "Yanai Elazar", "authors": "Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander,\n  Eduard Hovy, Hinrich Sch\\\"utze, Yoav Goldberg", "title": "Measuring and Improving Consistency in Pretrained Language Models", "comments": "Accepted to the TACL journal, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency of a model -- that is, the invariance of its behavior under\nmeaning-preserving alternations in its input -- is a highly desirable property\nin natural language processing. In this paper we study the question: Are\nPretrained Language Models (PLMs) consistent with respect to factual knowledge?\nTo this end, we create ParaRel, a high-quality resource of cloze-style query\nEnglish paraphrases. It contains a total of 328 paraphrases for 38 relations.\nUsing ParaRel, we show that the consistency of all PLMs we experiment with is\npoor -- though with high variance between relations. Our analysis of the\nrepresentational spaces of PLMs suggests that they have a poor structure and\nare currently not suitable for representing knowledge robustly. Finally, we\npropose a method for improving model consistency and experimentally demonstrate\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 17:48:42 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 12:50:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Elazar", "Yanai", ""], ["Kassner", "Nora", ""], ["Ravfogel", "Shauli", ""], ["Ravichander", "Abhilasha", ""], ["Hovy", "Eduard", ""], ["Sch\u00fctze", "Hinrich", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2102.01051", "submitter": "Akshat Gupta", "authors": "Sai Muralidhar Jayanthi, Akshat Gupta", "title": "SJ_AJ@DravidianLangTech-EACL2021: Task-Adaptive Pre-Training of\n  Multilingual BERT models for Offensive Language Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our submission for the EACL 2021-Shared Task on\nOffensive Language Identification in Dravidian languages. Our final system is\nan ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive\npre-training of multilingual BERT models with a masked language modeling\nobjective. Our system was ranked 1st for Kannada, 2nd for Malayalam and 3rd for\nTamil.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:41:56 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 15:27:59 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Jayanthi", "Sai Muralidhar", ""], ["Gupta", "Akshat", ""]]}, {"id": "2102.01065", "submitter": "Nelson F. Liu", "authors": "Nelson F. Liu and Tony Lee and Robin Jia and Percy Liang", "title": "Can Small and Synthetic Benchmarks Drive Modeling Innovation? A\n  Retrospective Study of Question Answering Modeling Approaches", "comments": "40 pages, 13 figures; preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets are not only resources for training accurate, deployable systems,\nbut are also benchmarks for developing new modeling approaches. While large,\nnatural datasets are necessary for training accurate systems, are they\nnecessary for driving modeling innovation? For example, while the popular SQuAD\nquestion answering benchmark has driven the development of new modeling\napproaches, could synthetic or smaller benchmarks have led to similar\ninnovations?\n  This counterfactual question is impossible to answer, but we can study a\nnecessary condition: the ability for a benchmark to recapitulate findings made\non SQuAD. We conduct a retrospective study of 20 SQuAD modeling approaches,\ninvestigating how well 32 existing and synthesized benchmarks concur with SQuAD\n-- i.e., do they rank the approaches similarly? We carefully construct small,\ntargeted synthetic benchmarks that do not resemble natural language, yet have\nhigh concurrence with SQuAD, demonstrating that naturalness and size are not\nnecessary for reflecting historical modeling improvements on SQuAD. Our results\nraise the intriguing possibility that small and carefully designed synthetic\nbenchmarks may be useful for driving the development of new modeling\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:55:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Liu", "Nelson F.", ""], ["Lee", "Tony", ""], ["Jia", "Robin", ""], ["Liang", "Percy", ""]]}, {"id": "2102.01106", "submitter": "Yunlong Jiao", "authors": "Yunlong Jiao, Adam Gabrys, Georgi Tinchev, Bartosz Putrycz, Daniel\n  Korzekwa, Viacheslav Klimkov", "title": "Universal Neural Vocoding with Parallel WaveNet", "comments": "5 pages, 2 figures. Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a universal neural vocoder based on Parallel WaveNet, with an\nadditional conditioning network called Audio Encoder. Our universal vocoder\noffers real-time high-quality speech synthesis on a wide range of use cases. We\ntested it on 43 internal speakers of diverse age and gender, speaking 20\nlanguages in 17 unique styles, of which 7 voices and 5 styles were not exposed\nduring training. We show that the proposed universal vocoder significantly\noutperforms speaker-dependent vocoders overall. We also show that the proposed\nvocoder outperforms several existing neural vocoder architectures in terms of\nnaturalness and universality. These findings are consistent when we further\ntest on more than 300 open-source voices.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 19:03:27 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:18:31 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jiao", "Yunlong", ""], ["Gabrys", "Adam", ""], ["Tinchev", "Georgi", ""], ["Putrycz", "Bartosz", ""], ["Korzekwa", "Daniel", ""], ["Klimkov", "Viacheslav", ""]]}, {"id": "2102.01156", "submitter": "Despina Christou", "authors": "Despina Christou, Grigorios Tsoumakas", "title": "Improving Distantly-Supervised Relation Extraction through BERT-based\n  Label & Instance Embeddings", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distantly-supervised relation extraction (RE) is an effective method to scale\nRE to large corpora but suffers from noisy labels. Existing approaches try to\nalleviate noise through multi-instance learning and by providing additional\ninformation, but manage to recognize mainly the top frequent relations,\nneglecting those in the long-tail. We propose REDSandT (Relation Extraction\nwith Distant Supervision and Transformers), a novel distantly-supervised\ntransformer-based RE method, that manages to capture a wider set of relations\nthrough highly informative instance and label embeddings for RE, by exploiting\nBERT's pre-trained model, and the relationship between labels and entities,\nrespectively. We guide REDSandT to focus solely on relational tokens by\nfine-tuning BERT on a structured input, including the sub-tree connecting an\nentity pair and the entities' types. Using the extracted informative vectors,\nwe shape label embeddings, which we also use as attention mechanism over\ninstances to further reduce noise. Finally, we represent sentences by\nconcatenating relation and instance embeddings. Experiments in the NYT-10\ndataset show that REDSandT captures a broader set of relations with higher\nconfidence, achieving state-of-the-art AUC (0.424).\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:50:24 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Christou", "Despina", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2102.01192", "submitter": "Kushal Lakhotia", "authors": "Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam\n  Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman\n  Mohamed, Emmanuel Dupoux", "title": "Generative Spoken Language Modeling from Raw Audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative spoken language modeling involves learning jointly the acoustic\nand linguistic characteristics of a language from raw audio only (without text\nor labels). We introduce metrics to automatically evaluate the generated output\nin terms of acoustic and linguistic quality in two associated end-to-end tasks,\nrespectively: speech resynthesis (repeating the speech input using the system's\nown voice), and speech generation (producing novel speech outputs conditional\non a spoken prompt, or unconditionally), and validate these metrics with human\njudgment. We test baseline systems consisting of a discrete speech encoder\n(returning discrete, low bitrate, pseudo-text units), a generative language\nmodel (trained on pseudo-text units), and a speech decoder (generating a\nwaveform from pseudo-text). By comparing three state-of-the-art unsupervised\nspeech encoders (Contrastive Predictive Coding (CPC), wav2vec 2.0, HuBERT), and\nvarying the number of discrete units (50, 100, 200), we investigate how the\ngenerative performance depends on the quality of the learned units as measured\nby unsupervised metrics (zero-shot probe tasks). We will open source our\nevaluation stack and baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 21:41:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lakhotia", "Kushal", ""], ["Kharitonov", "Evgeny", ""], ["Hsu", "Wei-Ning", ""], ["Adi", "Yossi", ""], ["Polyak", "Adam", ""], ["Bolte", "Benjamin", ""], ["Nguyen", "Tu-Anh", ""], ["Copet", "Jade", ""], ["Baevski", "Alexei", ""], ["Mohamed", "Adelrahman", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2102.01222", "submitter": "Kaushik Roy", "authors": "Kaushik Roy, Usha Lokala, Vedant Khandelwal, and Amit Sheth", "title": "\"Is depression related to cannabis?\": A knowledge-infused model for\n  Entity and Relation Extraction with Limited Supervision", "comments": "Accepted to AAAI-2021 Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With strong marketing advocacy of the benefits of cannabis use for improved\nmental health, cannabis legalization is a priority among legislators. However,\npreliminary scientific research does not conclusively associate cannabis with\nimproved mental health. In this study, we explore the relationship between\ndepression and consumption of cannabis in a targeted social media corpus\ninvolving personal use of cannabis with the intent to derive its potential\nmental health benefit. We use tweets that contain an association among three\ncategories annotated by domain experts - Reason, Effect, and Addiction. The\nstate-of-the-art Natural Langauge Processing techniques fall short in\nextracting these relationships between cannabis phrases and the depression\nindicators. We seek to address the limitation by using domain knowledge;\nspecifically, the Drug Abuse Ontology for addiction augmented with Diagnostic\nand Statistical Manual of Mental Disorders lexicons for mental health. Because\nof the lack of annotations due to the limited availability of the domain\nexperts' time, we use supervised contrastive learning in conjunction with GPT-3\ntrained on a vast corpus to achieve improved performance even with limited\nsupervision. Experimental results show that our method can significantly\nextract cannabis-depression relationships better than the state-of-the-art\nrelation extractor. High-quality annotations can be provided using a nearest\nneighbor approach using the learned representations that can be used by the\nscientific community to understand the association between cannabis and\ndepression better.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 23:02:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Roy", "Kaushik", ""], ["Lokala", "Usha", ""], ["Khandelwal", "Vedant", ""], ["Sheth", "Amit", ""]]}, {"id": "2102.01223", "submitter": "Melika Behjati", "authors": "Melika Behjati and James Henderson", "title": "Inducing Meaningful Units from Character Sequences with Slot Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characters do not convey meaning, but sequences of characters do. We propose\nan unsupervised distributional method to learn the abstract meaning-bearing\nunits in a sequence of characters. Rather than segmenting the sequence, this\nmodel discovers continuous representations of the \"objects\" in the sequence,\nusing a recently proposed architecture for object discovery in images called\nSlot Attention. We train our model on different languages and evaluate the\nquality of the obtained representations with probing classifiers. Our\nexperiments show promising results in the ability of our units to capture\nmeaning at a higher level of abstraction.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 23:11:57 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Behjati", "Melika", ""], ["Henderson", "James", ""]]}, {"id": "2102.01226", "submitter": "Kai Sun", "authors": "Dian Yu, Kai Sun, Dong Yu, Claire Cardie", "title": "Self-Teaching Machines to Read and Comprehend with Large-Scale\n  Multi-Subject Question-Answering Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of much recent research in the area, it is still unclear whether\nsubject-area question-answering data is useful for machine reading\ncomprehension (MRC) tasks. In this paper, we investigate this question. We\ncollect a large-scale multi-subject multiple-choice question-answering dataset,\nExamQA, and use incomplete and noisy snippets returned by a web search engine\nas the relevant context for each question-answering instance to convert it into\na weakly-labeled MRC instance. We then propose a self-teaching paradigm to\nbetter use the generated weakly-labeled MRC instances to improve a target MRC\ntask. Experimental results show that we can obtain +5.1% in accuracy on a\nmultiple-choice MRC dataset, C^3, and +3.8% in exact match on an extractive MRC\ndataset, CMRC 2018 over state-of-the-art MRC baselines, demonstrating the\neffectiveness of our framework and the usefulness of large-scale subject-area\nquestion-answering data for different types of machine reading comprehension\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 23:18:58 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:22:55 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yu", "Dian", ""], ["Sun", "Kai", ""], ["Yu", "Dong", ""], ["Cardie", "Claire", ""]]}, {"id": "2102.01260", "submitter": "Xiong Liu", "authors": "Xiong Liu, Craig E. Thomas, Christian C. Felder", "title": "The impact of external innovation on new drug approvals: A retrospective\n  analysis", "comments": null, "journal-ref": "International Journal of Pharmaceutics, Volume 563, Pages 273-281,\n  2019", "doi": "10.1016/j.ijpharm.2018.12.093", "report-no": "PMID: 30664998", "categories": "cs.CL cs.CY q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pharmaceutical companies are relying more often on external sources of\ninnovation to boost their discovery research productivity. However, more\nin-depth knowledge about how external innovation may translate to successful\nproduct launches is still required in order to better understand how to best\nleverage the innovation ecosystem. We analyzed the pre-approval publication\nhistories for FDA-approved new molecular entities (NMEs) and new biologic\nentities (NBEs) launched by 13 top research pharma companies during the last\ndecade (2006-2016). We found that academic institutions contributed the\nmajority of pre-approval publications and that publication subject matter is\nclosely aligned with the strengths of the respective innovator. We found this\nto also be true for candidate drugs terminated in Phase 3, but the volume of\nliterature on these molecules is substantially less than for approved drugs.\nThis may suggest that approved drugs are often associated with a more robust\ndataset provided by a large number of institutes. Collectively, the results of\nour analysis support the hypothesis that a collaborative research innovation\nenvironment spanning across academia, industry and government is highly\nconducive to successful drug approvals.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 02:21:34 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Liu", "Xiong", ""], ["Thomas", "Craig E.", ""], ["Felder", "Christian C.", ""]]}, {"id": "2102.01263", "submitter": "Yao Dou", "authors": "Yao Dou, Maxwell Forbes, Ari Holtzman, Yejin Choi", "title": "MultiTalk: A Highly-Branching Dialog Testbed for Diverse Conversations", "comments": "7 pages, AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study conversational dialog in which there are many possible responses to\na given history. We present the MultiTalk Dataset, a corpus of over 320,000\nsentences of written conversational dialog that balances a high branching\nfactor (10) with several conversation turns (6) through selective branch\ncontinuation. We make multiple contributions to study dialog generation in the\nhighly branching setting. In order to evaluate a diverse set of generations, we\npropose a simple scoring algorithm, based on bipartite graph matching, to\noptimally incorporate a set of diverse references. We study multiple language\ngeneration tasks at different levels of predictive conversation depth, using\ntextual attributes induced automatically from pretrained classifiers. Our\nculminating task is a challenging theory of mind problem, a controllable\ngeneration task which requires reasoning about the expected reaction of the\nlistener.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 02:29:40 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Dou", "Yao", ""], ["Forbes", "Maxwell", ""], ["Holtzman", "Ari", ""], ["Choi", "Yejin", ""]]}, {"id": "2102.01335", "submitter": "Kenton Lee", "authors": "Kenton Lee, Kelvin Guu, Luheng He, Tim Dozat, Hyung Won Chung", "title": "Neural Data Augmentation via Example Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of machine learning, certain categories of examples may\nbe underrepresented in the training data, causing systems to underperform on\nsuch \"few-shot\" cases at test time. A common remedy is to perform data\naugmentation, such as by duplicating underrepresented examples, or\nheuristically synthesizing new examples. But these remedies often fail to cover\nthe full diversity and complexity of real examples.\n  We propose a data augmentation approach that performs neural Example\nExtrapolation (Ex2). Given a handful of exemplars sampled from some\ndistribution, Ex2 synthesizes new examples that also belong to the same\ndistribution. The Ex2 model is learned by simulating the example generation\nprocedure on data-rich slices of the data, and it is applied to\nunderrepresented, few-shot slices.\n  We apply Ex2 to a range of language understanding tasks and significantly\nimprove over state-of-the-art methods on multiple few-shot learning benchmarks,\nincluding for relation extraction (FewRel) and intent classification + slot\nfilling (SNIPS).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:20:19 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lee", "Kenton", ""], ["Guu", "Kelvin", ""], ["He", "Luheng", ""], ["Dozat", "Tim", ""], ["Chung", "Hyung Won", ""]]}, {"id": "2102.01363", "submitter": "Shota Horiguchi", "authors": "Shota Horiguchi, Nelson Yalta, Paola Garcia, Yuki Takashima, Yawen\n  Xue, Desh Raj, Zili Huang, Yusuke Fujita, Shinji Watanabe, Sanjeev Khudanpur", "title": "The Hitachi-JHU DIHARD III System: Competitive End-to-End Neural\n  Diarization and X-Vector Clustering Systems Combined by DOVER-Lap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a detailed description of the Hitachi-JHU system that was\nsubmitted to the Third DIHARD Speech Diarization Challenge. The system outputs\nthe ensemble results of the five subsystems: two x-vector-based subsystems, two\nend-to-end neural diarization-based subsystems, and one hybrid subsystem. We\nrefine each system and all five subsystems become competitive and\ncomplementary. After the DOVER-Lap based system combination, it achieved\ndiarization error rates of 11.58 % and 14.09 % in Track 1 full and core, and\n16.94 % and 20.01 % in Track 2 full and core, respectively. With their results,\nwe won second place in all the tasks of the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:30:44 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Horiguchi", "Shota", ""], ["Yalta", "Nelson", ""], ["Garcia", "Paola", ""], ["Takashima", "Yuki", ""], ["Xue", "Yawen", ""], ["Raj", "Desh", ""], ["Huang", "Zili", ""], ["Fujita", "Yusuke", ""], ["Watanabe", "Shinji", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2102.01373", "submitter": "Wenxuan Zhou", "authors": "Wenxuan Zhou, Muhao Chen", "title": "An Improved Baseline for Sentence-level Relation Extraction", "comments": "Code available at https://github.com/wzhouad/RE_improved_baseline", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence-level relation extraction (RE) aims at identifying the relationship\nbetween two entities in a sentence. Many efforts have been devoted to this\nproblem, while the best performing methods are still far from perfect. In this\npaper, we revisit two problems that affect the performance of existing RE\nmodels, namely entity representation and noisy or ill-defined labels. Our\nimproved baseline model, incorporated with entity representations with typed\nmarkers, achieves an F1 of 74.6% on TACRED, significantly outperforms previous\nSOTA methods. Furthermore, the presented new baseline achieves an F1 of 91.1%\non the refined Re-TACRED dataset, demonstrating that the pre-trained language\nmodels achieve unexpectedly high performance on this task. We release our code\nto the community for future research.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:57:06 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 20:02:47 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 22:38:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhou", "Wenxuan", ""], ["Chen", "Muhao", ""]]}, {"id": "2102.01380", "submitter": "Zhong Meng", "authors": "Zhong Meng, Naoyuki Kanda, Yashesh Gaur, Sarangarajan Parthasarathy,\n  Eric Sun, Liang Lu, Xie Chen, Jinyu Li, Yifan Gong", "title": "Internal Language Model Training for Domain-Adaptive End-to-End Speech\n  Recognition", "comments": "5 pages, ICASSP 2021", "journal-ref": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), Toronto, Canada", "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The efficacy of external language model (LM) integration with existing\nend-to-end (E2E) automatic speech recognition (ASR) systems can be improved\nsignificantly using the internal language model estimation (ILME) method. In\nthis method, the internal LM score is subtracted from the score obtained by\ninterpolating the E2E score with the external LM score, during inference. To\nimprove the ILME-based inference, we propose an internal LM training (ILMT)\nmethod to minimize an additional internal LM loss by updating only the E2E\nmodel components that affect the internal LM estimation. ILMT encourages the\nE2E model to form a standalone LM inside its existing components, without\nsacrificing ASR accuracy. After ILMT, the more modular E2E model with matched\ntraining and inference criteria enables a more thorough elimination of the\nsource-domain internal LM, and therefore leads to a more effective integration\nof the target-domain external LM. Experimented with 30K-hour trained recurrent\nneural network transducer and attention-based encoder-decoder models, ILMT with\nILME-based inference achieves up to 31.5% and 11.4% relative word error rate\nreductions from standard E2E training with Shallow Fusion on out-of-domain\nLibriSpeech and in-domain Microsoft production test sets, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 08:15:02 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 19:16:04 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Meng", "Zhong", ""], ["Kanda", "Naoyuki", ""], ["Gaur", "Yashesh", ""], ["Parthasarathy", "Sarangarajan", ""], ["Sun", "Eric", ""], ["Lu", "Liang", ""], ["Chen", "Xie", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2102.01417", "submitter": "Miguel Domingo", "authors": "Miguel Domingo and Francisco Casacuberta", "title": "Two Demonstrations of the Machine Translation Applications to Historical\n  Documents", "comments": "Presented at the Demos session of ICPR 2020:\n  https://www.micc.unifi.it/icpr2020/index.php/demos/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present our demonstration of two machine translation applications to\nhistorical documents. The first task consists in generating a new version of a\nhistorical document, written in the modern version of its original language.\nThe second application is limited to a document's orthography. It adapts the\ndocument's spelling to modern standards in order to achieve an orthography\nconsistency and accounting for the lack of spelling conventions. We followed an\ninteractive, adaptive framework that allows the user to introduce corrections\nto the system's hypothesis. The system reacts to these corrections by\ngenerating a new hypothesis that takes them into account. Once the user is\nsatisfied with the system's hypothesis and validates it, the system adapts its\nmodel following an online learning strategy. This system is implemented\nfollowing a client-server architecture. We developed a website which\ncommunicates with the neural models. All code is open-source and publicly\navailable. The demonstration is hosted at http://demosmt.prhlt.upv.es/mthd/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 10:28:31 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Domingo", "Miguel", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "2102.01454", "submitter": "Krishna Pillutla", "authors": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun,\n  Sean Welleck, Yejin Choi, Zaid Harchaoui", "title": "An Information Divergence Measure Between Neural Text and Human Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As major progress is made in open-ended text generation, measuring how close\nmachine-generated text is to human language remains a critical open problem. We\npropose Mauve, a comparison measure for open-ended text generation, which\ndirectly compares a generation model's distribution to that of human-written\ntext. Mauve measures the mean area under a divergence curve for the two\ndistributions, exploring the trade-off between two types of errors: those\narising from parts of the human distribution that the model distribution\napproximates well, and those it does not. Mauve extends a family of information\ndivergence metrics, introducing a tractable approximation based on computing\nthe KL divergence in a quantized embedding space. This yields an efficient\nimplementation that scales up to modern text generation models. Through an\nextensive empirical study on three open-ended generation tasks, we find that\nMauve identifies known properties of generated text, scales naturally with\nmodel size, and correlates with human judgments, with fewer restrictions than\nexisting distributional evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:59:28 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 19:33:41 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pillutla", "Krishna", ""], ["Swayamdipta", "Swabha", ""], ["Zellers", "Rowan", ""], ["Thickstun", "John", ""], ["Welleck", "Sean", ""], ["Choi", "Yejin", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "2102.01497", "submitter": "Muhammad Noor Fakhruzzaman", "authors": "Muhammad N. Fakhruzzaman, Saidah Z. Jannah, Ratih A. Ningrum, Indah\n  Fahmiyah", "title": "Clickbait Headline Detection in Indonesian News Sites using Multilingual\n  Bidirectional Encoder Representations from Transformers (M-BERT)", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Click counts are related to the amount of money that online advertisers paid\nto news sites. Such business models forced some news sites to employ a dirty\ntrick of click-baiting, i.e., using a hyperbolic and interesting words,\nsometimes unfinished sentence in a headline to purposefully tease the readers.\nSome Indonesian online news sites also joined the party of clickbait, which\nindirectly degrade other established news sites' credibility. A neural network\nwith a pre-trained language model M-BERT that acted as a embedding layer is\nthen combined with a 100 nodes hidden layer and topped with a sigmoid\nclassifier was trained to detect clickbait headlines. With a total of 6632\nheadlines as a training dataset, the classifier performed remarkably well.\nEvaluated with 5-fold cross validation, it has an accuracy score of 0.914,\nf1-score of 0.914, precision score of 0.916, and ROC-AUC of 0.92. The usage of\nmultilingual BERT in Indonesian text classification task was tested and is\npossible to be enhanced further. Future possibilities, societal impact, and\nlimitations of the clickbait detection are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 14:13:02 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Fakhruzzaman", "Muhammad N.", ""], ["Jannah", "Saidah Z.", ""], ["Ningrum", "Ratih A.", ""], ["Fahmiyah", "Indah", ""]]}, {"id": "2102.01502", "submitter": "Satyapriya Krishna", "authors": "Satyapriya Krishna, Rahul Gupta, Christophe Dupuy", "title": "ADePT: Auto-encoder based Differentially Private Text Transformation", "comments": null, "journal-ref": "The 16th conference of the European Chapter of the Association for\n  Computational Linguistics (EACL), 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy is an important concern when building statistical models on data\ncontaining personal information. Differential privacy offers a strong\ndefinition of privacy and can be used to solve several privacy concerns (Dwork\net al., 2014). Multiple solutions have been proposed for the\ndifferentially-private transformation of datasets containing sensitive\ninformation. However, such transformation algorithms offer poor utility in\nNatural Language Processing (NLP) tasks due to noise added in the process. In\nthis paper, we address this issue by providing a utility-preserving\ndifferentially private text transformation algorithm using auto-encoders. Our\nalgorithm transforms text to offer robustness against attacks and produces\ntransformations with high semantic quality that perform well on downstream NLP\ntasks. We prove the theoretical privacy guarantee of our algorithm and assess\nits privacy leakage under Membership Inference Attacks(MIA) (Shokri et al.,\n2017) on models trained with transformed data. Our results show that the\nproposed model performs better against MIA attacks while offering lower to no\ndegradation in the utility of the underlying transformation process compared to\nexisting baselines.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 23:15:24 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Krishna", "Satyapriya", ""], ["Gupta", "Rahul", ""], ["Dupuy", "Christophe", ""]]}, {"id": "2102.01547", "submitter": "Binbin Zhang", "authors": "Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang,\n  Zhendong Peng, Xiaoyu Chen, Lei Xie, Xin Lei", "title": "WeNet: Production oriented Streaming and Non-streaming End-to-End Speech\n  Recognition Toolkit", "comments": "5 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an open source, production first, and production\nready speech recognition toolkit called WeNet in which a new two-pass approach\nis implemented to unify streaming and non-streaming end-to-end (E2E) speech\nrecognition in a single model. The main motivation of WeNet is to close the gap\nbetween the research and the production of E2E speechrecognition models. WeNet\nprovides an efficient way to ship ASR applications in several real-world\nscenarios, which is the main difference and advantage to other open source E2E\nspeech recognition toolkits. In our toolkit, a new two-pass method is\nimplemented. Our method propose a dynamic chunk-based attention strategy of the\nthe transformer layers to allow arbitrary right context length modifies in\nhybrid CTC/attention architecture. The inference latency could be easily\ncontrolled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. Our experiments on\nthe AISHELL-1 dataset using WeNet show that, our model achieves 5.03\\% relative\ncharacter error rate (CER) reduction in non-streaming ASR compared to a\nstandard non-streaming transformer. After model quantification, our model\nperform reasonable RTF and latency.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:19:41 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 03:49:11 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 13:31:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yao", "Zhuoyuan", ""], ["Wu", "Di", ""], ["Wang", "Xiong", ""], ["Zhang", "Binbin", ""], ["Yu", "Fan", ""], ["Yang", "Chao", ""], ["Peng", "Zhendong", ""], ["Chen", "Xiaoyu", ""], ["Xie", "Lei", ""], ["Lei", "Xin", ""]]}, {"id": "2102.01563", "submitter": "Zhuang Li", "authors": "Shuo Huang, Zhuang Li, Lizhen Qu, Lei Pan", "title": "On Robustness of Neural Semantic Parsers", "comments": "Long Paper, Accepted to EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic parsing maps natural language (NL) utterances into logical forms\n(LFs), which underpins many advanced NLP problems. Semantic parsers gain\nperformance boosts with deep neural networks, but inherit vulnerabilities\nagainst adversarial examples. In this paper, we provide the empirical study on\nthe robustness of semantic parsers in the presence of adversarial attacks.\nFormally, adversaries of semantic parsing are considered to be the perturbed\nutterance-LF pairs, whose utterances have exactly the same meanings as the\noriginal ones. A scalable methodology is proposed to construct robustness test\nsets based on existing benchmark corpora. Our results answered five research\nquestions in measuring the sate-of-the-art parsers' performance on robustness\ntest sets, and evaluating the effect of data augmentation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:41:28 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 12:19:10 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Huang", "Shuo", ""], ["Li", "Zhuang", ""], ["Qu", "Lizhen", ""], ["Pan", "Lei", ""]]}, {"id": "2102.01578", "submitter": "Marco Gaido", "authors": "Marco Gaido, Mauro Cettolo, Matteo Negri, Marco Turchi", "title": "CTC-based Compression for Direct Speech Translation", "comments": "Accepted at EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Previous studies demonstrated that a dynamic phone-informed compression of\nthe input audio is beneficial for speech translation (ST). However, they\nrequired a dedicated model for phone recognition and did not test this solution\nfor direct ST, in which a single model translates the input audio into the\ntarget language without intermediate representations. In this work, we propose\nthe first method able to perform a dynamic compression of the input indirect ST\nmodels. In particular, we exploit the Connectionist Temporal Classification\n(CTC) to compress the input sequence according to its phonetic characteristics.\nOur experiments demonstrate that our solution brings a 1.3-1.5 BLEU improvement\nover a strong baseline on two language pairs (English-Italian and\nEnglish-German), contextually reducing the memory footprint by more than 10%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:09:19 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gaido", "Marco", ""], ["Cettolo", "Mauro", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2102.01640", "submitter": "Debasish Mohapatra", "authors": "Pramit Saha, Debasish Ray Mohapatra, Sidney Fels", "title": "SPEAK WITH YOUR HANDS Using Continuous Hand Gestures to control\n  Articulatory Speech Synthesizer", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents our advancements in controlling an articulatory speech\nsynthesis engine, \\textit{viz.}, Pink Trombone, with hand gestures. Our\ninterface translates continuous finger movements and wrist flexion into\ncontinuous speech using vocal tract area-function based articulatory speech\nsynthesis. We use Cyberglove II with 18 sensors to capture the kinematic\ninformation of the wrist and the individual fingers, in order to control a\nvirtual tongue. The coordinates and the bending values of the sensors are then\nutilized to fit a spline tongue model that smoothens out the noisy values and\noutliers. Considering the upper palate as fixed and the spline model as the\ndynamically moving lower surface (tongue) of the vocal tract, we compute 1D\narea functional values that are fed to the Pink Trombone, generating continuous\nspeech sounds. Therefore, by learning to manipulate one's wrist and fingers,\none can learn to produce speech sounds just through one's hands, without the\nneed for using the vocal tract.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:49:51 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Saha", "Pramit", ""], ["Mohapatra", "Debasish Ray", ""], ["Fels", "Sidney", ""]]}, {"id": "2102.01672", "submitter": "Sebastian Gehrmann", "authors": "Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka\n  Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu,\n  Miruna Clinciu, Dipanjan Das, Kaustubh D. Dhole, Wanyu Du, Esin Durmus,\n  Ond\\v{r}ej Du\\v{s}ek, Chris Emezue, Varun Gangal, Cristina Garbacea,\n  Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji,\n  Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica\n  Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro\n  Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg,\n  Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Rubungo Andre Niyongabo,\n  Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao,\n  Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo\\~ao Sedoc, Thibault\n  Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla\n  Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila\n  Yerukola, Jiawei Zhou", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce GEM, a living benchmark for natural language Generation (NLG),\nits Evaluation, and Metrics. Measuring progress in NLG relies on a constantly\nevolving ecosystem of automated metrics, datasets, and human evaluation\nstandards. Due to this moving target, new models often still evaluate on\ndivergent anglo-centric corpora with well-established, but flawed, metrics.\nThis disconnect makes it challenging to identify the limitations of current\nmodels and opportunities for progress. Addressing this limitation, GEM provides\nan environment in which models can easily be applied to a wide set of tasks and\nin which evaluation strategies can be tested. Regular updates to the benchmark\nwill help NLG research become more multilingual and evolve the challenge\nalongside models. This paper serves as the description of the data for which we\nare organizing a shared task at our ACL 2021 Workshop and to which we invite\nthe entire NLG community to participate.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:42:05 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 18:09:36 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 17:42:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Adewumi", "Tosin", ""], ["Aggarwal", "Karmanya", ""], ["Ammanamanchi", "Pawan Sasanka", ""], ["Anuoluwapo", "Aremu", ""], ["Bosselut", "Antoine", ""], ["Chandu", "Khyathi Raghavi", ""], ["Clinciu", "Miruna", ""], ["Das", "Dipanjan", ""], ["Dhole", "Kaustubh D.", ""], ["Du", "Wanyu", ""], ["Durmus", "Esin", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Emezue", "Chris", ""], ["Gangal", "Varun", ""], ["Garbacea", "Cristina", ""], ["Hashimoto", "Tatsunori", ""], ["Hou", "Yufang", ""], ["Jernite", "Yacine", ""], ["Jhamtani", "Harsh", ""], ["Ji", "Yangfeng", ""], ["Jolly", "Shailza", ""], ["Kale", "Mihir", ""], ["Kumar", "Dhruv", ""], ["Ladhak", "Faisal", ""], ["Madaan", "Aman", ""], ["Maddela", "Mounica", ""], ["Mahajan", "Khyati", ""], ["Mahamood", "Saad", ""], ["Majumder", "Bodhisattwa Prasad", ""], ["Martins", "Pedro Henrique", ""], ["McMillan-Major", "Angelina", ""], ["Mille", "Simon", ""], ["van Miltenburg", "Emiel", ""], ["Nadeem", "Moin", ""], ["Narayan", "Shashi", ""], ["Nikolaev", "Vitaly", ""], ["Niyongabo", "Rubungo Andre", ""], ["Osei", "Salomey", ""], ["Parikh", "Ankur", ""], ["Perez-Beltrachini", "Laura", ""], ["Rao", "Niranjan Ramesh", ""], ["Raunak", "Vikas", ""], ["Rodriguez", "Juan Diego", ""], ["Santhanam", "Sashank", ""], ["Sedoc", "Jo\u00e3o", ""], ["Sellam", "Thibault", ""], ["Shaikh", "Samira", ""], ["Shimorina", "Anastasia", ""], ["Cabezudo", "Marco Antonio Sobrevilla", ""], ["Strobelt", "Hendrik", ""], ["Subramani", "Nishant", ""], ["Xu", "Wei", ""], ["Yang", "Diyi", ""], ["Yerukola", "Akhila", ""], ["Zhou", "Jiawei", ""]]}, {"id": "2102.01692", "submitter": "Marvin Coto Mr.", "authors": "Ana Lilia Alvarez-Blanco, Eugenia Cordoba-Warner, Marvin Coto-Jimenez,\n  Vivian Fallas-Lopez, Maribel Morales Rodriguez", "title": "Generacion de voces artificiales infantiles en castellano con acento\n  costarricense", "comments": "12 pages, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article evaluates a first experience of generating artificial children's\nvoices with a Costa Rican accent, using the technique of statistical parametric\nspeech synthesis based on Hidden Markov Models. The process of recording the\nvoice samples used for learning the models, the fundamentals of the technique\nused and the subjective evaluation of the results through the perception of a\ngroup of people is described. The results show that the intelligibility of the\nresults, evaluated in isolated words, is lower than the voices recorded by the\ngroup of participating children. Similarly, the detection of the age and gender\nof the speaking person is significantly affected in artificial voices, relative\nto recordings of natural voices. These results show the need to obtain larger\namounts of data, in addition to becoming a numerical reference for future\ndevelopments resulting from new data or from processes to improve results in\nthe same technique.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 02:12:28 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Alvarez-Blanco", "Ana Lilia", ""], ["Cordoba-Warner", "Eugenia", ""], ["Coto-Jimenez", "Marvin", ""], ["Fallas-Lopez", "Vivian", ""], ["Rodriguez", "Maribel Morales", ""]]}, {"id": "2102.01754", "submitter": "Weiquan Fan", "authors": "Weiquan Fan, Xiangmin Xu, Xiaofen Xing, Weidong Chen, Dongyan Huang", "title": "LSSED: a large-scale dataset and benchmark for speech emotion\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech emotion recognition is a vital contributor to the next generation of\nhuman-computer interaction (HCI). However, current existing small-scale\ndatabases have limited the development of related research. In this paper, we\npresent LSSED, a challenging large-scale english speech emotion dataset, which\nhas data collected from 820 subjects to simulate real-world distribution. In\naddition, we release some pre-trained models based on LSSED, which can not only\npromote the development of speech emotion recognition, but can also be\ntransferred to related downstream tasks such as mental health analysis where\ndata is extremely difficult to collect. Finally, our experiments show the\nnecessity of large-scale datasets and the effectiveness of pre-trained models.\nThe dateset will be released on https://github.com/tobefans/LSSED.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 11:15:32 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fan", "Weiquan", ""], ["Xu", "Xiangmin", ""], ["Xing", "Xiaofen", ""], ["Chen", "Weidong", ""], ["Huang", "Dongyan", ""]]}, {"id": "2102.01757", "submitter": "Elizabeth Salesky", "authors": "Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni,\n  Matteo Negri, Marco Turchi, Douglas W. Oard, Matt Post", "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Multilingual TEDx corpus, built to support speech recognition\n(ASR) and speech translation (ST) research across many non-English source\nlanguages. The corpus is a collection of audio recordings from TEDx talks in 8\nsource languages. We segment transcripts into sentences and align them to the\nsource-language audio and target-language translations. The corpus is released\nalong with open-sourced code enabling extension to new talks and languages as\nthey become available. Our corpus creation methodology can be applied to more\nlanguages than previous work, and creates multi-way parallel evaluation sets.\nWe provide baselines in multiple ASR and ST settings, including multilingual\nmodels to improve translation performance for low-resource language pairs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 21:16:25 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 03:36:12 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Salesky", "Elizabeth", ""], ["Wiesner", "Matthew", ""], ["Bremerman", "Jacob", ""], ["Cattoni", "Roldano", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""], ["Oard", "Douglas W.", ""], ["Post", "Matt", ""]]}, {"id": "2102.01818", "submitter": "Karin Verspoor", "authors": "Aparna Elangovan, Jiayuan He, Karin Verspoor", "title": "Memorization vs. Generalization: Quantifying Data Leakage in NLP\n  Performance Evaluation", "comments": "To appear EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public datasets are often used to evaluate the efficacy and generalizability\nof state-of-the-art methods for many tasks in natural language processing\n(NLP). However, the presence of overlap between the train and test datasets can\nlead to inflated results, inadvertently evaluating the model's ability to\nmemorize and interpreting it as the ability to generalize. In addition, such\ndata sets may not provide an effective indicator of the performance of these\nmethods in real world scenarios. We identify leakage of training data into test\ndata on several publicly available datasets used to evaluate NLP tasks,\nincluding named entity recognition and relation extraction, and study them to\nassess the impact of that leakage on the model's ability to memorize versus\ngeneralize.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 00:58:45 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Elangovan", "Aparna", ""], ["He", "Jiayuan", ""], ["Verspoor", "Karin", ""]]}, {"id": "2102.01826", "submitter": "Zhewei Sun", "authors": "Zhewei Sun, Richard Zemel, Yang Xu", "title": "A Computational Framework for Slang Generation", "comments": "Accepted for publication in TACL 2021. Author's final version", "journal-ref": "Transactions of the Association for Computational Linguistics\n  2021; 9 462-478", "doi": "10.1162/tacl_a_00378", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slang is a common type of informal language, but its flexible nature and\npaucity of data resources present challenges for existing natural language\nsystems. We take an initial step toward machine generation of slang by\ndeveloping a framework that models the speaker's word choice in slang context.\nOur framework encodes novel slang meaning by relating the conventional and\nslang senses of a word while incorporating syntactic and contextual knowledge\nin slang usage. We construct the framework using a combination of probabilistic\ninference and neural contrastive learning. We perform rigorous evaluations on\nthree slang dictionaries and show that our approach not only outperforms\nstate-of-the-art language models, but also better predicts the historical\nemergence of slang word usages from 1960s to 2000s. We interpret the proposed\nmodels and find that the contrastively learned semantic space is sensitive to\nthe similarities between slang and conventional senses of words. Our work\ncreates opportunities for the automated generation and interpretation of\ninformal language.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 01:19:07 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 04:46:48 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sun", "Zhewei", ""], ["Zemel", "Richard", ""], ["Xu", "Yang", ""]]}, {"id": "2102.01847", "submitter": "Yasufumi Taniguchi", "authors": "Yasufumi Taniguchi, Hiroki Nakayama, Kubo Takahiro, Jun Suzuki", "title": "An Investigation Between Schema Linking and Text-to-SQL Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-SQL is a crucial task toward developing methods for understanding\nnatural language by computers. Recent neural approaches deliver excellent\nperformance; however, models that are difficult to interpret inhibit future\ndevelopments. Hence, this study aims to provide a better approach toward the\ninterpretation of neural models. We hypothesize that the internal behavior of\nmodels at hand becomes much easier to analyze if we identify the detailed\nperformance of schema linking simultaneously as the additional information of\nthe text-to-SQL performance. We provide the ground-truth annotation of schema\nlinking information onto the Spider dataset. We demonstrate the usefulness of\nthe annotated data and how to analyze the current state-of-the-art neural\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 02:50:10 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Taniguchi", "Yasufumi", ""], ["Nakayama", "Hiroki", ""], ["Takahiro", "Kubo", ""], ["Suzuki", "Jun", ""]]}, {"id": "2102.01860", "submitter": "An Yan", "authors": "An Yan, Xin Eric Wang, Tsu-Jui Fu, William Yang Wang", "title": "L2C: Describing Visual Differences Needs Semantic Understanding of\n  Individuals", "comments": "EACL-2021 short", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in language and vision push forward the research of\ncaptioning a single image to describing visual differences between image pairs.\nSuppose there are two images, I_1 and I_2, and the task is to generate a\ndescription W_{1,2} comparing them, existing methods directly model { I_1, I_2\n} -> W_{1,2} mapping without the semantic understanding of individuals. In this\npaper, we introduce a Learning-to-Compare (L2C) model, which learns to\nunderstand the semantic structures of these two images and compare them while\nlearning to describe each one. We demonstrate that L2C benefits from a\ncomparison between explicit semantic representations and single-image captions,\nand generalizes better on the new testing image pairs. It outperforms the\nbaseline on both automatic evaluation and human evaluation for the\nBirds-to-Words dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:44:42 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Yan", "An", ""], ["Wang", "Xin Eric", ""], ["Fu", "Tsu-Jui", ""], ["Wang", "William Yang", ""]]}, {"id": "2102.01909", "submitter": "Inbal Yahav", "authors": "Avihay Chriqui, Inbal Yahav", "title": "HeBERT & HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis\n  and Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces HeBERT and HebEMO. HeBERT is a Transformer-based model\nfor modern Hebrew text, which relies on a BERT (Bidirectional Encoder\nRepresentations for Transformers) architecture. BERT has been shown to\noutperform alternative architectures in sentiment analysis, and is suggested to\nbe particularly appropriate for MRLs. Analyzing multiple BERT specifications,\nwe find that while model complexity correlates with high performance on\nlanguage tasks that aim to understand terms in a sentence, a more-parsimonious\nmodel better captures the sentiment of entire sentence. Either way, out\nBERT-based language model outperforms all existing Hebrew alternatives on all\ncommon language tasks. HebEMO is a tool that uses HeBERT to detect polarity and\nextract emotions from Hebrew UGC. HebEMO is trained on a unique\nCovid-19-related UGC dataset that we collected and annotated for this study.\nData collection and annotation followed an active learning procedure that aimed\nto maximize predictability. We show that HebEMO yields a high F1-score of 0.96\nfor polarity classification. Emotion detection reaches F1-scores of 0.78-0.97\nfor various target emotions, with the exception of surprise, which the model\nfailed to capture (F1 = 0.41). These results are better than the best-reported\nperformance, even among English-language models of emotion detection.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:59:59 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 07:43:43 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 07:04:34 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chriqui", "Avihay", ""], ["Yahav", "Inbal", ""]]}, {"id": "2102.01951", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang\n  Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume,\n  Sebastian Ruder, Dani Yogatama, Kris Cao, Tomas Kocisky, Susannah Young, Phil\n  Blunsom", "title": "Pitfalls of Static Language Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our world is open-ended, non-stationary and constantly evolving; thus what we\ntalk about and how we talk about it changes over time. This inherent dynamic\nnature of language comes in stark contrast to the current static language\nmodelling paradigm, which constructs training and evaluation sets from\noverlapping time periods. Despite recent progress, we demonstrate that\nstate-of-the-art Transformer models perform worse in the realistic setup of\npredicting future utterances from beyond their training period -- a consistent\npattern across three datasets from two domains. We find that, while increasing\nmodel size alone -- a key driver behind recent progress -- does not provide a\nsolution for the temporal generalization problem, having models that\ncontinually update their knowledge with new information can indeed slow down\nthe degradation over time. Hence, given the compilation of ever-larger language\nmodelling training datasets, combined with the growing list of\nlanguage-model-based NLP applications that require up-to-date knowledge about\nthe world, we argue that now is the right time to rethink our static language\nmodelling evaluation protocol, and develop adaptive language models that can\nremain up-to-date with respect to our ever-changing and non-stationary world.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:01:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Kuncoro", "Adhiguna", ""], ["Gribovskaya", "Elena", ""], ["Agrawal", "Devang", ""], ["Liska", "Adam", ""], ["Terzi", "Tayfun", ""], ["Gimenez", "Mai", ""], ["d'Autume", "Cyprien de Masson", ""], ["Ruder", "Sebastian", ""], ["Yogatama", "Dani", ""], ["Cao", "Kris", ""], ["Kocisky", "Tomas", ""], ["Young", "Susannah", ""], ["Blunsom", "Phil", ""]]}, {"id": "2102.02080", "submitter": "Fajri Koto", "authors": "Fajri Koto and Jey Han Lau and Timothy Baldwin", "title": "Top-down Discourse Parsing via Sequence Labelling", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a top-down approach to discourse parsing that is conceptually\nsimpler than its predecessors (Kobayashi et al., 2020; Zhang et al., 2020). By\nframing the task as a sequence labelling problem where the goal is to\niteratively segment a document into individual discourse units, we are able to\neliminate the decoder and reduce the search space for splitting points. We\nexplore both traditional recurrent models and modern pre-trained transformer\nmodels for the task, and additionally introduce a novel dynamic oracle for\ntop-down parsing. Based on the Full metric, our proposed LSTM model sets a new\nstate-of-the-art for RST parsing.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:30:21 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 20:32:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Koto", "Fajri", ""], ["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""]]}, {"id": "2102.02096", "submitter": "Siqi Bao", "authors": "Huang He, Hua Lu, Siqi Bao, Fan Wang, Hua Wu, Zhengyu Niu, Haifeng\n  Wang", "title": "Learning to Select External Knowledge with Multi-Scale Negative Sampling", "comments": "To be presented at AAAI-21 DSTC9 Workshop. First two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Track-1 of DSTC9 aims to effectively answer user requests or questions\nduring task-oriented dialogues, which are out of the scope of APIs/DB. By\nleveraging external knowledge resources, relevant information can be retrieved\nand encoded into the response generation for these out-of-API-coverage queries.\nIn this work, we have explored several advanced techniques to enhance the\nutilization of external knowledge and boost the quality of response generation,\nincluding schema guided knowledge decision, negatives enhanced knowledge\nselection, and knowledge grounded response generation. To evaluate the\nperformance of our proposed method, comprehensive experiments have been carried\nout on the publicly available dataset. Our approach was ranked as the best in\nhuman evaluation of DSTC9 Track-1.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:59:35 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["He", "Huang", ""], ["Lu", "Hua", ""], ["Bao", "Siqi", ""], ["Wang", "Fan", ""], ["Wu", "Hua", ""], ["Niu", "Zhengyu", ""], ["Wang", "Haifeng", ""]]}, {"id": "2102.02110", "submitter": "Maximin Coavoux", "authors": "Maximin Coavoux, Shay B. Cohen", "title": "Learning to Match Mathematical Statements with Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel task consisting in assigning a proof to a given\nmathematical statement. The task is designed to improve the processing of\nresearch-level mathematical texts. Applying Natural Language Processing (NLP)\ntools to research level mathematical articles is both challenging, since it is\na highly specialized domain which mixes natural language and mathematical\nformulae. It is also an important requirement for developing tools for\nmathematical information retrieval and computer-assisted theorem proving. We\nrelease a dataset for the task, consisting of over 180k statement-proof pairs\nextracted from mathematical research articles. We carry out preliminary\nexperiments to assess the difficulty of the task. We first experiment with two\nbag-of-words baselines. We show that considering the assignment problem\nglobally and using weighted bipartite matching algorithms helps a lot in\ntackling the task. Finally, we introduce a self-attention-based model that can\nbe trained either locally or globally and outperforms baselines by a wide\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 15:38:54 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Coavoux", "Maximin", ""], ["Cohen", "Shay B.", ""]]}, {"id": "2102.02111", "submitter": "Sandra Wankm\\\"uller", "authors": "Sandra Wankm\\\"uller", "title": "Neural Transfer Learning with Transformers for Social Science Text\n  Analysis", "comments": "67 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last years, there have been substantial increases in the\nprediction performances of natural language processing models on text-based\nsupervised learning tasks. Especially deep learning models that are based on\nthe Transformer architecture (Vaswani et al., 2017) and are used in a transfer\nlearning setting have contributed to this development. As Transformer-based\nmodels for transfer learning have the potential to achieve higher prediction\naccuracies with relatively few training data instances, they are likely to\nbenefit social scientists that seek to have as accurate as possible text-based\nmeasures but only have limited resources for annotating training data. To\nenable social scientists to leverage these potential benefits for their\nresearch, this paper explains how these methods work, why they might be\nadvantageous, and what their limitations are. Additionally, three\nTransformer-based models for transfer learning, BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), and the Longformer (Beltagy et al., 2020), are\ncompared to conventional machine learning algorithms on three social science\napplications. Across all evaluated tasks, textual styles, and training data set\nsizes, the conventional models are consistently outperformed by transfer\nlearning with Transformer-based models, thereby demonstrating the potential\nbenefits these models can bring to text-based social science research.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 15:41:20 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Wankm\u00fcller", "Sandra", ""]]}, {"id": "2102.02114", "submitter": "Irene Li", "authors": "Irene Li", "title": "Detecting Bias in Transfer Learning Approaches for Text Classification", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Classification is an essential and fundamental task in machine learning,\nplaying a cardinal role in the field of natural language processing (NLP) and\ncomputer vision (CV). In a supervised learning setting, labels are always\nneeded for the classification task. Especially for deep neural models, a large\namount of high-quality labeled data are required for training. However, when a\nnew domain comes out, it is usually hard or expensive to acquire the labels.\nTransfer learning could be an option to transfer the knowledge from a source\ndomain to a target domain. A challenge is that these two domains can be\ndifferent, either on the feature distribution, or the class distribution for\nthe nature of the samples. In this work, we evaluate some existing transfer\nlearning approaches on detecting the bias of imbalanced classes including\ntraditional and deep models. Besides, we propose an approach to bridge the gap\nof the domain class imbalance issue.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 15:48:21 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Li", "Irene", ""]]}, {"id": "2102.02183", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Ryan Cotterell, Brian Roark", "title": "Disambiguatory Signals are Stronger in Word-initial Positions", "comments": "Accepted at EACL 2021. Code is available in\n  https://github.com/tpimentelms/frontload-disambiguation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psycholinguistic studies of human word processing and lexical access provide\nample evidence of the preferred nature of word-initial versus word-final\nsegments, e.g., in terms of attention paid by listeners (greater) or the\nlikelihood of reduction by speakers (lower). This has led to the conjecture --\nas in Wedel et al. (2019b), but common elsewhere -- that languages have evolved\nto provide more information earlier in words than later. Information-theoretic\nmethods to establish such tendencies in lexicons have suffered from several\nmethodological shortcomings that leave open the question of whether this high\nword-initial informativeness is actually a property of the lexicon or simply an\nartefact of the incremental nature of recognition. In this paper, we point out\nthe confounds in existing methods for comparing the informativeness of segments\nearly in the word versus later in the word, and present several new measures\nthat avoid these confounds. When controlling for these confounds, we still find\nevidence across hundreds of languages that indeed there is a cross-linguistic\ntendency to front-load information in words.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:19:16 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Pimentel", "Tiago", ""], ["Cotterell", "Ryan", ""], ["Roark", "Brian", ""]]}, {"id": "2102.02189", "submitter": "Young-Suk Lee Dr.", "authors": "Janaki Sheth and Young-Suk Lee and Ramon Fernandez Astudillo and\n  Tahira Naseem and Radu Florian and Salim Roukos and Todd Ward", "title": "Bootstrapping Multilingual AMR with Contextual Word Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We develop high performance multilingualAbstract Meaning Representation (AMR)\nsys-tems by projecting English AMR annotationsto other languages with weak\nsupervision. Weachieve this goal by bootstrapping transformer-based\nmultilingual word embeddings, in partic-ular those from cross-lingual RoBERTa\n(XLM-R large). We develop a novel technique forforeign-text-to-English AMR\nalignment, usingthe contextual word alignment between En-glish and foreign\nlanguage tokens. This wordalignment is weakly supervised and relies onthe\ncontextualized XLM-R word embeddings.We achieve a highly competitive\nperformancethat surpasses the best published results forGerman, Italian,\nSpanish and Chinese.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:35:55 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Sheth", "Janaki", ""], ["Lee", "Young-Suk", ""], ["Astudillo", "Ramon Fernandez", ""], ["Naseem", "Tahira", ""], ["Florian", "Radu", ""], ["Roukos", "Salim", ""], ["Ward", "Todd", ""]]}, {"id": "2102.02191", "submitter": "Sarik Ghazarian", "authors": "Sarik Ghazarian, Zixi Liu, Tuhin Chakrabarty, Xuezhe Ma, Aram\n  Galstyan, and Nanyun Peng", "title": "DiSCoL: Toward Engaging Dialogue Systems through Conversational Line\n  Guided Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having engaging and informative conversations with users is the utmost goal\nfor open-domain conversational systems. Recent advances in transformer-based\nlanguage models and their applications to dialogue systems have succeeded to\ngenerate fluent and human-like responses. However, they still lack control over\nthe generation process towards producing contentful responses and achieving\nengaging conversations. To achieve this goal, we present \\textbf{DiSCoL}\n(\\textbf{Di}alogue \\textbf{S}ystems through \\textbf{Co}versational\n\\textbf{L}ine guided response generation). DiSCoL is an open-domain dialogue\nsystem that leverages conversational lines (briefly \\textbf{convlines}) as\ncontrollable and informative content-planning elements to guide the generation\nmodel produce engaging and informative responses. Two primary modules in\nDiSCoL's pipeline are conditional generators trained for 1) predicting relevant\nand informative convlines for dialogue contexts and 2) generating high-quality\nresponses conditioned on the predicted convlines. Users can also change the\nreturned convlines to \\textit{control} the direction of the conversations\ntowards topics that are more interesting for them. Through automatic and human\nevaluations, we demonstrate the efficiency of the convlines in producing\nengaging conversations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:36:58 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ghazarian", "Sarik", ""], ["Liu", "Zixi", ""], ["Chakrabarty", "Tuhin", ""], ["Ma", "Xuezhe", ""], ["Galstyan", "Aram", ""], ["Peng", "Nanyun", ""]]}, {"id": "2102.02201", "submitter": "Peter Hase", "authors": "Peter Hase, Mohit Bansal", "title": "When Can Models Learn From Explanations? A Formal Framework for\n  Understanding the Roles of Explanation Data", "comments": "25 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods now exist for conditioning model outputs on task instructions,\nretrieved documents, and user-provided explanations and feedback. Rather than\nrelying solely on examples of task inputs and outputs, these approaches use\nvaluable additional data for improving model correctness and aligning learned\nmodels with human priors. Meanwhile, a growing body of evidence suggests that\nsome language models can (1) store a large amount of knowledge in their\nparameters, and (2) perform inference over tasks in textual inputs at test\ntime. These results raise the possibility that, for some tasks, humans cannot\nexplain to a model any more about the task than it already knows or could infer\non its own. In this paper, we study the circumstances under which explanations\nof individual data points can (or cannot) improve modeling performance. In\norder to carefully control important properties of the data and explanations,\nwe introduce a synthetic dataset for experiments, and we also make use of three\nexisting datasets with explanations: e-SNLI, TACRED, and SemEval. We first give\na formal framework for the available modeling approaches, in which explanation\ndata can be used as model inputs, as targets, or as a prior. After arguing that\nthe most promising role for explanation data is as model inputs, we propose to\nuse a retrieval-based method and show that it solves our synthetic task with\naccuracies upwards of 95%, while baselines without explanation data achieve\nbelow 65% accuracy. We then identify properties of datasets for which\nretrieval-based modeling fails. With the three existing datasets, we find no\nimprovements from explanation retrieval. Drawing on findings from our synthetic\ntask, we suggest that at least one of six preconditions for successful modeling\nfails to hold with these datasets. Our code is publicly available at\nhttps://github.com/peterbhase/ExplanationRoles\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:57:08 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 20:26:36 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Hase", "Peter", ""], ["Bansal", "Mohit", ""]]}, {"id": "2102.02204", "submitter": "Vahid Salari", "authors": "Mina Abbaszadeh, S. Shahin Mousavi, Vahid Salari", "title": "Parametrized Quantum Circuits of Synonymous Sentences in Quantum Natural\n  Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a compositional vector-based semantics of positive\ntransitive sentences in quantum natural language processing for a non-English\nlanguage, i.e. Persian, to compare the parametrized quantum circuits of two\nsynonymous sentences in two languages, English and Persian. By considering\ngrammar+meaning of a transitive sentence, we translate DisCoCat diagram via\nZX-calculus into quantum circuit form. Also, we use a bigraph method to rewrite\nDisCoCat diagram and turn into quantum circuit in the semantic side.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 23:11:41 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Abbaszadeh", "Mina", ""], ["Mousavi", "S. Shahin", ""], ["Salari", "Vahid", ""]]}, {"id": "2102.02270", "submitter": "Prashanth Gurunath Shivakumar", "authors": "Prashanth Gurunath Shivakumar, Panayiotis Georgiou, Shrikanth\n  Narayanan", "title": "Confusion2vec 2.0: Enriching Ambiguous Spoken Language Representations\n  with Subwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word vector representations enable machines to encode human language for\nspoken language understanding and processing. Confusion2vec, motivated from\nhuman speech production and perception, is a word vector representation which\nencodes ambiguities present in human spoken language in addition to semantics\nand syntactic information. Confusion2vec provides a robust spoken language\nrepresentation by considering inherent human language ambiguities. In this\npaper, we propose a novel word vector space estimation by unsupervised learning\non lattices output by an automatic speech recognition (ASR) system. We encode\neach word in confusion2vec vector space by its constituent subword character\nn-grams. We show the subword encoding helps better represent the acoustic\nperceptual ambiguities in human spoken language via information modeled on\nlattice structured ASR output. The usefulness of the proposed Confusion2vec\nrepresentation is evaluated using semantic, syntactic and acoustic analogy and\nword similarity tasks. We also show the benefits of subword modeling for\nacoustic ambiguity representation on the task of spoken language intent\ndetection. The results significantly outperform existing word vector\nrepresentations when evaluated on erroneous ASR outputs. We demonstrate that\nConfusion2vec subword modeling eliminates the need for retraining/adapting the\nnatural language understanding models on ASR transcripts.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 20:03:50 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 13:37:41 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Shivakumar", "Prashanth Gurunath", ""], ["Georgiou", "Panayiotis", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2102.02335", "submitter": "Archita Pathak", "authors": "Archita Pathak, Mohammad Abuzar Shaikh, Rohini Srihari", "title": "Self-Supervised Claim Identification for Automated Fact Checking", "comments": "15 pages, 4 figures, Accepted at ICON 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, attention-based self-supervised approach to identify\n\"claim-worthy\" sentences in a fake news article, an important first step in\nautomated fact-checking. We leverage \"aboutness\" of headline and content using\nattention mechanism for this task. The identified claims can be used for\ndownstream task of claim verification for which we are releasing a benchmark\ndataset of manually selected compelling articles with veracity labels and\nassociated evidence. This work goes beyond stylistic analysis to identifying\ncontent that influences reader belief. Experiments with three datasets show the\nstrength of our model. Data and code available at\nhttps://github.com/architapathak/Self-Supervised-ClaimIdentification\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:37:09 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Pathak", "Archita", ""], ["Shaikh", "Mohammad Abuzar", ""], ["Srihari", "Rohini", ""]]}, {"id": "2102.02340", "submitter": "Zhen Xu", "authors": "Zhen Xu, David R. So, Andrew M. Dai", "title": "MUFASA: Multimodal Fusion Architecture Search for Electronic Health\n  Records", "comments": "Accepted for publication at the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important challenge of applying deep learning to electronic health\nrecords (EHR) is the complexity of their multimodal structure. EHR usually\ncontains a mixture of structured (codes) and unstructured (free-text) data with\nsparse and irregular longitudinal features -- all of which doctors utilize when\nmaking decisions. In the deep learning regime, determining how different\nmodality representations should be fused together is a difficult problem, which\nis often addressed by handcrafted modeling and intuition. In this work, we\nextend state-of-the-art neural architecture search (NAS) methods and propose\nMUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across\nmultimodal fusion strategies and modality-specific architectures for the first\ntime. We demonstrate empirically that our MUFASA method outperforms established\nunimodal NAS on public EHR data with comparable computation costs. In addition,\nMUFASA produces architectures that outperform Transformer and Evolved\nTransformer. Compared with these baselines on CCS diagnosis code prediction,\nour discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate\nthe ability to generalize to other EHR tasks. Studying our top architecture in\ndepth, we provide empirical evidence that MUFASA's improvements are derived\nfrom its ability to both customize modeling for each data modality and find\neffective fusion strategies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:48:54 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Xu", "Zhen", ""], ["So", "David R.", ""], ["Dai", "Andrew M.", ""]]}, {"id": "2102.02435", "submitter": "Han Liu", "authors": "Han Liu, Caixia Yuan, Xiaojie Wang, Yushu Yang, Huixing Jiang,\n  Zhongyuan Wang", "title": "Converse, Focus and Guess -- Towards Multi-Document Driven Dialogue", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel task, Multi-Document Driven Dialogue (MD3), in which an\nagent can guess the target document that the user is interested in by leading a\ndialogue. To benchmark progress, we introduce a new dataset of GuessMovie,\nwhich contains 16,881 documents, each describing a movie, and associated 13,434\ndialogues. Further, we propose the MD3 model. Keeping guessing the target\ndocument in mind, it converses with the user conditioned on both document\nengagement and user feedback. In order to incorporate large-scale external\ndocuments into the dialogue, it pretrains a document representation which is\nsensitive to attributes it talks about an object. Then it tracks dialogue state\nby detecting evolvement of document belief and attribute belief, and finally\noptimizes dialogue policy in principle of entropy decreasing and reward\nincreasing, which is expected to successfully guess the user's target in a\nminimum number of turns. Experiments show that our method significantly\noutperforms several strong baseline methods and is very close to human's\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 06:36:11 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Liu", "Han", ""], ["Yuan", "Caixia", ""], ["Wang", "Xiaojie", ""], ["Yang", "Yushu", ""], ["Jiang", "Huixing", ""], ["Wang", "Zhongyuan", ""]]}, {"id": "2102.02478", "submitter": "Faisal Bin Ashraf", "authors": "Md Faisal Ahmed, Zalish Mahmud, Zarin Tasnim Biash, Ahmed Ann Noor\n  Ryen, Arman Hossain, Faisal Bin Ashraf", "title": "Bangla Text Dataset and Exploratory Analysis for Online Harassment\n  Detection", "comments": "3 pages, 5 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being the seventh most spoken language in the world, the use of the Bangla\nlanguage online has increased in recent times. Hence, it has become very\nimportant to analyze Bangla text data to maintain a safe and harassment-free\nonline place. The data that has been made accessible in this article has been\ngathered and marked from the comments of people in public posts by celebrities,\ngovernment officials, athletes on Facebook. The total amount of collected\ncomments is 44001. The dataset is compiled with the aim of developing the\nability of machines to differentiate whether a comment is a bully expression or\nnot with the help of Natural Language Processing and to what extent it is\nimproper if it is an inappropriate comment. The comments are labeled with\ndifferent categories of harassment. Exploratory analysis from different\nperspectives is also included in this paper to have a detailed overview. Due to\nthe scarcity of data collection of categorized Bengali language comments, this\ndataset can have a significant role for research in detecting bully words,\nidentifying inappropriate comments, detecting different categories of Bengali\nbullies, etc. The dataset is publicly available at\nhttps://data.mendeley.com/datasets/9xjx8twk8p.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 08:35:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ahmed", "Md Faisal", ""], ["Mahmud", "Zalish", ""], ["Biash", "Zarin Tasnim", ""], ["Ryen", "Ahmed Ann Noor", ""], ["Hossain", "Arman", ""], ["Ashraf", "Faisal Bin", ""]]}, {"id": "2102.02503", "submitter": "Alex Tamkin", "authors": "Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli", "title": "Understanding the Capabilities, Limitations, and Societal Impact of\n  Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On October 14th, 2020, researchers from OpenAI, the Stanford Institute for\nHuman-Centered Artificial Intelligence, and other universities convened to\ndiscuss open research questions surrounding GPT-3, the largest\npublicly-disclosed dense language model at the time. The meeting took place\nunder Chatham House Rules. Discussants came from a variety of research\nbackgrounds including computer science, linguistics, philosophy, political\nscience, communications, cyber policy, and more. Broadly, the discussion\ncentered around two main questions: 1) What are the technical capabilities and\nlimitations of large language models? 2) What are the societal effects of\nwidespread use of large language models? Here, we provide a detailed summary of\nthe discussion organized by the two themes above.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:27:04 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Tamkin", "Alex", ""], ["Brundage", "Miles", ""], ["Clark", "Jack", ""], ["Ganguli", "Deep", ""]]}, {"id": "2102.02557", "submitter": "Dani Yogatama", "authors": "Dani Yogatama, Cyprien de Masson d'Autume, Lingpeng Kong", "title": "Adaptive Semiparametric Language Models", "comments": "Accepted to TACL, pre MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a language model that combines a large parametric neural network\n(i.e., a transformer) with a non-parametric episodic memory component in an\nintegrated architecture. Our model uses extended short-term context by caching\nlocal hidden states -- similar to transformer-XL -- and global long-term memory\nby retrieving a set of nearest neighbor tokens at each timestep. We design a\ngating function to adaptively combine multiple information sources to make a\nprediction. This mechanism allows the model to use either local context,\nshort-term memory, or long-term memory (or any combination of them) on an ad\nhoc basis depending on the context. Experiments on word-based and\ncharacter-based language modeling datasets demonstrate the efficacy of our\nproposed method compared to strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 11:47:03 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yogatama", "Dani", ""], ["d'Autume", "Cyprien de Masson", ""], ["Kong", "Lingpeng", ""]]}, {"id": "2102.02574", "submitter": "James Hargreaves", "authors": "James Hargreaves, Andreas Vlachos, Guy Emerson", "title": "Incremental Beam Manipulation for Natural Language Generation", "comments": "camera ready for EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of natural language generation systems has improved\nsubstantially with modern neural networks. At test time they typically employ\nbeam search to avoid locally optimal but globally suboptimal predictions.\nHowever, due to model errors, a larger beam size can lead to deteriorating\nperformance according to the evaluation metric. For this reason, it is common\nto rerank the output of beam search, but this relies on beam search to produce\na good set of hypotheses, which limits the potential gains. Other alternatives\nto beam search require changes to the training of the model, which restricts\ntheir applicability compared to beam search. This paper proposes incremental\nbeam manipulation, i.e. reranking the hypotheses in the beam during decoding\ninstead of only at the end. This way, hypotheses that are unlikely to lead to a\ngood final output are discarded, and in their place hypotheses that would have\nbeen ignored will be considered instead. Applying incremental beam manipulation\nleads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search\nfor the test sets of the E2E and WebNLG challenges respectively. The proposed\nmethod also outperformed a strong reranker by 1.04 BLEU points on the E2E\nchallenge, while being on par with it on the WebNLG dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 12:26:47 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 22:52:00 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 23:45:53 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hargreaves", "James", ""], ["Vlachos", "Andreas", ""], ["Emerson", "Guy", ""]]}, {"id": "2102.02585", "submitter": "V\\'it Novotn\\'y", "authors": "V\\'it Novotn\\'y (1) and Eniafe Festus Ayetiran (1) and D\\'avid\n  Lupt\\'ak (1) and Michal \\v{S}tef\\'anik (1) and Petr Sojka (1) ((1) Faculty of\n  Informatics Masaryk University)", "title": "One Size Does Not Fit All: Finding the Optimal N-gram Sizes for FastText\n  Models across Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised word representation learning from large corpora is badly needed\nfor downstream tasks such as text classification, information retrieval, and\nmachine translation. The representation precision of the fastText language\nmodels is mostly due to their use of subword information. In previous work, the\noptimization of fastText subword sizes has been largely neglected, and\nnon-English fastText language models were trained using subword sizes optimized\nfor English and German.\n  In our work, we train English, German, Czech, and Italian fastText language\nmodels on Wikipedia, and we optimize the subword sizes on the English, German,\nCzech, and Italian word analogy tasks. We show that the optimization of subword\nsizes results in a 5% improvement on the Czech word analogy task. We also show\nthat computationally expensive hyperparameter optimization can be replaced with\ncheap $n$-gram frequency analysis: subword sizes that are the closest to\ncovering 3.76% of all unique subwords in a language are shown to be the optimal\nfastText hyperparameters on the English, German, Czech, and Italian word\nanalogy tasks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 12:59:36 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Novotn\u00fd", "V\u00edt", ""], ["Ayetiran", "Eniafe Festus", ""], ["Lupt\u00e1k", "D\u00e1vid", ""], ["\u0160tef\u00e1nik", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "2102.02636", "submitter": "Hendri Murfi", "authors": "Hendri Murfi, Natasha Rosaline, Nora Hariadi", "title": "Deep Autoencoder-based Fuzzy C-Means for Topic Detection", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic detection is a process for determining topics from a collection of\ntextual data. One of the topic detection methods is a clustering-based method,\nwhich assumes that the centroids are topics. The clustering method has the\nadvantage that it can process data with negative representations. Therefore,\nthe clustering method allows a combination with a broader representation\nlearning method. In this paper, we adopt deep learning for topic detection by\nusing a deep autoencoder and fuzzy c-means called deep autoencoder-based fuzzy\nc-means (DFCM). The encoder of the autoencoder performs a lower-dimensional\nrepresentation learning. Fuzzy c-means groups the lower-dimensional\nrepresentation to identify the centroids. The autoencoder's decoder transforms\nback the centroids into the original representation to be interpreted as the\ntopics. Our simulation shows that DFCM improves the coherence score of\neigenspace-based fuzzy c-means (EFCM) and is comparable to the leading standard\nmethods, i.e., nonnegative matrix factorization (NMF) or latent Dirichlet\nallocation (LDA).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:41:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Murfi", "Hendri", ""], ["Rosaline", "Natasha", ""], ["Hariadi", "Nora", ""]]}, {"id": "2102.02723", "submitter": "Ratish Puduppully", "authors": "Ratish Puduppully and Mirella Lapata", "title": "Data-to-text Generation with Macro Planning", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics (TACL); 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches to data-to-text generation have adopted the very successful\nencoder-decoder architecture or variants thereof. These models generate text\nwhich is fluent (but often imprecise) and perform quite poorly at selecting\nappropriate content and ordering it coherently. To overcome some of these\nissues, we propose a neural model with a macro planning stage followed by a\ngeneration stage reminiscent of traditional methods which embrace separate\nmodules for planning and surface realization. Macro plans represent high level\norganization of important content such as entities, events and their\ninteractions; they are learnt from data and given as input to the generator.\nExtensive experiments on two data-to-text benchmarks (RotoWire and MLB) show\nthat our approach outperforms competitive baselines in terms of automatic and\nhuman evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 16:32:57 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Puduppully", "Ratish", ""], ["Lapata", "Mirella", ""]]}, {"id": "2102.02779", "submitter": "Jaemin Cho", "authors": "Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal", "title": "Unifying Vision-and-Language Tasks via Text Generation", "comments": "ICML 2021 (15 pages, 4 figures, 14 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:59:30 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 23:12:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cho", "Jaemin", ""], ["Lei", "Jie", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2102.02810", "submitter": "Marco Roberti", "authors": "Cl\\'ement Rebuffel, Marco Roberti, Laure Soulier, Geoffrey\n  Scoutheeten, Rossella Cancelliere, Patrick Gallinari", "title": "Controlling Hallucinations at Word Level in Data-to-Text Generation", "comments": "20 pages, 6 figures, 5 tables (excluding Appendix). Source code:\n  https://github.com/KaijuML/dtt-multi-branch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-to-Text Generation (DTG) is a subfield of Natural Language Generation\naiming at transcribing structured data in natural language descriptions. The\nfield has been recently boosted by the use of neural-based generators which\nexhibit on one side great syntactic skills without the need of hand-crafted\npipelines; on the other side, the quality of the generated text reflects the\nquality of the training data, which in realistic settings only offer\nimperfectly aligned structure-text pairs. Consequently, state-of-art neural\nmodels include misleading statements - usually called hallucinations - in their\noutputs. The control of this phenomenon is today a major challenge for DTG, and\nis the problem addressed in the paper.\n  Previous work deal with this issue at the instance level: using an alignment\nscore for each table-reference pair. In contrast, we propose a finer-grained\napproach, arguing that hallucinations should rather be treated at the word\nlevel. Specifically, we propose a Multi-Branch Decoder which is able to\nleverage word-level labels to learn the relevant parts of each training\ninstance. These labels are obtained following a simple and efficient scoring\nprocedure based on co-occurrence analysis and dependency parsing. Extensive\nevaluations, via automated metrics and human judgment on the standard WikiBio\nbenchmark, show the accuracy of our alignment labels and the effectiveness of\nthe proposed Multi-Branch Decoder. Our model is able to reduce and control\nhallucinations, while keeping fluency and coherence in generated texts. Further\nexperiments on a degraded version of ToTTo show that our model could be\nsuccessfully used on very noisy settings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:58:28 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 16:29:49 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Rebuffel", "Cl\u00e9ment", ""], ["Roberti", "Marco", ""], ["Soulier", "Laure", ""], ["Scoutheeten", "Geoffrey", ""], ["Cancelliere", "Rossella", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2102.02841", "submitter": "Costanza Conforti", "authors": "Stephanie Hirmer, Alycia Leonard, Josephine Tumwesige, Costanza\n  Conforti", "title": "Building Representative Corpora from Illiterate Communities: A Review of\n  Challenges and Mitigation Strategies for Developing Countries", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most well-established data collection methods currently adopted in NLP depend\non the assumption of speaker literacy. Consequently, the collected corpora\nlargely fail to represent swathes of the global population, which tend to be\nsome of the most vulnerable and marginalised people in society, and often live\nin rural developing areas. Such underrepresented groups are thus not only\nignored when making modeling and system design decisions, but also prevented\nfrom benefiting from development outcomes achieved through data-driven NLP.\nThis paper aims to address the under-representation of illiterate communities\nin NLP corpora: we identify potential biases and ethical issues that might\narise when collecting data from rural communities with high illiteracy rates in\nLow-Income Countries, and propose a set of practical mitigation strategies to\nhelp future work.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 19:20:35 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Hirmer", "Stephanie", ""], ["Leonard", "Alycia", ""], ["Tumwesige", "Josephine", ""], ["Conforti", "Costanza", ""]]}, {"id": "2102.02917", "submitter": "Allison Lahnala", "authors": "Allison Lahnala, Gauri Kambhatla, Jiajun Peng, Matthew Whitehead,\n  Gillian Minnehan, Eric Guldan, Jonathan K. Kummerfeld, An{\\i}l \\c{C}amc{\\i},\n  Rada Mihalcea", "title": "Chord Embeddings: Analyzing What They Capture and Their Role for Next\n  Chord Prediction and Artist Attribute Prediction", "comments": "16 pages, accepted to EvoMUSART", "journal-ref": "Computational Intelligence in Music, Sound, Art and Design, 10th\n  International Conference, EvoMUSART 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing methods have been applied in a variety of music\nstudies, drawing the connection between music and language. In this paper, we\nexpand those approaches by investigating \\textit{chord embeddings}, which we\napply in two case studies to address two key questions: (1) what musical\ninformation do chord embeddings capture?; and (2) how might musical\napplications benefit from them? In our analysis, we show that they capture\nsimilarities between chords that adhere to important relationships described in\nmusic theory. In the first case study, we demonstrate that using chord\nembeddings in a next chord prediction task yields predictions that more closely\nmatch those by experienced musicians. In the second case study, we show the\npotential benefits of using the representations in tasks related to musical\nstylometrics.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:17:17 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Lahnala", "Allison", ""], ["Kambhatla", "Gauri", ""], ["Peng", "Jiajun", ""], ["Whitehead", "Matthew", ""], ["Minnehan", "Gillian", ""], ["Guldan", "Eric", ""], ["Kummerfeld", "Jonathan K.", ""], ["\u00c7amc\u0131", "An\u0131l", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2102.02925", "submitter": "A.B. Siddique", "authors": "A.B. Siddique, Fuad Jamour, Luxun Xu, Vagelis Hristidis", "title": "Generalized Zero-shot Intent Detection via Commonsense Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying user intents from natural language utterances is a crucial step\nin conversational systems that has been extensively studied as a supervised\nclassification problem. However, in practice, new intents emerge after\ndeploying an intent detection model. Thus, these models should seamlessly adapt\nand classify utterances with both seen and unseen intents -- unseen intents\nemerge after deployment and they do not have training data. The few existing\nmodels that target this setting rely heavily on the scarcely available training\ndata and overfit to seen intents data, resulting in a bias to misclassify\nutterances with unseen intents into seen ones. We propose RIDE: an intent\ndetection model that leverages commonsense knowledge in an unsupervised fashion\nto overcome the issue of training data scarcity. RIDE computes robust and\ngeneralizable relationship meta-features that capture deep semantic\nrelationships between utterances and intent labels; these features are computed\nby considering how the concepts in an utterance are linked to those in an\nintent label via commonsense knowledge. Our extensive experimental analysis on\nthree widely-used intent detection benchmarks shows that relationship\nmeta-features significantly increase the accuracy of detecting both seen and\nunseen intents and that RIDE outperforms the state-of-the-art model for unseen\nintents.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 23:36:41 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Siddique", "A. B.", ""], ["Jamour", "Fuad", ""], ["Xu", "Luxun", ""], ["Hristidis", "Vagelis", ""]]}, {"id": "2102.02955", "submitter": "Rei Miyata", "authors": "Rei Miyata, Atsushi Fujita", "title": "Understanding Pre-Editing for Black-Box Neural Machine Translation", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pre-editing is the process of modifying the source text (ST) so that it can\nbe translated by machine translation (MT) in a better quality. Despite the\nunpredictability of black-box neural MT (NMT), pre-editing has been deployed in\nvarious practical MT use cases. Although many studies have demonstrated the\neffectiveness of pre-editing methods for particular settings, thus far, a deep\nunderstanding of what pre-editing is and how it works for black-box NMT is\nlacking. To elicit such understanding, we extensively investigated human\npre-editing practices. We first implemented a protocol to incrementally record\nthe minimum edits for each ST and collected 6,652 instances of pre-editing\nacross three translation directions, two MT systems, and four text domains. We\nthen analysed the instances from three perspectives: the characteristics of the\npre-edited ST, the diversity of pre-editing operations, and the impact of the\npre-editing operations on NMT outputs. Our findings include the following: (1)\nenhancing the explicitness of the meaning of an ST and its syntactic structure\nis more important for obtaining better translations than making the ST shorter\nand simpler, and (2) although the impact of pre-editing on NMT is generally\nunpredictable, there are some tendencies of changes in the NMT outputs\ndepending on the editing operation types.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:01:18 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Miyata", "Rei", ""], ["Fujita", "Atsushi", ""]]}, {"id": "2102.02959", "submitter": "Fakrul Islam Tushar", "authors": "Vincent M. D'Anniballe, Fakrul I. Tushar, Khrystyna Faryna, Songyue\n  Han, Maciej A. Mazurowski, Geoffrey D. Rubin, Joseph Y. Lo", "title": "Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text\n  Reports Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: To develop high throughput multi-label annotators for body (chest,\nabdomen, and pelvis) Computed Tomography (CT) reports that can be applied\nacross a variety of abnormalities, organs, and disease states.\n  Approach: We used a dictionary approach to develop rule-based algorithms\n(RBA) for extraction of disease labels from radiology text reports. We targeted\nthree organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with\nfour diseases per system based on their prevalence in our dataset. To expand\nthe algorithms beyond pre-defined keywords, attention-guided recurrent neural\nnetworks (RNN) were trained using the RBA-extracted labels to classify reports\nas being positive for one or more diseases or normal for each organ system.\nConfounding effects on model performance were evaluated using random\ninitialization or pre-trained embedding as well as different sizes of training\ndatasets. Performance was evaluated using the receiver operating characteristic\n(ROC) area under the curve (AUC) against 2,158 manually obtained labels.\n  Results: Our models extracted disease labels from 261,229 radiology reports\nof 112,501 unique subjects. Pre-trained models outperformed random\ninitialization across all diseases. As the training dataset size was reduced,\nperformance was robust except for a few diseases with relatively small number\nof cases. Pre-trained classification AUCs achieved > 0.95 for all five disease\noutcomes across all three organ systems.\n  Conclusions: Our label-extracting pipeline was able to encompass a variety of\ncases and diseases by generalizing beyond strict rules with exceptional\naccuracy. This method can be easily adapted to enable automated labeling of\nhospital-scale medical data sets for training image-based disease classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:07:39 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 03:12:11 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 09:37:37 GMT"}, {"version": "v4", "created": "Sat, 12 Jun 2021 11:38:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["D'Anniballe", "Vincent M.", ""], ["Tushar", "Fakrul I.", ""], ["Faryna", "Khrystyna", ""], ["Han", "Songyue", ""], ["Mazurowski", "Maciej A.", ""], ["Rubin", "Geoffrey D.", ""], ["Lo", "Joseph Y.", ""]]}, {"id": "2102.02963", "submitter": "Hong Chen", "authors": "Hong Chen, Yifei Huang, Hiroya Takamura, Hideki Nakayama", "title": "Commonsense Knowledge Aware Concept Selection For Diverse and\n  Informative Visual Storytelling", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual storytelling is a task of generating relevant and interesting stories\nfor given image sequences. In this work we aim at increasing the diversity of\nthe generated stories while preserving the informative content from the images.\nWe propose to foster the diversity and informativeness of a generated story by\nusing a concept selection module that suggests a set of concept candidates.\nThen, we utilize a large scale pre-trained model to convert concepts and images\ninto full stories. To enrich the candidate concepts, a commonsense knowledge\ngraph is created for each image sequence from which the concept candidates are\nproposed. To obtain appropriate concepts from the graph, we propose two novel\nmodules that consider the correlation among candidate concepts and the\nimage-concept correlation. Extensive automatic and human evaluation results\ndemonstrate that our model can produce reasonable concepts. This enables our\nmodel to outperform the previous models by a large margin on the diversity and\ninformativeness of the story, while retaining the relevance of the story to the\nimage sequence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:15:28 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chen", "Hong", ""], ["Huang", "Yifei", ""], ["Takamura", "Hiroya", ""], ["Nakayama", "Hideki", ""]]}, {"id": "2102.02967", "submitter": "Lin Sun", "authors": "Lin Sun, Jiquan Wang, Kai Zhang, Yindu Su, and Fangsheng Weng", "title": "RpBERT: A Text-image Relation Propagation-based BERT Model for\n  Multimodal NER", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently multimodal named entity recognition (MNER) has utilized images to\nimprove the accuracy of NER in tweets. However, most of the multimodal methods\nuse attention mechanisms to extract visual clues regardless of whether the text\nand image are relevant. Practically, the irrelevant text-image pairs account\nfor a large proportion in tweets. The visual clues that are unrelated to the\ntexts will exert uncertain or even negative effects on multimodal model\nlearning. In this paper, we introduce a method of text-image relation\npropagation into the multimodal BERT model. We integrate soft or hard gates to\nselect visual clues and propose a multitask algorithm to train on the MNER\ndatasets. In the experiments, we deeply analyze the changes in visual attention\nbefore and after the use of text-image relation propagation. Our model achieves\nstate-of-the-art performance on the MNER datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:45:30 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Sun", "Lin", ""], ["Wang", "Jiquan", ""], ["Zhang", "Kai", ""], ["Su", "Yindu", ""], ["Weng", "Fangsheng", ""]]}, {"id": "2102.02977", "submitter": "Hong Chen", "authors": "Hong Chen, Raphael Shu, Hiroya Takamura, Hideki Nakayama", "title": "GraphPlan: Story Generation by Planning with Event Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Story generation is a task that aims to automatically produce multiple\nsentences to make up a meaningful story. This task is challenging because it\nrequires high-level understanding of semantic meaning of sentences and\ncausality of story events. Naive sequence-to-sequence models generally fail to\nacquire such knowledge, as the logical correctness can hardly be guaranteed in\na text generation model without the strategic planning. In this paper, we focus\non planning a sequence of events assisted by event graphs, and use the events\nto guide the generator. Instead of using a sequence-to-sequence model to output\na storyline as in some existing works, we propose to generate an event sequence\nby walking on an event graph. The event graphs are built automatically based on\nthe corpus. To evaluate the proposed approach, we conduct human evaluation both\non event planning and story generation. Based on large-scale human annotation\nresults, our proposed approach is shown to produce more logically correct event\nsequences and stories.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 03:18:55 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chen", "Hong", ""], ["Shu", "Raphael", ""], ["Takamura", "Hiroya", ""], ["Nakayama", "Hideki", ""]]}, {"id": "2102.03016", "submitter": "Sagnik Majumder", "authors": "Sagnik Majumder, Chinmoy Samant, Greg Durrett", "title": "Model Agnostic Answer Reranking System for Adversarial Question\n  Answering", "comments": "EACL 2021 Student Research Workshop Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While numerous methods have been proposed as defenses against adversarial\nexamples in question answering (QA), these techniques are often model specific,\nrequire retraining of the model, and give only marginal improvements in\nperformance over vanilla models. In this work, we present a simple\nmodel-agnostic approach to this problem that can be applied directly to any QA\nmodel without any retraining. Our method employs an explicit answer candidate\nreranking mechanism that scores candidate answers on the basis of their content\noverlap with the question before making the final prediction. Combined with a\nstrong base QAmodel, our method outperforms state-of-the-art defense\ntechniques, calling into question how well these techniques are actually doing\nand strong these adversarial testbeds are.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 06:18:12 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Majumder", "Sagnik", ""], ["Samant", "Chinmoy", ""], ["Durrett", "Greg", ""]]}, {"id": "2102.03044", "submitter": "Cl\\'ement Hongler", "authors": "Sylvain Carr\\'e, Franck Gabriel, Cl\\'ement Hongler, Gustavo Lacerda,\n  and Gloria Capano", "title": "Smart Proofs via Smart Contracts: Succinct and Informative Mathematical\n  Derivations via Decentralized Markets", "comments": "45 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CL cs.LO cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mathematics is built on the idea that proofs should be translatable\ninto formal proofs, whose validity is an objective question, decidable by a\ncomputer. Yet, in practice, proofs are informal and may omit many details. An\nagent considers a proof valid if they trust that it could be expanded into a\nmachine-verifiable proof. A proof's validity can thus become a subjective\nmatter and lead to a debate, which may be difficult to settle. Hence, while the\nconcept of valid proof is well-defined, the process to establish validity is\nitself a complex multi-agent problem.\n  We introduce the SPRIG protocol. SPRIG allows agents to propose and verify\nsuccinct and informative proofs in a decentralized fashion; the trust is\nestablished by agents being able to request more details in the proof steps;\ndebates, if they arise, must isolate details of proofs and, if they persist, go\ndown to machine-level details, where they are automatically settled. A\nstructure of bounties and stakes is set to incentivize agents to act in good\nfaith.\n  We propose a game-theoretic discussion of SPRIG, showing how agents with\nvarious types of information interact, leading to a proof tree with an\nappropriate level of detail and to the invalidation of wrong proofs, and we\ndiscuss resilience against various attacks. We then analyze a simplified model,\ncharacterize its equilibria and compute the agents' level of trust.\n  SPRIG is designed to run as a smart contract on a blockchain platform. This\nallows anonymous agents to participate in the verification debate, and to\ncontribute with their information. The smart contract mediates the\ninteractions, settles debates, and guarantees that bounties and stakes are paid\nas specified.\n  SPRIG enables new applications, such as the issuance of bounties for open\nproblems, and the creation of derivatives markets, allowing agents to inject\nmore information pertaining to proofs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 08:00:19 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 09:17:06 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 15:03:04 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Carr\u00e9", "Sylvain", ""], ["Gabriel", "Franck", ""], ["Hongler", "Cl\u00e9ment", ""], ["Lacerda", "Gustavo", ""], ["Capano", "Gloria", ""]]}, {"id": "2102.03055", "submitter": "Ruizhi Li", "authors": "Ruizhi Li and Gregory Sell and Hynek Hermansky", "title": "Two-Stage Augmentation and Adaptive CTC Fusion for Improved Robustness\n  of Multi-Stream End-to-End ASR", "comments": "Accepted at IEEE SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance degradation of an Automatic Speech Recognition (ASR) system is\ncommonly observed when the test acoustic condition is different from training.\nHence, it is essential to make ASR systems robust against various environmental\ndistortions, such as background noises and reverberations. In a multi-stream\nparadigm, improving robustness takes account of handling a variety of unseen\nsingle-stream conditions and inter-stream dynamics. Previously, a practical\ntwo-stage training strategy was proposed within multi-stream end-to-end ASR,\nwhere Stage-2 formulates the multi-stream model with features from Stage-1\nUniversal Feature Extractor (UFE). In this paper, as an extension, we introduce\na two-stage augmentation scheme focusing on mismatch scenarios: Stage-1\nAugmentation aims to address single-stream input varieties with data\naugmentation techniques; Stage-2 Time Masking applies temporal masks on UFE\nfeatures of randomly selected streams to simulate diverse stream combinations.\nDuring inference, we also present adaptive Connectionist Temporal\nClassification (CTC) fusion with the help of hierarchical attention mechanisms.\nExperiments have been conducted on two datasets, DIRHA and AMI, as a\nmulti-stream scenario. Compared with the previous training strategy,\nsubstantial improvements are reported with relative word error rate reductions\nof 29.7-59.3% across several unseen stream combinations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 08:36:58 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Li", "Ruizhi", ""], ["Sell", "Gregory", ""], ["Hermansky", "Hynek", ""]]}, {"id": "2102.03062", "submitter": "Thomas \\\"Ubellacker", "authors": "Jonas Thiergart, Stefan Huber, Thomas \\\"Ubellacker", "title": "Understanding Emails and Drafting Responses -- An Approach Using GPT-3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing computer systems with the ability to understand and generate\nnatural language has long been a challenge of engineers. Recent progress in\nnatural language processing (NLP), like the GPT-3 language model released by\nOpenAI, has made both possible to an extent. In this paper, we explore the\npossibility of rationalising email communication using GPT-3. First, we\ndemonstrate the technical feasibility of understanding incoming emails and\ngenerating responses, drawing on literature from the disciplines of software\nengineering as well as data science. Second, we apply knowledge from both\nbusiness studies and, again, software engineering to identify ways to tackle\nchallenges we encountered. Third, we argue for the economic viability of such a\nsolution by analysing costs and market demand. We conclude that applying GPT-3\nto rationalising email communication is feasible both technically and\neconomically.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 08:56:42 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 15:15:38 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 11:11:32 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Thiergart", "Jonas", ""], ["Huber", "Stefan", ""], ["\u00dcbellacker", "Thomas", ""]]}, {"id": "2102.03216", "submitter": "Jaesong Lee", "authors": "Jaesong Lee, Shinji Watanabe", "title": "Intermediate Loss Regularization for CTC-based Speech Recognition", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and efficient auxiliary loss function for automatic\nspeech recognition (ASR) based on the connectionist temporal classification\n(CTC) objective. The proposed objective, an intermediate CTC loss, is attached\nto an intermediate layer in the CTC encoder network. This intermediate CTC loss\nwell regularizes CTC training and improves the performance requiring only small\nmodification of the code and small and no overhead during training and\ninference, respectively. In addition, we propose to combine this intermediate\nCTC loss with stochastic depth training, and apply this combination to a\nrecently proposed Conformer network. We evaluate the proposed method on various\ncorpora, reaching word error rate (WER) 9.9% on the WSJ corpus and character\nerror rate (CER) 5.2% on the AISHELL-1 corpus respectively, based on CTC greedy\nsearch without a language model. Especially, the AISHELL-1 task is comparable\nto other state-of-the-art ASR systems based on auto-regressive decoder with\nbeam search.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:01:03 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Lee", "Jaesong", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2102.03218", "submitter": "Ahmad Ahmadzade", "authors": "Ahmad Ahmadzade and Saber Malekzadeh", "title": "Spell Correction for Azerbaijani Language using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spell correction is used to detect and correct orthographic mistakes in\ntexts. Most of the time, traditional dictionary lookup with string similarity\nmethods is suitable for the languages that have a less complex structure such\nas the English language. However, the Azerbaijani language has a more complex\nstructure and due to its morphological structure, the derivation of words is\nplenty that several words are derived from adding suffices, affixes to the\nwords. Therefore, in this paper sequence to sequence model with an attention\nmechanism is used to develop spelling correction for Azerbaijani. Total 12000\nwrong and correct sentence pairs used for training, and the model is tested on\n1000 real-world misspelled words and F1-score results are 75% for distance 0,\n90% for distance 1, and 96% for distance 2.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:02:35 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Ahmadzade", "Ahmad", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "2102.03277", "submitter": "Llu\\'is Alemany-Puig", "authors": "Llu\\'is Alemany-Puig, Juan Luis Esteban, Ramon Ferrer-i-Cancho", "title": "Minimum projective linearizations of trees in linear time", "comments": "Improved connection with previous Iordanskii's works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run undoubtlessly in\n$O(n)$-time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:35:38 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:20:33 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 14:02:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Alemany-Puig", "Llu\u00eds", ""], ["Esteban", "Juan Luis", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "2102.03315", "submitter": "Peter Clark", "authors": "Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi\n  Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord,\n  Peter Clark", "title": "Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the\n  Direct-Answer AI2 Reasoning Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the ARC-DA dataset, a direct-answer (\"open response\", \"freeform\")\nversion of the ARC (AI2 Reasoning Challenge) multiple-choice dataset. While ARC\nhas been influential in the community, its multiple-choice format is\nunrepresentative of real-world questions, and multiple choice formats can be\nparticularly susceptible to artifacts. The ARC-DA dataset addresses these\nconcerns by converting questions to direct-answer format using a combination of\ncrowdsourcing and expert review. The resulting dataset contains 2985 questions\nwith a total of 8436 valid answers (questions typically have more than one\nvalid answer). ARC-DA is one of the first DA datasets of natural questions that\noften require reasoning, and where appropriate question decompositions are not\nevident from the questions themselves. We describe the conversion approach\ntaken, appropriate evaluation metrics, and several strong models. Although\nhigh, the best scores (81% GENIE, 61.4% F1, 63.2% ROUGE-L) still leave\nconsiderable room for improvement. In addition, the dataset provides a natural\nsetting for new research on explanation, as many questions require reasoning to\nconstruct answers. We hope the dataset spurs further advances in complex\nquestion-answering by the community. ARC-DA is available at\nhttps://allenai.org/data/arc-da\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 17:41:43 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Bhakthavatsalam", "Sumithra", ""], ["Khashabi", "Daniel", ""], ["Khot", "Tushar", ""], ["Mishra", "Bhavana Dalvi", ""], ["Richardson", "Kyle", ""], ["Sabharwal", "Ashish", ""], ["Schoenick", "Carissa", ""], ["Tafjord", "Oyvind", ""], ["Clark", "Peter", ""]]}, {"id": "2102.03382", "submitter": "Tu Le", "authors": "Tu Le, Danny Yuxing Huang, Noah Apthorpe, Yuan Tian", "title": "SkillBot: Identifying Risky Content for Children in Alexa Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many households include children who use voice personal assistants (VPA) such\nas Amazon Alexa. Children benefit from the rich functionalities of VPAs and\nthird-party apps but are also exposed to new risks in the VPA ecosystem (e.g.,\ninappropriate content or information collection). To study the risks VPAs pose\nto children, we build a Natural Language Processing (NLP)-based system to\nautomatically interact with VPA apps and analyze the resulting conversations to\nidentify contents risky to children. We identify 28 child-directed apps with\nrisky contents and maintain a growing dataset of 31,966 non-overlapping app\nbehaviors collected from 3,434 Alexa apps. Our findings suggest that although\nvoice apps designed for children are subject to more policy requirements and\nintensive vetting, children are still vulnerable to risky content. We then\nconduct a user study showing that parents are more concerned about VPA apps\nwith inappropriate content than those that ask for personal information, but\nmany parents are not aware that risky apps of either type exist. Finally, we\nidentify a new threat to users of VPA apps: confounding utterances, or voice\ncommands shared by multiple apps that may cause a user to invoke or interact\nwith a different app than intended. We identify 4,487 confounding utterances,\nincluding 581 shared by child-directed and non-child-directed apps.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:07:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Le", "Tu", ""], ["Huang", "Danny Yuxing", ""], ["Apthorpe", "Noah", ""], ["Tian", "Yuan", ""]]}, {"id": "2102.03419", "submitter": "Dora Jambor", "authors": "Dora Jambor, Komal Teru, Joelle Pineau, William L. Hamilton", "title": "Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs", "comments": "code available at\n  https://github.com/dorajam/few-shot-link-prediction-paper", "journal-ref": "European Chapter of the ACL (EACL), 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world knowledge graphs are often characterized by low-frequency\nrelations - a challenge that has prompted an increasing interest in few-shot\nlink prediction methods. These methods perform link prediction for a set of new\nrelations, unseen during training, given only a few example facts of each\nrelation at test time. In this work, we perform a systematic study on a\nspectrum of models derived by generalizing the current state of the art for\nfew-shot link prediction, with the goal of probing the limits of learning in\nthis few-shot setting. We find that a simple zero-shot baseline - which ignores\nany relation-specific information - achieves surprisingly strong performance.\nMoreover, experiments on carefully crafted synthetic datasets show that having\nonly a few examples of a relation fundamentally limits models from using\nfine-grained structural information and only allows for exploiting the\ncoarse-grained positional information of entities. Together, our findings\nchallenge the implicit assumptions and inductive biases of prior work and\nhighlight new directions for research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 21:04:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jambor", "Dora", ""], ["Teru", "Komal", ""], ["Pineau", "Joelle", ""], ["Hamilton", "William L.", ""]]}, {"id": "2102.03462", "submitter": "Stephan Meylan", "authors": "Stephan C. Meylan, Ruthe Foushee, Elika Bergelson, Roger P. Levy", "title": "Child-directed Listening: How Caregiver Inference Enables Children's\n  Early Verbal Communication", "comments": "13 pages, 3 figures, 2 tables. Edit #1 fixes formatting on table 1\n  (fitting it onto a single page) and reports correct contents for table 1\n  (previous version reported ants, not bits)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do adults understand children's speech? Children's productions over the\ncourse of language development often bear little resemblance to typical adult\npronunciations, yet caregivers nonetheless reliably recover meaning from them.\nHere, we employ a suite of Bayesian models of spoken word recognition to\nunderstand how adults overcome the noisiness of child language, showing that\ncommunicative success between children and adults relies heavily on adult\ninferential processes. By evaluating competing models on phonetically-annotated\ncorpora, we show that adults' recovered meanings are best predicted by prior\nexpectations fitted specifically to the child language environment, rather than\nto typical adult-adult language. After quantifying the contribution of this\n\"child-directed listening\" over developmental time, we discuss the consequences\nfor theories of language acquisition, as well as the implications for\ncommonly-used methods for assessing children's linguistic proficiency.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 00:54:34 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 06:35:47 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Meylan", "Stephan C.", ""], ["Foushee", "Ruthe", ""], ["Bergelson", "Elika", ""], ["Levy", "Roger P.", ""]]}, {"id": "2102.03551", "submitter": "Ernie Chang", "authors": "Ernie Chang, Vera Demberg, Alex Marin", "title": "Jointly Improving Language Understanding and Generation with\n  Quality-Weighted Weak Supervision of Automatic Labeling", "comments": "Accepted at EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural natural language generation (NLG) and understanding (NLU) models are\ndata-hungry and require massive amounts of annotated data to be competitive.\nRecent frameworks address this bottleneck with generative models that\nsynthesize weak labels at scale, where a small amount of training labels are\nexpert-curated and the rest of the data is automatically annotated. We follow\nthat approach, by automatically constructing a large-scale weakly-labeled data\nwith a fine-tuned GPT-2, and employ a semi-supervised framework to jointly\ntrain the NLG and NLU models. The proposed framework adapts the parameter\nupdates to the models according to the estimated label-quality. On both the E2E\nand Weather benchmarks, we show that this weakly supervised training paradigm\nis an effective approach under low resource scenarios and outperforming\nbenchmark systems on both datasets when 100% of training data is used.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 10:06:15 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chang", "Ernie", ""], ["Demberg", "Vera", ""], ["Marin", "Alex", ""]]}, {"id": "2102.03554", "submitter": "Ernie Chang", "authors": "Ernie Chang, Hui-Syuan Yeh, Vera Demberg", "title": "Does the Order of Training Samples Matter? Improving Neural Data-to-Text\n  Generation with Curriculum Learning", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in data-to-text generation largely take on the form of\nneural end-to-end systems. Efforts have been dedicated to improving text\ngeneration systems by changing the order of training samples in a process known\nas curriculum learning. Past research on sequence-to-sequence learning showed\nthat curriculum learning helps to improve both the performance and convergence\nspeed. In this work, we delve into the same idea surrounding the training\nsamples consisting of structured data and text pairs, where at each update, the\ncurriculum framework selects training samples based on the model's competence.\nSpecifically, we experiment with various difficulty metrics and put forward a\nsoft edit distance metric for ranking training samples. Our benchmarks show\nfaster convergence speed where training time is reduced by 38.7% and\nperformance is boosted by 4.84 BLEU.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 10:14:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chang", "Ernie", ""], ["Yeh", "Hui-Syuan", ""], ["Demberg", "Vera", ""]]}, {"id": "2102.03556", "submitter": "Ernie Chang", "authors": "Ernie Chang, Xiaoyu Shen, Dawei Zhu, Vera Demberg, Hui Su", "title": "Neural Data-to-Text Generation with LM-based Text Augmentation", "comments": "Accepted EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many new application domains for data-to-text generation, the main\nobstacle in training neural models consists of a lack of training data. While\nusually large numbers of instances are available on the data side, often only\nvery few text samples are available. To address this problem, we here propose a\nnovel few-shot approach for this setting. Our approach automatically augments\nthe data available for training by (i) generating new text samples based on\nreplacing specific values by alternative ones from the same category, (ii)\ngenerating new text samples based on GPT-2, and (iii) proposing an automatic\nmethod for pairing the new text samples with data samples. As the text\naugmentation can introduce noise to the training data, we use cycle consistency\nas an objective, in order to make sure that a given data sample can be\ncorrectly reconstructed after having been formulated as text (and that text\nsamples can be reconstructed from data). On both the E2E and WebNLG benchmarks,\nwe show that this weakly supervised training paradigm is able to outperform\nfully supervised seq2seq models with less than 10% annotations. By utilizing\nall annotated data, our model can boost the performance of a standard seq2seq\nmodel by over 5 BLEU points, establishing a new state-of-the-art on both\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 10:21:48 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chang", "Ernie", ""], ["Shen", "Xiaoyu", ""], ["Zhu", "Dawei", ""], ["Demberg", "Vera", ""], ["Su", "Hui", ""]]}, {"id": "2102.03596", "submitter": "L\\\"utfi Kerem \\c{S}enel", "authors": "Lutfi Kerem Senel and Hinrich Sch\\\"utze", "title": "Does He Wink or Does He Nod? A Challenging Benchmark for Evaluating Word\n  Understanding of Language Models", "comments": "5 pages, to appear in EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent progress in pretraining language models on large corpora has resulted\nin large performance gains on many NLP tasks. These large models acquire\nlinguistic knowledge during pretraining, which helps to improve performance on\ndownstream tasks via fine-tuning. To assess what kind of knowledge is acquired,\nlanguage models are commonly probed by querying them with `fill in the blank'\nstyle cloze questions. Existing probing datasets mainly focus on knowledge\nabout relations between words and entities. We introduce WDLMPro (Word\nDefinition Language Model Probing) to evaluate word understanding directly\nusing dictionary definitions of words. In our experiments, three popular\npretrained language models struggle to match words and their definitions. This\nindicates that they understand many words poorly and that our new probing task\nis a difficult challenge that could help guide research on LMs in the future.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 15:15:57 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Senel", "Lutfi Kerem", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2102.03662", "submitter": "Francis Tyers", "authors": "Anastasia Kuznetsova and Anurag Kumar and Francis M. Tyers", "title": "A bandit approach to curriculum generation for automatic speech\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Automated Speech Recognition (ASR) task has been a challenging domain\nespecially for low data scenarios with few audio examples. This is the main\nproblem in training ASR systems on the data from low-resource or marginalized\nlanguages. In this paper we present an approach to mitigate the lack of\ntraining data by employing Automated Curriculum Learning in combination with an\nadversarial bandit approach inspired by Reinforcement learning. The goal of the\napproach is to optimize the training sequence of mini-batches ranked by the\nlevel of difficulty and compare the ASR performance metrics against the random\ntraining sequence and discrete curriculum. We test our approach on a truly\nlow-resource language and show that the bandit framework has a good improvement\nover the baseline transfer-learning model.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 20:32:10 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kuznetsova", "Anastasia", ""], ["Kumar", "Anurag", ""], ["Tyers", "Francis M.", ""]]}, {"id": "2102.03671", "submitter": "Anushree Hede", "authors": "Anushree Hede, Oshin Agarwal, Linda Lu, Diana C. Mutz, Ani Nenkova", "title": "From Toxicity in Online Comments to Incivility in American News: Proceed\n  with Caution", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to quantify incivility online, in news and in congressional\ndebates, is of great interest to political scientists. Computational tools for\ndetecting online incivility for English are now fairly accessible and\npotentially could be applied more broadly. We test the Jigsaw Perspective API\nfor its ability to detect the degree of incivility on a corpus that we\ndeveloped, consisting of manual annotations of civility in American news. We\ndemonstrate that toxicity models, as exemplified by Perspective, are inadequate\nfor the analysis of incivility in news. We carry out error analysis that points\nto the need to develop methods to remove spurious correlations between words\noften mentioned in the news, especially identity descriptors and incivility.\nWithout such improvements, applying Perspective or similar models on news is\nlikely to lead to wrong conclusions, that are not aligned with the human\nperception of incivility.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:49:17 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hede", "Anushree", ""], ["Agarwal", "Oshin", ""], ["Lu", "Linda", ""], ["Mutz", "Diana C.", ""], ["Nenkova", "Ani", ""]]}, {"id": "2102.03732", "submitter": "Yankai Lin", "authors": "Zhiyuan Liu, Yankai Lin, Maosong Sun", "title": "Representation Learning for Natural Language Processing", "comments": "Published in Springer", "journal-ref": null, "doi": "10.1007/978-981-15-5573-2", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This book aims to review and present the recent advances of distributed\nrepresentation learning for NLP, including why representation learning can\nimprove NLP, how representation learning takes part in various important topics\nof NLP, and what challenges are still not well addressed by distributed\nrepresentation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 07:37:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Liu", "Zhiyuan", ""], ["Lin", "Yankai", ""], ["Sun", "Maosong", ""]]}, {"id": "2102.03741", "submitter": "Yiming Cui", "authors": "Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, Guoping Hu", "title": "Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question\n  Answering", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving information from correlative paragraphs or documents to answer\nopen-domain multi-hop questions is very challenging. To deal with this\nchallenge, most of the existing works consider paragraphs as nodes in a graph\nand propose graph-based methods to retrieve them. However, in this paper, we\npoint out the intrinsic defect of such methods. Instead, we propose a new\narchitecture that models paragraphs as sequential data and considers multi-hop\ninformation retrieval as a kind of sequence labeling task. Specifically, we\ndesign a rewritable external memory to model the dependency among paragraphs.\nMoreover, a threshold gate mechanism is proposed to eliminate the distraction\nof noise paragraphs. We evaluate our method on both full wiki and distractor\nsubtask of HotpotQA, a public textual multi-hop QA dataset requiring multi-hop\ninformation retrieval. Experiments show that our method achieves significant\nimprovement over the published state-of-the-art method in retrieval and\ndownstream QA task performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 08:15:51 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Shao", "Nan", ""], ["Cui", "Yiming", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "2102.03752", "submitter": "Yusheng Su", "authors": "Yusheng Su, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Peng Li,\n  Jie Zhou, Maosong Sun", "title": "CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of\n  Pre-trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning pre-trained language models (PLMs) has demonstrated its\neffectiveness on various downstream NLP tasks recently. However, in many\nlow-resource scenarios, the conventional fine-tuning strategies cannot\nsufficiently capture the important semantic features for downstream tasks. To\naddress this issue, we introduce a novel framework (named \"CSS-LM\") to improve\nthe fine-tuning phase of PLMs via contrastive semi-supervised learning.\nSpecifically, given a specific task, we retrieve positive and negative\ninstances from large-scale unlabeled corpora according to their domain-level\nand class-level semantic relatedness to the task. We then perform contrastive\nsemi-supervised learning on both the retrieved unlabeled and original labeled\ninstances to help PLMs capture crucial task-related semantic features. The\nexperimental results show that CSS-LM achieves better results than the\nconventional fine-tuning strategy on a series of downstream tasks with few-shot\nsettings, and outperforms the latest supervised contrastive fine-tuning\nstrategies. Our datasets and source code will be available to provide more\ndetails.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 09:27:26 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 08:50:38 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 11:47:00 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Su", "Yusheng", ""], ["Han", "Xu", ""], ["Lin", "Yankai", ""], ["Zhang", "Zhengyan", ""], ["Liu", "Zhiyuan", ""], ["Li", "Peng", ""], ["Zhou", "Jie", ""], ["Sun", "Maosong", ""]]}, {"id": "2102.03762", "submitter": "Catalin Zorila", "authors": "Jisi Zhang, Catalin Zorila, Rama Doddipatla, Jon Barker", "title": "Time-Domain Speech Extraction with Spatial Information and Multi Speaker\n  Conditioning Mechanism", "comments": "Accepted for ICASSP 2021", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9414092", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel multi-channel speech extraction system to\nsimultaneously extract multiple clean individual sources from a mixture in\nnoisy and reverberant environments. The proposed method is built on an improved\nmulti-channel time-domain speech separation network which employs speaker\nembeddings to identify and extract multiple targets without label permutation\nambiguity. To efficiently inform the speaker information to the extraction\nmodel, we propose a new speaker conditioning mechanism by designing an\nadditional speaker branch for receiving external speaker embeddings.\nExperiments on 2-channel WHAMR! data show that the proposed system improves by\n9% relative the source separation performance over a strong multi-channel\nbaseline, and it increases the speech recognition accuracy by more than 16%\nrelative over the same baseline.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 10:11:49 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhang", "Jisi", ""], ["Zorila", "Catalin", ""], ["Doddipatla", "Rama", ""], ["Barker", "Jon", ""]]}, {"id": "2102.03795", "submitter": "Subhradeep Kayal", "authors": "Subhradeep Kayal", "title": "Unsupervised Sentence-embeddings by Manifold Approximation and\n  Projection", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of unsupervised universal sentence encoders has gained traction\nrecently, wherein pre-trained models generate effective task-agnostic\nfixed-dimensional representations for phrases, sentences and paragraphs. Such\nmethods are of varying complexity, from simple weighted-averages of word\nvectors to complex language-models based on bidirectional transformers. In this\nwork we propose a novel technique to generate sentence-embeddings in an\nunsupervised fashion by projecting the sentences onto a fixed-dimensional\nmanifold with the objective of preserving local neighbourhoods in the original\nspace. To delineate such neighbourhoods we experiment with several set-distance\nmetrics, including the recently proposed Word Mover's distance, while the\nfixed-dimensional projection is achieved by employing a scalable and efficient\nmanifold approximation method rooted in topological data analysis. We test our\napproach, which we term EMAP or Embeddings by Manifold Approximation and\nProjection, on six publicly available text-classification datasets of varying\nsize and complexity. Empirical results show that our method consistently\nperforms similar to or better than several alternative state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 13:27:58 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kayal", "Subhradeep", ""]]}, {"id": "2102.03870", "submitter": "Punyajoy Saha", "authors": "Punyajoy Saha, Binny Mathew, Kiran Garimella, Animesh Mukherjee", "title": "\"Short is the Road that Leads from Fear to Hate\": Fear Speech in Indian\n  WhatsApp Groups", "comments": "13 pages, 9 figures, 8 tables, Accepted at The Web Conference 2021,\n  code and dataset public at https://github.com/punyajoy/Fear-Speech-analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WhatsApp is the most popular messaging app in the world. Due to its\npopularity, WhatsApp has become a powerful and cheap tool for political\ncampaigning being widely used during the 2019 Indian general election, where it\nwas used to connect to the voters on a large scale. Along with the campaigning,\nthere have been reports that WhatsApp has also become a breeding ground for\nharmful speech against various protected groups and religious minorities. Many\nsuch messages attempt to instil fear among the population about a specific\n(minority) community. According to research on inter-group conflict, such `fear\nspeech' messages could have a lasting impact and might lead to real offline\nviolence. In this paper, we perform the first large scale study on fear speech\nacross thousands of public WhatsApp groups discussing politics in India. We\ncurate a new dataset and try to characterize fear speech from this dataset. We\nobserve that users writing fear speech messages use various events and symbols\nto create the illusion of fear among the reader about a target community. We\nbuild models to classify fear speech and observe that current state-of-the-art\nNLP models do not perform well at this task. Fear speech messages tend to\nspread faster and could potentially go undetected by classifiers built to\ndetect traditional toxic speech due to their low toxic nature. Finally, using a\nnovel methodology to target users with Facebook ads, we conduct a survey among\nthe users of these WhatsApp groups to understand the types of users who consume\nand share fear speech. We believe that this work opens up new research\nquestions that are very different from tackling hate speech which the research\ncommunity has been traditionally involved in.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 18:14:16 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Saha", "Punyajoy", ""], ["Mathew", "Binny", ""], ["Garimella", "Kiran", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2102.03874", "submitter": "Wlodek Zadrozny", "authors": "Wlodek W. Zadrozny", "title": "A Note on Argumentative Topology: Circularity and Syllogisms as Unsolved\n  Problems", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last couple of years there were a few attempts to apply topological\ndata analysis to text, and in particular to natural language inference. A\nrecent work by Tymochko et al. suggests the possibility of capturing `the\nnotion of logical shape in text,' using `topological delay embeddings,' a\ntechnique derived from dynamical systems, applied to word embeddings.\n  In this note we reconstruct their argument and show, using several old and\nnew examples, that the problem of connecting logic, topology and text is still\nvery much unsolved. We conclude that there is no clear answer to the question:\n``Can we find a circle in a circular argument?'' We point out some possible\navenues of exploration. The code used in our experiment is also shown.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 18:30:37 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zadrozny", "Wlodek W.", ""]]}, {"id": "2102.03882", "submitter": "Marshall Ho", "authors": "Allen Bao, Marshall Ho, Saarthak Sangamnerkar", "title": "Spoiler Alert: Using Natural Language Processing to Detect Spoilers in\n  Book Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an NLP (Natural Language Processing) approach to\ndetecting spoilers in book reviews, using the University of California San\nDiego (UCSD) Goodreads Spoiler dataset. We explored the use of LSTM, BERT, and\nRoBERTa language models to perform spoiler detection at the sentence-level.\nThis was contrasted with a UCSD paper which performed the same task, but using\nhandcrafted features in its data preparation. Despite eschewing the use of\nhandcrafted features, our results from the LSTM model were able to slightly\nexceed the UCSD team's performance in spoiler detection.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 18:54:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bao", "Allen", ""], ["Ho", "Marshall", ""], ["Sangamnerkar", "Saarthak", ""]]}, {"id": "2102.03902", "submitter": "Yunyang Xiong", "authors": "Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan,\n  Glenn Fung, Yin Li, Vikas Singh", "title": "Nystr\\\"omformer: A Nystr\\\"om-Based Algorithm for Approximating\n  Self-Attention", "comments": "AAAI 2021; Code and supplement available at\n  https://github.com/mlpen/Nystromformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have emerged as a powerful tool for a broad range of natural\nlanguage processing tasks. A key component that drives the impressive\nperformance of Transformers is the self-attention mechanism that encodes the\ninfluence or dependence of other tokens on each specific token. While\nbeneficial, the quadratic complexity of self-attention on the input sequence\nlength has limited its application to longer sequences -- a topic being\nactively studied in the community. To address this limitation, we propose\nNystr\\\"{o}mformer -- a model that exhibits favorable scalability as a function\nof sequence length. Our idea is based on adapting the Nystr\\\"{o}m method to\napproximate standard self-attention with $O(n)$ complexity. The scalability of\nNystr\\\"{o}mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE\nbenchmark and IMDB reviews with standard sequence length, and find that our\nNystr\\\"{o}mformer performs comparably, or in a few cases, even slightly better,\nthan standard self-attention. On longer sequence tasks in the Long Range Arena\n(LRA) benchmark, Nystr\\\"{o}mformer performs favorably relative to other\nefficient self-attention methods. Our code is available at\nhttps://github.com/mlpen/Nystromformer.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 20:06:59 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 18:45:37 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 20:40:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Xiong", "Yunyang", ""], ["Zeng", "Zhanpeng", ""], ["Chakraborty", "Rudrasis", ""], ["Tan", "Mingxing", ""], ["Fung", "Glenn", ""], ["Li", "Yin", ""], ["Singh", "Vikas", ""]]}, {"id": "2102.03951", "submitter": "Feng-Ju Chang", "authors": "Feng-Ju Chang, Martin Radfar, Athanasios Mouchtaris, Brian King, and\n  Siegfried Kunzmann", "title": "End-to-End Multi-Channel Transformer for Speech Recognition", "comments": "Accepted by 2021 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are powerful neural architectures that allow integrating\ndifferent modalities using attention mechanisms. In this paper, we leverage the\nneural transformer architectures for multi-channel speech recognition systems,\nwhere the spectral and spatial information collected from different microphones\nare integrated using attention layers. Our multi-channel transformer network\nmainly consists of three parts: channel-wise self attention layers (CSA),\ncross-channel attention layers (CCA), and multi-channel encoder-decoder\nattention layers (EDA). The CSA and CCA layers encode the contextual\nrelationship within and between channels and across time, respectively. The\nchannel-attended outputs from CSA and CCA are then fed into the EDA layers to\nhelp decode the next token given the preceding ones. The experiments show that\nin a far-field in-house dataset, our method outperforms the baseline\nsingle-channel transformer, as well as the super-directive and neural\nbeamformers cascaded with the transformers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 00:12:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chang", "Feng-Ju", ""], ["Radfar", "Martin", ""], ["Mouchtaris", "Athanasios", ""], ["King", "Brian", ""], ["Kunzmann", "Siegfried", ""]]}, {"id": "2102.04009", "submitter": "Liang Ding", "authors": "Di Wu, Liang Ding, Shuo Yang, Dacheng Tao", "title": "SLUA: A Super Lightweight Unsupervised Word Alignment Model via\n  Cross-Lingual Contrastive Learning", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Word alignment is essential for the down-streaming cross-lingual language\nunderstanding and generation tasks. Recently, the performance of the neural\nword alignment models has exceeded that of statistical models. However, they\nheavily rely on sophisticated translation models. In this study, we propose a\nsuper lightweight unsupervised word alignment (SLUA) model, in which\nbidirectional symmetric attention trained with a contrastive learning objective\nis introduced, and an agreement loss is employed to bind the attention maps,\nsuch that the alignments follow mirror-like symmetry hypothesis. Experimental\nresults on several public benchmarks demonstrate that our model achieves\ncompetitive, if not better, performance compared to the state of the art in\nword alignment while significantly reducing the training and decoding time on\naverage. Further ablation analysis and case studies show the superiority of our\nproposed SLUA. Notably, we recognize our model as a pioneer attempt to unify\nbilingual word embedding and word alignments. Encouragingly, our approach\nachieves 16.4x speedup against GIZA++, and 50x parameter compression} compared\nwith the Transformer-based alignment methods. We will release our code to\nfacilitate the community.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 05:54:11 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 17:35:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wu", "Di", ""], ["Ding", "Liang", ""], ["Yang", "Shuo", ""], ["Tao", "Dacheng", ""]]}, {"id": "2102.04020", "submitter": "Yi-Lin Tuan", "authors": "Yi-Lin Tuan, Ahmed El-Kishky, Adithya Renduchintala, Vishrav\n  Chaudhary, Francisco Guzm\\'an, Lucia Specia", "title": "Quality Estimation without Human-labeled Data", "comments": "Accepted by EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality estimation aims to measure the quality of translated content without\naccess to a reference translation. This is crucial for machine translation\nsystems in real-world scenarios where high-quality translation is needed. While\nmany approaches exist for quality estimation, they are based on supervised\nmachine learning requiring costly human labelled data. As an alternative, we\npropose a technique that does not rely on examples from human-annotators and\ninstead uses synthetic training data. We train off-the-shelf architectures for\nsupervised quality estimation on our synthetic data and show that the resulting\nmodels achieve comparable performance to models trained on human-annotated\ndata, both for sentence and word-level prediction.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 06:25:46 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Tuan", "Yi-Lin", ""], ["El-Kishky", "Ahmed", ""], ["Renduchintala", "Adithya", ""], ["Chaudhary", "Vishrav", ""], ["Guzm\u00e1n", "Francisco", ""], ["Specia", "Lucia", ""]]}, {"id": "2102.04065", "submitter": "Yang Wei", "authors": "Yang Wei, Yuanbin Wu and Man Lan", "title": "In-Order Chart-Based Constituent Parsing", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel in-order chart-based model for constituent parsing.\nCompared with previous CKY-style and top-down models, our model gains\nadvantages from in-order traversal of a tree (rich features, lookahead\ninformation and high efficiency) and makes a better use of structural knowledge\nby encoding the history of decisions. Experiments on the Penn Treebank show\nthat our model outperforms previous chart-based models and achieves competitive\nperformance compared with other discriminative single models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 09:03:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wei", "Yang", ""], ["Wu", "Yuanbin", ""], ["Lan", "Man", ""]]}, {"id": "2102.04081", "submitter": "Vivek Iyer", "authors": "Lalit Mohan Sanagavarapu, Vivek Iyer and Y Raghu Reddy", "title": "OntoEnricher: A Deep Learning Approach for Ontology Enrichment from\n  Unstructured Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information Security in the cyber world is a major cause for concern, with\nsignificant increase in the number of attack surfaces. Existing information on\nvulnerabilities, attacks, controls, and advisories available on the web\nprovides an opportunity to represent knowledge and perform security analytics\nto mitigate some of the concerns. Representing security knowledge in the form\nof ontology facilitates anomaly detection, threat intelligence, reasoning and\nrelevance attribution of attacks, and many more. This necessitates dynamic and\nautomated enrichment of information security ontologies. However, existing\nontology enrichment algorithms based on natural language processing and ML\nmodels have issues with the contextual extraction of concepts in words, phrases\nand sentences. This motivates the need for sequential Deep Learning\narchitectures that traverse through dependency paths in text and extract\nembedded vulnerabilities, threats, controls, products and other security\nrelated concepts and instances from learned path representations. In the\nproposed approach, Bidirectional LSTMs trained on a large DBpedia dataset and\nWikipedia corpus of 2.8 GB along with Universal Sentence Encoder was deployed\nto enrich ISO 27001 based information security ontology. The approach yielded a\ntest accuracy of over 80\\% when tested with knocked out concepts from ontology\nand web page instances to validate the robustness.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 09:43:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sanagavarapu", "Lalit Mohan", ""], ["Iyer", "Vivek", ""], ["Reddy", "Y Raghu", ""]]}, {"id": "2102.04097", "submitter": "Onno Eberhard", "authors": "Onno Eberhard and Torsten Zesch", "title": "Effects of Layer Freezing when Transferring DeepSpeech to New Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we train Mozilla's DeepSpeech architecture on German and Swiss\nGerman speech datasets and compare the results of different training methods.\nWe first train the models from scratch on both languages and then improve upon\nthe results by using an English pretrained version of DeepSpeech for weight\ninitialization and experiment with the effects of freezing different layers\nduring training. We see that even freezing only one layer already improves the\nresults dramatically.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:05:22 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Eberhard", "Onno", ""], ["Zesch", "Torsten", ""]]}, {"id": "2102.04110", "submitter": "Betty van Aken", "authors": "Betty van Aken, Jens-Michalis Papaioannou, Manuel Mayrdorfer, Klemens\n  Budde, Felix A. Gers, Alexander L\\\"oser", "title": "Clinical Outcome Prediction from Admission Notes using Self-Supervised\n  Knowledge Integration", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outcome prediction from clinical text can prevent doctors from overlooking\npossible risks and help hospitals to plan capacities. We simulate patients at\nadmission time, when decision support can be especially valuable, and\ncontribute a novel admission to discharge task with four common outcome\nprediction targets: Diagnoses at discharge, procedures performed, in-hospital\nmortality and length-of-stay prediction. The ideal system should infer outcomes\nbased on symptoms, pre-conditions and risk factors of a patient. We evaluate\nthe effectiveness of language models to handle this scenario and propose\nclinical outcome pre-training to integrate knowledge about patient outcomes\nfrom multiple public sources. We further present a simple method to incorporate\nICD code hierarchy into the models. We show that our approach improves\nperformance on the outcome tasks against several baselines. A detailed analysis\nreveals further strengths of the model, including transferability, but also\nweaknesses such as handling of vital values and inconsistencies in the\nunderlying data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:26:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["van Aken", "Betty", ""], ["Papaioannou", "Jens-Michalis", ""], ["Mayrdorfer", "Manuel", ""], ["Budde", "Klemens", ""], ["Gers", "Felix A.", ""], ["L\u00f6ser", "Alexander", ""]]}, {"id": "2102.04114", "submitter": "Luca Pasqualini", "authors": "Andrea Zugarini, Luca Pasqualini, Stefano Melacci, Marco Maggini", "title": "Generate and Revise: Reinforcement Learning in Neural Poetry", "comments": "12 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writers, poets, singers usually do not create their compositions in just one\nbreath. Text is revisited, adjusted, modified, rephrased, even multiple times,\nin order to better convey meanings, emotions and feelings that the author wants\nto express. Amongst the noble written arts, Poetry is probably the one that\nneeds to be elaborated the most, since the composition has to formally respect\npredefined meter and rhyming schemes. In this paper, we propose a framework to\ngenerate poems that are repeatedly revisited and corrected, as humans do, in\norder to improve their overall quality. We frame the problem of revising poems\nin the context of Reinforcement Learning and, in particular, using Proximal\nPolicy Optimization. Our model generates poems from scratch and it learns to\nprogressively adjust the generated text in order to match a target criterion.\nWe evaluate this approach in the case of matching a rhyming scheme, without\nhaving any information on which words are responsible of creating rhymes and on\nhow to coherently alter the poem words. The proposed framework is general and,\nwith an appropriate reward shaping, it can be applied to other text generation\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:35:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zugarini", "Andrea", ""], ["Pasqualini", "Luca", ""], ["Melacci", "Stefano", ""], ["Maggini", "Marco", ""]]}, {"id": "2102.04130", "submitter": "Yuki Asano", "authors": "Hannah Kirk, Yennie Jun, Haider Iqbal, Elias Benussi, Filippo Volpin,\n  Frederic A. Dreyer, Aleksandar Shtedritski, Yuki M. Asano", "title": "How True is GPT-2? An Empirical Analysis of Intersectional Occupational\n  Biases", "comments": "Code is available at https://github.com/oxai/intersectional_gpt2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Downstream applications are at\nrisk of inheriting biases contained in these models, with potential negative\nconsequences especially for marginalized groups. In this paper, we analyze the\noccupational biases of a popular generative language model, GPT-2, intersecting\ngender with five protected categories: religion, sexuality, ethnicity,\npolitical affiliation, and name origin. Using a novel data collection pipeline\nwe collect 396k sentence completions of GPT-2 and find: (i) The\nmachine-predicted jobs are less diverse and more stereotypical for women than\nfor men, especially for intersections; (ii) Fitting 262 logistic models shows\nintersectional interactions to be highly relevant for occupational\nassociations; (iii) For a given job, GPT-2 reflects the societal skew of gender\nand ethnicity in the US, and in some cases, pulls the distribution towards\ngender parity, raising the normative question of what language models _should_\nlearn.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 11:10:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kirk", "Hannah", ""], ["Jun", "Yennie", ""], ["Iqbal", "Haider", ""], ["Benussi", "Elias", ""], ["Volpin", "Filippo", ""], ["Dreyer", "Frederic A.", ""], ["Shtedritski", "Aleksandar", ""], ["Asano", "Yuki M.", ""]]}, {"id": "2102.04310", "submitter": "Magnus Sahlgren", "authors": "Magnus Sahlgren, Fredrik Carlsson", "title": "The Singleton Fallacy: Why Current Critiques of Language Models Miss the\n  Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the current critique against neural network-based\nNatural Language Understanding (NLU) solutions known as language models. We\nargue that much of the current debate rests on an argumentation error that we\nwill refer to as the singleton fallacy: the assumption that language, meaning,\nand understanding are single and uniform phenomena that are unobtainable by\n(current) language models. By contrast, we will argue that there are many\ndifferent types of language use, meaning and understanding, and that (current)\nlanguage models are build with the explicit purpose of acquiring and\nrepresenting one type of structural understanding of language. We will argue\nthat such structural understanding may cover several different modalities, and\nas such can handle several different types of meaning. Our position is that we\ncurrently see no theoretical reason why such structural knowledge would be\ninsufficient to count as \"real\" understanding.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:12:36 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sahlgren", "Magnus", ""], ["Carlsson", "Fredrik", ""]]}, {"id": "2102.04427", "submitter": "Austin Wright", "authors": "Austin P Wright, Omar Shaikh, Haekyu Park, Will Epperson, Muhammed\n  Ahmed, Stephane Pinel, Duen Horng Chau, Diyi Yang", "title": "RECAST: Enabling User Recourse and Interpretability of Toxicity\n  Detection Models with Interactive Visualization", "comments": "26 pages, 5 figures, CSCW '21", "journal-ref": null, "doi": "10.1145/3449280", "report-no": null, "categories": "cs.HC cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of toxic language online, platforms are increasingly\nusing automated systems that leverage advances in natural language processing\nto automatically flag and remove toxic comments. However, most automated\nsystems -- when detecting and moderating toxic language -- do not provide\nfeedback to their users, let alone provide an avenue of recourse for these\nusers to make actionable changes. We present our work, RECAST, an interactive,\nopen-sourced web tool for visualizing these models' toxic predictions, while\nproviding alternative suggestions for flagged toxic language. Our work also\nprovides users with a new path of recourse when using these automated\nmoderation tools. RECAST highlights text responsible for classifying toxicity,\nand allows users to interactively substitute potentially toxic phrases with\nneutral alternatives. We examined the effect of RECAST via two large-scale user\nevaluations, and found that RECAST was highly effective at helping users reduce\ntoxicity as detected through the model. Users also gained a stronger\nunderstanding of the underlying toxicity criterion used by black-box models,\nenabling transparency and recourse. In addition, we found that when users focus\non optimizing language for these models instead of their own judgement (which\nis the implied incentive and goal of deploying automated models), these models\ncease to be effective classifiers of toxicity compared to human annotations.\nThis opens a discussion for how toxicity detection models work and should work,\nand their effect on the future of online discourse.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:37:50 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 14:42:17 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Wright", "Austin P", ""], ["Shaikh", "Omar", ""], ["Park", "Haekyu", ""], ["Epperson", "Will", ""], ["Ahmed", "Muhammed", ""], ["Pinel", "Stephane", ""], ["Chau", "Duen Horng", ""], ["Yang", "Diyi", ""]]}, {"id": "2102.04488", "submitter": "Yiming Wang", "authors": "Yiming Wang, Hang Lv, Daniel Povey, Lei Xie, Sanjeev Khudanpur", "title": "Wake Word Detection with Streaming Transformers", "comments": "Accepted at IEEE ICASSP 2021. 5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern wake word detection systems usually rely on neural networks for\nacoustic modeling. Transformers has recently shown superior performance over\nLSTM and convolutional networks in various sequence modeling tasks with their\nbetter temporal modeling power. However it is not clear whether this advantage\nstill holds for short-range temporal modeling like wake word detection.\nBesides, the vanilla Transformer is not directly applicable to the task due to\nits non-streaming nature and the quadratic time and space complexity. In this\npaper we explore the performance of several variants of chunk-wise streaming\nTransformers tailored for wake word detection in a recently proposed LF-MMI\nsystem, including looking-ahead to the next chunk, gradient stopping, different\npositional embedding methods and adding same-layer dependency between chunks.\nOur experiments on the Mobvoi wake word dataset demonstrate that our proposed\nTransformer model outperforms the baseline convolution network by 25% on\naverage in false rejection rate at the same false alarm rate with a comparable\nmodel size, while still maintaining linear complexity w.r.t. the sequence\nlength.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 19:14:32 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wang", "Yiming", ""], ["Lv", "Hang", ""], ["Povey", "Daniel", ""], ["Xie", "Lei", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2102.04490", "submitter": "Mir Tafseer Nayeem", "authors": "Radia Rayan Chowdhury, Mir Tafseer Nayeem, Tahsin Tasnim Mim, Md.\n  Saifur Rahman Chowdhury, Taufiqul Jannat", "title": "Unsupervised Abstractive Summarization of Bengali Text Documents", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstractive summarization systems generally rely on large collections of\ndocument-summary pairs. However, the performance of abstractive systems remains\na challenge due to the unavailability of parallel data for low-resource\nlanguages like Bengali. To overcome this problem, we propose a graph-based\nunsupervised abstractive summarization system in the single-document setting\nfor Bengali text documents, which requires only a Part-Of-Speech (POS) tagger\nand a pre-trained language model trained on Bengali texts. We also provide a\nhuman-annotated dataset with document-summary pairs to evaluate our abstractive\nmodel and to support the comparison of future abstractive summarization systems\nof the Bengali Language. We conduct experiments on this dataset and compare our\nsystem with several well-established unsupervised extractive summarization\nsystems. Our unsupervised abstractive summarization model outperforms the\nbaselines without being exposed to any human-annotated reference summaries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:41:28 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 16:37:51 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Chowdhury", "Radia Rayan", ""], ["Nayeem", "Mir Tafseer", ""], ["Mim", "Tahsin Tasnim", ""], ["Chowdhury", "Md. Saifur Rahman", ""], ["Jannat", "Taufiqul", ""]]}, {"id": "2102.04506", "submitter": "Boliang Zhang", "authors": "Boliang Zhang, Ying Lyu, Ning Ding, Tianhao Shen, Zhaoyang Jia, Kun\n  Han, Kevin Knight", "title": "A Hybrid Task-Oriented Dialog System with Domain and Task Adaptive\n  Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our submission for the End-to-end Multi-domain Task\nCompletion Dialog shared task at the 9th Dialog System Technology Challenge\n(DSTC-9). Participants in the shared task build an end-to-end task completion\ndialog system which is evaluated by human evaluation and a user simulator based\nautomatic evaluation. Different from traditional pipelined approaches where\nmodules are optimized individually and suffer from cascading failure, we\npropose an end-to-end dialog system that 1) uses Generative Pretraining 2\n(GPT-2) as the backbone to jointly solve Natural Language Understanding, Dialog\nState Tracking, and Natural Language Generation tasks, 2) adopts Domain and\nTask Adaptive Pretraining to tailor GPT-2 to the dialog domain before\nfinetuning, 3) utilizes heuristic pre/post-processing rules that greatly\nsimplify the prediction tasks and improve generalizability, and 4) equips a\nfault tolerance module to correct errors and inappropriate responses. Our\nproposed method significantly outperforms baselines and ties for first place in\nthe official evaluation. We make our source code publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 20:02:30 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Boliang", ""], ["Lyu", "Ying", ""], ["Ding", "Ning", ""], ["Shen", "Tianhao", ""], ["Jia", "Zhaoyang", ""], ["Han", "Kun", ""], ["Knight", "Kevin", ""]]}, {"id": "2102.04521", "submitter": "Chrysoula Themeli", "authors": "Chrysoula Themeli, George Giannakopoulos and Nikiforos Pittaras", "title": "A study of text representations in Hate Speech Detection", "comments": "14 pages, CICLing2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pervasiveness of the Internet and social media have enabled the rapid and\nanonymous spread of Hate Speech content on microblogging platforms such as\nTwitter. Current EU and US legislation against hateful language, in conjunction\nwith the large amount of data produced in these platforms has led to automatic\ntools being a necessary component of the Hate Speech detection task and\npipeline. In this study, we examine the performance of several, diverse text\nrepresentation techniques paired with multiple classification algorithms, on\nthe automatic Hate Speech detection and abusive language discrimination task.\nWe perform an experimental evaluation on binary and multiclass datasets, paired\nwith significance testing. Our results show that simple hate-keyword frequency\nfeatures (BoW) work best, followed by pre-trained word embeddings (GLoVe) as\nwell as N-gram graphs (NGGs): a graph-based representation which proved to\nproduce efficient, very low-dimensional but rich features for this task. A\ncombination of these representations paired with Logistic Regression or 3-layer\nneural network classifiers achieved the best detection performance, in terms of\nmicro and macro F-measure.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 20:39:17 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Themeli", "Chrysoula", ""], ["Giannakopoulos", "George", ""], ["Pittaras", "Nikiforos", ""]]}, {"id": "2102.04588", "submitter": "Debasish Mohapatra", "authors": "Debasish Ray Mohapatra, Victor Zappi, Sidney Fels", "title": "A comparative study of two-dimensional vocal tract acoustic modeling\n  based on Finite-Difference Time-Domain methods", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The two-dimensional (2D) numerical approaches for vocal tract (VT) modelling\ncan afford a better balance between the low computational cost and accurate\nrendering of acoustic wave propagation. However, they require a high\nspatio-temporal resolution in the numerical scheme for a precise estimation of\nacoustic formants at the simulation run-time expense. We have recently proposed\na new VT acoustic modelling technique, known as the 2.5D Finite-Difference\nTime-Domain (2.5D FDTD), which extends the existing 2D FDTD approach by adding\ntube depth to its acoustic wave solver. In this work, first, the simulated\nacoustic outputs of our new model are shown to be comparable with the 2D FDTD\nand a realistic 3D FEM VT model at a low spatio-temporal resolution. Next, a\nradiation model is developed by including a circular baffle around the VT as\nhead geometry. The transfer functions of the radiation model are analyzed using\nfive different vocal tract shapes for vowel sounds /a/, /e/, /i/, /o/ and /u/.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 00:40:52 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mohapatra", "Debasish Ray", ""], ["Zappi", "Victor", ""], ["Fels", "Sidney", ""]]}, {"id": "2102.04610", "submitter": "Pengfei Wei", "authors": "Pengfei Wei, Bi Zeng and Wenxiong Liao", "title": "Joint Intent Detection and Slot Filling with Wheel-Graph Attention\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent detection and slot filling are two fundamental tasks for building a\nspoken language understanding (SLU) system. Multiple deep learning-based joint\nmodels have demonstrated excellent results on the two tasks. In this paper, we\npropose a new joint model with a wheel-graph attention network (Wheel-GAT)\nwhich is able to model interrelated connections directly for intent detection\nand slot filling. To construct a graph structure for utterances, we create\nintent nodes, slot nodes, and directed edges. Intent nodes can provide\nutterance-level semantic information for slot filling, while slot nodes can\nalso provide local keyword information for intent. Experiments show that our\nmodel outperforms multiple baselines on two public datasets. Besides, we also\ndemonstrate that using Bidirectional Encoder Representation from Transformer\n(BERT) model further boosts the performance in the SLU task.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 02:37:56 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wei", "Pengfei", ""], ["Zeng", "Bi", ""], ["Liao", "Wenxiong", ""]]}, {"id": "2102.04632", "submitter": "Shanshan Huang", "authors": "Shanshan Huang and Kenny Q. Zhu", "title": "Statistically Profiling Biases in Natural Language Reasoning Datasets\n  and Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has indicated that many natural language understanding and\nreasoning datasets contain statistical cues that may be taken advantaged of by\nNLP models whose capability may thus be grossly overestimated. To discover the\npotential weakness in the models, some human-designed stress tests have been\nproposed but they are expensive to create and do not generalize to arbitrary\nmodels. We propose a light-weight and general statistical profiling framework,\nICQ (I-See-Cue), which automatically identifies possible biases in any\nmultiple-choice NLU datasets without the need to create any additional test\ncases, and further evaluates through blackbox testing the extent to which\nmodels may exploit these biases.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 03:51:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Huang", "Shanshan", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2102.04643", "submitter": "David Thulke", "authors": "David Thulke, Nico Daheim, Christian Dugast, Hermann Ney", "title": "Efficient Retrieval Augmented Generation from Unstructured Knowledge for\n  Task-Oriented Dialog", "comments": "Accepted by DSTC9 Workshop at AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our work on the first track of the ninth Dialog System\nTechnology Challenge (DSTC 9), \"Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access\". The goal of the\ntask is to generate responses to user turns in a task-oriented dialog that\nrequire knowledge from unstructured documents. The task is divided into three\nsubtasks: detection, selection and generation. In order to be compute\nefficient, we formulate the selection problem in terms of hierarchical\nclassification steps. We achieve our best results with this model.\nAlternatively, we employ siamese sequence embedding models, referred to as\nDense Knowledge Retrieval, to retrieve relevant documents. This method further\nreduces the computation time by a factor of more than 100x at the cost of\ndegradation in R@1 of 5-6% compared to the first model. Then for either\napproach, we use Retrieval Augmented Generation to generate responses based on\nmultiple selected snippets and we show how the method can be used to fine-tune\ntrained embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:50:35 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Thulke", "David", ""], ["Daheim", "Nico", ""], ["Dugast", "Christian", ""], ["Ney", "Hermann", ""]]}, {"id": "2102.04664", "submitter": "Shuai Lu", "authors": "Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,\n  Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,\n  Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou,\n  Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation", "comments": "14 pages; Revise CodeBLEU scores for all models on text-to-code task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmark datasets have a significant impact on accelerating research in\nprogramming language tasks. In this paper, we introduce CodeXGLUE, a benchmark\ndataset to foster machine learning research for program understanding and\ngeneration. CodeXGLUE includes a collection of 10 tasks across 14 datasets and\na platform for model evaluation and comparison. CodeXGLUE also features three\nbaseline systems, including the BERT-style, GPT-style, and Encoder-Decoder\nmodels, to make it easy for researchers to use the platform. The availability\nof such data and baselines can help the development and validation of new\nmethods that can be applied to various program understanding and generation\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 06:16:25 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 08:28:37 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lu", "Shuai", ""], ["Guo", "Daya", ""], ["Ren", "Shuo", ""], ["Huang", "Junjie", ""], ["Svyatkovskiy", "Alexey", ""], ["Blanco", "Ambrosio", ""], ["Clement", "Colin", ""], ["Drain", "Dawn", ""], ["Jiang", "Daxin", ""], ["Tang", "Duyu", ""], ["Li", "Ge", ""], ["Zhou", "Lidong", ""], ["Shou", "Linjun", ""], ["Zhou", "Long", ""], ["Tufano", "Michele", ""], ["Gong", "Ming", ""], ["Zhou", "Ming", ""], ["Duan", "Nan", ""], ["Sundaresan", "Neel", ""], ["Deng", "Shao Kun", ""], ["Fu", "Shengyu", ""], ["Liu", "Shujie", ""]]}, {"id": "2102.04708", "submitter": "Hang Liu", "authors": "Hang Liu, Meng Chen, Youzheng Wu, Xiaodong He, Bowen Zhou", "title": "Conversational Query Rewriting with Self-supervised Learning", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context modeling plays a critical role in building multi-turn dialogue\nsystems. Conversational Query Rewriting (CQR) aims to simplify the multi-turn\ndialogue modeling into a single-turn problem by explicitly rewriting the\nconversational query into a self-contained utterance. However, existing\napproaches rely on massive supervised training data, which is labor-intensive\nto annotate. And the detection of the omitted important information from\ncontext can be further improved. Besides, intent consistency constraint between\ncontextual query and rewritten query is also ignored. To tackle these issues,\nwe first propose to construct a large-scale CQR dataset automatically via\nself-supervised learning, which does not need human annotation. Then we\nintroduce a novel CQR model Teresa based on Transformer, which is enhanced by\nself-attentive keywords detection and intent consistency constraint. Finally,\nwe conduct extensive experiments on two public datasets. Experimental results\ndemonstrate that our proposed model outperforms existing CQR baselines\nsignificantly, and also prove the effectiveness of self-supervised learning on\nimproving the CQR performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 08:57:53 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 07:05:08 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Liu", "Hang", ""], ["Chen", "Meng", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "2102.04754", "submitter": "Boyang Xue", "authors": "Boyang Xue, Jianwei Yu, Junhao Xu, Shansong Liu, Shoukang Hu, Zi Ye,\n  Mengzhe Geng, Xunying Liu, Helen Meng", "title": "Bayesian Transformer Language Models for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art neural language models (LMs) represented by Transformers are\nhighly complex. Their use of fixed, deterministic parameter estimates fail to\naccount for model uncertainty and lead to over-fitting and poor generalization\nwhen given limited training data. In order to address these issues, this paper\nproposes a full Bayesian learning framework for Transformer LM estimation.\nEfficient variational inference based approaches are used to estimate the\nlatent parameter posterior distributions associated with different parts of the\nTransformer model architecture including multi-head self-attention, feed\nforward and embedding layers. Statistically significant word error rate (WER)\nreductions up to 0.5\\% absolute (3.18\\% relative) and consistent perplexity\ngains were obtained over the baseline Transformer LMs on state-of-the-art\nSwitchboard corpus trained LF-MMI factored TDNN systems with i-Vector speaker\nadaptation. Performance improvements were also obtained on a cross domain LM\nadaptation task requiring porting a Transformer LM trained on the Switchboard\nand Fisher data to a low-resource DementiaBank elderly speech corpus.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 10:55:27 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Xue", "Boyang", ""], ["Yu", "Jianwei", ""], ["Xu", "Junhao", ""], ["Liu", "Shansong", ""], ["Hu", "Shoukang", ""], ["Ye", "Zi", ""], ["Geng", "Mengzhe", ""], ["Liu", "Xunying", ""], ["Meng", "Helen", ""]]}, {"id": "2102.04811", "submitter": "Rog\\'erio Duarte", "authors": "Rog\\'erio Duarte, \\^Angela Lacerda Nobre, Fernando Pimentel, Marc\n  Jacquinet", "title": "Broader terms curriculum mapping: Using natural language processing and\n  visual-supported communication to create representative program planning\n  experiences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accreditation bodies call for curriculum development processes open to all\nstakeholders, reflecting viewpoints of students, industry, university faculty\nand society. However, communication difficulties between faculty and\nnon-faculty groups leave unexplored an immense collaboration potential. Using\nclassification of learning objectives, natural language processing, and data\nvisualization, this paper presents a method to deliver program plan\nrepresentations that are universal, self-explanatory, and empowering. A simple\nexample shows how the method contributes to representative program planning\nexperiences and a case study is used to confirm the method's accuracy and\nutility.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 13:27:04 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 09:05:48 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Duarte", "Rog\u00e9rio", ""], ["Nobre", "\u00c2ngela Lacerda", ""], ["Pimentel", "Fernando", ""], ["Jacquinet", "Marc", ""]]}, {"id": "2102.04830", "submitter": "Wenmeng Yu", "authors": "Wenmeng Yu, Hua Xu, Ziqi Yuan, Jiele Wu", "title": "Learning Modality-Specific Representations with Self-Supervised\n  Multi-Task Learning for Multimodal Sentiment Analysis", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Representation Learning is a significant and challenging task in multimodal\nlearning. Effective modality representations should contain two parts of\ncharacteristics: the consistency and the difference. Due to the unified\nmultimodal annotation, existing methods are restricted in capturing\ndifferentiated information. However, additional uni-modal annotations are high\ntime- and labor-cost. In this paper, we design a label generation module based\non the self-supervised learning strategy to acquire independent unimodal\nsupervisions. Then, joint training the multi-modal and uni-modal tasks to learn\nthe consistency and difference, respectively. Moreover, during the training\nstage, we design a weight-adjustment strategy to balance the learning progress\namong different subtasks. That is to guide the subtasks to focus on samples\nwith a larger difference between modality supervisions. Last, we conduct\nextensive experiments on three public multimodal baseline datasets. The\nexperimental results validate the reliability and stability of auto-generated\nunimodal supervisions. On MOSI and MOSEI datasets, our method surpasses the\ncurrent state-of-the-art methods. On the SIMS dataset, our method achieves\ncomparable performance than human-annotated unimodal labels. The full codes are\navailable at https://github.com/thuiar/Self-MM.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 14:05:02 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Yu", "Wenmeng", ""], ["Xu", "Hua", ""], ["Yuan", "Ziqi", ""], ["Wu", "Jiele", ""]]}, {"id": "2102.04887", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, Qi Liu", "title": "NewsBERT: Distilling Pre-trained Language Model for Intelligent News\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models (PLMs) like BERT have made great progress in NLP.\nNews articles usually contain rich textual information, and PLMs have the\npotentials to enhance news text modeling for various intelligent news\napplications like news recommendation and retrieval. However, most existing\nPLMs are in huge size with hundreds of millions of parameters. Many online news\napplications need to serve millions of users with low latency tolerance, which\nposes huge challenges to incorporating PLMs in these scenarios. Knowledge\ndistillation techniques can compress a large PLM into a much smaller one and\nmeanwhile keeps good performance. However, existing language models are\npre-trained and distilled on general corpus like Wikipedia, which has some gaps\nwith the news domain and may be suboptimal for news intelligence. In this\npaper, we propose NewsBERT, which can distill PLMs for efficient and effective\nnews intelligence. In our approach, we design a teacher-student joint learning\nand distillation framework to collaboratively learn both teacher and student\nmodels, where the student model can learn from the learning experience of the\nteacher model. In addition, we propose a momentum distillation method by\nincorporating the gradients of teacher model into the update of student model\nto better transfer useful knowledge learned by the teacher model. Extensive\nexperiments on two real-world datasets with three tasks show that NewsBERT can\neffectively improve the model performance in various intelligent news\napplications with much smaller models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:41:12 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Yu", "Yang", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""], ["Liu", "Qi", ""]]}, {"id": "2102.04889", "submitter": "Antonios Anastasopoulos", "authors": "Claytone Sikasote and Antonios Anastasopoulos", "title": "BembaSpeech: A Speech Recognition Corpus for the Bemba Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a preprocessed, ready-to-use automatic speech recognition corpus,\nBembaSpeech, consisting over 24 hours of read speech in the Bemba language, a\nwritten but low-resourced language spoken by over 30% of the population in\nZambia. To assess its usefulness for training and testing ASR systems for\nBemba, we train an end-to-end Bemba ASR system by fine-tuning a pre-trained\nDeepSpeech English model on the training portion of the BembaSpeech corpus. Our\nbest model achieves a word error rate (WER) of 54.78%. The results show that\nthe corpus can be used for building ASR systems for Bemba. The corpus and\nmodels are publicly released at https://github.com/csikasote/BembaSpeech.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:42:00 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sikasote", "Claytone", ""], ["Anastasopoulos", "Antonios", ""]]}, {"id": "2102.04895", "submitter": "John Gallacer", "authors": "John D Gallacher", "title": "Leveraging cross-platform data to improve automated hate speech\n  detection", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hate speech is increasingly prevalent online, and its negative outcomes\ninclude increased prejudice, extremism, and even offline hate crime. Automatic\ndetection of online hate speech can help us to better understand these impacts.\nHowever, while the field has recently progressed through advances in natural\nlanguage processing, challenges still remain. In particular, most existing\napproaches for hate speech detection focus on a single social media platform in\nisolation. This limits both the use of these models and their validity, as the\nnature of language varies from platform to platform. Here we propose a new\ncross-platform approach to detect hate speech which leverages multiple datasets\nand classification models from different platforms and trains a superlearner\nthat can combine existing and novel training data to improve detection and\nincrease model applicability. We demonstrate how this approach outperforms\nexisting models, and achieves good performance when tested on messages from\nnovel social media platforms not included in the original training data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 15:52:34 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Gallacher", "John D", ""]]}, {"id": "2102.04932", "submitter": "Hieu Nguyen", "authors": "Kai Zhen (1 and 2), Hieu Duy Nguyen (2), Feng-Ju Chang (2), Athanasios\n  Mouchtaris (2), and Ariya Rastrow (2). ((1) Indiana University Bloomington,\n  (2) Alexa Machine Learning, Amazon, USA)", "title": "Sparsification via Compressed Sensing for Automatic Speech Recognition", "comments": "5 pages, accepted for publication in (ICASSP 2021) 2021 IEEE\n  International Conference on Acoustics, Speech, and Signal Processing. June\n  6-12, 2021. Location: Toronto, ON, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve high accuracy for machine learning (ML) applications, it\nis essential to employ models with a large number of parameters. Certain\napplications, such as Automatic Speech Recognition (ASR), however, require\nreal-time interactions with users, hence compelling the model to have as low\nlatency as possible. Deploying large scale ML applications thus necessitates\nmodel quantization and compression, especially when running ML models on\nresource constrained devices. For example, by forcing some of the model weight\nvalues into zero, it is possible to apply zero-weight compression, which\nreduces both the model size and model reading time from the memory. In the\nliterature, such methods are referred to as sparse pruning. The fundamental\nquestions are when and which weights should be forced to zero, i.e. be pruned.\nIn this work, we propose a compressed sensing based pruning (CSP) approach to\neffectively address those questions. By reformulating sparse pruning as a\nsparsity inducing and compression-error reduction dual problem, we introduce\nthe classic compressed sensing process into the ML model training process.\nUsing ASR task as an example, we show that CSP consistently outperforms\nexisting approaches in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:41:31 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhen", "Kai", "", "1 and 2"], ["Nguyen", "Hieu Duy", ""], ["Chang", "Feng-Ju", ""], ["Mouchtaris", "Athanasios", ""], ["Rastrow", "Ariya", ""], [".", "", ""]]}, {"id": "2102.04958", "submitter": "Robyn Kozierok", "authors": "Robyn Kozierok, John Aberdeen, Cheryl Clark, Christopher Garay,\n  Bradley Goodman, Tonia Korves, Lynette Hirschman, Patricia L. McDermott,\n  Matthew W. Peterson", "title": "Hallmarks of Human-Machine Collaboration: A framework for assessment in\n  the DARPA Communicating with Computers Program", "comments": "20 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": "MITRE Document Number: MTR210002", "categories": "cs.HC cs.AI cs.CL cs.MA cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing desire to create computer systems that can communicate\neffectively to collaborate with humans on complex, open-ended activities.\nAssessing these systems presents significant challenges. We describe a\nframework for evaluating systems engaged in open-ended complex scenarios where\nevaluators do not have the luxury of comparing performance to a single right\nanswer. This framework has been used to evaluate human-machine creative\ncollaborations across story and music generation, interactive block building,\nand exploration of molecular mechanisms in cancer. These activities are\nfundamentally different from the more constrained tasks performed by most\ncontemporary personal assistants as they are generally open-ended, with no\nsingle correct solution, and often no obvious completion criteria.\n  We identified the Key Properties that must be exhibited by successful\nsystems. From there we identified \"Hallmarks\" of success -- capabilities and\nfeatures that evaluators can observe that would be indicative of progress\ntoward achieving a Key Property. In addition to being a framework for\nassessment, the Key Properties and Hallmarks are intended to serve as goals in\nguiding research direction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:13:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kozierok", "Robyn", ""], ["Aberdeen", "John", ""], ["Clark", "Cheryl", ""], ["Garay", "Christopher", ""], ["Goodman", "Bradley", ""], ["Korves", "Tonia", ""], ["Hirschman", "Lynette", ""], ["McDermott", "Patricia L.", ""], ["Peterson", "Matthew W.", ""]]}, {"id": "2102.04980", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Jordi Pont-Tuset, Vittorio Ferrari, Radu Soricut", "title": "Telling the What while Pointing to the Where: Multimodal Queries for\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing image retrieval systems use text queries as a way for the user\nto express what they are looking for. However, fine-grained image retrieval\noften requires the ability to also express the where in the image the content\nthey are looking for is. The text modality can only cumbersomely express such\nlocalization preferences, whereas pointing is a more natural fit. In this\npaper, we propose an image retrieval setup with a new form of multimodal\nqueries, where the user simultaneously uses both spoken natural language (the\nwhat) and mouse traces over an empty canvas (the where) to express the\ncharacteristics of the desired target image. We then describe simple\nmodifications to an existing image retrieval model, enabling it to operate in\nthis setup. Qualitative and quantitative experiments show that our model\neffectively takes this spatial guidance into account, and provides\nsignificantly more accurate retrieval results compared to text-only equivalent\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:54:34 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:55:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Pont-Tuset", "Jordi", ""], ["Ferrari", "Vittorio", ""], ["Soricut", "Radu", ""]]}, {"id": "2102.04990", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi and Kien Nguyen and Tanaya Guha and Bang Du and\n  Truong Q. Nguyen", "title": "SG2Caps: Revisiting Scene Graphs for Image Captioning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features with an additional attention to salient regions and\nobjects to generate captions via recurrent models. Recently, scene graph\nrepresentations of images have been used to augment captioning models so as to\nleverage their structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that naive use of scene graphs from a\nblack-box scene graph generator harms image caption-ing performance, and scene\ngraph-based captioning mod-els have to incur the overhead of explicit use of\nimage features to generate decent captions. Addressing these challenges, we\npropose a framework, SG2Caps, that utilizes only the scene graph labels for\ncompetitive image caption-ing performance. The basic idea is to close the\nsemantic gap between two scene graphs - one derived from the input image and\nthe other one from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. Our framework outperforms existing scene graph-only\ncaptioning models by a large margin (CIDEr score of 110 vs 71) indicating scene\ngraphs as a promising representation for image captioning. Direct utilization\nof the scene graph labels avoids expensive graph convolutions over\nhigh-dimensional CNN features resulting in 49%fewer trainable parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:00:53 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Tripathi", "Subarna", ""], ["Nguyen", "Kien", ""], ["Guha", "Tanaya", ""], ["Du", "Bang", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "2102.05007", "submitter": "Matan Eyal", "authors": "Matan Eyal, Asaf Amrami, Hillel Taub-Tabib, Yoav Goldberg", "title": "Bootstrapping Relation Extractors using Syntactic Search by Examples", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of neural-networks in NLP brought with it substantial improvements\nin supervised relation extraction. However, obtaining a sufficient quantity of\ntraining data remains a key challenge. In this work we propose a process for\nbootstrapping training datasets which can be performed quickly by\nnon-NLP-experts. We take advantage of search engines over syntactic-graphs\n(Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We\nuse these to obtain positive examples by searching for sentences that are\nsyntactically similar to user input examples. We apply this technique to\nrelations from TACRED and DocRED and show that the resulting models are\ncompetitive with models trained on manually annotated data and on data obtained\nfrom distant supervision. The models also outperform models trained using NLG\ndata augmentation techniques. Extending the search-based approach with the NLG\nmethod further improves the results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 18:17:59 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Eyal", "Matan", ""], ["Amrami", "Asaf", ""], ["Taub-Tabib", "Hillel", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2102.05067", "submitter": "Silvia Cascianelli PhD", "authors": "Silvia Cascianelli, Gabriele Costante, Alessandro Devo, Thomas A.\n  Ciarfuglia, Paolo Valigi, Mario L. Fravolini", "title": "The Role of the Input in Natural Language Video Description", "comments": "In IEEE Transactions on Multimedia", "journal-ref": "IEEE Transactions on Multimedia, 22(1), 271-283 (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Video Description (NLVD) has recently received strong\ninterest in the Computer Vision, Natural Language Processing (NLP), Multimedia,\nand Autonomous Robotics communities. The State-of-the-Art (SotA) approaches\nobtained remarkable results when tested on the benchmark datasets. However,\nthose approaches poorly generalize to new datasets. In addition, none of the\nexisting works focus on the processing of the input to the NLVD systems, which\nis both visual and textual. In this work, it is presented an extensive study\ndealing with the role of the visual input, evaluated with respect to the\noverall NLP performance. This is achieved performing data augmentation of the\nvisual component, applying common transformations to model camera distortions,\nnoise, lighting, and camera positioning, that are typical in real-world\noperative scenarios. A t-SNE based analysis is proposed to evaluate the effects\nof the considered transformations on the overall visual data distribution. For\nthis study, it is considered the English subset of Microsoft Research Video\nDescription (MSVD) dataset, which is used commonly for NLVD. It was observed\nthat this dataset contains a relevant amount of syntactic and semantic errors.\nThese errors have been amended manually, and the new version of the dataset\n(called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is\nreleased to help to gain insight into the NLVD problem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:00:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cascianelli", "Silvia", ""], ["Costante", "Gabriele", ""], ["Devo", "Alessandro", ""], ["Ciarfuglia", "Thomas A.", ""], ["Valigi", "Paolo", ""], ["Fravolini", "Mario L.", ""]]}, {"id": "2102.05126", "submitter": "Jon\\'a\\v{s} Kulh\\'anek", "authors": "Jon\\'a\\v{s} Kulh\\'anek and Vojt\\v{e}ch Hude\\v{c}ek and Tom\\'a\\v{s}\n  Nekvinda and Ond\\v{r}ej Du\\v{s}ek", "title": "AuGPT: Dialogue with Pre-trained Language Models and Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based pre-trained language models such as GPT-2 brought\nconsiderable progress to end-to-end dialogue modelling. However, they also\npresent considerable risks for task-oriented dialogue, such as lack of\nknowledge grounding or diversity. To address these issues, we introduce\nmodified training objectives for language model finetuning, and we employ\nmassive data augmentation via back-translation to increase the diversity of the\ntraining data. We further examine the possibilities of combining data from\nmultiples sources to improve performance on the target dataset. We carefully\nevaluate our contributions with both human and automatic methods. Our model\nachieves state-of-the-art performance on the MultiWOZ data and shows\ncompetitive performance in human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 20:53:34 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kulh\u00e1nek", "Jon\u00e1\u0161", ""], ["Hude\u010dek", "Vojt\u011bch", ""], ["Nekvinda", "Tom\u00e1\u0161", ""], ["Du\u0161ek", "Ond\u0159ej", ""]]}, {"id": "2102.05169", "submitter": "Eunsol Choi", "authors": "Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski,\n  Dipanjan Das, Michael Collins", "title": "Decontextualization: Making Sentences Stand-Alone", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics (TACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Models for question answering, dialogue agents, and summarization often\ninterpret the meaning of a sentence in a rich context and use that meaning in a\nnew context. Taking excerpts of text can be problematic, as key pieces may not\nbe explicit in a local window. We isolate and define the problem of sentence\ndecontextualization: taking a sentence together with its context and rewriting\nit to be interpretable out of context, while preserving its meaning. We\ndescribe an annotation procedure, collect data on the Wikipedia corpus, and use\nthe data to train models to automatically decontextualize sentences. We present\npreliminary studies that show the value of sentence decontextualization in a\nuser facing task, and as preprocessing for systems that perform document\nunderstanding. We argue that decontextualization is an important subtask in\nmany downstream applications, and that the definitions and resources provided\ncan benefit tasks that operate on sentences that occur in a richer context.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 22:52:37 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Choi", "Eunsol", ""], ["Palomaki", "Jennimaria", ""], ["Lamm", "Matthew", ""], ["Kwiatkowski", "Tom", ""], ["Das", "Dipanjan", ""], ["Collins", "Michael", ""]]}, {"id": "2102.05260", "submitter": "Sm Zobaed", "authors": "Sm Zobaed, Md Enamul Haque, Md Fazle Rabby, and Mohsen Amini Salehi", "title": "SensPick: Sense Picking for Word Sense Disambiguation", "comments": null, "journal-ref": "16th IEEE International Conference on Semantic Computing,\n  ICSC'2021", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Word sense disambiguation (WSD) methods identify the most suitable meaning of\na word with respect to the usage of that word in a specific context. Neural\nnetwork-based WSD approaches rely on a sense-annotated corpus since they do not\nutilize lexical resources. In this study, we utilize both context and related\ngloss information of a target word to model the semantic relationship between\nthe word and the set of glosses. We propose SensPick, a type of stacked\nbidirectional Long Short Term Memory (LSTM) network to perform the WSD task.\nThe experimental evaluation demonstrates that SensPick outperforms traditional\nand state-of-the-art models on most of the benchmark datasets with a relative\nimprovement of 3.5% in F-1 score. While the improvement is not significant,\nincorporating semantic relationships brings SensPick in the leading position\ncompared to others.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 04:52:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Zobaed", "Sm", ""], ["Haque", "Md Enamul", ""], ["Rabby", "Md Fazle", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2102.05281", "submitter": "Qiao Jin", "authors": "Qiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu, Chuanqi Tan, Mosha\n  Chen, Songfang Huang, Xiaozhong Liu, Sheng Yu", "title": "Biomedical Question Answering: A Comprehensive Review", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question Answering (QA) is a benchmark Natural Language Processing (NLP) task\nwhere models predict the answer for a given question using related documents,\nimages, knowledge bases and question-answer pairs. Automatic QA has been\nsuccessfully applied in various domains like search engines and chatbots.\nHowever, for specific domains like biomedicine, QA systems are still rarely\nused in real-life settings. Biomedical QA (BQA), as an emerging QA task,\nenables innovative applications to effectively perceive, access and understand\ncomplex biomedical knowledge. In this work, we provide a critical review of\nrecent efforts in BQA. We comprehensively investigate prior BQA approaches,\nwhich are classified into 6 major methodologies (open-domain, knowledge base,\ninformation retrieval, machine reading comprehension, question entailment and\nvisual QA), 4 topics of contents (scientific, clinical, consumer health and\nexamination) and 5 types of formats (yes/no, extraction, generation,\nmulti-choice and retrieval). In the end, we highlight several key challenges of\nBQA and explore potential directions for future works.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 06:16:35 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Jin", "Qiao", ""], ["Yuan", "Zheng", ""], ["Xiong", "Guangzhi", ""], ["Yu", "Qianlan", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Huang", "Songfang", ""], ["Liu", "Xiaozhong", ""], ["Yu", "Sheng", ""]]}, {"id": "2102.05331", "submitter": "Martin Schmitt", "authors": "Martin Schmitt and Hinrich Sch\\\"utze", "title": "Language Models for Lexical Inference in Context", "comments": "Final version of EACL 2021 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lexical inference in context (LIiC) is the task of recognizing textual\nentailment between two very similar sentences, i.e., sentences that only differ\nin one expression. It can therefore be seen as a variant of the natural\nlanguage inference task that is focused on lexical semantics. We formulate and\nevaluate the first approaches based on pretrained language models (LMs) for\nthis task: (i) a few-shot NLI classifier, (ii) a relation induction approach\nbased on handcrafted patterns expressing the semantics of lexical inference,\nand (iii) a variant of (ii) with patterns that were automatically extracted\nfrom a corpus. All our approaches outperform the previous state of the art,\nshowing the potential of pretrained LMs for LIiC. In an extensive analysis, we\ninvestigate factors of success and failure of our three approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:08:22 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 08:33:24 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Schmitt", "Martin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2102.05379", "submitter": "Emiel Hoogeboom", "authors": "Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\\'e, Max\n  Welling", "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative flows and diffusion models have been predominantly trained on\nordinal data, for example natural images. This paper introduces two extensions\nof flows and diffusion for categorical data such as language or image\nsegmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined\nby a composition of a continuous distribution (such as a normalizing flow), and\nan argmax function. To optimize this model, we learn a probabilistic inverse\nfor the argmax that lifts the categorical data to a continuous space.\nMultinomial Diffusion gradually adds categorical noise in a diffusion process,\nfor which the generative denoising process is learned. We demonstrate that our\nmethod outperforms existing dequantization approaches on text modelling and\nmodelling on image segmentation maps in log-likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 11:04:17 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 14:18:43 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hoogeboom", "Emiel", ""], ["Nielsen", "Didrik", ""], ["Jaini", "Priyank", ""], ["Forr\u00e9", "Patrick", ""], ["Welling", "Max", ""]]}, {"id": "2102.05408", "submitter": "David Sabate Barbera", "authors": "David Sabate Barbera, Mark Huckvale, Victoria Fleming, Emily Upton,\n  Henry Coley-Fisher, Catherine Doogan, Ian Shaw, William Latham, Alexander P.\n  Leff, Jenny Crinion", "title": "NUVA: A Naming Utterance Verifier for Aphasia Treatment", "comments": "Under review", "journal-ref": null, "doi": "10.1016/j.csl.2021.101221", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomia (word-finding difficulties) is the hallmark of aphasia, an acquired\nlanguage disorder most commonly caused by stroke. Assessment of speech\nperformance using picture naming tasks is a key method for both diagnosis and\nmonitoring of responses to treatment interventions by people with aphasia\n(PWA). Currently, this assessment is conducted manually by speech and language\ntherapists (SLT). Surprisingly, despite advancements in automatic speech\nrecognition (ASR) and artificial intelligence with technologies like deep\nlearning, research on developing automated systems for this task has been\nscarce. Here we present NUVA, an utterance verification system incorporating a\ndeep learning element that classifies 'correct' versus' incorrect' naming\nattempts from aphasic stroke patients. When tested on eight native\nBritish-English speaking PWA the system's performance accuracy ranged between\n83.6% to 93.6%, with a 10-fold cross-validation mean of 89.5%. This performance\nwas not only significantly better than a baseline created for this study using\none of the leading commercially available ASRs (Google speech-to-text service)\nbut also comparable in some instances with two independent SLT ratings for the\nsame dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 13:00:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Barbera", "David Sabate", ""], ["Huckvale", "Mark", ""], ["Fleming", "Victoria", ""], ["Upton", "Emily", ""], ["Coley-Fisher", "Henry", ""], ["Doogan", "Catherine", ""], ["Shaw", "Ian", ""], ["Latham", "William", ""], ["Leff", "Alexander P.", ""], ["Crinion", "Jenny", ""]]}, {"id": "2102.05439", "submitter": "Latika Tamrakar", "authors": "Latika Tamrakar, Dr.Padmavati Shrivastava, Dr. S. M. Ghosh", "title": "Student sentiment Analysis Using Classification With Feature Extraction\n  Techniques", "comments": "need to rework in this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technical growths have empowered, numerous revolutions in the educational\nsystem by acquainting with technology into the classroom and by elevating the\nlearning experience. Nowadays Web-based learning is getting much popularity.\nThis paper describes the web-based learning and their effectiveness towards\nstudents. One of the prime factors in education or learning system is feedback;\nit is beneficial to learning if it must be used effectively. In this paper, we\nworked on how machine learning techniques like Logistic Regression (LR),\nSupport Vector Machine (SVM), Naive Bayes (NB), Decision Tree (DT) can be\napplied over Web-based learning, emphasis given on sentiment present in the\nfeedback students. We also work on two types of Feature Extraction Technique\n(FETs) namely Count Vector (CVr) or Bag of Words) (BoW) and Term Frequency and\nInverse Document Frequency (TF-IDF) Vector. In the research study, it is our\ngoal for our proposed LR, SVM, NB, and DT models to classify the presence of\nStudent Feedback Dataset (SFB) with improved accuracy with cleaned dataset and\nfeature extraction techniques. The SFB is one of the significant concerns among\nthe student sentimental analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:48:06 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 07:35:32 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Tamrakar", "Latika", ""], ["Shrivastava", "Dr. Padmavati", ""], ["Ghosh", "Dr. S. M.", ""]]}, {"id": "2102.05456", "submitter": "Leo Laugier", "authors": "Leo Laugier, John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon", "title": "Civil Rephrases Of Toxic Texts With Self-Supervised Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Platforms that support online commentary, from social networks to news sites,\nare increasingly leveraging machine learning to assist their moderation\nefforts. But this process does not typically provide feedback to the author\nthat would help them contribute according to the community guidelines. This is\nprohibitively time-consuming for human moderators to do, and computational\napproaches are still nascent. This work focuses on models that can help suggest\nrephrasings of toxic comments in a more civil manner. Inspired by recent\nprogress in unpaired sequence-to-sequence tasks, a self-supervised learning\nmodel is introduced, called CAE-T5. CAE-T5 employs a pre-trained text-to-text\ntransformer, which is fine tuned with a denoising and cyclic auto-encoder loss.\nExperimenting with the largest toxicity detection dataset to date (Civil\nComments) our model generates sentences that are more fluent and better at\npreserving the initial content compared to earlier text style transfer systems\nwhich we compare with using several scoring systems and human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:27:52 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 14:11:35 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Laugier", "Leo", ""], ["Pavlopoulos", "John", ""], ["Sorensen", "Jeffrey", ""], ["Dixon", "Lucas", ""]]}, {"id": "2102.05474", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Junlong Li, Hai Zhao", "title": "Multi-turn Dialogue Reading Comprehension with Pivot Turns and Knowledge", "comments": "The early version accepted by IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing (TASLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-turn dialogue reading comprehension aims to teach machines to read\ndialogue contexts and solve tasks such as response selection and answering\nquestions. The major challenges involve noisy history contexts and especial\nprerequisites of commonsense knowledge that is unseen in the given material.\nExisting works mainly focus on context and response matching approaches. This\nwork thus makes the first attempt to tackle the above two challenges by\nextracting substantially important turns as pivot utterances and utilizing\nexternal knowledge to enhance the representation of context. We propose a\npivot-oriented deep selection model (PoDS) on top of the Transformer-based\nlanguage models for dialogue comprehension. In detail, our model first picks\nout the pivot utterances from the conversation history according to the\nsemantic matching with the candidate response or question, if any. Besides,\nknowledge items related to the dialogue context are extracted from a knowledge\ngraph as external knowledge. Then, the pivot utterances and the external\nknowledge are combined with a well-designed mechanism for refining predictions.\nExperimental results on four dialogue comprehension benchmark tasks show that\nour proposed model achieves great improvements on baselines. A series of\nempirical comparisons are conducted to show how our selection strategies and\nthe extra knowledge injection influence the results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:00:12 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Li", "Junlong", ""], ["Zhao", "Hai", ""]]}, {"id": "2102.05486", "submitter": "Pengfei Liu", "authors": "Zihuiwen Ye, Pengfei Liu, Jinlan Fu, Graham Neubig", "title": "Towards More Fine-grained and Reliable NLP Performance Prediction", "comments": "Accepted by EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance prediction, the task of estimating a system's performance without\nperforming experiments, allows us to reduce the experimental burden caused by\nthe combinatorial explosion of different datasets, languages, tasks, and\nmodels. In this paper, we make two contributions to improving performance\nprediction for NLP tasks. First, we examine performance predictors not only for\nholistic measures of accuracy like F1 or BLEU but also fine-grained performance\nmeasures such as accuracy over individual classes of examples. Second, we\npropose methods to understand the reliability of a performance prediction model\nfrom two angles: confidence intervals and calibration. We perform an analysis\nof four types of NLP tasks, and both demonstrate the feasibility of\nfine-grained performance prediction and the necessity to perform reliability\nanalysis for performance prediction methods in the future. We make our code\npublicly available: \\url{https://github.com/neulab/Reliable-NLPPP}\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:23:20 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Ye", "Zihuiwen", ""], ["Liu", "Pengfei", ""], ["Fu", "Jinlan", ""], ["Neubig", "Graham", ""]]}, {"id": "2102.05638", "submitter": "Zach Wood-Doughty", "authors": "Zach Wood-Doughty, Ilya Shpitser, Mark Dredze", "title": "Generating Synthetic Text Data to Evaluate Causal Inference Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing causal conclusions from observational data requires making\nassumptions about the true data-generating process. Causal inference research\ntypically considers low-dimensional data, such as categorical or numerical\nfields in structured medical records. High-dimensional and unstructured data\nsuch as natural language complicates the evaluation of causal inference\nmethods; such evaluations rely on synthetic datasets with known causal effects.\nModels for natural language generation have been widely studied and perform\nwell empirically. However, existing methods not immediately applicable to\nproducing synthetic datasets for causal evaluations, as they do not allow for\nquantifying a causal effect on the text itself. In this work, we develop a\nframework for adapting existing generation models to produce synthetic text\ndatasets with known causal effects. We use this framework to perform an\nempirical comparison of four recently-proposed methods for estimating causal\neffects from text data. We release our code and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:53:11 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Wood-Doughty", "Zach", ""], ["Shpitser", "Ilya", ""], ["Dredze", "Mark", ""]]}, {"id": "2102.05708", "submitter": "Fatemah Husain", "authors": "Fatemah Husain and Ozlem Uzuner", "title": "Transfer Learning Approach for Arabic Offensive Language Detection\n  System -- BERT-Based Model", "comments": "2021 4th International Conference on Computer Applications &\n  Information Security (ICCAIS) - Contemporary Computer Technologies and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing a system to detect online offensive language is very important to\nthe health and the security of online users. Studies have shown that cyberhate,\nonline harassment and other misuses of technology are on the rise, particularly\nduring the global Coronavirus pandemic in 2020. According to the latest report\nby the Anti-Defamation League (ADL), 35% of online users reported online\nharassment related to their identity-based characteristics, which is a 3%\nincrease over 2019. Applying advanced techniques from the Natural Language\nProcessing (NLP) field to support the development of an online hate-free\ncommunity is a critical task for social justice. Transfer learning enhances the\nperformance of the classifier by allowing the transfer of knowledge from one\ndomain or one dataset to others that have not been seen before, thus,\nsupporting the classifier to be more generalizable. In our study, we apply the\nprinciples of transfer learning cross multiple Arabic offensive language\ndatasets to compare the effects on system performance. This study aims at\ninvestigating the effects of fine-tuning and training Bidirectional Encoder\nRepresentations from Transformers (BERT) model on multiple Arabic offensive\nlanguage datasets individually and testing it using other datasets\nindividually. Our experiment starts with a comparison among multiple BERT\nmodels to guide the selection of the main model that is used for our study. The\nstudy also investigates the effects of concatenating all datasets to be used\nfor fine-tuning and training BERT model. Our results demonstrate the limited\neffects of transfer learning on the performance of the classifiers,\nparticularly for highly dialectic comments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:58:18 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Husain", "Fatemah", ""], ["Uzuner", "Ozlem", ""]]}, {"id": "2102.05717", "submitter": "Edoardo Maria Ponti", "authors": "Shijie Wu and Edoardo Maria Ponti and Ryan Cotterell", "title": "Differentiable Generative Phonology", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of generative phonology, as formulated by Chomsky and Halle (1968),\nis to specify a formal system that explains the set of attested phonological\nstrings in a language. Traditionally, a collection of rules (or constraints, in\nthe case of optimality theory) and underlying forms (UF) are posited to work in\ntandem to generate phonological strings. However, the degree of abstraction of\nUFs with respect to their concrete realizations is contentious. As the main\ncontribution of our work, we implement the phonological generative system as a\nneural model differentiable end-to-end, rather than as a set of rules or\nconstraints. Contrary to traditional phonology, in our model, UFs are\ncontinuous vectors in $\\mathbb{R}^d$, rather than discrete strings. As a\nconsequence, UFs are discovered automatically rather than posited by linguists,\nand the model can scale to the size of a realistic vocabulary. Moreover, we\ncompare several modes of the generative process, contemplating: i) the presence\nor absence of an underlying representation in between morphemes and surface\nforms (SFs); and ii) the conditional dependence or independence of UFs with\nrespect to SFs. We evaluate the ability of each mode to predict attested\nphonological strings on 2 datasets covering 5 and 28 languages, respectively.\nThe results corroborate two tenets of generative phonology, viz. the necessity\nfor UFs and their independence from SFs. In general, our neural model of\ngenerative phonology learns both UFs and SFs automatically and on a\nlarge-scale.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:50:16 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 03:35:57 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Wu", "Shijie", ""], ["Ponti", "Edoardo Maria", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2102.05757", "submitter": "Shohreh Shaghaghian Ms", "authors": "Shohreh Shaghaghian, Luna (Yue) Feng, Borna Jafarpour, Nicolai\n  Pogrebnyakov", "title": "Customizing Contextualized Language Models forLegal Document Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Inspired by the inductive transfer learning on computer vision, many efforts\nhave been made to train contextualized language models that boost the\nperformance of natural language processing tasks. These models are mostly\ntrained on large general-domain corpora such as news, books, or\nWikipedia.Although these pre-trained generic language models well perceive the\nsemantic and syntactic essence of a language structure, exploiting them in a\nreal-world domain-specific scenario still needs some practical considerations\nto be taken into account such as token distribution shifts, inference time,\nmemory, and their simultaneous proficiency in multiple tasks. In this paper, we\nfocus on the legal domain and present how different language model strained on\ngeneral-domain corpora can be best customized for multiple legal document\nreviewing tasks. We compare their efficiencies with respect to task\nperformances and present practical considerations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 22:14:15 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Shaghaghian", "Shohreh", "", "Yue"], ["Luna", "", "", "Yue"], ["Feng", "", ""], ["Jafarpour", "Borna", ""], ["Pogrebnyakov", "Nicolai", ""]]}, {"id": "2102.05766", "submitter": "Renjie Zheng", "authors": "Renjie Zheng and Junkun Chen and Mingbo Ma and Liang Huang", "title": "Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining\n  and Speech Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently text and speech representation learning has successfully improved\nmany language related tasks. However, all existing methods only learn from one\ninput modality, while a unified acoustic and text representation is desired by\nmany speech-related tasks such as speech translation. We propose a Fused\nAcoustic and Text Masked Language Model (FAT-MLM) which jointly learns a\nunified representation for both acoustic and text in-put. Within this cross\nmodal representation learning framework, we further present an end-to-end model\nfor Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three\ntranslation directions show that our proposed speech translation models\nfine-tuned from FAT-MLM substantially improve translation quality (+5.90 BLEU).\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 22:53:40 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zheng", "Renjie", ""], ["Chen", "Junkun", ""], ["Ma", "Mingbo", ""], ["Huang", "Liang", ""]]}, {"id": "2102.05918", "submitter": "Chao Jia", "authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham,\n  Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "comments": "ICML 2021", "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained representations are becoming crucial for many NLP and perception\ntasks. While representation learning in NLP has transitioned to training on raw\ntext without human annotations, visual and vision-language representations\nstill rely heavily on curated training datasets that are expensive or require\nexpert knowledge. For vision applications, representations are mostly learned\nusing datasets with explicit class labels such as ImageNet or OpenImages. For\nvision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all\ninvolve a non-trivial data collection (and cleaning) process. This costly\ncuration process limits the size of datasets and hence hinders the scaling of\ntrained models. In this paper, we leverage a noisy dataset of over one billion\nimage alt-text pairs, obtained without expensive filtering or post-processing\nsteps in the Conceptual Captions dataset. A simple dual-encoder architecture\nlearns to align visual and language representations of the image and text pairs\nusing a contrastive loss. We show that the scale of our corpus can make up for\nits noise and leads to state-of-the-art representations even with such a simple\nlearning scheme. Our visual representation achieves strong performance when\ntransferred to classification tasks such as ImageNet and VTAB. The aligned\nvisual and language representations enables zero-shot image classification and\nalso set new state-of-the-art results on Flickr30K and MSCOCO image-text\nretrieval benchmarks, even when compared with more sophisticated\ncross-attention models. The representations also enable cross-modality search\nwith complex text and text + image queries.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 10:08:12 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:51:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Jia", "Chao", ""], ["Yang", "Yinfei", ""], ["Xia", "Ye", ""], ["Chen", "Yi-Ting", ""], ["Parekh", "Zarana", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""], ["Sung", "Yunhsuan", ""], ["Li", "Zhen", ""], ["Duerig", "Tom", ""]]}, {"id": "2102.05924", "submitter": "Yiping Jin", "authors": "Yiping Jin, Akshay Bhatia, Dittaya Wanvarie, Phu T. V. Le", "title": "Generating Coherent and Diverse Slogans with Sequence-to-Sequence\n  Transformer", "comments": "Submitted to NLE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work in slogan generation focused on generating novel slogans by\nutilising templates mined from real slogans. While some such slogans can be\ncatchy, they are often not coherent with the company's focus or style across\ntheir marketing communications because the templates are mined from other\ncompanies' slogans. We propose a sequence-to-sequence transformer model to\ngenerate slogans from a brief company description. A naive sequence-to-sequence\nmodel fine-tuned for slogan generation is prone to introducing false\ninformation, especially unrelated company names appearing in the training data.\nWe use delexicalisation to address this problem and improve the generated\nslogans' quality by a large margin. Furthermore, we apply two simple but\neffective approaches to generate more diverse slogans. Firstly, we train a\nslogan generator conditioned on the industry. During inference time, by\nchanging the industry, we can obtain different \"flavours\" of slogans. Secondly,\ninstead of using only the company description as the input sequence, we sample\nrandom paragraphs from the company's website. Surprisingly, the model can\ngenerate meaningful slogans, even if the input sequence does not resemble a\ncompany description. We validate the effectiveness of the proposed method with\nboth quantitative evaluation and qualitative evaluation. Our best model\nachieved a ROUGE-1/-2/-L F1 score of 53.13/33.30/46.49. Besides, human\nevaluators assigned the generated slogans an average score of 3.39 on a scale\nof 1-5, indicating the system can generate plausible slogans with a quality\nclose to human-written ones (average score 3.55).\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 10:25:08 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Jin", "Yiping", ""], ["Bhatia", "Akshay", ""], ["Wanvarie", "Dittaya", ""], ["Le", "Phu T. V.", ""]]}, {"id": "2102.05951", "submitter": "Zuchao Li", "authors": "Zuchao Li, Zhuosheng Zhang, Hai Zhao, Rui Wang, Kehai Chen, Masao\n  Utiyama, and Eiichiro Sumita", "title": "Text Compression-aided Transformer Encoding", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2021.3058341", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text encoding is one of the most important steps in Natural Language\nProcessing (NLP). It has been done well by the self-attention mechanism in the\ncurrent state-of-the-art Transformer encoder, which has brought about\nsignificant improvements in the performance of many NLP tasks. Though the\nTransformer encoder may effectively capture general information in its\nresulting representations, the backbone information, meaning the gist of the\ninput text, is not specifically focused on. In this paper, we propose explicit\nand implicit text compression approaches to enhance the Transformer encoding\nand evaluate models using this approach on several typical downstream tasks\nthat rely on the encoding heavily. Our explicit text compression approaches use\ndedicated models to compress text, while our implicit text compression approach\nsimply adds an additional module to the main model to handle text compression.\nWe propose three ways of integration, namely backbone source-side fusion,\ntarget-side fusion, and both-side fusion, to integrate the backbone information\ninto Transformer-based models for various downstream tasks. Our evaluation on\nbenchmark datasets shows that the proposed explicit and implicit text\ncompression approaches improve results in comparison to strong baselines. We\ntherefore conclude, when comparing the encodings to the baseline models, text\ncompression helps the encoders to learn better language representations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 11:28:39 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Li", "Zuchao", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""], ["Chen", "Kehai", ""], ["Utiyama", "Masao", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "2102.05980", "submitter": "Markus Eberts", "authors": "Markus Eberts, Adrian Ulges", "title": "An End-to-end Model for Entity-level Relation Extraction using\n  Multi-instance Learning", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a joint model for entity-level relation extraction from documents.\nIn contrast to other approaches - which focus on local intra-sentence mention\npairs and thus require annotations on mention level - our model operates on\nentity level. To do so, a multi-task approach is followed that builds upon\ncoreference resolution and gathers relevant signals via multi-instance learning\nwith multi-level representations combining global entity and local mention\ninformation. We achieve state-of-the-art relation extraction results on the\nDocRED dataset and report the first entity-level end-to-end relation extraction\nresults for future reference. Finally, our experimental results suggest that a\njoint approach is on par with task-specific learning, though more efficient due\nto shared parameters and training steps.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:49:39 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Eberts", "Markus", ""], ["Ulges", "Adrian", ""]]}, {"id": "2102.06003", "submitter": "Sayan Nag", "authors": "Uddalok Sarkar, Sayan Nag, Chirayata Bhattacharya, Shankha Sanyal,\n  Archi Banerjee, Ranjan Sengupta and Dipak Ghosh", "title": "Language Independent Emotion Quantification using Non linear Modelling\n  of Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At present emotion extraction from speech is a very important issue due to\nits diverse applications. Hence, it becomes absolutely necessary to obtain\nmodels that take into consideration the speaking styles of a person, vocal\ntract information, timbral qualities and other congenital information regarding\nhis voice. Our speech production system is a nonlinear system like most other\nreal world systems. Hence the need arises for modelling our speech information\nusing nonlinear techniques. In this work we have modelled our articulation\nsystem using nonlinear multifractal analysis. The multifractal spectral width\nand scaling exponents reveals essentially the complexity associated with the\nspeech signals taken. The multifractal spectrums are well distinguishable the\nin low fluctuation region in case of different emotions. The source\ncharacteristics have been quantified with the help of different non-linear\nmodels like Multi-Fractal Detrended Fluctuation Analysis, Wavelet Transform\nModulus Maxima. The Results obtained from this study gives a very good result\nin emotion clustering.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:48:25 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Sarkar", "Uddalok", ""], ["Nag", "Sayan", ""], ["Bhattacharya", "Chirayata", ""], ["Sanyal", "Shankha", ""], ["Banerjee", "Archi", ""], ["Sengupta", "Ranjan", ""], ["Ghosh", "Dipak", ""]]}, {"id": "2102.06008", "submitter": "Arthur Brack", "authors": "Arthur Brack and Anett Hoppe and Pascal Buscherm\\\"ohle and Ralph\n  Ewerth", "title": "Sequential Sentence Classification in Research Papers using Cross-Domain\n  Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of sequential sentence classification enables the semantic\nstructuring of research papers. This can enhance academic search engines to\nsupport researchers in finding and exploring research literature more\neffectively. However, previous work has not investigated the potential of\ntransfer learning with datasets from different scientific domains for this task\nyet. We propose a uniform deep learning architecture and multi-task learning to\nimprove sequential sentence classification in scientific texts across domains\nby exploiting training data from multiple domains. Our contributions can be\nsummarised as follows: (1) We tailor two common transfer learning methods,\nsequential transfer learning and multi-task learning, and evaluate their\nperformance for sequential sentence classification; (2) The presented\nmulti-task model is able to recognise semantically related classes from\ndifferent datasets and thus supports manual comparison and assessment of\ndifferent annotation schemes; (3) The unified approach is capable of handling\ndatasets that contain either only abstracts or full papers without further\nfeature engineering. We demonstrate that models, which are trained on datasets\nfrom different scientific domains, benefit from one another when using the\nproposed multi-task learning architecture. Our approach outperforms the state\nof the art on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:54:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Brack", "Arthur", ""], ["Hoppe", "Anett", ""], ["Buscherm\u00f6hle", "Pascal", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2102.06038", "submitter": "Sayan Nag", "authors": "Sayan Nag, Uddalok Sarkar, Shankha Sanyal, Archi Banerjee, Souparno\n  Roy, Samir Karmakar, Ranjan Sengupta and Dipak Ghosh", "title": "A Fractal Approach to Characterize Emotions in Audio and Visual Domain:\n  A Study on Cross-Modal Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is already known that both auditory and visual stimulus is able to convey\nemotions in human mind to different extent. The strength or intensity of the\nemotional arousal vary depending on the type of stimulus chosen. In this study,\nwe try to investigate the emotional arousal in a cross-modal scenario involving\nboth auditory and visual stimulus while studying their source characteristics.\nA robust fractal analytic technique called Detrended Fluctuation Analysis (DFA)\nand its 2D analogue has been used to characterize three (3) standardized audio\nand video signals quantifying their scaling exponent corresponding to positive\nand negative valence. It was found that there is significant difference in\nscaling exponents corresponding to the two different modalities. Detrended\nCross Correlation Analysis (DCCA) has also been applied to decipher degree of\ncross-correlation among the individual audio and visual stimulus. This is the\nfirst of its kind study which proposes a novel algorithm with which emotional\narousal can be classified in cross-modal scenario using only the source audio\nand visual signals while also attempting a correlation between them.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 14:30:22 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Nag", "Sayan", ""], ["Sarkar", "Uddalok", ""], ["Sanyal", "Shankha", ""], ["Banerjee", "Archi", ""], ["Roy", "Souparno", ""], ["Karmakar", "Samir", ""], ["Sengupta", "Ranjan", ""], ["Ghosh", "Dipak", ""]]}, {"id": "2102.06183", "submitter": "Jie Lei", "authors": "Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit\n  Bansal, Jingjing Liu", "title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "comments": "12 pages, 5 figures, 11 tables. - Happy Chinese New Year!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The canonical approach to video-and-language learning (e.g., video question\nanswering) dictates a neural model to learn from offline-extracted dense video\nfeatures from vision models and text features from language models. These\nfeature extractors are trained independently and usually on tasks different\nfrom the target domains, rendering these fixed features sub-optimal for\ndownstream tasks. Moreover, due to the high computational overload of dense\nvideo features, it is often difficult (or infeasible) to plug feature\nextractors directly into existing approaches for easy finetuning. To provide a\nremedy to this dilemma, we propose a generic framework ClipBERT that enables\naffordable end-to-end learning for video-and-language tasks, by employing\nsparse sampling, where only a single or a few sparsely sampled short clips from\na video are used at each training step. Experiments on text-to-video retrieval\nand video question answering on six datasets demonstrate that ClipBERT\noutperforms (or is on par with) existing methods that exploit full-length\nvideos, suggesting that end-to-end learning with just a few sparsely sampled\nclips is often more accurate than using densely extracted offline features from\nfull-length videos, proving the proverbial less-is-more principle. Videos in\nthe datasets are from considerably different domains and lengths, ranging from\n3-second generic domain GIF videos to 180-second YouTube human activity videos,\nshowing the generalization ability of our approach. Comprehensive ablation\nstudies and thorough analyses are provided to dissect what factors lead to this\nsuccess. Our code is publicly available at https://github.com/jayleicn/ClipBERT\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:50:16 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Lei", "Jie", ""], ["Li", "Linjie", ""], ["Zhou", "Luowei", ""], ["Gan", "Zhe", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""], ["Liu", "Jingjing", ""]]}, {"id": "2102.06272", "submitter": "Vishakh Padmakumar", "authors": "Vishakh Padmakumar, He He", "title": "Unsupervised Extractive Summarization using Pointwise Mutual Information", "comments": "To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised approaches to extractive summarization usually rely on a notion\nof sentence importance defined by the semantic similarity between a sentence\nand the document. We propose new metrics of relevance and redundancy using\npointwise mutual information (PMI) between sentences, which can be easily\ncomputed by a pre-trained language model. Intuitively, a relevant sentence\nallows readers to infer the document content (high PMI with the document), and\na redundant sentence can be inferred from the summary (high PMI with the\nsummary). We then develop a greedy sentence selection algorithm to maximize\nrelevance and minimize redundancy of extracted sentences. We show that our\nmethod outperforms similarity-based methods on datasets in a range of domains\nincluding news, medical journal articles, and personal anecdotes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 21:05:50 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:53:41 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Padmakumar", "Vishakh", ""], ["He", "He", ""]]}, {"id": "2102.06282", "submitter": "Manuel Ciosici", "authors": "Mads Toftrup, S{\\o}ren Asger S{\\o}rensen, Manuel R. Ciosici, Ira\n  Assent", "title": "A reproduction of Apple's bi-directional LSTM models for language\n  identification in short strings", "comments": "Will be presented at EACL 2021 SRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Language Identification is the task of identifying a document's language. For\napplications like automatic spell checker selection, language identification\nmust use very short strings such as text message fragments. In this work, we\nreproduce a language identification architecture that Apple briefly sketched in\na blog post. We confirm the bi-LSTM model's performance and find that it\noutperforms current open-source language identifiers. We further find that its\nlanguage identification mistakes are due to confusion between related\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 21:46:43 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Toftrup", "Mads", ""], ["S\u00f8rensen", "S\u00f8ren Asger", ""], ["Ciosici", "Manuel R.", ""], ["Assent", "Ira", ""]]}, {"id": "2102.06283", "submitter": "Yao Qian", "authors": "Yao Qian, Ximo Bian, Yu Shi, Naoyuki Kanda, Leo Shen, Zhen Xiao and\n  Michael Zeng", "title": "Speech-language Pre-training for End-to-end Spoken Language\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) spoken language understanding (SLU) can infer semantics\ndirectly from speech signal without cascading an automatic speech recognizer\n(ASR) with a natural language understanding (NLU) module. However, paired\nutterance recordings and corresponding semantics may not always be available or\nsufficient to train an E2E SLU model in a real production environment. In this\npaper, we propose to unify a well-optimized E2E ASR encoder (speech) and a\npre-trained language model encoder (language) into a transformer decoder. The\nunified speech-language pre-trained model (SLP) is continually enhanced on\nlimited labeled data from a target domain by using a conditional masked\nlanguage model (MLM) objective, and thus can effectively generate a sequence of\nintent, slot type, and slot value for given input speech in the inference. The\nexperimental results on two public corpora show that our approach to E2E SLU is\nsuperior to the conventional cascaded method. It also outperforms the present\nstate-of-the-art approaches to E2E SLU with much less paired data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 21:55:48 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Qian", "Yao", ""], ["Bian", "Ximo", ""], ["Shi", "Yu", ""], ["Kanda", "Naoyuki", ""], ["Shen", "Leo", ""], ["Xiao", "Zhen", ""], ["Zeng", "Michael", ""]]}, {"id": "2102.06314", "submitter": "Amila Silva", "authors": "Amila Silva, Ling Luo, Shanika Karunasekera, Christopher Leckie", "title": "Embracing Domain Differences in Fake News: Cross-domain Fake News\n  Detection using Multi-modal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid evolution of social media, fake news has become a significant\nsocial problem, which cannot be addressed in a timely manner using manual\ninvestigation. This has motivated numerous studies on automating fake news\ndetection. Most studies explore supervised training models with different\nmodalities (e.g., text, images, and propagation networks) of news records to\nidentify fake news. However, the performance of such techniques generally drops\nif news records are coming from different domains (e.g., politics,\nentertainment), especially for domains that are unseen or rarely-seen during\ntraining. As motivation, we empirically show that news records from different\ndomains have significantly different word usage and propagation patterns.\nFurthermore, due to the sheer volume of unlabelled news records, it is\nchallenging to select news records for manual labelling so that the\ndomain-coverage of the labelled dataset is maximized. Hence, this work: (1)\nproposes a novel framework that jointly preserves domain-specific and\ncross-domain knowledge in news records to detect fake news from different\ndomains; and (2) introduces an unsupervised technique to select a set of\nunlabelled informative news records for manual labelling, which can be\nultimately used to train a fake news detection model that performs well for\nmany domains while minimizing the labelling cost. Our experiments show that the\nintegration of the proposed fake news model and the selective annotation\napproach achieves state-of-the-art performance for cross-domain news datasets,\nwhile yielding notable improvements for rarely-appearing domains in news\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 23:31:14 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 13:26:43 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 05:33:25 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 07:03:59 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Silva", "Amila", ""], ["Luo", "Ling", ""], ["Karunasekera", "Shanika", ""], ["Leckie", "Christopher", ""]]}, {"id": "2102.06320", "submitter": "Andriy Miranskyy", "authors": "Jared Rand and Andriy Miranskyy", "title": "On Automatic Parsing of Log Records", "comments": "Shortened version accepted for publication in Proceedings of the 43rd\n  International Conference on Software Engineering: New Ideas and Emerging\n  Results, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software log analysis helps to maintain the health of software solutions and\nensure compliance and security. Existing software systems consist of\nheterogeneous components emitting logs in various formats. A typical solution\nis to unify the logs using manually built parsers, which is laborious.\n  Instead, we explore the possibility of automating the parsing task by\nemploying machine translation (MT). We create a tool that generates synthetic\nApache log records which we used to train recurrent-neural-network-based MT\nmodels. Models' evaluation on real-world logs shows that the models can learn\nApache log format and parse individual log records. The median relative edit\ndistance between an actual real-world log record and the MT prediction is less\nthan or equal to 28%. Thus, we show that log parsing using an MT approach is\npromising.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 00:27:41 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Rand", "Jared", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "2102.06380", "submitter": "Monica Sunkara", "authors": "Monica Sunkara, Chaitanya Shivade, Sravan Bodapati, Katrin Kirchhoff", "title": "Neural Inverse Text Normalization", "comments": "5 pages, accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there have been several contributions exploring state of the art\ntechniques for text normalization, the problem of inverse text normalization\n(ITN) remains relatively unexplored. The best known approaches leverage finite\nstate transducer (FST) based models which rely on manually curated rules and\nare hence not scalable. We propose an efficient and robust neural solution for\nITN leveraging transformer based seq2seq models and FST-based text\nnormalization techniques for data preparation. We show that this can be easily\nextended to other languages without the need for a linguistic expert to\nmanually curate them. We then present a hybrid framework for integrating Neural\nITN with an FST to overcome common recoverable errors in production\nenvironments. Our empirical evaluations show that the proposed solution\nminimizes incorrect perturbations (insertions, deletions and substitutions) to\nASR output and maintains high quality even on out of domain data. A transformer\nbased model infused with pretraining consistently achieves a lower WER across\nseveral datasets and is able to outperform baselines on English, Spanish,\nGerman and Italian datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 07:53:53 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Sunkara", "Monica", ""], ["Shivade", "Chaitanya", ""], ["Bodapati", "Sravan", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "2102.06391", "submitter": "Laria Reynolds", "authors": "Laria Reynolds and Kyle McDonell", "title": "Multiversal views on language models", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The virtuosity of language models like GPT-3 opens a new world of possibility\nfor human-AI collaboration in writing. In this paper, we present a framework in\nwhich generative language models are conceptualized as multiverse generators.\nThis framework also applies to human imagination and is core to how we read and\nwrite fiction. We call for exploration into this commonality through new forms\nof interfaces which allow humans to couple their imagination to AI to write,\nexplore, and understand non-linear fiction. We discuss the early insights we\nhave gained from actively pursuing this approach by developing and testing a\nnovel multiversal GPT-3-assisted writing interface.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 08:28:28 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 05:25:35 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Reynolds", "Laria", ""], ["McDonell", "Kyle", ""]]}, {"id": "2102.06423", "submitter": "Susann Boy", "authors": "Susann Boy, Dana Ruiter, Dietrich Klakow", "title": "Emoji-Based Transfer Learning for Sentiment Tasks", "comments": "6 pages, 2 figures, accepted at EACL-SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment tasks such as hate speech detection and sentiment analysis,\nespecially when performed on languages other than English, are often\nlow-resource. In this study, we exploit the emotional information encoded in\nemojis to enhance the performance on a variety of sentiment tasks. This is done\nusing a transfer learning approach, where the parameters learned by an\nemoji-based source task are transferred to a sentiment target task. We analyse\nthe efficacy of the transfer under three conditions, i.e. i) the emoji content\nand ii) label distribution of the target task as well as iii) the difference\nbetween monolingually and multilingually learned source tasks. We find i.a.\nthat the transfer is most beneficial if the target task is balanced with high\nemoji content. Monolingually learned source tasks have the benefit of taking\ninto account the culturally specific use of emojis and gain up to F1 +0.280\nover the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 10:05:02 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Boy", "Susann", ""], ["Ruiter", "Dana", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2102.06431", "submitter": "Peng Liu", "authors": "Peng Liu, Yuewen Cao, Songxiang Liu, Na Hu, Guangzhi Li, Chao Weng,\n  Dan Su", "title": "VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep\n  VAE with Residual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes VARA-TTS, a non-autoregressive (non-AR) text-to-speech\n(TTS) model using a very deep Variational Autoencoder (VDVAE) with Residual\nAttention mechanism, which refines the textual-to-acoustic alignment\nlayer-wisely. Hierarchical latent variables with different temporal resolutions\nfrom the VDVAE are used as queries for residual attention module. By leveraging\nthe coarse global alignment from previous attention layer as an extra input,\nthe following attention layer can produce a refined version of alignment. This\namortizes the burden of learning the textual-to-acoustic alignment among\nmultiple attention layers and outperforms the use of only a single attention\nlayer in robustness. An utterance-level speaking speed factor is computed by a\njointly-trained speaking speed predictor, which takes the mean-pooled latent\nvariables of the coarsest layer as input, to determine number of acoustic\nframes at inference. Experimental results show that VARA-TTS achieves slightly\ninferior speech quality to an AR counterpart Tacotron 2 but an\norder-of-magnitude speed-up at inference; and outperforms an analogous non-AR\nmodel, BVAE-TTS, in terms of speech quality.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 10:26:57 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Liu", "Peng", ""], ["Cao", "Yuewen", ""], ["Liu", "Songxiang", ""], ["Hu", "Na", ""], ["Li", "Guangzhi", ""], ["Weng", "Chao", ""], ["Su", "Dan", ""]]}, {"id": "2102.06474", "submitter": "Guangzhi Sun", "authors": "G. Sun, C. Zhang, P. C. Woodland", "title": "Transformer Language Models with LSTM-based Cross-utterance Information\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective incorporation of cross-utterance information has the potential\nto improve language models (LMs) for automatic speech recognition (ASR). To\nextract more powerful and robust cross-utterance representations for the\nTransformer LM (TLM), this paper proposes the R-TLM which uses hidden states in\na long short-term memory (LSTM) LM. To encode the cross-utterance information,\nthe R-TLM incorporates an LSTM module together with a segment-wise recurrence\nin some of the Transformer blocks. In addition to the LSTM module output, a\nshortcut connection using a fusion layer that bypasses the LSTM module is also\ninvestigated. The proposed system was evaluated on the AMI meeting corpus, the\nEval2000 and the RT03 telephone conversation evaluation sets. The best R-TLM\nachieved 0.9%, 0.6%, and 0.8% absolute WER reductions over the single-utterance\nTLM baseline, and 0.5%, 0.3%, 0.2% absolute WER reductions over a strong\ncross-utterance TLM baseline on the AMI evaluation set, Eval2000 and RT03\nrespectively. Improvements on Eval2000 and RT03 were further supported by\nsignificance tests. R-TLMs were found to have better LM scores on words where\nrecognition errors are more likely to occur. The R-TLM WER can be further\nreduced by interpolation with an LSTM-LM.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 12:12:29 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Sun", "G.", ""], ["Zhang", "C.", ""], ["Woodland", "P. C.", ""]]}, {"id": "2102.06540", "submitter": "Qin Dai", "authors": "Qin Dai, Naoya Inoue, Ryo Takahashi and Kentaro Inui", "title": "Two Training Strategies for Improving Relation Extraction over Universal\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores how the Distantly Supervised Relation Extraction (DS-RE)\ncan benefit from the use of a Universal Graph (UG), the combination of a\nKnowledge Graph (KG) and a large-scale text collection. A straightforward\nextension of a current state-of-the-art neural model for DS-RE with a UG may\nlead to degradation in performance. We first report that this degradation is\nassociated with the difficulty in learning a UG and then propose two training\nstrategies: (1) Path Type Adaptive Pretraining, which sequentially trains the\nmodel with different types of UG paths so as to prevent the reliance on a\nsingle type of UG path; and (2) Complexity Ranking Guided Attention mechanism,\nwhich restricts the attention span according to the complexity of a UG path so\nas to force the model to extract features not only from simple UG paths but\nalso from complex ones. Experimental results on both biomedical and NYT10\ndatasets prove the robustness of our methods and achieve a new state-of-the-art\nresult on the NYT10 dataset. The code and datasets used in this paper are\navailable at https://github.com/baodaiqin/UGDSRE.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:09:35 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 07:18:22 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dai", "Qin", ""], ["Inoue", "Naoya", ""], ["Takahashi", "Ryo", ""], ["Inui", "Kentaro", ""]]}, {"id": "2102.06551", "submitter": "Jivnesh Sandhan", "authors": "Jivnesh Sandhan, Amrith Krishna, Ashim Gupta, Laxmidhar Behera and\n  Pawan Goyal", "title": "A Little Pretraining Goes a Long Way: A Case Study on Dependency Parsing\n  Task for Low-resource Morphologically Rich Languages", "comments": "6 pages, The work is accepted at EACL-SRW, 2021, Kyiv, Ukraine Typos\n  corrected in Section 3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural dependency parsing has achieved remarkable performance for many\ndomains and languages. The bottleneck of massive labeled data limits the\neffectiveness of these approaches for low resource languages. In this work, we\nfocus on dependency parsing for morphological rich languages (MRLs) in a\nlow-resource setting. Although morphological information is essential for the\ndependency parsing task, the morphological disambiguation and lack of powerful\nanalyzers pose challenges to get this information for MRLs. To address these\nchallenges, we propose simple auxiliary tasks for pretraining. We perform\nexperiments on 10 MRLs in low-resource settings to measure the efficacy of our\nproposed pretraining method and observe an average absolute gain of 2 points\n(UAS) and 3.6 points (LAS). Code and data available at:\nhttps://github.com/jivnesh/LCM\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:26:58 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 07:45:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sandhan", "Jivnesh", ""], ["Krishna", "Amrith", ""], ["Gupta", "Ashim", ""], ["Behera", "Laxmidhar", ""], ["Goyal", "Pawan", ""]]}, {"id": "2102.06558", "submitter": "Jan Niehues", "authors": "Jan Niehues", "title": "Continuous Learning in Neural Machine Translation using Bilingual\n  Dictionaries", "comments": "9 pages, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent advances in deep learning led to significant improvements in\nmachine translation, neural machine translation is often still not able to\ncontinuously adapt to the environment. For humans, as well as for machine\ntranslation, bilingual dictionaries are a promising knowledge source to\ncontinuously integrate new knowledge. However, their exploitation poses several\nchallenges: The system needs to be able to perform one-shot learning as well as\nmodel the morphology of source and target language.\n  In this work, we proposed an evaluation framework to assess the ability of\nneural machine translation to continuously learn new phrases. We integrate\none-shot learning methods for neural machine translation with different word\nrepresentations and show that it is important to address both in order to\nsuccessfully make use of bilingual dictionaries. By addressing both challenges\nwe are able to improve the ability to translate new, rare words and phrases\nfrom 30% to up to 70%. The correct lemma is even generated by more than 90%.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:46:13 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Niehues", "Jan", ""]]}, {"id": "2102.06578", "submitter": "Junwei Liao", "authors": "Junwei Liao, Yu Shi, Ming Gong, Linjun Shou, Hong Qu, Michael Zeng", "title": "Improving Zero-shot Neural Machine Translation on Language-specific\n  Encoders-Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, universal neural machine translation (NMT) with shared\nencoder-decoder gained good performance on zero-shot translation. Unlike\nuniversal NMT, jointly trained language-specific encoders-decoders aim to\nachieve universal representation across non-shared modules, each of which is\nfor a language or language family. The non-shared architecture has the\nadvantage of mitigating internal language competition, especially when the\nshared vocabulary and model parameters are restricted in their size. However,\nthe performance of using multiple encoders and decoders on zero-shot\ntranslation still lags behind universal NMT. In this work, we study zero-shot\ntranslation using language-specific encoders-decoders. We propose to generalize\nthe non-shared architecture and universal NMT by differentiating the\nTransformer layers between language-specific and interlingua. By selectively\nsharing parameters and applying cross-attentions, we explore maximizing the\nrepresentation universality and realizing the best alignment of\nlanguage-agnostic information. We also introduce a denoising auto-encoding\n(DAE) objective to jointly train the model with the translation task in a\nmulti-task manner. Experiments on two public multilingual parallel datasets\nshow that our proposed model achieves a competitive or better results than\nuniversal NMT and strong pivot baseline. Moreover, we experiment incrementally\nadding new language to the trained model by only updating the new model\nparameters. With this little effort, the zero-shot translation between this\nnewly added language and existing languages achieves a comparable result with\nthe model trained jointly from scratch on all languages.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:36:33 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Liao", "Junwei", ""], ["Shi", "Yu", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Qu", "Hong", ""], ["Zeng", "Michael", ""]]}, {"id": "2102.06621", "submitter": "Alex Kogan", "authors": "Dave Dice and Alex Kogan", "title": "Optimizing Inference Performance of Transformers on CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Transformer architecture revolutionized the field of natural language\nprocessing (NLP). Transformers-based models (e.g., BERT) power many important\nWeb services, such as search, translation, question-answering, etc. While\nenormous research attention is paid to the training of those models, relatively\nlittle efforts are made to improve their inference performance. This paper\ncomes to address this gap by presenting an empirical analysis of scalability\nand performance of inferencing a Transformer-based model on CPUs. Focusing on\nthe highly popular BERT model, we identify key components of the Transformer\narchitecture where the bulk of the computation happens, and propose three\noptimizations to speed them up. The optimizations are evaluated using the\ninference benchmark from HuggingFace, and are shown to achieve the speedup of\nup to x2.37. The considered optimizations do not require any changes to the\nimplementation of the models nor affect their accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 17:01:35 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 22:30:35 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 16:54:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dice", "Dave", ""], ["Kogan", "Alex", ""]]}, {"id": "2102.06744", "submitter": "Mario Campos Soberanis", "authors": "Rafael Viana-C\\'amara, Mario Campos-Soberanis, Diego Campos-Sobrino", "title": "Hybrid phonetic-neural model for correction in speech recognition\n  systems", "comments": "13 pages, 3 figures, presented in COMIA 2020\n  (http://smia.mx/comia/2020/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic speech recognition (ASR) is a relevant area in multiple settings\nbecause it provides a natural communication mechanism between applications and\nusers. ASRs often fail in environments that use language specific to particular\napplication domains. Some strategies have been explored to reduce errors in\nclosed ASRs through post-processing, particularly automatic spell checking, and\ndeep learning approaches. In this article, we explore using a deep neural\nnetwork to refine the results of a phonetic correction algorithm applied to a\ntelesales audio database. The results exhibit a reduction in the word error\nrate (WER), both in the original transcription and in the phonetic correction,\nwhich shows the viability of deep learning models together with post-processing\ncorrection strategies to reduce errors made by closed ASRs in specific language\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 19:57:16 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Viana-C\u00e1mara", "Rafael", ""], ["Campos-Soberanis", "Mario", ""], ["Campos-Sobrino", "Diego", ""]]}, {"id": "2102.06749", "submitter": "Linfeng Song", "authors": "Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge and\n  Dong Yu", "title": "Structural Information Preserving for Graph-to-Text Generation", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of graph-to-text generation aims at producing sentences that\npreserve the meaning of input graphs. As a crucial defect, the current\nstate-of-the-art models may mess up or even drop the core structural\ninformation of input graphs when generating outputs. We propose to tackle this\nproblem by leveraging richer training signals that can guide our model for\npreserving input information. In particular, we introduce two types of\nautoencoding losses, each individually focusing on different aspects (a.k.a.\nviews) of input graphs. The losses are then back-propagated to better calibrate\nour model via multi-task training. Experiments on two benchmarks for\ngraph-to-text generation show the effectiveness of our approach over a\nstate-of-the-art baseline. Our code is available at\n\\url{http://github.com/Soistesimmer/AMR-multiview}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:09:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Song", "Linfeng", ""], ["Wang", "Ante", ""], ["Su", "Jinsong", ""], ["Zhang", "Yue", ""], ["Xu", "Kun", ""], ["Ge", "Yubin", ""], ["Yu", "Dong", ""]]}, {"id": "2102.06750", "submitter": "Milind Rao", "authors": "Milind Rao, Pranav Dheram, Gautam Tiwari, Anirudh Raju, Jasha Droppo,\n  Ariya Rastrow, Andreas Stolcke", "title": "Do as I mean, not as I say: Sequence Loss Training for Spoken Language\n  Understanding", "comments": "Proc. IEEE ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU) systems extract transcriptions, as well\nas semantics of intent or named entities from speech, and are essential\ncomponents of voice activated systems. SLU models, which either directly\nextract semantics from audio or are composed of pipelined automatic speech\nrecognition (ASR) and natural language understanding (NLU) models, are\ntypically trained via differentiable cross-entropy losses, even when the\nrelevant performance metrics of interest are word or semantic error rates. In\nthis work, we propose non-differentiable sequence losses based on SLU metrics\nas a proxy for semantic error and use the REINFORCE trick to train ASR and SLU\nmodels with this loss. We show that custom sequence loss training is the\nstate-of-the-art on open SLU datasets and leads to 6% relative improvement in\nboth ASR and NLU performance metrics on large proprietary datasets. We also\ndemonstrate how the semantic sequence loss training paradigm can be used to\nupdate ASR and SLU models without transcripts, using semantic feedback alone.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:09:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Rao", "Milind", ""], ["Dheram", "Pranav", ""], ["Tiwari", "Gautam", ""], ["Raju", "Anirudh", ""], ["Droppo", "Jasha", ""], ["Rastrow", "Ariya", ""], ["Stolcke", "Andreas", ""]]}, {"id": "2102.06762", "submitter": "Nikos Voskarides", "authors": "Nikos Voskarides", "title": "Supporting search engines with knowledge and context", "comments": "PhD thesis of Nikos Voskarides", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines leverage knowledge to improve information access. In order to\neffectively leverage knowledge, search engines should account for context,\ni.e., information about the user and query. In this thesis, we aim to support\nsearch engines in leveraging knowledge while accounting for context. In the\nfirst part of this thesis, we study how to make structured knowledge more\naccessible to the user when the search engine proactively provides such\nknowledge as context to enrich search results. As a first task, we study how to\nretrieve descriptions of knowledge facts from a text corpus. Next, we study how\nto automatically generate knowledge fact descriptions. And finally, we study\nhow to contextualize knowledge facts, that is, to automatically find facts\nrelated to a query fact. In the second part of this thesis, we study how to\nimprove interactive knowledge gathering. We focus on conversational search,\nwhere the user interacts with the search engine to gather knowledge over large\nunstructured knowledge repositories. We focus on multi-turn passage retrieval\nas an instance of conversational search. We propose to model query resolution\nas a term classification task and propose a method to address it. In the final\npart of this thesis, we focus on search engine support for professional writers\nin the news domain. We study how to support such writers create\nevent-narratives by exploring knowledge from a corpus of news articles. We\npropose a dataset construction procedure for this task that relies on existing\nnews articles to simulate incomplete narratives and relevant articles. We study\nthe performance of multiple rankers, lexical and semantic, and provide insights\ninto the characteristics of this task.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:28:25 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Voskarides", "Nikos", ""]]}, {"id": "2102.06788", "submitter": "Tony Sun", "authors": "Tony Sun, Kellie Webster, Apu Shah, William Yang Wang, Melvin Johnson", "title": "They, Them, Theirs: Rewriting with Gender-Neutral English", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Responsible development of technology involves applications being inclusive\nof the diverse set of users they hope to support. An important part of this is\nunderstanding the many ways to refer to a person and being able to fluently\nchange between the different forms as needed. We perform a case study on the\nsingular they, a common way to promote gender inclusion in English. We define a\nre-writing task, create an evaluation benchmark, and show how a model can be\ntrained to produce gender-neutral English with <1% word error rate with no\nhuman-labeled data. We discuss the practical applications and ethical\nconsiderations of the task, providing direction for future work into inclusive\nnatural language systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 21:47:48 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sun", "Tony", ""], ["Webster", "Kellie", ""], ["Shah", "Apu", ""], ["Wang", "William Yang", ""], ["Johnson", "Melvin", ""]]}, {"id": "2102.06793", "submitter": "Ernest Davis", "authors": "Ernest Davis", "title": "Unanswerable Questions about Images and Texts", "comments": "15 pages, 4 figures", "journal-ref": "Frontiers in Artificial Intelligence: Language and Computation.\n  July 2020", "doi": "10.3389/frai.2020.00051", "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Questions about a text or an image that cannot be answered raise distinctive\nissues for an AI. This note discusses the problem of unanswerable questions in\nVQA (visual question answering), in QA (visual question answering), and in AI\ngenerally.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:56:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Davis", "Ernest", ""]]}, {"id": "2102.06815", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov, Zico Kolter", "title": "Exploring Classic and Neural Lexical Translation Models for Information\n  Retrieval: Interpretability, Effectiveness, and Efficiency Benefits", "comments": null, "journal-ref": "ECIR 2021 (The 43rd European Conference on Information Retrieval)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the utility of the lexical translation model (IBM Model 1) for\nEnglish text retrieval, in particular, its neural variants that are trained\nend-to-end. We use the neural Model1 as an aggregator layer applied to\ncontext-free or contextualized query/document embeddings. This new approach to\ndesign a neural ranking system has benefits for effectiveness, efficiency, and\ninterpretability. Specifically, we show that adding an interpretable neural\nModel 1 layer on top of BERT-based contextualized embeddings (1) does not\ndecrease accuracy and/or efficiency; and (2) may overcome the limitation on the\nmaximum sequence length of existing BERT models. The context-free neural Model\n1 is less effective than a BERT-based ranking model, but it can run efficiently\non a CPU (without expensive index-time precomputation or query-time operations\non large tensors). Using Model 1 we produced best neural and non-neural runs on\nthe MS MARCO document ranking leaderboard in late 2020.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 23:21:55 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:43:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Boytsov", "Leonid", ""], ["Kolter", "Zico", ""]]}, {"id": "2102.06820", "submitter": "Li Lucy", "authors": "Li Lucy and David Bamman", "title": "Characterizing English Variation across Social Media Communities with\n  BERT", "comments": "18 pages, 5 figures, accepted to TACL 2021, please cite that version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much previous work characterizing language variation across Internet social\ngroups has focused on the types of words used by these groups. We extend this\ntype of study by employing BERT to characterize variation in the senses of\nwords as well, analyzing two months of English comments in 474 Reddit\ncommunities. The specificity of different sense clusters to a community,\ncombined with the specificity of a community's unique word types, is used to\nidentify cases where a social group's language deviates from the norm. We\nvalidate our metrics using user-created glossaries and draw on sociolinguistic\ntheories to connect language variation with trends in community behavior. We\nfind that communities with highly distinctive language are medium-sized, and\ntheir loyal and highly engaged users interact in dense networks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 23:50:57 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lucy", "Li", ""], ["Bamman", "David", ""]]}, {"id": "2102.06856", "submitter": "Wei Wang", "authors": "Wei Wang, Piji Li, Hai-Tao Zheng", "title": "Generating Diversified Comments via Reader-Aware Topic Modeling and\n  Saliency Detection", "comments": "AAAI 2021. The potential ethical issues are also discussed in detail", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic comment generation is a special and challenging task to verify the\nmodel ability on news content comprehension and language generation. Comments\nnot only convey salient and interesting information in news articles, but also\nimply various and different reader characteristics which we treat as the\nessential clues for diversity. However, most of the comment generation\napproaches only focus on saliency information extraction, while the\nreader-aware factors implied by comments are neglected. To address this issue,\nwe propose a unified reader-aware topic modeling and saliency information\ndetection framework to enhance the quality of generated comments. For\nreader-aware topic modeling, we design a variational generative clustering\nalgorithm for latent semantic learning and topic mining from reader comments.\nFor saliency information detection, we introduce Bernoulli distribution\nestimating on news content to select saliency information. The obtained topic\nrepresentations as well as the selected saliency information are incorporated\ninto the decoder to generate diversified and informative comments. Experimental\nresults on three datasets show that our framework outperforms existing baseline\nmethods in terms of both automatic metrics and human evaluation. The potential\nethical issues are also discussed in detail.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 03:50:31 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Piji", ""], ["Zheng", "Hai-Tao", ""]]}, {"id": "2102.06859", "submitter": "Shujian Zhang", "authors": "Shujian Zhang, Chengyue Gong, Eunsol Choi", "title": "Capturing Label Distribution: A Case Study in NLI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimating inherent human disagreement (annotation label\ndistribution) in natural language inference task. Post-hoc smoothing of the\npredicted label distribution to match the expected label entropy is very\neffective. Such simple manipulation can reduce KL divergence by almost half,\nyet will not improve majority label prediction accuracy or learn label\ndistributions. To this end, we introduce a small amount of examples with\nmultiple references into training. We depart from the standard practice of\ncollecting a single reference per each training example, and find that\ncollecting multiple references can achieve better accuracy under the fixed\nannotation budget. Lastly, we provide rich analyses comparing these two methods\nfor improving label distribution estimation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 04:14:31 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Shujian", ""], ["Gong", "Chengyue", ""], ["Choi", "Eunsol", ""]]}, {"id": "2102.06991", "submitter": "Isa Inuwa-Dutse", "authors": "Isa Inuwa-Dutse", "title": "The first large scale collection of diverse Hausa language datasets", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hausa language belongs to the Afroasiatic phylum, and with more\nfirst-language speakers than any other sub-Saharan African language. With a\nmajority of its speakers residing in the Northern and Southern areas of Nigeria\nand the Republic of Niger, respectively, it is estimated that over 100 million\npeople speak the language. Hence, making it one of the most spoken Chadic\nlanguage. While Hausa is considered well-studied and documented language among\nthe sub-Saharan African languages, it is viewed as a low resource language from\nthe perspective of natural language processing (NLP) due to limited resources\nto utilise in NLP-related tasks. This is common to most languages in Africa;\nthus, it is crucial to enrich such languages with resources that will support\nand speed the pace of conducting various downstream tasks to meet the demand of\nthe modern society. While there exist useful datasets, notably from news sites\nand religious texts, more diversity is needed in the corpus.\n  We provide an expansive collection of curated datasets consisting of both\nformal and informal forms of the language from refutable websites and online\nsocial media networks, respectively. The collection is large and more diverse\nthan the existing corpora by providing the first and largest set of Hausa\nsocial media data posts to capture the peculiarities in the language. The\ncollection also consists of a parallel dataset, which can be used for tasks\nsuch as machine translation with applications in areas such as the detection of\nspurious or inciteful online content. We describe the curation process -- from\nthe collection, preprocessing and how to obtain the data -- and proffer some\nresearch problems that could be addressed using the data.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 19:34:20 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 20:13:34 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Inuwa-Dutse", "Isa", ""]]}, {"id": "2102.07024", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Dipendra Misra, Robert Schapire, Miro Dud\\'ik, Patrick\n  Shafto", "title": "Interactive Learning from Activity Description", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel interactive learning protocol that enables training\nrequest-fulfilling agents by verbally describing their activities. Unlike\nimitation learning (IL), our protocol allows the teaching agent to provide\nfeedback in a language that is most appropriate for them. Compared with reward\nin reinforcement learning (RL), the description feedback is richer and allows\nfor improved sample complexity. We develop a probabilistic framework and an\nalgorithm that practically implements our protocol. Empirical results in two\nchallenging request-fulfilling problems demonstrate the strengths of our\napproach: compared with RL baselines, it is more sample-efficient; compared\nwith IL baselines, it achieves competitive success rates without requiring the\nteaching agent to be able to demonstrate the desired behavior using the\nlearning agent's actions. Apart from empirical evaluation, we also provide\ntheoretical guarantees for our algorithm under certain assumptions about the\nteacher and the environment.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 22:51:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 23:40:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Nguyen", "Khanh", ""], ["Misra", "Dipendra", ""], ["Schapire", "Robert", ""], ["Dud\u00edk", "Miro", ""], ["Shafto", "Patrick", ""]]}, {"id": "2102.07033", "submitter": "Patrick Lewis", "authors": "Patrick Lewis and Yuxiang Wu and Linqing Liu and Pasquale Minervini\n  and Heinrich K\\\"uttler and Aleksandra Piktus and Pontus Stenetorp and\n  Sebastian Riedel", "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain Question Answering models which directly leverage question-answer\n(QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show\npromise in terms of speed and memory compared to conventional models which\nretrieve and read from text corpora. QA-pair retrievers also offer\ninterpretable answers, a high degree of control, and are trivial to update at\ntest time with new knowledge. However, these models lack the accuracy of\nretrieve-and-read systems, as substantially less knowledge is covered by the\navailable QA-pairs relative to text corpora like Wikipedia. To facilitate\nimproved QA-pair models, we introduce Probably Asked Questions (PAQ), a very\nlarge resource of 65M automatically-generated QA-pairs. We introduce a new\nQA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and\ncaches test questions, enabling RePAQ to match the accuracy of recent\nretrieve-and-read models, whilst being significantly faster. Using PAQ, we\ntrain CBQA models which outperform comparable baselines by 5%, but trail RePAQ\nby over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be\nconfigured for size (under 500MB) or speed (over 1K questions per second)\nwhilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at\nselective QA, abstaining from answering when it is likely to be incorrect. This\nenables RePAQ to ``back-off\" to a more expensive state-of-the-art model,\nleading to a combined system which is both more accurate and 2x faster than the\nstate-of-the-art model alone.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 23:43:45 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lewis", "Patrick", ""], ["Wu", "Yuxiang", ""], ["Liu", "Linqing", ""], ["Minervini", "Pasquale", ""], ["K\u00fcttler", "Heinrich", ""], ["Piktus", "Aleksandra", ""], ["Stenetorp", "Pontus", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2102.07043", "submitter": "Haitian Sun", "authors": "Haitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan Salakhutdinov, William\n  W. Cohen", "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations", "comments": "Accepted at the 38th International Conference on Machine Learning,\n  PMLR 139, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 01:29:54 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 19:34:42 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sun", "Haitian", ""], ["Verga", "Pat", ""], ["Dhingra", "Bhuwan", ""], ["Salakhutdinov", "Ruslan", ""], ["Cohen", "William W.", ""]]}, {"id": "2102.07061", "submitter": "Jinmiao Huang", "authors": "Jinmiao Huang, Waseem Gharbieh, Han Suk Shim, Eugene Kim", "title": "Query-by-Example Keyword Spotting system using Multi-head Attention and\n  Softtriple Loss", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a neural network architecture for tackling the\nquery-by-example user-defined keyword spotting task. A multi-head attention\nmodule is added on top of a multi-layered GRU for effective feature extraction,\nand a normalized multi-head attention module is proposed for feature\naggregation. We also adopt the softtriple loss - a combination of triplet loss\nand softmax loss - and showcase its effectiveness. We demonstrate the\nperformance of our model on internal datasets with different languages and the\npublic Hey-Snips dataset. We compare the performance of our model to a baseline\nsystem and conduct an ablation study to show the benefit of each component in\nour architecture. The proposed work shows solid performance while preserving\nsimplicity.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 03:37:37 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 20:53:01 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Jinmiao", ""], ["Gharbieh", "Waseem", ""], ["Shim", "Han Suk", ""], ["Kim", "Eugene", ""]]}, {"id": "2102.07150", "submitter": "Kushal Kedia", "authors": "Kushal Kedia, Abhilash Nandy", "title": "indicnlp@kgp at DravidianLangTech-EACL2021: Offensive Language\n  Identification in Dravidian Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents the submission of the team indicnlp@kgp to the EACL 2021\nshared task \"Offensive Language Identification in Dravidian Languages.\" The\ntask aimed to classify different offensive content types in 3 code-mixed\nDravidian language datasets. The work leverages existing state of the art\napproaches in text classification by incorporating additional data and transfer\nlearning on pre-trained models. Our final submission is an ensemble of an\nAWD-LSTM based model along with 2 different transformer model architectures\nbased on BERT and RoBERTa. We achieved weighted-average F1 scores of 0.97,\n0.77, and 0.72 in the Malayalam-English, Tamil-English, and Kannada-English\ndatasets ranking 1st, 2nd, and 3rd on the respective tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 13:24:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kedia", "Kushal", ""], ["Nandy", "Abhilash", ""]]}, {"id": "2102.07219", "submitter": "Christophe Van Gysel", "authors": "Sashank Gondala, Lyan Verwimp, Ernest Pusateri, Manos Tsagkias,\n  Christophe Van Gysel", "title": "Error-driven Pruning of Language Models for Virtual Assistants", "comments": "ICASSP '21. The 46th International IEEE Conference on Acoustics,\n  Speech, and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) for virtual assistants (VAs) are typically trained on\nlarge amounts of data, resulting in prohibitively large models which require\nexcessive memory and/or cannot be used to serve user requests in real-time.\nEntropy pruning results in smaller models but with significant degradation of\neffectiveness in the tail of the user request distribution. We customize\nentropy pruning by allowing for a keep list of infrequent n-grams that require\na more relaxed pruning threshold, and propose three methods to construct the\nkeep list. Each method has its own advantages and disadvantages with respect to\nLM size, ASR accuracy and cost of constructing the keep list. Our best LM gives\n8% average Word Error Rate (WER) reduction on a targeted test set, but is 3\ntimes larger than the baseline. We also propose discriminative methods to\nreduce the size of the LM while retaining the majority of the WER gains\nachieved by the largest LM.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 18:47:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gondala", "Sashank", ""], ["Verwimp", "Lyan", ""], ["Pusateri", "Ernest", ""], ["Tsagkias", "Manos", ""], ["Van Gysel", "Christophe", ""]]}, {"id": "2102.07259", "submitter": "Priyabarta Karmakar PhD", "authors": "Priyabrata Karmakar, Shyh Wei Teng, Guojun Lu", "title": "Thank you for Attention: A survey on Attention-based Artificial Neural\n  Networks for Automatic Speech Recognition", "comments": "Submitted to IEEE/ACM Trans. on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is a very popular and effective mechanism in artificial neural\nnetwork-based sequence-to-sequence models. In this survey paper, a\ncomprehensive review of the different attention models used in developing\nautomatic speech recognition systems is provided. The paper focuses on the\ndevelopment and evolution of attention models for offline and streaming speech\nrecognition within recurrent neural network- and Transformer- based\narchitectures.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 22:28:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Karmakar", "Priyabrata", ""], ["Teng", "Shyh Wei", ""], ["Lu", "Guojun", ""]]}, {"id": "2102.07349", "submitter": "Yu Zhang", "authors": "Yu Zhang, Zhihong Shen, Yuxiao Dong, Kuansan Wang, Jiawei Han", "title": "MATCH: Metadata-Aware Text Classification in A Large Hierarchy", "comments": "12 pages; Accepted to WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label text classification refers to the problem of assigning each given\ndocument its most relevant labels from the label set. Commonly, the metadata of\nthe given documents and the hierarchy of the labels are available in real-world\napplications. However, most existing studies focus on only modeling the text\ninformation, with a few attempts to utilize either metadata or hierarchy\nsignals, but not both of them. In this paper, we bridge the gap by formalizing\nthe problem of metadata-aware text classification in a large label hierarchy\n(e.g., with tens of thousands of labels). To address this problem, we present\nthe MATCH solution -- an end-to-end framework that leverages both metadata and\nhierarchy information. To incorporate metadata, we pre-train the embeddings of\ntext and metadata in the same space and also leverage the fully-connected\nattentions to capture the interrelations between them. To leverage the label\nhierarchy, we propose different ways to regularize the parameters and output\nprobability of each child label by its parents. Extensive experiments on two\nmassive text datasets with large-scale label hierarchies demonstrate the\neffectiveness of MATCH over state-of-the-art deep learning baselines.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 05:23:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Yu", ""], ["Shen", "Zhihong", ""], ["Dong", "Yuxiao", ""], ["Wang", "Kuansan", ""], ["Han", "Jiawei", ""]]}, {"id": "2102.07350", "submitter": "Laria Reynolds", "authors": "Laria Reynolds and Kyle McDonell", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prevailing methods for mapping large generative language models to supervised\ntasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as\na case study, we show that 0-shot prompts can significantly outperform few-shot\nprompts. We suggest that the function of few-shot examples in these cases is\nbetter described as locating an already learned task rather than meta-learning.\nThis analysis motivates rethinking the role of prompts in controlling and\nevaluating powerful language models. In this work, we discuss methods of prompt\nprogramming, emphasizing the usefulness of considering prompts through the lens\nof natural language. We explore techniques for exploiting the capacity of\nnarratives and cultural anchors to encode nuanced intentions and techniques for\nencouraging deconstruction of a problem into components before producing a\nverdict. Informed by this more encompassing theory of prompt programming, we\nalso introduce the idea of a metaprompt that seeds the model to generate its\nown natural language prompts for a range of tasks. Finally, we discuss how\nthese more general methods of interacting with language models can be\nincorporated into existing and future benchmarks and practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 05:27:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Reynolds", "Laria", ""], ["McDonell", "Kyle", ""]]}, {"id": "2102.07370", "submitter": "Bidisha Sharma", "authors": "Bidisha Sharma, Maulik Madhavi and Haizhou Li", "title": "Leveraging Acoustic and Linguistic Embeddings from Pretrained speech and\n  language Models for Intent Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent classification is a task in spoken language understanding. An intent\nclassification system is usually implemented as a pipeline process, with a\nspeech recognition module followed by text processing that classifies the\nintents. There are also studies of end-to-end system that takes acoustic\nfeatures as input and classifies the intents directly. Such systems don't take\nadvantage of relevant linguistic information, and suffer from limited training\ndata. In this work, we propose a novel intent classification framework that\nemploys acoustic features extracted from a pretrained speech recognition system\nand linguistic features learned from a pretrained language model. We use\nknowledge distillation technique to map the acoustic embeddings towards\nlinguistic embeddings. We perform fusion of both acoustic and linguistic\nembeddings through cross-attention approach to classify intents. With the\nproposed method, we achieve 90.86% and 99.07% accuracy on ATIS and Fluent\nspeech corpus, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:20:06 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sharma", "Bidisha", ""], ["Madhavi", "Maulik", ""], ["Li", "Haizhou", ""]]}, {"id": "2102.07380", "submitter": "Mana Ihori", "authors": "Mana Ihori, Naoki Makishima, Tomohiro Tanaka, Akihiko Takashima, Shota\n  Orihashi and Ryo Masumura", "title": "MAPGN: MAsked Pointer-Generator Network for sequence-to-sequence\n  pre-training", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-supervised learning method for pointer-generator\nnetworks to improve spoken-text normalization. Spoken-text normalization that\nconverts spoken-style text into style normalized text is becoming an important\ntechnology for improving subsequent processing such as machine translation and\nsummarization. The most successful spoken-text normalization method to date is\nsequence-to-sequence (seq2seq) mapping using pointer-generator networks that\npossess a copy mechanism from an input sequence. However, these models require\na large amount of paired data of spoken-style text and style normalized text,\nand it is difficult to prepare such a volume of data. In order to construct\nspoken-text normalization model from the limited paired data, we focus on\nself-supervised learning which can utilize unpaired text data to improve\nseq2seq models. Unfortunately, conventional self-supervised learning methods do\nnot assume that pointer-generator networks are utilized. Therefore, we propose\na novel self-supervised learning method, MAsked Pointer-Generator Network\n(MAPGN). The proposed method can effectively pre-train the pointer-generator\nnetwork by learning to fill masked tokens using the copy mechanism. Our\nexperiments demonstrate that MAPGN is more effective for pointer-generator\nnetworks than the conventional self-supervised learning methods in two\nspoken-text normalization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:44:25 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 02:30:25 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ihori", "Mana", ""], ["Makishima", "Naoki", ""], ["Tanaka", "Tomohiro", ""], ["Takashima", "Akihiko", ""], ["Orihashi", "Shota", ""], ["Masumura", "Ryo", ""]]}, {"id": "2102.07396", "submitter": "Valtteri Skantsi PhD", "authors": "Liina Repo, Valtteri Skantsi, Samuel R\\\"onnqvist, Saara Hellstr\\\"om,\n  Miika Oinonen, Anna Salmela, Douglas Biber, Jesse Egbert, Sampo Pyysalo and\n  Veronika Laippala", "title": "Beyond the English Web: Zero-Shot Cross-Lingual and Lightweight\n  Monolingual Classification of Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore cross-lingual transfer of register classification for web\ndocuments. Registers, that is, text varieties such as blogs or news are one of\nthe primary predictors of linguistic variation and thus affect the automatic\nprocessing of language. We introduce two new register annotated corpora,\nFreCORE and SweCORE, for French and Swedish. We demonstrate that deep\npre-trained language models perform strongly in these languages and outperform\nprevious state-of-the-art in English and Finnish. Specifically, we show 1) that\nzero-shot cross-lingual transfer from the large English CORE corpus can match\nor surpass previously published monolingual models, and 2) that lightweight\nmonolingual classification requiring very little training data can reach or\nsurpass our zero-shot performance. We further analyse classification results\nfinding that certain registers continue to pose challenges in particular for\ncross-lingual transfer.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 08:40:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Repo", "Liina", ""], ["Skantsi", "Valtteri", ""], ["R\u00f6nnqvist", "Samuel", ""], ["Hellstr\u00f6m", "Saara", ""], ["Oinonen", "Miika", ""], ["Salmela", "Anna", ""], ["Biber", "Douglas", ""], ["Egbert", "Jesse", ""], ["Pyysalo", "Sampo", ""], ["Laippala", "Veronika", ""]]}, {"id": "2102.07492", "submitter": "Baptiste Roziere", "authors": "Baptiste Roziere, Marie-Anne Lachaux, Marc Szafraniec and Guillaume\n  Lample", "title": "DOBF: A Deobfuscation Pre-Training Objective for Programming Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-supervised learning have dramatically improved the\nstate of the art on a wide variety of tasks. However, research in language\nmodel pre-training has mostly focused on natural languages, and it is unclear\nwhether models like BERT and its variants provide the best pre-training when\napplied to other modalities, such as source code. In this paper, we introduce a\nnew pre-training objective, DOBF, that leverages the structural aspect of\nprogramming languages and pre-trains a model to recover the original version of\nobfuscated source code. We show that models pre-trained with DOBF significantly\noutperform existing approaches on multiple downstream tasks, providing relative\nimprovements of up to 13% in unsupervised code translation, and 24% in natural\nlanguage code search. Incidentally, we found that our pre-trained model is able\nto de-obfuscate fully obfuscated source files, and to suggest descriptive\nvariable names.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:50:47 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 20:42:12 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Roziere", "Baptiste", ""], ["Lachaux", "Marie-Anne", ""], ["Szafraniec", "Marc", ""], ["Lample", "Guillaume", ""]]}, {"id": "2102.07594", "submitter": "Ye Bai", "authors": "Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi Wen, Shuai\n  Zhang", "title": "Fast End-to-End Speech Recognition via Non-Autoregressive Models and\n  Cross-Modal Knowledge Transferring from BERT", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder-decoder (AED) models have achieved promising\nperformance in speech recognition. However, because the decoder predicts text\ntokens (such as characters or words) in an autoregressive manner, it is\ndifficult for an AED model to predict all tokens in parallel. This makes the\ninference speed relatively slow. We believe that because the encoder already\ncaptures the whole speech utterance, which has the token-level relationship\nimplicitly, we can predict a token without explicitly autoregressive language\nmodeling. When the prediction of a token does not rely on other tokens, the\nparallel prediction of all tokens in the sequence is realizable. Based on this\nidea, we propose a non-autoregressive speech recognition model called LASO\n(Listen Attentively, and Spell Once). The model consists of an encoder, a\ndecoder, and a position dependent summarizer (PDS). The three modules are based\non basic attention blocks. The encoder extracts high-level representations from\nthe speech. The PDS uses positional encodings corresponding to tokens to\nconvert the acoustic representations into token-level representations. The\ndecoder further captures token-level relationships with the self-attention\nmechanism. At last, the probability distribution on the vocabulary is computed\nfor each token position. Therefore, speech recognition is re-formulated as a\nposition-wise classification problem. Further, we propose a cross-modal\ntransfer learning method to refine semantics from a large-scale pre-trained\nlanguage model BERT for improving the performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:18:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 01:58:56 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 09:06:42 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 06:27:22 GMT"}, {"version": "v5", "created": "Mon, 31 May 2021 06:29:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bai", "Ye", ""], ["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Tian", "Zhengkun", ""], ["Wen", "Zhengqi", ""], ["Zhang", "Shuai", ""]]}, {"id": "2102.07635", "submitter": "Rohan Sukumaran", "authors": "Rohan Sukumaran", "title": "Improved Customer Transaction Classification using Semi-Supervised\n  Knowledge Distillation", "comments": "8 pages, 1 figure, 7 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In pickup and delivery services, transaction classification based on customer\nprovided free text is a challenging problem. It involves the association of a\nwide variety of customer inputs to a fixed set of categories while adapting to\nthe various customer writing styles. This categorization is important for the\nbusiness: it helps understand the market needs and trends, and also assist in\nbuilding a personalized experience for different segments of the customers.\nHence, it is vital to capture these category information trends at scale, with\nhigh precision and recall. In this paper, we focus on a specific use-case where\na single category drives each transaction. We propose a cost-effective\ntransaction classification approach based on semi-supervision and knowledge\ndistillation frameworks. The approach identifies the category of a transaction\nusing free text input given by the customer. We use weak labelling and notice\nthat the performance gains are similar to that of using human-annotated\nsamples. On a large internal dataset and on 20Newsgroup dataset, we see that\nRoBERTa performs the best for the categorization tasks. Further, using an\nALBERT model (it has 33x fewer parameters vis-a-vis parameters of RoBERTa),\nwith RoBERTa as the Teacher, we see a performance similar to that of RoBERTa\nand better performance over unadapted ALBERT. This framework, with ALBERT as a\nstudent and RoBERTa as teacher, is further referred to as R-ALBERT in this\npaper. The model is in production and is used by business to understand\nchanging trends and take appropriate decisions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:16:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sukumaran", "Rohan", ""]]}, {"id": "2102.07662", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz and Daniel Campos", "title": "Overview of the TREC 2020 deep learning track", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.07820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the second year of the TREC Deep Learning Track, with the goal of\nstudying ad hoc ranking in the large training data regime. We again have a\ndocument retrieval task and a passage retrieval task, each with hundreds of\nthousands of human-labeled training queries. We evaluate using single-shot\nTREC-style evaluation, to give us a picture of which ranking methods work best\nwhen large data is available, with much more comprehensive relevance labeling\non the small number of test queries. This year we have further evidence that\nrankers with BERT-style pretraining outperform other rankers in the large data\nregime.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:47:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Campos", "Daniel", ""]]}, {"id": "2102.07739", "submitter": "Aditya Gourav", "authors": "Aditya Gourav, Linda Liu, Ankur Gandhe, Yile Gu, Guitang Lan,\n  Xiangyang Huang, Shashank Kalmane, Gautam Tiwari, Denis Filimonov, Ariya\n  Rastrow, Andreas Stolcke, Ivan Bulyko", "title": "Personalization Strategies for End-to-End Speech Recognition Systems", "comments": "5 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The recognition of personalized content, such as contact names, remains a\nchallenging problem for end-to-end speech recognition systems. In this work, we\ndemonstrate how first and second-pass rescoring strategies can be leveraged\ntogether to improve the recognition of such words. Following previous work, we\nuse a shallow fusion approach to bias towards recognition of personalized\ncontent in the first-pass decoding. We show that such an approach can improve\npersonalized content recognition by up to 16% with minimum degradation on the\ngeneral use case. We describe a fast and scalable algorithm that enables our\nbiasing models to remain at the word-level, while applying the biasing at the\nsubword level. This has the advantage of not requiring the biasing models to be\ndependent on any subword symbol table. We also describe a novel second-pass\nde-biasing approach: used in conjunction with a first-pass shallow fusion that\noptimizes on oracle WER, we can achieve an additional 14% improvement on\npersonalized content recognition, and even improve accuracy for the general use\ncase by up to 2.5%.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 18:36:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gourav", "Aditya", ""], ["Liu", "Linda", ""], ["Gandhe", "Ankur", ""], ["Gu", "Yile", ""], ["Lan", "Guitang", ""], ["Huang", "Xiangyang", ""], ["Kalmane", "Shashank", ""], ["Tiwari", "Gautam", ""], ["Filimonov", "Denis", ""], ["Rastrow", "Ariya", ""], ["Stolcke", "Andreas", ""], ["Bulyko", "Ivan", ""]]}, {"id": "2102.07836", "submitter": "Yanzhu Guo", "authors": "Yanzhu Guo, Christos Xypolopoulos and Michalis Vazirgiannis", "title": "How COVID-19 Is Changing Our Language : Detecting Semantic Shift in\n  Twitter Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words are malleable objects, influenced by events that are reflected in\nwritten texts. Situated in the global outbreak of COVID-19, our research aims\nat detecting semantic shifts in social media language triggered by the health\ncrisis. With COVID-19 related big data extracted from Twitter, we train\nseparate word embedding models for different time periods after the outbreak.\nWe employ an alignment-based approach to compare these embeddings with a\ngeneral-purpose Twitter embedding unrelated to COVID-19. We also compare our\ntrained embeddings among them to observe diachronic evolution. Carrying out\ncase studies on a set of words chosen by topic detection, we verify that our\nalignment approach is valid. Finally, we quantify the size of global semantic\nshift by a stability measure based on back-and-forth rotational alignment.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:29:00 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Guo", "Yanzhu", ""], ["Xypolopoulos", "Christos", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2102.07844", "submitter": "Samir Passi", "authors": "Samir Passi, Phoebe Sengers", "title": "\"From What I see, this makes sense\": Seeing meaning in algorithmic\n  results", "comments": "Paper presented at \"Algorithms at work: Empirical Diversity, Analytic\n  Vocabularies, and Design Implications\" workshop at the 2016 ACM conference on\n  Computer Supported Cooperative Work & Social Computing (CSCW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this workshop paper, we use an empirical example from our ongoing\nfieldwork, to showcase the complexity and situatedness of the process of making\nsense of algorithmic results; i.e. how to evaluate, validate, and contextualize\nalgorithmic outputs. So far, in our research work, we have focused on such\nsense-making processes in data analytic learning environments such as\nclassrooms and training workshops. Multiple moments in our fieldwork suggest\nthat meaning, in data analytics, is constructed through an iterative and\nreflexive dialogue between data, code, assumptions, prior knowledge, and\nalgorithmic results. A data analytic result is nothing short of a\nsociotechnical accomplishment - one in which it is extremely difficult, if not\nat times impossible, to clearly distinguish between 'human' and 'technical'\nforms of data analytic work. We conclude this paper with a set of questions\nthat we would like to explore further in this workshop.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:50:11 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 17:14:38 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Passi", "Samir", ""], ["Sengers", "Phoebe", ""]]}, {"id": "2102.07847", "submitter": "Hieu Pham", "authors": "Hieu Pham, Xinyi Wang, Yiming Yang, Graham Neubig", "title": "Meta Back-translation", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Back-translation is an effective strategy to improve the performance of\nNeural Machine Translation~(NMT) by generating pseudo-parallel data. However,\nseveral recent works have found that better translation quality of the\npseudo-parallel data does not necessarily lead to better final translation\nmodels, while lower-quality but more diverse data often yields stronger\nresults. In this paper, we propose a novel method to generate pseudo-parallel\ndata from a pre-trained back-translation model. Our method is a meta-learning\nalgorithm which adapts a pre-trained back-translation model so that the\npseudo-parallel data it generates would train a forward-translation model to do\nwell on a validation set. In our evaluations in both the standard datasets WMT\nEn-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our\nmethod leads to significant improvements over strong baselines. Our code will\nbe made available.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:58:32 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pham", "Hieu", ""], ["Wang", "Xinyi", ""], ["Yang", "Yiming", ""], ["Neubig", "Graham", ""]]}, {"id": "2102.07919", "submitter": "Haolan Zhan", "authors": "Haolan Zhan, Hainan Zhang, Hongshen Chen, Lei Shen, Yanyan Lan, Zhuoye\n  Ding, Dawei Yin", "title": "User-Inspired Posterior Network for Recommendation Reason Generation", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation reason generation, aiming at showing the selling points of\nproducts for customers, plays a vital role in attracting customers' attention\nas well as improving user experience. A simple and effective way is to extract\nkeywords directly from the knowledge-base of products, i.e., attributes or\ntitle, as the recommendation reason. However, generating recommendation reason\nfrom product knowledge doesn't naturally respond to users' interests.\nFortunately, on some E-commerce websites, there exists more and more\nuser-generated content (user-content for short), i.e., product\nquestion-answering (QA) discussions, which reflect user-cared aspects.\nTherefore, in this paper, we consider generating the recommendation reason by\ntaking into account not only the product attributes but also the\ncustomer-generated product QA discussions. In reality, adequate user-content is\nonly possible for the most popular commodities, whereas large sums of long-tail\nproducts or new products cannot gather a sufficient number of user-content. To\ntackle this problem, we propose a user-inspired multi-source posterior\ntransformer (MSPT), which induces the model reflecting the users' interests\nwith a posterior multiple QA discussions module, and generating recommendation\nreasons containing the product attributes as well as the user-cared aspects.\nExperimental results show that our model is superior to traditional generative\nmodels. Additionally, the analysis also shows that our model can focus more on\nthe user-cared aspects than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 02:08:52 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhan", "Haolan", ""], ["Zhang", "Hainan", ""], ["Chen", "Hongshen", ""], ["Shen", "Lei", ""], ["Lan", "Yanyan", ""], ["Ding", "Zhuoye", ""], ["Yin", "Dawei", ""]]}, {"id": "2102.07926", "submitter": "Ziyang Luo", "authors": "Ziyang Luo", "title": "Have Attention Heads in BERT Learned Constituency Grammar?", "comments": "Accept at the EACL 2021 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of pre-trained language models in recent years, more and\nmore researchers focus on opening the \"black box\" of these models. Following\nthis interest, we carry out a qualitative and quantitative analysis of\nconstituency grammar in attention heads of BERT and RoBERTa. We employ the\nsyntactic distance method to extract implicit constituency grammar from the\nattention weights of each head. Our results show that there exist heads that\ncan induce some grammar types much better than baselines, suggesting that some\nheads act as a proxy for constituency grammar. We also analyze how attention\nheads' constituency grammar inducing (CGI) ability changes after fine-tuning\nwith two kinds of tasks, including sentence meaning similarity (SMS) tasks and\nnatural language inference (NLI) tasks. Our results suggest that SMS tasks\ndecrease the average CGI ability of upper layers, while NLI tasks increase it.\nLastly, we investigate the connections between CGI ability and natural language\nunderstanding ability on QQP and MNLI tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 02:31:05 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 02:10:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Luo", "Ziyang", ""]]}, {"id": "2102.07935", "submitter": "Ryo Masumura", "authors": "Ryo Masumura, Naoki Makishima, Mana Ihori, Akihiko Takashima, Tomohiro\n  Tanaka, Shota Orihashi", "title": "Hierarchical Transformer-based Large-Context End-to-end ASR with\n  Large-Context Knowledge Distillation", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel large-context end-to-end automatic speech recognition\n(E2E-ASR) model and its effective training method based on knowledge\ndistillation. Common E2E-ASR models have mainly focused on utterance-level\nprocessing in which each utterance is independently transcribed. On the other\nhand, large-context E2E-ASR models, which take into account long-range\nsequential contexts beyond utterance boundaries, well handle a sequence of\nutterances such as discourses and conversations. However, the transformer\narchitecture, which has recently achieved state-of-the-art ASR performance\namong utterance-level ASR systems, has not yet been introduced into the\nlarge-context ASR systems. We can expect that the transformer architecture can\nbe leveraged for effectively capturing not only input speech contexts but also\nlong-range sequential contexts beyond utterance boundaries. Therefore, this\npaper proposes a hierarchical transformer-based large-context E2E-ASR model\nthat combines the transformer architecture with hierarchical encoder-decoder\nbased large-context modeling. In addition, in order to enable the proposed\nmodel to use long-range sequential contexts, we also propose a large-context\nknowledge distillation that distills the knowledge from a pre-trained\nlarge-context language model in the training phase. We evaluate the\neffectiveness of the proposed model and proposed training method on Japanese\ndiscourse ASR tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 03:15:15 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Masumura", "Ryo", ""], ["Makishima", "Naoki", ""], ["Ihori", "Mana", ""], ["Takashima", "Akihiko", ""], ["Tanaka", "Tomohiro", ""], ["Orihashi", "Shota", ""]]}, {"id": "2102.07983", "submitter": "Terra Blevins", "authors": "Terra Blevins, Mandar Joshi, and Luke Zettlemoyer", "title": "FEWS: Large-Scale, Low-Shot Word Sense Disambiguation with the\n  Dictionary", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current models for Word Sense Disambiguation (WSD) struggle to disambiguate\nrare senses, despite reaching human performance on global WSD metrics. This\nstems from a lack of data for both modeling and evaluating rare senses in\nexisting WSD datasets. In this paper, we introduce FEWS (Few-shot Examples of\nWord Senses), a new low-shot WSD dataset automatically extracted from example\nsentences in Wiktionary. FEWS has high sense coverage across different natural\nlanguage domains and provides: (1) a large training set that covers many more\nsenses than previous datasets and (2) a comprehensive evaluation set containing\nfew- and zero-shot examples of a wide variety of senses. We establish baselines\non FEWS with knowledge-based and neural WSD approaches and present transfer\nlearning experiments demonstrating that models additionally trained with FEWS\nbetter capture rare senses in existing WSD datasets. Finally, we find humans\noutperform the best baseline models on FEWS, indicating that FEWS will support\nsignificant future work on low-shot WSD.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:13:34 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Blevins", "Terra", ""], ["Joshi", "Mandar", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2102.07988", "submitter": "Zhuohan Li", "authors": "Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn\n  Song, Ion Stoica", "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model parallelism has become a necessity for training modern large-scale deep\nlanguage models. In this work, we identify a new and orthogonal dimension from\nexisting model parallel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for Transformer-based language\nmodels thanks to its autoregressive property. This enables a more fine-grained\npipeline compared with previous work. With this key idea, we design TeraPipe, a\nhigh-performance token-level pipeline parallel algorithm for synchronous\nmodel-parallel training of Transformer-based language models. We develop a\nnovel dynamic programming-based algorithm to calculate the optimal pipelining\nexecution scheme given a specific model and cluster configuration. We show that\nTeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175\nbillion parameters on an AWS cluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 07:34:32 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Li", "Zhuohan", ""], ["Zhuang", "Siyuan", ""], ["Guo", "Shiyuan", ""], ["Zhuo", "Danyang", ""], ["Zhang", "Hao", ""], ["Song", "Dawn", ""], ["Stoica", "Ion", ""]]}, {"id": "2102.08015", "submitter": "Yi Lin", "authors": "Yi Lin, Qin Li, Bo Yang, Zhen Yan, Huachun Tan, and Zhengmao Chen", "title": "Improving speech recognition models with small samples for air traffic\n  control systems", "comments": "This work has been accepted by Neurocomputing for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of air traffic control (ATC) systems, efforts to train a\npractical automatic speech recognition (ASR) model always faces the problem of\nsmall training samples since the collection and annotation of speech samples\nare expert- and domain-dependent task. In this work, a novel training approach\nbased on pretraining and transfer learning is proposed to address this issue,\nand an improved end-to-end deep learning model is developed to address the\nspecific challenges of ASR in the ATC domain. An unsupervised pretraining\nstrategy is first proposed to learn speech representations from unlabeled\nsamples for a certain dataset. Specifically, a masking strategy is applied to\nimprove the diversity of the sample without losing their general patterns.\nSubsequently, transfer learning is applied to fine-tune a pretrained or other\noptimized baseline models to finally achieves the supervised ASR task. By\nvirtue of the common terminology used in the ATC domain, the transfer learning\ntask can be regarded as a sub-domain adaption task, in which the transferred\nmodel is optimized using a joint corpus consisting of baseline samples and new\ntranscribed samples from the target dataset. This joint corpus construction\nstrategy enriches the size and diversity of the training samples, which is\nimportant for addressing the issue of the small transcribed corpus. In\naddition, speed perturbation is applied to augment the new transcribed samples\nto further improve the quality of the speech corpus. Three real ATC datasets\nare used to validate the proposed ASR model and training strategies. The\nexperimental results demonstrate that the ASR performance is significantly\nimproved on all three datasets, with an absolute character error rate only\none-third of that achieved through the supervised training. The applicability\nof the proposed strategies to other ASR approaches is also validated.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:28:52 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Lin", "Yi", ""], ["Li", "Qin", ""], ["Yang", "Bo", ""], ["Yan", "Zhen", ""], ["Tan", "Huachun", ""], ["Chen", "Zhengmao", ""]]}, {"id": "2102.08036", "submitter": "Anil Bas", "authors": "M. Onat Topal, Anil Bas, Imke van Heerden", "title": "Exploring Transformers in Natural Language Generation: GPT, BERT, and\n  XLNet", "comments": "Accepted as oral presentation to ICIDAAI 2021 - Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a proliferation of attention mechanisms and the rise\nof Transformers in Natural Language Generation (NLG). Previously,\nstate-of-the-art NLG architectures such as RNN and LSTM ran into vanishing\ngradient problems; as sentences grew larger, distance between positions\nremained linear, and sequential computation hindered parallelization since\nsentences were processed word by word. Transformers usher in a new era. In this\npaper, we explore three major Transformer-based models, namely GPT, BERT, and\nXLNet, that carry significant implications for the field. NLG is a burgeoning\narea that is now bolstered with rapid developments in attention mechanisms.\nFrom poetry generation to summarization, text generation derives benefit as\nTransformer-based language models achieve groundbreaking results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 09:18:16 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Topal", "M. Onat", ""], ["Bas", "Anil", ""], ["van Heerden", "Imke", ""]]}, {"id": "2102.08098", "submitter": "Chen Zhu", "authors": "Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W. Ronny Huang, Tom\n  Goldstein", "title": "GradInit: Learning to Initialize Neural Networks for Stable and\n  Efficient Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in neural architectures have fostered significant breakthroughs in\nlanguage modeling and computer vision. Unfortunately, novel architectures often\nrequire re-thinking the choice of hyperparameters (e.g., learning rate, warmup\nschedule, and momentum coefficients) to maintain stability of the optimizer.\nThis optimizer instability is often the result of poor parameter\ninitialization, and can be avoided by architecture-specific initialization\nschemes. In this paper, we present GradInit, an automated and architecture\nagnostic method for initializing neural networks. GradInit is based on a simple\nheuristic; the variance of each network layer is adjusted so that a single step\nof SGD or Adam results in the smallest possible loss value. This adjustment is\ndone by introducing a scalar multiplier variable in front of each parameter\nblock, and then optimizing these variables using a simple numerical scheme.\nGradInit accelerates the convergence and test performance of many convolutional\narchitectures, both with or without skip connections, and even without\nnormalization layers. It also enables training the original Post-LN Transformer\nfor machine translation without learning rate warmup under a wide range of\nlearning rates and momentum coefficients. Code is available at\nhttps://github.com/zhuchen03/gradinit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 11:45:35 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhu", "Chen", ""], ["Ni", "Renkun", ""], ["Xu", "Zheng", ""], ["Kong", "Kezhi", ""], ["Huang", "W. Ronny", ""], ["Goldstein", "Tom", ""]]}, {"id": "2102.08138", "submitter": "Callie Hao", "authors": "Nan Wu, Yuan Xie, Cong Hao", "title": "IronMan: GNN-assisted Design Space Exploration in High-Level Synthesis\n  via Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of High-Level Synthesis (HLS) tools, we observe\nseveral unresolved challenges: 1) the high-level abstraction of programming\nstyles in HLS sometimes conceals optimization opportunities; 2) existing HLS\ntools do not provide flexible trade-off (Pareto) solutions among different\nobjectives and constraints; 3) the actual quality of the resulting RTL designs\nis hard to predict. To address these challenges, we propose an end-to-end\nframework, namelyIronMan. The primary goal is to enable a flexible and\nautomated design space exploration (DSE), to provide either optimal solutions\nunder user-specified constraints, or various trade-offs among different\nobjectives (such as different types of resources, area, and latency). Such DSE\neither requires tedious manual efforts or is not achievable to attain these\ngoals through existing HLS tools. There are three components in IronMan: 1)\nGPP, a highly accurate graph-neural-network-based performance and resource\npredictor; 2) RLMD, a reinforcement-learning-based multi-objective DSE engine\nthat explores the optimal resource allocation strategy, to provide Pareto\nsolutions between different objectives; 3) CT, a code transformer to assist\nRLMD and GPP, which extracts the data flow graph from original HLS C/C++ and\nautomatically generates synthesizable code with HLS directives. The\nexperimental results show that: 1) GPP achieves high prediction accuracy,\nreducing prediction errors of HLS tools by 10.9x in resource utilization and\n5.7x in timing; 2) RLMD obtains optimal or Pareto solutions that outperform the\ngenetic algorithm and simulated annealing by 12.7% and 12.9%, respectively; 3)\nIronMan is able to find optimized solutions perfectly matching various DSP\nconstraints, with 2.54x fewer DSPs and up to 6x shorter latency than those of\nHLS tools while being up to 400x faster than the heuristic algorithms and HLS\ntools.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 13:22:00 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wu", "Nan", ""], ["Xie", "Yuan", ""], ["Hao", "Cong", ""]]}, {"id": "2102.08147", "submitter": "Ryo Masumura", "authors": "Ryo Masumura, Naoki Makishima, Mana Ihori, Akihiko Takashima, Tomohiro\n  Tanaka, Shota Orihashi", "title": "Large-Context Conversational Representation Learning: Self-Supervised\n  Learning for Conversational Documents", "comments": "Accepted at IEEE Spoken Language Technology Workshop (SLT), 2021,\n  pp.1012-1019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel self-supervised learning method for handling\nconversational documents consisting of transcribed text of human-to-human\nconversations. One of the key technologies for understanding conversational\ndocuments is utterance-level sequential labeling, where labels are estimated\nfrom the documents in an utterance-by-utterance manner. The main issue with\nutterance-level sequential labeling is the difficulty of collecting labeled\nconversational documents, as manual annotations are very costly. To deal with\nthis issue, we propose large-context conversational representation learning\n(LC-CRL), a self-supervised learning method specialized for conversational\ndocuments. A self-supervised learning task in LC-CRL involves the estimation of\nan utterance using all the surrounding utterances based on large-context\nlanguage modeling. In this way, LC-CRL enables us to effectively utilize\nunlabeled conversational documents and thereby enhances the utterance-level\nsequential labeling. The results of experiments on scene segmentation tasks\nusing contact center conversational datasets demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 13:37:08 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Masumura", "Ryo", ""], ["Makishima", "Naoki", ""], ["Ihori", "Mana", ""], ["Takashima", "Akihiko", ""], ["Tanaka", "Tomohiro", ""], ["Orihashi", "Shota", ""]]}, {"id": "2102.08154", "submitter": "Ryo Masumura", "authors": "Ryo Masumura, Mana Ihori, Akihiko Takashima, Tomohiro Tanaka, Takanori\n  Ashihara", "title": "End-to-End Automatic Speech Recognition with Deep Mutual Learning", "comments": "Accepted at Asia-Pacific Signal and Information Processing\n  Association Annual Summit and Conference (APSIPA ASC), 2020, pp.632-637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first study to apply deep mutual learning (DML) to\nend-to-end ASR models. In DML, multiple models are trained simultaneously and\ncollaboratively by mimicking each other throughout the training process, which\nhelps to attain the global optimum and prevent models from making\nover-confident predictions. While previous studies applied DML to simple\nmulti-class classification problems, there are no studies that have used it on\nmore complex sequence-to-sequence mapping problems. For this reason, this paper\npresents a method to apply DML to state-of-the-art Transformer-based end-to-end\nASR models. In particular, we propose to combine DML with recent representative\ntraining techniques. i.e., label smoothing, scheduled sampling, and\nSpecAugment, each of which are essential for powerful end-to-end ASR models. We\nexpect that these training techniques work well with DML because DML has\ncomplementary characteristics. We experimented with two setups for Japanese ASR\ntasks: large-scale modeling and compact modeling. We demonstrate that DML\nimproves the ASR performance of both modeling setups compared with conventional\nlearning methods including knowledge distillation. We also show that combining\nDML with the existing training techniques effectively improves ASR performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 13:52:06 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Masumura", "Ryo", ""], ["Ihori", "Mana", ""], ["Takashima", "Akihiko", ""], ["Tanaka", "Tomohiro", ""], ["Ashihara", "Takanori", ""]]}, {"id": "2102.08220", "submitter": "Yixuan Su", "authors": "Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li,\n  Nigel Collier", "title": "Non-Autoregressive Text Generation with Pre-trained Language Models", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-autoregressive generation (NAG) has recently attracted great attention\ndue to its fast inference speed. However, the generation quality of existing\nNAG models still lags behind their autoregressive counterparts. In this work,\nwe show that BERT can be employed as the backbone of a NAG model to greatly\nimprove performance. Additionally, we devise mechanisms to alleviate the two\ncommon problems of vanilla NAG models: the inflexibility of prefixed output\nlength and the conditional independence of individual token predictions.\nLastly, to further increase the speed advantage of the proposed model, we\npropose a new decoding strategy, ratio-first, for applications where the output\nlengths can be approximately estimated beforehand. For a comprehensive\nevaluation, we test the proposed model on three text generation tasks,\nincluding text summarization, sentence compression and machine translation.\nExperimental results show that our model significantly outperforms existing\nnon-autoregressive baselines and achieves competitive performance with many\nstrong autoregressive models. In addition, we also conduct extensive analysis\nexperiments to reveal the effect of each proposed component.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:30:33 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Su", "Yixuan", ""], ["Cai", "Deng", ""], ["Wang", "Yan", ""], ["Vandyke", "David", ""], ["Baker", "Simon", ""], ["Li", "Piji", ""], ["Collier", "Nigel", ""]]}, {"id": "2102.08322", "submitter": "Jiahuan Pei", "authors": "Jiahuan Pei, Pengjie Ren, Maarten de Rijke", "title": "A Cooperative Memory Network for Personalized Task-oriented Dialogue\n  Systems with Incomplete User Profiles", "comments": "In Proceedings of the Web Conference 2021 (WWW '21), April 19-23,\n  2021, Ljubljana, Slovenia. ACM, New York, NY, USA, 10 pages", "journal-ref": null, "doi": "10.1145/3442381.3449843", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is increasing interest in developing personalized Task-oriented\nDialogue Systems (TDSs). Previous work on personalized TDSs often assumes that\ncomplete user profiles are available for most or even all users. This is\nunrealistic because (1) not everyone is willing to expose their profiles due to\nprivacy concerns; and (2) rich user profiles may involve a large number of\nattributes (e.g., gender, age, tastes, . . .). In this paper, we study\npersonalized TDSs without assuming that user profiles are complete. We propose\na Cooperative Memory Network (CoMemNN) that has a novel mechanism to gradually\nenrich user profiles as dialogues progress and to simultaneously improve\nresponse selection based on the enriched profiles. CoMemNN consists of two core\nmodules: User Profile Enrichment (UPE) and Dialogue Response Selection (DRS).\nThe former enriches incomplete user profiles by utilizing collaborative\ninformation from neighbor users as well as current dialogues. The latter uses\nthe enriched profiles to update the current user query so as to encode more\nuseful information, based on which a personalized response to a user request is\nselected.\n  We conduct extensive experiments on the personalized bAbI dialogue benchmark\ndatasets. We find that CoMemNN is able to enrich user profiles effectively,\nwhich results in an improvement of 3.06% in terms of response selection\naccuracy compared to state-of-the-art methods. We also test the robustness of\nCoMemNN against incompleteness of user profiles by randomly discarding\nattribute values from user profiles. Even when discarding 50% of the attribute\nvalues, CoMemNN is able to match the performance of the best performing\nbaseline without discarding user profiles, showing the robustness of CoMemNN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:05:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pei", "Jiahuan", ""], ["Ren", "Pengjie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2102.08345", "submitter": "Abhilasha Ravichander", "authors": "Abhilasha Ravichander, Siddharth Dalmia, Maria Ryskina, Florian Metze,\n  Eduard Hovy, Alan W Black", "title": "NoiseQA: Challenge Set Evaluation for User-Centric Question Answering", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When Question-Answering (QA) systems are deployed in the real world, users\nquery them through a variety of interfaces, such as speaking to voice\nassistants, typing questions into a search engine, or even translating\nquestions to languages supported by the QA system. While there has been\nsignificant community attention devoted to identifying correct answers in\npassages assuming a perfectly formed question, we show that components in the\npipeline that precede an answering engine can introduce varied and considerable\nsources of error, and performance can degrade substantially based on these\nupstream noise sources even for powerful pre-trained QA models. We conclude\nthat there is substantial room for progress before QA systems can be\neffectively deployed, highlight the need for QA evaluation to expand to\nconsider real-world use, and hope that our findings will spur greater community\ninterest in the issues that arise when our systems actually need to be of\nutility to humans.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:35:29 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ravichander", "Abhilasha", ""], ["Dalmia", "Siddharth", ""], ["Ryskina", "Maria", ""], ["Metze", "Florian", ""], ["Hovy", "Eduard", ""], ["Black", "Alan W", ""]]}, {"id": "2102.08357", "submitter": "Shengjie Luo", "authors": "Shengjie Luo, Kaiyuan Gao, Shuxin Zheng, Guolin Ke, Di He, Liwei Wang,\n  Tie-Yan Liu", "title": "Revisiting Language Encoding in Learning Multilingual Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer has demonstrated its great power to learn contextual word\nrepresentations for multiple languages in a single model. To process\nmultilingual sentences in the model, a learnable vector is usually assigned to\neach language, which is called \"language embedding\". The language embedding can\nbe either added to the word embedding or attached at the beginning of the\nsentence. It serves as a language-specific signal for the Transformer to\ncapture contextual representations across languages. In this paper, we revisit\nthe use of language embedding and identify several problems in the existing\nformulations. By investigating the interaction between language embedding and\nword embedding in the self-attention module, we find that the current methods\ncannot reflect the language-specific word correlation well. Given these\nfindings, we propose a new approach called Cross-lingual Language Projection\n(XLP) to replace language embedding. For a sentence, XLP projects the word\nembeddings into language-specific semantic space, and then the projected\nembeddings will be fed into the Transformer model to process with their\nlanguage-specific meanings. In such a way, XLP achieves the purpose of\nappropriately encoding \"language\" in a multilingual Transformer model.\nExperimental results show that XLP can freely and significantly boost the model\nperformance on extensive multilingual benchmark datasets. Codes and models will\nbe released at https://github.com/lsj2408/XLP.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:47:10 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Luo", "Shengjie", ""], ["Gao", "Kaiyuan", ""], ["Zheng", "Shuxin", ""], ["Ke", "Guolin", ""], ["He", "Di", ""], ["Wang", "Liwei", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2102.08366", "submitter": "Gabriele Pergola", "authors": "Gabriele Pergola, Elena Kochkina, Lin Gui, Maria Liakata, Yulan He", "title": "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies", "comments": "EACL 2021 - Short Paper - European Chapter of the Association for\n  Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical question-answering (QA) has gained increased attention for its\ncapability to provide users with high-quality information from a vast\nscientific literature. Although an increasing number of biomedical QA datasets\nhas been recently made available, those resources are still rather limited and\nexpensive to produce. Transfer learning via pre-trained language models (LMs)\nhas been shown as a promising approach to leverage existing general-purpose\nknowledge. However, finetuning these large models can be costly and time\nconsuming, often yielding limited benefits when adapting to specific themes of\nspecialised domains, such as the COVID-19 literature. To bootstrap further\ntheir domain adaptation, we propose a simple yet unexplored approach, which we\ncall biomedical entity-aware masking (BEM). We encourage masked language models\nto learn entity-centric knowledge based on the pivotal entities characterizing\nthe domain at hand, and employ those entities to drive the LM fine-tuning. The\nresulting strategy is a downstream process applicable to a wide variety of\nmasked LMs, not requiring additional memory or components in the neural\narchitectures. Experimental results show performance on par with\nstate-of-the-art models on several biomedical QA datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:51:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pergola", "Gabriele", ""], ["Kochkina", "Elena", ""], ["Gui", "Lin", ""], ["Liakata", "Maria", ""], ["He", "Yulan", ""]]}, {"id": "2102.08368", "submitter": "David Jurgens", "authors": "Jiajun Bao, Junjie Wu, Yiming Zhang, Eshwar Chandrasekharan, and David\n  Jurgens", "title": "Conversations Gone Alright: Quantifying and Predicting Prosocial\n  Outcomes in Online Conversations", "comments": "Accepted for Publication at the Web Conference 2021; 12 pages", "journal-ref": null, "doi": "10.1145/3442381.3450122", "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online conversations can go in many directions: some turn out poorly due to\nantisocial behavior, while others turn out positively to the benefit of all.\nResearch on improving online spaces has focused primarily on detecting and\nreducing antisocial behavior. Yet we know little about positive outcomes in\nonline conversations and how to increase them-is a prosocial outcome simply the\nlack of antisocial behavior or something more? Here, we examine how\nconversational features lead to prosocial outcomes within online discussions.\nWe introduce a series of new theory-inspired metrics to define prosocial\noutcomes such as mentoring and esteem enhancement. Using a corpus of 26M Reddit\nconversations, we show that these outcomes can be forecasted from the initial\ncomment of an online conversation, with the best model providing a relative 24%\nimprovement over human forecasting performance at ranking conversations for\npredicted outcome. Our results indicate that platforms can use these early cues\nin their algorithmic ranking of early conversations to prioritize better\noutcomes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:53:41 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Bao", "Jiajun", ""], ["Wu", "Junjie", ""], ["Zhang", "Yiming", ""], ["Chandrasekharan", "Eshwar", ""], ["Jurgens", "David", ""]]}, {"id": "2102.08424", "submitter": "Clara Meister", "authors": "Martina Forster, Clara Meister, Ryan Cotterell", "title": "Searching for Search Errors in Neural Morphological Inflection", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence models are currently the predominant choice for\nlanguage generation tasks. Yet, on word-level tasks, exact inference of these\nmodels reveals the empty string is often the global optimum. Prior works have\nspeculated this phenomenon is a result of the inadequacy of neural models for\nlanguage generation. However, in the case of morphological inflection, we find\nthat the empty string is almost never the most probable solution under the\nmodel. Further, greedy search often finds the global optimum. These\nobservations suggest that the poor calibration of many neural models may stem\nfrom characteristics of a specific subset of tasks rather than general\nill-suitedness of such models for language generation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 19:42:42 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Forster", "Martina", ""], ["Meister", "Clara", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2102.08473", "submitter": "Yu Meng", "authors": "Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett,\n  Jiawei Han, Xia Song", "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model\n  Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COCO-LM, a new self-supervised learning framework that pretrains\nLanguage Models by COrrecting challenging errors and COntrasting text\nsequences. COCO-LM employs an auxiliary language model to mask-and-predict\ntokens in original text sequences. It creates more challenging pretraining\ninputs, where noises are sampled based on their likelihood in the auxiliary\nlanguage model. COCO-LM then pretrains with two tasks: The first task,\ncorrective language modeling, learns to correct the auxiliary model's\ncorruptions by recovering the original tokens. The second task, sequence\ncontrastive learning, ensures that the language model generates sequence\nrepresentations that are invariant to noises and transformations. In our\nexperiments on the GLUE and SQuAD benchmarks, COCO-LM outperforms recent\npretraining approaches in various pretraining settings and few-shot\nevaluations, with higher pretraining efficiency. Our analyses reveal that\nCOCO-LM's advantages come from its challenging training signals, more\ncontextualized token representations, and regularized sequence representations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 22:24:29 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Meng", "Yu", ""], ["Xiong", "Chenyan", ""], ["Bajaj", "Payal", ""], ["Tiwary", "Saurabh", ""], ["Bennett", "Paul", ""], ["Han", "Jiawei", ""], ["Song", "Xia", ""]]}, {"id": "2102.08513", "submitter": "Ozlem Uzuner", "authors": "Kahyun Lee, Mehmet Kayaalp, Sam Henry, \\\"Ozlem Uzuner", "title": "A Context-Enhanced De-identification System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern entity recognition systems, including the current\nstate-of-the-art de-identification systems, are based on bidirectional long\nshort-term memory (biLSTM) units augmented by a conditional random field (CRF)\nsequence optimizer. These systems process the input sentence by sentence. This\napproach prevents the systems from capturing dependencies over sentence\nboundaries and makes accurate sentence boundary detection a prerequisite. Since\nsentence boundary detection can be problematic especially in clinical reports,\nwhere dependencies and co-references across sentence boundaries are abundant,\nthese systems have clear limitations. In this study, we built a new system on\nthe framework of one of the current state-of-the-art de-identification systems,\nNeuroNER, to overcome these limitations. This new system incorporates context\nembeddings through forward and backward n-grams without using sentence\nboundaries. Our context-enhanced de-identification (CEDI) system captures\ndependencies over sentence boundaries and bypasses the sentence boundary\ndetection problem altogether. We enhanced this system with deep affix features\nand an attention mechanism to capture the pertinent parts of the input. The\nCEDI system outperforms NeuroNER on the 2006 i2b2 de-identification challenge\ndataset, the 2014 i2b2 shared task de-identification dataset, and the 2016 CEGS\nN-GRID de-identification dataset (p<0.01). All datasets comprise narrative\nclinical reports in English but contain different note types varying from\ndischarge summaries to psychiatric notes. Enhancing CEDI with deep affix\nfeatures and the attention mechanism further increased performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:43:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lee", "Kahyun", ""], ["Kayaalp", "Mehmet", ""], ["Henry", "Sam", ""], ["Uzuner", "\u00d6zlem", ""]]}, {"id": "2102.08517", "submitter": "Ozlem Uzuner", "authors": "Kahyun Lee, Nicholas J. Dobbins, Bridget McInnes, Meliha Yetisgen,\n  \\\"Ozlem Uzuner", "title": "Transferability of Neural Network-based De-identification Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods and Materials: We investigated transferability of neural\nnetwork-based de-identification sys-tems with and without domain\ngeneralization. We used two domain generalization approaches: a novel approach\nJoint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art\ndomain general-ization approach Common-Specific Decomposition (CSD) from the\nliterature. First, we measured trans-ferability from a single external source.\nSecond, we used two external sources and evaluated whether domain\ngeneralization can improve transferability of de-identification models across\ndomains which rep-resent different note types from the same institution. Third,\nusing two external sources with in-domain training data, we studied whether\nexternal source data are useful even in cases where sufficient in-domain\ntraining data are available. Finally, we investigated transferability of the\nde-identification mod-els across institutions. Results and Conclusions: We\nfound transferability from a single external source gave inconsistent re-sults.\nUsing additional external sources consistently yielded an F1-score of\napproximately 80%, but domain generalization was not always helpful to improve\ntransferability. We also found that external sources were useful even in cases\nwhere in-domain training data were available by reducing the amount of needed\nin-domain training data or by improving performance. Transferability across\ninstitutions was differed by note type and annotation label. External sources\nfrom a different institution were also useful to further improve performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:49:34 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lee", "Kahyun", ""], ["Dobbins", "Nicholas J.", ""], ["McInnes", "Bridget", ""], ["Yetisgen", "Meliha", ""], ["Uzuner", "\u00d6zlem", ""]]}, {"id": "2102.08535", "submitter": "Yi Lin", "authors": "Yi Lin, Bo Yang, Linchao Li, Dongyue Guo, Jianwei Zhang, Hu Chen, Yi\n  Zhang", "title": "ATCSpeechNet: A multilingual end-to-end speech recognition framework for\n  air traffic control systems", "comments": "An improved work based on our previous Interspeech 2020 paper\n  (https://isca-speech.org/archive/Interspeech_2020/pdfs/1020.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multilingual end-to-end framework, called as ATCSpeechNet,\nis proposed to tackle the issue of translating communication speech into\nhuman-readable text in air traffic control (ATC) systems. In the proposed\nframework, we focus on integrating the multilingual automatic speech\nrecognition (ASR) into one model, in which an end-to-end paradigm is developed\nto convert speech waveform into text directly, without any feature engineering\nor lexicon. In order to make up for the deficiency of the handcrafted feature\nengineering caused by ATC challenges, a speech representation learning (SRL)\nnetwork is proposed to capture robust and discriminative speech representations\nfrom the raw wave. The self-supervised training strategy is adopted to optimize\nthe SRL network from unlabeled data, and further to predict the speech\nfeatures, i.e., wave-to-feature. An end-to-end architecture is improved to\ncomplete the ASR task, in which a grapheme-based modeling unit is applied to\naddress the multilingual ASR issue. Facing the problem of small transcribed\nsamples in the ATC domain, an unsupervised approach with mask prediction is\napplied to pre-train the backbone network of the ASR model on unlabeled data by\na feature-to-feature process. Finally, by integrating the SRL with ASR, an\nend-to-end multilingual ASR framework is formulated in a supervised manner,\nwhich is able to translate the raw wave into text in one model, i.e.,\nwave-to-text. Experimental results on the ATCSpeech corpus demonstrate that the\nproposed approach achieves a high performance with a very small labeled corpus\nand less resource consumption, only 4.20% label error rate on the 58-hour\ntranscribed corpus. Compared to the baseline model, the proposed approach\nobtains over 100% relative performance improvement which can be further\nenhanced with the increasing of the size of the transcribed samples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:27:09 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lin", "Yi", ""], ["Yang", "Bo", ""], ["Li", "Linchao", ""], ["Guo", "Dongyue", ""], ["Zhang", "Jianwei", ""], ["Chen", "Hu", ""], ["Zhang", "Yi", ""]]}, {"id": "2102.08549", "submitter": "Lianzhe Huang", "authors": "Lianzhe Huang, Peiyi Wang, Sujian Li, Tianyu Liu, Xiaodong Zhang,\n  Zhicong Cheng, Dawei Yin, Houfeng Wang", "title": "First Target and Opinion then Polarity: Enhancing Target-opinion\n  Correlation for Aspect Sentiment Triplet Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Sentiment Triplet Extraction (ASTE) aims to extract triplets from a\nsentence, including target entities, associated sentiment polarities, and\nopinion spans which rationalize the polarities. Existing methods are short on\nbuilding correlation between target-opinion pairs, and neglect the mutual\ninterference among different sentiment triplets. To address these issues, we\npropose a novel two-stage method which enhances the correlation between targets\nand opinions: at stage one, we extract targets and opinions through sequence\ntagging; then we insert a group of artificial tags named Perceivable Pair,\nwhich indicate the span of the target and the opinion, into the sequence to\nestablish correlation for each candidate target-opinion pair. Meanwhile, we\nreduce the mutual interference between triplets by restricting tokens'\nattention field. Finally, the polarity is identified according to the\nrepresentation of the Perceivable Pair. We conduct experiments on four\ndatasets, and the experimental results show that our model outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 03:28:17 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 12:26:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Huang", "Lianzhe", ""], ["Wang", "Peiyi", ""], ["Li", "Sujian", ""], ["Liu", "Tianyu", ""], ["Zhang", "Xiaodong", ""], ["Cheng", "Zhicong", ""], ["Yin", "Dawei", ""], ["Wang", "Houfeng", ""]]}, {"id": "2102.08553", "submitter": "Jun Quan", "authors": "Jun Quan, Meng Yang, Qiang Gan, Deyi Xiong, Yiming Liu, Yuchen Dong,\n  Fangxin Ouyang, Jun Tian, Ruiling Deng, Yongzhi Li, Yang Yang and Daxin Jiang", "title": "Integrating Pre-trained Model into Rule-based Dialogue Management", "comments": "AAAI 2021 Demo Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based dialogue management is still the most popular solution for\nindustrial task-oriented dialogue systems for their interpretablility. However,\nit is hard for developers to maintain the dialogue logic when the scenarios get\nmore and more complex. On the other hand, data-driven dialogue systems, usually\nwith end-to-end structures, are popular in academic research and easier to deal\nwith complex conversations, but such methods require plenty of training data\nand the behaviors are less interpretable. In this paper, we propose a method to\nleverages the strength of both rule-based and data-driven dialogue managers\n(DM). We firstly introduce the DM of Carina Dialog System (CDS, an advanced\nindustrial dialogue system built by Microsoft). Then we propose the\n\"model-trigger\" design to make the DM trainable thus scalable to scenario\nchanges. Furthermore, we integrate pre-trained models and empower the DM with\nfew-shot capability. The experimental results demonstrate the effectiveness and\nstrong few-shot capability of our method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 03:44:22 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Quan", "Jun", ""], ["Yang", "Meng", ""], ["Gan", "Qiang", ""], ["Xiong", "Deyi", ""], ["Liu", "Yiming", ""], ["Dong", "Yuchen", ""], ["Ouyang", "Fangxin", ""], ["Tian", "Jun", ""], ["Deng", "Ruiling", ""], ["Li", "Yongzhi", ""], ["Yang", "Yang", ""], ["Jiang", "Daxin", ""]]}, {"id": "2102.08565", "submitter": "Dongjae Kim", "authors": "Dongjae Kim, Jong-Kook Kim", "title": "Contextual Skipgram: Training Word Representation Using Context\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skip-gram (SG) model learns word representation by predicting the words\nsurrounding a center word from unstructured text data. However, not all words\nin the context window contribute to the meaning of the center word. For\nexample, less relevant words could be in the context window, hindering the SG\nmodel from learning a better quality representation. In this paper, we propose\nan enhanced version of the SG that leverages context information to produce\nword representation. The proposed model, Contextual Skip-gram, is designed to\npredict contextual words with both the center words and the context\ninformation. This simple idea helps to reduce the impact of irrelevant words on\nthe training process, thus enhancing the final performance\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 04:19:56 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Kim", "Dongjae", ""], ["Kim", "Jong-Kook", ""]]}, {"id": "2102.08585", "submitter": "Tianyu Liu", "authors": "Tianyu Liu, Xin Zheng, Baobao Chang and Zhifang Sui", "title": "Towards Faithfulness in Open Domain Table-to-text Generation from an\n  Entity-centric View", "comments": "AAAI-21 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open domain table-to-text generation, we notice that the unfaithful\ngeneration usually contains hallucinated content which can not be aligned to\nany input table record. We thus try to evaluate the generation faithfulness\nwith two entity-centric metrics: table record coverage and the ratio of\nhallucinated entities in text, both of which are shown to have strong agreement\nwith human judgements. Then based on these metrics, we quantitatively analyze\nthe correlation between training data quality and generation fidelity which\nindicates the potential usage of entity information in faithful generation.\nMotivated by these findings, we propose two methods for faithful generation: 1)\naugmented training by incorporating the auxiliary entity information, including\nboth an augmented plan-based model and an unsupervised model and 2) training\ninstance selection based on faithfulness ranking. We show these approaches\nimprove generation fidelity in both full dataset setting and few shot learning\nsettings by both automatic and human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 05:41:06 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Liu", "Tianyu", ""], ["Zheng", "Xin", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "2102.08597", "submitter": "Aston Zhang", "authors": "Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung\n  Hui, Jie Fu", "title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of\n  Hypercomplex Multiplications with $1/n$ Parameters", "comments": "Published as a conference paper at the 9th International Conference\n  on Learning Representations (ICLR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated reasonable success of representation learning\nin hypercomplex space. Specifically, \"fully-connected layers with Quaternions\"\n(4D hypercomplex numbers), which replace real-valued matrix multiplications in\nfully-connected layers with Hamilton products of Quaternions, both enjoy\nparameter savings with only 1/4 learnable parameters and achieve comparable\nperformance in various applications. However, one key caveat is that\nhypercomplex space only exists at very few predefined dimensions (4D, 8D, and\n16D). This restricts the flexibility of models that leverage hypercomplex\nmultiplications. To this end, we propose parameterizing hypercomplex\nmultiplications, allowing models to learn multiplication rules from data\nregardless of whether such rules are predefined. As a result, our method not\nonly subsumes the Hamilton product, but also learns to operate on any arbitrary\nnD hypercomplex space, providing more architectural flexibility using\narbitrarily $1/n$ learnable parameters compared with the fully-connected layer\ncounterpart. Experiments of applications to the LSTM and Transformer models on\nnatural language inference, machine translation, text style transfer, and\nsubject verb agreement demonstrate architectural flexibility and effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 06:16:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhang", "Aston", ""], ["Tay", "Yi", ""], ["Zhang", "Shuai", ""], ["Chan", "Alvin", ""], ["Luu", "Anh Tuan", ""], ["Hui", "Siu Cheung", ""], ["Fu", "Jie", ""]]}, {"id": "2102.08633", "submitter": "Yifan Gao", "authors": "Yifan Gao, Jingjing Li, Michael R. Lyu, Irwin King", "title": "Open-Retrieval Conversational Machine Reading", "comments": "Technical Report, Dataset:\n  https://github.com/Yifan-Gao/open_retrieval_conversational_machine_reading", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In conversational machine reading, systems need to interpret natural language\nrules, answer high-level questions such as \"May I qualify for VA health care\nbenefits?\", and ask follow-up clarification questions whose answer is necessary\nto answer the original question. However, existing works assume the rule text\nis provided for each user question, which neglects the essential retrieval step\nin real scenarios. In this work, we propose and investigate an open-retrieval\nsetting of conversational machine reading. In the open-retrieval setting, the\nrelevant rule texts are unknown so that a system needs to retrieve\nquestion-relevant evidence from a collection of rule texts, and answer users'\nhigh-level questions according to multiple retrieved rule texts in a\nconversational manner. We propose MUDERN, a Multi-passage Discourse-aware\nEntailment Reasoning Network which extracts conditions in the rule texts\nthrough discourse segmentation, conducts multi-passage entailment reasoning to\nanswer user questions directly, or asks clarification follow-up questions to\ninquiry more information. On our created OR-ShARC dataset, MUDERN achieves the\nstate-of-the-art performance, outperforming existing single-passage\nconversational machine reading models as well as a new multi-passage\nconversational machine reading baseline by a large margin. In addition, we\nconduct in-depth analyses to provide new insights into this new setting and our\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 08:55:01 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Gao", "Yifan", ""], ["Li", "Jingjing", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2102.08655", "submitter": "Nora Hollenstein", "authors": "Nora Hollenstein, Cedric Renggli, Benjamin Glaus, Maria Barrett,\n  Marius Troendle, Nicolas Langer, Ce Zhang", "title": "Decoding EEG Brain Activity for Multi-Modal Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Until recently, human behavioral data from reading has mainly been of\ninterest to researchers to understand human cognition. However, these human\nlanguage processing signals can also be beneficial in machine learning-based\nnatural language processing tasks. Using EEG brain activity to this purpose is\nlargely unexplored as of yet. In this paper, we present the first large-scale\nstudy of systematically analyzing the potential of EEG brain activity data for\nimproving natural language processing tasks, with a special focus on which\nfeatures of the signal are most beneficial. We present a multi-modal machine\nlearning architecture that learns jointly from textual input as well as from\nEEG features. We find that filtering the EEG signals into frequency bands is\nmore beneficial than using the broadband signal. Moreover, for a range of word\nembedding types, EEG data improves binary and ternary sentiment classification\nand outperforms multiple baselines. For more complex tasks such as relation\ndetection, further research is needed. Finally, EEG data shows to be\nparticularly promising when limited training data is available.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 09:44:21 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:34:28 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hollenstein", "Nora", ""], ["Renggli", "Cedric", ""], ["Glaus", "Benjamin", ""], ["Barrett", "Maria", ""], ["Troendle", "Marius", ""], ["Langer", "Nicolas", ""], ["Zhang", "Ce", ""]]}, {"id": "2102.08747", "submitter": "Sebastian Monka", "authors": "Sebastian Monka, Lavdim Halilaj, Stefan Schmid, Achim Rettinger", "title": "Learning Visual Models using a Knowledge Graph as a Trainer", "comments": "ISWC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional computer vision approaches, based on neural networks (NN), are\ntypically trained on a large amount of image data. By minimizing the\ncross-entropy loss between a prediction and a given class label, the NN and its\nvisual embedding space are learned to fulfill a given task. However, due to the\nsole dependence on the image data distribution of the training domain, these\nmodels tend to fail when applied to a target domain that differs from their\nsource domain. To learn a more robust NN to domain shifts, we propose the\nknowledge graph neural network (KG-NN), a neuro-symbolic approach that\nsupervises the training using image-data-invariant auxiliary knowledge. The\nauxiliary knowledge is first encoded in a knowledge graph with respective\nconcepts and their relationships, which is then transformed into a dense vector\nrepresentation via an embedding method. Using a contrastive loss function,\nKG-NN learns to adapt its visual embedding space and thus its weights according\nto the image-data invariant knowledge graph embedding space. We evaluate KG-NN\non visual transfer learning tasks for classification using the mini-ImageNet\ndataset and its derivatives, as well as road sign recognition datasets from\nGermany and China. The results show that a visual model trained with a\nknowledge graph as a trainer outperforms a model trained with cross-entropy in\nall experiments, in particular when the domain gap increases. Besides better\nperformance and stronger robustness to domain shifts, these KG-NN adapts to\nmultiple datasets and classes without suffering heavily from catastrophic\nforgetting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 13:24:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 13:21:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Monka", "Sebastian", ""], ["Halilaj", "Lavdim", ""], ["Schmid", "Stefan", ""], ["Rettinger", "Achim", ""]]}, {"id": "2102.08773", "submitter": "Matthew Shardlow", "authors": "Matthew Shardlow, Richard Evans and Marcos Zampieri", "title": "Predicting Lexical Complexity in English Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first step in most text simplification is to predict which words are\nconsidered complex for a given target population before carrying out lexical\nsubstitution. This task is commonly referred to as Complex Word Identification\n(CWI) and it is often modelled as a supervised classification problem. For\ntraining such systems, annotated datasets in which words and sometimes\nmulti-word expressions are labelled regarding complexity are required. In this\npaper we analyze previous work carried out in this task and investigate the\nproperties of complex word identification datasets for English.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:05:30 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Shardlow", "Matthew", ""], ["Evans", "Richard", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2102.08795", "submitter": "Nikos Voskarides", "authors": "Svitlana Vakulenko, Nikos Voskarides, Zhucheng Tu, Shayne Longpre", "title": "Leveraging Query Resolution and Reading Comprehension for Conversational\n  Passage Retrieval", "comments": "TREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the participation of UvA.ILPS group at the TREC CAsT\n2020 track. Our passage retrieval pipeline consists of (i) an initial retrieval\nmodule that uses BM25, and (ii) a re-ranking module that combines the score of\na BERT ranking model with the score of a machine comprehension model adjusted\nfor passage retrieval. An important challenge in conversational passage\nretrieval is that queries are often under-specified. Thus, we perform query\nresolution, that is, add missing context from the conversation history to the\ncurrent turn query using QuReTeC, a term classification query resolution model.\nWe show that our best automatic and manual runs outperform the corresponding\nmedian runs by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:41:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Voskarides", "Nikos", ""], ["Tu", "Zhucheng", ""], ["Longpre", "Shayne", ""]]}, {"id": "2102.08818", "submitter": "Priyanshu Kumar", "authors": "Aadarsh Singh and Priyanshu Kumar", "title": "SciDr at SDU-2020: IDEAS -- Identifying and Disambiguating Everyday\n  Acronyms for Scientific Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our systems submitted for the shared tasks of Acronym\nIdentification (AI) and Acronym Disambiguation (AD) held under Workshop on SDU.\nWe mainly experiment with BERT and SciBERT. In addition, we assess the\neffectiveness of \"BIOless\" tagging and blending along with the prowess of\nensembling in AI. For AD, we formulate the problem as a span prediction task,\nexperiment with different training techniques and also leverage the use of\nexternal data. Our systems rank 11th and 3rd in AI and AD tasks respectively.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 15:24:50 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 13:34:34 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Singh", "Aadarsh", ""], ["Kumar", "Priyanshu", ""]]}, {"id": "2102.08858", "submitter": "Thomas Haider", "authors": "Thomas Haider", "title": "Metrical Tagging in the Wild: Building and Annotating Poetry Corpora\n  with Rhythmic Features", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prerequisite for the computational study of literature is the availability\nof properly digitized texts, ideally with reliable meta-data and ground-truth\nannotation. Poetry corpora do exist for a number of languages, but larger\ncollections lack consistency and are encoded in various standards, while\nannotated corpora are typically constrained to a particular genre and/or were\ndesigned for the analysis of certain linguistic features (like rhyme). In this\nwork, we provide large poetry corpora for English and German, and annotate\nprosodic features in smaller corpora to train corpus driven neural models that\nenable robust large scale analysis.\n  We show that BiLSTM-CRF models with syllable embeddings outperform a CRF\nbaseline and different BERT-based approaches. In a multi-task setup, particular\nbeneficial task relations illustrate the inter-dependence of poetic features. A\nmodel learns foot boundaries better when jointly predicting syllable stress,\naesthetic emotions and verse measures benefit from each other, and we find that\ncaesuras are quite dependent on syntax and also integral to shaping the overall\nmeasure of the line.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:38:57 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 09:35:47 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Haider", "Thomas", ""]]}, {"id": "2102.08886", "submitter": "Arkaitz Zubiaga", "authors": "Wenjie Yin, Arkaitz Zubiaga", "title": "Towards generalisable hate speech detection: a review on obstacles and\n  solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech is one type of harmful online content which directly attacks or\npromotes hate towards a group or an individual member based on their actual or\nperceived aspects of identity, such as ethnicity, religion, and sexual\norientation. With online hate speech on the rise, its automatic detection as a\nnatural language processing task is gaining increasing interest. However, it is\nonly recently that it has been shown that existing models generalise poorly to\nunseen data. This survey paper attempts to summarise how generalisable existing\nhate speech detection models are, reason why hate speech models struggle to\ngeneralise, sums up existing attempts at addressing the main obstacles, and\nthen proposes directions of future research to improve generalisation in hate\nspeech detection.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:27:48 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Yin", "Wenjie", ""], ["Zubiaga", "Arkaitz", ""]]}, {"id": "2102.08892", "submitter": "Rudolf Rosa", "authors": "Rudolf Rosa and Tom\\'a\\v{s} Musil and Ond\\v{r}ej Du\\v{s}ek and Dominik\n  Jurko and Patr\\'icia Schmidtov\\'a and David Mare\\v{c}ek and Ond\\v{r}ej Bojar\n  and Tom Kocmi and Daniel Hrbek and David Ko\\v{s}\\v{t}\\'ak and Martina\n  Kinsk\\'a and Marie Nov\\'akov\\'a and Josef Dole\\v{z}al and Kl\\'ara Voseck\\'a\n  and Tom\\'a\\v{s} Studen\\'ik and Petr \\v{Z}abka", "title": "THEaiTRE 1.0: Interactive generation of theatre play scripts", "comments": "Submitted to Text2Story workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first version of a system for interactive generation of\ntheatre play scripts. The system is based on a vanilla GPT-2 model with several\nadjustments, targeting specific issues we encountered in practice. We also list\nother issues we encountered but plan to only solve in a future version of the\nsystem. The presented system was used to generate a theatre play script planned\nfor premiere in February 2021.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:40:33 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Rosa", "Rudolf", ""], ["Musil", "Tom\u00e1\u0161", ""], ["Du\u0161ek", "Ond\u0159ej", ""], ["Jurko", "Dominik", ""], ["Schmidtov\u00e1", "Patr\u00edcia", ""], ["Mare\u010dek", "David", ""], ["Bojar", "Ond\u0159ej", ""], ["Kocmi", "Tom", ""], ["Hrbek", "Daniel", ""], ["Ko\u0161\u0165\u00e1k", "David", ""], ["Kinsk\u00e1", "Martina", ""], ["Nov\u00e1kov\u00e1", "Marie", ""], ["Dole\u017eal", "Josef", ""], ["Voseck\u00e1", "Kl\u00e1ra", ""], ["Studen\u00edk", "Tom\u00e1\u0161", ""], ["\u017dabka", "Petr", ""]]}, {"id": "2102.08898", "submitter": "Adam Fisch", "authors": "Adam Fisch, Tal Schuster, Tommi Jaakkola, Regina Barzilay", "title": "Few-shot Conformal Prediction with Auxiliary Tasks", "comments": "ICML camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approach to conformal prediction when the target task has\nlimited data available for training. Conformal prediction identifies a small\nset of promising output candidates in place of a single prediction, with\nguarantees that the set contains the correct answer with high probability. When\ntraining data is limited, however, the predicted set can easily become unusably\nlarge. In this work, we obtain substantially tighter prediction sets while\nmaintaining desirable marginal guarantees by casting conformal prediction as a\nmeta-learning paradigm over exchangeable collections of auxiliary tasks. Our\nconformalization algorithm is simple, fast, and agnostic to the choice of\nunderlying model, learning algorithm, or dataset. We demonstrate the\neffectiveness of this approach across a number of few-shot classification and\nregression tasks in natural language processing, computer vision, and\ncomputational chemistry for drug discovery.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 17:46:57 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 04:07:40 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Fisch", "Adam", ""], ["Schuster", "Tal", ""], ["Jaakkola", "Tommi", ""], ["Barzilay", "Regina", ""]]}, {"id": "2102.08924", "submitter": "Rachit Bansal", "authors": "William Scott Paka, Rachit Bansal, Abhay Kaushik, Shubhashis Sengupta,\n  Tanmoy Chakraborty", "title": "Cross-SEAN: A Cross-Stitch Semi-Supervised Neural Attention Model for\n  COVID-19 Fake News Detection", "comments": "The Journal of Applied Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the COVID-19 pandemic sweeps across the world, it has been accompanied by\na tsunami of fake news and misinformation on social media. At the time when\nreliable information is vital for public health and safety, COVID-19 related\nfake news has been spreading even faster than the facts. During times such as\nthe COVID-19 pandemic, fake news can not only cause intellectual confusion but\ncan also place lives of people at risk. This calls for an immediate need to\ncontain the spread of such misinformation on social media. We introduce CTF,\nthe first COVID-19 Twitter fake news dataset with labeled genuine and fake\ntweets. Additionally, we propose Cross-SEAN, a cross-stitch based\nsemi-supervised end-to-end neural attention model, which leverages the large\namount of unlabelled data. Cross-SEAN partially generalises to emerging fake\nnews as it learns from relevant external knowledge. We compare Cross-SEAN with\nseven state-of-the-art fake news detection methods. We observe that it achieves\n$0.95$ F1 Score on CTF, outperforming the best baseline by $9\\%$. We also\ndevelop Chrome-SEAN, a Cross-SEAN based chrome extension for real-time\ndetection of fake tweets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:30:43 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 05:49:17 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 08:38:02 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Paka", "William Scott", ""], ["Bansal", "Rachit", ""], ["Kaushik", "Abhay", ""], ["Sengupta", "Shubhashis", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2102.08934", "submitter": "No\\'e Casas", "authors": "Noe Casas, Jose A. R. Fonollosa, Marta R. Costa-juss\\`a", "title": "Sparsely Factored Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to incorporate linguistic information to neural machine\ntranslation systems consists in maintaining separate vocabularies for each of\nthe annotated features to be incorporated (e.g. POS tags, dependency relation\nlabel), embed them, and then aggregate them with each subword in the word they\nbelong to. This approach, however, cannot easily accommodate annotation schemes\nthat are not dense for every word.\n  We propose a method suited for such a case, showing large improvements in\nout-of-domain data, and comparable quality for the in-domain data. Experiments\nare performed in morphologically-rich languages like Basque and German, for the\ncase of low-resource scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 18:42:00 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Casas", "Noe", ""], ["Fonollosa", "Jose A. R.", ""], ["Costa-juss\u00e0", "Marta R.", ""]]}, {"id": "2102.08981", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize\n  Long-Tail Visual Concepts", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR\n  2021). Our dataset is available at\n  https://github.com/google-research-datasets/conceptual-12m", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of large-scale image captioning and visual question\nanswering datasets has contributed significantly to recent successes in\nvision-and-language pre-training. However, these datasets are often collected\nwith overrestrictive requirements inherited from their original target tasks\n(e.g., image caption generation), which limit the resulting dataset scale and\ndiversity. We take a step further in pushing the limits of vision-and-language\npre-training data by relaxing the data collection pipeline used in Conceptual\nCaptions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M\n(CC12M), a dataset with 12 million image-text pairs specifically meant to be\nused for vision-and-language pre-training. We perform an analysis of this\ndataset and benchmark its effectiveness against CC3M on multiple downstream\ntasks with an emphasis on long-tail visual recognition. Our results clearly\nillustrate the benefit of scaling up pre-training data for vision-and-language\ntasks, as indicated by the new state-of-the-art results on both the nocaps and\nConceptual Captions benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 19:15:53 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 08:20:34 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Sharma", "Piyush", ""], ["Ding", "Nan", ""], ["Soricut", "Radu", ""]]}, {"id": "2102.09094", "submitter": "Adam D. Lelkes", "authors": "Adam D. Lelkes, Vinh Q. Tran, Cong Yu", "title": "Quiz-Style Question Generation for News Stories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A large majority of American adults get at least some of their news from the\nInternet. Even though many online news products have the goal of informing\ntheir users about the news, they lack scalable and reliable tools for measuring\nhow well they are achieving this goal, and therefore have to resort to noisy\nproxy metrics (e.g., click-through rates or reading time) to track their\nperformance.\n  As a first step towards measuring news informedness at a scale, we study the\nproblem of quiz-style multiple-choice question generation, which may be used to\nsurvey users about their knowledge of recent news. In particular, we formulate\nthe problem as two sequence-to-sequence tasks: question-answer generation (QAG)\nand distractor, or incorrect answer, generation (DG). We introduce NewsQuizQA,\nthe first dataset intended for quiz-style question-answer generation,\ncontaining 20K human written question-answer pairs from 5K news article\nsummaries. Using this dataset, we propose a series of novel techniques for\napplying large pre-trained Transformer encoder-decoder models, namely PEGASUS\nand T5, to the tasks of question-answer generation and distractor generation.\n  We show that our models outperform strong baselines using both automated\nmetrics and human raters. We provide a case study of running weekly quizzes on\nreal-world users via the Google Surveys platform over the course of two months.\nWe found that users generally found the automatically generated questions to be\neducational and enjoyable. Finally, to serve the research community, we are\nreleasing the NewsQuizQA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:06:58 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Lelkes", "Adam D.", ""], ["Tran", "Vinh Q.", ""], ["Yu", "Cong", ""]]}, {"id": "2102.09114", "submitter": "Ankush Garg", "authors": "Harsh Shrivastava, Ankush Garg, Yuan Cao, Yu Zhang, Tara Sainath", "title": "Echo State Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose automatic speech recognition (ASR) models inspired by echo state\nnetwork (ESN), in which a subset of recurrent neural networks (RNN) layers in\nthe models are randomly initialized and untrained. Our study focuses on RNN-T\nand Conformer models, and we show that model quality does not drop even when\nthe decoder is fully randomized. Furthermore, such models can be trained more\nefficiently as the decoders do not require to be updated. By contrast,\nrandomizing encoders hurts model quality, indicating that optimizing encoders\nand learn proper representations for acoustic inputs are more vital for speech\nrecognition. Overall, we challenge the common practice of training ASR models\nfor all components, and demonstrate that ESN-based models can perform equally\nwell but enable more efficient training and storage than fully-trainable\ncounterparts.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 02:04:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Shrivastava", "Harsh", ""], ["Garg", "Ankush", ""], ["Cao", "Yuan", ""], ["Zhang", "Yu", ""], ["Sainath", "Tara", ""]]}, {"id": "2102.09130", "submitter": "Feng Nan", "authors": "Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos,\n  Henghui Zhu, Dejiao Zhang, Kathleen McKeown, Bing Xiang", "title": "Entity-level Factual Consistency of Abstractive Text Summarization", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A key challenge for abstractive summarization is ensuring factual consistency\nof the generated summary with respect to the original document. For example,\nstate-of-the-art models trained on existing datasets exhibit entity\nhallucination, generating names of entities that are not present in the source\ndocument. We propose a set of new metrics to quantify the entity-level factual\nconsistency of generated summaries and we show that the entity hallucination\nproblem can be alleviated by simply filtering the training data. In addition,\nwe propose a summary-worthy entity classification task to the training process\nas well as a joint entity and summary generation approach, which yield further\nimprovements in entity level metrics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:07:28 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Nan", "Feng", ""], ["Nallapati", "Ramesh", ""], ["Wang", "Zhiguo", ""], ["Santos", "Cicero Nogueira dos", ""], ["Zhu", "Henghui", ""], ["Zhang", "Dejiao", ""], ["McKeown", "Kathleen", ""], ["Xiang", "Bing", ""]]}, {"id": "2102.09136", "submitter": "Amir Tahmasebi", "authors": "Cansu Sen, Bingyang Ye, Javed Aslam, Amir Tahmasebi", "title": "From Extreme Multi-label to Multi-class: A Hierarchical Approach for\n  Automated ICD-10 Coding Using Phrase-level Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clinical coding is the task of assigning a set of alphanumeric codes,\nreferred to as ICD (International Classification of Diseases), to a medical\nevent based on the context captured in a clinical narrative. The latest version\nof ICD, ICD-10, includes more than 70,000 codes. As this is a labor-intensive\nand error-prone task, automatic ICD coding of medical reports using machine\nlearning has gained significant interest in the last decade. Existing\nliterature has modeled this problem as a multi-label task. Nevertheless, such\nmulti-label approach is challenging due to the extremely large label set size.\nFurthermore, the interpretability of the predictions is essential for the\nendusers (e.g., healthcare providers and insurance companies). In this paper,\nwe propose a novel approach for automatic ICD coding by reformulating the\nextreme multi-label problem into a simpler multi-class problem using a\nhierarchical solution. We made this approach viable through extensive data\ncollection to acquire phrase-level human coder annotations to supervise our\nmodels on learning the specific relations between the input text and predicted\nICD codes. Our approach employs two independently trained networks, the\nsentence tagger and the ICD classifier, stacked hierarchically to predict a\ncodeset for a medical report. The sentence tagger identifies focus sentences\ncontaining a medical event or concept relevant to an ICD coding. Using a\nsupervised attention mechanism, the ICD classifier then assigns each focus\nsentence with an ICD code. The proposed approach outperforms strong baselines\nby large margins of 23% in subset accuracy, 18% in micro-F1, and 15% in\ninstance based F-1. With our proposed approach, interpretability is achieved\nnot through implicitly learned attention scores but by attributing each\nprediction to a particular sentence and words selected by human coders.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:19:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Sen", "Cansu", ""], ["Ye", "Bingyang", ""], ["Aslam", "Javed", ""], ["Tahmasebi", "Amir", ""]]}, {"id": "2102.09268", "submitter": "Shitao Xiao", "authors": "Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di and Xing Xie", "title": "Training Large-Scale News Recommenders with Pretrained Language Models\n  in the Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News recommendation calls for deep insights of news articles' underlying\nsemantics. Therefore, pretrained language models (PLMs), like BERT and RoBERTa,\nmay substantially contribute to the recommendation quality. However, it's\nextremely challenging to have news recommenders trained together with such big\nmodels: the learning of news recommenders requires intensive news encoding\noperations, whose cost is prohibitive if PLMs are used as the news encoder. In\nthis paper, we propose a novel framework, {SpeedyFeed}, which efficiently\ntrains PLMs-based news recommenders of superior quality. SpeedyFeed is\nhighlighted for its light-weighted encoding pipeline, which gives rise to three\nmajor advantages. Firstly, it makes the intermedia results fully reusable for\nthe training workflow, which removes most of the repetitive but redundant\nencoding operations. Secondly, it improves the data efficiency of the training\nworkflow, where non-informative data can be eliminated from encoding. Thirdly,\nit further saves the cost by leveraging simplified news encoding and compact\nnews representation. Extensive experiments show that SpeedyFeed leads to more\nthan 100$\\times$ acceleration of the training process, which enables big models\nto be trained efficiently and effectively over massive user data. The\nwell-trained PLMs-based model from SpeedyFeed demonstrates highly competitive\nperformance, where it outperforms the state-of-the-art news recommenders with\nsignificant margins. SpeedyFeed is also a model-agnostic framework, which is\npotentially applicable to a wide spectrum of content-based recommender systems;\ntherefore, the whole framework is open-sourced to facilitate the progress in\nrelated areas.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 11:08:38 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 02:15:26 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Xiao", "Shitao", ""], ["Liu", "Zheng", ""], ["Shao", "Yingxia", ""], ["Di", "Tao", ""], ["Xie", "Xing", ""]]}, {"id": "2102.09282", "submitter": "Haolan Zhan", "authors": "Lei Shen, Haolan Zhan, Xin Shen, Yang Feng", "title": "Learning to Select Context in a Hierarchical and Global Perspective for\n  Open-domain Dialogue Generation", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain multi-turn conversations mainly have three features, which are\nhierarchical semantic structure, redundant information, and long-term\ndependency. Grounded on these, selecting relevant context becomes a challenge\nstep for multi-turn dialogue generation. However, existing methods cannot\ndifferentiate both useful words and utterances in long distances from a\nresponse. Besides, previous work just performs context selection based on a\nstate in the decoder, which lacks a global guidance and could lead some focuses\non irrelevant or unnecessary information. In this paper, we propose a novel\nmodel with hierarchical self-attention mechanism and distant supervision to not\nonly detect relevant words and utterances in short and long distances, but also\ndiscern related information globally when decoding. Experimental results on two\npublic datasets of both automatic and human evaluations show that our model\nsignificantly outperforms other baselines in terms of fluency, coherence, and\ninformativeness.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 11:56:42 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Shen", "Lei", ""], ["Zhan", "Haolan", ""], ["Shen", "Xin", ""], ["Feng", "Yang", ""]]}, {"id": "2102.09347", "submitter": "Valdigleis Silva Costa", "authors": "Valdigleis S. Costa, Benjam\\'in C. Bedregal and Regivan H. N. Santiago", "title": "On Typical Hesitant Fuzzy Languages and Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The idea of nondeterministic typical hesitant fuzzy automata is a\ngeneralization of the fuzzy automata presented by Costa and Bedregal. This\npaper, presents the sufficient and necessary conditions for a typical hesitant\nfuzzy language to be computed by nondeterministic typical hesitant fuzzy\nautomata. Besides, the paper introduces a new class of Typical Hesitant Fuzzy\nAutomata with crisp transitions, and we will show that this new class is\nequivalent to the original class introduced by Costa and Bedregal\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 13:59:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Costa", "Valdigleis S.", ""], ["Bedregal", "Benjam\u00edn C.", ""], ["Santiago", "Regivan H. N.", ""]]}, {"id": "2102.09379", "submitter": "Radu Tudor Ionescu", "authors": "Mihaela Gaman, Sebastian Cojocariu, Radu Tudor Ionescu", "title": "UnibucKernel: Geolocating Swiss German Jodels Using Ensemble Learning", "comments": "This paper describes our system for the SMG-CH shared task of the\n  VarDial 2021 Evaluation Campaign. arXiv admin note: text overlap with\n  arXiv:2010.03614", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe our approach addressing the Social Media Variety\nGeolocation task featured in the 2021 VarDial Evaluation Campaign. We focus on\nthe second subtask, which is based on a data set formed of approximately 30\nthousand Swiss German Jodels. The dialect identification task is about\naccurately predicting the latitude and longitude of test samples. We frame the\ntask as a double regression problem, employing an XGBoost meta-learner with the\ncombined power of a variety of machine learning approaches to predict both\nlatitude and longitude. The models included in our ensemble range from simple\nregression techniques, such as Support Vector Regression, to deep neural\nmodels, such as a hybrid neural network and a neural transformer. To minimize\nthe prediction error, we approach the problem from a few different perspectives\nand consider various types of features, from low-level character n-grams to\nhigh-level BERT embeddings. The XGBoost ensemble resulted from combining the\npower of the aforementioned methods achieves a median distance of 23.6 km on\nthe test data, which places us on the third place in the ranking, at a\ndifference of 6.05 km and 2.9 km from the submissions on the first and second\nplaces, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:26:00 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 08:31:31 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 20:05:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gaman", "Mihaela", ""], ["Cojocariu", "Sebastian", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2102.09397", "submitter": "Yi-Syuan Chen", "authors": "Yi-Syuan Chen and Hong-Han Shuai", "title": "Meta-Transfer Learning for Low-Resource Abstractive Summarization", "comments": "Accepted by AAAI 2021; Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural abstractive summarization has been studied in many pieces of\nliterature and achieves great success with the aid of large corpora. However,\nwhen encountering novel tasks, one may not always benefit from transfer\nlearning due to the domain shifting problem, and overfitting could happen\nwithout adequate labeled examples. Furthermore, the annotations of abstractive\nsummarization are costly, which often demand domain knowledge to ensure the\nground-truth quality. Thus, there are growing appeals for Low-Resource\nAbstractive Summarization, which aims to leverage past experience to improve\nthe performance with limited labeled examples of target corpus. In this paper,\nwe propose to utilize two knowledge-rich sources to tackle this problem, which\nare large pre-trained models and diverse existing corpora. The former can\nprovide the primary ability to tackle summarization tasks; the latter can help\ndiscover common syntactic or semantic information to improve the generalization\nability. We conduct extensive experiments on various summarization corpora with\ndifferent writing styles and forms. The results demonstrate that our approach\nachieves the state-of-the-art on 6 corpora in low-resource scenarios, with only\n0.7% of trainable parameters compared to previous work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:42:09 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 06:27:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Yi-Syuan", ""], ["Shuai", "Hong-Han", ""]]}, {"id": "2102.09470", "submitter": "Lovedeep Singh", "authors": "Lovedeep Singh", "title": "Fake News Detection: a comparison between available Deep Learning\n  techniques in vector space", "comments": "for citiation purpose, use details available on official IEEE Xplore\n  page: https://doi.org/10.1109/CICT51604.2020.9312099", "journal-ref": "2020 IEEE 4th Conference on Information & Communication Technology\n  (CICT)", "doi": "10.1109/CICT51604.2020.9312099", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fake News Detection is an essential problem in the field of Natural Language\nProcessing. The benefits of an effective solution in this area are manifold for\nthe goodwill of society. On a surface level, it broadly matches with the\ngeneral problem of text classification. Researchers have proposed various\napproaches to tackle fake news using simple as well as some complex techniques.\nIn this paper, we try to make a comparison between the present Deep Learning\ntechniques by representing the news instances in some vector space using a\ncombination of common mathematical operations with available vector space\nrepresentations. We do a number of experiments using various combinations and\npermutations. Finally, we conclude with a sound analysis of the results and\nevaluate the reasons for such results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:42:28 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Singh", "Lovedeep", ""]]}, {"id": "2102.09507", "submitter": "Jacqueline Liu", "authors": "Igor L. Markov, Jacqueline Liu, Adam Vagner", "title": "Regular Expressions for Fast-response COVID-19 Text Classification", "comments": "10 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text classifiers are at the core of many NLP applications and use a variety\nof algorithmic approaches and software. This paper introduces infrastructure\nand methodologies for text classifiers based on large-scale regular\nexpressions. In particular, we describe how Facebook determines if a given\npiece of text - anything from a hashtag to a post - belongs to a narrow topic\nsuch as COVID-19. To fully define a topic and evaluate classifier performance\nwe employ human-guided iterations of keyword discovery, but do not require\nlabeled data. For COVID-19, we build two sets of regular expressions: (1) for\n66 languages, with 99% precision and recall >50%, (2) for the 11 most common\nlanguages, with precision >90% and recall >90%. Regular expressions enable\nlow-latency queries from multiple platforms. Response to challenges like\nCOVID-19 is fast and so are revisions. Comparisons to a DNN classifier show\nexplainable results, higher precision and recall, and less overfitting. Our\nlearnings can be applied to other narrow-topic classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 17:48:49 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 19:23:33 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 23:13:24 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 15:46:18 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Markov", "Igor L.", ""], ["Liu", "Jacqueline", ""], ["Vagner", "Adam", ""]]}, {"id": "2102.09542", "submitter": "Bo Li", "authors": "Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, Xiao-Ming Wu", "title": "SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical\n  Visual Question Answering", "comments": "ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical visual question answering (Med-VQA) has tremendous potential in\nhealthcare. However, the development of this technology is hindered by the\nlacking of publicly-available and high-quality labeled datasets for training\nand evaluation. In this paper, we present a large bilingual dataset, SLAKE,\nwith comprehensive semantic labels annotated by experienced physicians and a\nnew structural medical knowledge base for Med-VQA. Besides, SLAKE includes\nricher modalities and covers more human body parts than the currently available\ndataset. We show that SLAKE can be used to facilitate the development and\nevaluation of Med-VQA systems. The dataset can be downloaded from\nhttp://www.med-vqa.com/slake.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:44:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liu", "Bo", ""], ["Zhan", "Li-Ming", ""], ["Xu", "Li", ""], ["Ma", "Lin", ""], ["Yang", "Yan", ""], ["Wu", "Xiao-Ming", ""]]}, {"id": "2102.09550", "submitter": "Lukasz Borchmann", "authors": "Rafa{\\l} Powalski, {\\L}ukasz Borchmann, Dawid Jurkiewicz, Tomasz\n  Dwojak, Micha{\\l} Pietruszka, Gabriela Pa{\\l}ka", "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout\n  Transformer", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of Natural Language Comprehension beyond\nplain-text documents by introducing the TILT neural network architecture which\nsimultaneously learns layout information, visual features, and textual\nsemantics. Contrary to previous approaches, we rely on a decoder capable of\nunifying a variety of problems involving natural language. The layout is\nrepresented as an attention bias and complemented with contextualized visual\ninformation, while the core of our model is a pretrained encoder-decoder\nTransformer. Our novel approach achieves state-of-the-art results in extracting\ninformation from documents and answering questions which demand layout\nunderstanding (DocVQA, CORD, SROIE). At the same time, we simplify the process\nby employing an end-to-end model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:51:47 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 11:27:37 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 09:31:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Powalski", "Rafa\u0142", ""], ["Borchmann", "\u0141ukasz", ""], ["Jurkiewicz", "Dawid", ""], ["Dwojak", "Tomasz", ""], ["Pietruszka", "Micha\u0142", ""], ["Pa\u0142ka", "Gabriela", ""]]}, {"id": "2102.09553", "submitter": "Arlene Casey J", "authors": "Arlene Casey, Emma Davidson, Michael Poon, Hang Dong, Daniel Duma,\n  Andreas Grivas, Claire Grover, V\\'ictor Su\\'arez-Paniagua, Richard Tobin,\n  William Whiteley, Honghan Wu, Beatrice Alex", "title": "A Systematic Review of Natural Language Processing Applied to Radiology\n  Reports", "comments": null, "journal-ref": "BMC Medical Informatics and Decision Making 2021", "doi": "10.1186/s12911-021-01533-7", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  NLP has a significant role in advancing healthcare and has been found to be\nkey in extracting structured information from radiology reports. Understanding\nrecent developments in NLP application to radiology is of significance but\nrecent reviews on this are limited. This study systematically assesses recent\nliterature in NLP applied to radiology reports. Our automated literature search\nyields 4,799 results using automated filtering, metadata enriching steps and\ncitation search combined with manual review. Our analysis is based on 21\nvariables including radiology characteristics, NLP methodology, performance,\nstudy, and clinical application characteristics. We present a comprehensive\nanalysis of the 164 publications retrieved with each categorised into one of 6\nclinical application categories. Deep learning use increases but conventional\nmachine learning approaches are still prevalent. Deep learning remains\nchallenged when data is scarce and there is little evidence of adoption into\nclinical practice. Despite 17% of studies reporting greater than 0.85 F1\nscores, it is hard to comparatively evaluate these approaches given that most\nof them use different datasets. Only 14 studies made their data and 15 their\ncode available with 10 externally validating results. Automated understanding\nof clinical narratives of the radiology reports has the potential to enhance\nthe healthcare process but reproducibility and explainability of models are\nimportant if the domain is to move applications into clinical use. More could\nbe done to share code enabling validation of methods on different institutional\ndata and to reduce heterogeneity in reporting of study properties allowing\ninter-study comparisons. Our results have significance for researchers\nproviding a systematic synthesis of existing work to build on, identify gaps,\nopportunities for collaboration and avoid duplication.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:54:41 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Casey", "Arlene", ""], ["Davidson", "Emma", ""], ["Poon", "Michael", ""], ["Dong", "Hang", ""], ["Duma", "Daniel", ""], ["Grivas", "Andreas", ""], ["Grover", "Claire", ""], ["Su\u00e1rez-Paniagua", "V\u00edctor", ""], ["Tobin", "Richard", ""], ["Whiteley", "William", ""], ["Wu", "Honghan", ""], ["Alex", "Beatrice", ""]]}, {"id": "2102.09600", "submitter": "Shafiuddin Rehan Ahmed", "authors": "Shafiuddin Rehan Ahmed and James H. Martin", "title": "Within-Document Event Coreference with BERT-Based Contextualized\n  Representations", "comments": "9 pages, 1 figure, 10 tables, rejected in aaai 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Event coreference continues to be a challenging problem in information\nextraction. With the absence of any external knowledge bases for events,\ncoreference becomes a clustering task that relies on effective representations\nof the context in which event mentions appear. Recent advances in\ncontextualized language representations have proven successful in many tasks,\nhowever, their use in event linking been limited. Here we present a three part\napproach that (1) uses representations derived from a pretrained BERT model to\n(2) train a neural classifier to (3) drive a simple clustering algorithm to\ncreate coreference chains. We achieve state of the art results with this model\non two standard datasets for within-document event coreference task and\nestablish a new standard on a third newer dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 21:12:43 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ahmed", "Shafiuddin Rehan", ""], ["Martin", "James H.", ""]]}, {"id": "2102.09604", "submitter": "Timour Igamberdiev", "authors": "Timour Igamberdiev and Ivan Habernal", "title": "Privacy-Preserving Graph Convolutional Networks for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph convolutional networks (GCNs) are a powerful architecture for\nrepresentation learning on documents that naturally occur as graphs, e.g.,\ncitation or social networks. However, sensitive personal information, such as\ndocuments with people's profiles or relationships as edges, are prone to\nprivacy leaks, as the trained model might reveal the original input. Although\ndifferential privacy (DP) offers a well-founded privacy-preserving framework,\nGCNs pose theoretical and practical challenges due to their training specifics.\nWe address these challenges by adapting differentially-private gradient-based\ntraining to GCNs and conduct experiments using two optimizers on five NLP\ndatasets in two languages. We propose a simple yet efficient method based on\nrandom graph splits that not only improves the baseline privacy bounds by a\nfactor of 2.7 while retaining competitive F1 scores, but also provides strong\nprivacy guarantees of epsilon = 1.0. We show that, under certain modeling\nchoices, privacy-preserving GCNs perform up to 90% of their non-private\nvariants, while formally guaranteeing strong privacy measures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:27:38 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 14:11:40 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Igamberdiev", "Timour", ""], ["Habernal", "Ivan", ""]]}, {"id": "2102.09665", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Marcos Zampieri", "title": "MUDES: Multilingual Detection of Offensive Spans", "comments": "Accepted to NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The interest in offensive content identification in social media has grown\nsubstantially in recent years. Previous work has dealt mostly with post level\nannotations. However, identifying offensive spans is useful in many ways. To\nhelp coping with this important challenge, we present MUDES, a multilingual\nsystem to detect offensive spans in texts. MUDES features pre-trained models, a\nPython API for developers, and a user-friendly web-based interface. A detailed\ndescription of MUDES' components is presented in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:19:00 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 15:22:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2102.09680", "submitter": "Mario Campos Soberanis", "authors": "Diego Campos-Sobrino, Mario Campos-Soberanis, Iv\\'an Mart\\'inez-Chin,\n  V\\'ictor Uc-Cetina", "title": "Fixing Errors of the Google Voice Recognizer through Phonetic Distance\n  Metrics", "comments": "13 pages, 4 figures. This article is a translation of the paper\n  \"Correcci\\'on de errores del reconocedor de voz de Google usando m\\'etricas\n  de distancia fon\\'etica\" presented in COMIA 2018", "journal-ref": "Research in Computing Science 148(1), 2019, pp. 57-70. ISSN\n  1870-4069", "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech recognition systems for the Spanish language, such as Google's,\nproduce errors quite frequently when used in applications of a specific domain.\nThese errors mostly occur when recognizing words new to the recognizer's\nlanguage model or ad hoc to the domain. This article presents an algorithm that\nuses Levenshtein distance on phonemes to reduce the speech recognizer's errors.\nThe preliminary results show that it is possible to correct the recognizer's\nerrors significantly by using this metric and using a dictionary of specific\nphrases from the domain of the application. Despite being designed for\nparticular domains, the algorithm proposed here is of general application. The\nphrases that must be recognized can be explicitly defined for each application,\nwithout the algorithm having to be modified. It is enough to indicate to the\nalgorithm the set of sentences on which it must work. The algorithm's\ncomplexity is $O(tn)$, where $t$ is the number of words in the transcript to be\ncorrected, and $n$ is the number of phrases specific to the domain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:54:59 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Campos-Sobrino", "Diego", ""], ["Campos-Soberanis", "Mario", ""], ["Mart\u00ednez-Chin", "Iv\u00e1n", ""], ["Uc-Cetina", "V\u00edctor", ""]]}, {"id": "2102.09681", "submitter": "Vinay Rao", "authors": "Robert Ormandi, Mohammad Saleh, Erin Winter, Vinay Rao", "title": "WebRED: Effective Pretraining And Finetuning For Relation Extraction On\n  The Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Relation extraction is used to populate knowledge bases that are important to\nmany applications. Prior datasets used to train relation extraction models\neither suffer from noisy labels due to distant supervision, are limited to\ncertain domains or are too small to train high-capacity models. This constrains\ndownstream applications of relation extraction. We therefore introduce: WebRED\n(Web Relation Extraction Dataset), a strongly-supervised human annotated\ndataset for extracting relationships from a variety of text found on the World\nWide Web, consisting of ~110K examples. We also describe the methods we used to\ncollect ~200M examples as pre-training data for this task. We show that\ncombining pre-training on a large weakly supervised dataset with fine-tuning on\na small strongly-supervised dataset leads to better relation extraction\nperformance. We provide baselines for this new dataset and present a case for\nthe importance of human annotation in improving the performance of relation\nextraction from text found on the web.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:56:12 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ormandi", "Robert", ""], ["Saleh", "Mohammad", ""], ["Winter", "Erin", ""], ["Rao", "Vinay", ""]]}, {"id": "2102.09690", "submitter": "Eric Wallace", "authors": "Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPT-3 can perform numerous tasks when provided a natural language prompt that\ncontains a few training examples. We show that this type of few-shot learning\ncan be unstable: the choice of prompt format, training examples, and even the\norder of the training examples can cause accuracy to vary from near chance to\nnear state-of-the-art. We demonstrate that this instability arises from the\nbias of language models towards predicting certain answers, e.g., those that\nare placed near the end of the prompt or are common in the pre-training data.\nTo mitigate this, we first estimate the model's bias towards each answer by\nasking for its prediction when given the training prompt and a content-free\ntest input such as \"N/A\". We then fit calibration parameters that cause the\nprediction for this input to be uniform across answers. On a diverse set of\ntasks, this contextual calibration procedure substantially improves GPT-3 and\nGPT-2's average accuracy (up to 30.0% absolute) and reduces variance across\ndifferent choices of the prompt.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 00:23:59 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 18:20:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhao", "Tony Z.", ""], ["Wallace", "Eric", ""], ["Feng", "Shi", ""], ["Klein", "Dan", ""], ["Singh", "Sameer", ""]]}, {"id": "2102.09708", "submitter": "Matthew Ciolino", "authors": "Matthew Ciolino, David Noever, Josh Kalin", "title": "Multilingual Augmenter: The Model Chooses", "comments": "18 Pages, 10 Figures, 4 Tables, 37 References", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) relies heavily on training data.\nTransformers, as they have gotten bigger, have required massive amounts of\ntraining data. To satisfy this requirement, text augmentation should be looked\nat as a way to expand your current dataset and to generalize your models. One\ntext augmentation we will look at is translation augmentation. We take an\nEnglish sentence and translate it to another language before translating it\nback to English. In this paper, we look at the effect of 108 different language\nback translations on various metrics and text embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 02:08:26 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ciolino", "Matthew", ""], ["Noever", "David", ""], ["Kalin", "Josh", ""]]}, {"id": "2102.09727", "submitter": "Seohyeong Jeong", "authors": "Seohyeong Jeong, Nojun Kwak", "title": "Learning Dynamic BERT via Trainable Gate Variables and a Bi-modal\n  Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BERT model has shown significant success on various natural language\nprocessing tasks. However, due to the heavy model size and high computational\ncost, the model suffers from high latency, which is fatal to its deployments on\nresource-limited devices. To tackle this problem, we propose a dynamic\ninference method on BERT via trainable gate variables applied on input tokens\nand a regularizer that has a bi-modal property. Our method shows reduced\ncomputational cost on the GLUE dataset with a minimal performance drop.\nMoreover, the model adjusts with a trade-off between performance and\ncomputational cost with the user-specified hyperparameter.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 03:59:23 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Jeong", "Seohyeong", ""], ["Kwak", "Nojun", ""]]}, {"id": "2102.09749", "submitter": "Anshul Wadhawan", "authors": "Anshul Wadhawan", "title": "Dialect Identification in Nuanced Arabic Tweets Using Farasa\n  Segmentation and AraBERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our approach to address the EACL WANLP-2021 Shared Task\n1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at\ndeveloping a system that identifies the geographical location(country/province)\nfrom where an Arabic tweet in the form of modern standard Arabic or dialect\ncomes from. We solve the task in two parts. The first part involves\npre-processing the provided dataset by cleaning, adding and segmenting various\nparts of the text. This is followed by carrying out experiments with different\nversions of two Transformer based models, AraBERT and AraELECTRA. Our final\napproach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four\nsubtasks, and we were ranked second in MSA identification subtasks and fourth\nin DA identification subtasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 05:39:21 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:51:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wadhawan", "Anshul", ""]]}, {"id": "2102.09761", "submitter": "Tom Hope", "authors": "Tom Hope, Ronen Tamari, Hyeonsu Kang, Daniel Hershcovich, Joel Chan,\n  Aniket Kittur, Dafna Shahaf", "title": "Scaling Creative Inspiration with Fine-Grained Functional Facets of\n  Product Ideas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-scale repositories of products, patents and scientific papers offer an\nopportunity for creating automated systems that scour millions of ideas and\nassist users in discovering inspirations and solutions. Yet the common\nrepresentation of ideas is in the form of raw textual descriptions, lacking\nimportant structure that is required for supporting creative innovation. Prior\nwork has pointed to the importance of functional structure -- capturing the\nmechanisms and purposes of inventions -- for allowing users to discover\nstructural connections across ideas and creatively adapt existing technologies.\nHowever, the use of functional representations was either coarse and limited in\nexpressivity, or dependent on curated knowledge bases with poor coverage and\nsignificant manual effort from users.\n  To help bridge this gap and unlock the potential of large-scale idea mining,\nwe propose a novel computational representation that automatically breaks up\nproducts into fine-grained functional facets. We train a model to extract these\nfacets from a challenging real-world corpus of invention descriptions, and\nrepresent each product as a set of facet embeddings. We design similarity\nmetrics that support granular matching between functional facets across ideas,\nand use them to build a novel functional search capability that enables\nexpressive queries for mechanisms and purposes. We construct a graph capturing\nhierarchical relations between purposes and mechanisms across an entire corpus\nof products, and use the graph to help problem-solvers explore the design space\naround a focal problem and view related problem perspectives. In empirical user\nstudies, our approach leads to a significant boost in search accuracy and in\nthe quality of creative inspirations, outperforming strong baselines and\nstate-of-art representations of product texts by 50-60%.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 06:30:41 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Hope", "Tom", ""], ["Tamari", "Ronen", ""], ["Kang", "Hyeonsu", ""], ["Hershcovich", "Daniel", ""], ["Chan", "Joel", ""], ["Kittur", "Aniket", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2102.09777", "submitter": "Farhad Nooralahzadeh", "authors": "Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder,\n  Koji Fujimoto, Michael Krauthammer", "title": "Progressive Transformer-Based Generation of Radiology Reports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by Curriculum Learning, we propose a consecutive (i.e.\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using transformer-based architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 07:42:13 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nooralahzadeh", "Farhad", ""], ["Gonzalez", "Nicolas Perez", ""], ["Frauenfelder", "Thomas", ""], ["Fujimoto", "Koji", ""], ["Krauthammer", "Michael", ""]]}, {"id": "2102.09786", "submitter": "ChaeHun Park", "authors": "ChaeHun Park and Sangwoo Seo", "title": "An Empirical Study on Measuring the Similarity of Sentential Arguments\n  with Language Model Domain Adaptation", "comments": "4+2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the similarity between two different sentential arguments is an\nimportant task in argument mining. However, one of the challenges in this field\nis that the dataset must be annotated using expertise in a variety of topics,\nmaking supervised learning with labeled data expensive. In this paper, we\ninvestigated whether this problem could be alleviated through transfer\nlearning. We first adapted a pretrained language model to a domain of interest\nusing self-supervised learning. Then, we fine-tuned the model to a task of\nmeasuring the similarity between sentences taken from different domains. Our\napproach improves a correlation with human-annotated similarity scores compared\nto competitive baseline models on the Argument Facet Similarity dataset in an\nunsupervised setting. Moreover, we achieve comparable performance to a fully\nsupervised baseline model by using only about 60% of the labeled data samples.\nWe believe that our work suggests the possibility of a generalized argument\nclustering model for various argumentative topics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:05:46 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Park", "ChaeHun", ""], ["Seo", "Sangwoo", ""]]}, {"id": "2102.09866", "submitter": "Varsha Pathak Dr.", "authors": "Varsha Pathak, Manish Joshi, Prasad Joshi, Monica Mundada and Tanmay\n  Joshi", "title": "KBCNMUJAL@HASOC-Dravidian-CodeMix-FIRE2020: Using Machine Learning for\n  Detection of Hate Speech and Offensive Code-Mixed Social Media text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the system submitted by our team, KBCNMUJAL, for Task 2\nof the shared task Hate Speech and Offensive Content Identification in\nIndo-European Languages (HASOC), at Forum for Information Retrieval Evaluation,\nDecember 16-20, 2020, Hyderabad, India. The datasets of two Dravidian languages\nViz. Malayalam and Tamil of size 4000 observations, each were shared by the\nHASOC organizers. These datasets are used to train the machine using different\nmachine learning algorithms, based on classification and regression models. The\ndatasets consist of tweets or YouTube comments with two class labels offensive\nand not offensive. The machine is trained to classify such social media\nmessages in these two categories. Appropriate n-gram feature sets are extracted\nto learn the specific characteristics of the Hate Speech text messages. These\nfeature models are based on TFIDF weights of n-gram. The referred work and\nrespective experiments show that the features such as word, character and\ncombined model of word and character n-grams could be used to identify the term\npatterns of offensive text contents. As a part of the HASOC shared task, the\ntest data sets are made available by the HASOC track organizers. The best\nperforming classification models developed for both languages are applied on\ntest datasets. The model which gives the highest accuracy result on training\ndataset for Malayalam language was experimented to predict the categories of\nrespective test data. This system has obtained an F1 score of 0.77. Similarly\nthe best performing model for Tamil language has obtained an F1 score of 0.87.\nThis work has received 2nd and 3rd rank in this shared Task 2 for Malayalam and\nTamil language respectively. The proposed system is named HASOC_kbcnmujal.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:08:02 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Pathak", "Varsha", ""], ["Joshi", "Manish", ""], ["Joshi", "Prasad", ""], ["Mundada", "Monica", ""], ["Joshi", "Tanmay", ""]]}, {"id": "2102.09914", "submitter": "Brooke Stephenson", "authors": "Brooke Stephenson, Thomas Hueber, Laurent Girin, Laurent Besacier", "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with\n  Predicted Future Text Input", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prosody of a spoken word is determined by its surrounding context. In\nincremental text-to-speech synthesis, where the synthesizer produces an output\nbefore it has access to the complete input, the full context is often unknown\nwhich can result in a loss of naturalness in the synthesized speech. In this\npaper, we investigate whether the use of predicted future text can attenuate\nthis loss. We compare several test conditions of next future word: (a) unknown\n(zero-word), (b) language model predicted, (c) randomly predicted and (d)\nground-truth. We measure the prosodic features (pitch, energy and duration) and\nfind that predicted text provides significant improvements over a zero-word\nlookahead, but only slight gains over random-word lookahead. We confirm these\nresults with a perceptive test.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 13:11:34 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 08:13:39 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Stephenson", "Brooke", ""], ["Hueber", "Thomas", ""], ["Girin", "Laurent", ""], ["Besacier", "Laurent", ""]]}, {"id": "2102.09923", "submitter": "Zijian Wang", "authors": "Zijian Wang, Hao Wang, Xiangfeng Luo, Jianqi Gao", "title": "Back to Prior Knowledge: Joint Event Causality Extraction via\n  Convolutional Semantic Infusion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Joint event and causality extraction is a challenging yet essential task in\ninformation retrieval and data mining. Recently, pre-trained language models\n(e.g., BERT) yield state-of-the-art results and dominate in a variety of NLP\ntasks. However, these models are incapable of imposing external knowledge in\ndomain-specific extraction. Considering the prior knowledge of frequent n-grams\nthat represent cause/effect events may benefit both event and causality\nextraction, in this paper, we propose convolutional knowledge infusion for\nfrequent n-grams with different windows of length within a joint extraction\nframework. Knowledge infusion during convolutional filter initialization not\nonly helps the model capture both intra-event (i.e., features in an event\ncluster) and inter-event (i.e., associations across event clusters) features\nbut also boosts training convergence. Experimental results on the benchmark\ndatasets show that our model significantly outperforms the strong BERT+CSNN\nbaseline.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 13:31:46 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Wang", "Zijian", ""], ["Wang", "Hao", ""], ["Luo", "Xiangfeng", ""], ["Gao", "Jianqi", ""]]}, {"id": "2102.09928", "submitter": "Lasse Borgholt", "authors": "Lasse Borgholt, Jakob Drachmann Havtorn, \\v{Z}eljko Agi\\'c, Anders\n  S{\\o}gaard, Lars Maal{\\o}e, Christian Igel", "title": "Do End-to-End Speech Recognition Models Care About Context?", "comments": "Published in the proceedings of INTERSPEECH 2020, pp. 4352-4356", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1750", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two most common paradigms for end-to-end speech recognition are\nconnectionist temporal classification (CTC) and attention-based encoder-decoder\n(AED) models. It has been argued that the latter is better suited for learning\nan implicit language model. We test this hypothesis by measuring temporal\ncontext sensitivity and evaluate how the models perform when we constrain the\namount of contextual information in the audio input. We find that the AED model\nis indeed more context sensitive, but that the gap can be closed by adding\nself-attention to the CTC model. Furthermore, the two models perform similarly\nwhen contextual information is constrained. Finally, in contrast to previous\nresearch, our results show that the CTC model is highly competitive on WSJ and\nLibriSpeech without the help of an external language model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 11:13:50 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Borgholt", "Lasse", ""], ["Havtorn", "Jakob Drachmann", ""], ["Agi\u0107", "\u017deljko", ""], ["S\u00f8gaard", "Anders", ""], ["Maal\u00f8e", "Lars", ""], ["Igel", "Christian", ""]]}, {"id": "2102.09943", "submitter": "Anshul Wadhawan", "authors": "Anshul Wadhawan, Akshita Aggarwal", "title": "Towards Emotion Recognition in Hindi-English Code-Mixed Data: A\n  Transformer Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last few years, emotion detection in social-media text has become a\npopular problem due to its wide ranging application in better understanding the\nconsumers, in psychology, in aiding human interaction with computers, designing\nsmart systems etc. Because of the availability of huge amounts of data from\nsocial-media, which is regularly used for expressing sentiments and opinions,\nthis problem has garnered great attention. In this paper, we present a Hinglish\ndataset labelled for emotion detection. We highlight a deep learning based\napproach for detecting emotions in Hindi-English code mixed tweets, using\nbilingual word embeddings derived from FastText and Word2Vec approaches, as\nwell as transformer based models. We experiment with various deep learning\nmodels, including CNNs, LSTMs, Bi-directional LSTMs (with and without\nattention), along with transformers like BERT, RoBERTa, and ALBERT. The\ntransformer based BERT model outperforms all other models giving the best\nperformance with an accuracy of 71.43%.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 14:07:20 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 08:43:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wadhawan", "Anshul", ""], ["Aggarwal", "Akshita", ""]]}, {"id": "2102.09965", "submitter": "Mahieddine Djoudi", "authors": "Hichem Rahab, Abdelhafid Zitouni, Mahieddine Djoudi (TECHN\\'E - EA\n  6316)", "title": "An Enhanced Corpus for Arabic Newspapers Comments", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.00459", "journal-ref": "International Arab Journal of Information Technology, Colleges of\n  Computing and Information Society (CCIS), 2020, 17 (5), pp.789-798", "doi": "10.34028/iajit/17/5/12", "report-no": null, "categories": "cs.IR cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose our enhanced approach to create a dedicated corpus\nfor Algerian Arabic newspapers comments. The developed approach has to enhance\nan existing approach by the enrichment of the available corpus and the\ninclusion of the annotation step by following the Model Annotate Train Test\nEvaluate Revise (MATTER) approach. A corpus is created by collecting comments\nfrom web sites of three well know Algerian newspapers. Three classifiers,\nsupport vector machines, na{\\\"i}ve Bayes, and k-nearest neighbors, were used\nfor classification of comments into positive and negative classes. To identify\nthe influence of the stemming in the obtained results, the classification was\ntested with and without stemming. Obtained results show that stemming does not\nenhance considerably the classification due to the nature of Algerian comments\ntied to Algerian Arabic Dialect. The promising results constitute a motivation\nfor us to improve our approach especially in dealing with non Arabic sentences,\nespecially Dialectal and French ones.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:15:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Rahab", "Hichem", "", "TECHN\u00c9 - EA\n  6316"], ["Zitouni", "Abdelhafid", "", "TECHN\u00c9 - EA\n  6316"], ["Djoudi", "Mahieddine", "", "TECHN\u00c9 - EA\n  6316"]]}, {"id": "2102.09990", "submitter": "Anvesh Rao Vijjini", "authors": "Anvesh Rao Vijjini, Kaveri Anuranjana, Radhika Mamidi", "title": "Analyzing Curriculum Learning for Sentiment Analysis along Task\n  Difficulty, Pacing and Visualization Axes", "comments": "Accepted for presentation at WASSA 2021 at EACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  While Curriculum Learning (CL) has recently gained traction in Natural\nlanguage Processing Tasks, it is still not adequately analyzed. Previous works\nonly show their effectiveness but fail short to explain and interpret the\ninternal workings fully. In this paper, we analyze curriculum learning in\nsentiment analysis along multiple axes. Some of these axes have been proposed\nby earlier works that need more in-depth study. Such analysis requires\nunderstanding where curriculum learning works and where it does not. Our axes\nof analysis include Task difficulty on CL, comparing CL pacing techniques, and\nqualitative analysis by visualizing the movement of attention scores in the\nmodel as curriculum phases progress. We find that curriculum learning works\nbest for difficult tasks and may even lead to a decrement in performance for\ntasks with higher performance without curriculum learning. We see that One-Pass\ncurriculum strategies suffer from catastrophic forgetting and attention\nmovement visualization within curriculum pacing. This shows that curriculum\nlearning breaks down the challenging main task into easier sub-tasks solved\nsequentially.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 15:42:14 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 09:58:25 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 04:49:38 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Vijjini", "Anvesh Rao", ""], ["Anuranjana", "Kaveri", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2102.09991", "submitter": "Sohom Ghosh", "authors": "Sohom Ghosh and Ankush Chopra", "title": "Using Transformer based Ensemble Learning to classify Scientific\n  Articles", "comments": "8 pages, 3 tables, 1 figure, Trends and Applications in Knowledge\n  Discovery and Data Mining. PAKDD 2021. Lecture Notes in Computer Science,\n  Springer", "journal-ref": null, "doi": "10.1007/978-3-030-75015-2_11", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time reviewers fail to appreciate novel ideas of a researcher and\nprovide generic feedback. Thus, proper assignment of reviewers based on their\narea of expertise is necessary. Moreover, reading each and every paper from\nend-to-end for assigning it to a reviewer is a tedious task. In this paper, we\ndescribe a system which our team FideLIPI submitted in the shared task of\nSDPRA-2021 [14]. It comprises four independent sub-systems capable of\nclassifying abstracts of scientific literature to one of the given seven\nclasses. The first one is a RoBERTa [10] based model built over these\nabstracts. Adding topic models / Latent dirichlet allocation (LDA) [2] based\nfeatures to the first model results in the second sub-system. The third one is\na sentence level RoBERTa [10] model. The fourth one is a Logistic Regression\nmodel built using Term Frequency Inverse Document Frequency (TF-IDF) features.\nWe ensemble predictions of these four sub-systems using majority voting to\ndevelop the final system which gives a F1 score of 0.93 on the test and\nvalidation set. This outperforms the existing State Of The Art (SOTA) model\nSciBERT's [1] in terms of F1 score on the validation set.Our codebase is\navailable at https://github.com/SDPRA-2021/shared-task/tree/main/FideLIPI\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 15:42:26 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 12:21:56 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ghosh", "Sohom", ""], ["Chopra", "Ankush", ""]]}, {"id": "2102.10075", "submitter": "Tooba Tehreem", "authors": "Tooba Tehreem (Hira Tahir National University of Computer and Emerging\n  Sciences Islamabad, Pakistan)", "title": "Sentiment Analysis for YouTube Comments in Roman Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sentiment analysis is a vast area in the Machine learning domain. A lot of\nwork is done on datasets and their analysis of the English Language. In\nPakistan, a huge amount of data is in roman Urdu language, it is scattered all\nover the social sites including Twitter, YouTube, Facebook and similar\napplications. In this study the focus domain of dataset gathering is YouTube\ncomments. The Dataset contains the comments of people over different Pakistani\ndramas and TV shows. The Dataset contains multi-class classification that is\ngrouped The comments into positive, negative and neutral sentiment. In this\nStudy comparative analysis is done for five supervised learning Algorithms\nincluding linear regression, SVM, KNN, Multi layer Perceptron and Na\\\"ive Bayes\nclassifier. Accuracy, recall, precision and F-measure are used for measuring\nperformance. Results show that accuracy of SVM is 64 percent, which is better\nthan the rest of the list.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:15:52 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Tehreem", "Tooba", "", "Hira Tahir National University of Computer and Emerging\n  Sciences Islamabad, Pakistan"]]}, {"id": "2102.10084", "submitter": "Debjoy Saha", "authors": "Debjoy Saha, Naman Paharia, Debajit Chakraborty, Punyajoy Saha,\n  Animesh Mukherjee", "title": "Hate-Alert@DravidianLangTech-EACL2021: Ensembling strategies for\n  Transformer-based Offensive language Detection", "comments": "6 pages, 1 figure, 3 tables, code available at\n  https://github.com/Debjoy10/Hate-Alert-DravidianLangTech", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media often acts as breeding grounds for different forms of offensive\ncontent. For low resource languages like Tamil, the situation is more complex\ndue to the poor performance of multilingual or language-specific models and\nlack of proper benchmark datasets. Based on this shared task, Offensive\nLanguage Identification in Dravidian Languages at EACL 2021, we present an\nexhaustive exploration of different transformer models, We also provide a\ngenetic algorithm technique for ensembling different models. Our ensembled\nmodels trained separately for each language secured the first position in\nTamil, the second position in Kannada, and the first position in Malayalam\nsub-tasks. The models and codes are provided.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:35:38 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Saha", "Debjoy", ""], ["Paharia", "Naman", ""], ["Chakraborty", "Debajit", ""], ["Saha", "Punyajoy", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2102.10094", "submitter": "William Merrill", "authors": "William Merrill", "title": "Formal Language Theory Meets Modern NLP", "comments": "24 pages, tutorial document. Updated based on feedback received", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  NLP is deeply intertwined with the formal study of language, both\nconceptually and historically. Arguably, this connection goes all the way back\nto Chomsky's Syntactic Structures in 1957. It also still holds true today, with\na strand of recent works building formal analysis of modern neural networks\nmethods in terms of formal languages. In this document, I aim to explain\nbackground about formal languages as they relate to this recent work. I will by\nnecessity ignore large parts of the rich history of this field, instead\nfocusing on concepts connecting to modern deep learning-based NLP.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:51:10 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 23:18:29 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 00:20:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Merrill", "William", ""]]}, {"id": "2102.10160", "submitter": "Pavel Levin", "authors": "Emmanouil Stergiadis, Satendra Kumar, Fedor Kovalev, Pavel Levin", "title": "Multi-Domain Adaptation in Neural Machine Translation Through\n  Multidimensional Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern Neural Machine Translation (NMT) systems are trained on\nnonhomogeneous datasets with several distinct dimensions of variation (e.g.\ndomain, source, generation method, style, etc.). We describe and empirically\nevaluate multidimensional tagging (MDT), a simple yet effective method for\npassing sentence-level information to the model. Our human and BLEU evaluation\nresults show that MDT can be applied to the problem of multi-domain adaptation\nand significantly reduce training costs without sacrificing the translation\nquality on any of the constituent domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:19:42 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Stergiadis", "Emmanouil", ""], ["Kumar", "Satendra", ""], ["Kovalev", "Fedor", ""], ["Levin", "Pavel", ""]]}, {"id": "2102.10176", "submitter": "Yuan Wu", "authors": "Yuan Wu, Diana Inkpen, Ahmed El-Roby", "title": "Conditional Adversarial Networks for Multi-Domain Text Classification", "comments": "The Second Workshop on Domain Adaptation for NLP at EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose conditional adversarial networks (CANs), a\nframework that explores the relationship between the shared features and the\nlabel predictions to impose more discriminability to the shared features, for\nmulti-domain text classification (MDTC). The proposed CAN introduces a\nconditional domain discriminator to model the domain variance in both shared\nfeature representations and class-aware information simultaneously and adopts\nentropy conditioning to guarantee the transferability of the shared features.\nWe provide theoretical analysis for the CAN framework, showing that CAN's\nobjective is equivalent to minimizing the total divergence among multiple joint\ndistributions of shared features and label predictions. Therefore, CAN is a\ntheoretically sound adversarial network that discriminates over multiple\ndistributions. Evaluation results on two MDTC benchmarks show that CAN\noutperforms prior methods. Further experiments demonstrate that CAN has a good\nability to generalize learned knowledge to unseen domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:59:03 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wu", "Yuan", ""], ["Inkpen", "Diana", ""], ["El-Roby", "Ahmed", ""]]}, {"id": "2102.10182", "submitter": "Corto Mascle", "authors": "Yoan G\\'eran, Bastien Laboureix, Corto Mascle, Valentin D. Richard", "title": "Keyboards as a new model of computation", "comments": "Two versions, in French and in English", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new formalisation of languages, called keyboards. We consider\na set of elementary operations (writing/erasing a letter, going to the right or\nto the left,...) and we define a keyboard as a set of finite sequences of such\noperations, called keys. The corresponding language is the set of words\nobtained by applying some sequence of those keys. Unlike classical models of\ncomputation, every key can be applied anytime. We define various classes of\nlanguages based on different sets of elementary operations, and compare their\nexpressive powers. We also compare them to well-known classes of languages\n(Chomsky hierarchy). We obtain a strict hierarchy of languages, whose\nexpressivity is orthogonal to the one of the aforementionned classical models.\n  --\n  Nous introduisons une nouvelle repr\\'esentation de langages, les claviers. On\nse munit d'un ensemble d'op\\'erations \\'el\\'ementaires (ajout, effacement d'une\nlettre, d\\'eplacement \\`a droite, \\`a gauche, ...), et on d\\'efinit un clavier\ncomme un ensemble de suites finies d'op\\'erations \\'el\\'ementaires, appel\\'ees\ntouches. Son langage sera l'ensemble des mots obtenus en appliquant une suite\nquelconque de touches. Contrairement \\`a des mod\\`eles de calcul classiques,\ntoutes les touches peuvent \\^etre appliqu\\'ees \\`a tout moment. En premier lieu\nnous d\\'efinissons diff\\'erentes classes de claviers en faisant varier\nl'ensemble des op\\'erations \\'el\\'ementaires autoris\\'ees, et nous comparons\nl'expressivit\\'e des classes de langages obtenues. Nous comparons \\'egalement\nces classes \\`a la hi\\'erarchie de Chomsky. Nous obtenons que toutes les\nclasses \\'etudi\\'ees sont diff\\'erentes, et nous caract\\'erisons les classes\ninclues dans les rationnels et les alg\\'ebriques. L'expressivit\\'e des claviers\nsemble orthogonale \\`a celle des mod\\`eles \\'evoqu\\'es pr\\'ec\\'edemment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:17:18 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 10:37:22 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 14:51:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["G\u00e9ran", "Yoan", ""], ["Laboureix", "Bastien", ""], ["Mascle", "Corto", ""], ["Richard", "Valentin D.", ""]]}, {"id": "2102.10242", "submitter": "Haoming Jiang", "authors": "Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, Wei Wei", "title": "Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy\n  Evaluation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable automatic evaluation of dialogue systems under an interactive\nenvironment has long been overdue. An ideal environment for evaluating dialog\nsystems, also known as the Turing test, needs to involve human interaction,\nwhich is usually not affordable for large-scale experiments. Though researchers\nhave attempted to use metrics (e.g., perplexity, BLEU) in language generation\ntasks or some model-based reinforcement learning methods (e.g., self-play\nevaluation) for automatic evaluation, these methods only show a very weak\ncorrelation with the actual human evaluation in practice. To bridge such a gap,\nwe propose a new framework named ENIGMA for estimating human evaluation scores\nbased on recent advances of off-policy evaluation in reinforcement learning.\nENIGMA only requires a handful of pre-collected experience data, and therefore\ndoes not involve human interaction with the target policy during the\nevaluation, making automatic evaluations feasible. More importantly, ENIGMA is\nmodel-free and agnostic to the behavior policies for collecting the experience\ndata (see details in Section 2), which significantly alleviates the technical\ndifficulties of modeling complex dialogue environments and human behaviors. Our\nexperiments show that ENIGMA significantly outperforms existing methods in\nterms of correlation with human evaluation scores.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:29:20 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 22:23:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Jiang", "Haoming", ""], ["Dai", "Bo", ""], ["Yang", "Mengjiao", ""], ["Zhao", "Tuo", ""], ["Wei", "Wei", ""]]}, {"id": "2102.10243", "submitter": "Thuy Vu", "authors": "Thuy Vu and Alessandro Moschitti", "title": "Machine Translation Customization via Automatic Training Data Selection\n  from the Web", "comments": null, "journal-ref": "ECIR 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) systems, especially when designed for an industrial\nsetting, are trained with general parallel data derived from the Web. Thus,\ntheir style is typically driven by word/structure distribution coming from the\naverage of many domains. In contrast, MT customers want translations to be\nspecialized to their domain, for which they are typically able to provide text\nsamples. We describe an approach for customizing MT systems on specific domains\nby selecting data similar to the target customer data to train neural\ntranslation models. We build document classifiers using monolingual target\ndata, e.g., provided by the customers to select parallel training data from Web\ncrawled data. Finally, we train MT models on our automatically selected data,\nobtaining a system specialized to the target domain. We tested our approach on\nthe benchmark from WMT-18 Translation Task for News domains enabling\ncomparisons with state-of-the-art MT systems. The results show that our models\noutperform the top systems while using less data and smaller models.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:29:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2102.10246", "submitter": "Thuy Vu", "authors": "Thuy Vu and Alessandro Moschitti", "title": "CDA: a Cost Efficient Content-based Multilingual Web Document Aligner", "comments": null, "journal-ref": "EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Content-based Document Alignment approach (CDA), an efficient\nmethod to align multilingual web documents based on content in creating\nparallel training data for machine translation (MT) systems operating at the\nindustrial level. CDA works in two steps: (i) projecting documents of a web\ndomain to a shared multilingual space; then (ii) aligning them based on the\nsimilarity of their representations in such space. We leverage lexical\ntranslation models to build vector representations using TF-IDF. CDA achieves\nperformance comparable with state-of-the-art systems in the WMT-16 Bilingual\nDocument Alignment Shared Task benchmark while operating in multilingual space.\nBesides, we created two web-scale datasets to examine the robustness of CDA in\nan industrial setting involving up to 28 languages and millions of documents.\nThe experiments show that CDA is robust, cost-effective, and is significantly\nsuperior in (i) processing large and noisy web data and (ii) scaling to new and\nlow-resourced languages.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:37:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2102.10249", "submitter": "Benfeng Xu", "authors": "Benfeng Xu, Quan Wang, Yajuan Lyu, Yong Zhu, Zhendong Mao", "title": "Entity Structure Within and Throughout: Modeling Mention Dependencies\n  for Document-Level Relation Extraction", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entities, as the essential elements in relation extraction tasks, exhibit\ncertain structure. In this work, we formulate such structure as distinctive\ndependencies between mention pairs. We then propose SSAN, which incorporates\nthese structural dependencies within the standard self-attention mechanism and\nthroughout the overall encoding stage. Specifically, we design two alternative\ntransformation modules inside each self-attention building block to produce\nattentive biases so as to adaptively regularize its attention flow. Our\nexperiments demonstrate the usefulness of the proposed entity structure and the\neffectiveness of SSAN. It significantly outperforms competitive baselines,\nachieving new state-of-the-art results on three popular document-level relation\nextraction datasets. We further provide ablation and visualization to show how\nthe entity structure guides the model for better relation extraction. Our code\nis publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:47:46 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Xu", "Benfeng", ""], ["Wang", "Quan", ""], ["Lyu", "Yajuan", ""], ["Zhu", "Yong", ""], ["Mao", "Zhendong", ""]]}, {"id": "2102.10250", "submitter": "Thuy Vu", "authors": "Thuy Vu and Alessandro Moschitti", "title": "Multilingual Answer Sentence Reranking via Automatically Translated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study on the design of multilingual Answer Sentence Selection\n(AS2) models, which are a core component of modern Question Answering (QA)\nsystems. The main idea is to transfer data, created from one resource rich\nlanguage, e.g., English, to other languages, less rich in terms of resources.\nThe main findings of this paper are: (i) the training data for AS2 translated\ninto a target language can be used to effectively fine-tune a Transformer-based\nmodel for that language; (ii) one multilingual Transformer model it is enough\nto rank answers in multiple languages; and (iii) mixed-language question/answer\npairs can be used to fine-tune models to select answers from any language,\nwhere the input question is just in one language. This highly reduces the\ncomplexity and technical requirement of a multilingual QA system. Our\nexperiments validate the findings above, showing a modest drop, at most 3%,\nwith respect to the state-of-the-art English model.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:52:08 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2102.10275", "submitter": "Atharva Kulkarni", "authors": "Atharva Kulkarni, Amey Hengle, Rutuja Udyawar", "title": "An Attention Ensemble Approach for Efficient Text Classification of\n  Indian Languages", "comments": "Paper accepted and presented at the 17th International Conference on\n  Natural Language Processing (ICON 2020) TechDoFication Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent surge of complex attention-based deep learning architectures has\nled to extraordinary results in various downstream NLP tasks in the English\nlanguage. However, such research for resource-constrained and morphologically\nrich Indian vernacular languages has been relatively limited. This paper\nproffers team SPPU\\_AKAH's solution for the TechDOfication 2020 subtask-1f:\nwhich focuses on the coarse-grained technical domain identification of short\ntext documents in Marathi, a Devanagari script-based Indian language. Availing\nthe large dataset at hand, a hybrid CNN-BiLSTM attention ensemble model is\nproposed that competently combines the intermediate sentence representations\ngenerated by the convolutional neural network and the bidirectional long\nshort-term memory, leading to efficient text classification. Experimental\nresults show that the proposed model outperforms various baseline machine\nlearning and deep learning models in the given task, giving the best validation\naccuracy of 89.57\\% and f1-score of 0.8875. Furthermore, the solution resulted\nin the best system submission for this subtask, giving a test accuracy of\n64.26\\% and f1-score of 0.6157, transcending the performances of other teams as\nwell as the baseline system given by the organizers of the shared task.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 07:31:38 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kulkarni", "Atharva", ""], ["Hengle", "Amey", ""], ["Udyawar", "Rutuja", ""]]}, {"id": "2102.10287", "submitter": "Siwen Luo", "authors": "Siwen Luo, Mengting Wu, Yiwen Gong, Wanying Zhou, Josiah Poon", "title": "Deep Structured Feature Networks for Table Detection and Tabular Data\n  Extraction from Scanned Financial Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic table detection in PDF documents has achieved a great success but\ntabular data extraction are still challenging due to the integrity and noise\nissues in detected table areas. The accurate data extraction is extremely\ncrucial in finance area. Inspired by this, the aim of this research is\nproposing an automated table detection and tabular data extraction from\nfinancial PDF documents. We proposed a method that consists of three main\nprocesses, which are detecting table areas with a Faster R-CNN (Region-based\nConvolutional Neural Network) model with Feature Pyramid Network (FPN) on each\npage image, extracting contents and structures by a compounded layout\nsegmentation technique based on optical character recognition (OCR) and\nformulating regular expression rules for table header separation. The tabular\ndata extraction feature is embedded with rule-based filtering and restructuring\nfunctions that are highly scalable. We annotate a new Financial Documents\ndataset with table regions for the experiment. The excellent table detection\nperformance of the detection model is obtained from our customized dataset. The\nmain contributions of this paper are proposing the Financial Documents dataset\nwith table-area annotations, the superior detection model and the rule-based\nlayout segmentation technique for the tabular data extraction from PDF files.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 08:21:17 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Luo", "Siwen", ""], ["Wu", "Mengting", ""], ["Gong", "Yiwen", ""], ["Zhou", "Wanying", ""], ["Poon", "Josiah", ""]]}, {"id": "2102.10290", "submitter": "Luca Lugini", "authors": "Luca Lugini, Diane Litman", "title": "Contextual Argument Component Classification for Class Discussions", "comments": null, "journal-ref": "In Proceedings of the 28th International Conference on\n  Computational Linguistics, pp. 1475-1480. 2020", "doi": "10.18653/v1/2020.coling-main.128", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argument mining systems often consider contextual information, i.e.\ninformation outside of an argumentative discourse unit, when trained to\naccomplish tasks such as argument component identification, classification, and\nrelation extraction. However, prior work has not carefully analyzed the utility\nof different contextual properties in context-aware models. In this work, we\nshow how two different types of contextual information, local discourse context\nand speaker context, can be incorporated into a computational model for\nclassifying argument components in multi-party classroom discussions. We find\nthat both context types can improve performance, although the improvements are\ndependent on context size and position.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 08:48:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lugini", "Luca", ""], ["Litman", "Diane", ""]]}, {"id": "2102.10293", "submitter": "Luca Lugini", "authors": "Luca Lugini, Christopher Olshefski, Ravneet Singh, Diane Litman,\n  Amanda Godley", "title": "Discussion Tracker: Supporting Teacher Learning about Students'\n  Collaborative Argumentation in High School Classrooms", "comments": null, "journal-ref": "\"Discussion Tracker: Supporting Teacher Learning about Students'\n  Collaborative Argumentation in High School Classrooms.\" In Proceedings of the\n  28th International Conference on Computational Linguistics: System\n  Demonstrations, 2020", "doi": "10.18653/v1/2020.coling-demos.10", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching collaborative argumentation is an advanced skill that many K-12\nteachers struggle to develop. To address this, we have developed Discussion\nTracker, a classroom discussion analytics system based on novel algorithms for\nclassifying argument moves, specificity, and collaboration. Results from a\nclassroom deployment indicate that teachers found the analytics useful, and\nthat the underlying classifiers perform with moderate to substantial agreement\nwith humans.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 09:06:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lugini", "Luca", ""], ["Olshefski", "Christopher", ""], ["Singh", "Ravneet", ""], ["Litman", "Diane", ""], ["Godley", "Amanda", ""]]}, {"id": "2102.10407", "submitter": "Jun Chen", "authors": "Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny", "title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for\n  Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:02:42 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 18:03:11 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 07:14:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Jun", ""], ["Guo", "Han", ""], ["Yi", "Kai", ""], ["Li", "Boyang", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2102.10410", "submitter": "Johar Shabbir", "authors": "Johar Shabbir, Muhammad Umair Arshad, Waseem Shahzad", "title": "NUBOT: Embedded Knowledge Graph With RASA Framework for Generating\n  Semantic Intents Responses in Roman Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding of the human language is quantified by identifying intents\nand entities. Even though classification methods that rely on labeled\ninformation are often used for the comprehension of language understanding, it\nis incredibly time consuming and tedious process to generate high propensity\nsupervised datasets. In this paper, we present the generation of accurate\nintents for the corresponding Roman Urdu unstructured data and integrate this\ncorpus in RASA NLU module for intent classification. We embed knowledge graph\nwith RASA Framework to maintain the dialog history for semantic based natural\nlanguage mechanism for chatbot communication. We compare results of our work\nwith existing linguistic systems combined with semantic technologies. Minimum\naccuracy of intents generation is 64 percent of confidence and in the response\ngeneration part minimum accuracy is 82.1 percent and maximum accuracy gain is\n96.7 percent. All the scores refers to log precision, recall, and f1 measure\nfor each intents once summarized for all. Furthermore, it creates a confusion\nmatrix represents that which intents are ambiguously recognized by approach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:17:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Shabbir", "Johar", ""], ["Arshad", "Muhammad Umair", ""], ["Shahzad", "Waseem", ""]]}, {"id": "2102.10437", "submitter": "Marzieh Fadaee", "authors": "Marzieh Fadaee", "title": "Understanding and Enhancing the Use of Context for Machine Translation", "comments": "PhD dissertation defended on November 10th, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand and infer meaning in language, neural models have to learn\ncomplicated nuances. Discovering distinctive linguistic phenomena from data is\nnot an easy task. For instance, lexical ambiguity is a fundamental feature of\nlanguage which is challenging to learn. Even more prominently, inferring the\nmeaning of rare and unseen lexical units is difficult with neural networks.\nMeaning is often determined from context. With context, languages allow meaning\nto be conveyed even when the specific words used are not known by the reader.\nTo model this learning process, a system has to learn from a few instances in\ncontext and be able to generalize well to unseen cases. The learning process is\nhindered when training data is scarce for a task. Even with sufficient data,\nlearning patterns for the long tail of the lexical distribution is challenging.\nIn this thesis, we focus on understanding certain potentials of contexts in\nneural models and design augmentation models to benefit from them. We focus on\nmachine translation as an important instance of the more general language\nunderstanding problem. To translate from a source language to a target\nlanguage, a neural model has to understand the meaning of constituents in the\nprovided context and generate constituents with the same meanings in the target\nlanguage. This task accentuates the value of capturing nuances of language and\nthe necessity of generalization from few observations. The main problem we\nstudy in this thesis is what neural machine translation models learn from data\nand how we can devise more focused contexts to enhance this learning. Looking\nmore in-depth into the role of context and the impact of data on learning\nmodels is essential to advance the NLP field. Moreover, it helps highlight the\nvulnerabilities of current neural networks and provides insights into designing\nmore robust models.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:19:27 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Fadaee", "Marzieh", ""]]}, {"id": "2102.10535", "submitter": "Luis Perez", "authors": "Luis Perez, Lizi Ottens, Sudharshan Viswanathan", "title": "Automatic Code Generation using Pre-Trained Language Models", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in natural language processing \\cite{gpt2} \\cite{BERT}\nhave led to near-human performance in multiple natural language tasks. In this\npaper, we seek to understand whether similar techniques can be applied to a\nhighly structured environment with strict syntax rules. Specifically, we\npropose an end-to-end machine learning model for code generation in the Python\nlanguage built on-top of pre-trained language models. We demonstrate that a\nfine-tuned model can perform well in code generation tasks, achieving a BLEU\nscore of 0.22, an improvement of 46\\% over a reasonable sequence-to-sequence\nbaseline. All results and related code used for training and data processing\nare available on GitHub.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 07:21:26 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Perez", "Luis", ""], ["Ottens", "Lizi", ""], ["Viswanathan", "Sudharshan", ""]]}, {"id": "2102.10601", "submitter": "Muhammad Noor Fakhruzzaman", "authors": "Muhammad Noor Fakhruzzaman, Sie Wildan Gunawan", "title": "Web-based Application for Detecting Indonesian Clickbait Headlines using\n  IndoBERT", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With increasing usage of clickbaits in Indonesian Online News, newsworthy\narticles sometimes get buried among clickbaity news. A reliable and lightweight\ntool is needed to detect such clickbaits on-the-go. Leveraging state-of-the-art\nnatural language processing model BERT, a RESTful API based application is\ndeveloped. This study offloaded the computing resources needed to train the\nmodel on the cloud server, while the client-side application only needs to send\na request to the API and the cloud server will handle the rest. This study\nproposed the design and developed a web-based application to detect clickbait\nin Indonesian using IndoBERT as a language model. The application usage is\ndiscussed and available for public use with a performance of mean ROC-AUC of\n89%.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 13:28:52 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Fakhruzzaman", "Muhammad Noor", ""], ["Gunawan", "Sie Wildan", ""]]}, {"id": "2102.10684", "submitter": "Ahmed Abdelali", "authors": "Ahmed Abdelali, Sabit Hassan, Hamdy Mubarak, Kareem Darwish and Younes\n  Samih", "title": "Pre-Training BERT on Arabic Tweets: Practical Considerations", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pretraining Bidirectional Encoder Representations from Transformers (BERT)\nfor downstream NLP tasks is a non-trival task. We pretrained 5 BERT models that\ndiffer in the size of their training sets, mixture of formal and informal\nArabic, and linguistic preprocessing. All are intended to support Arabic\ndialects and social media. The experiments highlight the centrality of data\ndiversity and the efficacy of linguistically aware segmentation. They also\nhighlight that more data or more training step do not necessitate better\nmodels. Our new models achieve new state-of-the-art results on several\ndownstream tasks. The resulting models are released to the community under the\nname QARiB.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 20:51:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Abdelali", "Ahmed", ""], ["Hassan", "Sabit", ""], ["Mubarak", "Hamdy", ""], ["Darwish", "Kareem", ""], ["Samih", "Younes", ""]]}, {"id": "2102.10697", "submitter": "Martin Faj\\v{c}\\'ik", "authors": "Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz", "title": "Pruning the Index Contents for Memory Efficient Open-Domain QA", "comments": "v2 - added connection between pruner and DPR, results on TriviaQA,\n  new reranker, results with HN-DPR checkpoint and additional analyses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a novel pipeline that demonstrates what is achievable with\na combined effort of state-of-the-art approaches. Specifically, it proposes the\nnovel R2-D2 (Rank twice, reaD twice) pipeline composed of retriever, passage\nreranker, extractive reader, generative reader and a simple way to combine\nthem. Furthermore, previous work often comes with a massive index of external\ndocuments that scales in the order of tens of GiB. This work presents a simple\napproach for pruning the contents of a massive index such that the open-domain\nQA system altogether with index, OS, and library components fits into 6GiB\ndocker image while retaining only 8% of original index contents and losing only\n3% EM accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 21:56:38 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 19:02:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Fajcik", "Martin", ""], ["Docekal", "Martin", ""], ["Ondrej", "Karel", ""], ["Smrz", "Pavel", ""]]}, {"id": "2102.10760", "submitter": "Snehasish Mukherjee", "authors": "Snehasish Mukherjee", "title": "Unsupervised Meta Learning for One Shot Title Compression in Voice\n  Commerce", "comments": "Work carried out as part of CS330, Stanford University, Fall 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Product title compression for voice and mobile commerce is a well studied\nproblem with several supervised models proposed so far. However these models\nhave 2 major limitations; they are not designed to generate compressions\ndynamically based on cues at inference time, and they do not transfer well to\ndifferent categories at test time. To address these shortcomings we model title\ncompression as a meta learning problem where we ask can we learn a title\ncompression model given only 1 example compression? We adopt an unsupervised\napproach to meta training by proposing an automatic task generation algorithm\nthat models the observed label generation process as the outcome of 4\nunobserved processes. We create parameterized approximations to each of these 4\nlatent processes to get a principled way of generating random compression\nrules, which are treated as different tasks. For our main meta learner, we use\n2 models; M1 and M2. M1 is a task agnostic embedding generator whose output\nfeeds into M2 which is a task specific label generator. We pre-train M1 on a\nnovel unsupervised segment rank prediction task that allows us to treat M1 as a\nsegment generator that also learns to rank segments during the meta-training\nprocess. Our experiments on 16000 crowd generated meta-test examples show that\nour unsupervised meta training regime is able to acquire a learning algorithm\nfor different tasks after seeing only 1 example for each task. Further, we show\nthat our model trained end to end as a black box meta learner, outperforms non\nparametric approaches. Our best model obtains an F1 score of 0.8412, beating\nthe baseline by a large margin of 25 F1 points.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 03:53:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mukherjee", "Snehasish", ""]]}, {"id": "2102.10772", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Amanpreet Singh", "title": "UniT: Multimodal Multitask Learning with a Unified Transformer", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with 87.5% fewer\nparameters. Code will be released in MMF at https://mmf.sh.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 04:45:06 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 23:23:06 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hu", "Ronghang", ""], ["Singh", "Amanpreet", ""]]}, {"id": "2102.10780", "submitter": "Shaoxiong Feng", "authors": "Shaoxiong Feng, Xuancheng Ren, Kan Li, Xu Sun", "title": "Multi-View Feature Representation for Dialogue Generation with\n  Bidirectional Distillation", "comments": "Accepted by AAAI 2021; Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural dialogue models suffer from low-quality responses when interacted in\npractice, demonstrating difficulty in generalization beyond training data.\nRecently, knowledge distillation has been used to successfully regularize the\nstudent by transferring knowledge from the teacher. However, the teacher and\nthe student are trained on the same dataset and tend to learn similar feature\nrepresentations, whereas the most general knowledge should be found through\ndifferences. The finding of general knowledge is further hindered by the\nunidirectional distillation, as the student should obey the teacher and may\ndiscard some knowledge that is truly general but refuted by the teacher. To\nthis end, we propose a novel training framework, where the learning of general\nknowledge is more in line with the idea of reaching consensus, i.e., finding\ncommon knowledge that is beneficial to different yet all datasets through\ndiversified learning partners. Concretely, the training task is divided into a\ngroup of subtasks with the same number of students. Each student assigned to\none subtask not only is optimized on the allocated subtask but also imitates\nmulti-view feature representation aggregated from other students (i.e., student\npeers), which induces students to capture common knowledge among different\nsubtasks and alleviates the over-fitting of students on the allocated subtasks.\nTo further enhance generalization, we extend the unidirectional distillation to\nthe bidirectional distillation that encourages the student and its student\npeers to co-evolve by exchanging complementary knowledge with each other.\nEmpirical results and analysis demonstrate that our training framework\neffectively improves the model generalization without sacrificing training\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:23:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Feng", "Shaoxiong", ""], ["Ren", "Xuancheng", ""], ["Li", "Kan", ""], ["Sun", "Xu", ""]]}, {"id": "2102.10794", "submitter": "Kim Thi-Thanh Nguyen", "authors": "Kim Thi-Thanh Nguyen, Kiet Van Nguyen", "title": "ReINTEL Challenge 2020: Exploiting Transfer Learning Models for Reliable\n  Intelligence Identification on Vietnamese Social Network Sites", "comments": "4 pages, ReINTEL Task at VLSP 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the system that we propose for the Reliable Intelligence\nIndentification on Vietnamese Social Network Sites (ReINTEL) task of the\nVietnamese Language and Speech Processing 2020 (VLSP 2020) Shared Task. In this\ntask, the VLSP 2020 provides a dataset with approximately 6,000 trainning\nnews/posts annotated with reliable or unreliable labels, and a test set\nconsists of 2,000 examples without labels. In this paper, we conduct\nexperiments on different transfer learning models, which are bert4news and\nPhoBERT fine-tuned to predict whether the news is reliable or not. In our\nexperiments, we achieve the AUC score of 94.52% on the private test set from\nReINTEL's organizers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 06:17:33 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 12:26:35 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 03:08:43 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Nguyen", "Kim Thi-Thanh", ""], ["Van Nguyen", "Kiet", ""]]}, {"id": "2102.10826", "submitter": "Zhiyuan Ning", "authors": "Zhiyuan Ning, Ziyue Qiao, Hao Dong, Yi Du, Yuanchun Zhou", "title": "LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph\n  Embedding", "comments": "Accepted by PAKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding (KGE) models learn to project symbolic entities and\nrelations into a continuous vector space based on the observed triplets.\nHowever, existing KGE models cannot make a proper trade-off between the graph\ncontext and the model complexity, which makes them still far from satisfactory.\nIn this paper, we propose a lightweight framework named LightCAKE for\ncontext-aware KGE. LightCAKE explicitly models the graph context without\nintroducing redundant trainable parameters, and uses an iterative aggregation\nstrategy to integrate the context information into the entity/relation\nembeddings. As a generic framework, it can be used with many simple KGE models\nto achieve excellent results. Finally, extensive experiments on public\nbenchmarks demonstrate the efficiency and effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 08:23:22 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 02:30:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ning", "Zhiyuan", ""], ["Qiao", "Ziyue", ""], ["Dong", "Hao", ""], ["Du", "Yi", ""], ["Zhou", "Yuanchun", ""]]}, {"id": "2102.10848", "submitter": "Judit Acs", "authors": "Judit \\'Acs and D\\'aniel L\\'evai and D\\'avid M\\'ark Nemeskey and\n  Andr\\'as Kornai", "title": "Evaluating Contextualized Language Models for Hungarian", "comments": null, "journal-ref": "Hungarian NLP Conference (MSZNY2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extended comparison of contextualized language models for\nHungarian. We compare huBERT, a Hungarian model against 4 multilingual models\nincluding the multilingual BERT model. We evaluate these models through three\ntasks, morphological probing, POS tagging and NER. We find that huBERT works\nbetter than the other models, often by a large margin, particularly near the\nglobal optimum (typically at the middle layers). We also find that huBERT tends\nto generate fewer subwords for one word and that using the last subword for\ntoken-level tasks is generally a better choice than using the first one.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 09:29:01 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["\u00c1cs", "Judit", ""], ["L\u00e9vai", "D\u00e1niel", ""], ["Nemeskey", "D\u00e1vid M\u00e1rk", ""], ["Kornai", "Andr\u00e1s", ""]]}, {"id": "2102.10864", "submitter": "Judit Acs", "authors": "Judit \\'Acs and \\'Akos K\\'ad\\'ar and Andr\\'as Kornai", "title": "Subword Pooling Makes a Difference", "comments": null, "journal-ref": "EACL2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual word-representations became a standard in modern natural language\nprocessing systems. These models use subword tokenization to handle large\nvocabularies and unknown words. Word-level usage of such systems requires a way\nof pooling multiple subwords that correspond to a single word. In this paper we\ninvestigate how the choice of subword pooling affects the downstream\nperformance on three tasks: morphological probing, POS tagging and NER, in 9\ntypologically diverse languages. We compare these in two massively multilingual\nmodels, mBERT and XLM-RoBERTa. For morphological tasks, the widely used `choose\nthe first subword' is the worst strategy and the best results are obtained by\nusing attention over the subwords. For POS tagging both of these strategies\nperform poorly and the best choice is to use a small LSTM over the subwords.\nThe same strategy works best for NER and we show that mBERT is better than\nXLM-RoBERTa in all 9 languages. We publicly release all code, data and the full\nresult tables at \\url{https://github.com/juditacs/subword-choice}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 09:59:30 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:32:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["\u00c1cs", "Judit", ""], ["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Kornai", "Andr\u00e1s", ""]]}, {"id": "2102.10905", "submitter": "Jianzong Wang", "authors": "Yanfei Hui, Jianzong Wang, Ning Cheng, Fengying Yu, Tianbo Wu, Jing\n  Xiao", "title": "Joint Intent Detection And Slot Filling Based on Continual Learning\n  Model", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Slot filling and intent detection have become a significant theme in the\nfield of natural language understanding. Even though slot filling is\nintensively associated with intent detection, the characteristics of the\ninformation required for both tasks are different while most of those\napproaches may not fully aware of this problem. In addition, balancing the\naccuracy of two tasks effectively is an inevitable problem for the joint\nlearning model. In this paper, a Continual Learning Interrelated Model (CLIM)\nis proposed to consider semantic information with different characteristics and\nbalance the accuracy between intent detection and slot filling effectively. The\nexperimental results show that CLIM achieves state-of-the-art performace on\nslot filling and intent detection on ATIS and Snips.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 11:10:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Hui", "Yanfei", ""], ["Wang", "Jianzong", ""], ["Cheng", "Ning", ""], ["Yu", "Fengying", ""], ["Wu", "Tianbo", ""], ["Xiao", "Jing", ""]]}, {"id": "2102.10934", "submitter": "Tingyu Xia", "authors": "Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang", "title": "Using Prior Knowledge to Guide BERT's Attention in Semantic Textual\n  Matching Tasks", "comments": "10 pages, WWW'21, April19-23, 2021, Ljubljana, Slovenia", "journal-ref": null, "doi": "10.1145/3442381.3449988", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of incorporating prior knowledge into a deep\nTransformer-based model,i.e.,Bidirectional Encoder Representations from\nTransformers (BERT), to enhance its performance on semantic textual matching\ntasks. By probing and analyzing what BERT has already known when solving this\ntask, we obtain better understanding of what task-specific knowledge BERT needs\nthe most and where it is most needed. The analysis further motivates us to take\na different approach than most existing works. Instead of using prior knowledge\nto create a new training task for fine-tuning BERT, we directly inject\nknowledge into BERT's multi-head attention mechanism. This leads us to a simple\nyet effective approach that enjoys fast training stage as it saves the model\nfrom training on additional data or tasks other than the main task. Extensive\nexperiments demonstrate that the proposed knowledge-enhanced BERT is able to\nconsistently improve semantic textual matching performance over the original\nBERT model, and the performance benefit is most salient when training data is\nscarce.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:07:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Xia", "Tingyu", ""], ["Wang", "Yue", ""], ["Tian", "Yuan", ""], ["Chang", "Yi", ""]]}, {"id": "2102.10952", "submitter": "Rupsa Saha", "authors": "Rupsa Saha, Ole-Christoffer Granmo, Vladimir I. Zadorozhny, Morten\n  Goodwin", "title": "A Relational Tsetlin Machine with Applications to Natural Language\n  Understanding", "comments": "14 pages, 3 figures, 7 tables, relational approach to TM in NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TMs are a pattern recognition approach that uses finite state machines for\nlearning and propositional logic to represent patterns. In addition to being\nnatively interpretable, they have provided competitive accuracy for various\ntasks. In this paper, we increase the computing power of TMs by proposing a\nfirst-order logic-based framework with Herbrand semantics. The resulting TM is\nrelational and can take advantage of logical structures appearing in natural\nlanguage, to learn rules that represent how actions and consequences are\nrelated in the real world. The outcome is a logic program of Horn clauses,\nbringing in a structured view of unstructured data. In closed-domain\nquestion-answering, the first-order representation produces 10x more compact\nKBs, along with an increase in answering accuracy from 94.83% to 99.48%. The\napproach is further robust towards erroneous, missing, and superfluous\ninformation, distilling the aspects of a text that are important for real-world\nunderstanding.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:40:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Saha", "Rupsa", ""], ["Granmo", "Ole-Christoffer", ""], ["Zadorozhny", "Vladimir I.", ""], ["Goodwin", "Morten", ""]]}, {"id": "2102.10956", "submitter": "Usama Khalid", "authors": "Usama Khalid, Mirza Omer Beg", "title": "Few Shot Learning for Information Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information verification is quite a challenging task, this is because many\ntimes verifying a claim can require picking pieces of information from multiple\npieces of evidence which can have a hierarchy of complex semantic relations.\nPreviously a lot of researchers have mainly focused on simply concatenating\nmultiple evidence sentences to accept or reject claims. These approaches are\nlimited as evidence can contain hierarchical information and dependencies. In\nthis research, we aim to verify facts based on evidence selected from a list of\narticles taken from Wikipedia. Pretrained language models such as XLNET are\nused to generate meaningful representations and graph-based attention and\nconvolutions are used in such a way that the system requires little additional\ntraining to learn to verify facts.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:56:12 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Khalid", "Usama", ""], ["Beg", "Mirza Omer", ""]]}, {"id": "2102.10957", "submitter": "Usama Khalid", "authors": "Usama Khalid, Aizaz Hussain, Muhammad Umair Arshad, Waseem Shahzad and\n  Mirza Omer Beg", "title": "Co-occurrences using Fasttext embeddings for word similarity tasks in\n  Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urdu is a widely spoken language in South Asia. Though immoderate literature\nexists for the Urdu language still the data isn't enough to naturally process\nthe language by NLP techniques. Very efficient language models exist for the\nEnglish language, a high resource language, but Urdu and other under-resourced\nlanguages have been neglected for a long time. To create efficient language\nmodels for these languages we must have good word embedding models. For Urdu,\nwe can only find word embeddings trained and developed using the skip-gram\nmodel. In this paper, we have built a corpus for Urdu by scraping and\nintegrating data from various sources and compiled a vocabulary for the Urdu\nlanguage. We also modify fasttext embeddings and N-Grams models to enable\ntraining them on our built corpus. We have used these trained embeddings for a\nword similarity task and compared the results with existing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:56:26 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Khalid", "Usama", ""], ["Hussain", "Aizaz", ""], ["Arshad", "Muhammad Umair", ""], ["Shahzad", "Waseem", ""], ["Beg", "Mirza Omer", ""]]}, {"id": "2102.10958", "submitter": "Usama Khalid", "authors": "Usama Khalid, Mirza Omer Beg, Muhammad Umair Arshad", "title": "Bilingual Language Modeling, A transfer learning technique for Roman\n  Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained language models are now of widespread use in Natural Language\nProcessing. Despite their success, applying them to Low Resource languages is\nstill a huge challenge. Although Multilingual models hold great promise,\napplying them to specific low-resource languages e.g. Roman Urdu can be\nexcessive. In this paper, we show how the code-switching property of languages\nmay be used to perform cross-lingual transfer learning from a corresponding\nhigh resource language. We also show how this transfer learning technique\ntermed Bilingual Language Modeling can be used to produce better performing\nmodels for Roman Urdu. To enable training and experimentation, we also present\na collection of novel corpora for Roman Urdu extracted from various sources and\nsocial networking sites, e.g. Twitter. We train Monolingual, Multilingual, and\nBilingual models of Roman Urdu - the proposed bilingual model achieves 23%\naccuracy compared to the 2% and 11% of the monolingual and multilingual models\nrespectively in the Masked Language Modeling (MLM) task.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:56:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Khalid", "Usama", ""], ["Beg", "Mirza Omer", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2102.10962", "submitter": "David Graus", "authors": "David Graus", "title": "Entities of Interest", "comments": "Ph.D. thesis of David Graus. Published in 2017. ISBN:\n  978-94-6182-800-2. DOI: 11245.1/51be80bb-1cbf-4633-8ff9-e3128e990bfa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, we continuously - and at times unknowingly - leave\nbehind digital traces, by browsing, sharing, posting, liking, searching,\nwatching, and listening to online content. When aggregated, these digital\ntraces can provide powerful insights into the behavior, preferences,\nactivities, and traits of people. While many have raised privacy concerns\naround the use of aggregated digital traces, it has undisputedly brought us\nmany advances, from the search engines that learn from their users and enable\nour access to unforeseen amounts of data, knowledge, and information, to, e.g.,\nthe discovery of previously unknown adverse drug reactions from search engine\nlogs.\n  Whether in online services, journalism, digital forensics, law, or research,\nwe increasingly set out to exploring large amounts of digital traces to\ndiscover new information. Consider for instance, the Enron scandal, Hillary\nClinton's email controversy, or the Panama papers: cases that revolve around\nanalyzing, searching, investigating, exploring, and turning upside down large\namounts of digital traces to gain new insights, knowledge, and information.\nThis discovery task is at its core about \"finding evidence of activity in the\nreal world.\"\n  This dissertation revolves around discovery in digital traces, and sits at\nthe intersection of Information Retrieval, Natural Language Processing, and\napplied Machine Learning. We propose computational methods that aim to support\nthe exploration and sense-making process of large collections of digital\ntraces. We focus on textual traces, e.g., emails and social media streams, and\naddress two aspects that are central to discovery in digital traces.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:07:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Graus", "David", ""]]}, {"id": "2102.10966", "submitter": "Mohamad Yaser Jaradeh", "authors": "Mohamad Yaser Jaradeh, Kuldeep Singh, Markus Stocker, Andreas Both,\n  S\\\"oren Auer", "title": "Better Call the Plumber: Orchestrating Dynamic Information Extraction\n  Pipelines", "comments": "Accepted in ICWE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, a large number of Knowledge Graph (KG) information\nextraction approaches were proposed. Albeit effective, these efforts are\ndisjoint, and their collective strengths and weaknesses in effective KG\ninformation extraction (IE) have not been studied in the literature. We propose\nPlumber, the first framework that brings together the research community's\ndisjoint IE efforts. The Plumber architecture comprises 33 reusable components\nfor various KG information extraction subtasks, such as coreference resolution,\nentity linking, and relation extraction. Using these components,Plumber\ndynamically generates suitable information extraction pipelines and offers\noverall 264 distinct pipelines.We study the optimization problem of choosing\nsuitable pipelines based on input sentences. To do so, we train a\ntransformer-based classification model that extracts contextual embeddings from\nthe input and finds an appropriate pipeline. We study the efficacy of Plumber\nfor extracting the KG triples using standard datasets over two KGs: DBpedia,\nand Open Research Knowledge Graph (ORKG). Our results demonstrate the\neffectiveness of Plumber in dynamically generating KG information extraction\npipelines,outperforming all baselines agnostics of the underlying KG.\nFurthermore,we provide an analysis of collective failure cases, study the\nsimilarities and synergies among integrated components, and discuss their\nlimitations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:14:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jaradeh", "Mohamad Yaser", ""], ["Singh", "Kuldeep", ""], ["Stocker", "Markus", ""], ["Both", "Andreas", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2102.10979", "submitter": "Sebastian Vincent", "authors": "Sebastian T. Vincent", "title": "Towards Personalised and Document-level Machine Translation of Dialogue", "comments": "Thesis Proposal, 6 pages, 7 figures, accepted to the EACL2021 Student\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art (SOTA) neural machine translation (NMT) systems translate\ntexts at sentence level, ignoring context: intra-textual information, like the\nprevious sentence, and extra-textual information, like the gender of the\nspeaker. Because of that, some sentences are translated incorrectly.\nPersonalised NMT (PersNMT) and document-level NMT (DocNMT) incorporate this\ninformation into the translation process. Both fields are relatively new and\nprevious work within them is limited. Moreover, there are no readily available\nrobust evaluation metrics for them, which makes it difficult to develop better\nsystems, as well as track global progress and compare different methods. This\nthesis proposal focuses on PersNMT and DocNMT for the domain of dialogue\nextracted from TV subtitles in five languages: English, Brazilian Portuguese,\nGerman, French and Polish. Three main challenges are addressed: (1)\nincorporating extra-textual information directly into NMT systems; (2)\nimproving the machine translation of cohesion devices; (3) reliable evaluation\nfor PersNMT and DocNMT.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 09:18:20 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vincent", "Sebastian T.", ""]]}, {"id": "2102.10984", "submitter": "Bob Coecke", "authors": "Bob Coecke, Dominic Horsman, Aleks Kissinger, Quanlong Wang", "title": "Kindergarden quantum mechanics graduates (...or how I learned to stop\n  gluing LEGO together and love the ZX-calculus)", "comments": "30 pages, many pictures, including legos", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a `spiritual child' of the 2005 lecture notes Kindergarten\nQuantum Mechanics, which showed how a simple, pictorial extension of Dirac\nnotation allowed several quantum features to be easily expressed and derived,\nusing language even a kindergartner can understand. Central to that approach\nwas the use of pictures and pictorial transformation rules to understand and\nderive features of quantum theory and computation. However, this approach left\nmany wondering `where's the beef?' In other words, was this new approach\ncapable of producing new results, or was it simply an aesthetically pleasing\nway to restate stuff we already know?\n  The aim of this sequel paper is to say `here's the beef!', and highlight some\nof the major results of the approach advocated in Kindergarten Quantum\nMechanics, and how they are being applied to tackle practical problems on real\nquantum computers. We will focus mainly on what has become the Swiss army knife\nof the pictorial formalism: the ZX-calculus. First we look at some of the ideas\nbehind the ZX-calculus, comparing and contrasting it with the usual quantum\ncircuit formalism. We then survey results from the past 2 years falling into\nthree categories: (1) completeness of the rules of the ZX-calculus, (2)\nstate-of-the-art quantum circuit optimisation results in commercial and\nopen-source quantum compilers relying on ZX, and (3) the use of ZX in\ntranslating real-world stuff like natural language into quantum circuits that\ncan be run on today's (very limited) quantum hardware.\n  We also take the title literally, and outline an ongoing experiment aiming to\nshow that ZX-calculus enables children to do cutting-edge quantum computing\nstuff. If anything, this would truly confirm that `kindergarten quantum\nmechanics' wasn't just a joke.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:42:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Coecke", "Bob", ""], ["Horsman", "Dominic", ""], ["Kissinger", "Aleks", ""], ["Wang", "Quanlong", ""]]}, {"id": "2102.10992", "submitter": "Andr\\'es Chacoma", "authors": "A. Chacoma, D. H. Zanette", "title": "Word frequency-rank relationship in tagged texts", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2021.126020", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the frequency-rank relationship in sub-vocabularies corresponding\nto three different grammatical classes (nouns, verbs, and others) in a\ncollection of literary works in English, whose words have been automatically\ntagged according to their grammatical role. Comparing with a null hypothesis\nwhich assumes that words belonging to each class are uniformly distributed\nacross the frequency-ranked vocabulary of the whole work, we disclose\nstatistically significant differences between the three classes. This results\npoint to the fact that frequency-rank relationships may reflect linguistic\nfeatures associated with grammatical function.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 15:17:51 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:38:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chacoma", "A.", ""], ["Zanette", "D. H.", ""]]}, {"id": "2102.11000", "submitter": "Ikram Chairi", "authors": "ElMehdi Boujou, Hamza Chataoui, Abdellah El Mekki, Saad Benjelloun,\n  Ikram Chairi, and Ismail Berrada", "title": "An open access NLP dataset for Arabic dialects : Data collection,\n  labeling, and model construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) is today a very active field of research\nand innovation. Many applications need however big sets of data for supervised\nlearning, suitably labelled for the training purpose. This includes\napplications for the Arabic language and its national dialects. However, such\nopen access labeled data sets in Arabic and its dialects are lacking in the\nData Science ecosystem and this lack can be a burden to innovation and research\nin this field. In this work, we present an open data set of social data content\nin several Arabic dialects. This data was collected from the Twitter social\nnetwork and consists on +50K twits in five (5) national dialects. Furthermore,\nthis data was labeled for several applications, namely dialect detection, topic\ndetection and sentiment analysis. We publish this data as an open access data\nto encourage innovation and encourage other works in the field of NLP for\nArabic dialects and social media. A selection of models were built using this\ndata set and are presented in this paper along with their performances.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 01:39:52 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Boujou", "ElMehdi", ""], ["Chataoui", "Hamza", ""], ["Mekki", "Abdellah El", ""], ["Benjelloun", "Saad", ""], ["Chairi", "Ikram", ""], ["Berrada", "Ismail", ""]]}, {"id": "2102.11008", "submitter": "Sidi Lu", "authors": "Sidi Lu and Nanyun Peng", "title": "On Efficient Training, Controllability and Compositional Generalization\n  of Insertion-based Language Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-regressive language models with the left-to-right generation order have\nbeen a predominant paradigm for language generation. Recently, out-of-order\ntext generation beyond the traditional left-to-right paradigm has attracted\nextensive attention, with a notable variation of insertion-based generation,\nwhere a model is used to gradually extend the context into a complete sentence\npurely with insertion operations. However, since insertion operations disturb\nthe position information of each token, it is often believed that each step of\nthe insertion-based likelihood estimation requires a bi-directional\n\\textit{re-encoding} of the whole generated sequence. This computational\noverhead prohibits the model from scaling up to generate long, diverse texts\nsuch as stories, news articles, and reports. To address this issue, we propose\nInsNet, an insertion-based sequence model that can be trained as efficiently as\ntraditional transformer decoders while maintaining the same performance as that\nwith a bi-directional context encoder. We evaluate InsNet on story generation\nand CleVR-CoGENT captioning, showing the advantages of InsNet in several\ndimensions, including computational costs, generation quality, the ability to\nperfectly incorporate lexical controls, and better compositional\ngeneralization.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 11:05:02 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 06:12:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lu", "Sidi", ""], ["Peng", "Nanyun", ""]]}, {"id": "2102.11009", "submitter": "Justin Lane", "authors": "Justin E. Lane, Kevin McCaffree, F. LeRon Shults", "title": "The Moral Foundations of Left-Wing Authoritarianism: On the Character,\n  Cohesion, and Clout of Tribal Equalitarian Discourse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Left-wing authoritarianism remains far less understood than right-wing\nauthoritarianism. We contribute to the literature on the former, which\ntypically relies on surveys, using a new social media analytics approach. We\nuse a list of 60 terms to provide an exploratory sketch of the outlines of a\npolitical ideology (tribal equalitarianism) with origins in 19th and 20th\ncentury social philosophy. We then use analyses of the English Corpus of Google\nBooks (over 8 million books) and scraped unique tweets from Twitter (n =\n202,852) to conduct a series of investigations to discern the extent to which\nthis ideology is cohesive amongst the public, reveals signatures of\nauthoritarianism and has been growing in popularity. Though exploratory, our\nresults provide some evidence of left-wing authoritarianism in two forms (1) a\nuniquely conservative moral signature amongst ostensible liberals using\nmeasures from Moral Foundations Theory and (2) a substantial prevalence of\nanger, relative to anxiety or sadness. In general, results indicate that this\nworldview is growing in popularity, is increasingly cohesive, and shows\nsignatures of authoritarianism.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 14:06:25 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lane", "Justin E.", ""], ["McCaffree", "Kevin", ""], ["Shults", "F. LeRon", ""]]}, {"id": "2102.11012", "submitter": "Barry-John Theobald", "authors": "Andrew Silva, Barry-John Theobald, Nicholas Apostoloff", "title": "Multimodal Punctuation Prediction with Contextual Dropout", "comments": "Accepted for publication at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) is widely used in consumer electronics.\nASR greatly improves the utility and accessibility of technology, but usually\nthe output is only word sequences without punctuation. This can result in\nambiguity in inferring user-intent. We first present a transformer-based\napproach for punctuation prediction that achieves 8% improvement on the IWSLT\n2012 TED Task, beating the previous state of the art [1]. We next describe our\nmultimodal model that learns from both text and audio, which achieves 8%\nimprovement over the text-only algorithm on an internal dataset for which we\nhave both the audio and transcriptions. Finally, we present an approach to\nlearning a model using contextual dropout that allows us to handle variable\namounts of future context at test time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 22:15:30 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Silva", "Andrew", ""], ["Theobald", "Barry-John", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "2102.11031", "submitter": "Paul Barry", "authors": "Paul Barry, Sam Henry, Meliha Yetisgen, Bridget McInnes, Ozlem Uzuner", "title": "Jointly Learning Clinical Entities and Relations with Contextual\n  Language Models and Explicit Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We hypothesize that explicit integration of contextual information into an\nMulti-task Learning framework would emphasize the significance of context for\nboosting performance in jointly learning Named Entity Recognition (NER) and\nRelation Extraction (RE). Our work proves this hypothesis by segmenting\nentities from their surrounding context and by building contextual\nrepresentations using each independent segment. This relation representation\nallows for a joint NER/RE system that achieves near state-of-the-art (SOTA)\nperformance on both NER and RE tasks while beating the SOTA RE system at\nend-to-end NER & RE with a 49.07 F1.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:06:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Barry", "Paul", ""], ["Henry", "Sam", ""], ["Yetisgen", "Meliha", ""], ["McInnes", "Bridget", ""], ["Uzuner", "Ozlem", ""]]}, {"id": "2102.11032", "submitter": "Ozlem Uzuner", "authors": "Nicholas Dobbins, David Wayne, Kahyun Lee, \\\"Ozlem Uzuner, Meliha\n  Yetisgen", "title": "Performance of Automatic De-identification Across Different Note Types", "comments": null, "journal-ref": "AMIA Virtual Summits 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Free-text clinical notes detail all aspects of patient care and have great\npotential to facilitate quality improvement and assurance initiatives as well\nas advance clinical research. However, concerns about patient privacy and\nconfidentiality limit the use of clinical notes for research. As a result, the\ninformation documented in these notes remains unavailable for most researchers.\nDe-identification (de-id), i.e., locating and removing personally identifying\nprotected health information (PHI), is one way of improving access to clinical\nnarratives. However, there are limited off-the-shelf de-identification systems\nable to consistently detect PHI across different data sources and medical\nspecialties. In this abstract, we present the performance of a state-of-the art\nde-id system called NeuroNER1 on a diverse set of notes from University of\nWashington (UW) when the models are trained on data from an external\ninstitution (Partners Healthcare) vs. from the same institution (UW). We\npresent results at the level of PHI and note types.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:55:40 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dobbins", "Nicholas", ""], ["Wayne", "David", ""], ["Lee", "Kahyun", ""], ["Uzuner", "\u00d6zlem", ""], ["Yetisgen", "Meliha", ""]]}, {"id": "2102.11033", "submitter": "Dachuan Zhang", "authors": "Dachuan Zhang, Haoyang Zhang, Zhisheng Wei, Yan Li, Zhiheng Mao,\n  Chunmeng He, Haorui Ma, Xin Zeng, Xiaoling Xie, Xingran Kou and Bingwen Zhang", "title": "IFoodCloud: A Platform for Real-time Sentiment Analysis of Public\n  Opinion about Food Safety in China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet contains a wealth of public opinion on food safety, including\nviews on food adulteration, food-borne diseases, agricultural pollution,\nirregular food distribution, and food production issues. In order to\nsystematically collect and analyse public opinion on food safety, we developed\nIFoodCloud, a platform for the real-time sentiment analysis of public opinion\non food safety in China. It collects data from more than 3,100 public sources\nthat can be used to explore public opinion trends, public sentiment, and\nregional attention differences of food safety incidents. At the same time, we\nconstructed a sentiment classification model using multiple lexicon-based and\ndeep learning-based algorithms integrated with IFoodCloud that provide an\nunprecedented rapid means of understanding the public sentiment toward specific\nfood safety incidents. Our best model's F1-score achieved 0.9737. Further,\nthree real-world cases are presented to demonstrate the application and\nrobustness. IFoodCloud could be considered a valuable tool for promote\nscientisation of food safety supervision and risk communication.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 04:42:33 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhang", "Dachuan", ""], ["Zhang", "Haoyang", ""], ["Wei", "Zhisheng", ""], ["Li", "Yan", ""], ["Mao", "Zhiheng", ""], ["He", "Chunmeng", ""], ["Ma", "Haorui", ""], ["Zeng", "Xin", ""], ["Xie", "Xiaoling", ""], ["Kou", "Xingran", ""], ["Zhang", "Bingwen", ""]]}, {"id": "2102.11037", "submitter": "Elie Azeraf", "authors": "Elie Azeraf, Emmanuel Monfrini, Emmanuel Vignon, Wojciech Pieczynski", "title": "Highly Fast Text Segmentation With Pairwise Markov Chains", "comments": "9 pages, 5 figures, 4 tables, MNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Processing (NLP) models' current trend consists of using\nincreasingly more extra-data to build the best models as possible. It implies\nmore expensive computational costs and training time, difficulties for\ndeployment, and worries about these models' carbon footprint reveal a critical\nproblem in the future. Against this trend, our goal is to develop NLP models\nrequiring no extra-data and minimizing training time. To do so, in this paper,\nwe explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov\nChain (PMC), for NLP segmentation tasks. We apply these models for three\nclassic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We\ndevelop an original method to adapt these models for text segmentation's\nspecific challenges to obtain relevant performances with very short training\nand execution times. PMC achieves equivalent results to those obtained by\nConditional Random Fields (CRF), one of the most applied models for these tasks\nwhen no extra-data are used. Moreover, PMC has training times 30 times shorter\nthan the CRF ones, which validates this model given our objectives.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 20:08:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Azeraf", "Elie", ""], ["Monfrini", "Emmanuel", ""], ["Vignon", "Emmanuel", ""], ["Pieczynski", "Wojciech", ""]]}, {"id": "2102.11038", "submitter": "Elie Azeraf", "authors": "Elie Azeraf, Emmanuel Monfrini, Emmanuel Vignon, Wojciech Pieczynski", "title": "Introducing the Hidden Neural Markov Chain framework", "comments": "13 pages, 4 figures, 4 tables, ICAART 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, neural network models achieve state-of-the-art results in many\nareas as computer vision or speech processing. For sequential data, especially\nfor Natural Language Processing (NLP) tasks, Recurrent Neural Networks (RNNs)\nand their extensions, the Long Short Term Memory (LSTM) network and the Gated\nRecurrent Unit (GRU), are among the most used models, having a \"term-to-term\"\nsequence processing. However, if many works create extensions and improvements\nof the RNN, few have focused on developing other ways for sequential data\nprocessing with neural networks in a \"term-to-term\" way. This paper proposes\nthe original Hidden Neural Markov Chain (HNMC) framework, a new family of\nsequential neural models. They are not based on the RNN but on the Hidden\nMarkov Model (HMM), a probabilistic graphical model. This neural extension is\npossible thanks to the recent Entropic Forward-Backward algorithm for HMM\nrestoration. We propose three different models: the classic HNMC, the HNMC2,\nand the HNMC-CN. After describing our models' whole construction, we compare\nthem with classic RNN and Bidirectional RNN (BiRNN) models for some sequence\nlabeling tasks: Chunking, Part-Of-Speech Tagging, and Named Entity Recognition.\nFor every experiment, whatever the architecture or the embedding method used,\none of our proposed models has the best results. It shows this new neural\nsequential framework's potential, which can open the way to new models, and\nmight eventually compete with the prevalent BiLSTM and BiGRU.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 20:13:45 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Azeraf", "Elie", ""], ["Monfrini", "Emmanuel", ""], ["Vignon", "Emmanuel", ""], ["Pieczynski", "Wojciech", ""]]}, {"id": "2102.11043", "submitter": "Domenic Rosati", "authors": "Domenic Rosati", "title": "How are journals cited? characterizing journal citations by type of\n  citation", "comments": "2 Pages, 2 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evaluation of journals for quality is one of the dominant themes of\nbibliometrics since journals are the primary venue of vetting and distribution\nof scholarship. There are many criticisms of quantifying journal impact with\nbibliometrics including disciplinary differences among journals, what source\nmaterials are used, time windows for the inclusion of works to measure, and\nskewness of citation distributions (Lariviere & Sugimoto, 2019). However,\ndespite various attempts to remediate these in newly proposed indicators such\nas SJR, SNIP, and Eigenfactor (Walters, 2017) indicators still remain based on\ncitation counts and fail to acknowledge the critical differences that the type\nof citation made, whether it's supporting or disputing a work when quantifying\njournal impact. While various programs have been suggested to apply and\nencompass citation content analysis within bibliometrics projects, citation\ncontent analysis has not been done at the scale needed in order to supplement\nquantitate journal citation analysis until the scite citation index was\nproduced. Using this citation index containing citation types based on citation\nfunction (supporting, disputing, or mentioning) we present initial results on\nthe statistical characterization of citations to journals based on citation\nfunction. We also present initial results of characterizing the ratio of\nsupports and disputes received by a journal as a potential indicator of quality\nand show two interesting results: the ratio of supports and disputes do not\ncorrelate with total citations and that the distribution of this ratio is not\nskewed showing a normal distribution. We conclude with a proposal for future\nresearch using citation analysis qualified by citation function as well as the\nimplications of performing bibliometrics tasks such as research evaluation and\ninformation retrieval using citation function.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 14:15:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rosati", "Domenic", ""]]}, {"id": "2102.11047", "submitter": "Muhammad Hamzah Mushtaq", "authors": "Muhammad Hamzah Mushtaq", "title": "Semantic Parsing to Manipulate Relational Database For a Management\n  System", "comments": "5 pages. Figures, methodology and comparisons included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Chatbots and AI assistants have claimed their importance in today life. The\nmain reason behind adopting this technology is to connect with the user,\nunderstand their requirements, and fulfill them. This has been achieved but at\nthe cost of heavy training data and complex learning models. This work is\ncarried out proposes a simple algorithm, a model which can be implemented in\ndifferent fields each with its own work scope. The proposed model converts\nhuman language text to computer-understandable SQL queries. The model requires\ndata only related to the specific field, saving data space. This model performs\nlinear computation hence solving the computational complexity. This work also\ndefines the stages where a new methodology is implemented and what previous\nmethod was adopted to fulfill the requirement at that stage. Two datasets\navailable online will be used in this work, the ATIS dataset, and WikiSQL. This\nwork compares the computation time among the 2 datasets and also compares the\naccuracy of both. This paper works over basic Natural language processing tasks\nlike semantic parsing, NER, parts of speech and tends to achieve results\nthrough these simple methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:08:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mushtaq", "Muhammad Hamzah", ""]]}, {"id": "2102.11048", "submitter": "Xinwei Deng", "authors": "Qiao Liang, Shyam Ranganathan, Kaibo Wang and Xinwei Deng", "title": "JST-RR Model: Joint Modeling of Ratings and Reviews in Sentiment-Topic\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of online reviews has attracted great attention with broad\napplications. Often times, the textual reviews are coupled with the numerical\nratings in the data. In this work, we propose a probabilistic model to\naccommodate both textual reviews and overall ratings with consideration of\ntheir intrinsic connection for a joint sentiment-topic prediction. The key of\nthe proposed method is to develop a unified generative model where the topic\nmodeling is constructed based on review texts and the sentiment prediction is\nobtained by combining review texts and overall ratings. The inference of model\nparameters are obtained by an efficient Gibbs sampling procedure. The proposed\nmethod can enhance the prediction accuracy of review data and achieve an\neffective detection of interpretable topics and sentiments. The merits of the\nproposed method are elaborated by the case study from Amazon datasets and\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:47:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liang", "Qiao", ""], ["Ranganathan", "Shyam", ""], ["Wang", "Kaibo", ""], ["Deng", "Xinwei", ""]]}, {"id": "2102.11090", "submitter": "Philipp Dufter", "authors": "Philipp Dufter, Martin Schmitt, Hinrich Sch\\\"utze", "title": "Position Information in Transformers: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are arguably the main workhorse in recent Natural Language\nProcessing research. By definition a Transformer is invariant with respect to\nreorderings of the input. However, language is inherently sequential and word\norder is essential to the semantics and syntax of an utterance. In this paper,\nwe provide an overview of common methods to incorporate position information\ninto Transformer models. The objectives of this survey are to i) showcase that\nposition information in Transformer is a vibrant and extensive research area;\nii) enable the reader to compare existing methods by providing a unified\nnotation and meaningful clustering; iii) indicate what characteristics of an\napplication should be taken into account when selecting a position encoding;\niv) provide stimuli for future research.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:03:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dufter", "Philipp", ""], ["Schmitt", "Martin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2102.11103", "submitter": "Xiaolei Huang", "authors": "Xiaolei Huang, Michael J. Paul, Robin Burke, Franck Dernoncourt, Mark\n  Dredze", "title": "User Factor Adaptation for User Embedding via Multitask Learning", "comments": "Accepted in the Second Workshop on Domain Adaptation for Natural\n  Language Processing (Adapted-NLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language varies across users and their interested fields in social media\ndata: words authored by a user across his/her interests may have different\nmeanings (e.g., cool) or sentiments (e.g., fast). However, most of the existing\nmethods to train user embeddings ignore the variations across user interests,\nsuch as product and movie categories (e.g., drama vs. action). In this study,\nwe treat the user interest as domains and empirically examine how the user\nlanguage can vary across the user factor in three English social media\ndatasets. We then propose a user embedding model to account for the language\nvariability of user interests via a multitask learning framework. The model\nlearns user language and its variations without human supervision. While\nexisting work mainly evaluated the user embedding by extrinsic tasks, we\npropose an intrinsic evaluation via clustering and evaluate user embeddings by\nan extrinsic task, text classification. The experiments on the three\nEnglish-language social media datasets show that our proposed approach can\ngenerally outperform baselines via adapting the user factor.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:21:01 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Huang", "Xiaolei", ""], ["Paul", "Michael J.", ""], ["Burke", "Robin", ""], ["Dernoncourt", "Franck", ""], ["Dredze", "Mark", ""]]}, {"id": "2102.11105", "submitter": "Matthew Sumpter", "authors": "Matthew Sumpter and Giovanni Luca Ciampaglia", "title": "REMOD: Relation Extraction for Modeling Online Discourse", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The enormous amount of discourse taking place online poses challenges to the\nfunctioning of a civil and informed public sphere. Efforts to standardize\nonline discourse data, such as ClaimReview, are making available a wealth of\nnew data about potentially inaccurate claims, reviewed by third-party\nfact-checkers. These data could help shed light on the nature of online\ndiscourse, the role of political elites in amplifying it, and its implications\nfor the integrity of the online information ecosystem. Unfortunately, the\nsemi-structured nature of much of this data presents significant challenges\nwhen it comes to modeling and reasoning about online discourse. A key challenge\nis relation extraction, which is the task of determining the semantic\nrelationships between named entities in a claim. Here we develop a novel\nsupervised learning method for relation extraction that combines graph\nembedding techniques with path traversal on semantic dependency graphs. Our\napproach is based on the intuitive observation that knowledge of the entities\nalong the path between the subject and object of a triple (e.g.\nWashington,_D.C.}, and United_States_of_America) provides useful information\nthat can be leveraged for extracting its semantic relation (i.e. capitalOf). As\nan example of a potential application of this technique for modeling online\ndiscourse, we show that our method can be integrated into a pipeline to reason\nabout potential misinformation claims.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:26:36 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 16:05:49 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sumpter", "Matthew", ""], ["Ciampaglia", "Giovanni Luca", ""]]}, {"id": "2102.11114", "submitter": "Junwei Liao", "authors": "Junwei Liao, Yu Shi, Ming Gong, Linjun Shou, Sefik Eskimez, Liyang Lu,\n  Hong Qu, Michael Zeng", "title": "Generating Human Readable Transcript for Automatic Speech Recognition\n  with Pre-trained Language Model", "comments": "Accepted in 2021 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Automatic Speech Recognition (ASR) systems can achieve high\nperformance in terms of recognition accuracy. However, a perfectly accurate\ntranscript still can be challenging to read due to disfluency, filter words,\nand other errata common in spoken communication. Many downstream tasks and\nhuman readers rely on the output of the ASR system; therefore, errors\nintroduced by the speaker and ASR system alike will be propagated to the next\ntask in the pipeline. In this work, we propose an ASR post-processing model\nthat aims to transform the incorrect and noisy ASR output into a readable text\nfor humans and downstream tasks. We leverage the Metadata Extraction (MDE)\ncorpus to construct a task-specific dataset for our study. Since the dataset is\nsmall, we propose a novel data augmentation method and use a two-stage training\nstrategy to fine-tune the RoBERTa pre-trained model. On the constructed test\nset, our model outperforms a production two-step pipeline-based post-processing\nmethod by a large margin of 13.26 on readability-aware WER (RA-WER) and 17.53\non BLEU metrics. Human evaluation also demonstrates that our method can\ngenerate more human-readable transcripts than the baseline method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:45:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liao", "Junwei", ""], ["Shi", "Yu", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Eskimez", "Sefik", ""], ["Lu", "Liyang", ""], ["Qu", "Hong", ""], ["Zeng", "Michael", ""]]}, {"id": "2102.11115", "submitter": "Adam Dahlgren Lindstr\\\"om", "authors": "Adam Dahlgren Lindstr\\\"om, Suna Bensch, Johanna Bj\\\"orklund, Frank\n  Drewes", "title": "Probing Multimodal Embeddings for Linguistic Properties: the\n  Visual-Semantic Case", "comments": "Submitted July 1 2020, COLING 2020 main conference", "journal-ref": null, "doi": "10.18653/v1/2020.coling-main.64", "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic embeddings have advanced the state of the art for countless natural\nlanguage processing tasks, and various extensions to multimodal domains, such\nas visual-semantic embeddings, have been proposed. While the power of\nvisual-semantic embeddings comes from the distillation and enrichment of\ninformation through machine learning, their inner workings are poorly\nunderstood and there is a shortage of analysis tools. To address this problem,\nwe generalize the notion of probing tasks to the visual-semantic case. To this\nend, we (i) discuss the formalization of probing tasks for embeddings of\nimage-caption pairs, (ii) define three concrete probing tasks within our\ngeneral framework, (iii) train classifiers to probe for those properties, and\n(iv) compare various state-of-the-art embeddings under the lens of the proposed\nprobing tasks. Our experiments reveal an up to 12% increase in accuracy on\nvisual-semantic embeddings compared to the corresponding unimodal embeddings,\nwhich suggest that the text and image dimensions represented in the former do\ncomplement each other.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:47:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lindstr\u00f6m", "Adam Dahlgren", ""], ["Bensch", "Suna", ""], ["Bj\u00f6rklund", "Johanna", ""], ["Drewes", "Frank", ""]]}, {"id": "2102.11146", "submitter": "Rui Ribeiro", "authors": "Rui Ribeiro, Alberto Abad and Jos\\'e Lopes", "title": "Domain Adaptation in Dialogue Systems using Transfer and Meta-Learning", "comments": "5 pages, 2 figures, accepted at IberSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current generative-based dialogue systems are data-hungry and fail to adapt\nto new unseen domains when only a small amount of target data is available.\nAdditionally, in real-world applications, most domains are underrepresented, so\nthere is a need to create a system capable of generalizing to these domains\nusing minimal data. In this paper, we propose a method that adapts to unseen\ndomains by combining both transfer and meta-learning (DATML). DATML improves\nthe previous state-of-the-art dialogue model, DiKTNet, by introducing a\ndifferent learning technique: meta-learning. We use Reptile, a first-order\noptimization-based meta-learning algorithm as our improved training method. We\nevaluated our model on the MultiWOZ dataset and outperformed DiKTNet in both\nBLEU and Entity F1 scores when the same amount of data is available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:16:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ribeiro", "Rui", ""], ["Abad", "Alberto", ""], ["Lopes", "Jos\u00e9", ""]]}, {"id": "2102.11152", "submitter": "Rob van der Goot", "authors": "Anouck Braggaar, Rob van der Goot", "title": "Creating a Universal Dependencies Treebank of Spoken Frisian-Dutch\n  Code-switched Data", "comments": "RESOURCEFUL-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the difficulties of annotating transcribed spoken\nDutch-Frisian code-switch utterances into Universal Dependencies. We make use\nof data from the FAME! corpus, which consists of transcriptions and audio data.\nBesides the usual annotation difficulties, this dataset is extra challenging\nbecause of Frisian being low-resource, the informal nature of the data,\ncode-switching and non-standard sentence segmentation. As a starting point, two\nannotators annotated 150 random utterances in three stages of 50 utterances.\nAfter each stage, disagreements where discussed and resolved. An increase of\n7.8 UAS and 10.5 LAS points was achieved between the first and third round.\nThis paper will focus on the issues that arise when annotating a transcribed\nspeech corpus. To resolve these issues several solutions are proposed.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:22:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Braggaar", "Anouck", ""], ["van der Goot", "Rob", ""]]}, {"id": "2102.11258", "submitter": "Sandeep Mathias", "authors": "Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, and Pushpak\n  Bhattacharyya", "title": "Cognitively Aided Zero-Shot Automatic Essay Grading", "comments": "This paper was accepted for publication at ICON 2020: The 17th\n  International Conference on Natural Language Processing, on December 20, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic essay grading (AEG) is a process in which machines assign a grade\nto an essay written in response to a topic, called the prompt. Zero-shot AEG is\nwhen we train a system to grade essays written to a new prompt which was not\npresent in our training data. In this paper, we describe a solution to the\nproblem of zero-shot automatic essay grading, using cognitive information, in\nthe form of gaze behaviour. Our experiments show that using gaze behaviour\nhelps in improving the performance of AEG systems, especially when we provide a\nnew essay written in response to a new prompt for scoring, by an average of\nalmost 5 percentage points of QWK.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:41:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mathias", "Sandeep", ""], ["Murthy", "Rudra", ""], ["Kanojia", "Diptesh", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2102.11265", "submitter": "Nikolaos Flemotomos", "authors": "Nikolaos Flemotomos, Victor R. Martinez, Zhuohao Chen, Karan Singla,\n  Victor Ardulov, Raghuveer Peri, Derek D. Caperton, James Gibson, Michael J.\n  Tanana, Panayiotis Georgiou, Jake Van Epps, Sarah P. Lord, Tad Hirsch, Zac E.\n  Imel, David C. Atkins, Shrikanth Narayanan", "title": "Automated Evaluation Of Psychotherapy Skills Using Speech And Language\n  Technologies", "comments": "new version has an updated title", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing prevalence of psychological interventions, it is vital to\nhave measures which rate the effectiveness of psychological care to assist in\ntraining, supervision, and quality assurance of services. Traditionally,\nquality assessment is addressed by human raters who evaluate recorded sessions\nalong specific dimensions, often codified through constructs relevant to the\napproach and domain. This is however a cost-prohibitive and time-consuming\nmethod that leads to poor feasibility and limited use in real-world settings.\nTo facilitate this process, we have developed an automated competency rating\ntool able to process the raw recorded audio of a session, analyzing who spoke\nwhen, what they said, and how the health professional used language to provide\ntherapy. Focusing on a use case of a specific type of psychotherapy called\nMotivational Interviewing, our system gives comprehensive feedback to the\ntherapist, including information about the dynamics of the session (e.g.,\ntherapist's vs. client's talking time), low-level psychological language\ndescriptors (e.g., type of questions asked), as well as other high-level\nbehavioral constructs (e.g., the extent to which the therapist understands the\nclients' perspective). We describe our platform and its performance using a\ndataset of more than 5,000 recordings drawn from its deployment in a real-world\nclinical setting used to assist training of new therapists. Widespread use of\nautomated psychotherapy rating tools may augment experts' capabilities by\nproviding an avenue for more effective training and skill improvement,\neventually leading to more positive clinical outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:52:52 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 22:24:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Flemotomos", "Nikolaos", ""], ["Martinez", "Victor R.", ""], ["Chen", "Zhuohao", ""], ["Singla", "Karan", ""], ["Ardulov", "Victor", ""], ["Peri", "Raghuveer", ""], ["Caperton", "Derek D.", ""], ["Gibson", "James", ""], ["Tanana", "Michael J.", ""], ["Georgiou", "Panayiotis", ""], ["Van Epps", "Jake", ""], ["Lord", "Sarah P.", ""], ["Hirsch", "Tad", ""], ["Imel", "Zac E.", ""], ["Atkins", "David C.", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2102.11276", "submitter": "Shivangi Singhal Ms", "authors": "Shivangi Singhal, Rajiv Ratn Shah, Ponnurangam Kumaraguru", "title": "Factorization of Fact-Checks for Low Resource Indian Languages", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advancement in technology and accessibility of internet to each\nindividual is revolutionizing the real time information. The liberty to express\nyour thoughts without passing through any credibility check is leading to\ndissemination of fake content in the ecosystem. It can have disastrous effects\non both individuals and society as a whole. The amplification of fake news is\nbecoming rampant in India too. Debunked information often gets republished with\na replacement description, claiming it to depict some different incidence. To\ncurb such fabricated stories, it is necessary to investigate such deduplicates\nand false claims made in public. The majority of studies on automatic\nfact-checking and fake news detection is restricted to English only. But for a\ncountry like India where only 10% of the literate population speak English,\nrole of regional languages in spreading falsity cannot be undermined. In this\npaper, we introduce FactDRIL: the first large scale multilingual Fact-checking\nDataset for Regional Indian Languages. We collect an exhaustive dataset across\n7 months covering 11 low-resource languages. Our propose dataset consists of\n9,058 samples belonging to English, 5,155 samples to Hindi and remaining 8,222\nsamples are distributed across various regional languages, i.e. Bangla,\nMarathi, Malayalam, Telugu, Tamil, Oriya, Assamese, Punjabi, Urdu, Sinhala and\nBurmese. We also present the detailed characterization of three M's\n(multi-lingual, multi-media, multi-domain) in the FactDRIL accompanied with the\ncomplete list of other varied attributes making it a unique dataset to study.\nLastly, we present some potential use cases of the dataset. We expect this\ndataset will be a valuable resource and serve as a starting point to fight\nproliferation of fake news in low resource languages.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 16:47:41 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Singhal", "Shivangi", ""], ["Shah", "Rajiv Ratn", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2102.11278", "submitter": "Usama Khalid", "authors": "Usama Khalid, Mirza Omer Beg, Muhammad Umair Arshad", "title": "RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer\n  Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.10958", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent studies, it has been shown that Multilingual language models\nunderperform their monolingual counterparts. It is also a well-known fact that\ntraining and maintaining monolingual models for each language is a costly and\ntime-consuming process. Roman Urdu is a resource-starved language used\npopularly on social media platforms and chat apps. In this research, we propose\na novel dataset of scraped tweets containing 54M tokens and 3M sentences.\nAdditionally, we also propose RUBERT a bilingual Roman Urdu model created by\nadditional pretraining of English BERT. We compare its performance with a\nmonolingual Roman Urdu BERT trained from scratch and a multilingual Roman Urdu\nBERT created by additional pretraining of Multilingual BERT. We show through\nour experiments that additional pretraining of the English BERT produces the\nmost notable performance improvement.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:56:49 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Khalid", "Usama", ""], ["Beg", "Mirza Omer", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2102.11318", "submitter": "Falguni Laljibhai Patel", "authors": "Falguni Patel, NirmalKumar Patel, Santosh Kumar Bharti", "title": "Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications\n  using Emotional Intelligence", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Veracity is an essential key in research and development of innovative\nproducts. Live Emotion analysis and verification nullify deceit made to\ncomplainers on live chat, corroborate messages of both ends in messaging apps\nand promote an honest conversation between users. The main concept behind this\nemotion artificial intelligent verifier is to license or decline message\naccountability by comparing variegated emotions of chat app users recognized\nthrough facial expressions and text prediction. In this paper, a proposed\nemotion intelligent live detector acts as an honest arbiter who distributes\nfacial emotions into labels namely, Happiness, Sadness, Surprise, and Hate.\nFurther, it separately predicts a label of messages through text\nclassification. Finally, it compares both labels and declares the message as a\nfraud or a bonafide. For emotion detection, we deployed Convolutional Neural\nNetwork (CNN) using a miniXception model and for text prediction, we selected\nSupport Vector Machine (SVM) natural language processing probability classifier\ndue to receiving the best accuracy on training dataset after applying Support\nVector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and\nLogistic regression.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 02:47:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Patel", "Falguni", ""], ["Patel", "NirmalKumar", ""], ["Bharti", "Santosh Kumar", ""]]}, {"id": "2102.11387", "submitter": "Julia Ive", "authors": "Julia Ive, Andy Mingren Li, Yishu Miao, Ozan Caglayan, Pranava\n  Madhyastha, Lucia Specia", "title": "Exploiting Multimodal Reinforcement Learning for Simultaneous Machine\n  Translation", "comments": "Long paper accepted to EACL 2021, Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of simultaneous machine translation (SiMT)\nby exploring two main concepts: (a) adaptive policies to learn a good trade-off\nbetween high translation quality and low latency; and (b) visual information to\nsupport this process by providing additional (visual) contextual information\nwhich may be available before the textual input is produced. For that, we\npropose a multimodal approach to simultaneous machine translation using\nreinforcement learning, with strategies to integrate visual and textual\ninformation in both the agent and the environment. We provide an exploration on\nhow different types of visual information and integration strategies affect the\nquality and latency of simultaneous translation models, and demonstrate that\nvisual cues lead to higher quality while keeping the latency low.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 22:26:22 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ive", "Julia", ""], ["Li", "Andy Mingren", ""], ["Miao", "Yishu", ""], ["Caglayan", "Ozan", ""], ["Madhyastha", "Pranava", ""], ["Specia", "Lucia", ""]]}, {"id": "2102.11402", "submitter": "Wancong Zhang", "authors": "Wancong Zhang, Ieshan Vaidya", "title": "MixUp Training Leads to Reduced Overfitting and Improved Calibration for\n  the Transformer Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MixUp is a computer vision data augmentation technique that uses convex\ninterpolations of input data and their labels to enhance model generalization\nduring training. However, the application of MixUp to the natural language\nunderstanding (NLU) domain has been limited, due to the difficulty of\ninterpolating text directly in the input space. In this study, we propose MixUp\nmethods at the Input, Manifold, and sentence embedding levels for the\ntransformer architecture, and apply them to finetune the BERT model for a\ndiverse set of NLU tasks. We find that MixUp can improve model performance, as\nwell as reduce test loss and model calibration error by up to 50%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 23:12:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhang", "Wancong", ""], ["Vaidya", "Ieshan", ""]]}, {"id": "2102.11403", "submitter": "Julia Ive", "authors": "Julia Ive, Zixu Wang, Marina Fomicheva, Lucia Specia", "title": "Exploring Supervised and Unsupervised Rewards in Machine Translation", "comments": "Long paper accepted to EACL 2021, Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a powerful framework to address the\ndiscrepancy between loss functions used during training and the final\nevaluation metrics to be used at test time. When applied to neural Machine\nTranslation (MT), it minimises the mismatch between the cross-entropy loss and\nnon-differentiable evaluation metrics like BLEU. However, the suitability of\nthese metrics as reward function at training time is questionable: they tend to\nbe sparse and biased towards the specific words used in the reference texts. We\npropose to address this problem by making models less reliant on such metrics\nin two ways: (a) with an entropy-regularised RL method that does not only\nmaximise a reward function but also explore the action space to avoid peaky\ndistributions; (b) with a novel RL method that explores a dynamic unsupervised\nreward function to balance between exploration and exploitation. We base our\nproposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy\nmaximum entropy model for language generation applications such as MT. We\ndemonstrate that SAC with BLEU reward tends to overfit less to the training\ndata and performs better on out-of-domain data. We also show that our dynamic\nunsupervised reward can lead to better translation of ambiguous words.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 23:18:25 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ive", "Julia", ""], ["Wang", "Zixu", ""], ["Fomicheva", "Marina", ""], ["Specia", "Lucia", ""]]}, {"id": "2102.11479", "submitter": "Xinyang Zhang", "authors": "Xinyang Zhang, Chenwei Zhang, Luna Xin Dong, Jingbo Shang, Jiawei Han", "title": "Minimally-Supervised Structure-Rich Text Categorization via Learning on\n  Text-Rich Networks", "comments": "WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text categorization is an essential task in Web content analysis. Considering\nthe ever-evolving Web data and new emerging categories, instead of the\nlaborious supervised setting, in this paper, we focus on the\nminimally-supervised setting that aims to categorize documents effectively,\nwith a couple of seed documents annotated per category. We recognize that texts\ncollected from the Web are often structure-rich, i.e., accompanied by various\nmetadata. One can easily organize the corpus into a text-rich network, joining\nraw text documents with document attributes, high-quality phrases, label\nsurface names as nodes, and their associations as edges. Such a network\nprovides a holistic view of the corpus' heterogeneous data sources and enables\na joint optimization for network-based analysis and deep textual model\ntraining. We therefore propose a novel framework for minimally supervised\ncategorization by learning from the text-rich network. Specifically, we jointly\ntrain two modules with different inductive biases -- a text analysis module for\ntext understanding and a network learning module for class-discriminative,\nscalable network learning. Each module generates pseudo training labels from\nthe unlabeled document set, and both modules mutually enhance each other by\nco-training using pooled pseudo labels. We test our model on two real-world\ndatasets. On the challenging e-commerce product categorization dataset with 683\ncategories, our experiments show that given only three seed documents per\ncategory, our framework can achieve an accuracy of about 92%, significantly\noutperforming all compared methods; our accuracy is only less than 2% away from\nthe supervised BERT model trained on about 50K labeled documents.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 04:14:34 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhang", "Xinyang", ""], ["Zhang", "Chenwei", ""], ["Dong", "Luna Xin", ""], ["Shang", "Jingbo", ""], ["Han", "Jiawei", ""]]}, {"id": "2102.11480", "submitter": "Mario Campos Soberanis", "authors": "Rafael Viana-C\\'amara, Diego Campos-Sobrino, Mario Campos-Soberanis", "title": "Evolutionary optimization of contexts for phonetic correction in speech\n  recognition systems", "comments": "13 pages, 4 figures, This article is a translation of the paper\n  \"Optimizaci\\'on evolutiva de contextos para la correcci\\'on fon\\'etica en\n  sistemas de reconocimiento del habla\" presented in COMIA 2019", "journal-ref": "Research in Computing Science Issue 148(8), 2019, pp. 293-306.\n  ISSN 1870-4069", "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic Speech Recognition (ASR) is an area of growing academic and\ncommercial interest due to the high demand for applications that use it to\nprovide a natural communication method. It is common for general purpose ASR\nsystems to fail in applications that use a domain-specific language. Various\nstrategies have been used to reduce the error, such as providing a context that\nmodifies the language model and post-processing correction methods. This\narticle explores the use of an evolutionary process to generate an optimized\ncontext for a specific application domain, as well as different correction\ntechniques based on phonetic distance metrics. The results show the viability\nof a genetic algorithm as a tool for context optimization, which, added to a\npost-processing correction based on phonetic representations, can reduce the\nerrors on the recognized speech.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 04:14:51 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Viana-C\u00e1mara", "Rafael", ""], ["Campos-Sobrino", "Diego", ""], ["Campos-Soberanis", "Mario", ""]]}, {"id": "2102.11531", "submitter": "Ganesh Venkatesh", "authors": "Ganesh Venkatesh, Alagappan Valliappan, Jay Mahadeokar, Yuan\n  Shangguan, Christian Fuegen, Michael L. Seltzer, Vikas Chandra", "title": "Memory-efficient Speech Recognition on Smart Devices", "comments": null, "journal-ref": "ICASSP 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent transducer models have emerged as a promising solution for speech\nrecognition on the current and next generation smart devices. The transducer\nmodels provide competitive accuracy within a reasonable memory footprint\nalleviating the memory capacity constraints in these devices. However, these\nmodels access parameters from off-chip memory for every input time step which\nadversely effects device battery life and limits their usability on low-power\ndevices.\n  We address transducer model's memory access concerns by optimizing their\nmodel architecture and designing novel recurrent cell designs. We demonstrate\nthat i) model's energy cost is dominated by accessing model weights from\noff-chip memory, ii) transducer model architecture is pivotal in determining\nthe number of accesses to off-chip memory and just model size is not a good\nproxy, iii) our transducer model optimizations and novel recurrent cell reduces\noff-chip memory accesses by 4.5x and model size by 2x with minimal accuracy\nimpact.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:43:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Venkatesh", "Ganesh", ""], ["Valliappan", "Alagappan", ""], ["Mahadeokar", "Jay", ""], ["Shangguan", "Yuan", ""], ["Fuegen", "Christian", ""], ["Seltzer", "Michael L.", ""], ["Chandra", "Vikas", ""]]}, {"id": "2102.11570", "submitter": "Jasmin Bogatinovski", "authors": "Harold Ott, Jasmin Bogatinovski, Alexander Acker, Sasho Nedelkoski,\n  Odej Kao", "title": "Robust and Transferable Anomaly Detection in Log Data using Pre-Trained\n  Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Anomalies or failures in large computer systems, such as the cloud, have an\nimpact on a large number of users that communicate, compute, and store\ninformation. Therefore, timely and accurate anomaly detection is necessary for\nreliability, security, safe operation, and mitigation of losses in these\nincreasingly important systems. Recently, the evolution of the software\nindustry opens up several problems that need to be tackled including (1)\naddressing the software evolution due software upgrades, and (2) solving the\ncold-start problem, where data from the system of interest is not available. In\nthis paper, we propose a framework for anomaly detection in log data, as a\nmajor troubleshooting source of system information. To that end, we utilize\npre-trained general-purpose language models to preserve the semantics of log\nmessages and map them into log vector embeddings. The key idea is that these\nrepresentations for the logs are robust and less invariant to changes in the\nlogs, and therefore, result in a better generalization of the anomaly detection\nmodels. We perform several experiments on a cloud dataset evaluating different\nlanguage models for obtaining numerical log representations such as BERT,\nGPT-2, and XL. The robustness is evaluated by gradually altering log messages,\nto simulate a change in semantics. Our results show that the proposed approach\nachieves high performance and robustness, which opens up possibilities for\nfuture research in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:17:05 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ott", "Harold", ""], ["Bogatinovski", "Jasmin", ""], ["Acker", "Alexander", ""], ["Nedelkoski", "Sasho", ""], ["Kao", "Odej", ""]]}, {"id": "2102.11573", "submitter": "Nikolaos Flemotomos", "authors": "Nikolaos Flemotomos, Victor R. Martinez, Zhuohao Chen, Torrey A.\n  Creed, David C. Atkins, Shrikanth Narayanan", "title": "Automated Quality Assessment of Cognitive Behavioral Therapy Sessions\n  Through Highly Contextualized Language Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a psychotherapy session, the counselor typically adopts techniques\nwhich are codified along specific dimensions (e.g., 'displays warmth and\nconfidence', or 'attempts to set up collaboration') to facilitate the\nevaluation of the session. Those constructs, traditionally scored by trained\nhuman raters, reflect the complex nature of psychotherapy and highly depend on\nthe context of the interaction. Recent advances in deep contextualized language\nmodels offer an avenue for accurate in-domain linguistic representations which\ncan lead to robust recognition and scoring of such psychotherapy-relevant\nbehavioral constructs, and support quality assurance and supervision. In this\nwork, a BERT-based model is proposed for automatic behavioral scoring of a\nspecific type of psychotherapy, called Cognitive Behavioral Therapy (CBT),\nwhere prior work is limited to frequency-based language features and/or short\ntext excerpts which do not capture the unique elements involved in a\nspontaneous long conversational interaction. The model is trained in a\nmulti-task manner in order to achieve higher interpretability. BERT-based\nrepresentations are further augmented with available therapy metadata,\nproviding relevant non-linguistic context and leading to consistent performance\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:22:29 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Flemotomos", "Nikolaos", ""], ["Martinez", "Victor R.", ""], ["Chen", "Zhuohao", ""], ["Creed", "Torrey A.", ""], ["Atkins", "David C.", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2102.11584", "submitter": "Xiangyu Liu", "authors": "Jinfeng Li, Tianyu Du, Xiangyu Liu, Rong Zhang, Hui Xue, Shouling Ji", "title": "Enhancing Model Robustness By Incorporating Adversarial Knowledge Into\n  Semantic Representation", "comments": "Accepted to ICASSP 2021. 5 pages, 3 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite that deep neural networks (DNNs) have achieved enormous success in\nmany domains like natural language processing (NLP), they have also been proven\nto be vulnerable to maliciously generated adversarial examples. Such inherent\nvulnerability has threatened various real-world deployed DNNs-based\napplications. To strength the model robustness, several countermeasures have\nbeen proposed in the English NLP domain and obtained satisfactory performance.\nHowever, due to the unique language properties of Chinese, it is not trivial to\nextend existing defenses to the Chinese domain. Therefore, we propose AdvGraph,\na novel defense which enhances the robustness of Chinese-based NLP models by\nincorporating adversarial knowledge into the semantic representation of the\ninput. Extensive experiments on two real-world tasks show that AdvGraph\nexhibits better performance compared with previous work: (i) effective - it\nsignificantly strengthens the model robustness even under the adaptive attacks\nsetting without negative impact on model performance over legitimate input;\n(ii) generic - its key component, i.e., the representation of connotative\nadversarial knowledge is task-agnostic, which can be reused in any\nChinese-based NLP models without retraining; and (iii) efficient - it is a\nlight-weight defense with sub-linear computational complexity, which can\nguarantee the efficiency required in practical scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:47:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Li", "Jinfeng", ""], ["Du", "Tianyu", ""], ["Liu", "Xiangyu", ""], ["Zhang", "Rong", ""], ["Xue", "Hui", ""], ["Ji", "Shouling", ""]]}, {"id": "2102.11588", "submitter": "Julio Wissing", "authors": "Julio Wissing, Benedikt Boenninghoff, Dorothea Kolossa, Tsubasa\n  Ochiai, Marc Delcroix, Keisuke Kinoshita, Tomohiro Nakatani, Shoko Araki,\n  Christopher Schymura", "title": "Data Fusion for Audiovisual Speaker Localization: Extending Dynamic\n  Stream Weights to the Spatial Domain", "comments": "4 pages, 6 figures, ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.CV cs.LG eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the positions of multiple speakers can be helpful for tasks like\nautomatic speech recognition or speaker diarization. Both applications benefit\nfrom a known speaker position when, for instance, applying beamforming or\nassigning unique speaker identities. Recently, several approaches utilizing\nacoustic signals augmented with visual data have been proposed for this task.\nHowever, both the acoustic and the visual modality may be corrupted in specific\nspatial regions, for instance due to poor lighting conditions or to the\npresence of background noise. This paper proposes a novel audiovisual data\nfusion framework for speaker localization by assigning individual dynamic\nstream weights to specific regions in the localization space. This fusion is\nachieved via a neural network, which combines the predictions of individual\naudio and video trackers based on their time- and location-dependent\nreliability. A performance evaluation using audiovisual recordings yields\npromising results, with the proposed fusion approach outperforming all baseline\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:59:31 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 07:57:47 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Wissing", "Julio", ""], ["Boenninghoff", "Benedikt", ""], ["Kolossa", "Dorothea", ""], ["Ochiai", "Tsubasa", ""], ["Delcroix", "Marc", ""], ["Kinoshita", "Keisuke", ""], ["Nakatani", "Tomohiro", ""], ["Araki", "Shoko", ""], ["Schymura", "Christopher", ""]]}, {"id": "2102.11651", "submitter": "Hossein Sadr", "authors": "Hossein Sadr, Mozhdeh Nazari Solimandarabi, Mir Mohsen Pedram,\n  Mohammad Teshnehlab", "title": "A Novel Deep Learning Method for Textual Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis is known as one of the most crucial tasks in the field of\nnatural language processing and Convolutional Neural Network (CNN) is one of\nthose prominent models that is commonly used for this aim. Although\nconvolutional neural networks have obtained remarkable results in recent years,\nthey are still confronted with some limitations. Firstly, they consider that\nall words in a sentence have equal contributions in the sentence meaning\nrepresentation and are not able to extract informative words. Secondly, they\nrequire a large number of training data to obtain considerable results while\nthey have many parameters that must be accurately adjusted. To this end, a\nconvolutional neural network integrated with a hierarchical attention layer is\nproposed which is able to extract informative words and assign them higher\nweight. Moreover, the effect of transfer learning that transfers knowledge\nlearned in the source domain to the target domain with the aim of improving the\nperformance is also explored. Based on the empirical results, the proposed\nmodel not only has higher classification accuracy and can extract informative\nwords but also applying incremental transfer learning can significantly enhance\nthe classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 12:11:36 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Sadr", "Hossein", ""], ["Solimandarabi", "Mozhdeh Nazari", ""], ["Pedram", "Mir Mohsen", ""], ["Teshnehlab", "Mohammad", ""]]}, {"id": "2102.11749", "submitter": "Ewan Dunbar", "authors": "Louis Fournier and Ewan Dunbar", "title": "Paraphrases do not explain word analogies", "comments": "To appear in Proceedings of the 16th Conference of the European\n  Chapter of the Association for Computational Linguistics: Volume 2, Short\n  Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many types of distributional word embeddings (weakly) encode linguistic\nregularities as directions (the difference between \"jump\" and \"jumped\" will be\nin a similar direction to that of \"walk\" and \"walked,\" and so on). Several\nattempts have been made to explain this fact. We respond to Allen and\nHospedales' recent (ICML, 2019) theoretical explanation, which claims that\nword2vec and GloVe will encode linguistic regularities whenever a specific\nrelation of paraphrase holds between the four words involved in the regularity.\nWe demonstrate that the explanation does not go through: the paraphrase\nrelations needed under this explanation do not hold empirically.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:25:10 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Fournier", "Louis", ""], ["Dunbar", "Ewan", ""]]}, {"id": "2102.11917", "submitter": "Edmon Begoli", "authors": "Jeremiah Duncan, Fabian Fallas, Chris Gropp, Emily Herron, Maria\n  Mahbub, Paula Olaya, Eduardo Ponce, Tabitha K. Samuel, Daniel Schultz,\n  Sudarshan Srinivasan, Maofeng Tang, Viktor Zenkov, Quan Zhou, Edmon Begoli", "title": "The Sensitivity of Word Embeddings-based Author Detection Models to\n  Semantic-preserving Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Authorship analysis is an important subject in the field of natural language\nprocessing. It allows the detection of the most likely writer of articles,\nnews, books, or messages. This technique has multiple uses in tasks related to\nauthorship attribution, detection of plagiarism, style analysis, sources of\nmisinformation, etc. The focus of this paper is to explore the limitations and\nsensitiveness of established approaches to adversarial manipulations of inputs.\nTo this end, and using those established techniques, we first developed an\nexperimental frame-work for author detection and input perturbations. Next, we\nexperimentally evaluated the performance of the authorship detection model to a\ncollection of semantic-preserving adversarial perturbations of input\nnarratives. Finally, we compare and analyze the effects of different\nperturbation strategies, input and model configurations, and the effects of\nthese on the author detection model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:55:45 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Duncan", "Jeremiah", ""], ["Fallas", "Fabian", ""], ["Gropp", "Chris", ""], ["Herron", "Emily", ""], ["Mahbub", "Maria", ""], ["Olaya", "Paula", ""], ["Ponce", "Eduardo", ""], ["Samuel", "Tabitha K.", ""], ["Schultz", "Daniel", ""], ["Srinivasan", "Sudarshan", ""], ["Tang", "Maofeng", ""], ["Zenkov", "Viktor", ""], ["Zhou", "Quan", ""], ["Begoli", "Edmon", ""]]}, {"id": "2102.11972", "submitter": "Sharan Narang", "authors": "Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry,\n  Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan,\n  Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel", "title": "Do Transformer Modifications Transfer Across Implementations and\n  Applications?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:44:54 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Narang", "Sharan", ""], ["Chung", "Hyung Won", ""], ["Tay", "Yi", ""], ["Fedus", "William", ""], ["Fevry", "Thibault", ""], ["Matena", "Michael", ""], ["Malkan", "Karishma", ""], ["Fiedel", "Noah", ""], ["Shazeer", "Noam", ""], ["Lan", "Zhenzhong", ""], ["Zhou", "Yanqi", ""], ["Li", "Wei", ""], ["Ding", "Nan", ""], ["Marcus", "Jake", ""], ["Roberts", "Adam", ""], ["Raffel", "Colin", ""]]}, {"id": "2102.12060", "submitter": "Ana Marasovi\\'c", "authors": "Sarah Wiegreffe and Ana Marasovi\\'c", "title": "Teach Me to Explain: A Review of Datasets for Explainable NLP", "comments": "Version 2: added missing references, changed the NLI example in Table\n  1, and corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable NLP (ExNLP) has increasingly focused on collecting\nhuman-annotated explanations. These explanations are used downstream in three\nways: as data augmentation to improve performance on a predictive task, as a\nloss signal to train models to produce explanations for their predictions, and\nas a means to evaluate the quality of model-generated explanations. In this\nreview, we identify three predominant classes of explanations (highlights,\nfree-text, and structured), organize the literature on annotating each type,\npoint to what has been learned to date, and give recommendations for collecting\nExNLP datasets in the future.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 04:25:01 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 04:42:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wiegreffe", "Sarah", ""], ["Marasovi\u0107", "Ana", ""]]}, {"id": "2102.12073", "submitter": "Boaz Shmueli", "authors": "Boaz Shmueli, Lun-Wei Ku, Soumya Ray", "title": "SocialNLP EmotionGIF 2020 Challenge Overview: Predicting Reaction GIF\n  Categories on Social Media", "comments": "The 8th International Workshop on Natural Language Processing for\n  Social Media co-located with ACL-2020. 7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of the EmotionGIF2020 Challenge, held at the 8th\nInternational Workshop on Natural Language Processing for Social Media\n(SocialNLP), in conjunction with ACL 2020. The challenge required predicting\naffective reactions to online texts, and included the EmotionGIF dataset, with\ntweets labeled for the reaction categories. The novel dataset included 40K\ntweets with their reaction GIFs. Due to the special circumstances of year 2020,\ntwo rounds of the competition were conducted. A total of 84 teams registered\nfor the task. Of these, 25 teams success-fully submitted entries to the\nevaluation phase in the first round, while 13 teams participated successfully\nin the second round. Of the top participants, five teams presented a technical\nreport and shared their code. The top score of the winning team using the\nRecall@K metric was 62.47%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 05:22:10 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Shmueli", "Boaz", ""], ["Ku", "Lun-Wei", ""], ["Ray", "Soumya", ""]]}, {"id": "2102.12082", "submitter": "Anshul Wadhawan", "authors": "Ishan Sanjeev Upadhyay, Nikhil E, Anshul Wadhawan, Radhika Mamidi", "title": "Hopeful_Men@LT-EDI-EACL2021: Hope Speech Detection Using Indic\n  Transliteration and Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to describe the approach we used to detect hope speech in the\nHopeEDI dataset. We experimented with two approaches. In the first approach, we\nused contextual embeddings to train classifiers using logistic regression,\nrandom forest, SVM, and LSTM based models.The second approach involved using a\nmajority voting ensemble of 11 models which were obtained by fine-tuning\npre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding\nan output layer. We found that the second approach was superior for English,\nTamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and\n0.49 for English,Malayalam and Tamil respectively. Our solution ranked first in\nEnglish, eighth in Malayalam and eleventh in Tamil.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 06:01:32 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 04:50:47 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Upadhyay", "Ishan Sanjeev", ""], ["E", "Nikhil", ""], ["Wadhawan", "Anshul", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2102.12109", "submitter": "Aso Mahmudi", "authors": "Aso Mahmudi, Hadi Veisi", "title": "Automatic Meter Classification of Kurdish Poems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the classic texts in Kurdish literature are poems. Knowing the meter\nof the poems is helpful for correct reading, a better understanding of the\nmeaning, and avoidance of ambiguity. This paper presents a rule-based method\nfor automatic classification of the poem meter for the Central Kurdish\nlanguage. The metrical system of Kurdish poetry is divided into three classes\nof quantitative, syllabic, and free verses. As the vowel length is not phonemic\nin the language, there are uncertainties in syllable weight and meter\nidentification. The proposed method generates all the possible situations and\nthen, by considering all lines of the input poem and the common meter patterns\nof Kurdish poetry, identifies the most probable meter type and pattern of the\ninput poem. Evaluation of the method on a dataset from VejinBooks Kurdish\ncorpus resulted in 97.3% of precision in meter type and 96.2% of precision in\npattern identification.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 07:57:38 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mahmudi", "Aso", ""], ["Veisi", "Hadi", ""]]}, {"id": "2102.12128", "submitter": "Shaobo Cui", "authors": "Shaobo Cui, Xintong Bao, Xinxing Zu, Yangyang Guo, Zhongzhou Zhao, Ji\n  Zhang, Haiqing Chen", "title": "OneStop QAMaker: Extract Question-Answer Pairs from Text in a One-Stop\n  Approach", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale question-answer (QA) pairs are critical for advancing research\nareas like machine reading comprehension and question answering. To construct\nQA pairs from documents requires determining how to ask a question and what is\nthe corresponding answer. Existing methods for QA pair generation usually\nfollow a pipeline approach. Namely, they first choose the most likely candidate\nanswer span and then generate the answer-specific question. This pipeline\napproach, however, is undesired in mining the most appropriate QA pairs from\ndocuments since it ignores the connection between question generation and\nanswer extraction, which may lead to incompatible QA pair generation, i.e., the\nselected answer span is inappropriate for question generation. However, for\nhuman annotators, we take the whole QA pair into account and consider the\ncompatibility between question and answer. Inspired by such motivation, instead\nof the conventional pipeline approach, we propose a model named OneStop\ngenerate QA pairs from documents in a one-stop approach. Specifically,\nquestions and their corresponding answer span is extracted simultaneously and\nthe process of question generation and answer extraction mutually affect each\nother. Additionally, OneStop is much more efficient to be trained and deployed\nin industrial scenarios since it involves only one model to solve the complex\nQA generation task. We conduct comprehensive experiments on three large-scale\nmachine reading comprehension datasets: SQuAD, NewsQA, and DuReader. The\nexperimental results demonstrate that our OneStop model outperforms the\nbaselines significantly regarding the quality of generated questions, quality\nof generated question-answer pairs, and model efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:45:00 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Cui", "Shaobo", ""], ["Bao", "Xintong", ""], ["Zu", "Xinxing", ""], ["Guo", "Yangyang", ""], ["Zhao", "Zhongzhou", ""], ["Zhang", "Ji", ""], ["Chen", "Haiqing", ""]]}, {"id": "2102.12136", "submitter": "Duc-Vu Nguyen", "authors": "Duc-Vu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen", "title": "Augmenting Part-of-speech Tagging with Syntactic Information for\n  Vietnamese and Chinese", "comments": "The comparison with existing methods in this paper is unfair because\n  the hyper-parameters of Bi-LSTM are different compared with previous\n  research. Importantly, there is a data leakage issue w.r.t this paper's\n  experimental setup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word segmentation and part-of-speech tagging are two critical preliminary\nsteps for downstream tasks in Vietnamese natural language processing. In\nreality, people tend to consider also the phrase boundary when performing word\nsegmentation and part of speech tagging rather than solely process word by word\nfrom left to right. In this paper, we implement this idea to improve word\nsegmentation and part of speech tagging the Vietnamese language by employing a\nsimplified constituency parser. Our neural model for joint word segmentation\nand part-of-speech tagging has the architecture of the syllable-based CRF\nconstituency parser. To reduce the complexity of parsing, we replace all\nconstituent labels with a single label indicating for phrases. This model can\nbe augmented with predicted word boundary and part-of-speech tags by other\ntools. Because Vietnamese and Chinese have some similar linguistic phenomena,\nwe evaluated the proposed model and its augmented versions on three Vietnamese\nbenchmark datasets and six Chinese benchmark datasets. Our experimental results\nshow that the proposed model achieves higher performances than previous works\nfor both languages.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 08:57:02 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 08:10:20 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nguyen", "Duc-Vu", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2102.12149", "submitter": "Gaurav Singh", "authors": "Gaurav Singh", "title": "Sentiment Analysis of Code-Mixed Social Media Text (Hinglish)", "comments": "17 pages, 12 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the results obtained for different techniques applied\nfor performing the sentiment analysis of social media (Twitter) code-mixed text\nwritten in Hinglish. The various stages involved in performing the sentiment\nanalysis were data consolidation, data cleaning, data transformation and\nmodelling. Various data cleaning techniques were applied, data was cleaned in\nfive iterations and the results of experiments conducted were noted after each\niteration. Data was transformed using count vectorizer, one hot vectorizer,\ntf-idf vectorizer, doc2vec, word2vec and fasttext embeddings. The models were\ncreated using various machine learning algorithms such as SVM, KNN, Decision\nTrees, Random Forests, Naive Bayes, Logistic Regression, and ensemble voting\nclassifiers. The data was obtained from a task on Codalab competition website\nwhich was listed as Task:9 on the Semeval-2020 competition website. The models\ncreated were evaluated using the F1-score (macro). The best F1-score of 69.07\nwas achieved using ensemble voting classifier.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:15:34 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Singh", "Gaurav", ""]]}, {"id": "2102.12162", "submitter": "Ngoc Tran", "authors": "Quang Huu Pham, Viet Anh Nguyen, Linh Bao Doan, Ngoc N. Tran and Ta\n  Minh Thanh", "title": "From Universal Language Model to Downstream Task: Improving\n  RoBERTa-Based Vietnamese Hate Speech Detection", "comments": "Published in 2020 12th International Conference on Knowledge and\n  Systems Engineering (KSE)", "journal-ref": "2020 12th International Conference on Knowledge and Systems\n  Engineering (KSE), Can Tho, Vietnam, 2020, pp. 37-42", "doi": "10.1109/KSE50997.2020.9287406", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Natural language processing is a fast-growing field of artificial\nintelligence. Since the Transformer was introduced by Google in 2017, a large\nnumber of language models such as BERT, GPT, and ELMo have been inspired by\nthis architecture. These models were trained on huge datasets and achieved\nstate-of-the-art results on natural language understanding. However,\nfine-tuning a pre-trained language model on much smaller datasets for\ndownstream tasks requires a carefully-designed pipeline to mitigate problems of\nthe datasets such as lack of training data and imbalanced data. In this paper,\nwe propose a pipeline to adapt the general-purpose RoBERTa language model to a\nspecific text classification task: Vietnamese Hate Speech Detection. We first\ntune the PhoBERT on our dataset by re-training the model on the Masked Language\nModel task; then, we employ its encoder for text classification. In order to\npreserve pre-trained weights while learning new feature representations, we\nfurther utilize different training techniques: layer freezing, block-wise\nlearning rate, and label smoothing. Our experiments proved that our proposed\npipeline boosts the performance significantly, achieving a new state-of-the-art\non Vietnamese Hate Speech Detection campaign with 0.7221 F1 score.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 09:30:55 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Pham", "Quang Huu", ""], ["Nguyen", "Viet Anh", ""], ["Doan", "Linh Bao", ""], ["Tran", "Ngoc N.", ""], ["Thanh", "Ta Minh", ""]]}, {"id": "2102.12179", "submitter": "Sunil Gundapu", "authors": "Sunil Gundapu, Radhika Mamidi", "title": "Multichannel LSTM-CNN for Telugu Technical Domain Identification", "comments": "Paper accepted in The seventeenth International Conference on Natural\n  Language Processing (ICON-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the instantaneous growth of text information, retrieving domain-oriented\ninformation from the text data has a broad range of applications in Information\nRetrieval and Natural language Processing. Thematic keywords give a compressed\nrepresentation of the text. Usually, Domain Identification plays a significant\nrole in Machine Translation, Text Summarization, Question Answering,\nInformation Extraction, and Sentiment Analysis. In this paper, we proposed the\nMultichannel LSTM-CNN methodology for Technical Domain Identification for\nTelugu. This architecture was used and evaluated in the context of the ICON\nshared task TechDOfication 2020 (task h), and our system got 69.9% of the F1\nscore on the test dataset and 90.01% on the validation set.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 10:15:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Gundapu", "Sunil", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2102.12206", "submitter": "Eyal Ben-David", "authors": "Eyal Ben-David, Nadav Oved, Roi Reichart", "title": "PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen\n  Domains", "comments": "First two authors contributed equally to this work. Our code and data\n  are available at: https://github.com/eyalbd2/PADA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing algorithms have made incredible progress, but\nthey still struggle when applied to out-of-distribution examples. We address a\nchallenging and underexplored version of this domain adaptation problem, where\nan algorithm is trained on several source domains, and then applied to examples\nfrom an unseen domain that is unknown at training time. Particularly, no\nexamples, labeled or unlabeled, or any other knowledge about the target domain\nare available to the algorithm at training time. We present PADA: A\nPrompt-based Autoregressive Domain Adaptation algorithm, based on the T5 model.\nGiven a test example, PADA first generates a unique prompt and then,\nconditioned on this prompt, labels the example with respect to the NLP task.\nThe prompt is a sequence of unrestricted length, consisting of pre-defined\nDomain Related Features (DRFs) that characterize each of the source domains.\nIntuitively, the prompt is a unique signature that maps the test example to the\nsemantic space spanned by the source domains. In experiments with 3 tasks (text\nclassification and sequence tagging), for a total of 14 multi-source adaptation\nscenarios, PADA substantially outperforms strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:02:29 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:01:21 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ben-David", "Eyal", ""], ["Oved", "Nadav", ""], ["Reichart", "Roi", ""]]}, {"id": "2102.12227", "submitter": "Andrea Galassi", "authors": "Andrea Galassi, Marco Lippi, Paolo Torroni", "title": "Multi-Task Attentive Residual Networks for Argument Mining", "comments": "12 pages, 2 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore the use of residual networks and neural attention for argument\nmining and in particular link prediction. The method we propose makes no\nassumptions on document or argument structure. We propose a residual\narchitecture that exploits attention, multi-task learning, and makes use of\nensemble. We evaluate it on a challenging data set consisting of user-generated\ncomments, as well as on two other datasets consisting of scientific\npublications. On the user-generated content dataset, our model outperforms\nstate-of-the-art methods that rely on domain knowledge. On the scientific\nliterature datasets it achieves results comparable to those yielded by\nBERT-based approaches but with a much smaller model size.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:35:28 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Galassi", "Andrea", ""], ["Lippi", "Marco", ""], ["Torroni", "Paolo", ""]]}, {"id": "2102.12254", "submitter": "Abheesht Sharma", "authors": "Gunjan Chhablani, Abheesht Sharma, Harshit Pandey, Yash Bhartia, Shan\n  Suthaharan", "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based\n  Token Classification and Span Prediction Techniques", "comments": "8 pages, 3 figures, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:30:09 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 14:08:12 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chhablani", "Gunjan", ""], ["Sharma", "Abheesht", ""], ["Pandey", "Harshit", ""], ["Bhartia", "Yash", ""], ["Suthaharan", "Shan", ""]]}, {"id": "2102.12255", "submitter": "Abheesht Sharma", "authors": "Abheesht Sharma, Harshit Pandey, Gunjan Chhablani, Yash Bhartia,\n  Tirtharaj Dash", "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with\n  Abstract Words using Augmentation, Linguistic Features and Voting", "comments": "10 pages, 4 figures, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:33:12 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 14:02:41 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sharma", "Abheesht", ""], ["Pandey", "Harshit", ""], ["Chhablani", "Gunjan", ""], ["Bhartia", "Yash", ""], ["Dash", "Tirtharaj", ""]]}, {"id": "2102.12266", "submitter": "Andreas Vlachos", "authors": "Gordon Buck, Andreas Vlachos", "title": "Trajectory-Based Meta-Learning for Out-Of-Vocabulary Word Embedding\n  Learning", "comments": "Camera ready for the EACL workshop Adapt-NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word embedding learning methods require a large number of occurrences of a\nword to accurately learn its embedding. However, out-of-vocabulary (OOV) words\nwhich do not appear in the training corpus emerge frequently in the smaller\ndownstream data. Recent work formulated OOV embedding learning as a few-shot\nregression problem and demonstrated that meta-learning can improve results\nobtained. However, the algorithm used, model-agnostic meta-learning (MAML) is\nknown to be unstable and perform worse when a large number of gradient steps\nare used for parameter updates. In this work, we propose the use of Leap, a\nmeta-learning algorithm which leverages the entire trajectory of the learning\nprocess instead of just the beginning and the end points, and thus ameliorates\nthese two issues. In our experiments on a benchmark OOV embedding learning\ndataset and in an extrinsic evaluation, Leap performs comparably or better than\nMAML. We go on to examine which contexts are most beneficial to learn an OOV\nembedding from, and propose that the choice of contexts may matter more than\nthe meta-learning employed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 12:57:58 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Buck", "Gordon", ""], ["Vlachos", "Andreas", ""]]}, {"id": "2102.12330", "submitter": "Matthias A{\\ss}enmacher", "authors": "M. A{\\ss}enmacher, A. Corvonato, C. Heumann", "title": "Re-Evaluating GermEval17 Using German Pre-Trained Language Models", "comments": "Accepted as a conference paper at the 6th Swiss Text Analytics\n  Conference (SwissText), Brugg, Switzerland (Online), June 14-16, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of a commonly used benchmark data set (collection) such as\n(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English\npre-trained language models is a severe shortcoming of current English-centric\nNLP-research. It concentrates a large part of the research on English,\nneglecting the uncertainty when transferring conclusions found for the English\nlanguage to other languages. We evaluate the performance of the German and\nmultilingual BERT-based models currently available via the huggingface\ntransformers library on the four tasks of the GermEval17 workshop. We compare\nthem to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;\nAttia et al., 2018) as well as to an ELMo-based architecture (Biesialska et\nal., 2020) and a BERT-based approach (Guhr et al., 2020). The observed\nimprovements are put in relation to those for similar tasks and similar models\n(pre-BERT vs. BERT-based) for the English language in order to draw tentative\nconclusions about whether the observed improvements are transferable to German\nor potentially other related languages.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:05:56 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 14:39:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["A\u00dfenmacher", "M.", ""], ["Corvonato", "A.", ""], ["Heumann", "C.", ""]]}, {"id": "2102.12362", "submitter": "Mirza Beg", "authors": "Ayesha Qamar, Tehreem Javed, and Mirza Omer Beg", "title": "Detecting Compliance of Privacy Policies with Data Protection Laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy Policies are the legal documents that describe the practices that an\norganization or company has adopted in the handling of the personal data of its\nusers. But as policies are a legal document, they are often written in\nextensive legal jargon that is difficult to understand. Though work has been\ndone on privacy policies but none that caters to the problem of verifying if a\ngiven privacy policy adheres to the data protection laws of a given country or\nstate. We aim to bridge that gap by providing a framework that analyzes privacy\npolicies in light of various data protection laws, such as the General Data\nProtection Regulation (GDPR). To achieve that, firstly we labeled both the\nprivacy policies and laws. Then a correlation scheme is developed to map the\ncontents of a privacy policy to the appropriate segments of law that a policy\nmust conform to. Then we check the compliance of privacy policy's text with the\ncorresponding text of the law using NLP techniques. By using such a tool, users\nwould be better equipped to understand how their personal data is managed. For\nnow, we have provided a mapping for the GDPR and PDPA, but other laws can\neasily be incorporated in the already built pipeline.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 09:15:15 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Qamar", "Ayesha", ""], ["Javed", "Tehreem", ""], ["Beg", "Mirza Omer", ""]]}, {"id": "2102.12382", "submitter": "Abhinav Tamaskar", "authors": "Abhinav Tamaskar, Roy Rinberg, Sunandan Chakraborty, Bud Mishra", "title": "Creolizing the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evolution of language has been a hotly debated subject with contradicting\nhypotheses and unreliable claims. Drawing from signalling games, dynamic\npopulation mechanics, machine learning and algebraic topology, we present a\nmethod for detecting evolutionary patterns in a sociological model of language\nevolution. We develop a minimalistic model that provides a rigorous base for\nany generalized evolutionary model for language based on communication between\nindividuals. We also discuss theoretical guarantees of this model, ranging from\nstability of language representations to fast convergence of language by\ntemporal communication and language drift in an interactive setting. Further we\npresent empirical results and their interpretations on a real world dataset\nfrom \\rdt to identify communities and echo chambers for opinions, thus placing\nobstructions to reliable communication among communities.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 16:08:45 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Tamaskar", "Abhinav", ""], ["Rinberg", "Roy", ""], ["Chakraborty", "Sunandan", ""], ["Mishra", "Bud", ""]]}, {"id": "2102.12407", "submitter": "Akshat Gupta", "authors": "Akshat Gupta, Sai Krishna Rallabandi, Alan Black", "title": "Task-Specific Pre-Training and Cross Lingual Transfer for Code-Switched\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using task-specific pre-training and leveraging cross-lingual transfer are\ntwo of the most popular ways to handle code-switched data. In this paper, we\naim to compare the effects of both for the task of sentiment analysis. We work\nwith two Dravidian Code-Switched languages - Tamil-Engish and Malayalam-English\nand four different BERT based models. We compare the effects of task-specific\npre-training and cross-lingual transfer and find that task-specific\npre-training results in superior zero-shot and supervised performance when\ncompared to performance achieved by leveraging cross-lingual transfer from\nmultilingual BERT models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 16:57:58 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Gupta", "Akshat", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan", ""]]}, {"id": "2102.12452", "submitter": "Yonatan Belinkov", "authors": "Yonatan Belinkov", "title": "Probing Classifiers: Promises, Shortcomings, and Alternatives", "comments": "Updated version, under review in Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological weaknesses of this approach. This article\ncritically reviews the probing classifiers framework, highlighting\nshortcomings, improvements, and alternative approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:36:14 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:20:30 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Belinkov", "Yonatan", ""]]}, {"id": "2102.12459", "submitter": "Tao Lei", "authors": "Tao Lei", "title": "When Attention Meets Fast Recurrence: Training Language Models with\n  Reduced Compute", "comments": "Improved speed in fp16; additional results on BILLION WORD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models have become increasingly difficult to train because of\nthe required computation time and cost. In this work, we present SRU++, a\nrecurrent unit with optional built-in attention that exhibits state-of-the-art\nmodeling capacity and training efficiency. On standard language modeling\nbenchmarks such as enwik8, Wiki-103 and Billion Word datasets, our model\nobtains better perplexity and bits-per-character (bpc) while using 3x-10x less\ntraining time and cost compared to top-performing Transformer models. Our\nresults reaffirm that attention is not all we need and can be complementary to\nother sequential modeling modules. Moreover, fast recurrence with little\nattention can be a leading model architecture.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:39:56 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 16:32:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lei", "Tao", ""]]}, {"id": "2102.12511", "submitter": "Hady Elsahar Dr", "authors": "Lucie-Aim\\'ee Kaffee, Hady Elsahar", "title": "References in Wikipedia: The Editors' Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  References are an essential part of Wikipedia. Each statement in Wikipedia\nshould be referenced. In this paper, we explore the creation and collection of\nreferences for new Wikipedia articles from an editors' perspective. We map out\nthe workflow of editors when creating a new article, emphasising how they\nselect references.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:04:17 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 12:40:58 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kaffee", "Lucie-Aim\u00e9e", ""], ["Elsahar", "Hady", ""]]}, {"id": "2102.12516", "submitter": "Autumn Toney", "authors": "Autumn Toney", "title": "A Large-Scale, Automated Study of Language Surrounding Artificial\n  Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents a large-scale analysis of artificial intelligence (AI) and\nmachine learning (ML) references within news articles and scientific\npublications between 2011 and 2019. We implement word association measurements\nthat automatically identify shifts in language co-occurring with AI/ML and\nquantify the strength of these word associations. Our results highlight the\nevolution of perceptions and definitions around AI/ML and detect emerging\napplication areas, models, and systems (e.g., blockchain and cybersecurity).\nRecent small-scale, manual studies have explored AI/ML discourse within the\ngeneral public, the policymaker community, and researcher community, but are\nlimited in their scalability and longevity. Our methods provide new views into\npublic perceptions and subject-area expert discussions of AI/ML and greatly\nexceed the explanative power of prior work.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:14:53 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Toney", "Autumn", ""]]}, {"id": "2102.12634", "submitter": "Xiangyu Peng", "authors": "Amal Alabdulkarim, Siyan Li, Xiangyu Peng", "title": "Automatic Story Generation: Challenges and Attempts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of this survey paper is to explore the challenges in automatic\nstory generation. We hope to contribute in the following ways: 1. Explore how\nprevious research in story generation addressed those challenges. 2. Discuss\nfuture research directions and new technologies that may aid more advancements.\n3. Shed light on emerging and often overlooked challenges such as creativity\nand discourse.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 02:03:35 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Alabdulkarim", "Amal", ""], ["Li", "Siyan", ""], ["Peng", "Xiangyu", ""]]}, {"id": "2102.12664", "submitter": "Linghui Meng", "authors": "Linghui Meng, Jin Xu, Xu Tan, Jindong Wang, Tao Qin, Bo Xu", "title": "MixSpeech: Data Augmentation for Low-resource Automatic Speech\n  Recognition", "comments": "To appear at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose MixSpeech, a simple yet effective data augmentation\nmethod based on mixup for automatic speech recognition (ASR). MixSpeech trains\nan ASR model by taking a weighted combination of two different speech features\n(e.g., mel-spectrograms or MFCC) as the input, and recognizing both text\nsequences, where the two recognition losses use the same combination weight. We\napply MixSpeech on two popular end-to-end speech recognition models including\nLAS (Listen, Attend and Spell) and Transformer, and conduct experiments on\nseveral low-resource datasets including TIMIT, WSJ, and HKUST. Experimental\nresults show that MixSpeech achieves better accuracy than the baseline models\nwithout data augmentation, and outperforms a strong data augmentation method\nSpecAugment on these recognition tasks. Specifically, MixSpeech outperforms\nSpecAugment with a relative PER improvement of 10.6$\\%$ on TIMIT dataset, and\nachieves a strong WER of 4.7$\\%$ on WSJ dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 03:40:43 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Meng", "Linghui", ""], ["Xu", "Jin", ""], ["Tan", "Xu", ""], ["Wang", "Jindong", ""], ["Qin", "Tao", ""], ["Xu", "Bo", ""]]}, {"id": "2102.12671", "submitter": "Boer Lyu", "authors": "Boer Lyu, Lu Chen, Su Zhu, Kai Yu", "title": "LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short\n  Text Matching", "comments": "Accepted by AAAI 2021; 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese short text matching is a fundamental task in natural language\nprocessing. Existing approaches usually take Chinese characters or words as\ninput tokens. They have two limitations: 1) Some Chinese words are polysemous,\nand semantic information is not fully utilized. 2) Some models suffer potential\nissues caused by word segmentation. Here we introduce HowNet as an external\nknowledge base and propose a Linguistic knowledge Enhanced graph Transformer\n(LET) to deal with word ambiguity. Additionally, we adopt the word lattice\ngraph as input to maintain multi-granularity information. Our model is also\ncomplementary to pre-trained language models. Experimental results on two\nChinese datasets show that our models outperform various typical text matching\napproaches. Ablation study also indicates that both semantic information and\nmulti-granularity information are important for text matching modeling.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 04:01:51 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Lyu", "Boer", ""], ["Chen", "Lu", ""], ["Zhu", "Su", ""], ["Yu", "Kai", ""]]}, {"id": "2102.12700", "submitter": "Nazanin Sabri", "authors": "Nazanin Sabri, Ali Edalat, Behnam Bahrak", "title": "Sentiment Analysis of Persian-English Code-mixed Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid production of data on the internet and the need to understand how\nusers are feeling from a business and research perspective has prompted the\ncreation of numerous automatic monolingual sentiment detection systems. More\nrecently however, due to the unstructured nature of data on social media, we\nare observing more instances of multilingual and code-mixed texts. This\ndevelopment in content type has created a new demand for code-mixed sentiment\nanalysis systems. In this study we collect, label and thus create a dataset of\nPersian-English code-mixed tweets. We then proceed to introduce a model which\nuses BERT pretrained embeddings as well as translation models to automatically\nlearn the polarity scores of these Tweets. Our model outperforms the baseline\nmodels that use Na\\\"ive Bayes and Random Forest methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 06:05:59 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Sabri", "Nazanin", ""], ["Edalat", "Ali", ""], ["Bahrak", "Behnam", ""]]}, {"id": "2102.12702", "submitter": "Guolin Ke", "authors": "Chengxuan Ying, Guolin Ke, Di He, Tie-Yan Liu", "title": "LazyFormer: Self Attention with Lazy Update", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Improving the efficiency of Transformer-based language pre-training is an\nimportant task in NLP, especially for the self-attention module, which is\ncomputationally expensive. In this paper, we propose a simple but effective\nsolution, called \\emph{LazyFormer}, which computes the self-attention\ndistribution infrequently. LazyFormer composes of multiple lazy blocks, each of\nwhich contains multiple Transformer layers. In each lazy block, the\nself-attention distribution is only computed once in the first layer and then\nis reused in all upper layers. In this way, the cost of computation could be\nlargely saved. We also provide several training tricks for LazyFormer.\nExtensive experiments demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 06:18:20 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Ying", "Chengxuan", ""], ["Ke", "Guolin", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2102.12777", "submitter": "Yuqiang Xie", "authors": "Yuqiang Xie, Luxi Xing, Wei Peng, Yue Hu", "title": "IIE-NLP-Eyas at SemEval-2021 Task 4: Enhancing PLM for ReCAM with\n  Special Tokens, Re-Ranking, Siamese Encoders and Back Translation", "comments": "5 pages, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces our systems for all three subtasks of SemEval-2021 Task\n4: Reading Comprehension of Abstract Meaning. To help our model better\nrepresent and understand abstract concepts in natural language, we well-design\nmany simple and effective approaches adapted to the backbone model (RoBERTa).\nSpecifically, we formalize the subtasks into the multiple-choice question\nanswering format and add special tokens to abstract concepts, then, the final\nprediction of question answering is considered as the result of subtasks.\nAdditionally, we employ many finetuning tricks to improve the performance.\nExperimental results show that our approaches achieve significant performance\ncompared with the baseline systems. Our approaches achieve eighth rank on\nsubtask-1 and tenth rank on subtask-2.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 10:51:48 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Xie", "Yuqiang", ""], ["Xing", "Luxi", ""], ["Peng", "Wei", ""], ["Hu", "Yue", ""]]}, {"id": "2102.12799", "submitter": "Massimo Stella", "authors": "Massimo Stella", "title": "Cognitive network science for understanding online social cognitions: A\n  brief review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media are digitalising massive amounts of users' cognitions in terms\nof timelines and emotional content. Such Big Data opens unprecedented\nopportunities for investigating cognitive phenomena like perception,\npersonality and information diffusion but requires suitable interpretable\nframeworks. Since social media data come from users' minds, worthy candidates\nfor this challenge are cognitive networks, models of cognition giving structure\nto mental conceptual associations. This work outlines how cognitive network\nscience can open new, quantitative ways for understanding cognition through\nonline media, like: (i) reconstructing how users semantically and emotionally\nframe events with contextual knowledge unavailable to machine learning, (ii)\ninvestigating conceptual salience/prominence through knowledge structure in\nsocial discourse; (iii) studying users' personality traits like\nopenness-to-experience, curiosity, and creativity through language in posts;\n(iv) bridging cognitive/emotional content and social dynamics via multilayer\nnetworks comparing the mindsets of influencers and followers. These\nadvancements combine cognitive-, network- and computer science to understand\ncognitive mechanisms in both digital and real-world settings but come with\nlimitations concerning representativeness, individual variability and data\nintegration. Such aspects are discussed along the ethical implications of\nmanipulating socio-cognitive data. In the future, reading cognitions through\nnetworks and social media can expose cognitive biases amplified by online\nplatforms and relevantly inform policy making, education and markets about\nmassive, complex cognitive trends.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 11:53:28 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Stella", "Massimo", ""]]}, {"id": "2102.12828", "submitter": "Ningyu Zhang", "authors": "Xin Xie, Xiangnan Chen, Xiang Chen, Yong Wang, Ningyu Zhang, Shumin\n  Deng, Huajun Chen", "title": "ZJUKLAB at SemEval-2021 Task 4: Negative Augmentation with Language\n  Model for Reading Comprehension of Abstract Meaning", "comments": "Accepted by SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our systems for the three Subtasks of SemEval Task4:\nReading Comprehension of Abstract Meaning (ReCAM). We explain the algorithms\nused to learn our models and the process of tuning the algorithms and selecting\nthe best model. Inspired by the similarity of the ReCAM task and the language\npre-training, we propose a simple yet effective technology, namely, negative\naugmentation with language model. Evaluation results demonstrate the\neffectiveness of our proposed approach. Our models achieve the 4th rank on both\nofficial test sets of Subtask 1 and Subtask 2 with an accuracy of 87.9% and an\naccuracy of 92.8%, respectively. We further conduct comprehensive model\nanalysis and observe interesting error cases, which may promote future\nresearches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:03:05 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 07:31:42 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 10:12:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Xie", "Xin", ""], ["Chen", "Xiangnan", ""], ["Chen", "Xiang", ""], ["Wang", "Yong", ""], ["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Chen", "Huajun", ""]]}, {"id": "2102.12843", "submitter": "Asier Guti\\'errez-Fandi\\~no", "authors": "Asier Guti\\'errez-Fandi\\~no, Jordi Armengol-Estap\\'e, Casimiro Pio\n  Carrino, Ona De Gibert, Aitor Gonzalez-Agirre, Marta Villegas", "title": "Spanish Biomedical and Clinical Language Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We computed both Word and Sub-word Embeddings using FastText. For Sub-word\nembeddings we selected Byte Pair Encoding (BPE) algorithm to represent the\nsub-words. We evaluated the Biomedical Word Embeddings obtaining better results\nthan previous versions showing the implication that with more data, we obtain\nbetter representations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:30:04 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Guti\u00e9rrez-Fandi\u00f1o", "Asier", ""], ["Armengol-Estap\u00e9", "Jordi", ""], ["Carrino", "Casimiro Pio", ""], ["De Gibert", "Ona", ""], ["Gonzalez-Agirre", "Aitor", ""], ["Villegas", "Marta", ""]]}, {"id": "2102.12846", "submitter": "Dimitri Kartsaklis", "authors": "Robin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri\n  Kartsaklis, Bob Coecke", "title": "QNLP in Practice: Running Compositional Models of Meaning on a Quantum\n  Computer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Natural Language Processing (QNLP) deals with the design and\nimplementation of NLP models intended to be run on quantum hardware. In this\npaper, we present results on the first NLP experiments conducted on Noisy\nIntermediate-Scale Quantum (NISQ) computers for datasets of size >= 100\nsentences. Exploiting the formal similarity of the compositional model of\nmeaning by Coecke et al. (2010) with quantum theory, we create representations\nfor sentences that have a natural mapping to quantum circuits. We use these\nrepresentations to implement and successfully train two NLP models that solve\nsimple sentence classification tasks on quantum hardware. We describe in detail\nthe main principles, the process and challenges of these experiments, in a way\naccessible to NLP researchers, thus paving the way for practical Quantum\nNatural Language Processing.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:37:33 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Lorenz", "Robin", ""], ["Pearson", "Anna", ""], ["Meichanetzidis", "Konstantinos", ""], ["Kartsaklis", "Dimitri", ""], ["Coecke", "Bob", ""]]}, {"id": "2102.12858", "submitter": "Roman Klinger", "authors": "Jan Hofmann and Enrica Troiano and Roman Klinger", "title": "Emotion-Aware, Emotion-Agnostic, or Automatic: Corpus Creation\n  Strategies to Obtain Cognitive Event Appraisal Annotations", "comments": "Published in WASSA 2021 at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Appraisal theories explain how the cognitive evaluation of an event leads to\na particular emotion. In contrast to theories of basic emotions or affect\n(valence/arousal), this theory has not received a lot of attention in natural\nlanguage processing. Yet, in psychology it has been proven powerful: Smith and\nEllsworth (1985) showed that the appraisal dimensions attention, certainty,\nanticipated effort, pleasantness, responsibility/control and situational\ncontrol discriminate between (at least) 15 emotion classes. We study different\nannotation strategies for these dimensions, based on the event-focused enISEAR\ncorpus (Troiano et al., 2019). We analyze two manual annotation settings: (1)\nshowing the text to annotate while masking the experienced emotion label; (2)\nrevealing the emotion associated with the text. Setting 2 enables the\nannotators to develop a more realistic intuition of the described event, while\nSetting 1 is a more standard annotation procedure, purely relying on text. We\nevaluate these strategies in two ways: by measuring inter-annotator agreement\nand by fine-tuning RoBERTa to predict appraisal variables. Our results show\nthat knowledge of the emotion increases annotators' reliability. Further, we\nevaluate a purely automatic rule-based labeling strategy (inferring appraisal\nfrom annotated emotion classes). Training on automatically assigned labels\nleads to a competitive performance of our classifier, even when tested on\nmanual annotations. This is an indicator that it might be possible to\nautomatically create appraisal corpora for every domain for which emotion\ncorpora already exist.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 13:55:44 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Hofmann", "Jan", ""], ["Troiano", "Enrica", ""], ["Klinger", "Roman", ""]]}, {"id": "2102.12971", "submitter": "Sowmya Vajjala", "authors": "Taraka Rama and Sowmya Vajjala", "title": "Are pre-trained text representations useful for multilingual and\n  multi-dimensional language proficiency modeling?", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Development of language proficiency models for non-native learners has been\nan active area of interest in NLP research for the past few years. Although\nlanguage proficiency is multidimensional in nature, existing research typically\nconsiders a single \"overall proficiency\" while building models. Further,\nexisting approaches also considers only one language at a time. This paper\ndescribes our experiments and observations about the role of pre-trained and\nfine-tuned multilingual embeddings in performing multi-dimensional,\nmultilingual language proficiency classification. We report experiments with\nthree languages -- German, Italian, and Czech -- and model seven dimensions of\nproficiency ranging from vocabulary control to sociolinguistic appropriateness.\nOur results indicate that while fine-tuned embeddings are useful for\nmultilingual proficiency modeling, none of the features achieve consistently\nbest performance for all dimensions of language proficiency. All code, data and\nrelated supplementary material can be found at:\nhttps://github.com/nishkalavallabhi/MultidimCEFRScoring.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 16:23:52 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Rama", "Taraka", ""], ["Vajjala", "Sowmya", ""]]}, {"id": "2102.12982", "submitter": "Nils Rethmeier", "authors": "Nils Rethmeier and Isabelle Augenstein", "title": "A Primer on Contrastive Pretraining in Language Processing: Methods,\n  Lessons Learned and Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern natural language processing (NLP) methods employ self-supervised\npretraining objectives such as masked language modeling to boost the\nperformance of various application tasks. These pretraining methods are\nfrequently extended with recurrence, adversarial or linguistic property\nmasking, and more recently with contrastive learning objectives. Contrastive\nself-supervised training objectives enabled recent successes in image\nrepresentation pretraining by learning to contrast input-input pairs of\naugmented images as either similar or dissimilar. However, in NLP, automated\ncreation of text input augmentations is still very challenging because a single\ntoken can invert the meaning of a sentence. For this reason, some contrastive\nNLP pretraining methods contrast over input-label pairs, rather than over\ninput-input pairs, using methods from Metric Learning and Energy Based Models.\nIn this survey, we summarize recent self-supervised and supervised contrastive\nNLP pretraining methods and describe where they are used to improve language\nmodeling, few or zero-shot learning, pretraining data-efficiency and specific\nNLP end-tasks. We introduce key contrastive learning concepts with lessons\nlearned from prior research and structure works by applications and cross-field\nrelations. Finally, we point to open challenges and future directions for\ncontrastive NLP to encourage bringing contrastive NLP pretraining closer to\nrecent successes in image representation pretraining.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 16:35:07 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Rethmeier", "Nils", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2102.13019", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin", "title": "Investigating the Limitations of Transformers with Simple Arithmetic\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform arithmetic tasks is a remarkable trait of human\nintelligence and might form a critical component of more complex reasoning\ntasks. In this work, we investigate if the surface form of a number has any\ninfluence on how sequence-to-sequence language models learn simple arithmetic\ntasks such as addition and subtraction across a wide range of values. We find\nthat how a number is represented in its surface form has a strong influence on\nthe model's accuracy. In particular, the model fails to learn addition of\nfive-digit numbers when using subwords (e.g., \"32\"), and it struggles to learn\nwith character-level representations (e.g., \"3 2\"). By introducing position\ntokens (e.g., \"3 10e1 2\"), the model learns to accurately add and subtract\nnumbers up to 60 digits. We conclude that modern pretrained language models can\neasily learn arithmetic from very few examples, as long as we use the proper\nsurface representation. This result bolsters evidence that subword tokenizers\nand positional encodings are components in current transformer designs that\nmight need improvement. Moreover, we show that regardless of the number of\nparameters and training examples, models cannot learn addition rules that are\nindependent of the length of the numbers seen during training. Code to\nreproduce our experiments is available at\nhttps://github.com/castorini/transformers-arithmetic\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:22:53 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 15:54:38 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 19:58:27 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Jiang", "Zhiying", ""], ["Lin", "Jimmy", ""]]}, {"id": "2102.13030", "submitter": "Rita Ramos", "authors": "Rita Parada Ramos, Patr\\'icia Pereira, Helena Moniz, Joao Paulo\n  Carvalho, Bruno Martins", "title": "Retrieval Augmentation for Deep Neural Networks", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have achieved state-of-the-art results in various vision\nand/or language tasks. Despite the use of large training datasets, most models\nare trained by iterating over single input-output pairs, discarding the\nremaining examples for the current prediction. In this work, we actively\nexploit the training data, using the information from nearest training examples\nto aid the prediction both during training and testing. Specifically, our\napproach uses the target of the most similar training example to initialize the\nmemory state of an LSTM model, or to guide attention mechanisms. We apply this\napproach to image captioning and sentiment analysis, respectively through image\nand text retrieval. Results confirm the effectiveness of the proposed approach\nfor the two tasks, on the widely used Flickr8 and IMDB datasets. Our code is\npublicly available at http://github.com/RitaRamo/retrieval-augmentation-nn.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 17:38:31 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:14:47 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ramos", "Rita Parada", ""], ["Pereira", "Patr\u00edcia", ""], ["Moniz", "Helena", ""], ["Carvalho", "Joao Paulo", ""], ["Martins", "Bruno", ""]]}, {"id": "2102.13129", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, Lukas Lange, Dietrich Klakow", "title": "ANEA: Distant Supervision for Low-Resource Named Entity Recognition", "comments": "Accepted at Practical Machine Learning For Developing Countries @\n  ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision allows obtaining labeled training corpora for\nlow-resource settings where only limited hand-annotated data exists. However,\nto be used effectively, the distant supervision must be easy to gather. In this\nwork, we present ANEA, a tool to automatically annotate named entities in texts\nbased on entity lists. It spans the whole pipeline from obtaining the lists to\nanalyzing the errors of the distant supervision. A tuning step allows the user\nto improve the automatic annotation with their linguistic insights without\nlabelling or checking all tokens manually. In six low-resource scenarios, we\nshow that the F1-score can be increased by on average 18 points through\ndistantly supervised data obtained by ANEA.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:07:45 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 11:45:36 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Lange", "Lukas", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2102.13136", "submitter": "Christopher Ormerod", "authors": "Christopher M Ormerod, Akanksha Malhotra, and Amir Jafari", "title": "Automated essay scoring using efficient transformer-based language\n  models", "comments": "11 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated Essay Scoring (AES) is a cross-disciplinary effort involving\nEducation, Linguistics, and Natural Language Processing (NLP). The efficacy of\nan NLP model in AES tests it ability to evaluate long-term dependencies and\nextrapolate meaning even when text is poorly written. Large pretrained\ntransformer-based language models have dominated the current state-of-the-art\nin many NLP tasks, however, the computational requirements of these models make\nthem expensive to deploy in practice. The goal of this paper is to challenge\nthe paradigm in NLP that bigger is better when it comes to AES. To do this, we\nevaluate the performance of several fine-tuned pretrained NLP models with a\nmodest number of parameters on an AES dataset. By ensembling our models, we\nachieve excellent results with fewer parameters than most pretrained\ntransformer-based models.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:28:39 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Ormerod", "Christopher M", ""], ["Malhotra", "Akanksha", ""], ["Jafari", "Amir", ""]]}, {"id": "2102.13139", "submitter": "Milos Jovanovik", "authors": "Nasi Jofche, Kostadin Mishev, Riste Stojanov, Milos Jovanovik, Dimitar\n  Trajanov", "title": "PharmKE: Knowledge Extraction Platform for Pharmaceutical Texts using\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of recognizing named entities in a given text has been a very\ndynamic field in recent years. This is due to the advances in neural network\narchitectures, increase of computing power and the availability of diverse\nlabeled datasets, which deliver pre-trained, highly accurate models. These\ntasks are generally focused on tagging common entities, but domain-specific\nuse-cases require tagging custom entities which are not part of the pre-trained\nmodels. This can be solved by either fine-tuning the pre-trained models, or by\ntraining custom models. The main challenge lies in obtaining reliable labeled\ntraining and test datasets, and manual labeling would be a highly tedious task.\n  In this paper we present PharmKE, a text analysis platform focused on the\npharmaceutical domain, which applies deep learning through several stages for\nthorough semantic analysis of pharmaceutical articles. It performs text\nclassification using state-of-the-art transfer learning models, and thoroughly\nintegrates the results obtained through a proposed methodology. The methodology\nis used to create accurately labeled training and test datasets, which are then\nused to train models for custom entity labeling tasks, centered on the\npharmaceutical domain. The obtained results are compared to the fine-tuned BERT\nand BioBERT models trained on the same dataset. Additionally, the PharmKE\nplatform integrates the results obtained from named entity recognition tasks to\nresolve co-references of entities and analyze the semantic relations in every\nsentence, thus setting up a baseline for additional text analysis tasks, such\nas question answering and fact extraction. The recognized entities are also\nused to expand the knowledge graph generated by DBpedia Spotlight for a given\npharmaceutical text.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:36:35 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Jofche", "Nasi", ""], ["Mishev", "Kostadin", ""], ["Stojanov", "Riste", ""], ["Jovanovik", "Milos", ""], ["Trajanov", "Dimitar", ""]]}, {"id": "2102.13196", "submitter": "Alexander M. Rush", "authors": "David Chiang, Alexander M. Rush, Boaz Barak", "title": "Named Tensor Notation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a notation for tensors with named axes, which relieves the author,\nreader, and future implementers from the burden of keeping track of the order\nof axes and the purpose of each. It also makes it easy to extend operations on\nlow-order tensors to higher order ones (e.g., to extend an operation on images\nto minibatches of images, or extend the attention mechanism to multiple\nattention heads). After a brief overview of our notation, we illustrate it\nthrough several examples from modern machine learning, from building blocks\nlike attention and convolution to full models like Transformers and LeNet.\nFinally, we give formal definitions and describe some extensions. Our proposals\nbuild on ideas from many previous papers and software libraries. We hope that\nthis document will encourage more authors to use named tensors, resulting in\nclearer papers and less bug-prone implementations.\n  The source code for this document can be found at\nhttps://github.com/namedtensor/notation/. We invite anyone to make comments on\nthis proposal by submitting issues or pull requests on this repository.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 22:21:30 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Chiang", "David", ""], ["Rush", "Alexander M.", ""], ["Barak", "Boaz", ""]]}, {"id": "2102.13247", "submitter": "Yury Zemlyanskiy", "authors": "Yury Zemlyanskiy, Sudeep Gandhe, Ruining He, Bhargav Kanagal, Anirudh\n  Ravula, Juraj Gottweis, Fei Sha and Ilya Eckstein", "title": "DOCENT: Learning Self-Supervised Entity Representations from Large\n  Document Collections", "comments": "To appear in the proceedings of EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores learning rich self-supervised entity representations from\nlarge amounts of the associated text. Once pre-trained, these models become\napplicable to multiple entity-centric tasks such as ranked retrieval, knowledge\nbase completion, question answering, and more. Unlike other methods that\nharvest self-supervision signals based merely on a local context within a\nsentence, we radically expand the notion of context to include any available\ntext related to an entity. This enables a new class of powerful, high-capacity\nrepresentations that can ultimately distill much of the useful information\nabout an entity from multiple text sources, without any human supervision.\n  We present several training strategies that, unlike prior approaches, learn\nto jointly predict words and entities -- strategies we compare experimentally\non downstream tasks in the TV-Movies domain, such as MovieLens tag prediction\nfrom user reviews and natural language movie search. As evidenced by results,\nour models match or outperform competitive baselines, sometimes with little or\nno fine-tuning, and can scale to very large corpora.\n  Finally, we make our datasets and pre-trained models publicly available. This\nincludes Reviews2Movielens (see https://goo.gle/research-docent ), mapping the\nup to 1B word corpus of Amazon movie reviews (He and McAuley, 2016) to\nMovieLens tags (Harper and Konstan, 2016), as well as Reddit Movie Suggestions\n(see https://urikz.github.io/docent ) with natural language queries and\ncorresponding community recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 01:00:12 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zemlyanskiy", "Yury", ""], ["Gandhe", "Sudeep", ""], ["He", "Ruining", ""], ["Kanagal", "Bhargav", ""], ["Ravula", "Anirudh", ""], ["Gottweis", "Juraj", ""], ["Sha", "Fei", ""], ["Eckstein", "Ilya", ""]]}, {"id": "2102.13249", "submitter": "Shubham Toshniwal", "authors": "Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel", "title": "Learning Chess Blindfolded: Evaluating Language Models on State Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer language models have made tremendous strides in natural language\nunderstanding tasks. However, the complexity of natural language makes it\nchallenging to ascertain how accurately these models are tracking the world\nstate underlying the text. Motivated by this issue, we consider the task of\nlanguage modeling for the game of chess. Unlike natural language, chess\nnotations describe a simple, constrained, and deterministic domain. Moreover,\nwe observe that the appropriate choice of chess notation allows for directly\nprobing the world state, without requiring any additional probing-related\nmachinery. We find that: (a) With enough training data, transformer language\nmodels can learn to track pieces and predict legal moves with high accuracy\nwhen trained solely on move sequences. (b) For small training sets providing\naccess to board state information during training can yield significant\nimprovements. (c) The success of transformer language models is dependent on\naccess to the entire game history i.e. \"full attention\". Approximating this\nfull attention results in a significant performance drop. We propose this\ntestbed as a benchmark for future work on the development and analysis of\ntransformer language models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 01:16:23 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Toshniwal", "Shubham", ""], ["Wiseman", "Sam", ""], ["Livescu", "Karen", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2102.13355", "submitter": "Andreas Liesenfeld", "authors": "Andreas Liesenfeld, G\\'abor Parti, Yu-Yin Hsu, Chu-Ren Huang", "title": "Predicting gender and age categories in English conversations using\n  lexical, non-lexical, and turn-taking features", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines gender and age salience and (stereo)typicality in British\nEnglish talk with the aim to predict gender and age categories based on\nlexical, phrasal and turn-taking features. We examine the SpokenBNC, a corpus\nof around 11.4 million words of British English conversations and identify\nbehavioural differences between speakers that are labelled for gender and age\ncategories. We explore differences in language use and turn-taking dynamics and\nidentify a range of characteristics that set the categories apart. We find that\nfemale speakers tend to produce more and slightly longer turns, while turns by\nmale speakers feature a higher type-token ratio and a distinct range of minimal\nparticles such as \"eh\", \"uh\" and \"em\". Across age groups, we observe, for\ninstance, that swear words and laughter characterize young speakers' talk,\nwhile old speakers tend to produce more truncated words. We then use the\nobserved characteristics to predict gender and age labels of speakers per\nconversation and per turn as a classification task, showing that non-lexical\nutterances such as minimal particles that are usually left out of dialog data\ncan contribute to setting the categories apart.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 08:23:08 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Liesenfeld", "Andreas", ""], ["Parti", "G\u00e1bor", ""], ["Hsu", "Yu-Yin", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "2102.13395", "submitter": "Congcong Wang", "authors": "Congcong Wang, David Lillis", "title": "Multi-task transfer learning for finding actionable information from\n  crisis-related messages on social media", "comments": "8 pages, TREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Incident streams (IS) track is a research challenge aimed at finding\nimportant information from social media during crises for emergency response\npurposes. More specifically, given a stream of crisis-related tweets, the IS\nchallenge asks a participating system to 1) classify what the types of users'\nconcerns or needs are expressed in each tweet, known as the information type\n(IT) classification task and 2) estimate how critical each tweet is with regard\nto emergency response, known as the priority level prediction task. In this\npaper, we describe our multi-task transfer learning approach for this\nchallenge. Our approach leverages state-of-the-art transformer models including\nboth encoder-based models such as BERT and a sequence-to-sequence based T5 for\njoint transfer learning on the two tasks. Based on this approach, we submitted\nseveral runs to the track. The returned evaluation results show that our runs\nsubstantially outperform other participating runs in both IT classification and\npriority level prediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:11:33 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wang", "Congcong", ""], ["Lillis", "David", ""]]}, {"id": "2102.13461", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Daniel Buschek", "title": "Methods for the Design and Evaluation of HCI+NLP Systems", "comments": "Accepted at the EACL 2021 Workshop on Bridging Human-Computer\n  Interaction and Natural Language Processing (HCI+NLP Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  HCI and NLP traditionally focus on different evaluation methods. While HCI\ninvolves a small number of people directly and deeply, NLP traditionally relies\non standardized benchmark evaluations that involve a larger number of people\nindirectly. We present five methodological proposals at the intersection of HCI\nand NLP and situate them in the context of ML-based NLP models. Our goal is to\nfoster interdisciplinary collaboration and progress in both fields by\nemphasizing what the fields can learn from each other.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 13:37:10 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Heuer", "Hendrik", ""], ["Buschek", "Daniel", ""]]}, {"id": "2102.13468", "submitter": "Bj\\\"orn Schuller", "authors": "Bj\\\"orn W. Schuller, Anton Batliner, Christian Bergler, Cecilia\n  Mascolo, Jing Han, Iulia Lefter, Heysem Kaya, Shahin Amiriparian, Alice\n  Baird, Lukas Stappen, Sandra Ottl, Maurice Gerczuk, Panagiotis Tzirakis,\n  Chlo\\\"e Brown, Jagmohan Chauhan, Andreas Grammenos, Apinan Hasthanasombat,\n  Dimitris Spathis, Tong Xia, Pietro Cicuta, Leon J. M. Rothkrantz, Joeri\n  Zwerts, Jelle Treep, Casper Kaandorp", "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19\n  Cough, COVID-19 Speech, Escalation & Primates", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The INTERSPEECH 2021 Computational Paralinguistics Challenge addresses four\ndifferent problems for the first time in a research competition under\nwell-defined conditions: In the COVID-19 Cough and COVID-19 Speech\nSub-Challenges, a binary classification on COVID-19 infection has to be made\nbased on coughing sounds and speech; in the Escalation SubChallenge, a\nthree-way assessment of the level of escalation in a dialogue is featured; and\nin the Primates Sub-Challenge, four species vs background need to be\nclassified. We describe the Sub-Challenges, baseline feature extraction, and\nclassifiers based on the 'usual' COMPARE and BoAW features as well as deep\nunsupervised representation learning using the AuDeep toolkit, and deep feature\nextraction from pre-trained CNNs using the Deep Spectrum toolkit; in addition,\nwe add deep end-to-end sequential modelling, and partially linguistic analysis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 21:39:59 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Schuller", "Bj\u00f6rn W.", ""], ["Batliner", "Anton", ""], ["Bergler", "Christian", ""], ["Mascolo", "Cecilia", ""], ["Han", "Jing", ""], ["Lefter", "Iulia", ""], ["Kaya", "Heysem", ""], ["Amiriparian", "Shahin", ""], ["Baird", "Alice", ""], ["Stappen", "Lukas", ""], ["Ottl", "Sandra", ""], ["Gerczuk", "Maurice", ""], ["Tzirakis", "Panagiotis", ""], ["Brown", "Chlo\u00eb", ""], ["Chauhan", "Jagmohan", ""], ["Grammenos", "Andreas", ""], ["Hasthanasombat", "Apinan", ""], ["Spathis", "Dimitris", ""], ["Xia", "Tong", ""], ["Cicuta", "Pietro", ""], ["Rothkrantz", "Leon J. M.", ""], ["Zwerts", "Joeri", ""], ["Treep", "Jelle", ""], ["Kaandorp", "Casper", ""]]}, {"id": "2102.13549", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Ankur Bapna, Melvin Johnson, Orhan Firat", "title": "Gradient-guided Loss Masking for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To mitigate the negative effect of low quality training data on the\nperformance of neural machine translation models, most existing strategies\nfocus on filtering out harmful data before training starts. In this paper, we\nexplore strategies that dynamically optimize data usage during the training\nprocess using the model's gradients on a small set of clean data. At each\ntraining step, our algorithm calculates the gradient alignment between the\ntraining data and the clean data to mask out data with negative alignment. Our\nmethod has a natural intuition: good training data should update the model\nparameters in a similar direction as the clean data. Experiments on three WMT\nlanguage pairs show that our method brings significant improvement over strong\nbaselines, and the improvements are generalizable across test data from\ndifferent domains.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:41:48 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wang", "Xinyi", ""], ["Bapna", "Ankur", ""], ["Johnson", "Melvin", ""], ["Firat", "Orhan", ""]]}, {"id": "2102.13558", "submitter": "Hao Zhang", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Liangli Zhen, Joey Tianyi Zhou, Rick\n  Siow Mong Goh", "title": "Natural Language Video Localization: A Revisit in Span-based Question\n  Answering Framework", "comments": "15 pages, 18 figures, and 10 tables. Accepted by IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (TPAMI). arXiv admin note:\n  substantial text overlap with arXiv:2004.13931", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3060449", "report-no": "TPAMI-2020-09-1337.R1", "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Video Localization (NLVL) aims to locate a target moment\nfrom an untrimmed video that semantically corresponds to a text query. Existing\napproaches mainly solve the NLVL problem from the perspective of computer\nvision by formulating it as ranking, anchor, or regression tasks. These methods\nsuffer from large performance degradation when localizing on long videos. In\nthis work, we address the NLVL from a new perspective, i.e., span-based\nquestion answering (QA), by treating the input video as a text passage. We\npropose a video span localizing network (VSLNet), on top of the standard\nspan-based QA framework (named VSLBase), to address NLVL. VSLNet tackles the\ndifferences between NLVL and span-based QA through a simple yet effective\nquery-guided highlighting (QGH) strategy. QGH guides VSLNet to search for the\nmatching video span within a highlighted region. To address the performance\ndegradation on long videos, we further extend VSLNet to VSLNet-L by applying a\nmulti-scale split-and-concatenation strategy. VSLNet-L first splits the\nuntrimmed video into short clip segments; then, it predicts which clip segment\ncontains the target moment and suppresses the importance of other segments.\nFinally, the clip segments are concatenated, with different confidences, to\nlocate the target moment accurately. Extensive experiments on three benchmark\ndatasets show that the proposed VSLNet and VSLNet-L outperform the\nstate-of-the-art methods; VSLNet-L addresses the issue of performance\ndegradation on long videos. Our study suggests that the span-based QA framework\nis an effective strategy to solve the NLVL problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 15:57:59 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 07:58:49 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 09:42:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhang", "Hao", ""], ["Sun", "Aixin", ""], ["Jing", "Wei", ""], ["Zhen", "Liangli", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2102.13589", "submitter": "Mathilde Veron", "authors": "Mathilde Veron, Sophie Rosset, Olivier Galibert, Guillaume Bernard", "title": "Evaluate On-the-job Learning Dialogue Systems and a Case Study for\n  Natural Language Understanding", "comments": "Accepted to NeurIPS 2020 Human in the Loop Dialogue Systems Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  On-the-job learning consists in continuously learning while being used in\nproduction, in an open environment, meaning that the system has to deal on its\nown with situations and elements never seen before. The kind of systems that\nseem to be especially adapted to on-the-job learning are dialogue systems,\nsince they can take advantage of their interactions with users to collect\nfeedback to adapt and improve their components over time. Some dialogue systems\nperforming on-the-job learning have been built and evaluated but no general\nmethodology has yet been defined. Thus in this paper, we propose a first\ngeneral methodology for evaluating on-the-job learning dialogue systems. We\nalso describe a task-oriented dialogue system which improves on-the-job its\nnatural language component through its user interactions. We finally evaluate\nour system with the described methodology.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 16:54:16 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Veron", "Mathilde", ""], ["Rosset", "Sophie", ""], ["Galibert", "Olivier", ""], ["Bernard", "Guillaume", ""]]}, {"id": "2102.13622", "submitter": "Pavithra Rajendran", "authors": "Pavithra Rajendran, Alexandros Zenonos, Josh Spear, Rebecca Pope", "title": "A Meta-embedding-based Ensemble Approach for ICD Coding Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  International Classification of Diseases (ICD) are the de facto codes used\nglobally for clinical coding. These codes enable healthcare providers to claim\nreimbursement and facilitate efficient storage and retrieval of diagnostic\ninformation. The problem of automatically assigning ICD codes has been\napproached in literature as a multilabel classification, using neural models on\nunstructured data. Our proposed approach enhances the performance of neural\nmodels by effectively training word vectors using routine medical data as well\nas external knowledge from scientific articles. Furthermore, we exploit the\ngeometric properties of the two sets of word vectors and combine them into a\ncommon dimensional space, using meta-embedding techniques. We demonstrate the\nefficacy of this approach for a multimodal setting, using unstructured and\nstructured information. We empirically show that our approach improves the\ncurrent state-of-the-art deep learning architectures and benefits ensemble\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 17:49:58 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Rajendran", "Pavithra", ""], ["Zenonos", "Alexandros", ""], ["Spear", "Josh", ""], ["Pope", "Rebecca", ""]]}]