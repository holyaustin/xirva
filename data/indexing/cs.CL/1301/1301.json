[{"id": "1301.0570", "submitter": "Joshua Goodman", "authors": "Joshua Goodman", "title": "Reduction of Maximum Entropy Models to Hidden Markov Models", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-179-186", "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that maximum entropy (maxent) models can be modeled with certain\nkinds of HMMs, allowing us to construct maxent models with hidden variables,\nhidden state sequences, or other characteristics. The models can be trained\nusing the forward-backward algorithm. While the results are primarily of\ntheoretical interest, unifying apparently unrelated concepts, we also give\nexperimental results for a maxent model with a hidden variable on a word\ndisambiguation task; the model outperforms standard techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:56:22 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Goodman", "Joshua", ""]]}, {"id": "1301.0722", "submitter": "Stefan Gerdjikov", "authors": "Stefan Gerdjikov, Stoyan Mihov, Petar Mitankin, Klaus U. Schulz", "title": "Good parts first - a new algorithm for approximate search in lexica and\n  string databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new efficient method for approximate search in electronic\nlexica. Given an input string (the pattern) and a similarity threshold, the\nalgorithm retrieves all entries of the lexicon that are sufficiently similar to\nthe pattern. Search is organized in subsearches that always start with an exact\npartial match where a substring of the input pattern is aligned with a\nsubstring of a lexicon word. Afterwards this partial match is extended stepwise\nto larger substrings. For aligning further parts of the pattern with\ncorresponding parts of lexicon entries, more errors are tolerated at each\nsubsequent step. For supporting this alignment order, which may start at any\npart of the pattern, the lexicon is represented as a structure that enables\nimmediate access to any substring of a lexicon word and permits the extension\nof such substrings in both directions. Experimental evaluations of the\napproximate search procedure are given that show significant efficiency\nimprovements compared to existing techniques. Since the technique can be used\nfor large error bounds it offers interesting possibilities for approximate\nsearch in special collections of \"long\" strings, such as phrases, sentences, or\nbook ti\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2013 13:45:35 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 10:53:17 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Gerdjikov", "Stefan", ""], ["Mihov", "Stoyan", ""], ["Mitankin", "Petar", ""], ["Schulz", "Klaus U.", ""]]}, {"id": "1301.1429", "submitter": "Christian Alis", "authors": "Christian M. Alis, May T. Lim", "title": "Adaptation of fictional and online conversations to communication media", "comments": null, "journal-ref": "Eur. Phys. J. B, vol. 85, no. 12, pp. 1-7, 2012", "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversations allow the quick transfer of short bits of information and it is\nreasonable to expect that changes in communication medium affect how we\nconverse. Using conversations in works of fiction and in an online social\nnetworking platform, we show that the utterance length of conversations is\nslowly shortening with time but adapts more strongly to the constraints of the\ncommunication medium. This indicates that the introduction of any new medium of\ncommunication can affect the way natural language evolves.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 07:11:19 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Alis", "Christian M.", ""], ["Lim", "May T.", ""]]}, {"id": "1301.1950", "submitter": "Bogdan Patrut", "authors": "Bogdan Patrut", "title": "Syntactic Analysis Based on Morphological Characteristic Features of the\n  Romanian Language", "comments": "13 pages, 3 figures, DIASEXP, International Journal on Natural\n  Language Computing, 2012, Volume 1, Number 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper refers to the syntactic analysis of phrases in Romanian, as an\nimportant process of natural language processing. We will suggest a real-time\nsolution, based on the idea of using some words or groups of words that\nindicate grammatical category; and some specific endings of some parts of\nsentence. Our idea is based on some characteristics of the Romanian language,\nwhere some prepositions, adverbs or some specific endings can provide a lot of\ninformation about the structure of a complex sentence. Such characteristics can\nbe found in other languages, too, such as French. Using a special grammar, we\ndeveloped a system (DIASEXP) that can perform a dialogue in natural language\nwith assertive and interogative sentences about a \"story\" (a set of sentences\ndescribing some events from the real life).\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 19:17:31 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Patrut", "Bogdan", ""]]}, {"id": "1301.2405", "submitter": "Gelila Tilahun", "authors": "Gelila Tilahun, Andrey Feuerverger, Michael Gervers", "title": "Dating medieval English charters", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS566 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1615-1640", "doi": "10.1214/12-AOAS566", "report-no": "IMS-AOAS-AOAS566", "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deeds, or charters, dealing with property rights, provide a continuous\ndocumentation which can be used by historians to study the evolution of social,\neconomic and political changes. This study is concerned with charters (written\nin Latin) dating from the tenth through early fourteenth centuries in England.\nOf these, at least one million were left undated, largely due to administrative\nchanges introduced by William the Conqueror in 1066. Correctly dating such\ncharters is of vital importance in the study of English medieval history. This\npaper is concerned with computer-automated statistical methods for dating such\ndocument collections, with the goal of reducing the considerable efforts\nrequired to date them manually and of improving the accuracy of assigned dates.\nProposed methods are based on such data as the variation over time of word and\nphrase usage, and on measures of distance between documents. The extensive (and\ndated) Documents of Early England Data Set (DEEDS) maintained at the University\nof Toronto was used for this purpose.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 07:46:27 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Tilahun", "Gelila", ""], ["Feuerverger", "Andrey", ""], ["Gervers", "Michael", ""]]}, {"id": "1301.2444", "submitter": "Laurent Romary", "authors": "Laurent Romary (ALPAGE, CMB)", "title": "TEI and LMF crosswalks", "comments": null, "journal-ref": "JLCL - Journal for Language Technology and Computational\n  Linguistics, 2015, 30 (1)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper explores various arguments in favour of making the Text\nEncoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO\nstandard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the\nissues that would have to be resolved in order to reach an appropriate\nimplementation of these ideas, in particular in terms of infor-mational\ncoverage. We show how the customisation facilities offered by the TEI\nguidelines can provide an adequate background, not only to cover missing\ncomponents within the current Dictionary chapter of the TEI guidelines, but\nalso to allow specific lexical projects to deal with local constraints. We\nexpect this proposal to be a basis for a future ISO project in the context of\nthe on going revision of LMF.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 10:38:09 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 17:37:02 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 07:37:59 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Romary", "Laurent", "", "ALPAGE, CMB"]]}, {"id": "1301.2466", "submitter": "Oleg Sychev", "authors": "Oleg Sychev, Dmitry Mamontov", "title": "Determining token sequence mistakes in responses to questions with open\n  text answer", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning grammar of the new language, a teacher should routinely check\nstudent's exercises for grammatical correctness. The paper describes a method\nof automatically detecting and reporting grammar mistakes, regarding an order\nof tokens in the response. It could report extra tokens, missing tokens and\nmisplaced tokens. The method is useful when teaching language, where order of\ntokens is important, which includes most formal languages and some natural ones\n(like English). The method was implemented in a question type plug-in\nCorrectWriting for the widely used learning manage system Moodle.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 12:02:23 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Sychev", "Oleg", ""], ["Mamontov", "Dmitry", ""]]}, {"id": "1301.2811", "submitter": "Christian Scheible", "authors": "Christian Scheible and Hinrich Schuetze", "title": "Cutting Recursive Autoencoder Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning models enjoy considerable success in Natural Language\nProcessing. While deep architectures produce useful representations that lead\nto improvements in various tasks, they are often difficult to interpret. This\nmakes the analysis of learned structures particularly difficult. In this paper,\nwe rely on empirical tests to see whether a particular structure makes sense.\nWe present an analysis of the Semi-Supervised Recursive Autoencoder, a\nwell-known model that produces structural representations of text. We show that\nfor certain tasks, the structure of the autoencoder can be significantly\nreduced without loss of classification accuracy and we evaluate the produced\nstructures using human judgment.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2013 19:33:31 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2013 09:09:08 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2013 12:33:50 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Scheible", "Christian", ""], ["Schuetze", "Hinrich", ""]]}, {"id": "1301.2857", "submitter": "Rami Al-Rfou'", "authors": "Rami Al-Rfou' and Steven Skiena", "title": "SpeedRead: A Fast Named Entity Recognition Pipeline", "comments": "Long paper at COLING 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online content analysis employs algorithmic methods to identify entities in\nunstructured text. Both machine learning and knowledge-base approaches lie at\nthe foundation of contemporary named entities extraction systems. However, the\nprogress in deploying these approaches on web-scale has been been hampered by\nthe computational cost of NLP over massive text corpora. We present SpeedRead\n(SR), a named entity recognition pipeline that runs at least 10 times faster\nthan Stanford NLP pipeline. This pipeline consists of a high performance Penn\nTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)\ntagger and knowledge-based named entity recognizer.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 04:01:25 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Al-Rfou'", "Rami", ""], ["Skiena", "Steven", ""]]}, {"id": "1301.3214", "submitter": "Seungyeon Kim", "authors": "Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa", "title": "The Manifold of Human Emotions", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis predicts the presence of positive or negative emotions in\na text document. In this paper, we consider higher dimensional extensions of\nthe sentiment concept, which represent a richer set of human emotions. Our\napproach goes beyond previous work in that our model contains a continuous\nmanifold rather than a finite set of human emotions. We investigate the\nresulting model, compare it to psychological observations, and explore its\npredictive capabilities.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 03:45:27 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Kim", "Seungyeon", ""], ["Li", "Fuxin", ""], ["Lebanon", "Guy", ""], ["Essa", "Irfan", ""]]}, {"id": "1301.3226", "submitter": "Rami Al-Rfou'", "authors": "Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena", "title": "The Expressive Power of Word Embeddings", "comments": "submitted to ICML 2013, Deep Learning for Audio, Speech and Language\n  Processing Workshop. 8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to better understand the difference in quality of the several\npublicly released embeddings. We propose several tasks that help to distinguish\nthe characteristics of different embeddings. Our evaluation of sentiment\npolarity and synonym/antonym relations shows that embeddings are able to\ncapture surprisingly nuanced semantics even in the absence of sentence\nstructure. Moreover, benchmarking the embeddings shows great variance in\nquality and characteristics of the semantics captured by the tested embeddings.\nFinally, we show the impact of varying the number of dimensions and the\nresolution of each dimension on the effective useful features captured by the\nembedding space. Our contributions highlight the importance of embeddings for\nNLP tasks and the effect of their quality on the final results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 04:52:10 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2013 21:44:29 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2013 17:08:14 GMT"}, {"version": "v4", "created": "Wed, 29 May 2013 21:06:09 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Chen", "Yanqing", ""], ["Perozzi", "Bryan", ""], ["Al-Rfou", "Rami", ""], ["Skiena", "Steven", ""]]}, {"id": "1301.3547", "submitter": "Benjamin Englard", "authors": "Benjamin Englard", "title": "A Rhetorical Analysis Approach to Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this research was to find a way to extend the capabilities of\ncomputers through the processing of language in a more human way, and present\napplications which demonstrate the power of this method. This research presents\na novel approach, Rhetorical Analysis, to solving problems in Natural Language\nProcessing (NLP). The main benefit of Rhetorical Analysis, as opposed to\nprevious approaches, is that it does not require the accumulation of large sets\nof training data, but can be used to solve a multitude of problems within the\nfield of NLP. The NLP problems investigated with Rhetorical Analysis were the\nAuthor Identification problem - predicting the author of a piece of text based\non its rhetorical strategies, Election Prediction - predicting the winner of a\npresidential candidate's re-election campaign based on rhetorical strategies\nwithin that president's inaugural address, Natural Language Generation - having\na computer produce text containing rhetorical strategies, and Document\nSummarization. The results of this research indicate that an Author\nIdentification system based on Rhetorical Analysis could predict the correct\nauthor 100% of the time, that a re-election predictor based on Rhetorical\nAnalysis could predict the correct winner of a re-election campaign 55% of the\ntime, that a Natural Language Generation system based on Rhetorical Analysis\ncould output text with up to 87.3% similarity to Shakespeare in style, and that\na Document Summarization system based on Rhetorical Analysis could extract\nhighly relevant sentences. Overall, this study demonstrated that Rhetorical\nAnalysis could be a useful approach to solving problems in NLP.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 01:42:53 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Englard", "Benjamin", ""]]}, {"id": "1301.3605", "submitter": "Michael Seltzer", "authors": "Dong Yu, Michael L. Seltzer, Jinyu Li, Jui-Ting Huang, Frank Seide", "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition\n  Tasks", "comments": "ICLR 2013, 9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that deep neural networks (DNNs) perform\nsignificantly better than shallow networks and Gaussian mixture models (GMMs)\non large vocabulary speech recognition tasks. In this paper, we argue that the\nimproved accuracy achieved by the DNNs is the result of their ability to\nextract discriminative internal representations that are robust to the many\nsources of variability in speech signals. We show that these representations\nbecome increasingly insensitive to small perturbations in the input with\nincreasing network depth, which leads to better speech recognition performance\nwith deeper networks. We also show that DNNs cannot extrapolate to test samples\nthat are substantially different from the training examples. If the training\ndata are sufficiently representative, however, internal features learned by the\nDNN are relatively stable with respect to speaker differences, bandwidth\ndifferences, and environment distortion. This enables DNN-based recognizers to\nperform as well or better than state-of-the-art systems based on GMMs or\nshallow networks without the need for explicit model adaptation or feature\nnormalization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 07:23:19 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 07:42:07 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2013 19:42:37 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yu", "Dong", ""], ["Seltzer", "Michael L.", ""], ["Li", "Jinyu", ""], ["Huang", "Jui-Ting", ""], ["Seide", "Frank", ""]]}, {"id": "1301.3614", "submitter": "Tsuyoshi Okita", "authors": "Tsuyoshi Okita", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A neural probabilistic language model (NPLM) provides an idea to achieve the\nbetter perplexity than n-gram language model and their smoothed language\nmodels. This paper investigates application area in bilingual NLP, specifically\nStatistical Machine Translation (SMT). We focus on the perspectives that NPLM\nhas potential to open the possibility to complement potentially `huge'\nmonolingual resources into the `resource-constraint' bilingual resources. We\nintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesian\nconstruction. In order to facilitate the application to various tasks, we\npropose the joint space model of ngram-HMM language model. We show an\nexperiment of system combination in the area of SMT. One discovery was that our\ntreatment of noise improved the results 0.20 BLEU points if NPLM is trained in\nrelatively small corpus, in our case 500,000 sentence pairs, which is often the\ncase due to the long training time of NPLM.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 07:56:20 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2013 22:58:41 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 02:42:35 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Okita", "Tsuyoshi", ""]]}, {"id": "1301.3618", "submitter": "Richard Socher", "authors": "Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng", "title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and\n  Semantic Word Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases provide applications with the benefit of easily accessible,\nsystematic relational knowledge but often suffer in practice from their\nincompleteness and lack of knowledge of new entities and relations. Much work\nhas focused on building or extending them by finding patterns in large\nunannotated text corpora. In contrast, here we mainly aim to complete a\nknowledge base by predicting additional true relationships between entities,\nbased on generalizations that can be discerned in the given knowledgebase. We\nintroduce a neural tensor network (NTN) model which predicts new relationship\nentries that can be added to the database. This model can be improved by\ninitializing entity representations with word vectors learned in an\nunsupervised fashion from text, and when doing this, existing relations can\neven be queried for entities that were not present in the database. Our model\ngeneralizes and outperforms existing models for this problem, and can classify\nunseen relationships in WordNet with an accuracy of 75.8%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 08:05:35 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 03:23:26 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Chen", "Danqi", ""], ["Socher", "Richard", ""], ["Manning", "Christopher D.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1301.3627", "submitter": "Hinrich Schuetze", "authors": "Hinrich Schuetze, Christian Scheible", "title": "Two SVDs produce more focal deep learning representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A key characteristic of work on deep learning and neural networks in general\nis that it relies on representations of the input that support generalization,\nrobust inference, domain adaptation and other desirable functionalities. Much\nrecent progress in the field has focused on efficient and effective methods for\ncomputing representations. In this paper, we propose an alternative method that\nis more efficient than prior work and produces representations that have a\nproperty we call focality -- a property we hypothesize to be important for\nneural network representations. The method consists of a simple application of\ntwo consecutive SVDs and is inspired by Anandkumar (2012).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 08:37:39 GMT"}, {"version": "v2", "created": "Sat, 11 May 2013 12:17:44 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Schuetze", "Hinrich", ""], ["Scheible", "Christian", ""]]}, {"id": "1301.3781", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean", "title": "Efficient Estimation of Word Representations in Vector Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 18:24:43 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 21:40:37 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2013 00:30:40 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Mikolov", "Tomas", ""], ["Chen", "Kai", ""], ["Corrado", "Greg", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1301.4432", "submitter": "Paul Vitanyi", "authors": "Anne S. Hsu (Department of Cognitive, Perceptual and Brain Sciences,\n  University College London), Nick Chater (Behavioural Science Group, Warwick\n  Business School, University of Warwick), Paul M.B. Vit\\'anyi (CWI, Amsterdam)", "title": "Language learning from positive evidence, reconsidered: A\n  simplicity-based approach", "comments": "39 pages, pdf, 1 figure", "journal-ref": "A.S. Hsu, N. Chater, P.M.B. Vitanyi, Language learning from\n  positive evidence, reconsidered: A simplicity-based approach. Topics in\n  Cognitive Science, 5:1(2013), 35-55", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children learn their native language by exposure to their linguistic and\ncommunicative environment, but apparently without requiring that their mistakes\nare corrected. Such learning from positive evidence has been viewed as raising\nlogical problems for language acquisition. In particular, without correction,\nhow is the child to recover from conjecturing an over-general grammar, which\nwill be consistent with any sentence that the child hears? There have been many\nproposals concerning how this logical problem can be dissolved. Here, we review\nrecent formal results showing that the learner has sufficient data to learn\nsuccessfully from positive evidence, if it favours the simplest encoding of the\nlinguistic input. Results include the ability to learn a linguistic prediction,\ngrammaticality judgements, language production, and form-meaning mappings. The\nsimplicity approach can also be scaled-down to analyse the ability to learn a\nspecific linguistic constructions, and is amenable to empirical test as a\nframework for describing human language acquisition.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 16:53:13 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Hsu", "Anne S.", "", "Department of Cognitive, Perceptual and Brain Sciences,\n  University College London"], ["Chater", "Nick", "", "Behavioural Science Group, Warwick\n  Business School, University of Warwick"], ["Vit\u00e1nyi", "Paul M. B.", "", "CWI, Amsterdam"]]}, {"id": "1301.4938", "submitter": "Christian Retore", "authors": "Christian Retor\\'e (LaBRI, IRIT)", "title": "A type theoretical framework for natural language semantics: the\n  Montagovian generative lexicon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework, named the Montagovian generative lexicon, for\ncomputing the semantics of natural language sentences, expressed in many sorted\nhigher order logic. Word meaning is depicted by lambda terms of second order\nlambda calculus (Girard's system F) with base types including a type for\npropositions and many types for sorts of a many sorted logic. This framework is\nable to integrate a proper treatment of lexical phenomena into a Montagovian\ncompositional semantics, including the restriction of selection which imposes\nthe nature of the arguments of a predicate, and the possible adaptation of a\nword meaning to some contexts. Among these adaptations of a word's sense to the\ncontext, ontological inclusions are handled by an extension of system F with\ncoercive subtyping that is introduced in the present paper. The benefits of\nthis framework for lexical pragmatics are illustrated on meaning transfers and\ncoercions, on possible and impossible copredication over different senses, on\ndeverbal ambiguities, and on \"fictive motion\". Next we show that the\ncompositional treatment of determiners, quantifiers, plurals,... are finer\ngrained in our framework. We then conclude with the linguistic, logical and\ncomputational perspectives opened by the Montagovian generative lexicon.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 17:42:19 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 23:58:30 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 15:43:25 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Retor\u00e9", "Christian", "", "LaBRI, IRIT"]]}, {"id": "1301.5686", "submitter": "Jeon-Hyung Kang", "authors": "Jeon-Hyung Kang, Jun Ma, Yan Liu", "title": "Transfer Topic Modeling with Ease and Scalability", "comments": "2012 SIAM International Conference on Data Mining (SDM12) Pages:\n  {564-575}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing volume of short texts generated on social media sites, such as\nTwitter or Facebook, creates a great demand for effective and efficient topic\nmodeling approaches. While latent Dirichlet allocation (LDA) can be applied, it\nis not optimal due to its weakness in handling short texts with fast-changing\ntopics and scalability concerns. In this paper, we propose a transfer learning\napproach that utilizes abundant labeled documents from other domains (such as\nYahoo! News or Wikipedia) to improve topic modeling, with better model fitting\nand result interpretation. Specifically, we develop Transfer Hierarchical LDA\n(thLDA) model, which incorporates the label information from other domains via\ninformative priors. In addition, we develop a parallel implementation of our\nmodel for large-scale applications. We demonstrate the effectiveness of our\nthLDA model on both a microblogging dataset and standard text collections\nincluding AP and RCV1 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 02:02:13 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2013 18:00:19 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Kang", "Jeon-Hyung", ""], ["Ma", "Jun", ""], ["Liu", "Yan", ""]]}, {"id": "1301.6939", "submitter": "Edward Grefenstette", "authors": "Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh\n  Sadrzadeh and Marco Baroni", "title": "Multi-Step Regression Learning for Compositional Distributional\n  Semantics", "comments": "10 pages + 1 page references, to be presented at the 10th\n  International Conference on Computational Semantics (IWCS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for compositional distributional semantics related to the\nframework of Coecke et al. (2010), and emulating formal semantics by\nrepresenting functions as tensors and arguments as vectors. We introduce a new\nlearning method for tensors, generalising the approach of Baroni and Zamparelli\n(2010). We evaluate it on two benchmark data sets, and find it to outperform\nexisting leading methods. We argue in our analysis that the nature of this\nlearning method also renders it suitable for solving more subtle problems\ncompositional distributional models might face.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 14:59:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 12:01:23 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Grefenstette", "Edward", ""], ["Dinu", "Georgiana", ""], ["Zhang", "Yao-Zhong", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Baroni", "Marco", ""]]}, {"id": "1301.7382", "submitter": "David Heckerman", "authors": "David Heckerman, Eric J. Horvitz", "title": "Inferring Informational Goals from Free-Text Queries: A Bayesian\n  Approach", "comments": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1998-PG-230-237", "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People using consumer software applications typically do not use technical\njargon when querying an online database of help topics. Rather, they attempt to\ncommunicate their goals with common words and phrases that describe software\nfunctionality in terms of structure and objects they understand. We describe a\nBayesian approach to modeling the relationship between words in a user's query\nfor assistance and the informational goals of the user. After reviewing the\ngeneral method, we describe several extensions that center on integrating\nadditional distinctions and structure about language usage and user goals into\nthe Bayesian models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:04:21 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:11:26 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Heckerman", "David", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1301.7738", "submitter": "Flavio Coelho", "authors": "Fl\\'avio Code\\c{c}o Coelho and Renato Rocha Souza and \\'Alvaro Justen\n  and Fl\\'avio Amieiro and Heliana Mello", "title": "PyPLN: a Distributed Platform for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a distributed platform for Natural Language Processing\ncalled PyPLN. PyPLN leverages a vast array of NLP and text processing open\nsource tools, managing the distribution of the workload on a variety of\nconfigurations: from a single server to a cluster of linux servers. PyPLN is\ndeveloped using Python 2.7.3 but makes it very easy to incorporate other\nsoftwares for specific tasks as long as a linux version is available. PyPLN\nfacilitates analyses both at document and corpus level, simplifying management\nand publication of corpora and analytical results through an easy to use web\ninterface. In the current (beta) release, it supports English and Portuguese\nlanguages with support to other languages planned for future releases. To\nsupport the Portuguese language PyPLN uses the PALAVRAS parser\\citep{Bick2000}.\nCurrently PyPLN offers the following features: Text extraction with encoding\nnormalization (to UTF-8), part-of-speech tagging, token frequency, semantic\nannotation, n-gram extraction, word and sentence repertoire, and full-text\nsearch across corpora. The platform is licensed as GPL-v3.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 20:21:52 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 11:54:23 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Coelho", "Fl\u00e1vio Code\u00e7o", ""], ["Souza", "Renato Rocha", ""], ["Justen", "\u00c1lvaro", ""], ["Amieiro", "Fl\u00e1vio", ""], ["Mello", "Heliana", ""]]}]