[{"id": "1410.0210", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "A Multi-World Approach to Question Answering about Real-World Scenes\n  based on Uncertain Input", "comments": "Published in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for automatically answering questions about images by\nbringing together recent advances from natural language processing and computer\nvision. We combine discrete reasoning with uncertain predictions by a\nmulti-world approach that represents uncertainty about the perceived world in a\nbayesian framework. Our approach can handle human questions of high complexity\nabout realistic scenes and replies with range of answer like counts, object\nclasses, instances and lists of them. The system is directly trained from\nquestion-answer pairs. We establish a first benchmark for this task that can be\nseen as a modern attempt at a visual turing test.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 12:59:16 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 16:29:44 GMT"}, {"version": "v3", "created": "Tue, 11 Nov 2014 12:13:18 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 17:39:10 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1410.0286", "submitter": "Dirk Roorda", "authors": "Dirk Roorda, Gino Kalkman, Martijn Naaijer, Andreas van Cranenburgh", "title": "LAF-Fabric: a data analysis tool for Linguistic Annotation Framework\n  with an application to the Hebrew Bible", "comments": null, "journal-ref": "Computational Linguistics in the Netherlands Journal, Volume 4,\n  December 2014, pp. 105-109", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Linguistic Annotation Framework (LAF) provides a general, extensible\nstand-off markup system for corpora. This paper discusses LAF-Fabric, a new\ntool to analyse LAF resources in general with an extension to process the\nHebrew Bible in particular. We first walk through the history of the Hebrew\nBible as text database in decennium-wide steps. Then we describe how LAF-Fabric\nmay serve as an analysis tool for this corpus. Finally, we describe three\nanalytic projects/workflows that benefit from the new LAF representation:\n  1) the study of linguistic variation: extract cooccurrence data of common\nnouns between the books of the Bible (Martijn Naaijer); 2) the study of the\ngrammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);\n3) construction of a parser of classical Hebrew by Data Oriented Parsing:\ngenerate tree structures from the database (Andreas van Cranenburgh).\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 16:53:35 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Roorda", "Dirk", ""], ["Kalkman", "Gino", ""], ["Naaijer", "Martijn", ""], ["van Cranenburgh", "Andreas", ""]]}, {"id": "1410.0291", "submitter": "Yanchuan Sim", "authors": "Yanchuan Sim", "title": "A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open source morphological analyzer for Japanese nouns, verbs\nand adjectives. The system builds upon the morphological analyzing capabilities\nof MeCab to incorporate finer details of classification such as politeness,\ntense, mood and voice attributes. We implemented our analyzer in the form of a\nfinite state transducer using the open source finite state compiler FOMA\ntoolkit. The source code and tool is available at\nhttps://bitbucket.org/skylander/yc-nlplab/.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 17:03:18 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 03:22:57 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Sim", "Yanchuan", ""]]}, {"id": "1410.0316", "submitter": "Brian Rowe", "authors": "Brian Lee Yung Rowe", "title": "Using social network graph analysis for interest detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A person's interests exist as an internal state and are difficult to define.\nSince only external actions are observable, a proxy must be used that\nrepresents someone's interests. Techniques like collaborative filtering,\nbehavioral targeting, and hashtag analysis implicitly model an individual's\ninterests. I argue that these models are limited to shallow, temporary\ninterests, which do not reflect people's deeper interests or passions. I\npropose an alternative model of interests that takes advantage of a user's\nsocial graph. The basic principle is that people only follow those that\ninterest them, so the social graph is an effective and robust proxy for\npeople's interests.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 18:15:50 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Rowe", "Brian Lee Yung", ""]]}, {"id": "1410.0718", "submitter": "Felix Hill Mr", "authors": "Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin and Yoshua\n  Bengio", "title": "Not All Neural Embeddings are Born Equal", "comments": "4 pages plus 1 page of references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models learn word representations that capture rich\nlinguistic and conceptual information. Here we investigate the embeddings\nlearned by neural machine translation models. We show that translation-based\nembeddings outperform those learned by cutting-edge monolingual models at\nsingle-language tasks requiring knowledge of conceptual similarity and/or\nsyntactic role. The findings suggest that, while monolingual models learn\ninformation about how concepts are related, neural-translation models better\ncapture their true ontological status.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 21:35:35 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 15:58:35 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Hill", "Felix", ""], ["Cho", "KyungHyun", ""], ["Jean", "Sebastien", ""], ["Devin", "Coline", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1410.1080", "submitter": "Vladimir Bochkarev", "authors": "Valery D. Solovyev, Vladimir V. Bochkarev", "title": "Generating abbreviations using Google Books library", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes the original method of creating a dictionary of\nabbreviations based on the Google Books Ngram Corpus. The dictionary of\nabbreviations is designed for Russian, yet as its methodology is universal it\ncan be applied to any language. The dictionary can be used to define the\nfunction of the period during text segmentation in various applied systems of\ntext processing. The article describes difficulties encountered in the process\nof its construction as well as the ways to overcome them. A model of evaluating\na probability of first and second type errors (extraction accuracy and\nfullness) is constructed. Certain statistical data for the use of abbreviations\nare provided.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 19:00:54 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Solovyev", "Valery D.", ""], ["Bochkarev", "Vladimir V.", ""]]}, {"id": "1410.1090", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille", "title": "Explain Images with Multimodal Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel sentence descriptions to explain the content of images. It\ndirectly models the probability distribution of generating a word given\nprevious words and the image. Image descriptions are generated by sampling from\nthis distribution. The model consists of two sub-networks: a deep recurrent\nneural network for sentences and a deep convolutional network for images. These\ntwo sub-networks interact with each other in a multimodal layer to form the\nwhole m-RNN model. The effectiveness of our model is validated on three\nbenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model\noutperforms the state-of-the-art generative method. In addition, the m-RNN\nmodel can be applied to retrieval tasks for retrieving images or sentences, and\nachieves significant performance improvement over the state-of-the-art methods\nwhich directly optimize the ranking objective function for retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 4 Oct 2014 20:24:34 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1410.1135", "submitter": "Ahmed Yousef Y", "authors": "Walaa Medhat, Ahmed H. Yousef, Hoda Korashy", "title": "Corpora Preparation and Stopword List Generation for Arabic data in\n  Social Network", "comments": "Language Engineering Conference 2014, Cairo, Egypt, 1-3 December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a methodology to prepare corpora in Arabic language from\nonline social network (OSN) and review site for Sentiment Analysis (SA) task.\nThe paper also proposes a methodology for generating a stopword list from the\nprepared corpora. The aim of the paper is to investigate the effect of removing\nstopwords on the SA task. The problem is that the stopwords lists generated\nbefore were on Modern Standard Arabic (MSA) which is not the common language\nused in OSN. We have generated a stopword list of Egyptian dialect and a\ncorpus-based list to be used with the OSN corpora. We compare the efficiency of\ntext classification when using the generated lists along with previously\ngenerated lists of MSA and combining the Egyptian dialect list with the MSA\nlist. The text classification was performed using Na\\\"ive Bayes and Decision\nTree classifiers and two feature selection approaches, unigrams and bigram. The\nexperiments show that the general lists containing the Egyptian dialects words\ngive better performance than using lists of MSA stopwords only.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 09:02:31 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Medhat", "Walaa", ""], ["Yousef", "Ahmed H.", ""], ["Korashy", "Hoda", ""]]}, {"id": "1410.2045", "submitter": "Ashis  Mandal", "authors": "Ashis Kumar Mandal and Rikta Sen", "title": "Supervised learning Methods for Bangla Web Document Categorization", "comments": "13 pages, International Journal of Artificial Intelligence &\n  Applications (IJAIA), Vol. 5, No. 5, September 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 10:01:47 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Mandal", "Ashis Kumar", ""], ["Sen", "Rikta", ""]]}, {"id": "1410.2082", "submitter": "Yang Liu", "authors": "Yang Liu, Maosong Sun", "title": "Contrastive Unsupervised Word Alignment with Non-Local Features", "comments": "Added the missing Table 1; corrected a typo in Related Work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word alignment is an important natural language processing task that\nindicates the correspondence between natural languages. Recently, unsupervised\nlearning of log-linear models for word alignment has received considerable\nattention as it combines the merits of generative and discriminative\napproaches. However, a major challenge still remains: it is intractable to\ncalculate the expectations of non-local features that are critical for\ncapturing the divergence between natural languages. We propose a contrastive\napproach that aims to differentiate observed training examples from noises. It\nnot only introduces prior knowledge to guide unsupervised learning but also\ncancels out partition functions. Based on the observation that the probability\nmass of log-linear models for word alignment is usually highly concentrated, we\npropose to use top-n alignments to approximate the expectations with respect to\nposterior distributions. This allows for efficient and accurate calculation of\nexpectations of non-local features. Experiments show that our approach achieves\nsignificant improvements over state-of-the-art unsupervised word alignment\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 12:24:38 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 00:25:46 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Liu", "Yang", ""], ["Sun", "Maosong", ""]]}, {"id": "1410.2149", "submitter": "Roger Bilisoly", "authors": "Roger Bilisoly", "title": "Language-based Examples in the Statistics Classroom", "comments": "Proceedings based on a Joint Statistical Meetings talk in 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics pedagogy values using a variety of examples. Thanks to text\nresources on the Web, and since statistical packages have the ability to\nanalyze string data, it is now easy to use language-based examples in a\nstatistics class. Three such examples are discussed here. First, many types of\nwordplay (e.g., crosswords and hangman) involve finding words with letters that\nsatisfy a certain pattern. Second, linguistics has shown that idiomatic pairs\nof words often appear together more frequently than chance. For example, in the\nBrown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)\nThird, a pangram contains all the letters of the alphabet at least once. These\nare searched for in Charles Dickens' A Christmas Carol, and their lengths are\ncompared to the expected value given by the unequal probability coupon\ncollector's problem as well as simulations.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 01:36:30 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Bilisoly", "Roger", ""]]}, {"id": "1410.2265", "submitter": "Chetan  Kaushik", "authors": "Chetan Kaushik, Atul Mishra", "title": "A Scalable, Lexicon Based Technique for Sentiment Analysis", "comments": "9 pages 1 figure 2 tables", "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol.4, No.5, September 2014", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid increase in the volume of sentiment rich social media on the web has\nresulted in an increased interest among researchers regarding Sentimental\nAnalysis and opinion mining. However, with so much social media available on\nthe web, sentiment analysis is now considered as a big data task. Hence the\nconventional sentiment analysis approaches fails to efficiently handle the vast\namount of sentiment data available now a days. The main focus of the research\nwas to find such a technique that can efficiently perform sentiment analysis on\nbig data sets. A technique that can categorize the text as positive, negative\nand neutral in a fast and accurate manner. In the research, sentiment analysis\nwas performed on a large data set of tweets using Hadoop and the performance of\nthe technique was measured in form of speed and accuracy. The experimental\nresults shows that the technique exhibits very good efficiency in handling big\nsentiment data sets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 20:29:39 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Kaushik", "Chetan", ""], ["Mishra", "Atul", ""]]}, {"id": "1410.2455", "submitter": "Stephan Gouws", "authors": "Stephan Gouws, Yoshua Bengio, Greg Corrado", "title": "BilBOWA: Fast Bilingual Distributed Representations without Word\n  Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple\nand computationally-efficient model for learning bilingual distributed\nrepresentations of words which can scale to large monolingual datasets and does\nnot require word-aligned parallel training data. Instead it trains directly on\nmonolingual data and extracts a bilingual signal from a smaller set of raw-text\nsentence-aligned data. This is achieved using a novel sampled bag-of-words\ncross-lingual objective, which is used to regularize two noise-contrastive\nlanguage models for efficient cross-lingual feature learning. We show that\nbilingual embeddings learned using the proposed model outperform\nstate-of-the-art methods on a cross-lingual document classification task as\nwell as a lexical translation task on WMT11 data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 13:41:18 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 20:52:32 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 05:51:59 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Gouws", "Stephan", ""], ["Bengio", "Yoshua", ""], ["Corrado", "Greg", ""]]}, {"id": "1410.2479", "submitter": "Andreas Schwarz", "authors": "Andreas Schwarz, Christian Huemmer, Roland Maas, Walter Kellermann", "title": "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy\n  and Reverberant Environments", "comments": "accepted for ICASSP2015", "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178798", "report-no": null, "categories": "cs.CL cs.NE cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a spatial diffuseness feature for deep neural network (DNN)-based\nautomatic speech recognition to improve recognition accuracy in reverberant and\nnoisy environments. The feature is computed in real-time from multiple\nmicrophone signals without requiring knowledge or estimation of the direction\nof arrival, and represents the relative amount of diffuse noise in each time\nand frequency bin. It is shown that using the diffuseness feature as an\nadditional input to a DNN-based acoustic model leads to a reduced word error\nrate for the REVERB challenge corpus, both compared to logmelspec features\nextracted from noisy signals, and features enhanced by spectral subtraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:15:42 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 13:54:06 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Schwarz", "Andreas", ""], ["Huemmer", "Christian", ""], ["Maas", "Roland", ""], ["Kellermann", "Walter", ""]]}, {"id": "1410.2646", "submitter": "Amine Chennoufi", "authors": "Mohamed Bebah, Chennoufi Amine, Mazroui Azzeddine, Lakhouaja Abdelhak", "title": "Hybrid approaches for automatic vowelization of Arabic texts", "comments": "19 pages", "journal-ref": null, "doi": "10.5121/ijnlc.2014.3404", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid approaches for automatic vowelization of Arabic texts are presented in\nthis article. The process is made up of two modules. In the first one, a\nmorphological analysis of the text words is performed using the open source\nmorphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out\nof context, are its different possible vowelizations. The integration of this\nAnalyzer in our vowelization system required the addition of a lexical database\ncontaining the most frequent words in Arabic language. Using a statistical\napproach based on two hidden Markov models (HMM), the second module aims to\neliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic\nwords are the observed states and the vowelized words are the hidden states.\nThe observed states of the second HMM are identical to those of the first, but\nthe hidden states are the lists of possible diacritics of the word without its\nArabic letters. Our system uses Viterbi algorithm to select the optimal path\namong the solutions proposed by Al Khalil Morpho Sys. Our approach opens an\nimportant way to improve the performance of automatic vowelization of Arabic\ntexts for other uses in automatic natural language processing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 22:56:44 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Bebah", "Mohamed", ""], ["Amine", "Chennoufi", ""], ["Azzeddine", "Mazroui", ""], ["Abdelhak", "Lakhouaja", ""]]}, {"id": "1410.2686", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak", "title": "Polarization Measurement of High Dimensional Social Media Messages With\n  Support Vector Machine Algorithm Using Mapreduce", "comments": "12 pages, in Turkish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new Support Vector Machine (SVM) training\nalgorithm based on distributed MapReduce technique. In literature, there are a\nlots of research that shows us SVM has highest generalization property among\nclassification algorithms used in machine learning area. Also, SVM classifier\nmodel is not affected by correlations of the features. But SVM uses quadratic\noptimization techniques in its training phase. The SVM algorithm is formulated\nas quadratic optimization problem. Quadratic optimization problem has $O(m^3)$\ntime and $O(m^2)$ space complexity, where m is the training set size. The\ncomputation time of SVM training is quadratic in the number of training\ninstances. In this reason, SVM is not a suitable classification algorithm for\nlarge scale dataset classification. To solve this training problem we developed\na new distributed MapReduce method developed. Accordingly, (i) SVM algorithm is\ntrained in distributed dataset individually; (ii) then merge all support\nvectors of classifier model in every trained node; and (iii) iterate these two\nsteps until the classifier model converges to the optimal classifier function.\nIn the implementation phase, large scale social media dataset is presented in\nTFxIDF matrix. The matrix is used for sentiment analysis to get polarization\nvalue. Two and three class models are created for classification method.\nConfusion matrices of each classification model are presented in tables. Social\nmedia messages corpus consists of 108 public and 66 private universities\nmessages in Turkey. Twitter is used for source of corpus. Twitter user messages\nare collected using Twitter Streaming API. Results are shown in graphics and\ntables.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 06:42:25 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 05:56:51 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""]]}, {"id": "1410.2871", "submitter": "Meenakshi Lakshmanan", "authors": "S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan", "title": "An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of\n  Sanskrit Grammar", "comments": null, "journal-ref": "European Journal of Scientific Research, ISSN 1450-216X /\n  1450-202X, Vol. 124, No. 4, September 2014, pp 460-467", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit\nmorphology and phonology. The traditional and modern methods of studying about\neuphonic conjunctions in Sanskrit follow different methodologies. The former\ninvolves a rigorous study of the Paninian system embodied in Panini's\nAshtadhyayi, while the latter usually involves the study of a few important\nsandhi rules with the use of examples. The former is not suitable for\nbeginners, and the latter, not sufficient to gain a comprehensive understanding\nof the operation of sandhi rules. This is so since there are not only numerous\nsandhi rules and exceptions, but also complex precedence rules involved. The\nneed for a new ontology for sandhi-tutoring was hence felt. This work presents\na comprehensive ontology designed to enable a student-user to learn in stages\nall about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar\nand to test and evaluate the progress of the student-user. The ontology forms\nthe basis of a multimedia sandhi tutor that was given to different categories\nof users including Sanskrit scholars for extensive and rigorous testing.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 19:19:31 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 08:10:20 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Raja", "S. V. Kasmir", ""], ["Rajitha", "V.", ""], ["Lakshmanan", "Meenakshi", ""]]}, {"id": "1410.2910", "submitter": "Daoud Clarke", "authors": "Daoud Clarke", "title": "Riesz Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Riesz Logic, whose models are abelian lattice ordered groups,\nwhich generalise Riesz spaces (vector lattices), and show soundness and\ncompleteness. Our motivation is to provide a logic for distributional semantics\nof natural language, where words are typically represented as elements of a\nvector space whose dimensions correspond to contexts in which words may occur.\nThis basis provides a lattice ordering on the space, and this ordering may be\ninterpreted as \"distributional entailment\". Several axioms of Riesz Logic are\nfamiliar from Basic Fuzzy Logic, and we show how the models of these two logics\nmay be related; Riesz Logic may thus be considered a new fuzzy logic. In\naddition to applications in natural language processing, there is potential for\napplying the theory to neuro-fuzzy systems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 21:14:02 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Clarke", "Daoud", ""]]}, {"id": "1410.3460", "submitter": "Junhui Shen sjh", "authors": "Junhui Shen, Peiyan Zhu, Rui Fan, Wei Tan", "title": "Sentiment Analysis based on User Tag for Traditional Chinese Medicine in\n  Weibo", "comments": "7 pages, 8 figures,3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the acceptance of Western culture and science, Traditional Chinese\nMedicine (TCM) has become a controversial issue in China. So, it's important to\nstudy the public's sentiment and opinion on TCM. The rapid development of\nonline social network, such as twitter, make it convenient and efficient to\nsample hundreds of millions of people for the aforementioned sentiment study.\nTo the best of our knowledge, the present work is the first attempt that\napplies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like\nmicroblogging service in China). In our work, firstly we collect tweets topic\nabout TCM from Sina Weibo, and label the tweets as supporting TCM and opposing\nTCM automatically based on user tag. Then, a support vector machine classifier\nhas been built to predict the sentiment of TCM tweets without labels. Finally,\nwe present a method to adjust the classifier result. The performance of\nF-measure attained with our method is 97%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 11:37:37 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Shen", "Junhui", ""], ["Zhu", "Peiyan", ""], ["Fan", "Rui", ""], ["Tan", "Wei", ""]]}, {"id": "1410.3791", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, Steven Skiena", "title": "POLYGLOT-NER: Massive Multilingual Named Entity Recognition", "comments": "9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing diversity of languages used on the web introduces a new level\nof complexity to Information Retrieval (IR) systems. We can no longer assume\nthat textual content is written in one language or even the same language\nfamily. In this paper, we demonstrate how to build massive multilingual\nannotators with minimal human expertise and intervention. We describe a system\nthat builds Named Entity Recognition (NER) annotators for 40 major languages\nusing Wikipedia and Freebase. Our approach does not require NER human annotated\ndatasets or language specific resources like treebanks, parallel corpora, and\northographic rules. The novelty of approach lies therein - using only language\nagnostic techniques, while achieving competitive performance.\n  Our method learns distributed word representations (word embeddings) which\nencode semantic and syntactic features of words in each language. Then, we\nautomatically generate datasets from Wikipedia link structure and Freebase\nattributes. Finally, we apply two preprocessing stages (oversampling and exact\nsurface form matching) which do not require any linguistic expertise.\n  Our evaluation is two fold: First, we demonstrate the system performance on\nhuman annotated datasets. Second, for languages where no gold-standard\nbenchmarks are available, we propose a new method, distant evaluation, based on\nstatistical machine translation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 18:37:32 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Kulkarni", "Vivek", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1410.3916", "submitter": "Jason  Weston", "authors": "Jason Weston, Sumit Chopra, Antoine Bordes", "title": "Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a new class of learning models called memory networks. Memory\nnetworks reason with inference components combined with a long-term memory\ncomponent; they learn how to use these jointly. The long-term memory can be\nread and written to, with the goal of using it for prediction. We investigate\nthese models in the context of question answering (QA) where the long-term\nmemory effectively acts as a (dynamic) knowledge base, and the output is a\ntextual response. We evaluate them on a large-scale QA task, and a smaller, but\nmore complex, toy task generated from a simulated world. In the latter, we show\nthe reasoning power of such models by chaining multiple supporting sentences to\nanswer questions that require understanding the intension of verbs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 03:13:18 GMT"}, {"version": "v10", "created": "Tue, 19 May 2015 21:48:30 GMT"}, {"version": "v11", "created": "Sun, 29 Nov 2015 07:00:41 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 16:33:51 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 03:53:26 GMT"}, {"version": "v4", "created": "Wed, 24 Dec 2014 21:56:07 GMT"}, {"version": "v5", "created": "Wed, 21 Jan 2015 07:26:45 GMT"}, {"version": "v6", "created": "Sat, 7 Feb 2015 01:25:50 GMT"}, {"version": "v7", "created": "Wed, 25 Feb 2015 04:08:49 GMT"}, {"version": "v8", "created": "Sat, 7 Mar 2015 15:29:16 GMT"}, {"version": "v9", "created": "Thu, 9 Apr 2015 20:45:05 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Weston", "Jason", ""], ["Chopra", "Sumit", ""], ["Bordes", "Antoine", ""]]}, {"id": "1410.4176", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Christopher Potts, and Christopher D. Manning", "title": "Learning Distributed Word Representations for Natural Logic Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural logic offers a powerful relational conception of meaning that is a\nnatural counterpart to distributed semantic representations, which have proven\nvaluable in a wide range of sophisticated language tasks. However, it remains\nan open question whether it is possible to train distributed representations to\nsupport the rich, diverse logical reasoning captured by natural logic. We\naddress this question using two neural network-based models for learning\nembeddings: plain neural networks and neural tensor networks. Our experiments\nevaluate the models' ability to learn the basic algebra of natural logic\nrelations from simulated data and from the WordNet noun graph. The overall\npositive results are promising for the future of learned distributed\nrepresentations in the applied modeling of logical semantics.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 19:27:10 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Potts", "Christopher", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1410.4281", "submitter": "Xiangang Li", "authors": "Xiangang Li, Xihong Wu", "title": "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks\n  for Large Vocabulary Speech Recognition", "comments": "submitted to ICASSP 2015 which does not perform blind reviews", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory (LSTM) based acoustic modeling methods have recently\nbeen shown to give state-of-the-art performance on some speech recognition\ntasks. To achieve a further performance improvement, in this research, deep\nextensions on LSTM are investigated considering that deep hierarchical model\nhas turned out to be more efficient than a shallow one. Motivated by previous\nresearch on constructing deep recurrent neural networks (RNNs), alternative\ndeep LSTM architectures are proposed and empirically evaluated on a large\nvocabulary conversational telephone speech recognition task. Meanwhile,\nregarding to multi-GPU devices, the training process for LSTM networks is\nintroduced and discussed. Experimental results demonstrate that the deep LSTM\nnetworks benefit from the depth and yield the state-of-the-art performance on\nthis task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 02:44:41 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 02:23:06 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Li", "Xiangang", ""], ["Wu", "Xihong", ""]]}, {"id": "1410.4445", "submitter": "Massimo Stella", "authors": "Massimo Stella and Markus Brede", "title": "Patterns in the English Language: Phonological Networks, Percolation and\n  Assembly Models", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": "10.1088/1742-5468/2015/05/P05006", "report-no": null, "categories": "cs.CL cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a quantitative framework for the study of\nphonological networks (PNs) for the English language by carrying out principled\ncomparisons to null models, either based on site percolation, randomization\ntechniques, or network growth models. In contrast to previous work, we mainly\nfocus on null models that reproduce lower order characteristics of the\nempirical data. We find that artificial networks matching connectivity\nproperties of the English PN are exceedingly rare: this leads to the hypothesis\nthat the word repertoire might have been assembled over time by preferentially\nintroducing new words which are small modifications of old words. Our null\nmodels are able to explain the \"power-law-like\" part of the degree\ndistributions and generally retrieve qualitative features of the PN such as\nhigh clustering, high assortativity coefficient, and small-world\ncharacteristics. However, the detailed comparison to expectations from null\nmodels also points out significant differences, suggesting the presence of\nadditional constraints in word assembly. Key constraints we identify are the\navoidance of large degrees, the avoidance of triadic closure, and the avoidance\nof large non-percolating clusters.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 14:25:01 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 17:34:24 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 10:28:22 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Stella", "Massimo", ""], ["Brede", "Markus", ""]]}, {"id": "1410.4510", "submitter": "Finale Doshi-Velez", "authors": "Finale Doshi-Velez and Byron Wallace and Ryan Adams", "title": "Graph-Sparse LDA: A Topic Model with Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originally designed to model text, topic modeling has become a powerful tool\nfor uncovering latent structure in domains including medicine, finance, and\nvision. The goals for the model vary depending on the application: in some\ncases, the discovered topics may be used for prediction or some other\ndownstream task. In other cases, the content of the topic itself may be of\nintrinsic scientific interest.\n  Unfortunately, even using modern sparse techniques, the discovered topics are\noften difficult to interpret due to the high dimensionality of the underlying\nspace. To improve topic interpretability, we introduce Graph-Sparse LDA, a\nhierarchical topic model that leverages knowledge of relationships between\nwords (e.g., as encoded by an ontology). In our model, topics are summarized by\na few latent concept-words from the underlying graph that explain the observed\nwords. Graph-Sparse LDA recovers sparse, interpretable summaries on two\nreal-world biomedical datasets while matching state-of-the-art prediction\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 17:35:31 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 16:38:59 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Wallace", "Byron", ""], ["Adams", "Ryan", ""]]}, {"id": "1410.4639", "submitter": "Darryl McAdams", "authors": "Darryl McAdams, Jonathan Sterling", "title": "Dependent Types for Pragmatics", "comments": "This version updates the paper for publication in LEUS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of dependent types for pragmatic phenomena such\nas pronoun binding and presupposition resolution as a type-theoretic\nalternative to formalisms such as Discourse Representation Theory and Dynamic\nSemantics.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 05:19:12 GMT"}, {"version": "v2", "created": "Mon, 20 Oct 2014 02:55:17 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2015 02:15:06 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["McAdams", "Darryl", ""], ["Sterling", "Jonathan", ""]]}, {"id": "1410.4863", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Yassir Elidrissi and Philippe Lenca", "title": "Arabic Language Text Classification Using Dependency Syntax-Based\n  Feature Selection", "comments": "10 pages, 4 figure, accepted at CITALA 2014 (http://www.citala.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of Arabic text classification combining various\ntechniques: (a) tfidf vs. dependency syntax, for feature selection and\nweighting; (b) class association rules vs. support vector machines, for\nclassification. The Arabic text is used in two forms: rootified and lightly\nstemmed. The results we obtain show that lightly stemmed text leads to better\nperformance than rootified text; that class association rules are better suited\nfor small feature sets obtained by dependency syntax constraints; and, finally,\nthat support vector machines are better suited for large feature sets based on\nmorphological feature selection criteria.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 21:05:01 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Haralambous", "Yannis", ""], ["Elidrissi", "Yassir", ""], ["Lenca", "Philippe", ""]]}, {"id": "1410.4868", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Nathaniel W.\n  Filardo, Lori Levin and Christine Piatko", "title": "A Modality Lexicon and its use in Automatic Tagging", "comments": "6 pages, 5 figures; appeared in Proceedings of the Seventh\n  International Conference on Language Resources and Evaluation (LREC'10), May\n  2010", "journal-ref": "In Proceedings of the Seventh International Conference on Language\n  Resources and Evaluation (LREC'10), pages 1402-1407, Valletta, Malta, May\n  2010. European Language Resources Association", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our resource-building results for an eight-week JHU\nHuman Language Technology Center of Excellence Summer Camp for Applied Language\nExploration (SCALE-2009) on Semantically-Informed Machine Translation.\nSpecifically, we describe the construction of a modality annotation scheme, a\nmodality lexicon, and two automated modality taggers that were built using the\nlexicon and annotation scheme. Our annotation scheme is based on identifying\nthree components of modality: a trigger, a target and a holder. We describe how\nour modality lexicon was produced semi-automatically, expanding from an initial\nhand-selected list of modality trigger words and phrases. The resulting\nexpanded modality lexicon is being made publicly available. We demonstrate that\none tagger---a structure-based tagger---results in precision around 86%\n(depending on genre) for tagging of a standard LDC data set. In a machine\ntranslation application, using the structure-based tagger to annotate English\nmodalities on an English-Urdu training corpus improved the translation quality\nscore for Urdu by 0.3 Bleu points in the face of sparse training data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 21:19:15 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Dorr", "Bonnie J.", ""], ["Filardo", "Nathaniel W.", ""], ["Levin", "Lori", ""], ["Piatko", "Christine", ""]]}, {"id": "1410.4966", "submitter": "Shay Cohen", "authors": "Chiraag Lala and Shay B. Cohen", "title": "The Visualization of Change in Word Meaning over Time using Temporal\n  Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a visualization tool that can be used to view the change in\nmeaning of words over time. The tool makes use of existing (static) word\nembedding datasets together with a timestamped $n$-gram corpus to create {\\em\ntemporal} word embeddings.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 14:53:19 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Lala", "Chiraag", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1410.5078", "submitter": "Paolo Pareti Mr.", "authors": "Paolo Pareti and Ewan Klein", "title": "Learning Vague Concepts for the Semantic Web", "comments": "The 10th International Semantic Web Conference (ISWC 2011), Joint\n  Workshop on Knowledge Evolution and Ontology Dynamics. Bonn, Germany (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Ontologies can be a powerful tool for structuring knowledge, and they are\ncurrently the subject of extensive research. Updating the contents of an\nontology or improving its interoperability with other ontologies is an\nimportant but difficult process. In this paper, we focus on the presence of\nvague concepts, which are pervasive in natural language, within the framework\nof formal ontologies. We will adopt a framework in which vagueness is captured\nvia numerical restrictions that can be automatically adjusted. Since updating\nvague concepts, either through ontology alignment or ontology evolution, can\nlead to inconsistent sets of axioms, we define and implement a method to\ndetecting and repairing such inconsistencies in a local fashion.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 14:34:23 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Pareti", "Paolo", ""], ["Klein", "Ewan", ""]]}, {"id": "1410.5485", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "A stronger null hypothesis for crossing dependencies", "comments": "typos corrected and English improved", "journal-ref": "(2014). Europhysics Letters 108 (5), 58003", "doi": "10.1209/0295-5075/108/58003", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The syntactic structure of a sentence can be modeled as a tree where vertices\nare words and edges indicate syntactic dependencies between words. It is\nwell-known that those edges normally do not cross when drawn over the sentence.\nHere a new null hypothesis for the number of edge crossings of a sentence is\npresented. That null hypothesis takes into account the length of the pair of\nedges that may cross and predicts the relative number of crossings in random\ntrees with a small error, suggesting that a ban of crossings or a principle of\nminimization of crossings are not needed in general to explain the origins of\nnon-crossing dependencies. Our work paves the way for more powerful null\nhypotheses to investigate the origins of non-crossing dependencies in nature.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 22:04:08 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 13:54:18 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1410.5491", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Chris Callison-Burch", "title": "Using Mechanical Turk to Build Machine Translation Evaluation Sets", "comments": "4 pages, 2 tables; appeared in Proceedings of the NAACL HLT 2010\n  Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk,\n  June 2010", "journal-ref": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech\n  and Language Data with Amazon's Mechanical Turk, pages 208-211, Los Angeles,\n  California, June 2010. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machine translation (MT) test sets is a relatively expensive task.\nAs MT becomes increasingly desired for more and more language pairs and more\nand more domains, it becomes necessary to build test sets for each case. In\nthis paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT\ntest sets cheaply. We find that MTurk can be used to make test sets much\ncheaper than professionally-produced test sets. More importantly, in\nexperiments with multiple MT systems, we find that the MTurk-produced test sets\nyield essentially the same conclusions regarding system performance as the\nprofessionally-produced test sets yield.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 22:28:55 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1410.5877", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Chris Callison-Burch", "title": "Bucking the Trend: Large-Scale Cost-Focused Active Learning for\n  Statistical Machine Translation", "comments": "11 pages, 14 figures; appeared in Proceedings of the 48th Annual\n  Meeting of the Association for Computational Linguistics, July 2010", "journal-ref": "In Proceedings of the 48th Annual Meeting of the Association for\n  Computational Linguistics, pages 854-864, Uppsala, Sweden, July 2010.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how to improve machine translation systems by adding more\ntranslation data in situations where we already have substantial resources. The\nmain challenge is how to buck the trend of diminishing returns that is commonly\nencountered. We present an active learning-style data solicitation algorithm to\nmeet this challenge. We test it, gathering annotations via Amazon Mechanical\nTurk, and find that we get an order of magnitude increase in performance rates\nof improvement.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 22:55:48 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "1410.6830", "submitter": "Isik Baris Fidaner", "authors": "I\\c{s}{\\i}k Bar{\\i}\\c{s} Fidaner, Ali Taylan Cemgil", "title": "Clustering Words by Projection Entropy", "comments": "Accepted to NIPS 2014 Modern ML+NLP Workshop:\n  http://www.cs.cmu.edu/~apparikh/nips2014ml-nlp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We apply entropy agglomeration (EA), a recently introduced algorithm, to\ncluster the words of a literary text. EA is a greedy agglomerative procedure\nthat minimizes projection entropy (PE), a function that can quantify the\nsegmentedness of an element set. To apply it, the text is reduced to a feature\nallocation, a combinatorial object to represent the word occurences in the\ntext's paragraphs. The experiment results demonstrate that EA, despite its\nreduction and simplicity, is useful in capturing significant relationships\namong the words in the text. This procedure was implemented in Python and\npublished as a free software: REBUS.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 20:34:01 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Fidaner", "I\u015f\u0131k Bar\u0131\u015f", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "1410.6903", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Laxmi Narayana M. and Sunil Kumar Kopparapu", "title": "Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used\nspeech features in most speech and speaker recognition applications. In this\npaper, we study the effect of resampling a speech signal on these speech\nfeatures. We first derive a relationship between the MFCC param- eters of the\nresampled speech and the MFCC parameters of the original speech. We propose six\nmethods of calculating the MFCC parameters of downsampled speech by\ntransforming the Mel filter bank used to com- pute MFCC of the original speech.\nWe then experimentally compute the MFCC parameters of the down sampled speech\nusing the proposed meth- ods and compute the Pearson coefficient between the\nMFCC parameters of the downsampled speech and that of the original speech to\nidentify the most effective choice of Mel-filter band that enables the computed\nMFCC of the resampled speech to be as close as possible to the original speech\nsample MFCC.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 09:40:46 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["M.", "Laxmi Narayana", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1410.7182", "submitter": "Leon Derczynski", "authors": "Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke van Erp,\n  Genevieve Gorrell, Rapha\\\"el Troncy, Johann Petrak, Kalina Bontcheva", "title": "Analysis of Named Entity Recognition and Linking for Tweets", "comments": "35 pages, accepted to journal Information Processing and Management", "journal-ref": "Information Processing & Management 51 (2), 32-49, 2014", "doi": "10.1016/j.ipm.2014.10.006", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying natural language processing for mining and intelligent information\naccess to tweets (a form of microblog) is a challenging, emerging research\narea. Unlike carefully authored news text and other longer content, tweets pose\na number of new challenges, due to their short, noisy, context-dependent, and\ndynamic nature. Information extraction from tweets is typically performed in a\npipeline, comprising consecutive stages of language identification,\ntokenisation, part-of-speech tagging, named entity recognition and entity\ndisambiguation (e.g. with respect to DBpedia). In this work, we describe a new\nTwitter entity disambiguation dataset, and conduct an empirical analysis of\nnamed entity recognition and disambiguation, investigating how robust a number\nof state-of-the-art systems are on such noisy texts, what the main sources of\nerror are, and which problems should be further investigated to improve the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 11:09:36 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Derczynski", "Leon", ""], ["Maynard", "Diana", ""], ["Rizzo", "Giuseppe", ""], ["van Erp", "Marieke", ""], ["Gorrell", "Genevieve", ""], ["Troncy", "Rapha\u00ebl", ""], ["Petrak", "Johann", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1410.7382", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Kiran Kumar Bhuvanagiri and Sunil Kumar Kopparapu", "title": "Modified Mel Filter Bank to Compute MFCC of Subsampled Speech", "comments": "arXiv admin note: substantial text overlap with arXiv:1410.6903", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used\nspeech features in most speech and speaker recognition applications. In this\nwork, we propose a modified Mel filter bank to extract MFCCs from subsampled\nspeech. We also propose a stronger metric which effectively captures the\ncorrelation between MFCCs of original speech and MFCC of resampled speech. It\nis found that the proposed method of filter bank construction performs\ndistinguishably well and gives recognition performance on resampled speech\nclose to recognition accuracies on original speech.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 10:00:14 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Bhuvanagiri", "Kiran Kumar", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1410.7787", "submitter": "Michael Bloodgood", "authors": "David Zajic, Michael Maxwell, David Doermann, Paul Rodrigues and\n  Michael Bloodgood", "title": "Correcting Errors in Digital Lexicographic Resources Using a Dictionary\n  Manipulation Language", "comments": "5 pages, 3 figures, 1 table; appeared in Proceedings of Electronic\n  Lexicography in the 21st Century (eLex), November 2011", "journal-ref": "In Proceedings of Electronic Lexicography in the 21st Century\n  (eLex), pages 297-301, Bled, Slovenia, November 2011. Trojina Institute for\n  Applied Slovene Studies", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a paradigm for combining manual and automatic error correction of\nnoisy structured lexicographic data. Modifications to the structure and\nunderlying text of the lexicographic data are expressed in a simple,\ninterpreted programming language. Dictionary Manipulation Language (DML)\ncommands identify nodes by unique identifiers, and manipulations are performed\nusing simple commands such as create, move, set text, etc. Corrected lexicons\nare produced by applying sequences of DML commands to the source version of the\nlexicon. DML commands can be written manually to repair one-off errors or\ngenerated automatically to correct recurring problems. We discuss advantages of\nthe paradigm for the task of editing digital bilingual dictionaries.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 20:12:50 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Zajic", "David", ""], ["Maxwell", "Michael", ""], ["Doermann", "David", ""], ["Rodrigues", "Paul", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1410.8027", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Towards a Visual Turing Challenge", "comments": "Published in the NIPS 2014 Workshop on Learning Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As language and visual understanding by machines progresses rapidly, we are\nobserving an increasing interest in holistic architectures that tightly\ninterlink both modalities in a joint learning and inference process. This trend\nhas allowed the community to progress towards more challenging and open tasks\nand refueled the hope at achieving the old AI dream of building machines that\ncould pass a turing test in open domains. In order to steadily make progress\ntowards this goal, we realize that quantifying performance becomes increasingly\ndifficult. Therefore we ask how we can precisely define such challenges and how\nwe can evaluate different algorithms on this open tasks? In this paper, we\nsummarize and discuss such challenges as well as try to give answers where\nappropriate options are available in the literature. We exemplify some of the\nsolutions on a recently presented dataset of question-answering task based on\nreal-world indoor images that establishes a visual turing challenge. Finally,\nwe argue despite the success of unique ground-truth annotation, we likely have\nto step away from carefully curated dataset and rather rely on 'social\nconsensus' as the main driving force to create suitable benchmarks. Providing\ncoverage in this inherently ambiguous output space is an emerging challenge\nthat we face in order to make quantifiable progress in this area.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 15:38:29 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 12:09:53 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 18:03:56 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1410.8149", "submitter": "Michael Bloodgood", "authors": "Paul Rodrigues, David Zajic, David Doermann, Michael Bloodgood and\n  Peng Ye", "title": "Detecting Structural Irregularity in Electronic Dictionaries Using\n  Language Modeling", "comments": "6 pages, 2 figures, 11 tables; appeared in Proceedings of Electronic\n  Lexicography in the 21st Century (eLex), November 2011", "journal-ref": "In Proceedings of Electronic Lexicography in the 21st Century\n  (eLex), pages 227-232, Bled, Slovenia, November 2011. Trojina Institute for\n  Applied Slovene Studies", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionaries are often developed using tools that save to Extensible Markup\nLanguage (XML)-based standards. These standards often allow high-level\nrepeating elements to represent lexical entries, and utilize descendants of\nthese repeating elements to represent the structure within each lexical entry,\nin the form of an XML tree. In many cases, dictionaries are published that have\nerrors and inconsistencies that are expensive to find manually. This paper\ndiscusses a method for dictionary writers to quickly audit structural\nregularity across entries in a dictionary by using statistical language\nmodeling. The approach learns the patterns of XML nodes that could occur within\nan XML tree, and then calculates the probability of each XML tree in the\ndictionary against these patterns to look for entries that diverge from the\nnorm.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:07:21 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Rodrigues", "Paul", ""], ["Zajic", "David", ""], ["Doermann", "David", ""], ["Bloodgood", "Michael", ""], ["Ye", "Peng", ""]]}, {"id": "1410.8206", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, Wojciech\n  Zaremba", "title": "Addressing the Rare Word Problem in Neural Machine Translation", "comments": "ACL 2015 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) is a new approach to machine translation\nthat has shown promising results that are comparable to traditional approaches.\nA significant weakness in conventional NMT systems is their inability to\ncorrectly translate very rare words: end-to-end NMTs tend to have relatively\nsmall vocabularies with a single unk symbol that represents every possible\nout-of-vocabulary (OOV) word. In this paper, we propose and implement an\neffective technique to address this problem. We train an NMT system on data\nthat is augmented by the output of a word alignment algorithm, allowing the NMT\nsystem to emit, for each OOV word in the target sentence, the position of its\ncorresponding word in the source sentence. This information is later utilized\nin a post-processing step that translates every OOV word using a dictionary.\nOur experiments on the WMT14 English to French translation task show that this\nmethod provides a substantial improvement of up to 2.8 BLEU points over an\nequivalent NMT system that does not use this technique. With 37.5 BLEU points,\nour NMT system is the first to surpass the best result achieved on a WMT14\ncontest task.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 00:20:31 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 19:44:50 GMT"}, {"version": "v3", "created": "Tue, 9 Dec 2014 23:11:46 GMT"}, {"version": "v4", "created": "Sat, 30 May 2015 19:57:28 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Sutskever", "Ilya", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1410.8326", "submitter": "Nicholas H. Kirk", "authors": "Nicholas H. Kirk", "title": "Towards Learning Object Affordance Priors from Technical Texts", "comments": "\"Active Learning in Robotics\" Workshop, IEEE-RAS International\n  Conference on Humanoid Robots [accepted]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyday activities performed by artificial assistants can potentially be\nexecuted naively and dangerously given their lack of common sense knowledge.\nThis paper presents conceptual work towards obtaining prior knowledge on the\nusual modality (passive or active) of any given entity, and their affordance\nestimates, by extracting high-confidence ability modality semantic relations (X\ncan Y relationship) from non-figurative texts, by analyzing co-occurrence of\ngrammatical instances of subjects and verbs, and verbs and objects. The\ndiscussion includes an outline of the concept, potential and limitations, and\npossible feature and learning framework adoption.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 11:02:39 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Kirk", "Nicholas H.", ""]]}, {"id": "1410.8498", "submitter": "Emma Strubell", "authors": "Emma Strubell, Luke Vilnis, Andrew McCallum", "title": "Training for Fast Sequential Prediction Using Dynamic Feature Selection", "comments": "5 pages, NIPS Modern ML + NLP Workshop 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present paired learning and inference algorithms for significantly\nreducing computation and increasing speed of the vector dot products in the\nclassifiers that are at the heart of many NLP components. This is accomplished\nby partitioning the features into a sequence of templates which are ordered\nsuch that high confidence can often be reached using only a small fraction of\nall features. Parameter estimation is arranged to maximize accuracy and early\nconfidence in this sequence. We present experiments in left-to-right\npart-of-speech tagging on WSJ, demonstrating that we can preserve accuracy\nabove 97% with over a five-fold reduction in run-time.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 19:02:48 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 21:46:25 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Strubell", "Emma", ""], ["Vilnis", "Luke", ""], ["McCallum", "Andrew", ""]]}, {"id": "1410.8553", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood, Peng Ye, Paul Rodrigues, David Zajic and David\n  Doermann", "title": "A random forest system combination approach for error detection in\n  digital dictionaries", "comments": "9 pages, 7 figures, 10 tables; appeared in Proceedings of the\n  Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,\n  April 2012", "journal-ref": "In Proceedings of the Workshop on Innovative Hybrid Approaches to\n  the Processing of Textual Data, pages 78-86, Avignon, France, April 2012.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When digitizing a print bilingual dictionary, whether via optical character\nrecognition or manual entry, it is inevitable that errors are introduced into\nthe electronic version that is created. We investigate automating the process\nof detecting errors in an XML representation of a digitized print dictionary\nusing a hybrid approach that combines rule-based, feature-based, and language\nmodel-based methods. We investigate combining methods and show that using\nrandom forests is a promising approach. We find that in isolation, unsupervised\nmethods rival the performance of supervised methods. Random forests typically\nrequire training data so we investigate how we can apply random forests to\ncombine individual base methods that are themselves unsupervised without\nrequiring large amounts of training data. Experiments reveal empirically that a\nrelatively small amount of data is sufficient and can potentially be further\nreduced through specific selection criteria.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 20:52:48 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Bloodgood", "Michael", ""], ["Ye", "Peng", ""], ["Rodrigues", "Paul", ""], ["Zajic", "David", ""], ["Doermann", "David", ""]]}, {"id": "1410.8581", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk and Yusuf Arslan", "title": "Semi-Automatic Construction of a Domain Ontology for Wind Energy Using\n  Wikipedia Articles", "comments": null, "journal-ref": "Renewable Energy, Volume 62, pp. 484-489, February 2014", "doi": "10.1016/j.renene.2013.08.002", "report-no": null, "categories": "cs.CL cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain ontologies are important information sources for knowledge-based\nsystems. Yet, building domain ontologies from scratch is known to be a very\nlabor-intensive process. In this study, we present our semi-automatic approach\nto building an ontology for the domain of wind energy which is an important\ntype of renewable energy with a growing share in electricity generation all\nover the world. Related Wikipedia articles are first processed in an automated\nmanner to determine the basic concepts of the domain together with their\nproperties and next the concepts, properties, and relationships are organized\nto arrive at the ultimate ontology. We also provide pointers to other\nengineering ontologies which could be utilized together with the proposed wind\nenergy ontology in addition to its prospective application areas. The current\nstudy is significant as, to the best of our knowledge, it proposes the first\nconsiderably wide-coverage ontology for the wind energy domain and the ontology\nis built through a semi-automatic process which makes use of the related Web\nresources, thereby reducing the overall cost of the ontology building process.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:38:11 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""], ["Arslan", "Yusuf", ""]]}, {"id": "1410.8668", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk and Ralf Steinberger", "title": "Experiments to Improve Named Entity Recognition on Turkish Tweets", "comments": "appears in Proceedings of the EACL Workshop on Language Analysis for\n  Social Media, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media texts are significant information sources for several\napplication areas including trend analysis, event monitoring, and opinion\nmining. Unfortunately, existing solutions for tasks such as named entity\nrecognition that perform well on formal texts usually perform poorly when\napplied to social media texts. In this paper, we report on experiments that\nhave the purpose of improving named entity recognition on Turkish tweets, using\ntwo different annotated data sets. In these experiments, starting with a\nbaseline named entity recognition system, we adapt its recognition rules and\nresources to better fit Twitter language by relaxing its capitalization\nconstraint and by diacritics-based expansion of its lexical resources, and we\nemploy a simplistic normalization scheme on tweets to observe the effects of\nthese on the overall named entity recognition performance on Turkish tweets.\nThe evaluation results of the system with these different settings are provided\nwith discussions of these results.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 08:35:55 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""], ["Steinberger", "Ralf", ""]]}, {"id": "1410.8749", "submitter": "Jiwei Li", "authors": "Jiwei Li, Xun Wang and Eduard Hovy", "title": "What a Nasty day: Exploring Mood-Weather Relationship from Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it has long been believed in psychology that weather somehow influences\nhuman's mood, the debates have been going on for decades about how they are\ncorrelated. In this paper, we try to study this long-lasting topic by\nharnessing a new source of data compared from traditional psychological\nresearches: Twitter. We analyze 2 years' twitter data collected by twitter API\nwhich amounts to $10\\%$ of all postings and try to reveal the correlations\nbetween multiple dimensional structure of human mood with meteorological\neffects. Some of our findings confirm existing hypotheses, while others\ncontradict them. We are hopeful that our approach, along with the new data\nsource, can shed on the long-going debates on weather-mood correlation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 18:51:55 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Li", "Jiwei", ""], ["Wang", "Xun", ""], ["Hovy", "Eduard", ""]]}, {"id": "1410.8783", "submitter": "Nabil Khoufi", "authors": "Nabil Khoufi, Chafik Aloulou, Lamia Hadrich Belguith", "title": "Supervised learning model for parsing Arabic language", "comments": "8 pages,1 figure, Proceedings of the 10th International Workshop on\n  Natural Language Processing and Cognitive Science (NLPCS 2013),2013,\n  Marseille, France, pp129-136", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing the Arabic language is a difficult task given the specificities of\nthis language and given the scarcity of digital resources (grammars and\nannotated corpora). In this paper, we suggest a method for Arabic parsing based\non supervised machine learning. We used the SVMs algorithm to select the\nsyntactic labels of the sentence. Furthermore, we evaluated our parser\nfollowing the cross validation method by using the Penn Arabic Treebank. The\nobtained results are very encouraging.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 15:53:49 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Khoufi", "Nabil", ""], ["Aloulou", "Chafik", ""], ["Belguith", "Lamia Hadrich", ""]]}, {"id": "1410.8808", "submitter": "Paolo Pareti Mr.", "authors": "Paolo Pareti, Ewan Klein and Adam Barker", "title": "A Semantic Web of Know-How: Linked Data for Community-Centric Tasks", "comments": "6th International Workshop on Web Intelligence & Communities (WIC14),\n  Proceedings of the companion publication of the 23rd International Conference\n  on World Wide Web (WWW 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes a novel framework for representing community know-how on\nthe Semantic Web. Procedural knowledge generated by web communities typically\ntakes the form of natural language instructions or videos and is largely\nunstructured. The absence of semantic structure impedes the deployment of many\nuseful applications, in particular the ability to discover and integrate\nknow-how automatically. We discuss the characteristics of community know-how\nand argue that existing knowledge representation frameworks fail to represent\nit adequately. We present a novel framework for representing the semantic\nstructure of community know-how and demonstrate the feasibility of our approach\nby providing a concrete implementation which includes a method for\nautomatically acquiring procedural knowledge for real-world tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 15:48:40 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Pareti", "Paolo", ""], ["Klein", "Ewan", ""], ["Barker", "Adam", ""]]}]