[{"id": "1508.00106", "submitter": "Ira Leviant", "authors": "Ira Leviant, Roi Reichart", "title": "Separated by an Un-common Language: Towards Judgment Language Informed\n  Vector Space Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common evaluation practice in the vector space models (VSMs) literature is\nto measure the models' ability to predict human judgments about lexical\nsemantic relations between word pairs. Most existing evaluation sets, however,\nconsist of scores collected for English word pairs only, ignoring the potential\nimpact of the judgment language in which word pairs are presented on the human\nscores. In this paper we translate two prominent evaluation sets, wordsim353\n(association) and SimLex999 (similarity), from English to Italian, German and\nRussian and collect scores for each dataset from crowdworkers fluent in its\nlanguage. Our analysis reveals that human judgments are strongly impacted by\nthe judgment language. Moreover, we show that the predictions of monolingual\nVSMs do not necessarily best correlate with human judgments made with the\nlanguage used for model training, suggesting that models and humans are\naffected differently by the language they use when making semantic judgments.\nFinally, we show that in a large number of setups, multilingual VSM combination\nresults in improved correlations with human judgments, suggesting that\nmultilingualism may partially compensate for the judgment language effect on\nhuman judgments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 10:24:27 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 09:48:38 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2015 19:31:42 GMT"}, {"version": "v4", "created": "Sun, 29 Nov 2015 20:12:13 GMT"}, {"version": "v5", "created": "Sun, 6 Dec 2015 09:58:17 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Leviant", "Ira", ""], ["Reichart", "Roi", ""]]}, {"id": "1508.00189", "submitter": "Devendra Sachan", "authors": "Devendra Singh Sachan, Shailesh Kumar", "title": "Class Vectors: Embedding representation of Document Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed representations of words and paragraphs as semantic embeddings in\nhigh dimensional data are used across a number of Natural Language\nUnderstanding tasks such as retrieval, translation, and classification. In this\nwork, we propose \"Class Vectors\" - a framework for learning a vector per class\nin the same embedding space as the word and paragraph embeddings. Similarity\nbetween these class vectors and word vectors are used as features to classify a\ndocument to a class. In experiment on several sentiment analysis tasks such as\nYelp reviews and Amazon electronic product reviews, class vectors have shown\nbetter or comparable results in classification while learning very meaningful\nclass embeddings.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 04:17:40 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Sachan", "Devendra Singh", ""], ["Kumar", "Shailesh", ""]]}, {"id": "1508.00200", "submitter": "Jian Tang", "authors": "Jian Tang, Meng Qu, Qiaozhu Mei", "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text\n  Networks", "comments": "KDD 2015", "journal-ref": null, "doi": "10.1145/2783258.2783307", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector,\nhave been attracting increasing attention due to their simplicity, scalability,\nand effectiveness. However, comparing to sophisticated deep learning\narchitectures such as convolutional neural networks, these methods usually\nyield inferior results when applied to particular machine learning tasks. One\npossible reason is that these text embedding methods learn the representation\nof text in a fully unsupervised way, without leveraging the labeled information\navailable for the task. Although the low dimensional representations learned\nare applicable to many different tasks, they are not particularly tuned for any\ntask. In this paper, we fill this gap by proposing a semi-supervised\nrepresentation learning method for text data, which we call the\n\\textit{predictive text embedding} (PTE). Predictive text embedding utilizes\nboth labeled and unlabeled data to learn the embedding of text. The labeled\ninformation and different levels of word co-occurrence information are first\nrepresented as a large-scale heterogeneous text network, which is then embedded\ninto a low dimensional space through a principled and efficient algorithm. This\nlow dimensional embedding not only preserves the semantic closeness of words\nand documents, but also has a strong predictive power for the particular task.\nCompared to recent supervised approaches based on convolutional neural\nnetworks, predictive text embedding is comparable or more effective, much more\nefficient, and has fewer parameters to tune.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 06:18:10 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Tang", "Jian", ""], ["Qu", "Meng", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1508.00305", "submitter": "Panupong Pasupat", "authors": "Panupong Pasupat, Percy Liang", "title": "Compositional Semantic Parsing on Semi-Structured Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two important aspects of semantic parsing for question answering are the\nbreadth of the knowledge source and the depth of logical compositionality.\nWhile existing work trades off one aspect for another, this paper\nsimultaneously makes progress on both fronts through a new task: answering\ncomplex questions on semi-structured tables using question-answer pairs as\nsupervision. The central challenge arises from two compounding factors: the\nbroader domain results in an open-ended set of relations, and the deeper\ncompositionality results in a combinatorial explosion in the space of logical\nforms. We propose a logical-form driven parsing algorithm guided by strong\ntyping constraints and show that it obtains significant improvements over\nnatural baselines. For evaluation, we created a new dataset of 22,033 complex\nquestions on Wikipedia tables, which is made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 02:53:01 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Pasupat", "Panupong", ""], ["Liang", "Percy", ""]]}, {"id": "1508.00354", "submitter": "Sivanand Achanta", "authors": "Sivanand Achanta, Anandaswarup Vadapalli, Sai Krishna R., Suryakanth\n  V. Gangashetty", "title": "Significance of Maximum Spectral Amplitude in Sub-bands for Spectral\n  Envelope Estimation and Its Application to Statistical Parametric Speech\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a technique for spectral envelope estimation using\nmaximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most\nother methods in the literature parametrize spectral envelope in cepstral\ndomain such as Mel-generalized cepstrum etc. Such cepstral domain\nrepresentations, although compact, are not readily interpretable. This\ndifficulty is overcome by our method which parametrizes in the spectral domain\nitself. In our experiments, spectral envelope estimated using MSASB method was\nincorporated in the STRAIGHT vocoder. Both objective and subjective results of\nanalysis-by-synthesis indicate that the proposed method is comparable to\nSTRAIGHT. We also evaluate the effectiveness of the proposed parametrization in\na statistical parametric speech synthesis framework using deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 09:28:22 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Achanta", "Sivanand", ""], ["Vadapalli", "Anandaswarup", ""], ["R.", "Sai Krishna", ""], ["Gangashetty", "Suryakanth V.", ""]]}, {"id": "1508.00504", "submitter": "Matilde Marcolli", "authors": "Karthik Siva, Jim Tao, Matilde Marcolli", "title": "Spin Glass Models of Syntax and Language Evolution", "comments": "19 pages, LaTeX, 20 png figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cond-mat.dis-nn physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the SSWL database of syntactic parameters of world languages, and the\nMIT Media Lab data on language interactions, we construct a spin glass model of\nlanguage evolution. We treat binary syntactic parameters as spin states, with\nlanguages as vertices of a graph, and assigned interaction energies along the\nedges. We study a rough model of syntax evolution, under the assumption that a\nstrong interaction energy tends to cause parameters to align, as in the case of\nferromagnetic materials. We also study how the spin glass model needs to be\nmodified to account for entailment relations between syntactic parameters. This\nmodification leads naturally to a generalization of Potts models with external\nmagnetic field, which consists of a coupling at the vertices of an Ising model\nand a Potts model with q=3, that have the same edge interactions. We describe\nthe results of simulations of the dynamics of these models, in different\ntemperature and energy regimes. We discuss the linguistic interpretation of the\nparameters of the physical model.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 16:21:17 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Siva", "Karthik", ""], ["Tao", "Jim", ""], ["Marcolli", "Matilde", ""]]}, {"id": "1508.00657", "submitter": "Miguel Ballesteros", "authors": "Miguel Ballesteros, Chris Dyer, Noah A. Smith", "title": "Improved Transition-Based Parsing by Modeling Characters instead of\n  Words with LSTMs", "comments": "In Proceedings of EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present extensions to a continuous-state dependency parsing method that\nmakes it applicable to morphologically rich languages. Starting with a\nhigh-performance transition-based parser that uses long short-term memory\n(LSTM) recurrent neural networks to learn representations of the parser state,\nwe replace lookup-based word representations with representations constructed\nfrom the orthographic representations of the words, also using LSTMs. This\nallows statistical sharing across word forms that are similar on the surface.\nExperiments for morphologically rich languages show that the parsing model\nbenefits from incorporating the character-based encodings of words.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 04:36:36 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 17:33:47 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Ballesteros", "Miguel", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1508.00715", "submitter": "Zhilin Yang", "authors": "Zhilin Yang, Jie Tang, William Cohen", "title": "Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the extent to which online social networks can be connected to open\nknowledge bases. The problem is referred to as learning social knowledge\ngraphs. We propose a multi-modal Bayesian embedding model, GenVector, to learn\nlatent topics that generate word and network embeddings. GenVector leverages\nlarge-scale unlabeled data with embeddings and represents data of two\nmodalities---i.e., social network users and knowledge concepts---in a shared\nlatent topic space. Experiments on three datasets show that the proposed method\nclearly outperforms state-of-the-art methods. We then deploy the method on\nAMiner, a large-scale online academic search system with a network of\n38,049,189 researchers with a knowledge base with 35,415,011 concepts. Our\nmethod significantly decreases the error rate in an online A/B test with live\nusers.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 09:34:22 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 19:57:37 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Yang", "Zhilin", ""], ["Tang", "Jie", ""], ["Cohen", "William", ""]]}, {"id": "1508.00973", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Leonard K.M. Poon, Zhourong Chen", "title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical latent tree analysis (HLTA) is recently proposed as a new method\nfor topic detection. It differs fundamentally from the LDA-based methods in\nterms of topic definition, topic-document relationship, and learning method. It\nhas been shown to discover significantly more coherent topics and better topic\nhierarchies. However, HLTA relies on the Expectation-Maximization (EM)\nalgorithm for parameter estimation and hence is not efficient enough to deal\nwith large datasets. In this paper, we propose a method to drastically speed up\nHLTA using a technique inspired by recent advances in the moments method.\nEmpirical experiments show that our method greatly improves the efficiency of\nHLTA. It is as efficient as the state-of-the-art LDA-based method for\nhierarchical topic detection and finds substantially better topics and topic\nhierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 05:00:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""]]}, {"id": "1508.01006", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang and Dong Wang", "title": "Relation Classification via Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has gained much success in sentence-level relation\nclassification. For example, convolutional neural networks (CNN) have delivered\ncompetitive performance without much effort on feature engineering as the\nconventional pattern-based methods. Thus a lot of works have been produced\nbased on CNN structures. However, a key issue that has not been well addressed\nby the CNN-based method is the lack of capability to learn temporal features,\nespecially long-distance dependency between nominal pairs. In this paper, we\npropose a simple framework based on recurrent neural networks (RNN) and compare\nit with CNN-based model. To show the limitation of popular used SemEval-2010\nTask 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al.,\n2014). Experiments on two different datasets strongly indicates that the\nRNN-based model can deliver better performance on relation classification, and\nit is particularly capable of learning long-distance relation patterns. This\nmakes it suitable for real-world applications where complicated expressions are\noften involved.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:03:46 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 03:51:00 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Zhang", "Dongxu", ""], ["Wang", "Dong", ""]]}, {"id": "1508.01011", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang, Tianyi Luo, Dong Wang and Rong Liu", "title": "Learning from LDA using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:22:25 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Zhang", "Dongxu", ""], ["Luo", "Tianyi", ""], ["Wang", "Dong", ""], ["Liu", "Rong", ""]]}, {"id": "1508.01067", "submitter": "Jing Su", "authors": "Jing Su and Ois\\'in Boydell and Derek Greene and Gerard Lynch", "title": "Topic Stability over Noisy Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modelling techniques such as LDA have recently been applied to speech\ntranscripts and OCR output. These corpora may contain noisy or erroneous texts\nwhich may undermine topic stability. Therefore, it is important to know how\nwell a topic modelling algorithm will perform when applied to noisy data. In\nthis paper we show that different types of textual noise will have diverse\neffects on the stability of different topic models. From these observations, we\npropose guidelines for text corpus generation, with a focus on automatic speech\ntranscription. We also suggest topic model selection methods for noisy corpora.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 13:18:51 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Su", "Jing", ""], ["Boydell", "Ois\u00edn", ""], ["Greene", "Derek", ""], ["Lynch", "Gerard", ""]]}, {"id": "1508.01211", "submitter": "Navdeep Jaitly", "authors": "William Chan and Navdeep Jaitly and Quoc V. Le and Oriol Vinyals", "title": "Listen, Attend and Spell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Listen, Attend and Spell (LAS), a neural network that learns to\ntranscribe speech utterances to characters. Unlike traditional DNN-HMM models,\nthis model learns all the components of a speech recognizer jointly. Our system\nhas two components: a listener and a speller. The listener is a pyramidal\nrecurrent network encoder that accepts filter bank spectra as inputs. The\nspeller is an attention-based recurrent network decoder that emits characters\nas outputs. The network produces character sequences without making any\nindependence assumptions between the characters. This is the key improvement of\nLAS over previous end-to-end CTC models. On a subset of the Google voice search\ntask, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a\nlanguage model, and 10.3% with language model rescoring over the top 32 beams.\nBy comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 20:17:58 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 00:38:43 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Chan", "William", ""], ["Jaitly", "Navdeep", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1508.01306", "submitter": "Michael Minock", "authors": "Michael Minock and Nils Everling", "title": "Replication and Generalization of PRECISE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes an initial replication study of the PRECISE system and\ndevelops a clearer, more formal description of the approach. Based on our\nevaluation, we conclude that the PRECISE results do not fully replicate.\nHowever the formalization developed here suggests a road map to further enhance\nand extend the approach pioneered by PRECISE.\n  After a long, productive discussion with Ana-Maria Popescu (one of the\nauthors of PRECISE) we got more clarity on the PRECISE approach and how the\nlexicon was authored for the GEO evaluation. Based on this we built a more\ndirect implementation over a repaired formalism. Although our new evaluation is\nnot yet complete, it is clear that the system is performing much better now. We\nwill continue developing our ideas and implementation and generate a future\nreport/publication that more accurately evaluates PRECISE like approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 07:56:59 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 09:41:34 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Minock", "Michael", ""], ["Everling", "Nils", ""]]}, {"id": "1508.01321", "submitter": "Jaderick Pabico", "authors": "Fatima M. Moncada and Jaderick P. Pabico", "title": "On Gobbledygook and Mood of the Philippine Senate: An Exploratory Study\n  on the Readability and Sentiment of Selected Philippine Senators' Microposts", "comments": "13 pages, 6 figures, submitted to the Asia Pacific Journal on\n  Education, Arts, and Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the findings of a readability assessment and sentiment\nanalysis of selected six Philippine senators' microposts over the popular\nTwitter microblog. Using the Simple Measure of Gobbledygook (SMOG), tweets of\nSenators Cayetano, Defensor-Santiago, Pangilinan, Marcos, Guingona, and\nEscudero were assessed. A sentiment analysis was also done to determine the\npolarity of the senators' respective microposts. Results showed that on the\naverage, the six senators are tweeting at an eight to ten SMOG level. This\nmeans that, at least a sixth grader will be able to understand the senators'\ntweets. Moreover, their tweets are mostly neutral and their sentiments vary in\nunison at some period of time. This could mean that a senator's tweet sentiment\nis affected by specific Philippine-based events.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 08:39:20 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Moncada", "Fatima M.", ""], ["Pabico", "Jaderick P.", ""]]}, {"id": "1508.01346", "submitter": "Alok Pal", "authors": "Alok Ranjan Pal and Diganta Saha", "title": "Word sense disambiguation: a survey", "comments": "International Journal of Control Theory and Computer Modeling\n  (IJCTCM) Vol.5, No.3, July 2015", "journal-ref": null, "doi": "10.5121/ijctcm.2015.5301", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we made a survey on Word Sense Disambiguation (WSD). Near\nabout in all major languages around the world, research in WSD has been\nconducted upto different extents. In this paper, we have gone through a survey\nregarding the different approaches adopted in different research works, the\nState of the Art in the performance in this domain, recent works in different\nIndian languages and finally a survey in Bengali language. We have made a\nsurvey on different competitions in this field and the bench mark results,\nobtained from those competitions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 10:15:51 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Pal", "Alok Ranjan", ""], ["Saha", "Diganta", ""]]}, {"id": "1508.01349", "submitter": "Alok Pal", "authors": "Alok Ranjan Pal, Diganta Saha and Niladri Sekhar Dash", "title": "Automatic classification of bengali sentences based on sense definitions\n  present in bengali wordnet", "comments": "International Journal of Control Theory and Computer Modeling\n  (IJCTCM) Vol.5, No.1, January 2015", "journal-ref": null, "doi": "10.5121/ijctcm.2015.5101", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the sense definition of words available in the Bengali WordNet, an\nattempt is made to classify the Bengali sentences automatically into different\ngroups in accordance with their underlying senses. The input sentences are\ncollected from 50 different categories of the Bengali text corpus developed in\nthe TDIL project of the Govt. of India, while information about the different\nsenses of particular ambiguous lexical item is collected from Bengali WordNet.\nIn an experimental basis we have used Naive Bayes probabilistic model as a\nuseful classifier of sentences. We have applied the algorithm over 1747\nsentences that contain a particular Bengali lexical item which, because of its\nambiguous nature, is able to trigger different senses that render sentences in\ndifferent meanings. In our experiment we have achieved around 84% accurate\nresult on the sense classification over the total input sentences. We have\nanalyzed those residual sentences that did not comply with our experiment and\ndid affect the results to note that in many cases, wrong syntactic structures\nand less semantic information are the main hurdles in semantic classification\nof sentences. The applicational relevance of this study is attested in\nautomatic text classification, machine learning, information extraction, and\nword sense disambiguation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 10:26:40 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Pal", "Alok Ranjan", ""], ["Saha", "Diganta", ""], ["Dash", "Niladri Sekhar", ""]]}, {"id": "1508.01420", "submitter": "Luis Marujo", "authors": "Lu\\'is Marujo, Jos\\'e Port\\^elo, Wang Ling, David Martins de Matos,\n  Jo\\~ao P. Neto, Anatole Gershman, Jaime Carbonell, Isabel Trancoso, Bhiksha\n  Raj", "title": "Privacy-Preserving Multi-Document Summarization", "comments": "4 pages, In Proceedings of 2nd ACM SIGIR Workshop on\n  Privacy-Preserving Information Retrieval, August 2015. arXiv admin note: text\n  overlap with arXiv:1407.5416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art extractive multi-document summarization systems are usually\ndesigned without any concern about privacy issues, meaning that all documents\nare open to third parties. In this paper we propose a privacy-preserving\napproach to multi-document summarization. Our approach enables other parties to\nobtain summaries without learning anything else about the original documents'\ncontent. We use a hashing scheme known as Secure Binary Embeddings to convert\ndocuments representation containing key phrases and bag-of-words into bit\nstrings, allowing the computation of approximate distances, instead of exact\nones. Our experiments indicate that our system yields similar results to its\nnon-private counterpart on standard multi-document evaluation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 14:30:47 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Marujo", "Lu\u00eds", ""], ["Port\u00ealo", "Jos\u00e9", ""], ["Ling", "Wang", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Trancoso", "Isabel", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1508.01447", "submitter": "Iyad AlAgha", "authors": "Iyad AlAgha", "title": "Using Linguistic Analysis to Translate Arabic Natural Language Queries\n  to SPARQL", "comments": "Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logic-based machine-understandable framework of the Semantic Web often\nchallenges naive users when they try to query ontology-based knowledge bases.\nExisting research efforts have approached this problem by introducing Natural\nLanguage (NL) interfaces to ontologies. These NL interfaces have the ability to\nconstruct SPARQL queries based on NL user queries. However, most efforts were\nrestricted to queries expressed in English, and they often benefited from the\nadvancement of English NLP tools. However, little research has been done to\nsupport querying the Arabic content on the Semantic Web by using NL queries.\nThis paper presents a domain-independent approach to translate Arabic NL\nqueries to SPARQL by leveraging linguistic analysis. Based on a special\nconsideration on Noun Phrases (NPs), our approach uses a language parser to\nextract NPs and the relations from Arabic parse trees and match them to the\nunderlying ontology. It then utilizes knowledge in the ontology to group NPs\ninto triple-based representations. A SPARQL query is finally generated by\nextracting targets and modifiers, and interpreting them into SPARQL. The\ninterpretation of advanced semantic features including negation, conjunctive\nand disjunctive modifiers is also supported. The approach was evaluated by\nusing two datasets consisting of OWL test data and queries, and the obtained\nresults have confirmed its feasibility to translate Arabic NL queries to\nSPARQL.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 16:10:21 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["AlAgha", "Iyad", ""]]}, {"id": "1508.01476", "submitter": "Qiang Zhan", "authors": "Qiang Zhan, Chunhong Wang", "title": "Hyponymy extraction of domain ontology concept based on ccrfs and\n  hierarchy clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept hierarchy is the backbone of ontology, and the concept hierarchy\nacquisition has been a hot topic in the field of ontology learning. this paper\nproposes a hyponymy extraction method of domain ontology concept based on\ncascaded conditional random field(CCRFs) and hierarchy clustering. It takes\nfree text as extracting object, adopts CCRFs identifying the domain concepts.\nFirst the low layer of CCRFs is used to identify simple domain concept, then\nthe results are sent to the high layer, in which the nesting concepts are\nrecognized. Next we adopt hierarchy clustering to identify the hyponymy\nrelation between domain ontology concepts. The experimental results demonstrate\nthe proposed method is efficient.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 18:02:54 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Zhan", "Qiang", ""], ["Wang", "Chunhong", ""]]}, {"id": "1508.01571", "submitter": "Humberto Corona", "authors": "Humberto Corona, Michael P. O'Mahony", "title": "A Mood-based Genre Classification of Television Content", "comments": "in ACM Workshop on Recommendation Systems for Television and Online\n  Video 2014 Foster City, California USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of television content helps users organise and navigate\nthrough the large list of channels and programs now available. In this paper,\nwe address the problem of television content classification by exploiting text\ninformation extracted from program transcriptions. We present an analysis which\nadapts a model for sentiment that has been widely and successfully applied in\nother fields such as music or blog posts. We use a real-world dataset obtained\nfrom the Boxfish API to compare the performance of classifiers trained on a\nnumber of different feature sets. Our experiments show that, over a large\ncollection of television content, program genres can be represented in a\nthree-dimensional space of valence, arousal and dominance, and that promising\nclassification results can be achieved using features based on this\nrepresentation. This finding supports the use of the proposed representation of\ntelevision content as a feature space for similarity computation and\nrecommendation generation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 23:53:30 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Corona", "Humberto", ""], ["O'Mahony", "Michael P.", ""]]}, {"id": "1508.01577", "submitter": "Javier Vera Z\\'u\\~niga", "authors": "Javier Vera, Felipe Urbina, Eric Goles", "title": "Automata networks model for alignment and least effort on vocabulary\n  formation", "comments": "This paper has been withdrawn due one author has declined its\n  participation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can artificial communities of agents develop language with scaling relations\nclose to the Zipf law? As a preliminary answer to this question, we propose an\nAutomata Networks model of the formation of a vocabulary on a population of\nindividuals, under two in principle opposite strategies: the alignment and the\nleast effort principle. Within the previous account to the emergence of\nlinguistic conventions (specially, the Naming Game), we focus on modeling\nspeaker and hearer efforts as actions over their vocabularies and we study the\nimpact of these actions on the formation of a shared language. The numerical\nsimulations are essentially based on an energy function, that measures the\namount of local agreement between the vocabularies. The results suggests that\non one dimensional lattices the best strategy to the formation of shared\nlanguages is the one that minimizes the efforts of speakers on communicative\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 00:57:18 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:28:40 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Vera", "Javier", ""], ["Urbina", "Felipe", ""], ["Goles", "Eric", ""]]}, {"id": "1508.01580", "submitter": "Javier Vera Z\\'u\\~niga", "authors": "Javier Vera, Eric Goles", "title": "Automata networks for memory loss effects in the formation of linguistic\n  conventions", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.1007/s12559-015-9371-7", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work attempts to give new theoretical insights to the absence of\nintermediate stages in the evolution of language. In particular, it is\ndeveloped an automata networks approach to a crucial question: how a population\nof language users can reach agreement on a linguistic convention? To describe\nthe appearance of sharp transitions in the self-organization of language, it is\nadopted an extremely simple model of (working) memory. At each time step,\nlanguage users simply loss part of their word-memories. Through computer\nsimulations of low-dimensional lattices, it appear sharp transitions at\ncritical values that depend on the size of the vicinities of the individuals.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 01:15:23 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 14:57:40 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Vera", "Javier", ""], ["Goles", "Eric", ""]]}, {"id": "1508.01585", "submitter": "Minwei Feng", "authors": "Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, Bowen Zhou", "title": "Applying Deep Learning to Answer Selection: A Study and An Open Task", "comments": "To appear in the proceedings of ASRU 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We apply a general deep learning framework to address the non-factoid\nquestion answering task. Our approach does not rely on any linguistic tools and\ncan be applied to different languages or domains. Various architectures are\npresented and compared. We create and release a QA corpus and setup a new QA\ntask in the insurance domain. Experimental results demonstrate superior\nperformance compared to the baseline methods and various technologies give\nfurther improvements. For this highly challenging task, the top-1 accuracy can\nreach up to 65.3% on a test set, which indicates a great potential for\npractical use.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 01:54:04 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 18:23:16 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Feng", "Minwei", ""], ["Xiang", "Bing", ""], ["Glass", "Michael R.", ""], ["Wang", "Lidan", ""], ["Zhou", "Bowen", ""]]}, {"id": "1508.01718", "submitter": "Rimah Amami", "authors": "Rimah Amami, Noureddine Ellouze", "title": "Study of Phonemes Confusions in Hierarchical Automatic Phoneme\n  Recognition System", "comments": "08 pages in Journal of Convergence Information Technology 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have analyzed the impact of confusions on the robustness of\nphoneme recognitions system. The confusions are detected at the pronunciation\nand the confusions matrices of the phoneme recognizer. The confusions show that\nsome similarities between phonemes at the pronunciation affect significantly\nthe recognition rates. This paper proposes to understand those confusions in\norder to improve the performance of the phoneme recognition system by isolating\nthe problematic phonemes. Confusion analysis leads to build a new hierarchical\nrecognizer using new phoneme distribution and the information from the\nconfusion matrices. This new hierarchical phoneme recognition system shows\nsignificant improvements of the recognition rates on TIMIT database.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 15:06:13 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Amami", "Rimah", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1508.01745", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David\n  Vandyke, Steve Young", "title": "Semantically Conditioned LSTM-based Natural Language Generation for\n  Spoken Dialogue Systems", "comments": "To be appear in EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language generation (NLG) is a critical component of spoken dialogue\nand it has a significant impact both on usability and perceived quality. Most\nNLG systems in common use employ rules and heuristics and tend to generate\nrigid and stylised responses without the natural variation of human language.\nThey are also not easily scaled to systems covering multiple domains and\nlanguages. This paper presents a statistical language generator based on a\nsemantically controlled Long Short-term Memory (LSTM) structure. The LSTM\ngenerator can learn from unaligned data by jointly optimising sentence planning\nand surface realisation using a simple cross entropy training criterion, and\nlanguage variation can be easily achieved by sampling from output candidates.\nWith fewer heuristics, an objective evaluation in two differing test domains\nshowed the proposed method improved performance compared to previous methods.\nHuman judges scored the LSTM system higher on informativeness and naturalness\nand overall preferred it to the other systems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:16:44 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 17:16:25 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Young", "Steve", ""]]}, {"id": "1508.01746", "submitter": "Alan Godoy", "authors": "Alan Godoy, Fl\\'avio Sim\\~oes, Jos\\'e Augusto Stuchi, Marcus de Assis\n  Angeloni, M\\'ario Uliani, Ricardo Violato", "title": "Using Deep Learning for Detecting Spoofing Attacks on Speech Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that speaker verification systems are subject to spoofing\nattacks. The Automatic Speaker Verification Spoofing and Countermeasures\nChallenge -- ASVSpoof2015 -- provides a standard spoofing database, containing\nattacks based on synthetic speech, along with a protocol for experiments. This\npaper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based\non deep neural networks, working both as a classifier and as a feature\nextraction module for a GMM and a SVM classifier. Results show the validity of\nthis approach, achieving less than 0.5\\% EER for known attacks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:20:52 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 16:27:49 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Godoy", "Alan", ""], ["Sim\u00f5es", "Fl\u00e1vio", ""], ["Stuchi", "Jos\u00e9 Augusto", ""], ["Angeloni", "Marcus de Assis", ""], ["Uliani", "M\u00e1rio", ""], ["Violato", "Ricardo", ""]]}, {"id": "1508.01755", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola Mrksic, Pei-Hao Su,\n  David Vandyke, Steve Young", "title": "Stochastic Language Generation in Dialogue using Recurrent Neural\n  Networks with Convolutional Sentence Reranking", "comments": "To be appear in SigDial 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural language generation (NLG) component of a spoken dialogue system\n(SDS) usually needs a substantial amount of handcrafting or a well-labeled\ndataset to be trained on. These limitations add significantly to development\ncosts and make cross-domain, multi-lingual dialogue systems intractable.\nMoreover, human languages are context-aware. The most natural response should\nbe directly learned from data rather than depending on predefined syntaxes or\nrules. This paper presents a statistical language generator based on a joint\nrecurrent and convolutional neural network structure which can be trained on\ndialogue act-utterance pairs without any semantic alignments or predefined\ngrammar trees. Objective metrics suggest that this new model outperforms\nprevious methods under the same experimental conditions. Results of an\nevaluation by human judges indicate that it produces not only high quality but\nlinguistically varied utterances which are preferred compared to n-gram and\nrule-based systems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:34:11 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Gasic", "Milica", ""], ["Kim", "Dongho", ""], ["Mrksic", "Nikola", ""], ["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Young", "Steve", ""]]}, {"id": "1508.01786", "submitter": "Daniel Romero", "authors": "Daniel M. Romero, Roderick I. Swaab, Brian Uzzi, and Adam D. Galinsky", "title": "Mimicry Is Presidential: Linguistic Style Matching in Presidential\n  Debates and Improved Polling Numbers", "comments": "in the Personality and Social Psychology Bulletin (2015)", "journal-ref": null, "doi": "10.1177/0146167215591168", "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current research used the contexts of U.S. presidential debates and\nnegotiations to examine whether matching the linguistic style of an opponent in\na two-party exchange affects the reactions of third-party observers. Building\noff communication accommodation theory (CAT), interaction alignment theory\n(IAT), and processing fluency, we propose that language style matching (LSM)\nwill improve subsequent third-party evaluations because matching an opponent's\nlinguistic style reflects greater perspective taking and will make one's\narguments easier to process. In contrast, research on status inferences\npredicts that LSM will negatively impact third-party evaluations because LSM\nimplies followership. We conduct two studies to test these competing\nhypotheses. Study 1 analyzed transcripts of U.S. presidential debates between\n1976 and 2012 and found that candidates who matched their opponent's linguistic\nstyle increased their standing in the polls. Study 2 demonstrated a causal\nrelationship between LSM and third-party observer evaluations using negotiation\ntranscripts.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 19:35:52 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Romero", "Daniel M.", ""], ["Swaab", "Roderick I.", ""], ["Uzzi", "Brian", ""], ["Galinsky", "Adam D.", ""]]}, {"id": "1508.01991", "submitter": "Zhiheng Huang", "authors": "Zhiheng Huang, Wei Xu, and Kai Yu", "title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a variety of Long Short-Term Memory (LSTM) based\nmodels for sequence tagging. These models include LSTM networks, bidirectional\nLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer\n(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is\nthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to\nNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model\ncan efficiently use both past and future input features thanks to a\nbidirectional LSTM component. It can also use sentence level tag information\nthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or\nclose to) accuracy on POS, chunking and NER data sets. In addition, it is\nrobust and has less dependence on word embedding as compared to previous\nobservations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 06:32:47 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Huang", "Zhiheng", ""], ["Xu", "Wei", ""], ["Yu", "Kai", ""]]}, {"id": "1508.01993", "submitter": "Stefan Feuerriegel", "authors": "Stefan Feuerriegel and Ralph Fehrer", "title": "Improving Decision Analytics with Deep Learning: The Case of Financial\n  Disclosures", "comments": null, "journal-ref": "Twenty-Fourth European Conference on Information Systems (ECIS\n  2016), Istanbul, Turkey, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision analytics commonly focuses on the text mining of financial news\nsources in order to provide managerial decision support and to predict stock\nmarket movements. Existing predictive frameworks almost exclusively apply\ntraditional machine learning methods, whereas recent research indicates that\ntraditional machine learning methods are not sufficiently capable of extracting\nsuitable features and capturing the non-linear nature of complex tasks. As a\nremedy, novel deep learning models aim to overcome this issue by extending\ntraditional neural network models with additional hidden layers. Indeed, deep\nlearning has been shown to outperform traditional methods in terms of\npredictive performance. In this paper, we adapt the novel deep learning\ntechnique to financial decision support. In this instance, we aim to predict\nthe direction of stock movements following financial disclosures. As a result,\nwe show how deep learning can outperform the accuracy of random forests as a\nbenchmark for machine learning by 5.66%.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 07:39:24 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 09:32:57 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Feuerriegel", "Stefan", ""], ["Fehrer", "Ralph", ""]]}, {"id": "1508.01996", "submitter": "Hui Yu", "authors": "Hui Yu, Xiaofeng Wu, Wenbin Jiang, Qun Liu, ShouXun Lin", "title": "An Automatic Machine Translation Evaluation Metric Based on Dependency\n  Parsing Model", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the syntax-based metrics obtain the similarity by comparing the\nsub-structures extracted from the trees of hypothesis and reference. These\nsub-structures are defined by human and can't express all the information in\nthe trees because of the limited length of sub-structures. In addition, the\noverlapped parts between these sub-structures are computed repeatedly. To avoid\nthese problems, we propose a novel automatic evaluation metric based on\ndependency parsing model, with no need to define sub-structures by human.\nFirst, we train a dependency parsing model by the reference dependency tree.\nThen we generate the hypothesis dependency tree and the corresponding\nprobability by the dependency parsing model. The quality of the hypothesis can\nbe judged by this probability. In order to obtain the lexicon similarity, we\nalso introduce the unigram F-score to the new metric. Experiment results show\nthat the new metric gets the state-of-the-art performance on system level, and\nis comparable with METEOR on sentence level.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 07:55:51 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 14:13:04 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Yu", "Hui", ""], ["Wu", "Xiaofeng", ""], ["Jiang", "Wenbin", ""], ["Liu", "Qun", ""], ["Lin", "ShouXun", ""]]}, {"id": "1508.02060", "submitter": "Ahmed Hassan Yousef Ph.D.", "authors": "Walaa Medhat, Ahmed H. Yousef, Hoda Korashy", "title": "Egyptian Dialect Stopword List Generation from Social Network Data", "comments": "The paper is an extension to the old paper found in the language\n  engineering conference, arXiv:1410.1135. It is accepted by the language\n  engineeringjournal. Although it has nearly the same structure, it is\n  different because extensive cross validation is added any many negation words\n  are added to dataset of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a methodology for generating a stopword list from online\nsocial network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper is\nto investigate the effect of removingED stopwords on the Sentiment Analysis\n(SA) task. The stopwords lists generated before were on Modern Standard Arabic\n(MSA) which is not the common language used in OSN. We have generated a\nstopword list of Egyptian dialect to be used with the OSN corpora. We compare\nthe efficiency of text classification when using the generated list along with\npreviously generated lists of MSA and combining the Egyptian dialect list with\nthe MSA list. The text classification was performed using Na\\\"ive Bayes and\nDecision Tree classifiers and two feature selection approaches, unigram and\nbigram. The experiments show that removing ED stopwords give better performance\nthan using lists of MSA stopwords only.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 15:56:00 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Medhat", "Walaa", ""], ["Yousef", "Ahmed H.", ""], ["Korashy", "Hoda", ""]]}, {"id": "1508.02091", "submitter": "Jack Hessel", "authors": "Jack Hessel, Nicolas Savva, Michael J. Wilber", "title": "Image Representations and New Domains in Neural Image Captioning", "comments": "11 Pages, 5 Images, To appear at EMNLP 2015's Vision + Learning\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the possibility that recent promising results in automatic caption\ngeneration are due primarily to language models. By varying image\nrepresentation quality produced by a convolutional neural network, we find that\na state-of-the-art neural captioning algorithm is able to produce quality\ncaptions even when provided with surprisingly poor image representations. We\nreplicate this result in a new, fine-grained, transfer learned captioning\ndomain, consisting of 66K recipe image/title pairs. We also provide some\nexperiments regarding the appropriateness of datasets for automatic captioning,\nand find that having multiple captions per image is beneficial, but not an\nabsolute requirement.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 22:52:10 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Hessel", "Jack", ""], ["Savva", "Nicolas", ""], ["Wilber", "Michael J.", ""]]}, {"id": "1508.02096", "submitter": "Wang Ling", "authors": "Wang Ling and Tiago Lu\\'is and Lu\\'is Marujo and Ram\\'on Fernandez\n  Astudillo and Silvio Amir and Chris Dyer and Alan W. Black and Isabel\n  Trancoso", "title": "Finding Function in Form: Compositional Character Models for Open\n  Vocabulary Word Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for constructing vector representations of words by\ncomposing characters using bidirectional LSTMs. Relative to traditional word\nrepresentation models that have independent vectors for each word type, our\nmodel requires only a single vector per character type and a fixed set of\nparameters for the compositional model. Despite the compactness of this model\nand, more importantly, the arbitrary nature of the form-function relationship\nin language, our \"composed\" word representations yield state-of-the-art results\nin language modeling and part-of-speech tagging. Benefits over traditional\nbaselines are particularly pronounced in morphologically rich languages (e.g.,\nTurkish).\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 23:41:38 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 20:57:19 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Ling", "Wang", ""], ["Lu\u00eds", "Tiago", ""], ["Marujo", "Lu\u00eds", ""], ["Astudillo", "Ram\u00f3n Fernandez", ""], ["Amir", "Silvio", ""], ["Dyer", "Chris", ""], ["Black", "Alan W.", ""], ["Trancoso", "Isabel", ""]]}, {"id": "1508.02131", "submitter": "Trevor Cohn", "authors": "Daniel Beck, Trevor Cohn, Christian Hardmeier, Lucia Specia", "title": "Learning Structural Kernels for Natural Language Processing", "comments": "Transactions of the Association for Computational Linguistics, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural kernels are a flexible learning paradigm that has been widely used\nin Natural Language Processing. However, the problem of model selection in\nkernel-based methods is usually overlooked. Previous approaches mostly rely on\nsetting default values for kernel hyperparameters or using grid search, which\nis slow and coarse-grained. In contrast, Bayesian methods allow efficient model\nselection by maximizing the evidence on the training data through\ngradient-based methods. In this paper we show how to perform this in the\ncontext of structural kernels by using Gaussian Processes. Experimental results\non tree kernels show that this procedure results in better prediction\nperformance compared to hyperparameter optimization via grid search. The\nframework proposed in this paper can be adapted to other structures besides\ntrees, e.g., strings and graphs, thereby extending the utility of kernel-based\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 05:57:14 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Beck", "Daniel", ""], ["Cohn", "Trevor", ""], ["Hardmeier", "Christian", ""], ["Specia", "Lucia", ""]]}, {"id": "1508.02142", "submitter": "Iftekhar Naim", "authors": "Iftekhar Naim and Daniel Gildea", "title": "Feature-based Decipherment for Large Vocabulary Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthographic similarities across languages provide a strong signal for\nprobabilistic decipherment, especially for closely related language pairs. The\nexisting decipherment models, however, are not well-suited for exploiting these\northographic similarities. We propose a log-linear model with latent variables\nthat incorporates orthographic similarity features. Maximum likelihood training\nis computationally expensive for the proposed log-linear model. To address this\nchallenge, we perform approximate inference via MCMC sampling and contrastive\ndivergence. Our results show that the proposed log-linear model with\ncontrastive divergence scales to large vocabularies and outperforms the\nexisting generative decipherment models by exploiting the orthographic\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 07:02:49 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Naim", "Iftekhar", ""], ["Gildea", "Daniel", ""]]}, {"id": "1508.02225", "submitter": "Hui Yu", "authors": "Hui Yu, Xiaofeng Wu, Wenbin Jiang, Qun Liu, Shouxun Lin", "title": "Improve the Evaluation of Fluency Using Entropy for Machine Translation\n  Evaluation Metrics", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely-used automatic evaluation metrics cannot adequately reflect the\nfluency of the translations. The n-gram-based metrics, like BLEU, limit the\nmaximum length of matched fragments to n and cannot catch the matched fragments\nlonger than n, so they can only reflect the fluency indirectly. METEOR, which\nis not limited by n-gram, uses the number of matched chunks but it does not\nconsider the length of each chunk. In this paper, we propose an entropy-based\nmethod, which can sufficiently reflect the fluency of translations through the\ndistribution of matched words. This method can easily combine with the\nwidely-used automatic evaluation metrics to improve the evaluation of fluency.\nExperiments show that the correlations of BLEU and METEOR are improved on\nsentence level after combining with the entropy-based method on WMT 2010 and\nWMT 2012.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 12:46:52 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 13:58:02 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Yu", "Hui", ""], ["Wu", "Xiaofeng", ""], ["Jiang", "Wenbin", ""], ["Liu", "Qun", ""], ["Lin", "Shouxun", ""]]}, {"id": "1508.02285", "submitter": "Nut Limsopatham", "authors": "Nut Limsopatham and Nigel Collier", "title": "Adapting Phrase-based Machine Translation to Normalise Medical Terms in\n  Social Media Messages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have shown that health reports in social media, such as\nDailyStrength and Twitter, have potential for monitoring health conditions\n(e.g. adverse drug reactions, infectious diseases) in particular communities.\nHowever, in order for a machine to understand and make inferences on these\nhealth conditions, the ability to recognise when laymen's terms refer to a\nparticular medical concept (i.e.\\ text normalisation) is required. To achieve\nthis, we propose to adapt an existing phrase-based machine translation (MT)\ntechnique and a vector representation of words to map between a social media\nphrase and a medical concept. We evaluate our proposed approach using a\ncollection of phrases from tweets related to adverse drug reactions. Our\nexperimental results show that the combination of a phrase-based MT technique\nand the similarity between word vector representations outperforms the\nbaselines that apply only either of them by up to 55%.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 15:29:22 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Limsopatham", "Nut", ""], ["Collier", "Nigel", ""]]}, {"id": "1508.02297", "submitter": "Adriaan Schakel", "authors": "Adriaan M. J. Schakel and Benjamin J. Wilson", "title": "Measuring Word Significance using Distributed Representations of Words", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of words as real-valued vectors in a relatively\nlow-dimensional space aim at extracting syntactic and semantic features from\nlarge text corpora. A recently introduced neural network, named word2vec\n(Mikolov et al., 2013a; Mikolov et al., 2013b), was shown to encode semantic\ninformation in the direction of the word vectors. In this brief report, it is\nproposed to use the length of the vectors, together with the term frequency, as\nmeasure of word significance in a corpus. Experimental evidence using a\ndomain-specific corpus of abstracts is presented to support this proposal. A\nuseful visualization technique for text corpora emerges, where words are mapped\nonto a two-dimensional plane and automatically ranked by significance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 15:52:49 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Schakel", "Adriaan M. J.", ""], ["Wilson", "Benjamin J.", ""]]}, {"id": "1508.02354", "submitter": "Dimitri Kartsaklis", "authors": "Jianpeng Cheng and Dimitri Kartsaklis", "title": "Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models\n  of Meaning", "comments": "Accepted for presentation at EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep compositional models of meaning acting on distributional representations\nof words in order to produce vectors of larger text constituents are evolving\nto a popular area of NLP research. We detail a compositional distributional\nframework based on a rich form of word embeddings that aims at facilitating the\ninteractions between words in the context of a sentence. Embeddings and\ncomposition layers are jointly learned against a generic objective that\nenhances the vectors with syntactic information from the surrounding context.\nFurthermore, each word is associated with a number of senses, the most\nplausible of which is selected dynamically during the composition process. We\nevaluate the produced vectors qualitatively and quantitatively with positive\nresults. At the sentence level, the effectiveness of the framework is\ndemonstrated on the MSRPar task, for which we report results within the\nstate-of-the-art range.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 19:04:18 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:50:43 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Kartsaklis", "Dimitri", ""]]}, {"id": "1508.02375", "submitter": "Matthew Gormley", "authors": "Matthew R. Gormley, Mark Dredze, Jason Eisner", "title": "Approximation-Aware Dependency Parsing by Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to train the fast dependency parser of Smith and Eisner (2008)\nfor improved accuracy. This parser can consider higher-order interactions among\nedges while retaining O(n^3) runtime. It outputs the parse with maximum\nexpected recall -- but for speed, this expectation is taken under a posterior\ndistribution that is constructed only approximately, using loopy belief\npropagation through structured factors. We show how to adjust the model\nparameters to compensate for the errors introduced by this approximation, by\nfollowing the gradient of the actual loss on training data. We find this\ngradient by back-propagation. That is, we treat the entire parser\n(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)\nand Domke (2010) did for loopy CRFs. The resulting trained parser obtains\nhigher accuracy with fewer iterations of belief propagation than one trained by\nconditional log-likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 19:48:33 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Gormley", "Matthew R.", ""], ["Dredze", "Mark", ""], ["Eisner", "Jason", ""]]}, {"id": "1508.02445", "submitter": "Milo\\v{s} Stanojevi\\'c", "authors": "Milo\\v{s} Stanojevi\\'c", "title": "Removing Biases from Trainable MT Metrics by Using Self-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most trainable machine translation (MT) metrics train their weights on human\njudgments of state-of-the-art MT systems outputs. This makes trainable metrics\nbiases in many ways. One of them is preferring longer translations. These\nbiased metrics when used for tuning are evaluating different types of\ntranslations -- n-best lists of translations with very diverse quality. Systems\ntuned with these metrics tend to produce overly long translations that are\npreferred by the metric but not by humans. This is usually solved by manually\ntweaking metric's weights to equally value recall and precision. Our solution\nis more general: (1) it does not address only the recall bias but also all\nother biases that might be present in the data and (2) it does not require any\nknowledge of the types of features used which is useful in cases when manual\ntuning of metric's weights is not possible. This is accomplished by\nself-training on unlabeled n-best lists by using metric that was initially\ntrained on standard human judgments. One way of looking at this is as domain\nadaptation from the domain of state-of-the-art MT translations to diverse\nn-best list translations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 22:24:36 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Stanojevi\u0107", "Milo\u0161", ""]]}, {"id": "1508.03040", "submitter": "Ram\u00f3n Casares", "authors": "Ram\\'on Casares", "title": "Syntax Evolution: Problems and Recursion", "comments": "37 pages", "journal-ref": null, "doi": "10.6084/m9.figshare.4956359", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To investigate the evolution of syntax, we need to ascertain the evolutionary\nr\\^ole of syntax and, before that, the very nature of syntax. Here, we will\nassume that syntax is computing. And then, since we are computationally Turing\ncomplete, we meet an evolutionary anomaly, the anomaly of sytax: we are\nsyntactically too competent for syntax. Assuming that problem solving is\ncomputing, and realizing that the evolutionary advantage of Turing completeness\nis full problem solving and not syntactic proficiency, we explain the anomaly\nof syntax by postulating that syntax and problem solving co-evolved in humans\ntowards Turing completeness. Examining the requirements that full problem\nsolving impose on language, we find firstly that semantics is not sufficient\nand that syntax is necessary to represent problems. Our final conclusion is\nthat full problem solving requires a functional semantics on an infinite\ntree-structured syntax. Besides these results, the introduction of Turing\ncompleteness and problem solving to explain the evolution of syntax should help\nus to fit the evolution of language within the evolution of cognition, giving\nus some new clues to understand the elusive relation between language and\nthinking.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 09:04:01 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 16:55:24 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 09:51:09 GMT"}, {"version": "v4", "created": "Mon, 23 May 2016 11:25:40 GMT"}, {"version": "v5", "created": "Tue, 14 Jun 2016 09:15:15 GMT"}, {"version": "v6", "created": "Thu, 1 Sep 2016 08:41:41 GMT"}, {"version": "v7", "created": "Tue, 2 Jul 2019 07:49:40 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Casares", "Ram\u00f3n", ""]]}, {"id": "1508.03170", "submitter": "Paulo Figueiredo", "authors": "Paulo Figueiredo and Marta Apar\\'icio and David Martins de Matos and\n  Ricardo Ribeiro", "title": "Generation of Multimedia Artifacts: An Extractive Summarization-based\n  Approach", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore methods for content selection and address the issue of coherence\nin the context of the generation of multimedia artifacts. We use audio and\nvideo to present two case studies: generation of film tributes, and\nlecture-driven science talks. For content selection, we use centrality-based\nand diversity-based summarization, along with topic analysis. To establish\ncoherence, we use the emotional content of music, for film tributes, and ensure\ntopic similarity between lectures and documentaries, for science talks.\nComposition techniques for the production of multimedia artifacts are addressed\nas a means of organizing content, in order to improve coherence. We discuss our\nresults considering the above aspects.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 10:56:42 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Figueiredo", "Paulo", ""], ["Apar\u00edcio", "Marta", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1508.03276", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Harshita Jhavar", "title": "Talking about the Moving Image: A Declarative Model for Image Schema\n  Based Embodied Perception Grounding and Language Generation", "comments": "19 pages. Unpublished report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory and corresponding declarative model for the\nembodied grounding and natural language based analytical summarisation of\ndynamic visuo-spatial imagery. The declarative model ---ecompassing\nspatio-linguistic abstractions, image schemas, and a spatio-temporal feature\nbased language generator--- is modularly implemented within Constraint Logic\nProgramming (CLP). The implemented model is such that primitives of the theory,\ne.g., pertaining to space and motion, image schemata, are available as\nfirst-class objects with `deep semantics' suited for inference and query. We\ndemonstrate the model with select examples broadly motivated by areas such as\nfilm, design, geography, smart environments where analytical natural language\nbased externalisations of the moving image are central from the viewpoint of\nhuman interaction, evidence-based qualitative analysis, and sensemaking.\n  Keywords: moving image, visual semantics and embodiment, visuo-spatial\ncognition and computation, cognitive vision, computational models of narrative,\ndeclarative spatial reasoning\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:34:07 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Jhavar", "Harshita", ""]]}, {"id": "1508.03386", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, David Vandyke, Milica Gasic, Dongho Kim, Nikola Mrksic,\n  Tsung-Hsien Wen, Steve Young", "title": "Learning from Real Users: Rating Dialogue Success with Neural Networks\n  for Reinforcement Learning in Spoken Dialogue Systems", "comments": "Accepted for publication in INTERSPEECH 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train a statistical spoken dialogue system (SDS) it is essential that an\naccurate method for measuring task success is available. To date training has\nrelied on presenting a task to either simulated or paid users and inferring the\ndialogue's success by observing whether this presented task was achieved or\nnot. Our aim however is to be able to learn from real users acting under their\nown volition, in which case it is non-trivial to rate the success as any prior\nknowledge of the task is simply unavailable. User feedback may be utilised but\nhas been found to be inconsistent. Hence, here we present two neural network\nmodels that evaluate a sequence of turn-level features to rate the success of a\ndialogue. Importantly these models make no use of any prior knowledge of the\nuser's task. The models are trained on dialogues generated by a simulated user\nand the best model is then used to train a policy on-line which is shown to\nperform at least as well as a baseline system using prior knowledge of the\nuser's task. We note that the models should also be of interest for evaluating\nSDS and for monitoring a dialogue in rule-based SDS.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 23:44:03 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Gasic", "Milica", ""], ["Kim", "Dongho", ""], ["Mrksic", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1508.03391", "submitter": "Pei-Hao Su", "authors": "Pei-Hao Su, David Vandyke, Milica Gasic, Nikola Mrksic, Tsung-Hsien\n  Wen, Steve Young", "title": "Reward Shaping with Recurrent Neural Networks for Speeding up On-Line\n  Policy Learning in Spoken Dialogue Systems", "comments": "Accepted for publication in SigDial 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical spoken dialogue systems have the attractive property of being\nable to be optimised from data via interactions with real users. However in the\nreinforcement learning paradigm the dialogue manager (agent) often requires\nsignificant time to explore the state-action space to learn to behave in a\ndesirable manner. This is a critical issue when the system is trained on-line\nwith real users where learning costs are expensive. Reward shaping is one\npromising technique for addressing these concerns. Here we examine three\nrecurrent neural network (RNN) approaches for providing reward shaping\ninformation in addition to the primary (task-orientated) environmental\nfeedback. These RNNs are trained on returns from dialogues generated by a\nsimulated user and attempt to diffuse the overall evaluation of the dialogue\nback down to the turn level to guide the agent towards good behaviour faster.\nIn both simulated and real user scenarios these RNNs are shown to increase\npolicy learning speed. Importantly, they do not require prior knowledge of the\nuser's goal.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 00:41:12 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 12:42:42 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1508.03530", "submitter": "Dami\\'an G. Hern\\'andez", "authors": "Dami\\'an G. Hern\\'andez, Dami\\'an H. Zanette, In\\'es Samengo", "title": "Information-theoretical analysis of the statistical dependencies among\n  three variables: Applications to written language", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.92.022813", "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the information-theoretical concepts required to study the\nstatistical dependencies among three variables. Some of such dependencies are\npure triple interactions, in the sense that they cannot be explained in terms\nof a combination of pairwise correlations. We derive bounds for triple\ndependencies, and characterize the shape of the joint probability distribution\nof three binary variables with high triple interaction. The analysis also\nallows us to quantify the amount of redundancy in the mutual information\nbetween pairs of variables, and to assess whether the information between two\nvariables is or is not mediated by a third variable. These concepts are applied\nto the analysis of written texts. We find that the probability that a given\nword is found in a particular location within the text is not only modulated by\nthe presence or absence of other nearby words, but also, on the presence or\nabsence of nearby pairs of words. We identify the words enclosing the key\nsemantic concepts of the text, the triplets of words with high pairwise and\ntriple interactions, and the words that mediate the pairwise interactions\nbetween other words.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 03:10:18 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Hern\u00e1ndez", "Dami\u00e1n G.", ""], ["Zanette", "Dami\u00e1n H.", ""], ["Samengo", "In\u00e9s", ""]]}, {"id": "1508.03601", "submitter": "Sanjay Singh", "authors": "Ranjitha R. K. and Sanjay Singh", "title": "Is Stack Overflow Overflowing With Questions and Tags", "comments": "11 pages, 7 figures, 3 tables Presented at Third International\n  Symposium on Women in Computing and Informatics (WCI-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming question and answer (Q & A) websites, such as Quora, Stack\nOverflow, and Yahoo! Answer etc. helps us to understand the programming\nconcepts easily and quickly in a way that has been tested and applied by many\nsoftware developers. Stack Overflow is one of the most frequently used\nprogramming Q\\&A website where the questions and answers posted are presently\nanalyzed manually, which requires a huge amount of time and resource. To save\nthe effort, we present a topic modeling based technique to analyze the words of\nthe original texts to discover the themes that run through them. We also\npropose a method to automate the process of reviewing the quality of questions\non Stack Overflow dataset in order to avoid ballooning the stack overflow with\ninsignificant questions. The proposed method also recommends the appropriate\ntags for the new post, which averts the creation of unnecessary tags on Stack\nOverflow.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:39:18 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["K.", "Ranjitha R.", ""], ["Singh", "Sanjay", ""]]}, {"id": "1508.03720", "submitter": "Lili Mou", "authors": "Xu Yan, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, Zhi Jin", "title": "Classifying Relations via Long Short Term Memory Networks along Shortest\n  Dependency Path", "comments": "EMNLP '15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation classification is an important research arena in the field of\nnatural language processing (NLP). In this paper, we present SDP-LSTM, a novel\nneural network to classify the relation of two entities in a sentence. Our\nneural architecture leverages the shortest dependency path (SDP) between two\nentities; multichannel recurrent neural networks, with long short term memory\n(LSTM) units, pick up heterogeneous information along the SDP. Our proposed\nmodel has several distinct features: (1) The shortest dependency paths retain\nmost relevant information (to relation classification), while eliminating\nirrelevant words in the sentence. (2) The multichannel LSTM networks allow\neffective information integration from heterogeneous sources over the\ndependency paths. (3) A customized dropout strategy regularizes the neural\nnetwork to alleviate overfitting. We test our model on the SemEval 2010\nrelation classification task, and achieve an $F_1$-score of 83.7\\%, higher than\ncompeting methods in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 11:15:32 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Yan", "Xu", ""], ["Mou", "Lili", ""], ["Li", "Ge", ""], ["Chen", "Yunchuan", ""], ["Peng", "Hao", ""], ["Jin", "Zhi", ""]]}, {"id": "1508.03721", "submitter": "Lili Mou", "authors": "Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin", "title": "A Comparative Study on Regularization Strategies for Embedding-based\n  Neural Networks", "comments": "EMNLP '15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to compare different regularization strategies to address a\ncommon phenomenon, severe overfitting, in embedding-based neural networks for\nNLP. We chose two widely studied neural models and tasks as our testbed. We\ntried several frequently applied or newly proposed regularization strategies,\nincluding penalizing weights (embeddings excluded), penalizing embeddings,\nre-embedding words, and dropout. We also emphasized on incremental\nhyperparameter tuning, and combining different regularizations. The results\nprovide a picture on tuning hyperparameters for neural NLP models.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 11:16:39 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Peng", "Hao", ""], ["Mou", "Lili", ""], ["Li", "Ge", ""], ["Chen", "Yunchuan", ""], ["Lu", "Yangyang", ""], ["Jin", "Zhi", ""]]}, {"id": "1508.03790", "submitter": "Kaisheng Yao", "authors": "Kaisheng Yao, Trevor Cohn, Katerina Vylomova, Kevin Duh, and Chris\n  Dyer", "title": "Depth-Gated LSTM", "comments": "Content presented in 2015 Jelinek Summer Workshop on Speech and\n  Language Technology on August 14th 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we present an extension of long short-term memory (LSTM)\nneural networks to using a depth gate to connect memory cells of adjacent\nlayers. Doing so introduces a linear dependence between lower and upper layer\nrecurrent units. Importantly, the linear dependence is gated through a gating\nfunction, which we call depth gate. This gate is a function of the lower layer\nmemory cell, the input to and the past memory cell of this layer. We conducted\nexperiments and verified that this new architecture of LSTMs was able to\nimprove machine translation and language modeling performances.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 04:31:37 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 19:38:58 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2015 07:13:04 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2015 04:24:20 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Yao", "Kaisheng", ""], ["Cohn", "Trevor", ""], ["Vylomova", "Katerina", ""], ["Duh", "Kevin", ""], ["Dyer", "Chris", ""]]}, {"id": "1508.03826", "submitter": "Shaohua Li", "authors": "Shaohua Li, Jun Zhu, Chunyan Miao", "title": "A Generative Word Embedding Model and its Low Rank Positive Semidefinite\n  Solution", "comments": "Proceedings of the Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) 2015 2015, 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing word embedding methods can be categorized into Neural Embedding\nModels and Matrix Factorization (MF)-based methods. However some models are\nopaque to probabilistic interpretation, and MF-based methods, typically solved\nusing Singular Value Decomposition (SVD), may incur loss of corpus information.\nIn addition, it is desirable to incorporate global latent factors, such as\ntopics, sentiments or writing styles, into the word embedding model. Since\ngenerative models provide a principled way to incorporate latent factors, we\npropose a generative word embedding model, which is easy to interpret, and can\nserve as a basis of more sophisticated latent factor models. The model\ninference reduces to a low rank weighted positive semidefinite approximation\nproblem. Its optimization is approached by eigendecomposition on a submatrix,\nfollowed by online blockwise regression, which is scalable and avoids the\ninformation loss in SVD. In experiments on 7 common benchmark datasets, our\nvectors are competitive to word2vec, and better than other MF-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 14:12:17 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Li", "Shaohua", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1508.03854", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Online Representation Learning in Recurrent Neural Language Models", "comments": "In Proceedings of EMNLP 2015", "journal-ref": null, "doi": "10.18653/v1/D15-1026", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an extension of continuous online learning in recurrent neural\nnetwork language models. The model keeps a separate vector representation of\nthe current unit of text being processed and adaptively adjusts it after each\nprediction. The initial experiments give promising results, indicating that the\nmethod is able to increase language modelling accuracy, while also decreasing\nthe parameters needed to store the model along with the computation required at\neach step.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 18:27:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1508.03868", "submitter": "Brendan Jou", "authors": "Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara,\n  Shih-Fu Chang", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual\n  Sentiment Ontology", "comments": "11 pages, to appear at ACM MM'15", "journal-ref": null, "doi": "10.1145/2733373.2806246", "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every culture and language is unique. Our work expressly focuses on the\nuniqueness of culture and language in relation to human affect, specifically\nsentiment and emotion semantics, and how they manifest in social multimedia. We\ndevelop sets of sentiment- and emotion-polarized visual concepts by adapting\nsemantic structures called adjective-noun pairs, originally introduced by Borth\net al. (2013), but in a multilingual context. We propose a new\nlanguage-dependent method for automatic discovery of these adjective-noun\nconstructs. We show how this pipeline can be applied on a social multimedia\nplatform for the creation of a large-scale multilingual visual sentiment\nconcept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our\nunified ontology is organized hierarchically by multilingual clusters of\nvisually detectable nouns and subclusters of emotionally biased versions of\nthese nouns. In addition, we present an image-based prediction task to show how\ngeneralizable language-specific models are in a multilingual context. A new,\npublicly available dataset of >15.6K sentiment-biased visual concepts across 12\nlanguages with language-specific detector banks, >7.36M images and their\nmetadata is also released.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 21:43:59 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 16:33:13 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 19:07:14 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Jou", "Brendan", ""], ["Chen", "Tao", ""], ["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1508.04025", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Hieu Pham, Christopher D. Manning", "title": "Effective Approaches to Attention-based Neural Machine Translation", "comments": "11 pages, 7 figures, EMNLP 2015 camera-ready version, more training\n  details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attentional mechanism has lately been used to improve neural machine\ntranslation (NMT) by selectively focusing on parts of the source sentence\nduring translation. However, there has been little work exploring useful\narchitectures for attention-based NMT. This paper examines two simple and\neffective classes of attentional mechanism: a global approach which always\nattends to all source words and a local one that only looks at a subset of\nsource words at a time. We demonstrate the effectiveness of both approaches\nover the WMT translation tasks between English and German in both directions.\nWith local attention, we achieve a significant gain of 5.0 BLEU points over\nnon-attentional systems which already incorporate known techniques such as\ndropout. Our ensemble model using different attention architectures has\nestablished a new state-of-the-art result in the WMT'15 English to German\ntranslation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over\nthe existing best system backed by NMT and an n-gram reranker.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 13:43:19 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 10:27:26 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2015 08:14:59 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2015 09:03:04 GMT"}, {"version": "v5", "created": "Sun, 20 Sep 2015 08:25:52 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Pham", "Hieu", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1508.04112", "submitter": "Tao Lei", "authors": "Tao Lei, Regina Barzilay and Tommi Jaakkola", "title": "Molding CNNs for text: non-linear, non-consecutive convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning often derives from well-chosen operational\nbuilding blocks. In this work, we revise the temporal convolution operation in\nCNNs to better adapt it to text processing. Instead of concatenating word\nrepresentations, we appeal to tensor algebra and use low-rank n-gram tensors to\ndirectly exploit interactions between words already at the convolution stage.\nMoreover, we extend the n-gram convolution to non-consecutive words to\nrecognize patterns with intervening words. Through a combination of low-rank\ntensors, and pattern weighting, we can efficiently evaluate the resulting\nconvolution operation via dynamic programming. We test the resulting\narchitecture on standard sentiment classification and news categorization\ntasks. Our model achieves state-of-the-art performance both in terms of\naccuracy and training speed. For instance, we obtain 51.2% accuracy on the\nfine-grained sentiment classification task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 19:02:45 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 02:52:40 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Lei", "Tao", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1508.04257", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Hinrich Sch\\\"utze", "title": "Learning Meta-Embeddings by Using Ensembles of Embedding Sets", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings -- distributed representations of words -- in deep learning\nare beneficial for many tasks in natural language processing (NLP). However,\ndifferent embedding sets vary greatly in quality and characteristics of the\ncaptured semantics. Instead of relying on a more advanced algorithm for\nembedding learning, this paper proposes an ensemble approach of combining\ndifferent public embedding sets with the aim of learning meta-embeddings.\nExperiments on word similarity and analogy tasks and on part-of-speech tagging\nshow better performance of meta-embeddings compared to individual embedding\nsets. One advantage of meta-embeddings is the increased vocabulary coverage. We\nwill release our meta-embeddings publicly.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 09:29:22 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 08:29:54 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1508.04271", "submitter": "Jan Botha", "authors": "Jan A. Botha", "title": "Probabilistic Modelling of Morphologically Rich Languages", "comments": "DPhil thesis, University of Oxford, submitted and accepted 2014.\n  http://ora.ox.ac.uk/objects/uuid:8df7324f-d3b8-47a1-8b0b-3a6feb5f45c7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis investigates how the sub-structure of words can be accounted for\nin probabilistic models of language. Such models play an important role in\nnatural language processing tasks such as translation or speech recognition,\nbut often rely on the simplistic assumption that words are opaque symbols. This\nassumption does not fit morphologically complex language well, where words can\nhave rich internal structure and sub-word elements are shared across distinct\nword forms.\n  Our approach is to encode basic notions of morphology into the assumptions of\nthree different types of language models, with the intention that leveraging\nshared sub-word structure can improve model performance and help overcome data\nsparsity that arises from morphological processes.\n  In the context of n-gram language modelling, we formulate a new Bayesian\nmodel that relies on the decomposition of compound words to attain better\nsmoothing, and we develop a new distributed language model that learns vector\nrepresentations of morphemes and leverages them to link together\nmorphologically related words. In both cases, we show that accounting for word\nsub-structure improves the models' intrinsic performance and provides benefits\nwhen applied to other tasks, including machine translation.\n  We then shift the focus beyond the modelling of word sequences and consider\nmodels that automatically learn what the sub-word elements of a given language\nare, given an unannotated list of words. We formulate a novel model that can\nlearn discontiguous morphemes in addition to the more conventional contiguous\nmorphemes that most previous models are limited to. This approach is\ndemonstrated on Semitic languages, and we find that modelling discontiguous\nsub-word structures leads to improvements in the task of segmenting words into\ntheir contiguous morphemes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 10:29:10 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Botha", "Jan A.", ""]]}, {"id": "1508.04395", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel,\n  Yoshua Bengio", "title": "End-to-End Attention-based Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the current state-of-the-art Large Vocabulary Continuous Speech\nRecognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov\nModels (HMMs). Most of these systems contain separate components that deal with\nthe acoustic modelling, language modelling and sequence decoding. We\ninvestigate a more direct approach in which the HMM is replaced with a\nRecurrent Neural Network (RNN) that performs sequence prediction directly at\nthe character level. Alignment between the input features and the desired\ncharacter sequence is learned automatically by an attention mechanism built\ninto the RNN. For each predicted character, the attention mechanism scans the\ninput sequence and chooses relevant frames. We propose two methods to speed up\nthis operation: limiting the scan to a subset of most promising frames and\npooling over time the information contained in neighboring frames, thereby\nreducing source sequence length. Integrating an n-gram language model into the\ndecoding process yields recognition accuracies similar to other HMM-free\nRNN-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 17:40:00 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 23:07:20 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Chorowski", "Jan", ""], ["Serdyuk", "Dmitriy", ""], ["Brakel", "Philemon", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1508.04515", "submitter": "Wei Zhang", "authors": "Wei Zhang, Judith Gelernter", "title": "Exploring Metaphorical Senses and Word Representations for Identifying\n  Metonyms", "comments": "9 pages, 8 pages content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A metonym is a word with a figurative meaning, similar to a metaphor. Because\nmetonyms are closely related to metaphors, we apply features that are used\nsuccessfully for metaphor recognition to the task of detecting metonyms. On the\nACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system\nachieved 86.45% accuracy on the location metonyms. Our code can be found on\nGitHub.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 03:26:05 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Zhang", "Wei", ""], ["Gelernter", "Judith", ""]]}, {"id": "1508.04525", "submitter": "Wei Zhang", "authors": "Wei Zhang, Yang Yu, Osho Gupta, Judith Gelernter", "title": "Recognizing Extended Spatiotemporal Expressions by Actively Trained\n  Average Perceptron Ensembles", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise geocoding and time normalization for text requires that location and\ntime phrases be identified. Many state-of-the-art geoparsers and temporal\nparsers suffer from low recall. Categories commonly missed by parsers are:\nnouns used in a non- spatiotemporal sense, adjectival and adverbial phrases,\nprepositional phrases, and numerical phrases. We collected and annotated data\nset by querying commercial web searches API with such spatiotemporal\nexpressions as were missed by state-of-the- art parsers. Due to the high cost\nof sentence annotation, active learning was used to label training data, and a\nnew strategy was designed to better select training examples to reduce labeling\ncost. For the learning algorithm, we applied an average perceptron trained\nFeaturized Hidden Markov Model (FHMM). Five FHMM instances were used to create\nan ensemble, with the output phrase selected by voting. Our ensemble model was\ntested on a range of sequential labeling tasks, and has shown competitive\nperformance. Our contributions include (1) an new dataset annotated with named\nentities and expanded spatiotemporal expressions; (2) a comparison of inference\nalgorithms for ensemble models showing the superior accuracy of Belief\nPropagation over Viterbi Decoding; (3) a new example re-weighting method for\nactive ensemble learning that 'memorizes' the latest examples trained; (4) a\nspatiotemporal parser that jointly recognizes expanded spatiotemporal\nexpressions as well as named entities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 04:17:47 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Zhang", "Wei", ""], ["Yu", "Yang", ""], ["Gupta", "Osho", ""], ["Gelernter", "Judith", ""]]}, {"id": "1508.04562", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Aaron Gerow, Jaan Altosaar, James Evans, Richard Jean\n  So", "title": "Fast, Flexible Models for Discovering Topic Correlation across\n  Weakly-Related Collections", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak topic correlation across document collections with different numbers of\ntopics in individual collections presents challenges for existing\ncross-collection topic models. This paper introduces two probabilistic topic\nmodels, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address\nproblems that can arise when analyzing large, asymmetric, and potentially\nweakly-related collections. Topic correlations in weakly-related collections\ntypically lie in the tail of the topic distribution, where they would be\noverlooked by models unable to fit large numbers of topics. To efficiently\nmodel this long tail for large-scale analysis, our models implement a parallel\nsampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et\nal., 2015). The models are first evaluated on synthetic data, generated to\nsimulate various collection-level asymmetries. We then present a case study of\nmodeling over 300k documents in collections of sciences and humanities research\nfrom JSTOR.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:30:37 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Zhang", "Jingwei", ""], ["Gerow", "Aaron", ""], ["Altosaar", "Jaan", ""], ["Evans", "James", ""], ["So", "Richard Jean", ""]]}, {"id": "1508.05051", "submitter": "Kenton Murray", "authors": "Kenton Murray and David Chiang", "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been shown to improve performance across a range of\nnatural-language tasks. However, designing and training them can be\ncomplicated. Frequently, researchers resort to repeated experimentation to pick\noptimal settings. In this paper, we address the issue of choosing the correct\nnumber of units in hidden layers. We introduce a method for automatically\nadjusting network size by pruning out hidden units through $\\ell_{\\infty,1}$\nand $\\ell_{2,1}$ regularization. We apply this method to language modeling and\ndemonstrate its ability to correctly choose the number of hidden units while\nmaintaining perplexity. We also include these models in a machine translation\ndecoder and show that these smaller neural models maintain the significant\nimprovements of their unpruned versions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 17:21:50 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Murray", "Kenton", ""], ["Chiang", "David", ""]]}, {"id": "1508.05154", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen and Brendan O'Connor", "title": "Posterior calibration and exploratory analysis for natural language\n  processing models", "comments": "15 pages (including supplementary information), proceedings of EMNLP\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many models in natural language processing define probabilistic distributions\nover linguistic structures. We argue that (1) the quality of a model' s\nposterior distribution can and should be directly evaluated, as to whether\nprobabilities correspond to empirical frequencies, and (2) NLP uncertainty can\nbe projected not only to pipeline components, but also to exploratory data\nanalysis, telling a user when to trust and not trust the NLP analysis. We\npresent a method to analyze calibration, and apply it to compare the\nmiscalibration of several commonly used models. We also contribute a\ncoreference sampling algorithm that can create confidence intervals for a\npolitical event extraction task.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 00:25:51 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 17:26:24 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Nguyen", "Khanh", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1508.05163", "submitter": "Yustinus Soelistio Eko", "authors": "Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra", "title": "Simple Text Mining for Sentiment Analysis of Political Figure Using\n  Naive Bayes Classifier Method", "comments": "5 pages, published in the Proceedings of the 7th ICTS", "journal-ref": null, "doi": "10.12962/p9772338185001.a18", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining can be applied to many fields. One of the application is using\ntext mining in digital newspaper to do politic sentiment analysis. In this\npaper sentiment analysis is applied to get information from digital news\narticles about its positive or negative sentiment regarding particular\npolitician. This paper suggests a simple model to analyze digital newspaper\nsentiment polarity using naive Bayes classifier method. The model uses a set of\ninitial data to begin with which will be updated when new information appears.\nThe model showed promising result when tested and can be implemented to some\nother sentiment analysis problems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 01:40:54 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Soelistio", "Yustinus Eko", ""], ["Surendra", "Martinus Raditia Sigit", ""]]}, {"id": "1508.05326", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D.\n  Manning", "title": "A large annotated corpus for learning natural language inference", "comments": "To appear at EMNLP 2015. The data will be posted shortly before the\n  conference (the week of 14 Sep) at http://nlp.stanford.edu/projects/snli/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding entailment and contradiction is fundamental to understanding\nnatural language, and inference about entailment and contradiction is a\nvaluable testing ground for the development of semantic representations.\nHowever, machine learning research in this area has been dramatically limited\nby the lack of large-scale resources. To address this, we introduce the\nStanford Natural Language Inference corpus, a new, freely available collection\nof labeled sentence pairs, written by humans doing a novel grounded task based\non image captioning. At 570K pairs, it is two orders of magnitude larger than\nall other resources of its type. This increase in scale allows lexicalized\nclassifiers to outperform some sophisticated existing entailment models, and it\nallows a neural network-based model to perform competitively on natural\nlanguage inference benchmarks for the first time.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 16:17:01 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Angeli", "Gabor", ""], ["Potts", "Christopher", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1508.05508", "submitter": "Baolin Peng", "authors": "Baolin Peng, Zhengdong Lu, Hang Li and Kam-Fai Wong", "title": "Towards Neural Network-based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Reasoner, a framework for neural network-based reasoning\nover natural language sentences. Given a question, Neural Reasoner can infer\nover multiple supporting facts and find an answer to the question in specific\nforms. Neural Reasoner has 1) a specific interaction-pooling mechanism,\nallowing it to examine multiple facts, and 2) a deep architecture, allowing it\nto model the complicated logical relations in reasoning tasks. Assuming no\nparticular structure exists in the question and facts, Neural Reasoner is able\nto accommodate different types of reasoning and different forms of language\nexpressions. Despite the model complexity, Neural Reasoner can still be trained\neffectively in an end-to-end manner. Our empirical studies show that Neural\nReasoner can outperform existing neural reasoning systems with remarkable\nmargins on two difficult artificial tasks (Positional Reasoning and Path\nFinding) proposed in [8]. For example, it improves the accuracy on Path\nFinding(10K) from 33.4% [6] to over 98%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:15:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Peng", "Baolin", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1508.05565", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions and a Provably Efficient Algorithm\n  for Separable Topic Discovery", "comments": "Typo corrected; Revised argument in Lemma 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop necessary and sufficient conditions and a novel provably\nconsistent and efficient algorithm for discovering topics (latent factors) from\nobservations (documents) that are realized from a probabilistic mixture of\nshared latent factors that have certain properties. Our focus is on the class\nof topic models in which each shared latent factor contains a novel word that\nis unique to that factor, a property that has come to be known as separability.\nOur algorithm is based on the key insight that the novel words correspond to\nthe extreme points of the convex hull formed by the row-vectors of a suitably\nnormalized word co-occurrence matrix. We leverage this geometric insight to\nestablish polynomial computation and sample complexity bounds based on a few\nisotropic random projections of the rows of the normalized word co-occurrence\nmatrix. Our proposed random-projections-based algorithm is naturally amenable\nto an efficient distributed implementation and is attractive for modern\nweb-scale distributed data mining applications.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 03:44:26 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 18:26:33 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1508.05817", "submitter": "Marco Guerini", "authors": "Marco Guerini, G\\\"ozde \\\"Ozbal, Carlo Strapparava", "title": "Echoes of Persuasion: The Effect of Euphony in Persuasive Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the effect of various lexical, syntactic, semantic and stylistic\nfeatures have been addressed in persuasive language from a computational point\nof view, the persuasive effect of phonetics has received little attention. By\nmodeling a notion of euphony and analyzing four datasets comprising persuasive\nand non-persuasive sentences in different domains (political speeches, movie\nquotes, slogans and tweets), we explore the impact of sounds on different forms\nof persuasiveness. We conduct a series of analyses and prediction experiments\nwithin and across datasets. Our results highlight the positive role of phonetic\ndevices on persuasion.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 14:15:39 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Guerini", "Marco", ""], ["\u00d6zbal", "G\u00f6zde", ""], ["Strapparava", "Carlo", ""]]}, {"id": "1508.05902", "submitter": "Arun Maiya", "authors": "Arun S. Maiya", "title": "A Framework for Comparing Groups of Documents", "comments": "6 pages; 2015 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP '15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for comparing multiple groups of documents. A\nbipartite graph model is proposed where document groups are represented as one\nnode set and the comparison criteria are represented as the other node set.\nUsing this model, we present basic algorithms to extract insights into\nsimilarities and differences among the document groups. Finally, we demonstrate\nthe versatility of our framework through an analysis of NSF funding programs\nfor basic research.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 18:14:34 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Maiya", "Arun S.", ""]]}, {"id": "1508.06034", "submitter": "Jun-Ping Ng", "authors": "Jun-Ping Ng, Viktoria Abrecht", "title": "Better Summarization Evaluation with Word Embeddings for ROUGE", "comments": "Pre-print - To appear in proceedings of the Conference on Empirical\n  Methods in Natural Language Processing (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ROUGE is a widely adopted, automatic evaluation measure for text\nsummarization. While it has been shown to correlate well with human judgements,\nit is biased towards surface lexical similarities. This makes it unsuitable for\nthe evaluation of abstractive summarization, or summaries with substantial\nparaphrasing. We study the effectiveness of word embeddings to overcome this\ndisadvantage of ROUGE. Specifically, instead of measuring lexical overlaps,\nword embeddings are used to compute the semantic similarity of the words used\nin summaries instead. Our experimental results show that our proposal is able\nto achieve better correlations with human judgements when measured with the\nSpearman and Kendall rank coefficients.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 05:04:53 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Ng", "Jun-Ping", ""], ["Abrecht", "Viktoria", ""]]}, {"id": "1508.06044", "submitter": "Shengliang Xu", "authors": "Hanchuan Li, Haichen Shen, Shengliang Xu and Congle Zhang", "title": "Visualizing NLP annotations for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing NLP annotation is useful for the collection of training data for\nthe statistical NLP approaches. Existing toolkits either provide limited visual\naid, or introduce comprehensive operators to realize sophisticated linguistic\nrules. Workers must be well trained to use them. Their audience thus can hardly\nbe scaled to large amounts of non-expert crowdsourced workers. In this paper,\nwe present CROWDANNO, a visualization toolkit to allow crowd-sourced workers to\nannotate two general categories of NLP problems: clustering and parsing.\nWorkers can finish the tasks with simplified operators in an interactive\ninterface, and fix errors conveniently. User studies show our toolkit is very\nfriendly to NLP non-experts, and allow them to produce high quality labels for\nseveral sophisticated problems. We release our source code and toolkit to spur\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 06:34:00 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Li", "Hanchuan", ""], ["Shen", "Haichen", ""], ["Xu", "Shengliang", ""], ["Zhang", "Congle", ""]]}, {"id": "1508.06161", "submitter": "Scott Bronikowski", "authors": "Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, and Jeffrey\n  Mark Siskind", "title": "Robot Language Learning, Generation, and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework which supports grounding natural-language\nsemantics in robotic driving. This framework supports acquisition (learning\ngrounded meanings of nouns and prepositions from human annotation of robotic\ndriving paths), generation (using such acquired meanings to generate sentential\ndescription of new robotic driving paths), and comprehension (using such\nacquired meanings to support automated driving to accomplish navigational goals\nspecified in natural language). We evaluate the performance of these three\ntasks by having independent human judges rate the semantic fidelity of the\nsentences associated with paths, achieving overall average correctness of 94.6%\nand overall average completeness of 85.6%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 14:10:21 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Barrett", "Daniel Paul", ""], ["Bronikowski", "Scott Alan", ""], ["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1508.06374", "submitter": "Alexander Koplenig", "authors": "Alexander Koplenig", "title": "A fully data-driven method to identify (correlated) changes in\n  diachronic corpora", "comments": "typological changes only: reference-source-not-found-errors removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method for measuring synchronic corpus (dis-)similarity put\nforward by Kilgarriff (2001) is adapted and extended to identify trends and\ncorrelated changes in diachronic text data, using the Corpus of Historical\nAmerican English (Davies 2010a) and the Google Ngram Corpora (Michel et al.\n2010a). This paper shows that this fully data-driven method, which extracts\nword types that have undergone the most pronounced change in frequency in a\ngiven period of time, is computationally very cheap and that it allows\ninterpretations of diachronic trends that are both intuitively plausible and\nmotivated from the perspective of information theory. Furthermore, it\ndemonstrates that the method is able to identify correlated linguistic changes\nand diachronic shifts that can be linked to historical events. Finally, it can\nhelp to improve diachronic POS tagging and complement existing NLP approaches.\nThis indicates that the approach can facilitate an improved understanding of\ndiachronic processes in language change.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 06:18:51 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 06:29:43 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Koplenig", "Alexander", ""]]}, {"id": "1508.06451", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho and Carlos G\\'omez-Rodr\\'iguez", "title": "Crossings as a side effect of dependency lengths", "comments": "the discussion section has been expanded significantly; in press in\n  Complexity (Wiley)", "journal-ref": "Complexity, 21(S2):320-328, 2016", "doi": "10.1002/cplx.21810", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The syntactic structure of sentences exhibits a striking regularity:\ndependencies tend to not cross when drawn above the sentence. We investigate\ntwo competing explanations. The traditional hypothesis is that this trend\narises from an independent principle of syntax that reduces crossings\npractically to zero. An alternative to this view is the hypothesis that\ncrossings are a side effect of dependency lengths, i.e. sentences with shorter\ndependency lengths should tend to have fewer crossings. We are able to reject\nthe traditional view in the majority of languages considered. The alternative\nhypothesis can lead to a more parsimonious theory of language.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 11:39:18 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 16:18:49 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "1508.06491", "submitter": "Jacob Andreas", "authors": "Jacob Andreas and Dan Klein", "title": "Alignment-based compositional semantics for instruction following", "comments": "in proceedings of EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper describes an alignment-based model for interpreting natural\nlanguage instructions in context. We approach instruction following as a search\nover plans, scoring sequences of actions conditioned on structured observations\nof text and the environment. By explicitly modeling both the low-level\ncompositional structure of individual actions and the high-level structure of\nfull plans, we are able to learn both grounded representations of sentence\nmeaning and pragmatic constraints on interpretation. To demonstrate the model's\nflexibility, we apply it to a diverse set of benchmark tasks. On every task, we\noutperform strong task-specific baselines, and achieve several new\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 13:44:54 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 19:51:09 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Andreas", "Jacob", ""], ["Klein", "Dan", ""]]}, {"id": "1508.06615", "submitter": "Yoon Kim", "authors": "Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush", "title": "Character-Aware Neural Language Models", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 19:25:34 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 23:18:00 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 03:18:13 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2015 22:59:24 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Kim", "Yoon", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1508.06669", "submitter": "Yanran Li", "authors": "Yanran Li, Wenjie Li, Fei Sun and Sujian Li", "title": "Component-Enhanced Chinese Character Embeddings", "comments": "6 pages, 2 figures, conference, EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word representations are very useful for capturing semantic\ninformation and have been successfully applied in a variety of NLP tasks,\nespecially on English. In this work, we innovatively develop two\ncomponent-enhanced Chinese character embedding models and their bigram\nextensions. Distinguished from English word embeddings, our models explore the\ncompositions of Chinese characters, which often serve as semantic indictors\ninherently. The evaluations on both word similarity and text classification\ndemonstrate the effectiveness of our models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 21:25:25 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Li", "Yanran", ""], ["Li", "Wenjie", ""], ["Sun", "Fei", ""], ["Li", "Sujian", ""]]}, {"id": "1508.07266", "submitter": "Suin Kim", "authors": "Suin Kim, Sungjoon Park, Scott A. Hale, Sooyoung Kim, Jeongmin Byun\n  and Alice Oh", "title": "Understanding Editing Behaviors in Multilingual Wikipedia", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0155305", "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingualism is common offline, but we have a more limited understanding\nof the ways multilingualism is displayed online and the roles that\nmultilinguals play in the spread of content between speakers of different\nlanguages. We take a computational approach to studying multilingualism using\none of the largest user-generated content platforms, Wikipedia. We study\nmultilingualism by collecting and analyzing a large dataset of the content\nwritten by multilingual editors of the English, German, and Spanish editions of\nWikipedia. This dataset contains over two million paragraphs edited by over\n15,000 multilingual users from July 8 to August 9, 2013. We analyze these\nmultilingual editors in terms of their engagement, interests, and language\nproficiency in their primary and non-primary (secondary) languages and find\nthat the English edition of Wikipedia displays different dynamics from the\nSpanish and German editions. Users primarily editing the Spanish and German\neditions make more complex edits than users who edit these editions as a second\nlanguage. In contrast, users editing the English edition as a second language\nmake edits that are just as complex as the edits by users who primarily edit\nthe English edition. In this way, English serves a special role bringing\ntogether content written by multilinguals from many language editions.\nNonetheless, language remains a formidable hurdle to the spread of content: we\nfind evidence for a complexity barrier whereby editors are less likely to edit\ncomplex content in a second language. In addition, we find that multilinguals\nare less engaged and show lower levels of language proficiency in their second\nlanguages. We also examine the topical interests of multilingual editors and\nfind that there is no significant difference between primary and non-primary\neditors in each language.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 16:21:03 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Kim", "Suin", ""], ["Park", "Sungjoon", ""], ["Hale", "Scott A.", ""], ["Kim", "Sooyoung", ""], ["Byun", "Jeongmin", ""], ["Oh", "Alice", ""]]}, {"id": "1508.07544", "submitter": "Dong Nguyen", "authors": "Dong Nguyen, A. Seza Do\\u{g}ru\\\"oz, Carolyn P. Ros\\'e, Franciska de\n  Jong", "title": "Computational Sociolinguistics: A Survey", "comments": "To appear in Computational Linguistics. Accepted for publication:\n  18th February, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is a social phenomenon and variation is inherent to its social\nnature. Recently, there has been a surge of interest within the computational\nlinguistics (CL) community in the social dimension of language. In this article\nwe present a survey of the emerging field of \"Computational Sociolinguistics\"\nthat reflects this increased interest. We aim to provide a comprehensive\noverview of CL research on sociolinguistic themes, featuring topics such as the\nrelation between language and social identity, language use in social\ninteraction and multilingual communication. Moreover, we demonstrate the\npotential for synergy between the research communities involved, by showing how\nthe large-scale data-driven methods that are widely used in CL can complement\nexisting sociolinguistic studies, and how sociolinguistics can inform and\nchallenge the methods and assumptions employed in CL studies. We hope to convey\nthe possible benefits of a closer collaboration between the two communities and\nconclude with a discussion of open challenges.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 08:39:02 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 10:14:24 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Nguyen", "Dong", ""], ["Do\u011fru\u00f6z", "A. Seza", ""], ["Ros\u00e9", "Carolyn P.", ""], ["de Jong", "Franciska", ""]]}, {"id": "1508.07555", "submitter": "Yanping Chen", "authors": "Yanping Chen", "title": "An Event Network for Exploring Open Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an event network is presented for exploring open information,\nwhere linguistic units about an event are organized for analysing. The process\nis divided into three steps: document event detection, event network\nconstruction and event network analysis. First, by implementing event detection\nor tracking, documents are retrospectively (or on-line) organized into document\nevents. Secondly, for each of the document event, linguistic units are\nextracted and combined into event networks. Thirdly, various analytic methods\nare proposed for event network analysis. In our application methodologies are\npresented for exploring open information.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 11:22:38 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Chen", "Yanping", ""]]}, {"id": "1508.07709", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster and Gertjan van Noord and Ivan Titov", "title": "Word Representations, Tree Models and Syntactic Functions", "comments": "Add github code repository link. Fix equation 4.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word representations induced from models with discrete latent variables\n(e.g.\\ HMMs) have been shown to be beneficial in many NLP applications. In this\nwork, we exploit labeled syntactic dependency trees and formalize the induction\nproblem as unsupervised learning of tree-structured hidden Markov models.\nSyntactic functions are used as additional observed variables in the model,\ninfluencing both transition and emission components. Such syntactic information\ncan potentially lead to capturing more fine-grain and functional distinctions\nbetween words, which, in turn, may be desirable in many NLP applications. We\nevaluate the word representations on two tasks -- named entity recognition and\nsemantic frame identification. We observe improvements from exploiting\nsyntactic function information in both cases, and the results rivaling those of\nstate-of-the-art representation learning methods. Additionally, we revisit the\nrelationship between sequential and unlabeled-tree models and find that the\nadvantage of the latter is not self-evident.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 07:52:50 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 13:26:56 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["\u0160uster", "Simon", ""], ["van Noord", "Gertjan", ""], ["Titov", "Ivan", ""]]}, {"id": "1508.07909", "submitter": "Rico Sennrich", "authors": "Rico Sennrich, Barry Haddow and Alexandra Birch", "title": "Neural Machine Translation of Rare Words with Subword Units", "comments": "accepted at ACL 2016; new in this version: figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural machine translation (NMT) models typically operate with a fixed\nvocabulary, but translation is an open-vocabulary problem. Previous work\naddresses the translation of out-of-vocabulary words by backing off to a\ndictionary. In this paper, we introduce a simpler and more effective approach,\nmaking the NMT model capable of open-vocabulary translation by encoding rare\nand unknown words as sequences of subword units. This is based on the intuition\nthat various word classes are translatable via smaller units than words, for\ninstance names (via character copying or transliteration), compounds (via\ncompositional translation), and cognates and loanwords (via phonological and\nmorphological transformations). We discuss the suitability of different word\nsegmentation techniques, including simple character n-gram models and a\nsegmentation based on the byte pair encoding compression algorithm, and\nempirically show that subword models improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks English-German and English-Russian by\n1.1 and 1.3 BLEU, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 16:37:31 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 15:41:25 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 14:56:06 GMT"}, {"version": "v4", "created": "Fri, 3 Jun 2016 15:01:02 GMT"}, {"version": "v5", "created": "Fri, 10 Jun 2016 14:45:08 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Sennrich", "Rico", ""], ["Haddow", "Barry", ""], ["Birch", "Alexandra", ""]]}]