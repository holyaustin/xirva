[{"id": "1109.0069", "submitter": "Shibamouli Lahiri", "authors": "Shibamouli Lahiri, Xiaofei Lu", "title": "Inter-rater Agreement on Sentence Formality", "comments": "5 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formality is one of the most important dimensions of writing style variation.\nIn this study we conducted an inter-rater reliability experiment for assessing\nsentence formality on a five-point Likert scale, and obtained good agreement\nresults as well as different rating distributions for different sentence\ncategories. We also performed a difficulty analysis to identify the bottlenecks\nof our rating procedure. Our main objective is to design an automatic scoring\nmechanism for sentence-level formality, and this study is important for that\npurpose.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 02:20:12 GMT"}, {"version": "v2", "created": "Sun, 20 Apr 2014 16:56:40 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Lahiri", "Shibamouli", ""], ["Lu", "Xiaofei", ""]]}, {"id": "1109.0624", "submitter": "Marwa Graja", "authors": "Marwa Graja and Maher Jaoua and Lamia Hadrich Belguith", "title": "Building Ontologies to Understand Spoken Tunisian Dialect", "comments": "10 pages, 3 figures", "journal-ref": "International Journal of Computer Science, Engineering and\n  Applications (IJCSEA) Vol.1, No.4, August 2011", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to understand spoken Tunisian dialect based on\nlexical semantic. This method takes into account the specificity of the\nTunisian dialect which has no linguistic processing tools. This method is\nontology-based which allows exploiting the ontological concepts for semantic\nannotation and ontological relations for speech interpretation. This\ncombination increases the rate of comprehension and limits the dependence on\nlinguistic resources. This paper also details the process of building the\nontology used for annotation and interpretation of Tunisian dialect in the\ncontext of speech understanding in dialogue systems for restricted domain.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2011 14:30:44 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Graja", "Marwa", ""], ["Jaoua", "Maher", ""], ["Belguith", "Lamia Hadrich", ""]]}, {"id": "1109.1618", "submitter": "Son Doan", "authors": "Son Doan, Bao-Khanh Ho Vo, and Nigel Collier", "title": "An analysis of Twitter messages in the 2011 Tohoku Earthquake", "comments": "9 pages, 4 figures, eHealth 2011 conference, Malaga (Spain)\n  (accepted)", "journal-ref": "Lecture Notes of the Institute for Computer Sciences, Social\n  Informatics and Telecommunications Engineering, 2012, Volume 91, Part 4,\n  58-66", "doi": "10.1007/978-3-642-29262-0_8", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media such as Facebook and Twitter have proven to be a useful resource\nto understand public opinion towards real world events. In this paper, we\ninvestigate over 1.5 million Twitter messages (tweets) for the period 9th March\n2011 to 31st May 2011 in order to track awareness and anxiety levels in the\nTokyo metropolitan district to the 2011 Tohoku Earthquake and subsequent\ntsunami and nuclear emergencies. These three events were tracked using both\nEnglish and Japanese tweets. Preliminary results indicated: 1) close\ncorrespondence between Twitter data and earthquake events, 2) strong\ncorrelation between English and Japanese tweets on the same events, 3) tweets\nin the native language play an important roles in early warning, 4) tweets\nshowed how quickly Japanese people's anxiety returned to normal levels after\nthe earthquake event. Several distinctions between English and Japanese tweets\non earthquake events are also discussed. The results suggest that Twitter data\ncan be used as a useful resource for tracking the public mood of populations\naffected by natural disasters as well as an early warning system.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2011 03:13:12 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Doan", "Son", ""], ["Vo", "Bao-Khanh Ho", ""], ["Collier", "Nigel", ""]]}, {"id": "1109.2128", "submitter": "Daniel Bryce", "authors": "Gunes Erkan, Dragomir R. Radev", "title": "LexRank: Graph-based Lexical Centrality as Salience in Text\n  Summarization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  457-479, 2004", "doi": "10.1613/jair.1523", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stochastic graph-based method for computing relative\nimportance of textual units for Natural Language Processing. We test the\ntechnique on the problem of Text Summarization (TS). Extractive TS relies on\nthe concept of sentence salience to identify the most important sentences in a\ndocument or set of documents. Salience is typically defined in terms of the\npresence of particular important words or in terms of similarity to a centroid\npseudo-sentence. We consider a new approach, LexRank, for computing sentence\nimportance based on the concept of eigenvector centrality in a graph\nrepresentation of sentences. In this model, a connectivity matrix based on\nintra-sentence cosine similarity is used as the adjacency matrix of the graph\nrepresentation of sentences. Our system, based on LexRank ranked in first place\nin more than one task in the recent DUC 2004 evaluation. In this paper we\npresent a detailed analysis of our approach and apply it to a larger data set\nincluding data from earlier DUC evaluations. We discuss several methods to\ncompute centrality using the similarity graph. The results show that\ndegree-based methods (including LexRank) outperform both centroid-based methods\nand other systems participating in DUC in most of the cases. Furthermore, the\nLexRank with threshold method outperforms the other degree-based techniques\nincluding continuous LexRank. We also show that our approach is quite\ninsensitive to the noise in the data that may result from an imperfect topical\nclustering of documents.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:20:38 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2011 20:20:06 GMT"}], "update_date": "2011-09-28", "authors_parsed": [["Erkan", "Gunes", ""], ["Radev", "Dragomir R.", ""]]}, {"id": "1109.2130", "submitter": "A. Montoyo", "authors": "A. Montoyo, M. Palomar, G. Rigau, A. Suarez", "title": "Combining Knowledge- and Corpus-based Word-Sense-Disambiguation Methods", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  299-330, 2005", "doi": "10.1613/jair.1529", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we concentrate on the resolution of the lexical ambiguity that\narises when a given word has several different meanings. This specific task is\ncommonly referred to as word sense disambiguation (WSD). The task of WSD\nconsists of assigning the correct sense to words using an electronic dictionary\nas the source of word definitions. We present two WSD methods based on two main\nmethodological approaches in this research area: a knowledge-based method and a\ncorpus-based method. Our hypothesis is that word-sense disambiguation requires\nseveral knowledge sources in order to solve the semantic ambiguity of the\nwords. These sources can be of different kinds--- for example, syntagmatic,\nparadigmatic or statistical information. Our approach combines various sources\nof knowledge, through combinations of the two WSD methods mentioned above.\nMainly, the paper concentrates on how to combine these methods and sources of\ninformation in order to achieve good results in the disambiguation. Finally,\nthis paper presents a comprehensive study and experimental work on evaluation\nof the methods and their combinations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:22:04 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Montoyo", "A.", ""], ["Palomar", "M.", ""], ["Rigau", "G.", ""], ["Suarez", "A.", ""]]}, {"id": "1109.2136", "submitter": "P. W. Jordan", "authors": "P. W. Jordan, M. A. Walker", "title": "Learning Content Selection Rules for Generating Object Descriptions in\n  Dialogue", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 24, pages\n  157-194, 2005", "doi": "10.1613/jair.1591", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental requirement of any task-oriented dialogue system is the ability\nto generate object descriptions that refer to objects in the task domain. The\nsubproblem of content selection for object descriptions in task-oriented\ndialogue has been the focus of much previous work and a large number of models\nhave been proposed. In this paper, we use the annotated COCONUT corpus of\ntask-oriented design dialogues to develop feature sets based on Dale and\nReiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact\nmodel, and Jordans (2000b) intentional influences model, and use these feature\nsets in a machine learning experiment to automatically learn a model of content\nselection for object descriptions. Since Dale and Reiters model requires a\nrepresentation of discourse structure, the corpus annotations are used to\nderive a representation based on Grosz and Sidners (1986) theory of the\nintentional structure of discourse, as well as two very simple representations\nof discourse structure based purely on recency. We then apply the\nrule-induction program RIPPER to train and test the content selection component\nof an object description generator on a set of 393 object descriptions from the\ncorpus. To our knowledge, this is the first reported experiment of a trainable\ncontent selection component for object description generation in dialogue.\nThree separate content selection models that are based on the three theoretical\nmodels, all independently achieve accuracies significantly above the majority\nclass baseline (17%) on unseen test data, with the intentional influences model\n(42.4%) performing significantly better than either the incremental model\n(30.4%) or the conceptual pact model (28.9%). But the best performing models\ncombine all the feature sets, achieving accuracies near 60%. Surprisingly, a\nsimple recency-based representation of discourse structure does as well as one\nbased on intentional structure. To our knowledge, this is also the first\nempirical comparison of a representation of Grosz and Sidners model of\ndiscourse structure with a simpler model for any generation task.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 20:24:57 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Jordan", "P. W.", ""], ["Walker", "M. A.", ""]]}, {"id": "1109.2657", "submitter": "EPTCS", "authors": "Seyed M. Montazeri (University of Gothenburg, Sweden), Nivir K.S. Roy\n  (University of Gothenburg, Sweden), Gerardo Schneider (Chalmers | University\n  of Gothenburg, Sweden)", "title": "From Contracts in Structured English to CL Specifications", "comments": "In Proceedings FLACOS 2011, arXiv:1109.2399", "journal-ref": "EPTCS 68, 2011, pp. 55-69", "doi": "10.4204/EPTCS.68.6", "report-no": null, "categories": "cs.CL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a framework to analyze conflicts of contracts\nwritten in structured English. A contract that has manually been rewritten in a\nstructured English is automatically translated into a formal language using the\nGrammatical Framework (GF). In particular we use the contract language CL as a\ntarget formal language for this translation. In our framework CL specifications\ncould then be input into the tool CLAN to detect the presence of conflicts\n(whether there are contradictory obligations, permissions, and prohibitions. We\nalso use GF to get a version in (restricted) English of CL formulae. We discuss\nthe implementation of such a framework.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2011 01:25:43 GMT"}], "update_date": "2011-09-14", "authors_parsed": [["Montazeri", "Seyed M.", "", "University of Gothenburg, Sweden"], ["Roy", "Nivir K. S.", "", "University of Gothenburg, Sweden"], ["Schneider", "Gerardo", "", "Chalmers | University\n  of Gothenburg, Sweden"]]}, {"id": "1109.4531", "submitter": "Janne V. Kujala", "authors": "Janne V. Kujala, Aleksi Keurulainen", "title": "A Probabilistic Approach to Pronunciation by Analogy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between written and spoken words is convoluted in languages\nwith a deep orthography such as English and therefore it is difficult to devise\nexplicit rules for generating the pronunciations for unseen words.\nPronunciation by analogy (PbA) is a data-driven method of constructing\npronunciations for novel words from concatenated segments of known words and\ntheir pronunciations. PbA performs relatively well with English and outperforms\nseveral other proposed methods. However, the best published word accuracy of\n65.5% (for the 20,000 word NETtalk corpus) suggests there is much room for\nimprovement in it.\n  Previous PbA algorithms have used several different scoring strategies such\nas the product of the frequencies of the component pronunciations of the\nsegments, or the number of different segmentations that yield the same\npronunciation, and different combinations of these methods, to evaluate the\ncandidate pronunciations. In this article, we instead propose to use a\nprobabilistically justified scoring rule. We show that this principled approach\nalone yields better accuracy (66.21% for the NETtalk corpus) than any\npreviously published PbA algorithm. Furthermore, combined with certain ad hoc\nmodifications motivated by earlier algorithms, the performance climbs up to\n66.6%, and further improvements are possible by combining this method with\nother methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 13:57:49 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Kujala", "Janne V.", ""], ["Keurulainen", "Aleksi", ""]]}, {"id": "1109.4906", "submitter": "Odile Piton", "authors": "Odile Piton (SAMM), Slim Mesfar (RIADI), H\\'el\\`ene Pignot (SAMM)", "title": "Automatic transcription of 17th century English text in Contemporary\n  English with NooJ: Method and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2006 we have undertaken to describe the differences between 17th\ncentury English and contemporary English thanks to NLP software. Studying a\ncorpus spanning the whole century (tales of English travellers in the Ottoman\nEmpire in the 17th century, Mary Astell's essay A Serious Proposal to the\nLadies and other literary texts) has enabled us to highlight various lexical,\nmorphological or grammatical singularities. Thanks to the NooJ linguistic\nplatform, we created dictionaries indexing the lexical variants and their\ntranscription in CE. The latter is often the result of the validation of forms\nrecognized dynamically by morphological graphs. We also built syntactical\ngraphs aimed at transcribing certain archaic forms in contemporary English. Our\nprevious research implied a succession of elementary steps alternating textual\nanalysis and result validation. We managed to provide examples of\ntranscriptions, but we have not created a global tool for automatic\ntranscription. Therefore we need to focus on the results we have obtained so\nfar, study the conditions for creating such a tool, and analyze possible\ndifficulties. In this paper, we will be discussing the technical and linguistic\naspects we have not yet covered in our previous work. We are using the results\nof previous research and proposing a transcription method for words or\nsequences identified as archaic.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 18:37:17 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Piton", "Odile", "", "SAMM"], ["Mesfar", "Slim", "", "RIADI"], ["Pignot", "H\u00e9l\u00e8ne", "", "SAMM"]]}, {"id": "1109.5798", "submitter": "Yuriy Ostapov", "authors": "Yuriy Ostapov", "title": "Object-oriented semantics of English in natural language understanding\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to the problem of natural language understanding is proposed.\nThe knowledge domain under consideration is the social behavior of people.\nEnglish sentences are translated into set of predicates of a semantic database,\nwhich describe persons, occupations, organizations, projects, actions, events,\nmessages, machines, things, animals, location and time of actions, relations\nbetween objects, thoughts, cause-and-effect relations, abstract objects. There\nis a knowledge base containing the description of semantics of objects\n(functions and structure), actions (motives and causes), and operations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 08:00:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ostapov", "Yuriy", ""]]}, {"id": "1109.6018", "submitter": "Lillian Lee", "authors": "Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li", "title": "User-level sentiment analysis incorporating social networks", "comments": "Proceedings of KDD 2011. Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that information about social relationships can be used to improve\nuser-level sentiment analysis. The main motivation behind our approach is that\nusers that are somehow \"connected\" may be more likely to hold similar opinions;\ntherefore, relationship information can complement what we can extract about a\nuser's viewpoints from their utterances. Employing Twitter as a source for our\nexperimental data, and working within a semi-supervised framework, we propose\nmodels that are induced either from the Twitter follower/followee network or\nfrom the network in Twitter formed by users referring to each other using \"@\"\nmentions. Our transductive learning results reveal that incorporating\nsocial-network information can indeed lead to statistically significant\nsentiment-classification improvements over the performance of an approach based\non Support Vector Machines having access only to textual features.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 20:00:47 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Tan", "Chenhao", ""], ["Lee", "Lillian", ""], ["Tang", "Jie", ""], ["Jiang", "Long", ""], ["Zhou", "Ming", ""], ["Li", "Ping", ""]]}, {"id": "1109.6341", "submitter": "H. Daume III", "authors": "H. Daume III, D. Marcu", "title": "Domain Adaptation for Statistical Classifiers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  101-126, 2006", "doi": "10.1613/jair.1872", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most basic assumption used in statistical learning theory is that\ntraining data and test data are drawn from the same underlying distribution.\nUnfortunately, in many applications, the \"in-domain\" test data is drawn from a\ndistribution that is related, but not identical, to the \"out-of-domain\"\ndistribution of the training data. We consider the common case in which labeled\nout-of-domain data is plentiful, but labeled in-domain data is scarce. We\nintroduce a statistical formulation of this problem in terms of a simple\nmixture model and present an instantiation of this framework to maximum entropy\nclassifiers and their linear chain counterparts. We present efficient inference\nalgorithms for this special case based on the technique of conditional\nexpectation maximization. Our experimental results show that our approach leads\nto improved performance on three real world tasks on four different data sets\nfrom the natural language processing domain.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 20:18:30 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Daume", "H.", "III"], ["Marcu", "D.", ""]]}]