[{"id": "1310.0201", "submitter": "Moreno Coco MIC", "authors": "Moreno I. Coco and Rick Dale", "title": "Cross-Recurrence Quantification Analysis of Categorical and Continuous\n  Time Series: an R package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the R package crqa to perform cross-recurrence\nquantification analysis of two time series of either a categorical or\ncontinuous nature. Streams of behavioral information, from eye movements to\nlinguistic elements, unfold over time. When two people interact, such as in\nconversation, they often adapt to each other, leading these behavioral levels\nto exhibit recurrent states. In dialogue, for example, interlocutors adapt to\neach other by exchanging interactive cues: smiles, nods, gestures, choice of\nwords, and so on. In order for us to capture closely the goings-on of dynamic\ninteraction, and uncover the extent of coupling between two individuals, we\nneed to quantify how much recurrence is taking place at these levels. Methods\navailable in crqa would allow researchers in cognitive science to pose such\nquestions as how much are two people recurrent at some level of analysis, what\nis the characteristic lag time for one person to maximally match another, or\nwhether one person is leading another. First, we set the theoretical ground to\nunderstand the difference between 'correlation' and 'co-visitation' when\ncomparing two time series, using an aggregative or cross-recurrence approach.\nThen, we describe more formally the principles of cross-recurrence, and show\nwith the current package how to carry out analyses applying them. We end the\npaper by comparing computational efficiency, and results' consistency, of crqa\nR package, with the benchmark MATLAB toolbox crptoolbox. We show perfect\ncomparability between the two libraries on both levels.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 09:20:21 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 16:09:07 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Coco", "Moreno I.", ""], ["Dale", "Rick", ""]]}, {"id": "1310.0573", "submitter": "Nisheeth Joshi", "authors": "Deepti Bhalla, Nisheeth Joshi, Iti Mathur", "title": "Improving the Quality of MT Output using Novel Name Entity Translation\n  Scheme", "comments": "In Proceedings of 2013 International Conference on Advances in\n  Computing, Communications and Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to machine translation by combining the\nstate of art name entity translation scheme. Improper translation of name\nentities lapse the quality of machine translated output. In this work, name\nentities are transliterated by using statistical rule based approach. This\npaper describes the translation and transliteration of name entities from\nEnglish to Punjabi. We have experimented on four types of name entities which\nare: Proper names, Location names, Organization names and miscellaneous.\nVarious rules for the purpose of syllabification have been constructed.\nTransliteration of name entities is accomplished with the help of Probability\ncalculation. N-Gram probabilities for the extracted syllables have been\ncalculated using statistical machine translation toolkit MOSES.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 05:58:52 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Bhalla", "Deepti", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1310.0575", "submitter": "Nisheeth Joshi", "authors": "Jyoti Singh, Nisheeth Joshi, Iti Mathur", "title": "Development of Marathi Part of Speech Tagger Using Statistical Approach", "comments": "In Proceedings of 2013 International Conference on Advances in\n  Computing, Communications and Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-of-speech (POS) tagging is a process of assigning the words in a text\ncorresponding to a particular part of speech. A fundamental version of POS\ntagging is the identification of words as nouns, verbs, adjectives etc. For\nprocessing natural languages, Part of Speech tagging is a prominent tool. It is\none of the simplest as well as most constant and statistical model for many NLP\napplications. POS Tagging is an initial stage of linguistics, text analysis\nlike information retrieval, machine translator, text to speech synthesis,\ninformation extraction etc. In POS Tagging we assign a Part of Speech tag to\neach word in a sentence and literature. Various approaches have been proposed\nto implement POS taggers. In this paper we present a Marathi part of speech\ntagger. It is morphologically rich language. Marathi is spoken by the native\npeople of Maharashtra. The general approach used for development of tagger is\nstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear\nidea about all the algorithms with suitable examples. It also introduces a tag\nset for Marathi which can be used for tagging Marathi text. In this paper we\nhave shown the development of the tagger as well as compared to check the\naccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,\nTrigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 06:04:53 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 10:57:37 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Singh", "Jyoti", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1310.0578", "submitter": "Nisheeth Joshi", "authors": "Vaishali Gupta, Nisheeth Joshi, Iti Mathur", "title": "Subjective and Objective Evaluation of English to Urdu Machine\n  Translation", "comments": "In Proceedings of 2013 International Conference on Advances in\n  Computing, Communications and Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is research based area where evaluation is very important\nphenomenon for checking the quality of MT output. The work is based on the\nevaluation of English to Urdu Machine translation. In this research work we\nhave evaluated the translation quality of Urdu language which has been\ntranslated by using different Machine Translation systems like Google, Babylon\nand Ijunoon. The evaluation process is done by using two approaches - Human\nevaluation and Automatic evaluation. We have worked for both the approaches\nwhere in human evaluation emphasis is given to scales and parameters while in\nautomatic evaluation emphasis is given to some automatic metric such as BLEU,\nGTM, METEOR and ATEC.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 06:10:49 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Gupta", "Vaishali", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1310.0581", "submitter": "Nisheeth Joshi", "authors": "Vaishali Gupta, Nisheeth Joshi, Iti Mathur", "title": "Rule Based Stemmer in Urdu", "comments": "In Proceedings of 4th International Conference on Computer and\n  Communication Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urdu is a combination of several languages like Arabic, Hindi, English,\nTurkish, Sanskrit etc. It has a complex and rich morphology. This is the reason\nwhy not much work has been done in Urdu language processing. Stemming is used\nto convert a word into its respective root form. In stemming, we separate the\nsuffix and prefix from the word. It is useful in search engines, natural\nlanguage processing and word processing, spell checkers, word parsing, word\nfrequency and count studies. This paper presents a rule based stemmer for Urdu.\nThe stemmer that we have discussed here is used in information retrieval. We\nhave also evaluated our results by verifying it with a human expert.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 06:15:03 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Gupta", "Vaishali", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}, {"id": "1310.0754", "submitter": "Mahima Sharma", "authors": "M.Thangarasu, R.Manavalan", "title": "Stemmers for Tamil Language: Performance Analysis", "comments": null, "journal-ref": "International Journal of Computer Science & Engineering\n  Technology, Vol. 4, No. 07, Jul 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming is the process of extracting root word from the given inflection\nword and also plays significant role in numerous application of Natural\nLanguage Processing (NLP). Tamil Language raises several challenges to NLP,\nsince it has rich morphological patterns than other languages. The rule based\napproach light-stemmer is proposed in this paper, to find stem word for given\ninflection Tamil word. The performance of proposed approach is compared to a\nrule based suffix removal stemmer based on correctly and incorrectly predicted.\nThe experimental result clearly show that the proposed approach light stemmer\nfor Tamil language perform better than suffix removal stemmer and also more\neffective in Information Retrieval System (IRS).\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 16:23:00 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Thangarasu", "M.", ""], ["Manavalan", "R.", ""]]}, {"id": "1310.1249", "submitter": "Andrzej Jarynowski", "authors": "Andrzej Jarynowski, Amir Rostami", "title": "Reading Stockholm Riots 2013 in social media by text-mining", "comments": "5p", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The riots in Stockholm in May 2013 were an event that reverberated in the\nworld media for its dimension of violence that had spread through the Swedish\ncapital. In this study we have investigated the role of social media in\ncreating media phenomena via text mining and natural language processing. We\nhave focused on two channels of communication for our analysis: Twitter and\nPoloniainfo.se (Forum of Polish community in Sweden). Our preliminary results\nshow some hot topics driving discussion related mostly to Swedish Police and\nSwedish Politics by counting word usage. Typical features for media\nintervention are presented. We have built networks of most popular phrases,\nclustered by categories (geography, media institution, etc.). Sentiment\nanalysis shows negative connotation with Police. The aim of this preliminary\nexploratory quantitative study was to generate questions and hypotheses, which\nwe could carefully follow by deeper more qualitative methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 13:04:45 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Jarynowski", "Andrzej", ""], ["Rostami", "Amir", ""]]}, {"id": "1310.1285", "submitter": "S\\'ebastien Harispe", "authors": "S\\'ebastien Harispe, Sylvie Ranwez, Stefan Janaqi, Jacky Montmain", "title": "Semantic Measures for the Comparison of Units of Language, Concepts or\n  Instances from Text and Knowledge Base Analysis", "comments": "survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic measures are widely used today to estimate the strength of the\nsemantic relationship between elements of various types: units of language\n(e.g., words, sentences, documents), concepts or even instances semantically\ncharacterized (e.g., diseases, genes, geographical locations). Semantic\nmeasures play an important role to compare such elements according to semantic\nproxies: texts and knowledge representations, which support their meaning or\ndescribe their nature. Semantic measures are therefore essential for designing\nintelligent agents which will for example take advantage of semantic analysis\nto mimic human ability to compare abstract or concrete objects. This paper\nproposes a comprehensive survey of the broad notion of semantic measure for the\ncomparison of units of language, concepts or instances based on semantic proxy\nanalyses. Semantic measures generalize the well-known notions of semantic\nsimilarity, semantic relatedness and semantic distance, which have been\nextensively studied by various communities over the last decades (e.g.,\nCognitive Sciences, Linguistics, and Artificial Intelligence to mention a few).\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 14:21:42 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 17:28:29 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 13:59:35 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Harispe", "S\u00e9bastien", ""], ["Ranwez", "Sylvie", ""], ["Janaqi", "Stefan", ""], ["Montmain", "Jacky", ""]]}, {"id": "1310.1425", "submitter": "Mohammad Nasiruddin", "authors": "Mohammad Nasiruddin", "title": "A State of the Art of Word Sense Induction: A Way Towards Word Sense\n  Disambiguation for Under-Resourced Languages", "comments": "14 pages TALN/RECITAL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word Sense Disambiguation (WSD), the process of automatically identifying the\nmeaning of a polysemous word in a sentence, is a fundamental task in Natural\nLanguage Processing (NLP). Progress in this approach to WSD opens up many\npromising developments in the field of NLP and its applications. Indeed,\nimprovement over current performance levels could allow us to take a first step\ntowards natural language understanding. Due to the lack of lexical resources it\nis sometimes difficult to perform WSD for under-resourced languages. This paper\nis an investigation on how to initiate research in WSD for under-resourced\nlanguages by applying Word Sense Induction (WSI) and suggests some interesting\ntopics to focus on.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 00:33:46 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Nasiruddin", "Mohammad", ""]]}, {"id": "1310.1426", "submitter": "Mohammad Nasiruddin", "authors": "Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman,\n  Mohammad Nasiruddin, Md. Abdul Latif and Mohammad Nurul Huda", "title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?", "comments": "9 pages Advances in Computing and Communications (ACC) 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 00:39:02 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Hassan", "Foyzul", ""], ["Kotwal", "Mohammed Rokibul Alam", ""], ["Rahman", "Md. Mostafizur", ""], ["Nasiruddin", "Mohammad", ""], ["Latif", "Md. Abdul", ""], ["Huda", "Mohammad Nurul", ""]]}, {"id": "1310.1590", "submitter": "Arnab Bhattacharya", "authors": "Paheli Bhattacharya and Arnab Bhattacharya", "title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study", "comments": "LCC 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 14:37:05 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1310.1597", "submitter": "Mengqiu Wang", "authors": "Mengqiu Wang and Christopher D. Manning", "title": "Cross-lingual Pseudo-Projected Expectation Regularization for Weakly\n  Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multilingual weakly supervised learning scenario where\nknowledge from annotated corpora in a resource-rich language is transferred via\nbitext to guide the learning in other languages. Past approaches project labels\nacross bitext and use them as features or gold labels for training. We propose\na new method that projects model expectations rather than labels, which\nfacilities transfer of model uncertainty across language boundaries. We encode\nexpectations as constraints and train a discriminative CRF model using\nGeneralized Expectation Criteria (Mann and McCallum, 2010). Evaluated on\nstandard Chinese-English and German-English NER datasets, our method\ndemonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining\nthe same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.\nFurthermore, when combined with labeled examples, our method yields significant\nimprovements over state-of-the-art supervised methods, achieving best reported\nnumbers to date on Chinese OntoNotes and German CoNLL-03 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 16:34:30 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Wang", "Mengqiu", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1310.1964", "submitter": "Flavio Massimiliano Cecchini", "authors": "Flavio Massimiliano Cecchini (Universit\\`a degli Studi di Milano),\n  Elisabetta Fersini (Universiy of Milano-Bicocca)", "title": "Named entity recognition using conditional random fields with non-local\n  relational constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin by introducing the Computer Science branch of Natural Language\nProcessing, then narrowing the attention on its subbranch of Information\nExtraction and particularly on Named Entity Recognition, discussing briefly its\nmain methodological approaches. It follows an introduction to state-of-the-art\nConditional Random Fields under the form of linear chains. Subsequently, the\nidea of constrained inference as a way to model long-distance relationships in\na text is presented, based on an Integer Linear Programming representation of\nthe problem. Adding such relationships to the problem as automatically inferred\nlogical formulas, translatable into linear conditions, we propose to solve the\nresulting more complex problem with the aid of Lagrangian relaxation, of which\nsome technical details are explained. Lastly, we give some experimental\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 22:08:18 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Cecchini", "Flavio Massimiliano", "", "Universit\u00e0 degli Studi di Milano"], ["Fersini", "Elisabetta", "", "Universiy of Milano-Bicocca"]]}, {"id": "1310.1975", "submitter": "Brendan O'Connor", "authors": "Brendan O'Connor and Michael Heilman", "title": "ARKref: a rule-based coreference resolution system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ARKref is a tool for noun phrase coreference. It is a deterministic,\nrule-based system that uses syntactic information from a constituent parser,\nand semantic information from an entity recognition component. Its architecture\nis based on the work of Haghighi and Klein (2009). ARKref was originally\nwritten in 2009. At the time of writing, the last released version was in March\n2011. This document describes that version, which is open-source and publicly\navailable at: http://www.ark.cs.cmu.edu/ARKref\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 00:30:51 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["O'Connor", "Brendan", ""], ["Heilman", "Michael", ""]]}, {"id": "1310.2408", "submitter": "Jun Zhu", "authors": "Jun Zhu, Xun Zheng, Bo Zhang", "title": "Improved Bayesian Logistic Supervised Topic Models with Data\n  Augmentation", "comments": "9 pages, ACL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models with a logistic likelihood have two issues that\npotentially limit their practical use: 1) response variables are usually\nover-weighted by document word counts; and 2) existing variational inference\nmethods make strict mean-field assumptions. We address these issues by: 1)\nintroducing a regularization constant to better balance the two parts based on\nan optimization formulation of Bayesian inference; and 2) developing a simple\nGibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and\ncollapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm\nhas analytical forms of each conditional distribution without making any\nrestricting assumptions and can be easily parallelized. Empirical results\ndemonstrate significant improvements on prediction performance and time\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:23:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Zhu", "Jun", ""], ["Zheng", "Xun", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.2479", "submitter": "Christian Alis", "authors": "Christian M. Alis, May T. Lim", "title": "Spatio-temporal variation of conversational utterances on Twitter", "comments": "13 pages, 7 figures, published in PLoS One", "journal-ref": "PLoS ONE 8: e77793", "doi": "10.1371/journal.pone.0077793", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Conversations reflect the existing norms of a language. Previously, we found\nthat utterance lengths in English fictional conversations in books and movies\nhave shortened over a period of 200 years. In this work, we show that this\nshortening occurs even for a brief period of 3 years (September 2009-December\n2012) using 229 million utterances from Twitter. Furthermore, the subset of\ngeographically-tagged tweets from the United States show an inverse proportion\nbetween utterance lengths and the state-level percentage of the Black\npopulation. We argue that shortening of utterances can be explained by the\nincreasing usage of jargon including coined words.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 13:38:01 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 10:44:38 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2013 08:33:14 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Alis", "Christian M.", ""], ["Lim", "May T.", ""]]}, {"id": "1310.2527", "submitter": "Maxime Amblard", "authors": "Maxime Amblard (INRIA Nancy - Grand Est / LORIA, MSH Lorraine)", "title": "Treating clitics with minimalist grammars", "comments": "The 11th conference on Formal Grammar, Malaga : Spain (2006)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of Stabler's version of clitics treatment for a wider\ncoverage of the French language. For this, we present the lexical entries\nneeded in the lexicon. Then, we show the recognition of complex syntactic\nphenomena as (left and right) dislo- cation, clitic climbing over modal and\nextraction from determiner phrase. The aim of this presentation is the\nsyntax-semantic interface for clitics analyses in which we will stress on\nclitic climbing over verb and raising verb.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 17:19:06 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Amblard", "Maxime", "", "INRIA Nancy - Grand Est / LORIA, MSH Lorraine"]]}, {"id": "1310.3099", "submitter": "Roland Maas", "authors": "Roland Maas, Christian Huemmer, Armin Sehr, Walter Kellermann", "title": "A Bayesian Network View on Acoustic Model-Based Techniques for Robust\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a unifying Bayesian network view on various approaches\nfor acoustic model adaptation, missing feature, and uncertainty decoding that\nare well-known in the literature of robust automatic speech recognition. The\nrepresentatives of these classes can often be deduced from a Bayesian network\nthat extends the conventional hidden Markov models used in speech recognition.\nThese extensions, in turn, can in many cases be motivated from an underlying\nobservation model that relates clean and distorted feature vectors. By\nconverting the observation models into a Bayesian network representation, we\nformulate the corresponding compensation rules leading to a unified view on\nknown derivations as well as to new formulations for certain approaches. The\ngeneric Bayesian perspective provided in this contribution thus highlights\nstructural differences and similarities between the analyzed approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2013 12:07:57 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 13:52:44 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Maas", "Roland", ""], ["Huemmer", "Christian", ""], ["Sehr", "Armin", ""], ["Kellermann", "Walter", ""]]}, {"id": "1310.3333", "submitter": "Sriramkumar Balasubramanian", "authors": "Sriramkumar Balasubramanian and Raghuram Reddy Nagireddy", "title": "Visualizing Bags of Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this work is two-fold - a) to compare between two different\nmodes of visualizing data that exists in a bag of vectors format b) to propose\na theoretical model that supports a new mode of visualizing data. Visualizing\nhigh dimensional data can be achieved using Minimum Volume Embedding, but the\ndata has to exist in a format suitable for computing similarities while\npreserving local distances. This paper compares the visualization between two\nmethods of representing data and also proposes a new method providing sample\nvisualizations for that method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2013 03:48:38 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Balasubramanian", "Sriramkumar", ""], ["Nagireddy", "Raghuram Reddy", ""]]}, {"id": "1310.3499", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Forecasting of Events by Tweet Data Mining", "comments": "13 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the analysis of quantitative characteristics of frequent\nsets and association rules in the posts of Twitter microblogs related to\ndifferent event discussions. For the analysis, we used a theory of frequent\nsets, association rules and a theory of formal concept analysis. We revealed\nthe frequent sets and association rules which characterize the semantic\nrelations between the concepts of analyzed subjects. The support of some\nfrequent sets reaches its global maximum before the expected event but with\nsome time delay. Such frequent sets may be considered as predictive markers\nthat characterize the significance of expected events for blogosphere users. We\nshowed that the time dynamics of confidence in some revealed association rules\ncan also have predictive characteristics. Exceeding a certain threshold may be\na signal for corresponding reaction in the society within the time interval\nbetween the maximum and the probable coming of an event. In this paper, we\nconsidered two types of events: the Olympic tennis tournament final in London,\n2012 and the prediction of Eurovision 2013 winner.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2013 18:17:54 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1310.3500", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Can Twitter Predict Royal Baby's Name ?", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the existence of possible correlation between\npublic opinion of twitter users and the decision-making of persons who are\ninfluential in the society. We carry out this analysis on the example of the\ndiscussion of probable name of the British crown baby, born in July, 2013. In\nour study, we use the methods of quantitative processing of natural language,\nthe theory of frequent sets, the algorithms of visual displaying of users'\ncommunities. We also analyzed the time dynamics of keyword frequencies. The\nanalysis showed that the main predictable name was dominating in the spectrum\nof names before the official announcement. Using the theories of frequent sets,\nwe showed that the full name consisting of three component names was the part\nof top 5 by the value of support. It was revealed that the structure of\ndynamically formed users' communities participating in the discussion is\ndetermined by only a few leaders who influence significantly the viewpoints of\nother users.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2013 18:22:44 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1310.4546", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean", "title": "Distributed Representations of Words and Phrases and their\n  Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 23:28:53 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Mikolov", "Tomas", ""], ["Sutskever", "Ilya", ""], ["Chen", "Kai", ""], ["Corrado", "Greg", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1310.4909", "submitter": "Chinchu Jose", "authors": "M. Sudheep Elayidom, Chinchu Jose, Anitta Puthussery, Neenu K Sasi", "title": "Text Classification For Authorship Attribution Analysis", "comments": "10 pages", "journal-ref": "Advanced Computing: An International Journal (ACIJ), Vol.4, No.5,\n  September 2013", "doi": "10.5121/acij.2013.4501", "report-no": null, "categories": "cs.DL cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution mainly deals with undecided authorship of literary\ntexts. Authorship attribution is useful in resolving issues like uncertain\nauthorship, recognize authorship of unknown texts, spot plagiarism so on.\nStatistical methods can be used to set apart the approach of an author\nnumerically. The basic methodologies that are made use in computational\nstylometry are word length, sentence length, vocabulary affluence, frequencies\netc. Each author has an inborn style of writing, which is particular to\nhimself. Statistical quantitative techniques can be used to differentiate the\napproach of an author in a numerical way. The problem can be broken down into\nthree sub problems as author identification, author characterization and\nsimilarity detection. The steps involved are pre-processing, extracting\nfeatures, classification and author identification. For this different\nclassifiers can be used. Here fuzzy learning classifier and SVM are used. After\nauthor identification the SVM was found to have more accuracy than Fuzzy\nclassifier. Later combined the classifiers to obtain a better accuracy when\ncompared to individual SVM and fuzzy classifier.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 04:18:09 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Elayidom", "M. Sudheep", ""], ["Jose", "Chinchu", ""], ["Puthussery", "Anitta", ""], ["Sasi", "Neenu K", ""]]}, {"id": "1310.4938", "submitter": "Andreas Wotzlaw", "authors": "Andreas Wotzlaw and Ravi Coote", "title": "A Logic-based Approach for Recognizing Textual Entailment Supported by\n  Ontological Background Knowledge", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the architecture and the evaluation of a new system for\nrecognizing textual entailment (RTE). In RTE we want to identify automatically\nthe type of a logical relation between two input texts. In particular, we are\ninterested in proving the existence of an entailment between them. We conceive\nour system as a modular environment allowing for a high-coverage syntactic and\nsemantic text analysis combined with logical inference. For the syntactic and\nsemantic analysis we combine a deep semantic analysis with a shallow one\nsupported by statistical models in order to increase the quality and the\naccuracy of results. For RTE we use logical inference of first-order employing\nmodel-theoretic techniques and automated reasoning tools. The inference is\nsupported with problem-relevant background knowledge extracted automatically\nand on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or\nother, more experimental sources with, e.g., manually defined presupposition\nresolutions, or with axiomatized general and common sense knowledge. The\nresults show that fine-grained and consistent knowledge coming from diverse\nsources is a necessary condition determining the correctness and traceability\nof results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 08:10:32 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Wotzlaw", "Andreas", ""], ["Coote", "Ravi", ""]]}, {"id": "1310.5042", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Distributional semantics beyond words: Supervised learning of analogy\n  and paraphrase", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), (2013), 1, 353-366", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several efforts to extend distributional semantics beyond\nindividual words, to measure the similarity of word pairs, phrases, and\nsentences (briefly, tuples; ordered sets of words, contiguous or\nnoncontiguous). One way to extend beyond words is to compare two tuples using a\nfunction that combines pairwise similarities between the component words in the\ntuples. A strength of this approach is that it works with both relational\nsimilarity (analogy) and compositional similarity (paraphrase). However, past\nwork required hand-coding the combination function for different tasks. The\nmain contribution of this paper is that combination functions are generated by\nsupervised learning. We achieve state-of-the-art results in measuring\nrelational similarity between word pairs (SAT analogies and SemEval~2012 Task\n2) and measuring compositional similarity between noun-modifier phrases and\nunigrams (multiple-choice paraphrase questions).\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:50:39 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1310.5884", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "The optimality of attaching unlinked labels to unlinked meanings", "comments": "Little mathematical errors have been corrected; clarity, English and\n  format have been improved. In press in Glottometrics", "journal-ref": "Glottometrics 36, 1-16 (2017)", "doi": null, "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vocabulary learning by children can be characterized by many biases. When\nencountering a new word, children as well as adults, are biased towards\nassuming that it means something totally different from the words that they\nalready know. To the best of our knowledge, the 1st mathematical proof of the\noptimality of this bias is presented here. First, it is shown that this bias is\na particular case of the maximization of mutual information between words and\nmeanings. Second, the optimality is proven within a more general information\ntheoretic framework where mutual information maximization competes with other\ninformation theoretic principles. The bias is a prediction from modern\ninformation theory. The relationship between information theoretic principles\nand the principles of contrast and mutual exclusivity is also shown.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 11:37:36 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 16:21:16 GMT"}, {"version": "v3", "created": "Fri, 11 Nov 2016 10:58:26 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1310.5963", "submitter": "Foruzan Kiamarzpour", "authors": "Foruzan Kiamarzpour, Rouhollah Dianat, Mohammad bahrani, Mehdi\n  Sadeghzadeh", "title": "Improving the methods of email classification based on words ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has dramatically changed the relationship among people and their\nrelationships with others people and made the valuable information available\nfor the users. Email is the service, which the Internet provides today for its\nown users; this service has attracted most of the users' attention due to the\nlow cost. Along with the numerous benefits of Email, one of the weaknesses of\nthis service is that the number of received emails is continually being\nenhanced, thus the ways are needed to automatically filter these disturbing\nletters. Most of these filters utilize a combination of several techniques such\nas the Black or white List, using the keywords and so on in order to identify\nthe spam more accurately In this paper, we introduce a new method to classify\nthe spam. We are seeking to increase the accuracy of Email classification by\ncombining the output of several decision trees and the concept of ontology.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 15:35:46 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Kiamarzpour", "Foruzan", ""], ["Dianat", "Rouhollah", ""], ["bahrani", "Mohammad", ""], ["Sadeghzadeh", "Mehdi", ""]]}, {"id": "1310.6772", "submitter": "Ragib Hasan", "authors": "Thamar Solorio and Ragib Hasan and Mainul Mizan", "title": "Sockpuppet Detection in Wikipedia: A Corpus of Real-World Deceptive\n  Writing for Linking Identities", "comments": "4 pages, under submission at LREC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes the corpus of sockpuppet cases we gathered from\nWikipedia. A sockpuppet is an online user account created with a fake identity\nfor the purpose of covering abusive behavior and/or subverting the editing\nregulation process. We used a semi-automated method for crawling and curating a\ndataset of real sockpuppet investigation cases. To the best of our knowledge,\nthis is the first corpus available on real-world deceptive writing. We describe\nthe process for crawling the data and some preliminary results that can be used\nas baseline for benchmarking research. The dataset will be released under a\nCreative Commons license from our project website: http://docsig.cis.uab.edu.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 20:59:27 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Solorio", "Thamar", ""], ["Hasan", "Ragib", ""], ["Mizan", "Mainul", ""]]}, {"id": "1310.6775", "submitter": "Linas Vepstas PhD", "authors": "Linas Vepstas", "title": "Durkheim Project Data Analysis Report", "comments": "43 pages, to appear as appendix of primary science publication\n  Poulin, et al \"Predicting the risk of suicide by analyzing the text of\n  clinical notes\"", "journal-ref": null, "doi": "10.1371/journal.pone.0085733.s001", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the suicidality prediction models created under the\nDARPA DCAPS program in association with the Durkheim Project\n[http://durkheimproject.org/]. The models were built primarily from\nunstructured text (free-format clinician notes) for several hundred patient\nrecords obtained from the Veterans Health Administration (VHA). The models were\nconstructed using a genetic programming algorithm applied to bag-of-words and\nbag-of-phrases datasets. The influence of additional structured data was\nexplored but was found to be minor. Given the small dataset size,\nclassification between cohorts was high fidelity (98%). Cross-validation\nsuggests these models are reasonably predictive, with an accuracy of 50% to 69%\non five rotating folds, with ensemble averages of 58% to 67%. One particularly\nnoteworthy result is that word-pairs can dramatically improve classification\naccuracy; but this is the case only when one of the words in the pair is\nalready known to have a high predictive value. By contrast, the set of all\npossible word-pairs does not improve on a simple bag-of-words model.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 21:10:53 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Vepstas", "Linas", ""]]}, {"id": "1310.7782", "submitter": "Andrea Baronchelli", "authors": "Andrea Baronchelli, Vittorio Loreto, Andrea Puglisi", "title": "Individual Biases, Cultural Evolution, and the Statistical Nature of\n  Language Universals: The Case of Colour Naming Systems", "comments": null, "journal-ref": "PLoS ONE 10(5): e0125019 (2015)", "doi": "10.1371/journal.pone.0125019", "report-no": null, "categories": "physics.soc-ph cs.CL cs.MA q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language universals have long been attributed to an innate Universal Grammar.\nAn alternative explanation states that linguistic universals emerged\nindependently in every language in response to shared cognitive or perceptual\nbiases. A computational model has recently shown how this could be the case,\nfocusing on the paradigmatic example of the universal properties of colour\nnaming patterns, and producing results in quantitative agreement with the\nexperimental data. Here we investigate the role of an individual perceptual\nbias in the framework of the model. We study how, and to what extent, the\nstructure of the bias influences the corresponding linguistic universal\npatterns. We show that the cultural history of a group of speakers introduces\npopulation-specific constraints that act against the pressure for uniformity\narising from the individual bias, and we clarify the interplay between these\ntwo forces.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 12:29:12 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 10:23:50 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Baronchelli", "Andrea", ""], ["Loreto", "Vittorio", ""], ["Puglisi", "Andrea", ""]]}, {"id": "1310.8059", "submitter": "Thabet Slimani", "authors": "Thabet Slimani", "title": "Description and Evaluation of Semantic Similarity Measures Approaches", "comments": "10 pages", "journal-ref": "International Journal of Computer Applications 80(10):25-33,\n  October 2013. Published by Foundation of Computer Science, New York, USA", "doi": "10.5120/13897-1851", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, semantic similarity measure has a great interest in Semantic\nWeb and Natural Language Processing (NLP). Several similarity measures have\nbeen developed, being given the existence of a structured knowledge\nrepresentation offered by ontologies and corpus which enable semantic\ninterpretation of terms. Semantic similarity measures compute the similarity\nbetween concepts/terms included in knowledge sources in order to perform\nestimations. This paper discusses the existing semantic similarity methods\nbased on structure, information content and feature approaches. Additionally,\nwe present a critical evaluation of several categories of semantic similarity\napproaches based on two standard benchmarks. The aim of this paper is to give\nan efficient evaluation of all these measures which help researcher and\npractitioners to select the measure that best fit for their requirements.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 08:08:43 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Slimani", "Thabet", ""]]}, {"id": "1310.8511", "submitter": "{\\L}ukasz D\\k{e}bowski", "authors": "{\\L}ukasz D\\k{e}bowski", "title": "A Preadapted Universal Switch Distribution for Testing Hilberg's\n  Conjecture", "comments": "17 pages, 3 figures", "journal-ref": "IEEE Transactions on Information Theory 61(10):5708-5715, 2015", "doi": "10.1109/TIT.2015.2466693", "report-no": null, "categories": "cs.IT cs.CL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hilberg's conjecture about natural language states that the mutual\ninformation between two adjacent long blocks of text grows like a power of the\nblock length. The exponent in this statement can be upper bounded using the\npointwise mutual information estimate computed for a carefully chosen code. The\nbound is the better, the lower the compression rate is but there is a\nrequirement that the code be universal. So as to improve a received upper bound\nfor Hilberg's exponent, in this paper, we introduce two novel universal codes,\ncalled the plain switch distribution and the preadapted switch distribution.\nGenerally speaking, switch distributions are certain mixtures of adaptive\nMarkov chains of varying orders with some additional communication to avoid so\ncalled catch-up phenomenon. The advantage of these distributions is that they\nboth achieve a low compression rate and are guaranteed to be universal. Using\nthe switch distributions we obtain that a sample of a text in English is\nnon-Markovian with Hilberg's exponent being $\\le 0.83$, which improves over the\nprevious bound $\\le 0.94$ obtained using the Lempel-Ziv code.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 14:12:30 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 11:51:04 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["D\u0119bowski", "\u0141ukasz", ""]]}]