[{"id": "1312.0482", "submitter": "Li Deng", "authors": "Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng", "title": "Learning Semantic Representations for the Phrase Translation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel semantic-based phrase translation model. A pair\nof source and target phrases are projected into continuous-valued vector\nrepresentations in a low-dimensional latent semantic space, where their\ntranslation score is computed by the distance between the pair in this new\nspace. The projection is performed by a multi-layer neural network whose\nweights are learned on parallel training data. The learning is aimed to\ndirectly optimize the quality of end-to-end machine translation results.\nExperimental evaluation has been performed on two Europarl translation tasks,\nEnglish-French and German-English. The results show that the new semantic-based\nphrase translation model significantly improves the performance of a\nstate-of-the-art phrase-based statistical machine translation sys-tem, leading\nto a gain of 0.7-1.0 BLEU points.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 04:58:59 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Yih", "Wen-tau", ""], ["Deng", "Li", ""]]}, {"id": "1312.0493", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Claire Cardie", "title": "Bidirectional Recursive Neural Networks for Token-Level Labeling with\n  Structure", "comments": "9 pages, 5 figures, NIPS Deep Learning Workshop 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 15:54:40 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Cardie", "Claire", ""]]}, {"id": "1312.0976", "submitter": "Scott A. Hale", "authors": "Scott A. Hale", "title": "Multilinguals and Wikipedia Editing", "comments": null, "journal-ref": "Proceedings of the 6th Annual ACM Web Science Conference, WebSci\n  2014, ACM", "doi": "10.1145/2615569.2615684", "report-no": null, "categories": "cs.CY cs.CL cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article analyzes one month of edits to Wikipedia in order to examine the\nrole of users editing multiple language editions (referred to as multilingual\nusers). Such multilingual users may serve an important function in diffusing\ninformation across different language editions of the encyclopedia, and prior\nwork has suggested this could reduce the level of self-focus bias in each\nedition. This study finds multilingual users are much more active than their\nsingle-edition (monolingual) counterparts. They are found in all language\neditions, but smaller-sized editions with fewer users have a higher percentage\nof multilingual users than larger-sized editions. About a quarter of\nmultilingual users always edit the same articles in multiple languages, while\njust over 40% of multilingual users edit different articles in different\nlanguages. When non-English users do edit a second language edition, that\nedition is most frequently English. Nonetheless, several regional and\nlinguistic cross-editing patterns are also present.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 23:04:36 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 11:15:18 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Hale", "Scott A.", ""]]}, {"id": "1312.2087", "submitter": "Nicholas H. Kirk", "authors": "Nicholas H. Kirk", "title": "Towards Structural Natural Language Formalization: Mapping Discourse to\n  Controlled Natural Language", "comments": "The 17th Workshop on the Semantics and Pragmatics of Dialogue,\n  Amsterdam, 16-18 December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The author describes a conceptual study towards mapping grounded natural\nlanguage discourse representation structures to instances of controlled\nlanguage statements. This can be achieved via a pipeline of preexisting state\nof the art technologies, namely natural language syntax to semantic discourse\nmapping, and a reduction of the latter to controlled language discourse, given\na set of previously learnt reduction rules. Concludingly a description on\nevaluation, potential and limitations for ontology-based reasoning is\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 11:19:20 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Kirk", "Nicholas H.", ""]]}, {"id": "1312.2137", "submitter": "Dimitri Palaz", "authors": "Dimitri Palaz, Ronan Collobert, Mathew Magimai.-Doss", "title": "End-to-end Phoneme Sequence Recognition using Convolutional Neural\n  Networks", "comments": "NIPS Deep Learning Workshop, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most phoneme recognition state-of-the-art systems rely on a classical neural\nnetwork classifiers, fed with highly tuned features, such as MFCC or PLP\nfeatures. Recent advances in ``deep learning'' approaches questioned such\nsystems, but while some attempts were made with simpler features such as\nspectrograms, state-of-the-art systems still rely on MFCCs. This might be\nviewed as a kind of failure from deep learning approaches, which are often\nclaimed to have the ability to train with raw signals, alleviating the need of\nhand-crafted features. In this paper, we investigate a convolutional neural\nnetwork approach for raw speech signals. While convolutional architectures got\ntremendous success in computer vision or text processing, they seem to have\nbeen let down in the past recent years in the speech processing field. We show\nthat it is possible to learn an end-to-end phoneme sequence classifier system\ndirectly from raw signal, with similar performance on the TIMIT and WSJ\ndatasets than existing systems based on MFCC, questioning the need of complex\nhand-crafted features on large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 19:55:02 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Palaz", "Dimitri", ""], ["Collobert", "Ronan", ""], ["-Doss", "Mathew Magimai.", ""]]}, {"id": "1312.2244", "submitter": "Tao Wang", "authors": "Rumeng Li, Tao Wang, Xun Wang", "title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation", "comments": null, "journal-ref": "SDM(2015)p550-558", "doi": "10.1137/1.9781611974010.62", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timeline Generation aims at summarizing news from different epochs and\ntelling readers how an event evolves. It is a new challenge that combines\nsalience ranking with novelty detection. For long-term public events, the main\ntopic usually includes various aspects across different epochs and each aspect\nhas its own evolving pattern. Existing approaches neglect such hierarchical\ntopic structure involved in the news corpus in timeline generation. In this\npaper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) for\ntimeline generation. Our model can aptly detect different levels of topic\ninformation across corpus and such structure is further used for sentence\nselection. Based on the topic mined fro HDM, sentences are selected by\nconsidering different aspects such as relevance, coherence and coverage. We\ndevelop experimental systems to evaluate 8 long-term events that public\nconcern. Performance comparison between different systems demonstrates the\neffectiveness of our model in terms of ROUGE metrics.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 19:15:15 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 11:44:47 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 01:34:50 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Li", "Rumeng", ""], ["Wang", "Tao", ""], ["Wang", "Xun", ""]]}, {"id": "1312.2844", "submitter": "Patrice Descourt", "authors": "Norbert Rimoux, Patrice Descourt", "title": "mARC: Memory by Association and Reinforcement of Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL nlin.AO nlin.CD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper introduces the memory by Association and Reinforcement of Contexts\n(mARC). mARC is a novel data modeling technology rooted in the second\nquantization formulation of quantum mechanics. It is an all-purpose incremental\nand unsupervised data storage and retrieval system which can be applied to all\ntypes of signal or data, structured or unstructured, textual or not. mARC can\nbe applied to a wide range of information clas-sification and retrieval\nproblems like e-Discovery or contextual navigation. It can also for-mulated in\nthe artificial life framework a.k.a Conway \"Game Of Life\" Theory. In contrast\nto Conway approach, the objects evolve in a massively multidimensional space.\nIn order to start evaluating the potential of mARC we have built a mARC-based\nInternet search en-gine demonstrator with contextual functionality. We compare\nthe behavior of the mARC demonstrator with Google search both in terms of\nperformance and relevance. In the study we find that the mARC search engine\ndemonstrator outperforms Google search by an order of magnitude in response\ntime while providing more relevant results for some classes of queries.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 15:56:53 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Rimoux", "Norbert", ""], ["Descourt", "Patrice", ""]]}, {"id": "1312.3005", "submitter": "Ciprian Chelba", "authors": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants,\n  Phillipp Koehn, Tony Robinson", "title": "One Billion Word Benchmark for Measuring Progress in Statistical\n  Language Modeling", "comments": "Accompanied by a code.google.com project allowing anyone to generate\n  the benchmark data, and use it to compare their language model against the\n  ones described in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new benchmark corpus to be used for measuring progress in\nstatistical language modeling. With almost one billion words of training data,\nwe hope this benchmark will be useful to quickly evaluate novel language\nmodeling techniques, and to compare their contribution when combined with other\nadvanced techniques. We show performance of several well-known types of\nlanguage models, with the best results achieved with a recurrent neural network\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\nperplexity 67.6; a combination of techniques leads to 35% reduction in\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n  The benchmark is available as a code.google.com project; besides the scripts\nneeded to rebuild the training/held-out data, it also makes available\nlog-probability values for each word in each of ten held-out data sets, for\neach of the baseline n-gram models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 00:25:57 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 22:26:59 GMT"}, {"version": "v3", "created": "Tue, 4 Mar 2014 18:30:26 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Chelba", "Ciprian", ""], ["Mikolov", "Tomas", ""], ["Schuster", "Mike", ""], ["Ge", "Qi", ""], ["Brants", "Thorsten", ""], ["Koehn", "Phillipp", ""], ["Robinson", "Tony", ""]]}, {"id": "1312.3168", "submitter": "Bruno Mery", "authors": "Bruno Mery (LaBRI), Christian Retor\\'e (LaBRI)", "title": "Semantic Types, Lexical Sorts and Classifiers", "comments": null, "journal-ref": "NLPCS '10- 10th International Workshop on Natural Language\n  Processing and Computer Science - 2013 (2013)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cognitively and linguistically motivated set of sorts for\nlexical semantics in a compositional setting: the classifiers in languages that\ndo have such pronouns. These sorts are needed to include lexical considerations\nin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical\nextensions of usual Montague semantics to model restriction of selection,\nfelicitous and infelicitous copredication require a rich and refined type\nsystem whose base types are the lexical sorts, the basis of the many-sorted\nlogic in which semantical representations of sentences are stated. However,\nnone of those approaches define precisely the actual base types or sorts to be\nused in the lexicon. In this article, we shall discuss some of the options\ncommonly adopted by researchers in formal lexical semantics, and defend the\nview that classifiers in the languages which have such pronouns are an\nappealing solution, both linguistically and cognitively motivated.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 14:04:52 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Mery", "Bruno", "", "LaBRI"], ["Retor\u00e9", "Christian", "", "LaBRI"]]}, {"id": "1312.3251", "submitter": "Nayan Jyoti Kalita", "authors": "Nayan Jyoti Kalita, Navanath Saharia and Smriti Kumar Sinha", "title": "Towards The Development of a Bishnupriya Manipuri Corpus", "comments": "5 pages, conference at National Conference on Recent Trends in\n  Computer Sciences at Bodoland University, 25th-26th March, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any deep computational processing of language we need evidences, and one\nsuch set of evidences is corpus. This paper describes the development of a\ntext-based corpus for the Bishnupriya Manipuri language. A Corpus is considered\nas a building block for any language processing tasks. Due to the lack of\nawareness like other Indian languages, it is also studied less frequently. As a\nresult the language still lacks a good corpus and basic language processing\ntools. As per our knowledge this is the first effort to develop a corpus for\nBishnupriya Manipuri language.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 17:24:35 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Kalita", "Nayan Jyoti", ""], ["Saharia", "Navanath", ""], ["Sinha", "Smriti Kumar", ""]]}, {"id": "1312.3258", "submitter": "Henda Chorfi Ouertani", "authors": "Henda Chorfi Ouertani", "title": "Implicit Sensitive Text Summarization based on Data Conveyed by\n  Connectives", "comments": "4 pages, 2 figures, journal IJACSA; (IJACSA) International Journal of\n  Advanced Computer Science and Applications, Vol. 4, No. 11, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far and trying to reach human capabilities, research in automatic\nsummarization has been based on hypothesis that are both enabling and limiting.\nSome of these limitations are: how to take into account and reflect (in the\ngenerated summary) the implicit information conveyed in the text, the author\nintention, the reader intention, the context influence, the general world\nknowledge. Thus, if we want machines to mimic human abilities, then they will\nneed access to this same large variety of knowledge. The implicit is affecting\nthe orientation and the argumentation of the text and consequently its summary.\nMost of Text Summarizers (TS) are processing as compressing the initial data\nand they necessarily suffer from information loss. TS are focusing on features\nof the text only, not on what the author intended or why the reader is reading\nthe text. In this paper, we address this problem and we present a system\nfocusing on acquiring knowledge that is implicit. We principally spotlight the\nimplicit information conveyed by the argumentative connectives such as: but,\neven, yet and their effect on the summary.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 17:50:21 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Ouertani", "Henda Chorfi", ""]]}, {"id": "1312.4092", "submitter": "Edouard Grave", "authors": "Edouard Grave (LIENS, INRIA Paris - Rocquencourt), Guillaume Obozinski\n  (LIGM), Francis Bach (LIENS, INRIA Paris - Rocquencourt)", "title": "Domain adaptation for sequence labeling using hidden Markov models", "comments": "New Directions in Transfer and Multi-Task: Learning Across Domains\n  and Tasks (NIPS Workshop) (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most natural language processing systems based on machine learning are not\nrobust to domain shift. For example, a state-of-the-art syntactic dependency\nparser trained on Wall Street Journal sentences has an absolute drop in\nperformance of more than ten points when tested on textual data from the Web.\nAn efficient solution to make these methods more robust to domain shift is to\nfirst learn a word representation using large amounts of unlabeled data from\nboth domains, and then use this representation as features in a supervised\nlearning algorithm. In this paper, we propose to use hidden Markov models to\nlearn word representations for part-of-speech tagging. In particular, we study\nthe influence of using data from the source, the target or both domains to\nlearn the representation and the different ways to represent words using an\nHMM.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 21:48:49 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Grave", "Edouard", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIGM"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1312.4617", "submitter": "Mariam Adedoyin-Olowe", "authors": "Mariam Adedoyin-Olowe, Mohamed Medhat Gaber and Frederic Stahl", "title": "A Survey of Data Mining Techniques for Social Media Analysis", "comments": "25 pages, 9 figures", "journal-ref": "Journal of Data Mining & Digital Humanities, 2014 (June 24, 2014)\n  jdmdh:18", "doi": "10.46298/jdmdh.5", "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Social network has gained remarkable attention in the last decade. Accessing\nsocial network sites such as Twitter, Facebook LinkedIn and Google+ through the\ninternet and the web 2.0 technologies has become more affordable. People are\nbecoming more interested in and relying on social network for information, news\nand opinion of other users on diverse subject matters. The heavy reliance on\nsocial network sites causes them to generate massive data characterised by\nthree computational issues namely; size, noise and dynamism. These issues often\nmake social network data very complex to analyse manually, resulting in the\npertinent use of computational means of analysing them. Data mining provides a\nwide range of techniques for detecting useful knowledge from massive datasets\nlike trends, patterns and rules [44]. Data mining techniques are used for\ninformation retrieval, statistical modelling and machine learning. These\ntechniques employ data pre-processing, data analysis, and data interpretation\nprocesses in the course of data analysis. This survey discusses different data\nmining techniques used in mining diverse aspects of the social network over\ndecades going from the historical techniques to the up-to-date models,\nincluding our novel technique named TRCM. All the techniques covered in this\nsurvey are listed in the Table.1 including the tools employed as well as names\nof their authors.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 02:53:38 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 23:20:08 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Adedoyin-Olowe", "Mariam", ""], ["Gaber", "Mohamed Medhat", ""], ["Stahl", "Frederic", ""]]}, {"id": "1312.4706", "submitter": "Donna Vakharia", "authors": "Donna Vakharia, Rachel Gibbs", "title": "Designing Spontaneous Speech Search Interface for Historical Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous speech in the form of conversations, meetings, voice-mail,\ninterviews, oral history, etc. is one of the most ubiquitous forms of human\ncommunication. Search engines providing access to such speech collections have\nthe potential to better inform intelligence and make relevant data over vast\naudio/video archives available to users. This project presents a search user\ninterface design supporting search tasks over a speech collection consisting of\nan historical archive with nearly 52,000 audiovisual testimonies of survivors\nand witnesses of the Holocaust and other genocides. The design incorporates\nfaceted search, along with other UI elements like highlighted search items,\ntags, snippets, etc., to promote discovery and exploratory search. Two\ndifferent designs have been created to support both manual and automated\ntranscripts. Evaluation was performed using human subjects to measure accuracy\nin retrieving results, understanding user-perspective on the design elements,\nand ease of parsing information.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 10:18:44 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Vakharia", "Donna", ""], ["Gibbs", "Rachel", ""]]}, {"id": "1312.4824", "submitter": "Bhagwati Pande", "authors": "B. P. Pande, Pawan Tamta, H. S. Dhami", "title": "Generation, Implementation and Appraisal of an N-gram based Stemming\n  Algorithm", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A language independent stemmer has always been looked for. Single N-gram\ntokenization technique works well, however, it often generates stems that start\nwith intermediate characters, rather than initial ones. We present a novel\ntechnique that takes the concept of N gram stemming one step ahead and compare\nour method with an established algorithm in the field, Porter's Stemmer.\nResults indicate that our N gram stemmer is not inferior to Porter's linguistic\nstemmer.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 15:32:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 04:05:36 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Pande", "B. P.", ""], ["Tamta", "Pawan", ""], ["Dhami", "H. S.", ""]]}, {"id": "1312.5129", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin and Hinrich Sch\\\"utze", "title": "Deep Learning Embeddings for Discontinuous Linguistic Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning embeddings have been successfully used for many natural\nlanguage processing problems. Embeddings are mostly computed for word forms\nalthough a number of recent papers have extended this to other linguistic units\nlike morphemes and phrases. In this paper, we argue that learning embeddings\nfor discontinuous linguistic units should also be considered. In an\nexperimental evaluation on coreference resolution, we show that such embeddings\nperform better than word form embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 13:34:16 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2013 11:01:02 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1312.5198", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi and Ivan Titov", "title": "Learning Semantic Script Knowledge with Event Embeddings", "comments": "4 Pages, 1 figure, ICLR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Induction of common sense knowledge about prototypical sequences of events\nhas recently received much attention. Instead of inducing this knowledge in the\nform of graphs, as in much of the previous work, in our method, distributed\nrepresentations of event realizations are computed based on distributed\nrepresentations of predicates and their arguments, and then these\nrepresentations are used to predict prototypical event orderings. The\nparameters of the compositional process for computing the event representations\nand the ranking component of the model are jointly estimated from texts. We\nshow that this approach results in a substantial boost in ordering performance\nwith respect to previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:13:08 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 10:42:35 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 17:08:34 GMT"}, {"version": "v4", "created": "Fri, 25 Apr 2014 13:31:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""]]}, {"id": "1312.5542", "submitter": "R\\'emi Lebret", "authors": "R\\'emi Lebret and Ronan Collobert", "title": "Word Emdeddings through Hellinger PCA", "comments": "9 pages, 5 tables", "journal-ref": "Conference of the European Chapter of the Association for\n  Computational Linguistics (EACL), 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings resulting from neural language models have been shown to be\nsuccessful for a large variety of NLP tasks. However, such architecture might\nbe difficult to train and time-consuming. Instead, we propose to drastically\nsimplify the word embeddings computation through a Hellinger PCA of the word\nco-occurence matrix. We compare those new word embeddings with some well-known\nembeddings on NER and movie review tasks and show that we can reach similar or\neven better performance. Although deep learning is not really necessary for\ngenerating good word embeddings, we show that it can provide an easy way to\nadapt embeddings to specific tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 13:31:11 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 10:23:35 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 17:01:11 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Lebret", "R\u00e9mi", ""], ["Collobert", "Ronan", ""]]}, {"id": "1312.5559", "submitter": "Irina Sergienya", "authors": "Irina Sergienya and Hinrich Sch\\\"utze", "title": "Distributional Models and Deep Learning Embeddings: Combining the Best\n  of Both Worlds", "comments": "4 pages, 1 table, ICLR Workshop; main experimental table was extended\n  with more experimental results; related word added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main approaches to the distributed representation of words:\nlow-dimensional deep learning embeddings and high-dimensional distributional\nmodels, in which each dimension corresponds to a context word. In this paper,\nwe combine these two approaches by learning embeddings based on\ndistributional-model vectors - as opposed to one-hot vectors as is standardly\ndone in deep learning. We show that the combined approach has better\nperformance on a word relatedness judgment task.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 14:18:14 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 17:33:49 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 14:17:46 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Sergienya", "Irina", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1312.5985", "submitter": "Tamara Polajnar", "authors": "Tamara Polajnar and Luana Fagarasan and Stephen Clark", "title": "Learning Type-Driven Tensor-Based Meaning Representations", "comments": "Submitted as part of the open review process for ICLR'14. The paper\n  contains 10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the learning of 3rd-order tensors representing the\nsemantics of transitive verbs. The meaning representations are part of a\ntype-driven tensor-based semantic framework, from the newly emerging field of\ncompositional distributional semantics. Standard techniques from the neural\nnetworks literature are used to learn the tensors, which are tested on a\nselectional preference-style task with a simple 2-dimensional sentence space.\nPromising results are obtained against a competitive corpus-based baseline. We\nargue that extending this work beyond transitive verbs, and to\nhigher-dimensional sentence spaces, is an interesting and challenging problem\nfor the machine learning community to consider.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 15:21:15 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 15:27:24 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Polajnar", "Tamara", ""], ["Fagarasan", "Luana", ""], ["Clark", "Stephen", ""]]}, {"id": "1312.6168", "submitter": "Anjan Nepal", "authors": "Anjan Nepal and Alexander Yates", "title": "Factorial Hidden Markov Models for Learning Representations of Natural\n  Language", "comments": "12 pages, 2 tables, ICLR-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most representation learning algorithms for language and image processing are\nlocal, in that they identify features for a data point based on surrounding\npoints. Yet in language processing, the correct meaning of a word often depends\non its global context. As a step toward incorporating global context into\nrepresentation learning, we develop a representation learning algorithm that\nincorporates joint prediction into its technique for producing features for a\nword. We develop efficient variational methods for learning Factorial Hidden\nMarkov Models from large texts, and use variational distributions to produce\nfeatures for each word that are sensitive to the entire input sequence, not\njust to a local context window. Experiments on part-of-speech tagging and\nchunking indicate that the features are competitive with or better than\nexisting state-of-the-art representation learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 22:44:26 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 04:49:47 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 11:22:30 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Nepal", "Anjan", ""], ["Yates", "Alexander", ""]]}, {"id": "1312.6173", "submitter": "Karl Moritz Hermann", "authors": "Karl Moritz Hermann and Phil Blunsom", "title": "Multilingual Distributed Representations without Word Alignment", "comments": "To appear at ICLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of meaning are a natural way to encode covariance\nrelationships between words and phrases in NLP. By overcoming data sparsity\nproblems, as well as providing information about semantic relatedness which is\nnot available in discrete representations, distributed representations have\nproven useful in many NLP tasks. Recent work has shown how compositional\nsemantic representations can successfully be applied to a number of monolingual\napplications such as sentiment analysis. At the same time, there has been some\ninitial success in work on learning shared word-level representations across\nlanguages. We combine these two approaches by proposing a method for learning\ndistributed representations in a multilingual setup. Our model learns to assign\nsimilar embeddings to aligned sentences and dissimilar ones to sentence which\nare not aligned while not requiring word alignments. We show that our\nrepresentations are semantically informative and apply them to a cross-lingual\ndocument classification task where we outperform the previous state of the art.\nFurther, by employing parallel corpora of multiple language pairs we find that\nour model learns representations that capture semantic relationships across\nlanguages for which no parallel data was used.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 23:13:38 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 20:24:06 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 17:52:13 GMT"}, {"version": "v4", "created": "Thu, 20 Mar 2014 13:55:02 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Hermann", "Karl Moritz", ""], ["Blunsom", "Phil", ""]]}, {"id": "1312.6192", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman", "title": "Can recursive neural tensor networks learn logical reasoning?", "comments": "Submitted for presentation at ICLR 2014. Source code and data:\n  http://goo.gl/PSyF5u", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural network models and their accompanying vector representations\nfor words have seen success in an array of increasingly semantically\nsophisticated tasks, but almost nothing is known about their ability to\naccurately capture the aspects of linguistic meaning that are necessary for\ninterpretation or reasoning. To evaluate this, I train a recursive model on a\nnew corpus of constructed examples of logical reasoning in short sentences,\nlike the inference of \"some animal walks\" from \"some dog walks\" or \"some cat\nwalks,\" given that dogs and cats are animals. This model learns representations\nthat generalize well to new types of reasoning pattern in all but a few cases,\na result which is promising for the ability of learned representation models to\ncapture logical reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 02:29:42 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 01:42:09 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2014 18:02:09 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2014 20:59:04 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Bowman", "Samuel R.", ""]]}, {"id": "1312.6802", "submitter": "Bhagwati Pande", "authors": "B. P. Pande, Pawan Tamta and H. S. Dhami", "title": "Suffix Stripping Problem as an Optimization Problem", "comments": "14 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming or suffix stripping, an important part of the modern Information\nRetrieval systems, is to find the root word (stem) out of a given cluster of\nwords. Existing algorithms targeting this problem have been developed in a\nhaphazard manner. In this work, we model this problem as an optimization\nproblem. An Integer Program is being developed to overcome the shortcomings of\nthe existing approaches. The sample results of the proposed method are also\nbeing compared with an established technique in the field for English language.\nAn AMPL code for the same IP has also been given.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 12:06:48 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Pande", "B. P.", ""], ["Tamta", "Pawan", ""], ["Dhami", "H. S.", ""]]}, {"id": "1312.6849", "submitter": "Zoran Cvetkovic", "authors": "Matthew Ager and Zoran Cvetkovic and Peter Sollich", "title": "Speech Recognition Front End Without Information Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech representation and modelling in high-dimensional spaces of acoustic\nwaveforms, or a linear transformation thereof, is investigated with the aim of\nimproving the robustness of automatic speech recognition to additive noise. The\nmotivation behind this approach is twofold: (i) the information in acoustic\nwaveforms that is usually removed in the process of extracting low-dimensional\nfeatures might aid robust recognition by virtue of structured redundancy\nanalogous to channel coding, (ii) linear feature domains allow for exact noise\nadaptation, as opposed to representations that involve non-linear processing\nwhich makes noise adaptation challenging. Thus, we develop a generative\nframework for phoneme modelling in high-dimensional linear feature domains, and\nuse it in phoneme classification and recognition tasks. Results show that\nclassification and recognition in this framework perform better than analogous\nPLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional\nand MFCC features at the likelihood level performs uniformly better than either\nof the individual representations across all noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 16:36:16 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 09:17:46 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ager", "Matthew", ""], ["Cvetkovic", "Zoran", ""], ["Sollich", "Peter", ""]]}, {"id": "1312.6947", "submitter": "Sourish Dasgupta", "authors": "Sourish Dasgupta, Ankur Padia, Kushal Shah, Prasenjit Majumder", "title": "Formal Ontology Learning on Factual IS-A Corpus in English using\n  Description Logics", "comments": "This paper has been withdrawn by the author due to requirement of\n  re-evaluation of results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology Learning (OL) is the computational task of generating a knowledge\nbase in the form of an ontology given an unstructured corpus whose content is\nin natural language (NL). Several works can be found in this area most of which\nare limited to statistical and lexico-syntactic pattern matching based\ntechniques Light-Weight OL. These techniques do not lead to very accurate\nlearning mostly because of several linguistic nuances in NL. Formal OL is an\nalternative (less explored) methodology were deep linguistics analysis is made\nusing theory and tools found in computational linguistics to generate formal\naxioms and definitions instead simply inducing a taxonomy. In this paper we\npropose \"Description Logic (DL)\" based formal OL framework for learning factual\nIS-A type sentences in English. We claim that semantic construction of IS-A\nsentences is non trivial. Hence, we also claim that such sentences requires\nspecial studies in the context of OL before any truly formal OL can be\nproposed. We introduce a learner tool, called DLOL_IS-A, that generated such\nontologies in the owl format. We have adopted \"Gold Standard\" based OL\nevaluation on IS-A rich WCL v.1.1 dataset and our own Community representative\nIS-A dataset. We observed significant improvement of DLOL_IS-A when compared to\nthe light-weight OL tool Text2Onto and formal OL tool FRED.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 09:17:28 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 05:20:47 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Dasgupta", "Sourish", ""], ["Padia", "Ankur", ""], ["Shah", "Kushal", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1312.6948", "submitter": "Sourish Dasgupta", "authors": "Sourish Dasgupta, Rupali KaPatel, Ankur Padia, Kushal Shah", "title": "Description Logics based Formalization of Wh-Queries", "comments": "Natural Language Query Processing, Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Natural Language Query Formalization (NLQF) is to translate a\ngiven user query in natural language (NL) into a formal language so that the\nsemantic interpretation has equivalence with the NL interpretation.\nFormalization of NL queries enables logic based reasoning during information\nretrieval, database query, question-answering, etc. Formalization also helps in\nWeb query normalization and indexing, query intent analysis, etc. In this paper\nwe are proposing a Description Logics based formal methodology for wh-query\nintent (also called desire) identification and corresponding formal\ntranslation. We evaluated the scalability of our proposed formalism using\nMicrosoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 09:23:49 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Dasgupta", "Sourish", ""], ["KaPatel", "Rupali", ""], ["Padia", "Ankur", ""], ["Shah", "Kushal", ""]]}, {"id": "1312.6962", "submitter": "Ahmad Kamal", "authors": "Ahmad Kamal", "title": "Subjectivity Classification using Machine Learning Techniques for Mining\n  Feature-Opinion Pairs from Web Opinion Sources", "comments": "10 pages, 2 Color Photographs, 1 Diagram, 14 Charts, 2 Graphs,\n  International Journal of Computer Science Issues (IJCSI), Vol. 10, Issue 5,\n  No 1, September 2013", "journal-ref": "International Journal of Computer Science Issues (IJCSI), Volume\n  10 Issue 5, 2013, pp 191-200", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to flourish of the Web 2.0, web opinion sources are rapidly emerging\ncontaining precious information useful for both customers and manufactures.\nRecently, feature based opinion mining techniques are gaining momentum in which\ncustomer reviews are processed automatically for mining product features and\nuser opinions expressed over them. However, customer reviews may contain both\nopinionated and factual sentences. Distillations of factual contents improve\nmining performance by preventing noisy and irrelevant extraction. In this\npaper, combination of both supervised machine learning and rule-based\napproaches are proposed for mining feasible feature-opinion pairs from\nsubjective review sentences. In the first phase of the proposed approach, a\nsupervised machine learning technique is applied for classifying subjective and\nobjective sentences from customer reviews. In the next phase, a rule based\nmethod is implemented which applies linguistic and semantic analysis of texts\nto mine feasible feature-opinion pairs from subjective sentences retained after\nthe first phase. The effectiveness of the proposed methods is established\nthrough experimentation over customer reviews on different electronic products.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 12:38:17 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Kamal", "Ahmad", ""]]}, {"id": "1312.7077", "submitter": "Ankur Parikh", "authors": "Ankur P. Parikh, Avneesh Saluja, Chris Dyer, Eric P. Xing", "title": "Language Modeling with Power Low Rank Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present power low rank ensembles (PLRE), a flexible framework for n-gram\nlanguage modeling where ensembles of low rank matrices and tensors are used to\nobtain smoothed probability estimates of words in context. Our method can be\nunderstood as a generalization of n-gram modeling to non-integer n, and\nincludes standard techniques such as absolute discounting and Kneser-Ney\nsmoothing as special cases. PLRE training is efficient and our approach\noutperforms state-of-the-art modified Kneser Ney baselines in terms of\nperplexity on large corpora as well as on BLEU score in a downstream machine\ntranslation task.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 09:45:02 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 08:28:03 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Parikh", "Ankur P.", ""], ["Saluja", "Avneesh", ""], ["Dyer", "Chris", ""], ["Xing", "Eric P.", ""]]}, {"id": "1312.7223", "submitter": "Mahima Sharma", "authors": "Rashmi Gupta, Nisheeth Joshi, Iti Mathur", "title": "Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier", "comments": "In Proceedings of 2013 International Conference on Advances in\n  Computing, Communications and Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach for estimating the quality of machine\ntranslation system. There are various methods for estimating the quality of\noutput sentences, but in this paper we focus on Na\\\"ive Bayes classifier to\nbuild model using features which are extracted from the input sentences. These\nfeatures are used for finding the likelihood of each of the sentences of the\ntraining data which are then further used for determining the scores of the\ntest data. On the basis of these scores we determine the class labels of the\ntest data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 09:39:52 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Gupta", "Rashmi", ""], ["Joshi", "Nisheeth", ""], ["Mathur", "Iti", ""]]}]