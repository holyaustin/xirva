[{"id": "1704.00016", "submitter": "Esra Akbas", "authors": "Esra Akbas", "title": "Opinion Mining on Non-English Short Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the type and the number of such venues increase, automated analysis of\nsentiment on textual resources has become an essential data mining task. In\nthis paper, we investigate the problem of mining opinions on the collection of\ninformal short texts. Both positive and negative sentiment strength of texts\nare detected. We focus on a non-English language that has few resources for\ntext mining. This approach would help enhance the sentiment analysis in\nlanguages where a list of opinionated words does not exist. We propose a new\nmethod projects the text into dense and low dimensional feature vectors\naccording to the sentiment strength of the words. We detect the mixture of\npositive and negative sentiments on a multi-variant scale. Empirical evaluation\nof the proposed framework on Turkish tweets shows that our approach gets good\nresults for opinion mining.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 18:05:44 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 01:51:43 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Akbas", "Esra", ""]]}, {"id": "1704.00051", "submitter": "Danqi Chen", "authors": "Danqi Chen, Adam Fisch, Jason Weston and Antoine Bordes", "title": "Reading Wikipedia to Answer Open-Domain Questions", "comments": "ACL2017, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to tackle open- domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval (finding the relevant articles) with that of\nmachine comprehension of text (identifying the answer spans from those\narticles). Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 20:39:10 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 03:53:14 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Chen", "Danqi", ""], ["Fisch", "Adam", ""], ["Weston", "Jason", ""], ["Bordes", "Antoine", ""]]}, {"id": "1704.00052", "submitter": "Katharina Kann", "authors": "Katharina Kann, Ryan Cotterell, Hinrich Sch\\\"utze", "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel cross-lingual transfer method for paradigm completion, the\ntask of mapping a lemma to its inflected forms, using a neural encoder-decoder\nmodel, the state of the art for the monolingual task. We use labeled data from\na high-resource language to increase performance on a low-resource language. In\nexperiments on 21 language pairs from four different language families, we\nobtain up to 58% higher accuracy than without transfer and show that even\nzero-shot and one-shot learning are possible. We further find that the degree\nof language relatedness strongly influences the ability to transfer\nmorphological knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 20:39:38 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Kann", "Katharina", ""], ["Cotterell", "Ryan", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1704.00057", "submitter": "Layla El Asri", "authors": "Layla El Asri and Hannes Schulz and Shikhar Sharma and Jeremie Zumer\n  and Justin Harris and Emery Fine and Rahul Mehrotra and Kaheer Suleman", "title": "Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Frames dataset (Frames is available at\nhttp://datasets.maluuba.com/Frames), a corpus of 1369 human-human dialogues\nwith an average of 15 turns per dialogue. We developed this dataset to study\nthe role of memory in goal-oriented dialogue systems. Based on Frames, we\nintroduce a task called frame tracking, which extends state tracking to a\nsetting where several states are tracked simultaneously. We propose a baseline\nmodel for this task. We show that Frames can also be used to study memory in\ndialogue management and information presentation through natural language\ngeneration.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 21:03:58 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 18:22:49 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Asri", "Layla El", ""], ["Schulz", "Hannes", ""], ["Sharma", "Shikhar", ""], ["Zumer", "Jeremie", ""], ["Harris", "Justin", ""], ["Fine", "Emery", ""], ["Mehrotra", "Rahul", ""], ["Suleman", "Kaheer", ""]]}, {"id": "1704.00119", "submitter": "Meysam Alizadeh", "authors": "Meysam Alizadeh, Ingmar Weber, Claudio Cioffi-Revilla, Santo\n  Fortunato, Michael Macy", "title": "Psychological and Personality Profiles of Political Extremists", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global recruitment into radical Islamic movements has spurred renewed\ninterest in the appeal of political extremism. Is the appeal a rational\nresponse to material conditions or is it the expression of psychological and\npersonality disorders associated with aggressive behavior, intolerance,\nconspiratorial imagination, and paranoia? Empirical answers using surveys have\nbeen limited by lack of access to extremist groups, while field studies have\nlacked psychological measures and failed to compare extremists with contrast\ngroups. We revisit the debate over the appeal of extremism in the U.S. context\nby comparing publicly available Twitter messages written by over 355,000\npolitical extremist followers with messages written by non-extremist U.S.\nusers. Analysis of text-based psychological indicators supports the moral\nfoundation theory which identifies emotion as a critical factor in determining\npolitical orientation of individuals. Extremist followers also differ from\nothers in four of the Big Five personality traits.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 04:31:37 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Alizadeh", "Meysam", ""], ["Weber", "Ingmar", ""], ["Cioffi-Revilla", "Claudio", ""], ["Fortunato", "Santo", ""], ["Macy", "Michael", ""]]}, {"id": "1704.00135", "submitter": "Vadim Markovtsev", "authors": "Vadim Markovtsev and Eiso Kant", "title": "Topic modeling of public repositories at scale using names in source\n  code", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming languages themselves have a limited number of reserved keywords\nand character based tokens that define the language specification. However,\nprogrammers have a rich use of natural language within their code through\ncomments, text literals and naming entities. The programmer defined names that\ncan be found in source code are a rich source of information to build a high\nlevel understanding of the project. The goal of this paper is to apply topic\nmodeling to names used in over 13.6 million repositories and perceive the\ninferred topics. One of the problems in such a study is the occurrence of\nduplicate repositories not officially marked as forks (obscure forks). We show\nhow to address it using the same identifiers which are extracted for topic\nmodeling.\n  We open with a discussion on naming in source code, we then elaborate on our\napproach to remove exact duplicate and fuzzy duplicate repositories using\nLocality Sensitive Hashing on the bag-of-words model and then discuss our work\non topic modeling; and finally present the results from our data analysis\ntogether with open-access to the source code, tools and datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 08:16:20 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 08:29:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Markovtsev", "Vadim", ""], ["Kant", "Eiso", ""]]}, {"id": "1704.00177", "submitter": "Haixia Liu", "authors": "Haixia Liu", "title": "Sentiment Analysis of Citations Using Word2vec", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation sentiment analysis is an important task in scientific paper\nanalysis. Existing machine learning techniques for citation sentiment analysis\nare focusing on labor-intensive feature engineering, which requires large\nannotated corpus. As an automatic feature extraction tool, word2vec has been\nsuccessfully applied to sentiment analysis of short texts. In this work, I\nconducted empirical research with the question: how well does word2vec work on\nthe sentiment analysis of citations? The proposed method constructed sentence\nvectors (sent2vec) by averaging the word embeddings, which were learned from\nAnthology Collections (ACL-Embeddings). I also investigated polarity-specific\nword embeddings (PS-Embeddings) for classifying positive and negative\ncitations. The sentence vectors formed a feature space, to which the examined\ncitation sentence was mapped to. Those features were input into classifiers\n(support vector machines) for supervised classification. Using\n10-cross-validation scheme, evaluation was conducted on a set of annotated\ncitations. The results showed that word embeddings are effective on classifying\npositive and negative citations. However, hand-crafted features performed\nbetter for the overall classification.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 14:53:54 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Liu", "Haixia", ""]]}, {"id": "1704.00200", "submitter": "Amrita Saha", "authors": "Amrita Saha, Mitesh Khapra, Karthik Sankaranarayanan", "title": "Towards Building Large Scale Multimodal Domain-Aware Conversation\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While multimodal conversation agents are gaining importance in several\ndomains such as retail, travel etc., deep learning research in this area has\nbeen limited primarily due to the lack of availability of large-scale, open\nchatlogs. To overcome this bottleneck, in this paper we introduce the task of\nmultimodal, domain-aware conversations, and propose the MMD benchmark dataset.\nThis dataset was gathered by working in close coordination with large number of\ndomain experts in the retail domain. These experts suggested various\nconversations flows and dialog states which are typically seen in multimodal\nconversations in the fashion domain. Keeping these flows and states in mind, we\ncreated a dataset consisting of over 150K conversation sessions between\nshoppers and sales agents, with the help of in-house annotators using a\nsemi-automated manually intense iterative process. With this dataset, we\npropose 5 new sub-tasks for multimodal conversations along with their\nevaluation methodology. We also propose two multimodal neural models in the\nencode-attend-decode paradigm and demonstrate their performance on two of the\nsub-tasks, namely text response generation and best image response selection.\nThese experiments serve to establish baseline performance and open new research\ndirections for each of these sub-tasks. Further, for each of the sub-tasks, we\npresent a `per-state evaluation' of 9 most significant dialog states, which\nwould enable more focused research into understanding the challenges and\ncomplexities involved in each of these states.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 17:05:35 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 07:50:08 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 06:43:48 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Saha", "Amrita", ""], ["Khapra", "Mitesh", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "1704.00217", "submitter": "Zhiting Hu", "authors": "Lianhui Qin, Zhisong Zhang, Hai Zhao, Zhiting Hu, Eric P. Xing", "title": "Adversarial Connective-exploiting Networks for Implicit Discourse\n  Relation Classification", "comments": "To appear in ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit discourse relation classification is of great challenge due to the\nlack of connectives as strong linguistic cues, which motivates the use of\nannotated implicit connectives to improve the recognition. We propose a feature\nimitation framework in which an implicit relation network is driven to learn\nfrom another neural network with access to connectives, and thus encouraged to\nextract similarly salient features for accurate classification. We develop an\nadversarial model to enable an adaptive imitation scheme through competition\nbetween the implicit network and a rival feature discriminator. Our method\neffectively transfers discriminability of connectives to the implicit features,\nand achieves state-of-the-art performance on the PDTB benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 19:29:21 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Qin", "Lianhui", ""], ["Zhang", "Zhisong", ""], ["Zhao", "Hai", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1704.00253", "submitter": "Jaehong Park", "authors": "Jaehong Park, Jongyoon Song, Sungroh Yoon", "title": "Building a Neural Machine Translation System Using Only Synthetic\n  Parallel Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that synthetic parallel data automatically generated\nby translation models can be effective for various neural machine translation\n(NMT) issues. In this study, we build NMT systems using only synthetic parallel\ndata. As an efficient alternative to real parallel data, we also present a new\ntype of synthetic parallel corpus. The proposed pseudo parallel data are\ndistinct from previous works in that ground truth and synthetic examples are\nmixed on both sides of sentence pairs. Experiments on Czech-German and\nFrench-German translations demonstrate the efficacy of the proposed pseudo\nparallel corpus, which shows not only enhanced results for bidirectional\ntranslation tasks but also substantial improvement with the aid of a ground\ntruth real parallel corpus.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 05:54:14 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 02:58:02 GMT"}, {"version": "v3", "created": "Mon, 15 May 2017 09:23:21 GMT"}, {"version": "v4", "created": "Sun, 17 Sep 2017 01:58:39 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Park", "Jaehong", ""], ["Song", "Jongyoon", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1704.00380", "submitter": "Mamoru Komachi", "authors": "Junki Matsuo, Mamoru Komachi and Katsuhito Sudoh", "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using\n  Word Embeddings", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the most important problems in machine translation (MT) evaluation is\nto evaluate the similarity between translation hypotheses with different\nsurface forms from the reference, especially at the segment level. We propose\nto use word embeddings to perform word alignment for segment-level MT\nevaluation. We performed experiments with three types of alignment methods\nusing word embeddings. We evaluated our proposed methods with various\ntranslation datasets. Experimental results show that our proposed methods\noutperform previous word embeddings-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 22:36:56 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Matsuo", "Junki", ""], ["Komachi", "Mamoru", ""], ["Sudoh", "Katsuhito", ""]]}, {"id": "1704.00405", "submitter": "Lei Sha", "authors": "Feng Qian, Lei Sha, Baobao Chang, Lu-chen Liu, Ming Zhang", "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As for semantic role labeling (SRL) task, when it comes to utilizing parsing\ninformation, both traditional methods and recent recurrent neural network (RNN)\nbased methods use the feature engineering way. In this paper, we propose Syntax\nAware Long Short Time Memory(SA-LSTM). The structure of SA-LSTM modifies\naccording to dependency parsing information in order to model parsing\ninformation directly in an architecture engineering way instead of feature\nengineering way. We experimentally demonstrate that SA-LSTM gains more\nimprovement from the model architecture. Furthermore, SA-LSTM outperforms the\nstate-of-the-art on CPB 1.0 significantly according to Student t-test\n($p<0.05$).\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 02:10:19 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 01:55:26 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Qian", "Feng", ""], ["Sha", "Lei", ""], ["Chang", "Baobao", ""], ["Liu", "Lu-chen", ""], ["Zhang", "Ming", ""]]}, {"id": "1704.00440", "submitter": "Yinfei Yang Redfin Corportation", "authors": "Yinfei Yang, Ani Nenkova", "title": "Combining Lexical and Syntactic Features for Detecting Content-dense\n  Texts in News", "comments": "In submission to JAIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-dense news report important factual information about an event in\ndirect, succinct manner. Information seeking applications such as information\nextraction, question answering and summarization normally assume all text they\ndeal with is content-dense. Here we empirically test this assumption on news\narticles from the business, U.S. international relations, sports and science\njournalism domains. Our findings clearly indicate that about half of the news\ntexts in our study are in fact not content-dense and motivate the development\nof a supervised content-density detector. We heuristically label a large\ntraining corpus for the task and train a two-layer classifying model based on\nlexical and unlexicalized syntactic features. On manually annotated data, we\ncompare the performance of domain-specific classifiers, trained on data only\nfrom a given news domain and a general classifier in which data from all four\ndomains is pooled together. Our annotation and prediction experiments\ndemonstrate that the concept of content density varies depending on the domain\nand that naive annotators provide judgement biased toward the stereotypical\ndomain label. Domain-specific classifiers are more accurate for domains in\nwhich content-dense texts are typically fewer. Domain independent classifiers\nreproduce better naive crowdsourced judgements. Classification prediction is\nhigh across all conditions, around 80%.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 06:22:04 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Yang", "Yinfei", ""], ["Nenkova", "Ani", ""]]}, {"id": "1704.00514", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein, Anders S{\\o}gaard", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase boundary classification (KBC) is the task of detecting keyphrases\nin scientific articles and labelling them with respect to predefined types.\nAlthough important in practice, this task is so far underexplored, partly due\nto the lack of labelled data. To overcome this, we explore several auxiliary\ntasks, including semantic super-sense tagging and identification of multi-word\nexpressions, and cast the task as a multi-task learning problem with deep\nrecurrent neural networks. Our multi-task models perform significantly better\nthan previous state of the art approaches on two scientific KBC datasets,\nparticularly for long keyphrases.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:25:22 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 16:48:49 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1704.00552", "submitter": "Daniel Hershcovich", "authors": "Daniel Hershcovich, Omri Abend and Ari Rappoport", "title": "A Transition-Based Directed Acyclic Graph Parser for UCCA", "comments": "16 pages; Accepted as long paper at ACL2017", "journal-ref": null, "doi": "10.18653/v1/P17-1104", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first parser for UCCA, a cross-linguistically applicable\nframework for semantic representation, which builds on extensive typological\nwork and supports rapid annotation. UCCA poses a challenge for existing parsing\ntechniques, as it exhibits reentrancy (resulting in DAG structures),\ndiscontinuous structures and non-terminal nodes corresponding to complex\nsemantic units. To our knowledge, the conjunction of these formal properties is\nnot supported by any existing parser. Our transition-based parser, which uses a\nnovel transition set and features based on bidirectional LSTMs, has value not\njust for UCCA parsing: its ability to handle more general graph structures can\ninform the development of parsers for other semantic DAG structures, and in\nlanguages that frequently use discontinuous structures.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 12:40:54 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 10:51:51 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Hershcovich", "Daniel", ""], ["Abend", "Omri", ""], ["Rappoport", "Ari", ""]]}, {"id": "1704.00559", "submitter": "Matthias Sperber", "authors": "Matthias Sperber, Graham Neubig, Jan Niehues, Alex Waibel", "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to a neural sequence-to-sequence model is often determined by an\nup-stream system, e.g. a word segmenter, part of speech tagger, or speech\nrecognizer. These up-stream models are potentially error-prone. Representing\ninputs through word lattices allows making this uncertainty explicit by\ncapturing alternative sequences and their posterior probabilities in a compact\nform. In this work, we extend the TreeLSTM (Tai et al., 2015) into a\nLatticeLSTM that is able to consume word lattices, and can be used as encoder\nin an attentional encoder-decoder model. We integrate lattice posterior scores\ninto this architecture by extending the TreeLSTM's child-sum and forget gates\nand introducing a bias term into the attention mechanism. We experiment with\nspeech translation lattices and report consistent improvements over baselines\nthat translate either the 1-best hypothesis or the lattice without posterior\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:03:40 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 13:31:07 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Sperber", "Matthias", ""], ["Neubig", "Graham", ""], ["Niehues", "Jan", ""], ["Waibel", "Alex", ""]]}, {"id": "1704.00656", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, Rob\n  Procter", "title": "Detection and Resolution of Rumours in Social Media: A Survey", "comments": "ACM Computing Surveys", "journal-ref": "ACM Computing Surveys 51, 2, Article 32 (February 2018), 36 pages", "doi": "10.1145/3161603", "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the increasing use of social media platforms for information and news\ngathering, its unmoderated nature often leads to the emergence and spread of\nrumours, i.e. pieces of information that are unverified at the time of posting.\nAt the same time, the openness of social media platforms provides opportunities\nto study how users share and discuss rumours, and to explore how natural\nlanguage processing and data mining techniques may be used to find ways of\ndetermining their veracity. In this survey we introduce and discuss two types\nof rumours that circulate on social media; long-standing rumours that circulate\nfor long periods of time, and newly-emerging rumours spawned during fast-paced\nevents such as breaking news, where reports are released piecemeal and often\nwith an unverified status in their early stages. We provide an overview of\nresearch into social media rumours with the ultimate goal of developing a\nrumour classification system that consists of four components: rumour\ndetection, rumour tracking, rumour stance classification and rumour veracity\nclassification. We delve into the approaches presented in the scientific\nliterature for the development of each of these four components. We summarise\nthe efforts and achievements so far towards the development of rumour\nclassification systems and conclude with suggestions for avenues for future\nresearch in social media mining for detection and resolution of rumours.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:57:44 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 20:28:29 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 08:24:46 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Aker", "Ahmet", ""], ["Bontcheva", "Kalina", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1704.00717", "submitter": "Viraj Prabhu", "authors": "Arjun Chandrasekaran, Deshraj Yadav, Prithvijit Chattopadhyay, Viraj\n  Prabhu, Devi Parikh", "title": "It Takes Two to Tango: Towards Theory of AI's Mind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory of Mind is the ability to attribute mental states (beliefs, intents,\nknowledge, perspectives, etc.) to others and recognize that these mental states\nmay differ from one's own. Theory of Mind is critical to effective\ncommunication and to teams demonstrating higher collective performance. To\neffectively leverage the progress in Artificial Intelligence (AI) to make our\nlives more productive, it is important for humans and AI to work well together\nin a team. Traditionally, there has been much emphasis on research to make AI\nmore accurate, and (to a lesser extent) on having it better understand human\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\nmore human-like and having it develop a theory of our minds. In this work, we\nargue that for human-AI teams to be effective, humans must also develop a\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\nand quirks. We instantiate these ideas within the domain of Visual Question\nAnswering (VQA). We find that using just a few examples (50), lay people can be\ntrained to better predict responses and oncoming failures of a complex VQA\nmodel. We further evaluate the role existing explanation (or interpretability)\nmodalities play in helping humans build ToAIM. Explainable AI has received\nconsiderable scientific and popular attention in recent times. Surprisingly, we\nfind that having access to the model's internal states - its confidence in its\ntop-k predictions, explicit or implicit attention maps which highlight regions\nin the image (and words in the question) the model is looking at (and listening\nto) while answering a question about an image - do not help people better\npredict its behavior.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 17:58:07 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 17:55:50 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Yadav", "Deshraj", ""], ["Chattopadhyay", "Prithvijit", ""], ["Prabhu", "Viraj", ""], ["Parikh", "Devi", ""]]}, {"id": "1704.00774", "submitter": "Alexandre Salle", "authors": "Alexandre Salle, Aline Villavicencio", "title": "Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency\n  and Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the capacity of recurrent neural networks (RNN) usually involves\naugmenting the size of the hidden layer, with significant increase of\ncomputational cost. Recurrent neural tensor networks (RNTN) increase capacity\nusing distinct hidden layer weights for each word, but with greater costs in\nmemory usage. In this paper, we introduce restricted recurrent neural tensor\nnetworks (r-RNTN) which reserve distinct hidden layer weights for frequent\nvocabulary words while sharing a single set of weights for infrequent words.\nPerplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve\nlanguage model performance over RNNs using only a small fraction of the\nparameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated\nRecurrent Units and Long Short-Term Memory.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:17:58 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 02:59:14 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 22:49:10 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Salle", "Alexandre", ""], ["Villavicencio", "Aline", ""]]}, {"id": "1704.00784", "submitter": "Colin Raffel", "authors": "Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas\n  Eck", "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments", "comments": "ICML camera-ready version; 10 pages + 9 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:45:27 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 21:14:58 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Raffel", "Colin", ""], ["Luong", "Minh-Thang", ""], ["Liu", "Peter J.", ""], ["Weiss", "Ron J.", ""], ["Eck", "Douglas", ""]]}, {"id": "1704.00849", "submitter": "Chin-Cheng Hsu", "authors": "Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang", "title": "Voice Conversion from Unaligned Corpora using Variational Autoencoding\n  Wasserstein Generative Adversarial Networks", "comments": "Submitted to INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a voice conversion (VC) system from non-parallel speech corpora is\nchallenging but highly valuable in real application scenarios. In most\nsituations, the source and the target speakers do not repeat the same texts or\nthey may even speak different languages. In this case, one possible, although\nindirect, solution is to build a generative model for speech. Generative models\nfocus on explaining the observations with latent variables instead of learning\na pairwise transformation function, thereby bypassing the requirement of speech\nframe alignment. In this paper, we propose a non-parallel VC framework with a\nvariational autoencoding Wasserstein generative adversarial network (VAW-GAN)\nthat explicitly considers a VC objective when building the speech model.\nExperimental results corroborate the capability of our framework for building a\nVC system from unaligned data, and demonstrate improved conversion quality.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 01:47:14 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 04:19:07 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 01:08:38 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Hsu", "Chin-Cheng", ""], ["Hwang", "Hsin-Te", ""], ["Wu", "Yi-Chiao", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1704.00898", "submitter": "Ganesh J", "authors": "J Ganesh, Manish Gupta, Vasudeva Varma", "title": "Interpretation of Semantic Tweet Representations", "comments": "Accepted at ASONAM 2017; Initial version presented at NIPS 2016\n  workshop can be found at arXiv:1611.04887", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in analysis of microblogging platforms is experiencing a renewed\nsurge with a large number of works applying representation learning models for\napplications like sentiment analysis, semantic textual similarity computation,\nhashtag prediction, etc. Although the performance of the representation\nlearning models has been better than the traditional baselines for such tasks,\nlittle is known about the elementary properties of a tweet encoded within these\nrepresentations, or why particular representations work better for certain\ntasks. Our work presented here constitutes the first step in opening the\nblack-box of vector embeddings for tweets. Traditional feature engineering\nmethods for high-level applications have exploited various elementary\nproperties of tweets. We believe that a tweet representation is effective for\nan application because it meticulously encodes the application-specific\nelementary properties of tweets. To understand the elementary properties\nencoded in a tweet representation, we evaluate the representations on the\naccuracy to which they can model each of those properties such as tweet length,\npresence of particular words, hashtags, mentions, capitalization, etc. Our\nsystematic extensive study of nine supervised and four unsupervised tweet\nrepresentations against most popular eight textual and five social elementary\nproperties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors\n(STV) best encode the textual and social properties of tweets respectively.\nFastText is the best model for low resource settings, providing very little\ndegradation with reduction in embedding size. Finally, we draw interesting\ninsights by correlating the model performance obtained for elementary property\nprediction tasks with the highlevel downstream applications.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 07:24:15 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 06:59:17 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Ganesh", "J", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1704.00924", "submitter": "Mamoru Komachi", "authors": "Ryosuke Miyazaki and Mamoru Komachi", "title": "Japanese Sentiment Classification using a Tree-Structured Long\n  Short-Term Memory with Attention", "comments": "10 pages; PACLIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous approaches to training syntax-based sentiment classification models\nrequired phrase-level annotated corpora, which are not readily available in\nmany languages other than English. Thus, we propose the use of tree-structured\nLong Short-Term Memory with an attention mechanism that pays attention to each\nsubtree of the parse tree. Experimental results indicate that our model\nachieves the state-of-the-art performance in a Japanese sentiment\nclassification task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 09:08:46 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 03:11:29 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Miyazaki", "Ryosuke", ""], ["Komachi", "Mamoru", ""]]}, {"id": "1704.00939", "submitter": "Jacopo Staiano", "authors": "Youness Mansar, Lorenzo Gatti, Sira Ferradans, Marco Guerini, Jacopo\n  Staiano", "title": "Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring\n  Sentiment towards Brands from Financial News Headlines", "comments": "6 pages, 1 figure; accepted for publication at the International\n  Workshop on Semantic Evaluation (SemEval-2017) to be held in conjunction with\n  ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a methodology to infer Bullish or Bearish\nsentiment towards companies/brands. More specifically, our approach leverages\naffective lexica and word embeddings in combination with convolutional neural\nnetworks to infer the sentiment of financial news headlines towards a target\ncompany. Such architecture was used and evaluated in the context of the SemEval\n2017 challenge (task 5, subtask 2), in which it obtained the best performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 10:01:47 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Mansar", "Youness", ""], ["Gatti", "Lorenzo", ""], ["Ferradans", "Sira", ""], ["Guerini", "Marco", ""], ["Staiano", "Jacopo", ""]]}, {"id": "1704.01074", "submitter": "Hao Zhou", "authors": "Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, Bing Liu", "title": "Emotional Chatting Machine: Emotional Conversation Generation with\n  Internal and External Memory", "comments": "Accepted in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception and expression of emotion are key factors to the success of\ndialogue systems or conversational agents. However, this problem has not been\nstudied in large-scale conversation generation so far. In this paper, we\npropose Emotional Chatting Machine (ECM) that can generate appropriate\nresponses not only in content (relevant and grammatical) but also in emotion\n(emotionally consistent). To the best of our knowledge, this is the first work\nthat addresses the emotion factor in large-scale conversation generation. ECM\naddresses the factor using three new mechanisms that respectively (1) models\nthe high-level abstraction of emotion expressions by embedding emotion\ncategories, (2) captures the change of implicit internal emotion states, and\n(3) uses explicit emotion expressions with an external emotion vocabulary.\nExperiments show that the proposed model can generate responses appropriate not\nonly in content but also in emotion.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:44:48 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 10:17:53 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 10:29:06 GMT"}, {"version": "v4", "created": "Fri, 1 Jun 2018 03:38:59 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Zhou", "Hao", ""], ["Huang", "Minlie", ""], ["Zhang", "Tianyang", ""], ["Zhu", "Xiaoyan", ""], ["Liu", "Bing", ""]]}, {"id": "1704.01314", "submitter": "Yan Shao", "authors": "Yan Shao and Christian Hardmeier and J\\\"org Tiedemann and Joakim Nivre", "title": "Character-based Joint Segmentation and POS Tagging for Chinese using\n  Bidirectional RNN-CRF", "comments": "10 pages plus 1 page appendix, 3 figures, IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a character-based model for joint segmentation and POS tagging for\nChinese. The bidirectional RNN-CRF architecture for general sequence tagging is\nadapted and applied with novel vector representations of Chinese characters\nthat capture rich contextual information and lower-than-character level\nfeatures. The proposed model is extensively evaluated and compared with a\nstate-of-the-art tagger respectively on CTB5, CTB9 and UD Chinese. The\nexperimental results indicate that our model is accurate and robust across\ndatasets in different sizes, genres and annotation schemes. We obtain\nstate-of-the-art performance on CTB5, achieving 94.38 F1-score for joint\nsegmentation and POS tagging.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 08:58:44 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 22:53:37 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 09:29:15 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Shao", "Yan", ""], ["Hardmeier", "Christian", ""], ["Tiedemann", "J\u00f6rg", ""], ["Nivre", "Joakim", ""]]}, {"id": "1704.01346", "submitter": "Laurent Besacier", "authors": "Jeremy Ferrero, Frederic Agnes, Laurent Besacier, Didier Schwab", "title": "CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection\n  Methods for Semantic Textual Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our submitted systems for Semantic Textual Similarity (STS) Track\n4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must\nestimate their semantic similarity by a score between 0 and 5. In our\nsubmission, we use syntax-based, dictionary-based, context-based, and MT-based\nmethods. We also combine these methods in unsupervised and supervised way. Our\nbest run ranked 1st on track 4a with a correlation of 83.02% with human\nannotations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 10:07:22 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Ferrero", "Jeremy", ""], ["Agnes", "Frederic", ""], ["Besacier", "Laurent", ""], ["Schwab", "Didier", ""]]}, {"id": "1704.01419", "submitter": "Kairit Sirts", "authors": "Avo Murom\\\"agi, Kairit Sirts, Sven Laur", "title": "Linear Ensembles of Word Embedding Models", "comments": "Nodalida 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores linear methods for combining several word embedding\nmodels into an ensemble. We construct the combined models using an iterative\nmethod based on either ordinary least squares regression or the solution to the\northogonal Procrustes problem.\n  We evaluate the proposed approaches on Estonian---a morphologically complex\nlanguage, for which the available corpora for training word embeddings are\nrelatively small. We compare both combined models with each other and with the\ninput word embedding models using synonym and analogy tests. The results show\nthat while using the ordinary least squares regression performs poorly in our\nexperiments, using orthogonal Procrustes to combine several word embedding\nmodels into an ensemble model leads to 7-10% relative improvements over the\nmean result of the initial models in synonym tests and 19-47% in analogy tests.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 13:38:01 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Murom\u00e4gi", "Avo", ""], ["Sirts", "Kairit", ""], ["Laur", "Sven", ""]]}, {"id": "1704.01444", "submitter": "Alec Radford", "authors": "Alec Radford, Rafal Jozefowicz, Ilya Sutskever", "title": "Learning to Generate Reviews and Discovering Sentiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the properties of byte-level recurrent language models. When given\nsufficient amounts of capacity, training data, and compute time, the\nrepresentations learned by these models include disentangled features\ncorresponding to high-level concepts. Specifically, we find a single unit which\nperforms sentiment analysis. These representations, learned in an unsupervised\nmanner, achieve state of the art on the binary subset of the Stanford Sentiment\nTreebank. They are also very data efficient. When using only a handful of\nlabeled examples, our approach matches the performance of strong baselines\ntrained on full datasets. We also demonstrate the sentiment unit has a direct\ninfluence on the generative process of the model. Simply fixing its value to be\npositive or negative generates samples with the corresponding positive or\nnegative sentiment.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 14:20:28 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 09:48:20 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Radford", "Alec", ""], ["Jozefowicz", "Rafal", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1704.01523", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\n  Neural Networks", "comments": "Accepted at SemEval 2017. The first two authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 50 million scholarly articles have been published: they constitute a\nunique repository of knowledge. In particular, one may infer from them\nrelations between scientific concepts, such as synonyms and hyponyms.\nArtificial neural networks have been recently explored for relation extraction.\nIn this work, we continue this line of work and present a system based on a\nconvolutional neural network to extract relations. Our model ranked first in\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\narticles (subtask C).\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:54:20 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1704.01599", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Wei Lu", "title": "Rhetorical relations for information retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, every part in most coherent text has some plausible reason for its\npresence, some function that it performs to the overall semantics of the text.\nRhetorical relations, e.g. contrast, cause, explanation, describe how the parts\nof a text are linked to each other. Knowledge about this socalled discourse\nstructure has been applied successfully to several natural language processing\ntasks. This work studies the use of rhetorical relations for Information\nRetrieval (IR): Is there a correlation between certain rhetorical relations and\nretrieval performance? Can knowledge about a document's rhetorical relations be\nuseful to IR? We present a language model modification that considers\nrhetorical relations when estimating the relevance of a document to a query.\nEmpirical evaluation of different versions of our model on TREC settings shows\nthat certain rhetorical relations can benefit retrieval effectiveness notably\n(> 10% in mean average precision over a state-of-the-art baseline).\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:21:39 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Lu", "Wei", ""]]}, {"id": "1704.01631", "submitter": "Shubham Toshniwal", "authors": "Shubham Toshniwal, Hao Tang, Liang Lu, Karen Livescu", "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder\n  Based Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end training of deep learning-based models allows for implicit\nlearning of intermediate representations based on the final task loss. However,\nthe end-to-end approach ignores the useful domain knowledge encoded in explicit\nintermediate-level supervision. We hypothesize that using intermediate\nrepresentations as auxiliary supervision at lower levels of deep networks may\nbe a good way of combining the advantages of end-to-end training and more\ntraditional pipeline approaches. We present experiments on conversational\nspeech recognition where we use lower-level tasks, such as phoneme recognition,\nin a multitask training approach with an encoder-decoder model for direct\ncharacter transcription. We compare multiple types of lower-level tasks and\nanalyze the effects of the auxiliary tasks. Our results on the Switchboard\ncorpus show that this approach improves recognition accuracy over a standard\nencoder-decoder model on the Eval2000 test set.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 19:44:23 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 16:01:53 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Toshniwal", "Shubham", ""], ["Tang", "Hao", ""], ["Lu", "Liang", ""], ["Livescu", "Karen", ""]]}, {"id": "1704.01653", "submitter": "Joseph Keshet", "authors": "Yaniv Sheena, M\\'i\\v{s}a Hejn\\'a, Yossi Adi, Joseph Keshet", "title": "Automatic Measurement of Pre-aspiration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-aspiration is defined as the period of glottal friction occurring in\nsequences of vocalic/consonantal sonorants and phonetically voiceless\nobstruents. We propose two machine learning methods for automatic measurement\nof pre-aspiration duration: a feedforward neural network, which works at the\nframe level; and a structured prediction model, which relies on manually\ndesigned feature functions, and works at the segment level. The input for both\nalgorithms is a speech signal of an arbitrary length containing a single\nobstruent, and the output is a pair of times which constitutes the\npre-aspiration boundaries. We train both models on a set of manually annotated\nexamples. Results suggest that the structured model is superior to the\nframe-based model as it yields higher accuracy in predicting the boundaries and\ngeneralizes to new speakers and new languages. Finally, we demonstrate the\napplicability of our structured prediction algorithm by replicating linguistic\nanalysis of pre-aspiration in Aberystwyth English with high correlation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 21:10:07 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 18:56:58 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 09:47:26 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Sheena", "Yaniv", ""], ["Hejn\u00e1", "M\u00ed\u0161a", ""], ["Adi", "Yossi", ""], ["Keshet", "Joseph", ""]]}, {"id": "1704.01691", "submitter": "Chunting Zhou", "authors": "Chunting Zhou and Graham Neubig", "title": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled\n  Sequence Transduction", "comments": "Accepted by ACL 2017", "journal-ref": "ACL 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled sequence transduction is a task of transforming one sequence into\nanother sequence that satisfies desiderata specified by a set of labels. In\nthis paper we propose multi-space variational encoder-decoders, a new model for\nlabeled sequence transduction with semi-supervised learning. The generative\nmodel can use neural networks to handle both discrete and continuous latent\nvariables to exploit various features of data. Experiments show that our model\nprovides not only a powerful supervised framework but also can effectively take\nadvantage of the unlabeled data. On the SIGMORPHON morphological inflection\nbenchmark, our model outperforms single-model state-of-art results by a large\nmargin for the majority of languages.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 02:36:56 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 03:22:23 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Zhou", "Chunting", ""], ["Neubig", "Graham", ""]]}, {"id": "1704.01696", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Graham Neubig", "title": "A Syntactic Neural Model for General-Purpose Code Generation", "comments": "To appear in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parsing natural language descriptions into source\ncode written in a general-purpose programming language like Python. Existing\ndata-driven methods treat this problem as a language generation task without\nconsidering the underlying syntax of the target programming language. Informed\nby previous work in semantic parsing, in this paper we propose a novel neural\narchitecture powered by a grammar model to explicitly capture the target syntax\nas prior knowledge. Experiments find this an effective way to scale up to\ngeneration of complex programs from natural language descriptions, achieving\nstate-of-the-art results that well outperform previous code generation and\nsemantic parsing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 03:13:46 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Yin", "Pengcheng", ""], ["Neubig", "Graham", ""]]}, {"id": "1704.01748", "submitter": "Lu\\'is Campos", "authors": "Lu\\'is Campos, Francisco Couto", "title": "MRA - Proof of Concept of a Multilingual Report Annotator Web\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MRA (Multilingual Report Annotator) is a web application that translates\nRadiology text and annotates it with RadLex terms. Its goal is to explore the\nsolution of translating non-English Radiology reports as a way to solve the\nproblem of most of the Text Mining tools being developed for English. In this\nbrief paper we explain the language barrier problem and shortly describe the\napplication. MRA can be found at https://github.com/lasigeBioTM/MRA .\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 08:32:16 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 08:16:57 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 13:15:48 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Campos", "Lu\u00eds", ""], ["Couto", "Francisco", ""]]}, {"id": "1704.01792", "submitter": "Qingyu Zhou", "authors": "Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, Ming Zhou", "title": "Neural Question Generation from Text: A Preliminary Study", "comments": "Submitted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic question generation aims to generate questions from a text passage\nwhere the generated questions can be answered by certain sub-spans of the given\npassage. Traditional methods mainly use rigid heuristic rules to transform a\nsentence into related questions. In this work, we propose to apply the neural\nencoder-decoder model to generate meaningful and diverse questions from natural\nlanguage sentences. The encoder reads the input text and the answer position,\nto produce an answer-aware input representation, which is fed to the decoder to\ngenerate an answer focused question. We conduct a preliminary study on neural\nquestion generation from text with the SQuAD dataset, and the experiment\nresults show that our method can produce fluent and diverse questions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 11:44:07 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 03:27:15 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 07:54:52 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Zhou", "Qingyu", ""], ["Yang", "Nan", ""], ["Wei", "Furu", ""], ["Tan", "Chuanqi", ""], ["Bao", "Hangbo", ""], ["Zhou", "Ming", ""]]}, {"id": "1704.01938", "submitter": "Oded Avraham", "authors": "Oded Avraham and Yoav Goldberg", "title": "The Interplay of Semantics and Morphology in Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the ability of word embeddings to capture both semantic and\nmorphological similarity, as affected by the different types of linguistic\nproperties (surface form, lemma, morphological tag) used to compose the\nrepresentation of each word. We train several models, where each uses a\ndifferent subset of these properties to compose its representations. By\nevaluating the models on semantic and morphological measures, we reveal some\nuseful insights on the relationship between semantics and morphology.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 17:07:40 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Avraham", "Oded", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1704.01975", "submitter": "Daniela Moctezuma", "authors": "Eric S. Tellez, Daniela Moctezuma, Sabino Miranda-J\\'imenez, Mario\n  Graff", "title": "An Automated Text Categorization Framework based on Hyperparameter\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great variety of text tasks such as topic or spam identification, user\nprofiling, and sentiment analysis can be posed as a supervised learning problem\nand tackle using a text classifier. A text classifier consists of several\nsubprocesses, some of them are general enough to be applied to any supervised\nlearning problem, whereas others are specifically designed to tackle a\nparticular task, using complex and computational expensive processes such as\nlemmatization, syntactic analysis, etc. Contrary to traditional approaches, we\npropose a minimalistic and wide system able to tackle text classification tasks\nindependent of domain and language, namely microTC. It is composed by some easy\nto implement text transformations, text representations, and a supervised\nlearning algorithm. These pieces produce a competitive classifier even in the\ndomain of informally written text. We provide a detailed description of microTC\nalong with an extensive experimental comparison with relevant state-of-the-art\nmethods. mircoTC was compared on 30 different datasets. Regarding accuracy,\nmicroTC obtained the best performance in 20 datasets while achieves competitive\nresults in the remaining 10. The compared datasets include several problems\nlike topic and polarity classification, spam detection, user profiling and\nauthorship attribution. Furthermore, it is important to state that our approach\nallows the usage of the technology even without knowledge of machine learning\nand natural language processing.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 18:01:22 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 22:30:13 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Tellez", "Eric S.", ""], ["Moctezuma", "Daniela", ""], ["Miranda-J\u00edmenez", "Sabino", ""], ["Graff", "Mario", ""]]}, {"id": "1704.02080", "submitter": "Vicky Zayats", "authors": "Vicky Zayats and Mari Ostendorf", "title": "Conversation Modeling on Reddit using a Graph-Structured LSTM", "comments": "Submitted to TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for modeling threaded discussions on\nsocial media using a graph-structured bidirectional LSTM which represents both\nhierarchical and temporal conversation structure. In experiments with a task of\npredicting popularity of comments in Reddit discussions, the proposed model\noutperforms a node-independent architecture for different sets of input\nfeatures. Analyses show a benefit to the model over the full course of the\ndiscussion, improving detection in both early and late stages. Further, the use\nof language cues with the bidirectional tree state updates helps with\nidentifying controversial comments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 03:27:54 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Zayats", "Vicky", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1704.02090", "submitter": "Xian-Ling Mao", "authors": "Yi-Kun Tang, Xian-Ling Mao, Heyan Huang, Guihua Wen", "title": "Conceptualization Topic Modeling", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, topic modeling has been widely used to discover the abstract topics\nin text corpora. Most of the existing topic models are based on the assumption\nof three-layer hierarchical Bayesian structure, i.e. each document is modeled\nas a probability distribution over topics, and each topic is a probability\ndistribution over words. However, the assumption is not optimal. Intuitively,\nit's more reasonable to assume that each topic is a probability distribution\nover concepts, and then each concept is a probability distribution over words,\ni.e. adding a latent concept layer between topic layer and word layer in\ntraditional three-layer assumption. In this paper, we verify the proposed\nassumption by incorporating the new assumption in two representative topic\nmodels, and obtain two novel topic models. Extensive experiments were conducted\namong the proposed models and corresponding baselines, and the results show\nthat the proposed models significantly outperform the baselines in terms of\ncase study and perplexity, which means the new assumption is more reasonable\nthan traditional one.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 05:12:38 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Tang", "Yi-Kun", ""], ["Mao", "Xian-Ling", ""], ["Huang", "Heyan", ""], ["Wen", "Guihua", ""]]}, {"id": "1704.02134", "submitter": "Nathan Schneider", "authors": "Nathan Schneider, Jena D. Hwang, Archna Bhatia, Vivek Srikumar, Na-Rae\n  Han, Tim O'Gorman, Sarah R. Moeller, Omri Abend, Adi Shalev, Austin Blodgett,\n  Jakob Prange", "title": "Adposition and Case Supersenses v2.5: Guidelines for English", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document offers a detailed linguistic description of SNACS (Semantic\nNetwork of Adposition and Case Supersenses; Schneider et al., 2018), an\ninventory of 50 semantic labels (\"supersenses\") that characterize the use of\nadpositions and case markers at a somewhat coarse level of granularity, as\ndemonstrated in the STREUSLE corpus (https://github.com/nert-gu/streusle/;\nversion 4.3 tracks guidelines version 2.5). Though the SNACS inventory aspires\nto be universal, this document is specific to English; documentation for other\nlanguages will be published separately.\n  Version 2 is a revision of the supersense inventory proposed for English by\nSchneider et al. (2015, 2016) (henceforth \"v1\"), which in turn was based on\nprevious schemes. The present inventory was developed after extensive review of\nthe v1 corpus annotations for English, plus previously unanalyzed genitive case\npossessives (Blodgett and Schneider, 2018), as well as consideration of\nadposition and case phenomena in Hebrew, Hindi, Korean, and German. Hwang et\nal. (2017) present the theoretical underpinnings of the v2 scheme. Schneider et\nal. (2018) summarize the scheme, its application to English corpus data, and an\nautomatic disambiguation task.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 08:42:45 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 03:59:48 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 03:44:11 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2019 22:07:40 GMT"}, {"version": "v5", "created": "Thu, 2 Jan 2020 03:55:21 GMT"}, {"version": "v6", "created": "Wed, 1 Apr 2020 03:44:48 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Schneider", "Nathan", ""], ["Hwang", "Jena D.", ""], ["Bhatia", "Archna", ""], ["Srikumar", "Vivek", ""], ["Han", "Na-Rae", ""], ["O'Gorman", "Tim", ""], ["Moeller", "Sarah R.", ""], ["Abend", "Omri", ""], ["Shalev", "Adi", ""], ["Blodgett", "Austin", ""], ["Prange", "Jakob", ""]]}, {"id": "1704.02156", "submitter": "Rik van Noord", "authors": "Rik van Noord and Johan Bos", "title": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural\n  Semantic Parsing", "comments": "To appear in Proceedings of SemEval, 2017 (camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate a semantic parser based on a character-based sequence-to-sequence\nmodel in the context of the SemEval-2017 shared task on semantic parsing for\nAMRs. With data augmentation, super characters, and POS-tagging we gain major\nimprovements in performance compared to a baseline character-level model.\nAlthough we improve on previous character-based neural semantic parsing models,\nthe overall accuracy is still lower than a state-of-the-art AMR parser. An\nensemble combining our neural semantic parser with an existing, traditional\nparser, yields a small gain in performance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 09:37:36 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 16:03:45 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["van Noord", "Rik", ""], ["Bos", "Johan", ""]]}, {"id": "1704.02215", "submitter": "Erik-L\\^an Do Dinh", "authors": "Steffen Eger, Erik-L\\^an Do Dinh, Ilia Kuznetsov, Masoud Kiaeeha,\n  Iryna Gurevych", "title": "EELECTION at SemEval-2017 Task 10: Ensemble of nEural Learners for\n  kEyphrase ClassificaTION", "comments": "In revision, changed to pdfTeX output", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach to the SemEval 2017 Task 10: \"Extracting\nKeyphrases and Relations from Scientific Publications\", specifically to Subtask\n(B): \"Classification of identified keyphrases\". We explored three different\ndeep learning approaches: a character-level convolutional neural network (CNN),\na stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM.\nFrom these approaches, we created an ensemble of differently\nhyper-parameterized systems, achieving a micro-F1-score of 0.63 on the test\ndata. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four\naccording to this official score. However, we erroneously trained 2 out of 3\nneural nets (the stacker and the CNN) on only roughly 15% of the full data,\nnamely, the original development set. When trained on the full data\n(training+development), our ensemble has a micro-F1-score of 0.69. Our code is\navailable from https://github.com/UKPLab/semeval2017-scienceie.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:07:15 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 10:31:49 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Eger", "Steffen", ""], ["Dinh", "Erik-L\u00e2n Do", ""], ["Kuznetsov", "Ilia", ""], ["Kiaeeha", "Masoud", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1704.02263", "submitter": "Edilson Anselmo Corr\\^ea J\\'unior", "authors": "Edilson A. Corr\\^ea Jr., Vanessa Queiroz Marinho, Leandro Borges dos\n  Santos", "title": "NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter\n  Sentiment Analysis", "comments": "Published in Proceedings of SemEval-2017, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our multi-view ensemble approach to SemEval-2017 Task 4\non Sentiment Analysis in Twitter, specifically, the Message Polarity\nClassification subtask for English (subtask A). Our system is a voting\nensemble, where each base classifier is trained in a different feature space.\nThe first space is a bag-of-words model and has a Linear SVM as base\nclassifier. The second and third spaces are two different strategies of\ncombining word embeddings to represent sentences and use a Linear SVM and a\nLogistic Regressor as base classifiers. The proposed system was ranked 18th out\nof 38 systems considering F1 score and 20th considering recall.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 15:27:10 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Corr\u00eaa", "Edilson A.", "Jr."], ["Marinho", "Vanessa Queiroz", ""], ["Santos", "Leandro Borges dos", ""]]}, {"id": "1704.02293", "submitter": "Didier Schwab", "authors": "Lo\\\"ic Vial and Andon Tchechmedjiev and Didier Schwab", "title": "Comparison of Global Algorithms in Word Sense Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article compares four probabilistic algorithms (global algorithms) for\nWord Sense Disambiguation (WSD) in terms of the number of scorer calls (local\nalgo- rithm) and the F1 score as determined by a gold-standard scorer. Two\nalgorithms come from the state of the art, a Simulated Annealing Algorithm\n(SAA) and a Genetic Algorithm (GA) as well as two algorithms that we first\nadapt from WSD that are state of the art probabilistic search algorithms,\nnamely a Cuckoo search algorithm (CSA) and a Bat Search algorithm (BS). As WSD\nrequires to evaluate exponentially many word sense combinations (with branching\nfactors of up to 6 or more), probabilistic algorithms allow to find approximate\nsolution in a tractable time by sampling the search space. We find that CSA, GA\nand SA all eventually converge to similar results (0.98 F1 score), but CSA gets\nthere faster (in fewer scorer calls) and reaches up to 0.95 F1 before SA in\nfewer scorer calls. In BA a strict convergence criterion prevents it from\nreaching above 0.89 F1.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:04:51 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Vial", "Lo\u00efc", ""], ["Tchechmedjiev", "Andon", ""], ["Schwab", "Didier", ""]]}, {"id": "1704.02298", "submitter": "Rose Catherine", "authors": "Rose Catherine, William Cohen", "title": "TransNets: Learning to Transform for Recommendation", "comments": "Accepted for publication in the 11th ACM Conference on Recommender\n  Systems (RecSys 2017)", "journal-ref": null, "doi": "10.1145/3109859.3109878", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning methods have been shown to improve the performance of\nrecommender systems over traditional methods, especially when review text is\navailable. For example, a recent model, DeepCoNN, uses neural nets to learn one\nlatent representation for the text of all reviews written by a target user, and\na second latent representation for the text of all reviews for a target item,\nand then combines these latent representations to obtain state-of-the-art\nperformance on recommendation tasks. We show that (unsurprisingly) much of the\npredictive value of review text comes from reviews of the target user for the\ntarget item. We then introduce a way in which this information can be used in\nrecommendation, even when the target user's review for the target item is not\navailable. Our model, called TransNets, extends the DeepCoNN model by\nintroducing an additional latent layer representing the target user-target item\npair. We then regularize this layer, at training time, to be similar to another\nlatent representation of the target user's review of the target item. We show\nthat TransNets and extensions of it improve substantially over the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:13:03 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 15:14:22 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Catherine", "Rose", ""], ["Cohen", "William", ""]]}, {"id": "1704.02312", "submitter": "Yaoyuan Zhang", "authors": "Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan Zhao, Rui Yan", "title": "A Constrained Sequence-to-Sequence Neural Model for Sentence\n  Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence simplification reduces semantic complexity to benefit people with\nlanguage impairments. Previous simplification studies on the sentence level and\nword level have achieved promising results but also meet great challenges. For\nsentence-level studies, sentences after simplification are fluent but sometimes\nare not really simplified. For word-level studies, words are simplified but\nalso have potential grammar errors due to different usages of words before and\nafter simplification. In this paper, we propose a two-step simplification\nframework by combining both the word-level and the sentence-level\nsimplifications, making use of their corresponding advantages. Based on the\ntwo-step framework, we implement a novel constrained neural generation model to\nsimplify sentences given simplified words. The final results on Wikipedia and\nSimple Wikipedia aligned datasets indicate that our method yields better\nperformance than various baselines.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:53:24 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Zhang", "Yaoyuan", ""], ["Ye", "Zhenxu", ""], ["Feng", "Yansong", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1704.02360", "submitter": "Yuki Saito", "authors": "Hiroyuki Miyoshi, Yuki Saito, Shinnosuke Takamichi, and Hiroshi\n  Saruwatari", "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context\n  Posterior Probabilities", "comments": "Accepted to INTERSPEECH 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice conversion (VC) using sequence-to-sequence learning of context\nposterior probabilities is proposed. Conventional VC using shared context\nposterior probabilities predicts target speech parameters from the context\nposterior probabilities estimated from the source speech parameters. Although\nconventional VC can be built from non-parallel data, it is difficult to convert\nspeaker individuality such as phonetic property and speaking rate contained in\nthe posterior probabilities because the source posterior probabilities are\ndirectly used for predicting target speech parameters. In this work, we assume\nthat the training data partly include parallel speech data and propose\nsequence-to-sequence learning between the source and target posterior\nprobabilities. The conversion models perform non-linear and variable-length\ntransformation from the source probability sequence to the target one. Further,\nwe propose a joint training algorithm for the modules. In contrast to\nconventional VC, which separately trains the speech recognition that estimates\nposterior probabilities and the speech synthesis that predicts target speech\nparameters, our proposed method jointly trains these modules along with the\nproposed probability conversion modules. Experimental results demonstrate that\nour approach outperforms the conventional VC.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 12:35:33 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 04:43:37 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 08:11:02 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 02:42:01 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Miyoshi", "Hiroyuki", ""], ["Saito", "Yuki", ""], ["Takamichi", "Shinnosuke", ""], ["Saruwatari", "Hiroshi", ""]]}, {"id": "1704.02362", "submitter": "Zhe Liu", "authors": "Zhe Liu, Anbang Xu, Mengdi Zhang, Jalal Mahmud and Vibha Sinha", "title": "Fostering User Engagement: Rhetorical Devices for Applause Generation\n  Learnt from TED Talks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One problem that every presenter faces when delivering a public discourse is\nhow to hold the listeners' attentions or to keep them involved. Therefore, many\nstudies in conversation analysis work on this issue and suggest qualitatively\ncon-structions that can effectively lead to audience's applause. To investigate\nthese proposals quantitatively, in this study we an-alyze the transcripts of\n2,135 TED Talks, with a particular fo-cus on the rhetorical devices that are\nused by the presenters for applause elicitation. Through conducting regression\nanal-ysis, we identify and interpret 24 rhetorical devices as triggers of\naudience applauding. We further build models that can rec-ognize\napplause-evoking sentences and conclude this work with potential implications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 15:57:45 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 07:39:44 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Liu", "Zhe", ""], ["Xu", "Anbang", ""], ["Zhang", "Mengdi", ""], ["Mahmud", "Jalal", ""], ["Sinha", "Vibha", ""]]}, {"id": "1704.02385", "submitter": "Luis Gerardo Mojica de la Vega", "authors": "Luis Gerardo Mojica", "title": "A Trolling Hierarchy in Social Media and A Conditional Random Field For\n  Trolling Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An-ever increasing number of social media websites, electronic newspapers and\nInternet forums allow visitors to leave comments for others to read and\ninteract. This exchange is not free from participants with malicious\nintentions, which do not contribute with the written conversation. Among\ndifferent communities users adopt strategies to handle such users. In this\npaper we present a comprehensive categorization of the trolling phenomena\nresource, inspired by politeness research and propose a model that jointly\npredicts four crucial aspects of trolling: intention, interpretation, intention\ndisclosure and response strategy. Finally, we present a new annotated dataset\ncontaining excerpts of conversations involving trolls and the interactions with\nother users that we hope will be a useful resource for the research community.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 22:08:39 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Mojica", "Luis Gerardo", ""]]}, {"id": "1704.02497", "submitter": "Steffen Eger", "authors": "Steffen Eger and Alexander Mehler", "title": "On the Linearity of Semantic Change: Investigating Meaning Variation via\n  Dynamic Graph Models", "comments": "Published at ACL 2016, Berlin (short papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two graph models of semantic change. The first is a time-series\nmodel that relates embedding vectors from one time period to embedding vectors\nof previous time periods. In the second, we construct one graph for each word:\nnodes in this graph correspond to time points and edge weights to the\nsimilarity of the word's meaning across two time points. We apply our two\nmodels to corpora across three different languages. We find that semantic\nchange is linear in two senses. Firstly, today's embedding vectors (= meaning)\nof words can be derived as linear combinations of embedding vectors of their\nneighbors in previous time periods. Secondly, self-similarity of words decays\nlinearly in time. We consider both findings as new laws/hypotheses of semantic\nchange.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 13:56:55 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Eger", "Steffen", ""], ["Mehler", "Alexander", ""]]}, {"id": "1704.02565", "submitter": "Dafydd Gibbon", "authors": "Dafydd Gibbon", "title": "Prosody: The Rhythms and Melodies of Speech", "comments": "35 pages, 22 figures (2nd version at arxiv.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present contribution is a tutorial on selected aspects of prosody, the\nrhythms and melodies of speech, based on a course of the same name at the\nSummer School on Contemporary Phonetics and Phonology at Tongji University,\nShanghai, China in July 2016. The tutorial is not intended as an introduction\nto experimental methodology or as an overview of the literature on the topic,\nbut as an outline of observationally accessible aspects of fundamental\nfrequency and timing patterns with the aid of computational visualisation,\nsituated in a semiotic framework of sign ranks and interpretations. After an\ninformal introduction to the basic concepts of prosody in the introduction and\na discussion of the place of prosody in the architecture of language, a\nselection of acoustic phonetic topics in phonemic tone and accent prosody, word\nprosody, phrasal prosody and discourse prosody are discussed, and a stylisation\nmethod for visualising aspects of prosody is introduced. Examples are taken\nfrom a number of typologically different languages: Anyi/Agni (Niger-Congo>Kwa,\nIvory Coast), English, Kuki-Thadou (Sino-Tibetan, North-East India and\nMyanmar), Mandarin Chinese, Tem (Niger-Congo>Gur, Togo) and Farsi. The main\nfocus is on fundamental frequency patterns, but issues of timing and rhythm are\nalso discussed. In the final section, further reading and possible future\nresearch directions are outlined.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 06:17:08 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 14:39:37 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Gibbon", "Dafydd", ""]]}, {"id": "1704.02686", "submitter": "Shuchin Aeron", "authors": "Eric Bailey and Shuchin Aeron", "title": "Word Embeddings via Tensor Factorization", "comments": "More simulation results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most popular word embedding techniques involve implicit or explicit\nfactorization of a word co-occurrence based matrix into low rank factors. In\nthis paper, we aim to generalize this trend by using numerical methods to\nfactor higher-order word co-occurrence based arrays, or \\textit{tensors}. We\npresent four word embeddings using tensor factorization and analyze their\nadvantages and disadvantages. One of our main contributions is a novel joint\nsymmetric tensor factorization technique related to the idea of coupled tensor\nfactorization. We show that embeddings based on tensor factorization can be\nused to discern the various meanings of polysemous words without being\nexplicitly trained to do so, and motivate the intuition behind why this works\nin a way that doesn't with existing methods. We also modify an existing word\nembedding evaluation metric known as Outlier Detection [Camacho-Collados and\nNavigli, 2016] to evaluate the quality of the order-$N$ relations that a word\nembedding captures, and show that tensor-based methods outperform existing\nmatrix-based methods at this task. Experimentally, we show that all of our word\nembeddings either outperform or are competitive with state-of-the-art baselines\ncommonly used today on a variety of recent datasets. Suggested applications of\ntensor factorization-based word embeddings are given, and all source code and\npre-trained vectors are publicly available online.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 02:24:37 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 18:56:30 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bailey", "Eric", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1704.02709", "submitter": "Quynh Ngoc Thi Do", "authors": "Quynh Ngoc Thi Do, Steven Bethard, Marie-Francine Moens", "title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame\n  Arguments", "comments": "IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit semantic role labeling (iSRL) is the task of predicting the semantic\nroles of a predicate that do not appear as explicit arguments, but rather\nregard common sense knowledge or are mentioned earlier in the discourse. We\nintroduce an approach to iSRL based on a predictive recurrent neural semantic\nframe model (PRNSFM) that uses a large unannotated corpus to learn the\nprobability of a sequence of semantic arguments given a predicate. We leverage\nthe sequence probabilities predicted by the PRNSFM to estimate selectional\npreferences for predicates and their arguments. On the NomBank iSRL test set,\nour approach improves state-of-the-art performance on implicit semantic role\nlabeling with less reliance than prior work on manually constructed language\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 04:48:53 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 10:32:17 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Do", "Quynh Ngoc Thi", ""], ["Bethard", "Steven", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1704.02788", "submitter": "Chuanqi Tan", "authors": "Chuanqi Tan, Furu Wei, Pengjie Ren, Weifeng Lv, Ming Zhou", "title": "Entity Linking for Queries by Searching Wikipedia Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective approach for linking entities in queries.\nThe key idea is to search sentences similar to a query from Wikipedia articles\nand directly use the human-annotated entities in the similar sentences as\ncandidate entities for the query. Then, we employ a rich set of features, such\nas link-probability, context-matching, word embeddings, and relatedness among\ncandidate entities as well as their related entities, to rank the candidates\nunder a regression based framework. The advantages of our approach lie in two\naspects, which contribute to the ranking process and final linking result.\nFirst, it can greatly reduce the number of candidate entities by filtering out\nirrelevant entities with the words in the query. Second, we can obtain the\nquery sensitive prior probability in addition to the static link-probability\nderived from all Wikipedia articles. We conduct experiments on two benchmark\ndatasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ\ndataset. Experimental results show that our method outperforms state-of-the-art\nsystems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 10:19:53 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 06:59:56 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 08:03:49 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Tan", "Chuanqi", ""], ["Wei", "Furu", ""], ["Ren", "Pengjie", ""], ["Lv", "Weifeng", ""], ["Zhou", "Ming", ""]]}, {"id": "1704.02813", "submitter": "Lyan Verwimp", "authors": "Lyan Verwimp, Joris Pelemans, Hugo Van hamme and Patrick Wambacq", "title": "Character-Word LSTM Language Models", "comments": null, "journal-ref": "European Chapter of the Association for Computational Linguistics\n  (EACL) 2017, Valencia, Spain, pp. 417-427", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Character-Word Long Short-Term Memory Language Model which both\nreduces the perplexity with respect to a baseline word-level language model and\nreduces the number of parameters of the model. Character information can reveal\nstructural (dis)similarities between words and can even be used when a word is\nout-of-vocabulary, thus improving the modeling of infrequent and unknown words.\nBy concatenating word and character embeddings, we achieve up to 2.77% relative\nimprovement on English compared to a baseline model with a similar amount of\nparameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level\nmodels with a larger number of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 11:42:09 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Verwimp", "Lyan", ""], ["Pelemans", "Joris", ""], ["Van hamme", "Hugo", ""], ["Wambacq", "Patrick", ""]]}, {"id": "1704.02841", "submitter": "Maria Chiara Caschera Dr.", "authors": "Maria Chiara Caschera, Fernando Ferri, Patrizia Grifoni", "title": "From Modal to Multimodal Ambiguities: a Classification Approach", "comments": "23 pages", "journal-ref": "JNIT (Journal of Next Generation Information Technology), Volume 4\n  Issue 5, July, 2013,Pages 87-109, ISSN 2092-8637. GlobalCIS (Convergence\n  Information Society, Republic of Korea)", "doi": "10.4156/jnit.vol4.issue5.10", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with classifying ambiguities for Multimodal Languages. It\nevolves the classifications and the methods of the literature on ambiguities\nfor Natural Language and Visual Language, empirically defining an original\nclassification of ambiguities for multimodal interaction using a linguistic\nperspective. This classification distinguishes between Semantic and Syntactic\nmultimodal ambiguities and their subclasses, which are intercepted using a\nrule-based method implemented in a software module. The experimental results\nhave achieved an accuracy of the obtained classification compared to the\nexpected one, which are defined by the human judgment, of 94.6% for the\nsemantic ambiguities classes, and 92.1% for the syntactic ambiguities classes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 12:06:51 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Caschera", "Maria Chiara", ""], ["Ferri", "Fernando", ""], ["Grifoni", "Patrizia", ""]]}, {"id": "1704.02853", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman,\n  Andrew McCallum", "title": "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations\n  from Scientific Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the SemEval task of extracting keyphrases and relations between\nthem from scientific documents, which is crucial for understanding which\npublications describe which processes, tasks and materials. Although this was a\nnew task, we had a total of 26 submissions across 3 evaluation scenarios. We\nexpect the task and the findings reported in this paper to be relevant for\nresearchers working on understanding scientific content, as well as the broader\nknowledge base population and information extraction communities.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 13:43:40 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 10:41:31 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 15:32:41 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Das", "Mrinal", ""], ["Riedel", "Sebastian", ""], ["Vikraman", "Lakshmi", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.02923", "submitter": "Sandro Pezzelle", "authors": "Ionut Sorodoc, Sandro Pezzelle, Aur\\'elie Herbelot, Mariella\n  Dimiccoli, Raffaella Bernardi", "title": "Pay Attention to Those Sets! Learning Quantification from Images", "comments": "Submitted to Journal Paper, 28 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major advances have recently been made in merging language and vision\nrepresentations. But most tasks considered so far have confined themselves to\nthe processing of objects and lexicalised relations amongst objects (content\nwords). We know, however, that humans (even pre-school children) can abstract\nover raw data to perform certain types of higher-level reasoning, expressed in\nnatural language by function words. A case in point is given by their ability\nto learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From\nformal semantics and cognitive linguistics, we know that quantifiers are\nrelations over sets which, as a simplification, we can see as proportions. For\ninstance, in 'most fish are red', most encodes the proportion of fish which are\nred fish. In this paper, we study how well current language and vision\nstrategies model such relations. We show that state-of-the-art attention\nmechanisms coupled with a traditional linguistic formalisation of quantifiers\ngives best performance on the task. Additionally, we provide insights on the\nrole of 'gist' representations in quantification. A 'logical' strategy to\ntackle the task would be to first obtain a numerosity estimation for the two\ninvolved sets and then compare their cardinalities. We however argue that\nprecisely identifying the composition of the sets is not only beyond current\nstate-of-the-art models but perhaps even detrimental to a task that is most\nefficiently performed by refining the approximate numerosity estimator of the\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 16:03:31 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Sorodoc", "Ionut", ""], ["Pezzelle", "Sandro", ""], ["Herbelot", "Aur\u00e9lie", ""], ["Dimiccoli", "Mariella", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1704.02963", "submitter": "Thales Felipe Costa Bertaglia", "authors": "Thales Felipe Costa Bertaglia and Maria das Gra\\c{c}as Volpe Nunes", "title": "Exploring Word Embeddings for Unsupervised Textual User-Generated\n  Content Normalization", "comments": "Published in Proceedings of the 2nd Workshop on Noisy User-generated\n  Text, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text normalization techniques based on rules, lexicons or supervised training\nrequiring large corpora are not scalable nor domain interchangeable, and this\nmakes them unsuitable for normalizing user-generated content (UGC). Current\ntools available for Brazilian Portuguese make use of such techniques. In this\nwork we propose a technique based on distributed representation of words (or\nword embeddings). It generates continuous numeric vectors of\nhigh-dimensionality to represent words. The vectors explicitly encode many\nlinguistic regularities and patterns, as well as syntactic and semantic word\nrelationships. Words that share semantic similarity are represented by similar\nvectors. Based on these features, we present a totally unsupervised, expandable\nand language and domain independent method for learning normalization lexicons\nfrom word embeddings. Our approach obtains high correction rate of orthographic\nerrors and internet slang in product reviews, outperforming the current\navailable tools for Brazilian Portuguese.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 17:37:22 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Bertaglia", "Thales Felipe Costa", ""], ["Nunes", "Maria das Gra\u00e7as Volpe", ""]]}, {"id": "1704.03013", "submitter": "Nathan Hartmann", "authors": "Nathan Siegle Hartmann and Livia Cucatto and Danielle Brants and\n  Sandra Alu\\'isio", "title": "Automatic Classification of the Complexity of Nonfiction Texts in\n  Portuguese for Early School Years", "comments": "PROPOR International Conference on the Computational Processing of\n  Portuguese, 2016, 9 pages", "journal-ref": "Hartmann N., Cucatto L., Brants D., Alu\\'isio S. (2016) Automatic\n  Classification of the Complexity of Nonfiction Texts in Portuguese for Early\n  School Years. In: Computational Processing of the Portuguese Language. PROPOR\n  2016. Springer", "doi": "10.1007/978-3-319-41552-9_2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that most Brazilian students have serious problems\nregarding their reading skills. The full development of this skill is key for\nthe academic and professional future of every citizen. Tools for classifying\nthe complexity of reading materials for children aim to improve the quality of\nthe model of teaching reading and text comprehension. For English, Fengs work\n[11] is considered the state-of-art in grade level prediction and achieved 74%\nof accuracy in automatically classifying 4 levels of textual complexity for\nclose school grades. There are no classifiers for nonfiction texts for close\ngrades in Portuguese. In this article, we propose a scheme for manual\nannotation of texts in 5 grade levels, which will be used for customized\nreading to avoid the lack of interest by students who are more advanced in\nreading and the blocking of those that still need to make further progress. We\nobtained 52% of accuracy in classifying texts into 5 levels and 74% in 3\nlevels. The results prove to be promising when compared to the state-of-art\nwork.9\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:44:08 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Hartmann", "Nathan Siegle", ""], ["Cucatto", "Livia", ""], ["Brants", "Danielle", ""], ["Alu\u00edsio", "Sandra", ""]]}, {"id": "1704.03016", "submitter": "Nathan Hartmann", "authors": "Nathan Siegle Hartmann and Magali Sanches Duran and Sandra Maria\n  Alu\\'isio", "title": "Automatic semantic role labeling on non-revised syntactic trees of\n  journalistic texts", "comments": "PROPOR International Conference on the Computational Processing of\n  Portuguese, 2016, 8 pages", "journal-ref": "PROPOR 2016. Springer. Lecture Notes in Computer Science volume\n  9727 (2016) pgs. 202-212", "doi": "10.1007/978-3-319-41552-9_20", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Role Labeling (SRL) is a Natural Language Processing task that\nenables the detection of events described in sentences and the participants of\nthese events. For Brazilian Portuguese (BP), there are two studies recently\nconcluded that perform SRL in journalistic texts. [1] obtained F1-measure\nscores of 79.6, using the PropBank.Br corpus, which has syntactic trees\nmanually revised, [8], without using a treebank for training, obtained\nF1-measure scores of 68.0 for the same corpus. However, the use of manually\nrevised syntactic trees for this task does not represent a real scenario of\napplication. The goal of this paper is to evaluate the performance of SRL on\nrevised and non-revised syntactic trees using a larger and balanced corpus of\nBP journalistic texts. First, we have shown that [1]'s system also performs\nbetter than [8]'s system on the larger corpus. Second, the SRL system trained\non non-revised syntactic trees performs better over non-revised trees than a\nsystem trained on gold-standard data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 19:02:18 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Hartmann", "Nathan Siegle", ""], ["Duran", "Magali Sanches", ""], ["Alu\u00edsio", "Sandra Maria", ""]]}, {"id": "1704.03084", "submitter": "Xiujun Li", "authors": "Baolin Peng and Xiujun Li and Lihong Li and Jianfeng Gao and Asli\n  Celikyilmaz and Sungjin Lee and Kam-Fai Wong", "title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep\n  Reinforcement Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a dialogue agent to fulfill complex tasks, such as travel planning,\nis challenging because the agent has to learn to collectively complete multiple\nsubtasks. For example, the agent needs to reserve a hotel and book a flight so\nthat there leaves enough time for commute between arrival and hotel check-in.\nThis paper addresses this challenge by formulating the task in the mathematical\nframework of options over Markov Decision Processes (MDPs), and proposing a\nhierarchical deep reinforcement learning approach to learning a dialogue\nmanager that operates at different temporal scales. The dialogue manager\nconsists of: (1) a top-level dialogue policy that selects among subtasks or\noptions, (2) a low-level dialogue policy that selects primitive actions to\ncomplete the subtask given by the top-level policy, and (3) a global state\ntracker that helps ensure all cross-subtask constraints be satisfied.\nExperiments on a travel planning task with simulated and real users show that\nour approach leads to significant improvements over three baselines, two based\non handcrafted rules and the other based on flat deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 23:24:46 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 19:36:30 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 22:23:53 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Li", "Lihong", ""], ["Gao", "Jianfeng", ""], ["Celikyilmaz", "Asli", ""], ["Lee", "Sungjin", ""], ["Wong", "Kam-Fai", ""]]}, {"id": "1704.03169", "submitter": "Raphael Shu", "authors": "Raphael Shu and Hideki Nakayama", "title": "Later-stage Minimum Bayes-Risk Decoding for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For extended periods of time, sequence generation models rely on beam search\nalgorithm to generate output sequence. However, the correctness of beam search\ndegrades when the a model is over-confident about a suboptimal prediction. In\nthis paper, we propose to perform minimum Bayes-risk (MBR) decoding for some\nextra steps at a later stage. In order to speed up MBR decoding, we compute the\nBayes risks on GPU in batch mode. In our experiments, we found that MBR\nreranking works with a large beam size. Later-stage MBR decoding is shown to\noutperform simple MBR reranking in machine translation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 06:48:45 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 08:02:00 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Shu", "Raphael", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1704.03223", "submitter": "Zahra Mousavi", "authors": "Zahra Mousavi, Heshaam Faili", "title": "Persian Wordnet Construction using Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated supervised method for Persian wordnet\nconstruction. Using a Persian corpus and a bi-lingual dictionary, the initial\nlinks between Persian words and Princeton WordNet synsets have been generated.\nThese links will be discriminated later as correct or incorrect by employing\nseven features in a trained classification system. The whole method is just a\nclassification system, which has been trained on a train set containing FarsNet\nas a set of correct instances. State of the art results on the automatically\nderived Persian wordnet is achieved. The resulted wordnet with a precision of\n91.18% includes more than 16,000 words and 22,000 synsets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 09:47:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Mousavi", "Zahra", ""], ["Faili", "Heshaam", ""]]}, {"id": "1704.03242", "submitter": "Santosh Kumar Bharti", "authors": "Santosh Kumar Bharti and Korra Sathya Babu", "title": "Automatic Keyword Extraction for Text Summarization: A Survey", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, data is growing rapidly in every domain such as news, social\nmedia, banking, education, etc. Due to the excessiveness of data, there is a\nneed of automatic summarizer which will be capable to summarize the data\nespecially textual data in original document without losing any critical\npurposes. Text summarization is emerged as an important research area in recent\npast. In this regard, review of existing work on text summarization process is\nuseful for carrying out further research. In this paper, recent literature on\nautomatic keyword extraction and text summarization are presented since text\nsummarization process is highly depend on keyword extraction. This literature\nincludes the discussion about different methodology used for keyword extraction\nand text summarization. It also discusses about different databases used for\ntext summarization in several domains along with evaluation matrices. Finally,\nit discusses briefly about issues and research challenges faced by researchers\nalong with future direction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 11:20:19 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Bharti", "Santosh Kumar", ""], ["Babu", "Korra Sathya", ""]]}, {"id": "1704.03279", "submitter": "Felix Stahlberg", "authors": "Felix Stahlberg and Bill Byrne", "title": "Unfolding and Shrinking Neural Machine Translation Ensembles", "comments": "Accepted at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling is a well-known technique in neural machine translation (NMT) to\nimprove system performance. Instead of a single neural net, multiple neural\nnets with the same topology are trained separately, and the decoder generates\npredictions by averaging over the individual models. Ensembling often improves\nthe quality of the generated translations drastically. However, it is not\nsuitable for production systems because it is cumbersome and slow. This work\naims to reduce the runtime to be on par with a single system without\ncompromising the translation quality. First, we show that the ensemble can be\nunfolded into a single large neural network which imitates the output of the\nensemble system. We show that unfolding can already improve the runtime in\npractice since more work can be done on the GPU. We proceed by describing a set\nof techniques to shrink the unfolded network by reducing the dimensionality of\nlayers. On Japanese-English we report that the resulting network has the size\nand decoding speed of a single NMT network but performs on the level of a\n3-ensemble system.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 13:27:00 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 13:04:22 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Stahlberg", "Felix", ""], ["Byrne", "Bill", ""]]}, {"id": "1704.03407", "submitter": "Hwiyeol Jo", "authors": "Hwiyeol Jo, Soo-Min Kim, Jeong Ryu", "title": "What we really want to find by Sentiment Analysis: The Relationship\n  between Computational Models and Psychological State", "comments": "Paper version of \"Psychological State in Text: A Limitation of\n  Sentiment Analysis.\". arXiv admin note: text overlap with arXiv:1607.03707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first step to model emotional state of a person, we build sentiment\nanalysis models with existing deep neural network algorithms and compare the\nmodels with psychological measurements to enlighten the relationship. In the\nexperiments, we first examined psychological state of 64 participants and asked\nthem to summarize the story of a book, Chronicle of a Death Foretold (Marquez,\n1981). Secondly, we trained models using crawled 365,802 movie review data;\nthen we evaluated participants' summaries using the pretrained model as a\nconcept of transfer learning. With the background that emotion affects on\nmemories, we investigated the relationship between the evaluation score of the\nsummaries from computational models and the examined psychological\nmeasurements. The result shows that although CNN performed the best among other\ndeep neural network algorithms (LSTM, GRU), its results are not related to the\npsychological state. Rather, GRU shows more explainable results depending on\nthe psychological state. The contribution of this paper can be summarized as\nfollows: (1) we enlighten the relationship between computational models and\npsychological measurements. (2) we suggest this framework as objective methods\nto evaluate the emotion; the real sentiment analysis of a person.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:42:55 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 08:53:42 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Jo", "Hwiyeol", ""], ["Kim", "Soo-Min", ""], ["Ryu", "Jeong", ""]]}, {"id": "1704.03471", "submitter": "Yonatan Belinkov", "authors": "Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James\n  Glass", "title": "What do Neural Machine Translation Models Learn about Morphology?", "comments": "Updated decoder experiments", "journal-ref": "ACL 55 (2017), volume 1, 861-872", "doi": "10.18653/v1/P17-1080", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (MT) models obtain state-of-the-art performance\nwhile maintaining a simple, end-to-end architecture. However, little is known\nabout what these models learn about source and target languages during the\ntraining process. In this work, we analyze the representations learned by\nneural MT models at various levels of granularity and empirically evaluate the\nquality of the representations for learning morphology through extrinsic\npart-of-speech and morphological tagging tasks. We conduct a thorough\ninvestigation along several parameters: word-based vs. character-based\nrepresentations, depth of the encoding layer, the identity of the target\nlanguage, and encoder vs. decoder representations. Our data-driven,\nquantitative evaluation sheds light on important aspects in the neural MT\nsystem and its ability to capture word structure.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:01:07 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 13:38:20 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 14:27:40 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Belinkov", "Yonatan", ""], ["Durrani", "Nadir", ""], ["Dalvi", "Fahim", ""], ["Sajjad", "Hassan", ""], ["Glass", "James", ""]]}, {"id": "1704.03520", "submitter": "Niek Tax", "authors": "Felix Mannhardt and Niek Tax", "title": "Unsupervised Event Abstraction using Pattern Abstraction and Local\n  Process Models", "comments": "Accepted at Enabling Business Transformation by Business Process\n  Modeling, Development, and Support Working Conference 2017 (BPMDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining analyzes business processes based on events stored in event\nlogs. However, some recorded events may correspond to activities on a very low\nlevel of abstraction. When events are recorded on a too low level of\ngranularity, process discovery methods tend to generate overgeneralizing\nprocess models. Grouping low-level events to higher level activities, i.e.,\nevent abstraction, can be used to discover better process models. Existing\nevent abstraction methods are mainly based on common sub-sequences and\nclustering techniques. In this paper, we propose to first discover local\nprocess models and then use those models to lift the event log to a higher\nlevel of abstraction. Our conjecture is that process models discovered on the\nobtained high-level event log return process models of higher quality: their\nfitness and precision scores are more balanced. We show this with preliminary\nresults on several real-life event logs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 20:08:14 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 16:48:18 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Mannhardt", "Felix", ""], ["Tax", "Niek", ""]]}, {"id": "1704.03543", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse\n  Vectors", "comments": "Related datasets can be found at http://allenai.org/data.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While open-domain question answering (QA) systems have proven effective for\nanswering simple questions, they struggle with more complex questions. Our goal\nis to answer more complex questions reliably, without incurring a significant\ncost in knowledge resource construction to support the QA. One readily\navailable knowledge resource is a term bank, enumerating the key concepts in a\ndomain. We have developed an unsupervised learning approach that leverages a\nterm bank to guide a QA system, by representing the terminological knowledge\nwith thousands of specialized vector spaces. In experiments with complex\nscience questions, we show that this approach significantly outperforms several\nstate-of-the-art QA systems, demonstrating that significant leverage can be\ngained from continuous vector representations of domain terminology.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 21:21:39 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1704.03560", "submitter": "Robyn Speer", "authors": "Robyn Speer and Joanna Lowry-Duda", "title": "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with\n  Multilingual Relational Knowledge", "comments": "5 pages, accepted to the SemEval workshop at ACL 2017", "journal-ref": "Proceedings of the 11th International Workshop on Semantic\n  Evaluation (SemEval-2017), p. 85-89", "doi": "10.18653/v1/S17-2008", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Luminoso's participation in SemEval 2017 Task 2,\n\"Multilingual and Cross-lingual Semantic Word Similarity\", with a system based\non ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses\non general knowledge that relates the meanings of words and phrases. Our\nsubmission to SemEval was an update of previous work that builds high-quality,\nmultilingual word embeddings from a combination of ConceptNet and\ndistributional semantics. Our system took first place in both subtasks. It\nranked first in 4 out of 5 of the separate languages, and also ranked first in\nall 10 of the cross-lingual language pairs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 22:44:35 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 18:32:53 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Speer", "Robyn", ""], ["Lowry-Duda", "Joanna", ""]]}, {"id": "1704.03617", "submitter": "Matthew Riemer", "authors": "Matthew Riemer, Elham Khabiri, and Richard Goodwin", "title": "Representation Stability as a Regularizer for Improved Text Analytics\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural networks are well suited for sequential transfer learning\ntasks, the catastrophic forgetting problem hinders proper integration of prior\nknowledge. In this work, we propose a solution to this problem by using a\nmulti-task objective based on the idea of distillation and a mechanism that\ndirectly penalizes forgetting at the shared representation layer during the\nknowledge integration phase of training. We demonstrate our approach on a\nTwitter domain sentiment analysis task with sequential knowledge transfer from\nfour related tasks. We show that our technique outperforms networks fine-tuned\nto the target task. Additionally, we show both through empirical evidence and\nexamples that it does not forget useful knowledge from the source task that is\nforgotten during standard fine-tuning. Surprisingly, we find that first\ndistilling a human made rule based sentiment engine into a recurrent neural\nnetwork and then integrating the knowledge with the target task data leads to a\nsubstantial gain in generalization performance. Our experiments demonstrate the\npower of multi-source transfer techniques in practical text analytics problems\nwhen paired with distillation. In particular, for the SemEval 2016 Task 4\nSubtask A (Nakov et al., 2016) dataset we surpass the state of the art\nestablished during the competition with a comparatively simple model\narchitecture that is not even competitive when trained on only the labeled task\nspecific data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 04:38:18 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Riemer", "Matthew", ""], ["Khabiri", "Elham", ""], ["Goodwin", "Richard", ""]]}, {"id": "1704.03627", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang, Yun-Nung Chen, Jeffrey P. Bigham", "title": "Real-time On-Demand Crowd-powered Entity Extraction", "comments": "Accepted by the 5th Edition Of The Collective Intelligence Conference\n  (CI 2017) as an oral presentation. Interface code and data are available at:\n  https://github.com/windx0303/dialogue-esp-game", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Output-agreement mechanisms such as ESP Game have been widely used in human\ncomputation to obtain reliable human-generated labels. In this paper, we argue\nthat a \"time-limited\" output-agreement mechanism can be used to create a fast\nand robust crowd-powered component in interactive systems, particularly\ndialogue systems, to extract key information from user utterances on the fly.\nOur experiments on Amazon Mechanical Turk using the Airline Travel Information\nSystem (ATIS) dataset showed that the proposed approach achieves high-quality\nresults with an average response time shorter than 9 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 05:48:18 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 17:12:12 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""], ["Chen", "Yun-Nung", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1704.03693", "submitter": "Ivandre Paraboni", "authors": "Thiago castro Ferreira and Ivandre Paraboni", "title": "Trainable Referring Expression Generation using Overspecification\n  Preferences", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression generation (REG) models that use speaker-dependent\ninformation require a considerable amount of training data produced by every\nindividual speaker, or may otherwise perform poorly. In this work we present a\nsimple REG experiment that allows the use of larger training data sets by\ngrouping speakers according to their overspecification preferences. Intrinsic\nevaluation shows that this method generally outperforms the personalised method\nfound in previous work.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 10:47:10 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Ferreira", "Thiago castro", ""], ["Paraboni", "Ivandre", ""]]}, {"id": "1704.03809", "submitter": "Merlijn Blaauw", "authors": "Merlijn Blaauw, Jordi Bonada", "title": "A Neural Parametric Singing Synthesizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for singing synthesis based on a modified version of\nthe WaveNet architecture. Instead of modeling raw waveform, we model features\nproduced by a parametric vocoder that separates the influence of pitch and\ntimbre. This allows conveniently modifying pitch to match any target melody,\nfacilitates training on more modest dataset sizes, and significantly reduces\ntraining and generation times. Our model makes frame-wise predictions using\nmixture density outputs rather than categorical outputs in order to reduce the\nrequired parameter count. As we found overfitting to be an issue with the\nrelatively small datasets used in our experiments, we propose a method to\nregularize the model and make the autoregressive generation process more robust\nto prediction errors. Using a simple multi-stream architecture, harmonic,\naperiodic and voiced/unvoiced components can all be predicted in a coherent\nmanner. We compare our method to existing parametric statistical and\nstate-of-the-art concatenative methods using quantitative metrics and a\nlistening test. While naive implementations of the autoregressive generation\nalgorithm tend to be inefficient, using a smart algorithm we can greatly speed\nup the process and obtain a system that's competitive in both speed and\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 15:57:08 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 10:31:56 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 12:20:01 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Blaauw", "Merlijn", ""], ["Bonada", "Jordi", ""]]}, {"id": "1704.03940", "submitter": "Kai Hui", "authors": "Kai Hui, Andrew Yates, Klaus Berberich, Gerard de Melo", "title": "PACRR: A Position-Aware Neural IR Model for Relevance Matching", "comments": "To appear in EMNLP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to adopt deep learning for information retrieval, models are needed\nthat can capture all relevant information required to assess the relevance of a\ndocument to a given user query. While previous works have successfully captured\nunigram term matches, how to fully employ position-dependent information such\nas proximity and term dependencies has been insufficiently explored. In this\nwork, we propose a novel neural IR model named PACRR aiming at better modeling\nposition-dependent interactions between a query and a document. Extensive\nexperiments on six years' TREC Web Track data confirm that the proposed model\nyields better results under multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 21:56:59 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 16:12:36 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 23:02:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hui", "Kai", ""], ["Yates", "Andrew", ""], ["Berberich", "Klaus", ""], ["de Melo", "Gerard", ""]]}, {"id": "1704.03956", "submitter": "Nobuhiro Kaji", "authors": "Nobuhiro Kaji, Hayato Kobayashi", "title": "Incremental Skip-gram Model with Negative Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores an incremental training strategy for the skip-gram model\nwith negative sampling (SGNS) from both empirical and theoretical perspectives.\nExisting methods of neural word embeddings, including SGNS, are multi-pass\nalgorithms and thus cannot perform incremental model update. To address this\nproblem, we present a simple incremental extension of SGNS and provide a\nthorough theoretical analysis to demonstrate its validity. Empirical\nexperiments demonstrated the correctness of the theoretical analysis as well as\nthe practical usefulness of the incremental algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 00:36:33 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 07:15:00 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kaji", "Nobuhiro", ""], ["Kobayashi", "Hayato", ""]]}, {"id": "1704.03987", "submitter": "Tom Ouyang", "authors": "Tom Ouyang, David Rybach, Fran\\c{c}oise Beaufays, Michael Riley", "title": "Mobile Keyboard Input Decoding with Finite-State Transducers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a finite-state transducer (FST) representation for the models used\nto decode keyboard inputs on mobile devices. Drawing from learnings from the\nfield of speech recognition, we describe a decoding framework that can satisfy\nthe strict memory and latency constraints of keyboard input. We extend this\nframework to support functionalities typically not present in speech\nrecognition, such as literal decoding, autocorrections, word completions, and\nnext word predictions.\n  We describe the general framework of what we call for short the keyboard \"FST\ndecoder\" as well as the implementation details that are new compared to a\nspeech FST decoder. We demonstrate that the FST decoder enables new UX features\nsuch as post-corrections. Finally, we sketch how this decoder can support\nadvanced features such as personalization and contextualization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 04:00:07 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ouyang", "Tom", ""], ["Rybach", "David", ""], ["Beaufays", "Fran\u00e7oise", ""], ["Riley", "Michael", ""]]}, {"id": "1704.04008", "submitter": "Afshin Rahimi", "authors": "Afshin Rahimi, Trevor Cohn, Timothy Baldwin", "title": "A Neural Model for User Geolocation and Lexical Dialectology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective text- based user geolocation model based on\na neural network with one hidden layer, which achieves state of the art\nperformance over three Twitter benchmark geolocation datasets, in addition to\nproducing word and phrase embeddings in the hidden layer that we show to be\nuseful for detecting dialectal terms. As part of our analysis of dialectal\nterms, we release DAREDS, a dataset for evaluating dialect term detection\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 06:35:55 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 00:38:27 GMT"}, {"version": "v3", "created": "Thu, 27 Apr 2017 01:18:58 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Rahimi", "Afshin", ""], ["Cohn", "Trevor", ""], ["Baldwin", "Timothy", ""]]}, {"id": "1704.04100", "submitter": "Chlo\\'e Braud", "authors": "Chlo\\'e Braud and Oph\\'elie Lacroix and Anders S{\\o}gaard", "title": "Cross-lingual and cross-domain discourse segmentation of entire\n  documents", "comments": "To appear in Proceedings of ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse segmentation is a crucial step in building end-to-end discourse\nparsers. However, discourse segmenters only exist for a few languages and\ndomains. Typically they only detect intra-sentential segment boundaries,\nassuming gold standard sentence and token segmentation, and relying on\nhigh-quality syntactic parses and rich heuristics that are not generally\navailable across languages and domains. In this paper, we propose statistical\ndiscourse segmenters for five languages and three domains that do not rely on\ngold pre-annotations. We also consider the problem of learning discourse\nsegmenters when no labeled data is available for a language. Our fully\nsupervised system obtains 89.5% F1 for English newswire, with slight drops in\nperformance on other domains, and we report supervised and unsupervised\n(cross-lingual) results for five languages in total.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 12:54:30 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 14:03:10 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Braud", "Chlo\u00e9", ""], ["Lacroix", "Oph\u00e9lie", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1704.04154", "submitter": "Holger Schwenk", "authors": "Holger Schwenk and Matthijs Douze", "title": "Learning Joint Multilingual Sentence Representations with Neural Machine\n  Translation", "comments": "11 pages, 2 figures, published at ACL workshop RepL4NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use the framework of neural machine translation to learn\njoint sentence representations across six very different languages. Our aim is\nthat a representation which is independent of the language, is likely to\ncapture the underlying semantics. We define a new cross-lingual similarity\nmeasure, compare up to 1.4M sentence representations and study the\ncharacteristics of close sentences. We provide experimental evidence that\nsentences that are close in embedding space are indeed semantically highly\nrelated, but often have quite different structure and syntax. These relations\nalso hold when comparing sentences in different languages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 14:40:40 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 15:09:44 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Schwenk", "Holger", ""], ["Douze", "Matthijs", ""]]}, {"id": "1704.04198", "submitter": "Desmond Elliott", "authors": "Emiel van Miltenburg, Desmond Elliott", "title": "Room for improvement in automatic image description: an error analysis", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years we have seen rapid and significant progress in automatic\nimage description but what are the open problems in this area? Most work has\nbeen evaluated using text-based similarity metrics, which only indicate that\nthere have been improvements, without explaining what has improved. In this\npaper, we present a detailed error analysis of the descriptions generated by a\nstate-of-the-art attention-based model. Our analysis operates on two levels:\nfirst we check the descriptions for accuracy, and then we categorize the types\nof errors we observe in the inaccurate descriptions. We find only 20% of the\ndescriptions are free from errors, and surprisingly that 26% are unrelated to\nthe image. Finally, we manually correct the most frequently occurring error\ntypes (e.g. gender identification) to estimate the performance reward for\naddressing these errors, observing gains of 0.2--1 BLEU point per type.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 16:21:18 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["van Miltenburg", "Emiel", ""], ["Elliott", "Desmond", ""]]}, {"id": "1704.04222", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang, James Glass", "title": "Learning Latent Representations for Speech Generation and Transformation", "comments": "Accepted to Interspeech 2017", "journal-ref": "Interspeech 2017, pp 1273-1277", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ability to model a generative process and learn a latent representation\nfor speech in an unsupervised fashion will be crucial to process vast\nquantities of unlabelled speech data. Recently, deep probabilistic generative\nmodels such as Variational Autoencoders (VAEs) have achieved tremendous success\nin modeling natural images. In this paper, we apply a convolutional VAE to\nmodel the generative process of natural speech. We derive latent space\narithmetic operations to disentangle learned latent representations. We\ndemonstrate the capability of our model to modify the phonetic content or the\nspeaker identity for speech segments using the derived operations, without the\nneed for parallel supervisory data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 17:41:11 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 16:41:54 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1704.04259", "submitter": "Piek Vossen", "authors": "Piek Vossen and Agata Cybulska", "title": "Identity and Granularity of Events in Text", "comments": "Invited keynote speech by Piek Vossen at Cicling 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a method to detect event descrip- tions in\ndifferent news articles and to model the semantics of events and their\ncomponents using RDF representations. We compare these descriptions to solve a\ncross-document event coreference task. Our com- ponent approach to event\nsemantics defines identity and granularity of events at different levels. It\nperforms close to state-of-the-art approaches on the cross-document event\ncoreference task, while outperforming other works when assuming similar quality\nof event detection. We demonstrate how granularity and identity are\ninterconnected and we discuss how se- mantic anomaly could be used to define\ndifferences between coreference, subevent and topical relations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 19:23:43 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Vossen", "Piek", ""], ["Cybulska", "Agata", ""]]}, {"id": "1704.04336", "submitter": "Fan Xu", "authors": "Fan Xu, Shujing Du, Maoxi Li and Mingwen Wang", "title": "An entity-driven recursive neural network model for chinese discourse\n  coherence modeling", "comments": "9 pages", "journal-ref": "International Journal of Artificial Intelligence and Applications\n  (IJAIA), Vol.8, No.2, March 2017", "doi": "10.5121/ijaia.2017.8201", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese discourse coherence modeling remains a challenge taskin Natural\nLanguage Processing field.Existing approaches mostlyfocus on the need for\nfeature engineering, whichadoptthe sophisticated features to capture the logic\nor syntactic or semantic relationships acrosssentences within a text.In this\npaper, we present an entity-drivenrecursive deep modelfor the Chinese discourse\ncoherence evaluation based on current English discourse coherenceneural network\nmodel. Specifically, to overcome the shortage of identifying the entity(nouns)\noverlap across sentences in the currentmodel, Our combined modelsuccessfully\ninvestigatesthe entities information into the recursive neural network\nfreamework.Evaluation results on both sentence ordering and machine translation\ncoherence rating task show the effectiveness of the proposed model, which\nsignificantly outperforms the existing strong baseline.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 02:41:13 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Xu", "Fan", ""], ["Du", "Shujing", ""], ["Li", "Maoxi", ""], ["Wang", "Mingwen", ""]]}, {"id": "1704.04347", "submitter": "Longyue Wang", "authors": "Longyue Wang, Zhaopeng Tu, Andy Way, Qun Liu", "title": "Exploiting Cross-Sentence Context for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In translation, considering the document as a whole can help to resolve\nambiguities and inconsistencies. In this paper, we propose a cross-sentence\ncontext-aware approach and investigate the influence of historical contextual\ninformation on the performance of neural machine translation (NMT). First, this\nhistory is summarized in a hierarchical way. We then integrate the historical\nrepresentation into NMT in two strategies: 1) a warm-start of encoder and\ndecoder states, and 2) an auxiliary context source for updating decoder states.\nExperimental results on a large Chinese-English translation task show that our\napproach significantly improves upon a strong attention-based NMT system by up\nto +2.1 BLEU points.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 04:56:36 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 00:21:19 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 05:19:42 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wang", "Longyue", ""], ["Tu", "Zhaopeng", ""], ["Way", "Andy", ""], ["Liu", "Qun", ""]]}, {"id": "1704.04368", "submitter": "Abigail See", "authors": "Abigail See, Peter J. Liu, Christopher D. Manning", "title": "Get To The Point: Summarization with Pointer-Generator Networks", "comments": "Add METEOR evaluation results, add some citations, fix some equations\n  (what are now equations 1, 8 and 11 were missing a bias term), fix url to\n  pyrouge package, add acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence models have provided a viable new approach for\nabstractive text summarization (meaning they are not restricted to simply\nselecting and rearranging passages from the original text). However, these\nmodels have two shortcomings: they are liable to reproduce factual details\ninaccurately, and they tend to repeat themselves. In this work we propose a\nnovel architecture that augments the standard sequence-to-sequence attentional\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\nthat can copy words from the source text via pointing, which aids accurate\nreproduction of information, while retaining the ability to produce novel words\nthrough the generator. Second, we use coverage to keep track of what has been\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\nMail summarization task, outperforming the current abstractive state-of-the-art\nby at least 2 ROUGE points.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 07:55:19 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 05:47:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["See", "Abigail", ""], ["Liu", "Peter J.", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1704.04441", "submitter": "Georg Heigold", "authors": "Georg Heigold and G\\\"unter Neumann and Josef van Genabith", "title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against\n  Wrod Scramlbing or Randdm Nouse?", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the robustness of NLP against perturbed word forms.\nWhile neural approaches can achieve (almost) human-like accuracy for certain\ntasks and conditions, they often are sensitive to small changes in the input\nsuch as non-canonical input (e.g., typos). Yet both stability and robustness\nare desired properties in applications involving user-generated content, and\nthe more as humans easily cope with such noisy or adversary conditions. In this\npaper, we study the impact of noisy input. We consider different noise\ndistributions (one type of noise, combination of noise types) and mismatched\nnoise distributions for training and testing. Moreover, we empirically evaluate\nthe robustness of different models (convolutional neural networks, recurrent\nneural networks, non-neural models), different basic units (characters, byte\npair encoding units), and different NLP tasks (morphological tagging, machine\ntranslation).\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 14:43:44 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Heigold", "Georg", ""], ["Neumann", "G\u00fcnter", ""], ["van Genabith", "Josef", ""]]}, {"id": "1704.04451", "submitter": "Phong Le", "authors": "Phong Le and Ivan Titov", "title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "comments": "10 pages. CoNLL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference evaluation metrics are hard to optimize directly as they are\nnon-differentiable functions, not easily decomposable into elementary\ndecisions. Consequently, most approaches optimize objectives only indirectly\nrelated to the end goal, resulting in suboptimal performance. Instead, we\npropose a differentiable relaxation that lends itself to gradient-based\noptimisation, thus bypassing the need for reinforcement learning or heuristic\nmodification of cross-entropy. We show that by modifying the training objective\nof a competitive neural coreference system, we obtain a substantial gain in\nperformance. This suggests that our approach can be regarded as a viable\nalternative to using reinforcement learning or more computationally expensive\nimitation learning.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:22:51 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 08:30:32 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 07:55:47 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Le", "Phong", ""], ["Titov", "Ivan", ""]]}, {"id": "1704.04452", "submitter": "Tobias Falke", "authors": "Tobias Falke and Iryna Gurevych", "title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of\n  Concept Maps", "comments": "Published at EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept maps can be used to concisely represent important information and\nbring structure into large document collections. Therefore, we study a variant\nof multi-document summarization that produces summaries in the form of concept\nmaps. However, suitable evaluation datasets for this task are currently\nmissing. To close this gap, we present a newly created corpus of concept maps\nthat summarize heterogeneous collections of web documents on educational\ntopics. It was created using a novel crowdsourcing approach that allows us to\nefficiently determine important elements in large document collections. We\nrelease the corpus along with a baseline system and proposed evaluation\nprotocol to enable further research on this variant of summarization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:23:45 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 13:44:37 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Falke", "Tobias", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1704.04455", "submitter": "Paramita Mirza", "authors": "Paramita Mirza, Simon Razniewski, Fariz Darari, Gerhard Weikum", "title": "Cardinal Virtues: Extracting Relation Cardinalities from Text", "comments": "5 pages, ACL 2017 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction (IE) from text has largely focused on relations\nbetween individual entities, such as who has won which award. However, some\nfacts are never fully mentioned, and no IE method has perfect recall. Thus, it\nis beneficial to also tap contents about the cardinalities of these relations,\nfor example, how many awards someone has won. We introduce this novel problem\nof extracting cardinalities and discusses the specific challenges that set it\napart from standard IE. We present a distant supervision method using\nconditional random fields. A preliminary evaluation results in precision\nbetween 3% and 55%, depending on the difficulty of relations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:28:06 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 00:55:36 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Mirza", "Paramita", ""], ["Razniewski", "Simon", ""], ["Darari", "Fariz", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1704.04517", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "ShapeWorld - A new test methodology for multimodal language\n  understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for evaluating multimodal deep learning models\nwith respect to their language understanding and generalization abilities. In\nthis approach, artificial data is automatically generated according to the\nexperimenter's specifications. The content of the data, both during training\nand evaluation, can be controlled in detail, which enables tasks to be created\nthat require true generalization abilities, in particular the combination of\npreviously introduced concepts in novel ways. We demonstrate the potential of\nour methodology by evaluating various visual question answering models on four\ndifferent tasks, and show how our framework gives us detailed insights into\ntheir capabilities and limitations. By open-sourcing our framework, we hope to\nstimulate progress in the field of multimodal language understanding.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 19:01:51 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1704.04520", "submitter": "Zi Long", "authors": "Zi Long, Ryuichiro Kimura, Takehito Utsuro, Tomoharu Mitsuhashi, Mikio\n  Yamamoto", "title": "Neural Machine Translation Model with a Large Vocabulary Selected by\n  Branching Entropy", "comments": "MT summit 2017 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT), a new approach to machine translation, has\nachieved promising results comparable to those of traditional approaches such\nas statistical machine translation (SMT). Despite its recent success, NMT\ncannot handle a larger vocabulary because the training complexity and decoding\ncomplexity proportionally increase with the number of target words. This\nproblem becomes even more serious when translating patent documents, which\ncontain many technical terms that are observed infrequently. In this paper, we\npropose to select phrases that contain out-of-vocabulary words using the\nstatistical approach of branching entropy. This allows the proposed NMT system\nto be applied to a translation task of any language pair without any\nlanguage-specific knowledge about technical term identification. The selected\nphrases are then replaced with tokens during training and post-translated by\nthe phrase translation table of SMT. Evaluation on Japanese-to-Chinese,\nChinese-to-Japanese, Japanese-to-English and English-to-Japanese patent\nsentence translation proved the effectiveness of phrases selected with\nbranching entropy, where the proposed NMT model achieves a substantial\nimprovement over a baseline NMT model without our proposed technique. Moreover,\nthe number of translation errors of under-translation by the baseline NMT model\nwithout our proposed technique reduces to around half by the proposed NMT\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 19:35:14 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 11:41:39 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 05:58:48 GMT"}, {"version": "v4", "created": "Thu, 20 Jul 2017 01:59:10 GMT"}, {"version": "v5", "created": "Wed, 26 Jul 2017 08:26:54 GMT"}, {"version": "v6", "created": "Wed, 6 Sep 2017 04:06:07 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Long", "Zi", ""], ["Kimura", "Ryuichiro", ""], ["Utsuro", "Takehito", ""], ["Mitsuhashi", "Tomoharu", ""], ["Yamamoto", "Mikio", ""]]}, {"id": "1704.04521", "submitter": "Zi Long", "authors": "Zi Long, Takehito Utsuro, Tomoharu Mitsuhashi, Mikio Yamamoto", "title": "Translation of Patent Sentences with a Large Vocabulary of Technical\n  Terms Using Neural Machine Translation", "comments": "WAT 2016. arXiv admin note: substantial text overlap with\n  arXiv:1704.04520", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT), a new approach to machine translation, has\nachieved promising results comparable to those of traditional approaches such\nas statistical machine translation (SMT). Despite its recent success, NMT\ncannot handle a larger vocabulary because training complexity and decoding\ncomplexity proportionally increase with the number of target words. This\nproblem becomes even more serious when translating patent documents, which\ncontain many technical terms that are observed infrequently. In NMTs, words\nthat are out of vocabulary are represented by a single unknown token. In this\npaper, we propose a method that enables NMT to translate patent sentences\ncomprising a large vocabulary of technical terms. We train an NMT system on\nbilingual data wherein technical terms are replaced with technical term tokens;\nthis allows it to translate most of the source sentences except technical\nterms. Further, we use it as a decoder to translate source sentences with\ntechnical term tokens and replace the tokens with technical term translations\nusing SMT. We also use it to rerank the 1,000-best SMT translations on the\nbasis of the average of the SMT score and that of the NMT rescoring of the\ntranslated sentences with technical term tokens. Our experiments on\nJapanese-Chinese patent sentences show that the proposed NMT system achieves a\nsubstantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over\ntraditional SMT systems and an improvement of approximately 0.6 BLEU points and\n0.8 RIBES points over an equivalent NMT system without our proposed technique.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 19:36:54 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Long", "Zi", ""], ["Utsuro", "Takehito", ""], ["Mitsuhashi", "Tomoharu", ""], ["Yamamoto", "Mikio", ""]]}, {"id": "1704.04530", "submitter": "Shashi Narayan", "authors": "Shashi Narayan, Nikos Papasarantopoulos, Shay B. Cohen, Mirella Lapata", "title": "Neural Extractive Summarization with Side Information", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most extractive summarization methods focus on the main body of the document\nfrom which sentences need to be extracted. However, the gist of the document\nmay lie in side information, such as the title and image captions which are\noften available for newswire articles. We propose to explore side information\nin the context of single-document extractive summarization. We develop a\nframework for single-document summarization composed of a hierarchical document\nencoder and an attention-based extractor with attention over side information.\nWe evaluate our model on a large scale news dataset. We show that extractive\nsummarization with side information consistently outperforms its counterpart\nthat does not use any side information, in terms of both informativeness and\nfluency.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 20:29:23 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 19:56:15 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Narayan", "Shashi", ""], ["Papasarantopoulos", "Nikos", ""], ["Cohen", "Shay B.", ""], ["Lapata", "Mirella", ""]]}, {"id": "1704.04539", "submitter": "Marco Damonte", "authors": "Marco Damonte and Shay B. Cohen", "title": "Cross-lingual Abstract Meaning Representation Parsing", "comments": "To appear at NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Meaning Representation (AMR) annotation efforts have mostly focused\non English. In order to train parsers on other languages, we propose a method\nbased on annotation projection, which involves exploiting annotations in a\nsource language and a parallel corpus of the source language and a target\nlanguage. Using English as the source language, we show promising results for\nItalian, Spanish, German and Chinese as target languages. Besides evaluating\nthe target parsers on non-gold datasets, we further propose an evaluation\nmethod that exploits the English gold annotations and does not require access\nto gold annotations for the target languages. This is achieved by inverting the\nprojection process: a new English parser is learned from the target language\nparser and evaluated on the existing English gold standard.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 20:41:27 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 18:48:05 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Damonte", "Marco", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1704.04550", "submitter": "Su Wang", "authors": "Su Wang, Stephen Roller, Katrin Erk", "title": "Distributional Modeling on a Diet: One-shot Word Learning from Text Only", "comments": "The 8th International Joint Conference on Natural Language Processing\n  (IJCNLP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test whether distributional models can do one-shot learning of\ndefinitional properties from text only. Using Bayesian models, we find that\nfirst learning overarching structure in the known data, regularities in textual\ncontexts and in properties, helps one-shot learning, and that individual\ncontext items can be highly informative. Our experiments show that our model\ncan learn properties from a single exposure when given an informative\nutterance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 22:29:27 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 14:25:46 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 01:14:58 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 14:14:25 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Wang", "Su", ""], ["Roller", "Stephen", ""], ["Erk", "Katrin", ""]]}, {"id": "1704.04565", "submitter": "Dipanjan Das", "authors": "Gaurav Singh Tomar and Thyago Duque and Oscar T\\\"ackstr\\\"om and Jakob\n  Uszkoreit and Dipanjan Das", "title": "Neural Paraphrase Identification of Questions with Noisy Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a solution to the problem of paraphrase identification of\nquestions. We focus on a recent dataset of question pairs annotated with binary\nparaphrase labels and show that a variant of the decomposable attention model\n(Parikh et al., 2016) results in accurate performance on this task, while being\nfar simpler than many competing neural architectures. Furthermore, when the\nmodel is pretrained on a noisy dataset of automatically collected question\nparaphrases, it obtains the best reported performance on the dataset.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 02:09:31 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 02:41:42 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Tomar", "Gaurav Singh", ""], ["Duque", "Thyago", ""], ["T\u00e4ckstr\u00f6m", "Oscar", ""], ["Uszkoreit", "Jakob", ""], ["Das", "Dipanjan", ""]]}, {"id": "1704.04601", "submitter": "Guang-He Lee", "authors": "Guang-He Lee and Yun-Nung Chen", "title": "MUSE: Modularizing Unsupervised Sense Embeddings", "comments": null, "journal-ref": "Proceedings of the 2017 Conference on Empirical Methods in Natural\n  Language Processing, pages 327-337", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to address the word sense ambiguity issue in an\nunsupervised manner, where word sense representations are learned along a word\nsense selection mechanism given contexts. Prior work focused on designing a\nsingle model to deliver both mechanisms, and thus suffered from either\ncoarse-grained representation learning or inefficient sense selection. The\nproposed modular approach, MUSE, implements flexible modules to optimize\ndistinct mechanisms, achieving the first purely sense-level representation\nlearning system with linear-time sense selection. We leverage reinforcement\nlearning to enable joint training on the proposed modules, and introduce\nvarious exploration techniques on sense selection for better robustness. The\nexperiments on benchmark data show that the proposed approach achieves the\nstate-of-the-art performance on synonym selection as well as on contextual word\nsimilarities in terms of MaxSimC.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 07:36:49 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 06:20:34 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Lee", "Guang-He", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "1704.04664", "submitter": "Akira Taniguchi", "authors": "Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi, Tetsunari\n  Inamura", "title": "Online Spatial Concept and Lexical Acquisition with Simultaneous\n  Localization and Mapping", "comments": "This paper was accepted in the 2017 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS 2017)", "journal-ref": null, "doi": "10.1109/IROS.2017.8202243", "report-no": null, "categories": "cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an online learning algorithm based on a\nRao-Blackwellized particle filter for spatial concept acquisition and mapping.\nWe have proposed a nonparametric Bayesian spatial concept acquisition model\n(SpCoA). We propose a novel method (SpCoSLAM) integrating SpCoA and FastSLAM in\nthe theoretical framework of the Bayesian generative model. The proposed method\ncan simultaneously learn place categories and lexicons while incrementally\ngenerating an environmental map. Furthermore, the proposed method has scene\nimage features and a language model added to SpCoA. In the experiments, we\ntested online learning of spatial concepts and environmental maps in a novel\nenvironment of which the robot did not have a map. Then, we evaluated the\nresults of online learning of spatial concepts and lexical acquisition. The\nexperimental results demonstrated that the robot was able to more accurately\nlearn the relationships between words and the place in the environmental map\nincrementally by using the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 17:18:11 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 12:20:51 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Taniguchi", "Akira", ""], ["Hagiwara", "Yoshinobu", ""], ["Taniguchi", "Tadahiro", ""], ["Inamura", "Tetsunari", ""]]}, {"id": "1704.04675", "submitter": "Joost Bastings", "authors": "Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil\n  Sima'an", "title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective approach to incorporating syntactic\nstructure into neural attention-based encoder-decoder models for machine\ntranslation. We rely on graph-convolutional networks (GCNs), a recent class of\nneural networks developed for modeling graph-structured data. Our GCNs use\npredicted syntactic dependency trees of source sentences to produce\nrepresentations of words (i.e. hidden states of the encoder) that are sensitive\nto their syntactic neighborhoods. GCNs take word representations as input and\nproduce word representations as output, so they can easily be incorporated as\nlayers into standard encoders (e.g., on top of bidirectional RNNs or\nconvolutional neural networks). We evaluate their effectiveness with\nEnglish-German and English-Czech translation experiments for different types of\nencoders and observe substantial improvements over their syntax-agnostic\nversions in all the considered setups.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 19:04:59 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 15:02:17 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 18:18:52 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 19:29:11 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Bastings", "Jasmijn", ""], ["Titov", "Ivan", ""], ["Aziz", "Wilker", ""], ["Marcheggiani", "Diego", ""], ["Sima'an", "Khalil", ""]]}, {"id": "1704.04683", "submitter": "Guokun Lai", "authors": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RACE, a new dataset for benchmark evaluation of methods in the\nreading comprehension task. Collected from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, RACE consists\nof near 28,000 passages and near 100,000 questions generated by human experts\n(English instructors), and covers a variety of topics which are carefully\ndesigned for evaluating the students' ability in understanding and reasoning.\nIn particular, the proportion of questions that requires reasoning is much\nlarger in RACE than that in other benchmark datasets for reading comprehension,\nand there is a significant gap between the performance of the state-of-the-art\nmodels (43%) and the ceiling human performance (95%). We hope this new dataset\ncan serve as a valuable resource for research and evaluation in machine\ncomprehension. The dataset is freely available at\nhttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available at\nhttps://github.com/qizhex/RACE_AR_baselines.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 19:31:41 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 15:47:40 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 03:21:55 GMT"}, {"version": "v4", "created": "Sat, 15 Jul 2017 18:48:57 GMT"}, {"version": "v5", "created": "Tue, 5 Dec 2017 19:36:03 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Lai", "Guokun", ""], ["Xie", "Qizhe", ""], ["Liu", "Hanxiao", ""], ["Yang", "Yiming", ""], ["Hovy", "Eduard", ""]]}, {"id": "1704.04743", "submitter": "Roee Aharoni", "authors": "Roee Aharoni and Yoav Goldberg", "title": "Towards String-to-Tree Neural Machine Translation", "comments": "Accepted as a short paper in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple method to incorporate syntactic information about the\ntarget language in a neural machine translation system by translating into\nlinearized, lexicalized constituency trees. An experiment on the WMT16\nGerman-English news translation task resulted in an improved BLEU score when\ncompared to a syntax-agnostic NMT baseline trained on the same dataset. An\nanalysis of the translations from the syntax-aware system shows that it\nperforms more reordering during translation in comparison to the baseline. A\nsmall-scale human evaluation also showed an advantage to the syntax-aware\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 09:54:50 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 10:20:28 GMT"}, {"version": "v3", "created": "Sat, 6 May 2017 07:25:19 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Aharoni", "Roee", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1704.04856", "submitter": "Pablo Loyola", "authors": "Pablo Loyola, Edison Marrese-Taylor and Yutaka Matsuo", "title": "A Neural Architecture for Generating Natural Language Descriptions from\n  Source Code Changes", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model to automatically describe changes introduced in the source\ncode of a program using natural language. Our method receives as input a set of\ncode commits, which contains both the modifications and message introduced by\nan user. These two modalities are used to train an encoder-decoder\narchitecture. We evaluated our approach on twelve real world open source\nprojects from four different programming languages. Quantitative and\nqualitative results showed that the proposed approach can generate feasible and\nsemantically sound descriptions not only in standard in-project settings, but\nalso in a cross-project setting.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 03:20:07 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Loyola", "Pablo", ""], ["Marrese-Taylor", "Edison", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1704.04859", "submitter": "Frederick Liu", "authors": "Frederick Liu, Han Lu, Chieh Lo and Graham Neubig", "title": "Learning Character-level Compositionality with Visual Features", "comments": "Accepted to ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has modeled the compositionality of words by creating\ncharacter-level models of meaning, reducing problems of sparsity for rare\nwords. However, in many writing systems compositionality has an effect even on\nthe character-level: the meaning of a character is derived by the sum of its\nparts. In this paper, we model this effect by creating embeddings for\ncharacters based on their visual characteristics, creating an image for the\ncharacter and running it through a convolutional neural network to produce a\nvisual character embedding. Experiments on a text classification task\ndemonstrate that such model allows for better processing of instances with rare\ncharacters in languages such as Chinese, Japanese, and Korean. Additionally,\nqualitative analyses demonstrate that our proposed model learns to focus on the\nparts of characters that carry semantic content, resulting in embeddings that\nare coherent in visual space.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 03:30:30 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 15:13:24 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Liu", "Frederick", ""], ["Lu", "Han", ""], ["Lo", "Chieh", ""], ["Neubig", "Graham", ""]]}, {"id": "1704.04920", "submitter": "Octavian-Eugen Ganea", "authors": "Octavian-Eugen Ganea and Thomas Hofmann", "title": "Deep Joint Entity Disambiguation with Local Neural Attention", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP) 2017 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning model for joint document-level entity\ndisambiguation, which leverages learned neural representations. Key components\nare entity embeddings, a neural attention mechanism over local context windows,\nand a differentiable joint inference stage for disambiguation. Our approach\nthereby combines benefits of deep learning with more traditional approaches\nsuch as graphical models and probabilistic mention-entity maps. Extensive\nexperiments show that we are able to obtain competitive or state-of-the-art\naccuracy at moderate computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 10:18:32 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 23:47:24 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 18:25:57 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ganea", "Octavian-Eugen", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1704.05021", "submitter": "Alham Fikri Aji", "authors": "Alham Fikri Aji and Kenneth Heafield", "title": "Sparse Communication for Distributed Gradient Descent", "comments": "EMNLP 2017", "journal-ref": null, "doi": "10.18653/v1/D17-1045", "report-no": null, "categories": "cs.CL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make distributed stochastic gradient descent faster by exchanging sparse\nupdates instead of dense updates. Gradient updates are positively skewed as\nmost updates are near zero, so we map the 99% smallest updates (by absolute\nvalue) to zero then exchange sparse matrices. This method can be combined with\nquantization to further improve the compression. We explore different\nconfigurations and apply them to neural machine translation and MNIST image\nclassification tasks. Most configurations work on MNIST, whereas different\nconfigurations reduce convergence rate on the more complex translation task.\nOur experiments show that we can achieve up to 49% speed up on MNIST and 22% on\nNMT without damaging the final accuracy or BLEU.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:32:02 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 21:47:51 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Aji", "Alham Fikri", ""], ["Heafield", "Kenneth", ""]]}, {"id": "1704.05091", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Eduarda Mendes Rodrigues, Carlos Soares, Eug\\'enio\n  Oliveira", "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity\n  with Financial Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the approach developed at the Faculty of Engineering of\nUniversity of Porto, to participate in SemEval 2017, Task 5: Fine-grained\nSentiment Analysis on Financial Microblogs and News. The task consisted in\npredicting a real continuous variable from -1.0 to +1.0 representing the\npolarity and intensity of sentiment concerning companies/stocks mentioned in\nshort texts. We modeled the task as a regression analysis problem and combined\ntraditional techniques such as pre-processing short texts, bag-of-words\nrepresentations and lexical-based features with enhanced financial specific\nbag-of-embeddings. We used an external collection of tweets and news headlines\nmentioning companies/stocks from S\\&P 500 to create financial word embeddings\nwhich are able to capture domain-specific syntactic and semantic similarities.\nThe resulting approach obtained a cosine similarity score of 0.69 in sub-task\n5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 18:48:00 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Saleiro", "Pedro", ""], ["Rodrigues", "Eduarda Mendes", ""], ["Soares", "Carlos", ""], ["Oliveira", "Eug\u00e9nio", ""]]}, {"id": "1704.05119", "submitter": "Sharan Narang", "authors": "Sharan Narang, Erich Elsen, Gregory Diamos, Shubho Sengupta", "title": "Exploring Sparsity in Recurrent Neural Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) are widely used to solve a variety of\nproblems and as the quantity of data and the amount of available compute have\nincreased, so have model sizes. The number of parameters in recent\nstate-of-the-art networks makes them hard to deploy, especially on mobile\nphones and embedded devices. The challenge is due to both the size of the model\nand the time it takes to evaluate it. In order to deploy these RNNs\nefficiently, we propose a technique to reduce the parameters of a network by\npruning weights during the initial training of the network. At the end of\ntraining, the parameters of the network are sparse while accuracy is still\nclose to the original dense neural network. The network size is reduced by 8x\nand the time required to train the model remains constant. Additionally, we can\nprune a larger dense network to achieve better than baseline performance while\nstill reducing the total number of parameters significantly. Pruning RNNs\nreduces the size of the model and can also help achieve significant inference\ntime speed-up using sparse matrix multiply. Benchmarks show that using our\ntechnique model size can be reduced by 90% and speed-up is around 2x to 7x.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 20:42:05 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 22:10:47 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Narang", "Sharan", ""], ["Elsen", "Erich", ""], ["Diamos", "Gregory", ""], ["Sengupta", "Shubho", ""]]}, {"id": "1704.05135", "submitter": "Stanislas Lauly", "authors": "Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho", "title": "Does Neural Machine Translation Benefit from Larger Context?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural machine translation architecture that models the\nsurrounding text in addition to the source sentence. These models lead to\nbetter performance, both in terms of general translation quality and pronoun\nprediction, when trained on small corpora, although this improvement largely\ndisappears when trained with a larger corpus. We also discover that\nattention-based neural machine translation is well suited for pronoun\nprediction and compares favorably with other approaches that were specifically\ndesigned for this task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 21:42:19 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Jean", "Sebastien", ""], ["Lauly", "Stanislas", ""], ["Firat", "Orhan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1704.05162", "submitter": "Majid Laali", "authors": "Majid Laali and Leila Kosseim", "title": "Automatic Disambiguation of French Discourse Connectives", "comments": null, "journal-ref": "International Journal of Computational Linguistics and\n  Applications, vol. 7, no. 1, 2016, pp. 11-30", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse connectives (e.g. however, because) are terms that can explicitly\nconvey a discourse relation within a text. While discourse connectives have\nbeen shown to be an effective clue to automatically identify discourse\nrelations, they are not always used to convey such relations, thus they should\nfirst be disambiguated between discourse-usage non-discourse-usage. In this\npaper, we investigate the applicability of features proposed for the\ndisambiguation of English discourse connectives for French. Our results with\nthe French Discourse Treebank (FDTB) show that syntactic and lexical features\ndeveloped for English texts are as effective for French and allow the\ndisambiguation of French discourse connectives with an accuracy of 94.2%.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 01:04:49 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Laali", "Majid", ""], ["Kosseim", "Leila", ""]]}, {"id": "1704.05179", "submitter": "Levent Sagun", "authors": "Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik\n  and Kyunghyun Cho", "title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We publicly release a new large-scale dataset, called SearchQA, for machine\ncomprehension, or question-answering. Unlike recently released datasets, such\nas DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to\nreflect a full pipeline of general question-answering. That is, we start not\nfrom an existing article and generate a question-answer pair, but start from an\nexisting question-answer pair, crawled from J! Archive, and augment it with\ntext snippets retrieved by Google. Following this approach, we built SearchQA,\nwhich consists of more than 140k question-answer pairs with each pair having\n49.6 snippets on average. Each question-answer-context tuple of the SearchQA\ncomes with additional meta-data such as the snippet's URL, which we believe\nwill be valuable resources for future research. We conduct human evaluation as\nwell as test two baseline methods, one simple word selection and the other deep\nlearning based, on the SearchQA. We show that there is a meaningful gap between\nthe human and machine performances. This suggests that the proposed dataset\ncould well serve as a benchmark for question-answering.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 02:42:17 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 14:07:21 GMT"}, {"version": "v3", "created": "Sun, 11 Jun 2017 11:51:06 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Dunn", "Matthew", ""], ["Sagun", "Levent", ""], ["Higgins", "Mike", ""], ["Guney", "V. Ugur", ""], ["Cirik", "Volkan", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1704.05228", "submitter": "Mathias Kraus", "authors": "Mathias Kraus, Stefan Feuerriegel", "title": "Sentiment analysis based on rhetorical structure theory: Learning deep\n  neural networks from discourse trees", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2018.10.002", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prominent applications of sentiment analysis are countless, covering areas\nsuch as marketing, customer service and communication. The conventional\nbag-of-words approach for measuring sentiment merely counts term frequencies;\nhowever, it neglects the position of the terms within the discourse. As a\nremedy, we develop a discourse-aware method that builds upon the discourse\nstructure of documents. For this purpose, we utilize rhetorical structure\ntheory to label (sub-)clauses according to their hierarchical relationships and\nthen assign polarity scores to individual leaves. To learn from the resulting\nrhetorical structure, we propose a tensor-based, tree-structured deep neural\nnetwork (named Discourse-LSTM) in order to process the complete discourse tree.\nThe underlying tensors infer the salient passages of narrative materials. In\naddition, we suggest two algorithms for data augmentation (node reordering and\nartificial leaf insertion) that increase our training set and reduce\noverfitting. Our benchmarks demonstrate the superior performance of our\napproach. Moreover, our tensor structure reveals the salient text passages and\nthereby provides explanatory insights.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 08:24:20 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 08:03:06 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 08:02:49 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Kraus", "Mathias", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "1704.05295", "submitter": "S\\'ebastien Harispe", "authors": "S\\'ebastien Harispe, Sylvie Ranwez, Stefan Janaqi, Jacky Montmain", "title": "Semantic Similarity from Natural Language and Ontology Analysis", "comments": "preprint version of the book Semantic Similarity from Natural\n  Language and Ontology Analysis (Synthesis Lectures on Human Language\n  Technologies - Morgan & Claypool publishers)", "journal-ref": null, "doi": "10.2200/S00639ED1V01Y201504HLT027", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence federates numerous scientific fields in the aim of\ndeveloping machines able to assist human operators performing complex\ntreatments -- most of which demand high cognitive skills (e.g. learning or\ndecision processes). Central to this quest is to give machines the ability to\nestimate the likeness or similarity between things in the way human beings\nestimate the similarity between stimuli.\n  In this context, this book focuses on semantic measures: approaches designed\nfor comparing semantic entities such as units of language, e.g. words,\nsentences, or concepts and instances defined into knowledge bases. The aim of\nthese measures is to assess the similarity or relatedness of such semantic\nentities by taking into account their semantics, i.e. their meaning --\nintuitively, the words tea and coffee, which both refer to stimulating\nbeverage, will be estimated to be more semantically similar than the words\ntoffee (confection) and coffee, despite that the last pair has a higher\nsyntactic similarity. The two state-of-the-art approaches for estimating and\nquantifying semantic similarities/relatedness of semantic entities are\npresented in detail: the first one relies on corpora analysis and is based on\nNatural Language Processing techniques and semantic models while the second is\nbased on more or less formal, computer-readable and workable forms of knowledge\nsuch as semantic networks, thesaurus or ontologies. (...) Beyond a simple\ninventory and categorization of existing measures, the aim of this monograph is\nto convey novices as well as researchers of these domains towards a better\nunderstanding of semantic similarity estimation and more generally semantic\nmeasures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 12:24:26 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Harispe", "S\u00e9bastien", ""], ["Ranwez", "Sylvie", ""], ["Janaqi", "Stefan", ""], ["Montmain", "Jacky", ""]]}, {"id": "1704.05347", "submitter": "Zeljko Agic", "authors": "\\v{Z}eljko Agi\\'c and Natalie Schluter", "title": "Baselines and test data for cross-lingual inference", "comments": "To appear at LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have seen a revival of interest in textual entailment,\nsparked by i) the emergence of powerful deep neural network learners for\nnatural language processing and ii) the timely development of large-scale\nevaluation datasets such as SNLI. Recast as natural language inference, the\nproblem now amounts to detecting the relation between pairs of statements: they\neither contradict or entail one another, or they are mutually neutral. Current\nresearch in natural language inference is effectively exclusive to English. In\nthis paper, we propose to advance the research in SNLI-style natural language\ninference toward multilingual evaluation. To that end, we provide test data for\nfour major languages: Arabic, French, Spanish, and Russian. We experiment with\na set of baselines. Our systems are based on cross-lingual word embeddings and\nmachine translation. While our best system scores an average accuracy of just\nover 75%, we focus largely on enabling further research in multilingual\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 14:12:37 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 18:24:49 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Agi\u0107", "\u017deljko", ""], ["Schluter", "Natalie", ""]]}, {"id": "1704.05358", "submitter": "Jiaqi Mu", "authors": "Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "Representing Sentences as Low-Rank Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentences are important semantic units of natural language. A generic,\ndistributional representation of sentences that can capture the latent\nsemantics is beneficial to multiple downstream applications. We observe a\nsimple geometry of sentences -- the word representations of a given sentence\n(on average 10.23 words in all SemEval datasets with a standard deviation 4.84)\nroughly lie in a low-rank subspace (roughly, rank 4). Motivated by this\nobservation, we represent a sentence by the low-rank subspace spanned by its\nword vectors. Such an unsupervised representation is empirically validated via\nsemantic textual similarity tasks on 19 different datasets, where it\noutperforms the sophisticated neural network models, including skip-thought\nvectors, by 15% on average.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 14:30:32 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1704.05393", "submitter": "Michela Fazzolari", "authors": "Michela Fazzolari, Marinella Petrocchi, Alessandro Tommasi, Cesare\n  Zavattari", "title": "Mining Worse and Better Opinions. Unsupervised and Agnostic Aggregation\n  of Online Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for aggregating online reviews,\naccording to the opinions they express. Our methodology is unsupervised - due\nto the fact that it does not rely on pre-labeled reviews - and it is agnostic -\nsince it does not make any assumption about the domain or the language of the\nreview content. We measure the adherence of a review content to the domain\nterminology extracted from a review set. First, we demonstrate the\ninformativeness of the adherence metric with respect to the score associated\nwith a review. Then, we exploit the metric values to group reviews, according\nto the opinions they express. Our experimental campaign has been carried out on\ntwo large datasets collected from Booking and Amazon, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 15:20:25 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Fazzolari", "Michela", ""], ["Petrocchi", "Marinella", ""], ["Tommasi", "Alessandro", ""], ["Zavattari", "Cesare", ""]]}, {"id": "1704.05415", "submitter": "Cristina Espa\\~na-Bonet", "authors": "Cristina Espa\\~na-Bonet, \\'Ad\\'am Csaba Varga, Alberto\n  Barr\\'on-Cede\\~no and Josef van Genabith", "title": "An Empirical Analysis of NMT-Derived Interlingual Embeddings and their\n  Use in Parallel Sentence Identification", "comments": "11 pages, 4 figures", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 11, no.\n  8, pp. 1340-1350, December 2017", "doi": "10.1109/JSTSP.2017.2764273", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end neural machine translation has overtaken statistical machine\ntranslation in terms of translation quality for some language pairs, specially\nthose with large amounts of parallel data. Besides this palpable improvement,\nneural networks provide several new properties. A single system can be trained\nto translate between many languages at almost no additional cost other than\ntraining time. Furthermore, internal representations learned by the network\nserve as a new semantic representation of words -or sentences- which, unlike\nstandard word embeddings, are learned in an essentially bilingual or even\nmultilingual context. In view of these properties, the contribution of the\npresent work is two-fold. First, we systematically study the NMT context\nvectors, i.e. output of the encoder, and their power as an interlingua\nrepresentation of a sentence. We assess their quality and effectiveness by\nmeasuring similarities across translations, as well as semantically related and\nsemantically unrelated sentence pairs. Second, as extrinsic evaluation of the\nfirst point, we identify parallel sentences in comparable corpora, obtaining an\nF1=98.2% on data from a shared task when using only NMT context vectors. Using\ncontext vectors jointly with similarity measures F1 reaches 98.9%.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 16:38:01 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 10:01:13 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Espa\u00f1a-Bonet", "Cristina", ""], ["Varga", "\u00c1d\u00e1m Csaba", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["van Genabith", "Josef", ""]]}, {"id": "1704.05426", "submitter": "Adina Williams", "authors": "Adina Williams, Nikita Nangia and Samuel R. Bowman", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through\n  Inference", "comments": "10 pages, 1 figures, 5 tables. v2 corrects a misreported accuracy\n  number for the CBOW model in the 'matched' setting. v3 adds a discussion of\n  the difficulty of the corpus to the analysis section. v4 is the version that\n  was accepted to NAACL2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)\ncorpus, a dataset designed for use in the development and evaluation of machine\nlearning models for sentence understanding. In addition to being one of the\nlargest corpora available for the task of NLI, at 433k examples, this corpus\nimproves upon available resources in its coverage: it offers data from ten\ndistinct genres of written and spoken English--making it possible to evaluate\nsystems on nearly the full complexity of the language--and it offers an\nexplicit setting for the evaluation of cross-genre domain adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 17:10:13 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 16:40:41 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 18:25:11 GMT"}, {"version": "v4", "created": "Mon, 19 Feb 2018 19:19:51 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Williams", "Adina", ""], ["Nangia", "Nikita", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1704.05513", "submitter": "Pierre-Hadrien Arnoux", "authors": "Pierre-Hadrien Arnoux, Anbang Xu, Neil Boyette, Jalal Mahmud, Rama\n  Akkiraju, Vibha Sinha", "title": "25 Tweets to Know You: A New Model to Predict Personality with Social\n  Media", "comments": "Accepted as a short paper at ICWSM 2017. Please cite the ICWSM\n  version and not the ArXiv version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting personality is essential for social applications supporting\nhuman-centered activities, yet prior modeling methods with users written text\nrequire too much input data to be realistically used in the context of social\nmedia. In this work, we aim to drastically reduce the data requirement for\npersonality modeling and develop a model that is applicable to most users on\nTwitter. Our model integrates Word Embedding features with Gaussian Processes\nregression. Based on the evaluation of over 1.3K users on Twitter, we find that\nour model achieves comparable or better accuracy than state of the art\ntechniques with 8 times fewer data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 20:16:31 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Arnoux", "Pierre-Hadrien", ""], ["Xu", "Anbang", ""], ["Boyette", "Neil", ""], ["Mahmud", "Jalal", ""], ["Akkiraju", "Rama", ""], ["Sinha", "Vibha", ""]]}, {"id": "1704.05543", "submitter": "Sreecharan Sankaranarayanan", "authors": "Gaurav Singh Tomar, Sreecharan Sankaranarayanan, Xu Wang and Carolyn\n  Penstein Ros\\'e", "title": "Coordinating Collaborative Chat in Massive Open Online Courses", "comments": "8 pages", "journal-ref": "Proceedings of the International Conference of the Learning\n  Sciences 2016, Volume 1, pp 607-614", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An earlier study of a collaborative chat intervention in a Massive Open\nOnline Course (MOOC) identified negative effects on attrition stemming from a\nrequirement for students to be matched with exactly one partner prior to\nbeginning the activity. That study raised questions about how to orchestrate a\ncollaborative chat intervention in a MOOC context in order to provide the\nbenefit of synchronous social engagement without the coordination difficulties.\nIn this paper we present a careful analysis of an intervention designed to\novercome coordination difficulties by welcoming students into the chat on a\nrolling basis as they arrive rather than requiring them to be matched with a\npartner before beginning. The results suggest the most positive impact when\nexperiencing a chat with exactly one partner rather than more or less. A\nqualitative analysis of the chat data reveals differential experiences between\nthese configurations that suggests a potential explanation for the effect and\nraises questions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 21:57:10 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tomar", "Gaurav Singh", ""], ["Sankaranarayanan", "Sreecharan", ""], ["Wang", "Xu", ""], ["Ros\u00e9", "Carolyn Penstein", ""]]}, {"id": "1704.05550", "submitter": "Daniel Lee", "authors": "Rakesh Verma and Daniel Lee", "title": "Extractive Summarization: Limits, Compression, Generalized Model and\n  Heuristics", "comments": null, "journal-ref": "Computaci\\'on y Sistemas 21(4) (2017)", "doi": "10.13053/CyS-21-4-2885", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its promise to alleviate information overload, text summarization has\nattracted the attention of many researchers. However, it has remained a serious\nchallenge. Here, we first prove empirical limits on the recall (and F1-scores)\nof extractive summarizers on the DUC datasets under ROUGE evaluation for both\nthe single-document and multi-document summarization tasks. Next we define the\nconcept of compressibility of a document and present a new model of\nsummarization, which generalizes existing models in the literature and\nintegrates several dimensions of the summarization, viz., abstractive versus\nextractive, single versus multi-document, and syntactic versus semantic.\nFinally, we examine some new and existing single-document summarization\nalgorithms in a single framework and compare with state of the art summarizers\non DUC data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 22:21:22 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Verma", "Rakesh", ""], ["Lee", "Daniel", ""]]}, {"id": "1704.05571", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal", "title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial\n  Domain", "comments": "DSMM 2017 workshop at ACM SIGMOD conference", "journal-ref": null, "doi": "10.1145/3077240.3077249", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have made enormous inroads in recent years in a wide variety\nof text mining applications. In this paper, we explore a word embedding-based\narchitecture for predicting the relevance of a role between two financial\nentities within the context of natural language sentences. In this extended\nabstract, we propose a pooled approach that uses a collection of sentences to\ntrain word embeddings using the skip-gram word2vec architecture. We use the\nword embeddings to obtain context vectors that are assigned one or more labels\nbased on manual annotations. We train a machine learning classifier using the\nlabeled context vectors, and use the trained classifier to predict contextual\nrole relevance on test data. Our approach serves as a good minimal-expertise\nbaseline for the task as it is simple and intuitive, uses open-source modules,\nrequires little feature crafting effort and performs well across roles.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 00:55:23 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Kejriwal", "Mayank", ""]]}, {"id": "1704.05572", "submitter": "Tushar Khot", "authors": "Tushar Khot and Ashish Sabharwal and Peter Clark", "title": "Answering Complex Questions Using Open Information Extraction", "comments": "Accepted as short paper at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been substantial progress in factoid question-answering (QA),\nanswering complex questions remains challenging, typically requiring both a\nlarge body of knowledge and inference techniques. Open Information Extraction\n(Open IE) provides a way to generate semi-structured knowledge for QA, but to\ndate such knowledge has only been used to answer simple questions with\nretrieval-based methods. We overcome this limitation by presenting a method for\nreasoning with Open IE knowledge, allowing more complex questions to be\nhandled. Using a recently proposed support graph optimization framework for QA,\nwe develop a new inference model for Open IE, in particular one that can work\neffectively with multiple short facts, noise, and the relational structure of\ntuples. Our model significantly outperforms a state-of-the-art structured\nsolver on complex questions of varying difficulty, while also removing the\nreliance on manually curated knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 01:07:56 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Clark", "Peter", ""]]}, {"id": "1704.05579", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak, Nikunj Saunshi and Kiran Vodrahalli", "title": "A Large Self-Annotated Corpus for Sarcasm", "comments": "6 pages, 4 Figures. To Appear in LREC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Self-Annotated Reddit Corpus (SARC), a large corpus for\nsarcasm research and for training and evaluating systems for sarcasm detection.\nThe corpus has 1.3 million sarcastic statements -- 10 times more than any\nprevious dataset -- and many times more instances of non-sarcastic statements,\nallowing for learning in both balanced and unbalanced label regimes. Each\nstatement is furthermore self-annotated -- sarcasm is labeled by the author,\nnot an independent annotator -- and provided with user, topic, and conversation\ncontext. We evaluate the corpus for accuracy, construct benchmarks for sarcasm\ndetection, and evaluate baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:01:39 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 01:25:08 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 03:09:01 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 22:23:10 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Khodak", "Mikhail", ""], ["Saunshi", "Nikunj", ""], ["Vodrahalli", "Kiran", ""]]}, {"id": "1704.05611", "submitter": "Vijay Krishna Menon Mr", "authors": "Vijay Krishna Menon, S Rajendran, M Anandkumar, K P Soman", "title": "Dependency resolution and semantic mining using Tree Adjoining Grammars\n  for Tamil Language", "comments": "9 pages. arXiv admin note: text overlap with arXiv:1604.01235", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree adjoining grammars (TAGs) provide an ample tool to capture syntax of\nmany Indian languages. Tamil represents a special challenge to computational\nformalisms as it has extensive agglutinative morphology and a comparatively\ndifficult argument structure. Modelling Tamil syntax and morphology using TAG\nis an interesting problem which has not been in focus even though TAGs are over\n4 decades old, since its inception. Our research with Tamil TAGs have shown us\nthat we can not only represent syntax of the language, but to an extent mine\nout semantics through dependency resolution of the sentence. But in order to\ndemonstrate this phenomenal property, we need to parse Tamil language sentences\nusing TAGs we have built and through parsing obtain a derivation we could use\nto resolve dependencies, thus proving the semantic property. We use an in-house\ndeveloped pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi\n(1988), for generating derivations of sentences. We do not use any statistics\nto rank out ambiguous derivations but rather use all of them to understand the\nmentioned semantic relation with in TAGs for Tamil. We shall also present a\nbrief parser analysis for the completeness of our discussions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 05:02:05 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Menon", "Vijay Krishna", ""], ["Rajendran", "S", ""], ["Anandkumar", "M", ""], ["Soman", "K P", ""]]}, {"id": "1704.05742", "submitter": "Pengfei Liu", "authors": "Pengfei Liu and Xipeng Qiu and Xuanjing Huang", "title": "Adversarial Multi-task Learning for Text Classification", "comments": "Accepted by ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models have shown their promising opportunities for multi-task\nlearning, which focus on learning the shared layers to extract the common and\ntask-invariant features. However, in most existing approaches, the extracted\nshared features are prone to be contaminated by task-specific features or the\nnoise brought by other tasks. In this paper, we propose an adversarial\nmulti-task learning framework, alleviating the shared and private latent\nfeature spaces from interfering with each other. We conduct extensive\nexperiments on 16 different text classification tasks, which demonstrates the\nbenefits of our approach. Besides, we show that the shared knowledge learned by\nour proposed model can be regarded as off-the-shelf knowledge and easily\ntransferred to new tasks. The datasets of all 16 tasks are publicly available\nat \\url{http://nlp.fudan.edu.cn/data/}\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 14:17:25 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Liu", "Pengfei", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1704.05753", "submitter": "Jonathan K Kummerfeld", "authors": "Youxuan Jiang, Jonathan K. Kummerfeld and Walter S. Lasecki", "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase\n  Collection", "comments": "Published at ACL 2017", "journal-ref": "ACL (2017) 103-109", "doi": "10.18653/v1/P17-2017", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linguistically diverse datasets are critical for training and evaluating\nrobust machine learning systems, but data collection is a costly process that\noften requires experts. Crowdsourcing the process of paraphrase generation is\nan effective means of expanding natural language datasets, but there has been\nlimited analysis of the trade-offs that arise when designing tasks. In this\npaper, we present the first systematic study of the key factors in\ncrowdsourcing paraphrase collection. We consider variations in instructions,\nincentives, data domains, and workflows. We manually analyzed paraphrases for\ncorrectness, grammaticality, and linguistic diversity. Our observations provide\nnew insight into the trade-offs between accuracy and diversity in crowd\nresponses that arise as a result of task design, providing guidance for future\nparaphrase generation procedures.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 14:41:21 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 23:04:39 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Jiang", "Youxuan", ""], ["Kummerfeld", "Jonathan K.", ""], ["Lasecki", "Walter S.", ""]]}, {"id": "1704.05781", "submitter": "Andrey Kutuzov", "authors": "Pierre Lison, Andrey Kutuzov", "title": "Redefining Context Windows for Word Embedding Models: An Experimental\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributional semantic models learn vector representations of words through\nthe contexts they occur in. Although the choice of context (which often takes\nthe form of a sliding window) has a direct influence on the resulting\nembeddings, the exact role of this model component is still not fully\nunderstood. This paper presents a systematic analysis of context windows based\non a set of four distinct hyper-parameters. We train continuous Skip-Gram\nmodels on two English-language corpora for various combinations of these\nhyper-parameters, and evaluate them on both lexical similarity and analogy\ntasks. Notable experimental results are the positive impact of cross-sentential\ncontexts and the surprisingly good performance of right-context windows.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 15:41:34 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Lison", "Pierre", ""], ["Kutuzov", "Andrey", ""]]}, {"id": "1704.05907", "submitter": "Hongyu Guo", "authors": "Hongyu Guo and Colin Cherry and Jiang Su", "title": "End-to-End Multi-View Networks for Text Classification", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-view network for text classification. Our method\nautomatically creates various views of its input text, each taking the form of\nsoft attention weights that distribute the classifier's focus among a set of\nbase features. For a bag-of-words representation, each view focuses on a\ndifferent subset of the text's words. Aggregating many such views results in a\nmore discriminative and robust representation. Through a novel architecture\nthat both stacks and concatenates views, we produce a network that emphasizes\nboth depth and width, allowing training to converge quickly. Using our\nmulti-view architecture, we establish new state-of-the-art accuracies on two\nbenchmark tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 19:33:38 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Guo", "Hongyu", ""], ["Cherry", "Colin", ""], ["Su", "Jiang", ""]]}, {"id": "1704.05908", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy", "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "comments": "Accepted by ACL 2017. Minor update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are important resources for a variety of natural language\nprocessing tasks but suffer from incompleteness. We propose a novel embedding\nmodel, \\emph{ITransF}, to perform knowledge base completion. Equipped with a\nsparse attention mechanism, ITransF discovers hidden concepts of relations and\ntransfer statistical strength through the sharing of concepts. Moreover, the\nlearned associations between relations and concepts, which are represented by\nsparse attention vectors, can be interpreted easily. We evaluate ITransF on two\nbenchmark datasets---WN18 and FB15k for knowledge base completion and obtains\nimprovements on both the mean rank and Hits@10 metrics, over all baselines that\ndo not use additional information.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 19:35:54 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 05:20:09 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Xie", "Qizhe", ""], ["Ma", "Xuezhe", ""], ["Dai", "Zihang", ""], ["Hovy", "Eduard", ""]]}, {"id": "1704.05958", "submitter": "Yu Su", "authors": "Yu Su, Honglei Liu, Semih Yavuz, Izzeddin Gur, Huan Sun, Xifeng Yan", "title": "Global Relation Embedding for Relation Extraction", "comments": "Accepted to NAACL HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of textual relation embedding with distant supervision.\nTo combat the wrong labeling problem of distant supervision, we propose to\nembed textual relations with global statistics of relations, i.e., the\nco-occurrence statistics of textual and knowledge base relations collected from\nthe entire corpus. This approach turns out to be more robust to the training\nnoise introduced by distant supervision. On a popular relation extraction\ndataset, we show that the learned textual relation embedding can be used to\naugment existing relation extraction models and significantly improve their\nperformance. Most remarkably, for the top 1,000 relational facts discovered by\nthe best existing model, the precision can be improved from 83.9% to 89.3%.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 23:54:46 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 07:50:55 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Su", "Yu", ""], ["Liu", "Honglei", ""], ["Yavuz", "Semih", ""], ["Gur", "Izzeddin", ""], ["Sun", "Huan", ""], ["Yan", "Xifeng", ""]]}, {"id": "1704.05972", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and Kalina Bontcheva and Maria Liakata and Rob Procter\n  and Geraldine Wong Sak Hoi and Arkaitz Zubiaga", "title": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support\n  for rumours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media is full of false claims. Even Oxford Dictionaries named \"post-truth\" as\nthe word of 2016. This makes it more important than ever to build systems that\ncan identify the veracity of a story, and the kind of discourse there is around\nit. RumourEval is a SemEval shared task that aims to identify and handle\nrumours and reactions to them, in text. We present an annotation scheme, a\nlarge dataset covering multiple topics - each having their own families of\nclaims and replies - and use these to pose two concrete challenges as well as\nthe results achieved by participants on these challenges.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 01:21:20 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Derczynski", "Leon", ""], ["Bontcheva", "Kalina", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""], ["Hoi", "Geraldine Wong Sak", ""], ["Zubiaga", "Arkaitz", ""]]}, {"id": "1704.05973", "submitter": "Lin Wu", "authors": "Tong Chen, Lin Wu, Xue Li, Jun Zhang, Hongzhi Yin, Yang Wang", "title": "Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks\n  for Early Rumor Detection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of social media in communication and information\ndissemination has made it an ideal platform for spreading rumors. Automatically\ndebunking rumors at their stage of diffusion is known as \\textit{early rumor\ndetection}, which refers to dealing with sequential posts regarding disputed\nfactual claims with certain variations and highly textual duplication over\ntime. Thus, identifying trending rumors demands an efficient yet flexible model\nthat is able to capture long-range dependencies among postings and produce\ndistinct representations for the accurate early detection. However, it is a\nchallenging task to apply conventional classification algorithms to rumor\ndetection in earliness since they rely on hand-crafted features which require\nintensive manual efforts in the case of large amount of posts. This paper\npresents a deep attention model on the basis of recurrent neural networks (RNN)\nto learn \\textit{selectively} temporal hidden representations of sequential\nposts for identifying rumors. The proposed model delves soft-attention into the\nrecurrence to simultaneously pool out distinct features with particular focus\nand produce hidden representations that capture contextual variations of\nrelevant posts over time. Extensive experiments on real datasets collected from\nsocial media websites demonstrate that (1) the deep attention based RNN model\noutperforms state-of-the-arts that rely on hand-crafted features; (2) the\nintroduction of soft attention mechanism can effectively distill relevant parts\nto rumors from original posts in advance; (3) the proposed method detects\nrumors more quickly and accurately than competitors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 01:22:57 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Chen", "Tong", ""], ["Wu", "Lin", ""], ["Li", "Xue", ""], ["Zhang", "Jun", ""], ["Yin", "Hongzhi", ""], ["Wang", "Yang", ""]]}, {"id": "1704.05974", "submitter": "Yu Su", "authors": "Yu Su, Xifeng Yan", "title": "Cross-domain Semantic Parsing via Paraphrasing", "comments": "12 pages, 2 figures, accepted by EMNLP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies on semantic parsing mainly focus on the in-domain setting.\nWe formulate cross-domain semantic parsing as a domain adaptation problem:\ntrain a semantic parser on some source domains and then adapt it to the target\ndomain. Due to the diversity of logical forms in different domains, this\nproblem presents unique and intriguing challenges. By converting logical forms\ninto canonical utterances in natural language, we reduce semantic parsing to\nparaphrasing, and develop an attentive sequence-to-sequence paraphrase model\nthat is general and flexible to adapt to different domains. We discover two\nproblems, small micro variance and large macro variance, of pre-trained word\nembeddings that hinder their direct use in neural networks, and propose\nstandardization techniques as a remedy. On the popular Overnight dataset, which\ncontains eight domains, we show that both cross-domain training and\nstandardized pre-trained word embeddings can bring significant improvement.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 01:26:23 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 19:35:17 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Su", "Yu", ""], ["Yan", "Xifeng", ""]]}, {"id": "1704.06104", "submitter": "Steffen Eger", "authors": "Steffen Eger and Johannes Daxenberger and Iryna Gurevych", "title": "Neural End-to-End Learning for Computational Argumentation Mining", "comments": "To be published at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate neural techniques for end-to-end computational argumentation\nmining (AM). We frame AM both as a token-based dependency parsing and as a\ntoken-based sequence tagging problem, including a multi-task learning setup.\nContrary to models that operate on the argument component level, we find that\nframing AM as dependency parsing leads to subpar performance results. In\ncontrast, less complex (local) tagging models based on BiLSTMs perform robustly\nacross classification scenarios, being able to catch long-range dependencies\ninherent to the AM problem. Moreover, we find that jointly learning 'natural'\nsubtasks, in a multi-task learning setup, improves performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 12:20:43 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 12:20:45 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Eger", "Steffen", ""], ["Daxenberger", "Johannes", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1704.06125", "submitter": "Mathieu Cliche", "authors": "Mathieu Cliche", "title": "BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and\n  LSTMs", "comments": "Published in Proceedings of SemEval-2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our attempt at producing a state-of-the-art Twitter\nsentiment classifier using Convolutional Neural Networks (CNNs) and Long Short\nTerm Memory (LSTMs) networks. Our system leverages a large amount of unlabeled\ndata to pre-train word embeddings. We then use a subset of the unlabeled data\nto fine tune the embeddings using distant supervision. The final CNNs and LSTMs\nare trained on the SemEval-2017 Twitter dataset where the embeddings are fined\ntuned again. To boost performances we ensemble several CNNs and LSTMs together.\nOur approach achieved first rank on all of the five English subtasks amongst 40\nteams.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 13:10:25 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Cliche", "Mathieu", ""]]}, {"id": "1704.06194", "submitter": "Mo Yu", "authors": "Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos, Bing Xiang,\n  Bowen Zhou", "title": "Improved Neural Relation Detection for Knowledge Base Question Answering", "comments": "Accepted by ACL 2017 (updated for camera-ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation detection is a core component for many NLP applications including\nKnowledge Base Question Answering (KBQA). In this paper, we propose a\nhierarchical recurrent neural network enhanced by residual learning that\ndetects KB relations given an input question. Our method uses deep residual\nbidirectional LSTMs to compare questions and relation names via different\nhierarchies of abstraction. Additionally, we propose a simple KBQA system that\nintegrates entity linking and our proposed relation detector to enable one\nenhance another. Experimental results evidence that our approach achieves not\nonly outstanding relation detection performance, but more importantly, it helps\nour KBQA system to achieve state-of-the-art accuracy for both single-relation\n(SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 15:48:05 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 17:45:35 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yu", "Mo", ""], ["Yin", "Wenpeng", ""], ["Hasan", "Kazi Saidul", ""], ["Santos", "Cicero dos", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1704.06217", "submitter": "Ji He", "authors": "Ji He, Mari Ostendorf, Xiaodong He", "title": "Reinforcement Learning with External Knowledge and Two-Stage Q-functions\n  for Predicting Popular Reddit Threads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of predicting popularity of comments in an\nonline discussion forum using reinforcement learning, particularly addressing\ntwo challenges that arise from having natural language state and action spaces.\nFirst, the state representation, which characterizes the history of comments\ntracked in a discussion at a particular point, is augmented to incorporate the\nglobal context represented by discussions on world events available in an\nexternal knowledge source. Second, a two-stage Q-learning framework is\nintroduced, making it feasible to search the combinatorial action space while\nalso accounting for redundancy among sub-actions. We experiment with five\nReddit communities, showing that the two methods improve over previous reported\nresults on this task.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 16:30:39 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["He", "Ji", ""], ["Ostendorf", "Mari", ""], ["He", "Xiaodong", ""]]}, {"id": "1704.06259", "submitter": "Ping Chen Dr.", "authors": "Ping Chen, Fei Wu, Tong Wang, Wei Ding", "title": "A Semantic QA-Based Approach for Text Summarization Evaluation", "comments": null, "journal-ref": "AAAI 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Natural Language Processing and Computational Linguistics applications\ninvolves the generation of new texts based on some existing texts, such as\nsummarization, text simplification and machine translation. However, there has\nbeen a serious problem haunting these applications for decades, that is, how to\nautomatically and accurately assess quality of these applications. In this\npaper, we will present some preliminary results on one especially useful and\nchallenging problem in NLP system evaluation: how to pinpoint content\ndifferences of two text passages (especially for large pas-sages such as\narticles and books). Our idea is intuitive and very different from existing\napproaches. We treat one text passage as a small knowledge base, and ask it a\nlarge number of questions to exhaustively identify all content points in it. By\ncomparing the correctly answered questions from two text passages, we will be\nable to compare their content precisely. The experiment using 2007 DUC\nsummarization corpus clearly shows promising results.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 15:32:01 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 00:34:01 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Chen", "Ping", ""], ["Wu", "Fei", ""], ["Wang", "Tong", ""], ["Ding", "Wei", ""]]}, {"id": "1704.06358", "submitter": "Paul Tupper", "authors": "Benjamin Goodman, Paul Tupper", "title": "Stability and Fluctuations in a Simple Model of Phonetic Category Change", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spoken languages, speakers divide up the space of phonetic possibilities\ninto different regions, corresponding to different phonemes. We consider a\nsimple exemplar model of how this division of phonetic space varies over time\namong a population of language users. In the particular model we consider, we\nshow that, once the system is initialized with a given set of phonemes, that\nphonemes do not become extinct: all phonemes will be maintained in the system\nfor all time. This is in contrast to what is observed in more complex models.\nFurthermore, we show that the boundaries between phonemes fluctuate and we\nquantitatively study the fluctuations in a simple instance of our model. These\nresults prepare the ground for more sophisticated models in which some phonemes\ngo extinct or new phonemes emerge through other processes.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 22:28:14 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 04:46:08 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 00:23:20 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Goodman", "Benjamin", ""], ["Tupper", "Paul", ""]]}, {"id": "1704.06360", "submitter": "Jason Fries", "authors": "Jason Fries, Sen Wu, Alex Ratner, Christopher R\\'e", "title": "SwellShark: A Generative Model for Biomedical Named Entity Recognition\n  without Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SwellShark, a framework for building biomedical named entity\nrecognition (NER) systems quickly and without hand-labeled data. Our approach\nviews biomedical resources like lexicons as function primitives for\nautogenerating weak supervision. We then use a generative model to unify and\ndenoise this supervision and construct large-scale, probabilistically labeled\ndatasets for training high-accuracy NER taggers. In three biomedical NER tasks,\nSwellShark achieves competitive scores with state-of-the-art supervised\nbenchmarks using no hand-labeled training data. In a drug name extraction task\nusing patient medical records, one domain expert using SwellShark achieved\nwithin 5.1% of a crowdsourced annotation approach -- which originally utilized\n20 teams over the course of several weeks -- in 24 hours.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 23:02:14 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Fries", "Jason", ""], ["Wu", "Sen", ""], ["Ratner", "Alex", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1704.06380", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Mari Ostendorf", "title": "Improving Context Aware Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased adaptability of RNN language models leads to improved predictions\nthat benefit many applications. However, current methods do not take full\nadvantage of the RNN structure. We show that the most widely-used approach to\nadaptation (concatenating the context with the word embedding at the input to\nthe recurrent layer) is outperformed by a model that has some low-cost\nimprovements: adaptation of both the hidden and output layers. and a feature\nhashing bias term to capture context idiosyncrasies. Experiments on language\nmodeling and classification tasks using three different corpora demonstrate the\nadvantages of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 02:27:26 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Jaech", "Aaron", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1704.06393", "submitter": "Jiajun Zhang", "authors": "Long Zhou, Wenpeng Hu, Jiajun Zhang and Chengqing Zong", "title": "Neural System Combination for Machine Translation", "comments": "Accepted as a short paper by ACL-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) becomes a new approach to machine\ntranslation and generates much more fluent results compared to statistical\nmachine translation (SMT).\n  However, SMT is usually better than NMT in translation adequacy. It is\ntherefore a promising direction to combine the advantages of both NMT and SMT.\n  In this paper, we propose a neural system combination framework leveraging\nmulti-source NMT, which takes as input the outputs of NMT and SMT systems and\nproduces the final translation.\n  Extensive experiments on the Chinese-to-English translation task show that\nour model archives significant improvement by 5.3 BLEU points over the best\nsingle system output and 3.4 BLEU points over the state-of-the-art traditional\nsystem combination methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 04:36:55 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Zhou", "Long", ""], ["Hu", "Wenpeng", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1704.06485", "submitter": "Byeongchang Kim", "authors": "Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim", "title": "Attend to You: Personalized Image Captioning with Context Sequence\n  Memory Networks", "comments": "Accepted paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address personalization issues of image captioning, which have not been\ndiscussed yet in previous research. For a query image, we aim to generate a\ndescriptive sentence, accounting for prior knowledge such as the user's active\nvocabularies in previous documents. As applications of personalized image\ncaptioning, we tackle two post automation tasks: hashtag prediction and post\ngeneration, on our newly collected Instagram dataset, consisting of 1.1M posts\nfrom 6.3K users. We propose a novel captioning model named Context Sequence\nMemory Network (CSMN). Its unique updates over previous memory network models\ninclude (i) exploiting memory as a repository for multiple types of context\ninformation, (ii) appending previously generated words into memory to capture\nlong-term information without suffering from the vanishing gradient problem,\nand (iii) adopting CNN memory structure to jointly represent nearby ordered\nmemory slots for better context understanding. With quantitative evaluation and\nuser studies via Amazon Mechanical Turk, we show the effectiveness of the three\nnovel features of CSMN and its performance enhancement for personalized image\ncaptioning over state-of-the-art captioning models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:29:07 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 23:30:43 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Park", "Cesc Chunseong", ""], ["Kim", "Byeongchang", ""], ["Kim", "Gunhee", ""]]}, {"id": "1704.06497", "submitter": "Julia Kreutzer", "authors": "Julia Kreutzer, Artem Sokolov, Stefan Riezler", "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit structured prediction describes a stochastic optimization framework\nwhere learning is performed from partial feedback. This feedback is received in\nthe form of a task loss evaluation to a predicted output structure, without\nhaving access to gold standard structures. We advance this framework by lifting\nlinear bandit learning to neural sequence-to-sequence learning problems using\nattention-based recurrent neural networks. Furthermore, we show how to\nincorporate control variates into our learning algorithms for variance\nreduction and improved generalization. We present an evaluation on a neural\nmachine translation task that shows improvements of up to 5.89 BLEU points for\ndomain adaptation from simulated bandit feedback.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 11:56:00 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 17:00:18 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Kreutzer", "Julia", ""], ["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""]]}, {"id": "1704.06567", "submitter": "Jind\\v{r}ich Helcl", "authors": "Jind\\v{r}ich Libovick\\'y and Jind\\v{r}ich Helcl", "title": "Attention Strategies for Multi-Source Sequence-to-Sequence Learning", "comments": "7 pages; Accepted to ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling attention in neural multi-source sequence-to-sequence learning\nremains a relatively unexplored area, despite its usefulness in tasks that\nincorporate multiple source languages or modalities. We propose two novel\napproaches to combine the outputs of attention mechanisms over each source\nsequence, flat and hierarchical. We compare the proposed methods with existing\ntechniques and present results of systematic evaluation of those methods on the\nWMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the\nproposed methods achieve competitive results on both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 14:39:27 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Helcl", "Jind\u0159ich", ""]]}, {"id": "1704.06619", "submitter": "Arman Cohan", "authors": "Arman Cohan and Nazli Goharian", "title": "Scientific Article Summarization Using Citation-Context and Article's\n  Discourse Structure", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a summarization approach for scientific articles which takes\nadvantage of citation-context and the document discourse model. While citations\nhave been previously used in generating scientific summaries, they lack the\nrelated context from the referenced article and therefore do not accurately\nreflect the article's content. Our method overcomes the problem of\ninconsistency between the citation summary and the article's content by\nproviding context for each citation. We also leverage the inherent scientific\narticle's discourse for producing better summaries. We show that our proposed\nmethod effectively improves over existing summarization approaches (greater\nthan 30% improvement over the best performing baseline) in terms of\n\\textsc{Rouge} scores on TAC2014 scientific summarization dataset. While the\ndataset we use for evaluation is in the biomedical domain, most of our\napproaches are general and therefore adaptable to other domains.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:17:58 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1704.06692", "submitter": "Thomas Kober", "authors": "Thomas Kober, Julie Weeds, Jeremy Reffin and David Weir", "title": "Improving Semantic Composition with Offset Inference", "comments": "to appear at ACL 2017 (short papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count-based distributional semantic models suffer from sparsity due to\nunobserved but plausible co-occurrences in any text collection. This problem is\namplified for models like Anchored Packed Trees (APTs), that take the\ngrammatical type of a co-occurrence into account. We therefore introduce a\nnovel form of distributional inference that exploits the rich type structure in\nAPTs and infers missing data by the same mechanism that is used for semantic\ncomposition.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:47:30 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Kober", "Thomas", ""], ["Weeds", "Julie", ""], ["Reffin", "Jeremy", ""], ["Weir", "David", ""]]}, {"id": "1704.06779", "submitter": "Nafise Sadat Moosavi", "authors": "Nafise Sadat Moosavi and Michael Strube", "title": "Lexical Features in Coreference Resolution: To be Used With Caution", "comments": "6 pages, ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical features are a major source of information in state-of-the-art\ncoreference resolvers. Lexical features implicitly model some of the linguistic\nphenomena at a fine granularity level. They are especially useful for\nrepresenting the context of mentions. In this paper we investigate a drawback\nof using many lexical features in state-of-the-art coreference resolvers. We\nshow that if coreference resolvers mainly rely on lexical features, they can\nhardly generalize to unseen domains. Furthermore, we show that the current\ncoreference resolution evaluation is clearly flawed by only evaluating on a\nspecific split of a specific dataset in which there is a notable overlap\nbetween the training, development and test sets.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 09:59:42 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Moosavi", "Nafise Sadat", ""], ["Strube", "Michael", ""]]}, {"id": "1704.06836", "submitter": "Lotem Peled", "authors": "Lotem Peled and Roi Reichart", "title": "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm is a form of speech in which speakers say the opposite of what they\ntruly mean in order to convey a strong sentiment. In other words, \"Sarcasm is\nthe giant chasm between what I say, and the person who doesn't get it.\". In\nthis paper we present the novel task of sarcasm interpretation, defined as the\ngeneration of a non-sarcastic utterance conveying the same message as the\noriginal sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets,\neach interpreted by five human judges. Addressing the task as monolingual\nmachine translation (MT), we experiment with MT algorithms and evaluation\nmeasures. We then present SIGN: an MT based sarcasm interpretation algorithm\nthat targets sentiment words, a defining element of textual sarcasm. We show\nthat while the scores of n-gram based automatic measures are similar for all\ninterpretation models, SIGN's interpretations are scored higher by humans for\nadequacy and sentiment polarity. We conclude with a discussion on future\nresearch directions for our new task.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 18:59:25 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Peled", "Lotem", ""], ["Reichart", "Roi", ""]]}, {"id": "1704.06841", "submitter": "Toyotaro Suzumura Prof", "authors": "Mark Hughes, Irene Li, Spyros Kotoulas, Toyotaro Suzumura", "title": "Medical Text Classification using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach to automatically classify clinical text at a sentence\nlevel. We are using deep convolutional neural networks to represent complex\nfeatures. We train the network on a dataset providing a broad categorization of\nhealth information. Through a detailed evaluation, we demonstrate that our\nmethod outperforms several approaches widely used in natural language\nprocessing tasks by about 15%.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 19:39:32 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hughes", "Mark", ""], ["Li", "Irene", ""], ["Kotoulas", "Spyros", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1704.06851", "submitter": "Stefan Scherer", "authors": "Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency,\n  Stefan Scherer", "title": "Affect-LM: A Neural Language Model for Customizable Affective Text\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human verbal communication includes affective messages which are conveyed\nthrough use of emotionally colored words. There has been a lot of research in\nthis direction but the problem of integrating state-of-the-art neural language\nmodels with affective information remains an area ripe for exploration. In this\npaper, we propose an extension to an LSTM (Long Short-Term Memory) language\nmodel for generating conversational text, conditioned on affect categories. Our\nproposed model, Affect-LM enables us to customize the degree of emotional\ncontent in generated sentences through an additional design parameter.\nPerception studies conducted using Amazon Mechanical Turk show that Affect-LM\ngenerates naturally looking emotional sentences without sacrificing grammatical\ncorrectness. Affect-LM also learns affect-discriminative word representations,\nand perplexity experiments show that additional affective information in\nconversational text can improve language model prediction.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 21:10:10 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Ghosh", "Sayan", ""], ["Chollet", "Mathieu", ""], ["Laksana", "Eugene", ""], ["Morency", "Louis-Philippe", ""], ["Scherer", "Stefan", ""]]}, {"id": "1704.06855", "submitter": "Hao Peng", "authors": "Hao Peng, Sam Thomson, Noah A. Smith", "title": "Deep Multitask Learning for Semantic Dependency Parsing", "comments": "Proceedings of ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural architecture that parses sentences into three\nsemantic dependency graph formalisms. By using efficient, nearly arc-factored\ninference and a bidirectional-LSTM composed with a multi-layer perceptron, our\nbase system is able to significantly improve the state of the art for semantic\ndependency parsing, without using hand-engineered features or syntax. We then\nexplore two multitask learning approaches---one that shares parameters across\nformalisms, and one that uses higher-order structures to predict the graphs\njointly. We find that both approaches improve performance across formalisms on\naverage, achieving a new state of the art. Our code is open-source and\navailable at https://github.com/Noahs-ARK/NeurboParser.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 22:56:04 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 19:15:03 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Peng", "Hao", ""], ["Thomson", "Sam", ""], ["Smith", "Noah A.", ""]]}, {"id": "1704.06869", "submitter": "Vlad Niculae", "authors": "Vlad Niculae, Joonsuk Park, Claire Cardie", "title": "Argument Mining with Structured SVMs and RNNs", "comments": "Accepted for publication at ACL 2017. 11 pages, 5 figures. Code at\n  https://github.com/vene/marseille and data at http://joonsuk.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel factor graph model for argument mining, designed for\nsettings in which the argumentative relations in a document do not necessarily\nform a tree structure. (This is the case in over 20% of the web comments\ndataset we release.) Our model jointly learns elementary unit type\nclassification and argumentative relation prediction. Moreover, our model\nsupports SVM and RNN parametrizations, can enforce structure constraints (e.g.,\ntransitivity), and can express dependencies between adjacent relations and\npropositions. Our approaches outperform unstructured baselines in both web\ncomments and argumentative essay datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 01:14:55 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Niculae", "Vlad", ""], ["Park", "Joonsuk", ""], ["Cardie", "Claire", ""]]}, {"id": "1704.06877", "submitter": "Adams Wei Yu", "authors": "Adams Wei Yu, Hongrae Lee, Quoc V. Le", "title": "Learning to Skim Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks are showing much promise in many sub-areas of\nnatural language processing, ranging from document classification to machine\ntranslation to automatic question answering. Despite their promise, many\nrecurrent models have to read the whole text word by word, making it slow to\nhandle long documents. For example, it is difficult to use a recurrent network\nto read a book and answer questions about it. In this paper, we present an\napproach of reading text while skipping irrelevant information if needed. The\nunderlying model is a recurrent network that learns how far to jump after\nreading a few words of the input text. We employ a standard policy gradient\nmethod to train the model to make discrete jumping decisions. In our benchmarks\non four different tasks, including number prediction, sentiment analysis, news\narticle classification and automatic Q\\&A, our proposed model, a modified LSTM\nwith jumping, is up to 6 times faster than the standard sequential LSTM, while\nmaintaining the same or even better accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 03:54:22 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 19:58:31 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yu", "Adams Wei", ""], ["Lee", "Hongrae", ""], ["Le", "Quoc V.", ""]]}, {"id": "1704.06879", "submitter": "Rui Meng", "authors": "Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky,\n  Yu Chi", "title": "Deep Keyphrase Generation", "comments": "Accepted by ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase provides highly-condensed information that can be effectively used\nfor understanding, organizing and retrieving text content. Though previous\nstudies have provided many workable solutions for automated keyphrase\nextraction, they commonly divided the to-be-summarized content into multiple\ntext chunks, then ranked and selected the most meaningful ones. These\napproaches could neither identify keyphrases that do not appear in the text,\nnor capture the real semantic meaning behind the text. We propose a generative\nmodel for keyphrase prediction with an encoder-decoder framework, which can\neffectively overcome the above drawbacks. We name it as deep keyphrase\ngeneration since it attempts to capture the deep semantic meaning of the\ncontent with a deep learning method. Empirical analysis on six datasets\ndemonstrates that our proposed model not only achieves a significant\nperformance boost on extracting keyphrases that appear in the source text, but\nalso can generate absent keyphrases based on the semantic meaning of the text.\nCode and dataset are available at\nhttps://github.com/memray/OpenNMT-kpg-release.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 04:34:26 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 21:24:51 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 22:34:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Meng", "Rui", ""], ["Zhao", "Sanqiang", ""], ["Han", "Shuguang", ""], ["He", "Daqing", ""], ["Brusilovsky", "Peter", ""], ["Chi", "Yu", ""]]}, {"id": "1704.06913", "submitter": "Rahma Chaabouni", "authors": "Rahma Chaabouni, Ewan Dunbar, Neil Zeghidour, Emmanuel Dupoux", "title": "Learning weakly supervised multimodal phoneme embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have explored deep architectures for learning multimodal speech\nrepresentation (e.g. audio and images, articulation and audio) in a supervised\nway. Here we investigate the role of combining different speech modalities,\ni.e. audio and visual information representing the lips movements, in a weakly\nsupervised way using Siamese networks and lexical same-different side\ninformation. In particular, we ask whether one modality can benefit from the\nother to provide a richer representation for phone recognition in a weakly\nsupervised setting. We introduce mono-task and multi-task methods for merging\nspeech and visual modalities for phone recognition. The mono-task learning\nconsists in applying a Siamese network on the concatenation of the two\nmodalities, while the multi-task learning receives several different\ncombinations of modalities at train time. We show that multi-task learning\nenhances discriminability for visual and multimodal inputs while minimally\nimpacting auditory inputs. Furthermore, we present a qualitative analysis of\nthe obtained phone embeddings, and show that cross-modal visual input can\nimprove the discriminability of phonological features which are visually\ndiscernable (rounding, open/close, labial place of articulation), resulting in\nrepresentations that are closer to abstract linguistic features than those\nbased on audio only.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 11:27:53 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 12:21:22 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Chaabouni", "Rahma", ""], ["Dunbar", "Ewan", ""], ["Zeghidour", "Neil", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1704.06918", "submitter": "Yusuke Oda", "authors": "Yusuke Oda, Philip Arthur, Graham Neubig, Koichiro Yoshino, Satoshi\n  Nakamura", "title": "Neural Machine Translation via Binary Code Prediction", "comments": "Accepted as a long paper at ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method for calculating the output layer in\nneural machine translation systems. The method is based on predicting a binary\ncode for each word and can reduce computation time/memory requirements of the\noutput layer to be logarithmic in vocabulary size in the best case. In\naddition, we also introduce two advanced approaches to improve the robustness\nof the proposed model: using error-correcting codes and combining softmax and\nbinary codes. Experiments on two English-Japanese bidirectional translation\ntasks show proposed models achieve BLEU scores that approach the softmax, while\nreducing memory usage to the order of less than 1/10 and improving decoding\nspeed on CPUs by x5 to x10.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 12:38:13 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Oda", "Yusuke", ""], ["Arthur", "Philip", ""], ["Neubig", "Graham", ""], ["Yoshino", "Koichiro", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1704.06933", "submitter": "Lijun Wu", "authors": "Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai,\n  Tie-Yan Liu", "title": "Adversarial Neural Machine Translation", "comments": "ACML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish$\\rightarrow$French and German$\\rightarrow$English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 05:08:47 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 13:35:31 GMT"}, {"version": "v3", "created": "Sat, 24 Jun 2017 03:29:54 GMT"}, {"version": "v4", "created": "Sun, 30 Sep 2018 14:04:21 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wu", "Lijun", ""], ["Xia", "Yingce", ""], ["Zhao", "Li", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Lai", "Jianhuang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1704.06936", "submitter": "Masashi Yoshikawa", "authors": "Masashi Yoshikawa and Hiroshi Noji and Yuji Matsumoto", "title": "A* CCG Parsing with a Supertag and Dependency Factored Model", "comments": "long paper (11 pages) accepted to ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new A* CCG parsing model in which the probability of a tree is\ndecomposed into factors of CCG categories and its syntactic dependencies both\ndefined on bi-directional LSTMs. Our factored model allows the precomputation\nof all probabilities and runs very efficiently, while modeling sentence\nstructures explicitly via dependencies. Our model achieves the state-of-the-art\nresults on English and Japanese CCG parsing.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 15:16:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Yoshikawa", "Masashi", ""], ["Noji", "Hiroshi", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1704.06956", "submitter": "Sida Wang", "authors": "Sida I. Wang and Samuel Ginn and Percy Liang and Christoper D. Manning", "title": "Naturalizing a Programming Language via Interactive Learning", "comments": "10 pages, ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to create a convenient natural language interface for performing\nwell-specified but complex actions such as analyzing data, manipulating text,\nand querying databases. However, existing natural language interfaces for such\ntasks are quite primitive compared to the power one wields with a programming\nlanguage. To bridge this gap, we start with a core programming language and\nallow users to \"naturalize\" the core language incrementally by defining\nalternative, more natural syntax and increasingly complex concepts in terms of\ncompositions of simpler ones. In a voxel world, we show that a community of\nusers can simultaneously teach a common system a diverse language and use it to\nbuild hundreds of complex voxel structures. Over the course of three days,\nthese users went from using only the core language to using the naturalized\nlanguage in 85.9\\% of the last 10K utterances.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 18:13:10 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wang", "Sida I.", ""], ["Ginn", "Samuel", ""], ["Liang", "Percy", ""], ["Manning", "Christoper D.", ""]]}, {"id": "1704.06960", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Anca Dragan, Dan Klein", "title": "Translating Neuralese", "comments": "Fixes typos and cleans ups some model presentation details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches have recently been proposed for learning decentralized\ndeep multiagent policies that coordinate via a differentiable communication\nchannel. While these policies are effective for many tasks, interpretation of\ntheir induced communication strategies has remained a challenge. Here we\npropose to interpret agents' messages by translating them. Unlike in typical\nmachine translation problems, we have no parallel data to learn from. Instead\nwe develop a translation model based on the insight that agent messages and\nnatural language strings mean the same thing if they induce the same belief\nabout the world in a listener. We present theoretical guarantees and empirical\nevidence that our approach preserves both the semantics and pragmatics of\nmessages by ensuring that players communicating through a translation layer do\nnot suffer a substantial loss in reward relative to players with a common\nlanguage.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 18:46:42 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 21:28:48 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 15:10:24 GMT"}, {"version": "v4", "created": "Sat, 6 Jan 2018 15:29:58 GMT"}, {"version": "v5", "created": "Sat, 22 Dec 2018 19:13:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Andreas", "Jacob", ""], ["Dragan", "Anca", ""], ["Klein", "Dan", ""]]}, {"id": "1704.06970", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer and Taylor Berg-Kirkpatrick", "title": "Differentiable Scheduled Sampling for Credit Assignment", "comments": "Accepted at ACL2017 (http://bit.ly/2oj1muX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a continuous relaxation of the argmax operation can be\nused to create a differentiable approximation to greedy decoding for\nsequence-to-sequence (seq2seq) models. By incorporating this approximation into\nthe scheduled sampling training procedure (Bengio et al., 2015)--a well-known\ntechnique for correcting exposure bias--we introduce a new training objective\nthat is continuous and differentiable everywhere and that can provide\ninformative gradients near points where previous decoding decisions change\ntheir value. In addition, by using a related approximation, we demonstrate a\nsimilar approach to sampled-based training. Finally, we show that our approach\noutperforms cross-entropy training and scheduled sampling procedures in two\nsequence prediction tasks: named entity recognition and machine translation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:05:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1704.06986", "submitter": "Kazuya Kawakami", "authors": "Kazuya Kawakami, Chris Dyer, Phil Blunsom", "title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language\n  Modeling", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixed-vocabulary language models fail to account for one of the most\ncharacteristic statistical facts of natural language: the frequent creation and\nreuse of new word types. Although character-level language models offer a\npartial solution in that they can create word types not attested in the\ntraining corpus, they do not capture the \"bursty\" distribution of such words.\nIn this paper, we augment a hierarchical LSTM language model that generates\nsequences of word tokens character by character with a caching mechanism that\nlearns to reuse previously generated words. To validate our model we construct\na new open-vocabulary language modeling corpus (the Multilingual Wikipedia\nCorpus, MWC) from comparable Wikipedia articles in 7 typologically diverse\nlanguages and demonstrate the effectiveness of our model across this range of\nlanguages.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 21:31:22 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Kawakami", "Kazuya", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1704.07047", "submitter": "Deng Cai", "authors": "Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu, Feiyue Huang", "title": "Fast and Accurate Neural Word Segmentation for Chinese", "comments": "To appear in ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models with minimal feature engineering have achieved competitive\nperformance against traditional methods for the task of Chinese word\nsegmentation. However, both training and working procedures of the current\nneural models are computationally inefficient. This paper presents a greedy\nneural word segmenter with balanced word and character embedding inputs to\nalleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of\nperforming segmentation much faster and even more accurate than\nstate-of-the-art neural models on Chinese benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 05:50:29 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Cai", "Deng", ""], ["Zhao", "Hai", ""], ["Zhang", "Zhisong", ""], ["Xin", "Yuan", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""]]}, {"id": "1704.07050", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Using Global Constraints and Reranking to Improve Cognates Detection", "comments": "10 pages, 6 figures, 6 tables; published in the Proceedings of the\n  55th Annual Meeting of the Association for Computational Linguistics, pages\n  1983-1992, Vancouver, Canada, July 2017", "journal-ref": "In Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics, pages 1983-1992, Vancouver, Canada, July 2017.\n  Association for Computational Linguistics", "doi": "10.18653/v1/P17-1181", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global constraints and reranking have not been used in cognates detection\nresearch to date. We propose methods for using global constraints by performing\nrescoring of the score matrices produced by state of the art cognates detection\nsystems. Using global constraints to perform rescoring is complementary to\nstate of the art methods for performing cognates detection and results in\nsignificant performance improvements beyond current state of the art\nperformance on publicly available datasets with different language pairs and\nvarious conditions such as different levels of baseline state of the art\nperformance and different data size conditions, including with more realistic\nlarge data size conditions than have been evaluated with in the past.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 06:04:50 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 20:19:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1704.07073", "submitter": "Qingyu Zhou", "authors": "Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou", "title": "Selective Encoding for Abstractive Sentence Summarization", "comments": "10 pages; To appear in ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1101", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a selective encoding model to extend the sequence-to-sequence\nframework for abstractive sentence summarization. It consists of a sentence\nencoder, a selective gate network, and an attention equipped decoder. The\nsentence encoder and decoder are built with recurrent neural networks. The\nselective gate network constructs a second level sentence representation by\ncontrolling the information flow from encoder to decoder. The second level\nrepresentation is tailored for sentence summarization task, which leads to\nbetter performance. We evaluate our model on the English Gigaword, DUC 2004 and\nMSR abstractive sentence summarization datasets. The experimental results show\nthat the proposed selective encoding model outperforms the state-of-the-art\nbaseline models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 07:57:37 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Zhou", "Qingyu", ""], ["Yang", "Nan", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "1704.07092", "submitter": "Jan Buys", "authors": "Jan Buys and Phil Blunsom", "title": "Robust Incremental Neural Semantic Graph Parsing", "comments": "12 pages; ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing sentences to linguistically-expressive semantic representations is a\nkey goal of Natural Language Processing. Yet statistical parsing has focused\nalmost exclusively on bilexical dependencies or domain-specific logical forms.\nWe propose a neural encoder-decoder transition-based parser which is the first\nfull-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The\nmodel architecture uses stack-based embedding features, predicting graphs\njointly with unlexicalized predicates and their token alignments. Our parser is\nmore accurate than attention-based baselines on MRS, and on an additional\nAbstract Meaning Representation (AMR) benchmark, and GPU batch processing makes\nit an order of magnitude faster than a high-precision grammar-based parser.\nFurther, the 86.69% Smatch score of our MRS parser is higher than the\nupper-bound on AMR parsing, making MRS an attractive choice as a semantic\nrepresentation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 08:50:15 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 15:39:41 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Buys", "Jan", ""], ["Blunsom", "Phil", ""]]}, {"id": "1704.07121", "submitter": "Hexiang Hu", "authors": "Wei-Lun Chao, Hexiang Hu, Fei Sha", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better\n  Visual Question Answering Datasets", "comments": "Accepted for Oral Presentation at NAACL-HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (Visual QA) has attracted a lot of attention\nlately, seen essentially as a form of (visual) Turing test that artificial\nintelligence should strive to achieve. In this paper, we study a crucial\ncomponent of this task: how can we design good datasets for the task? We focus\non the design of multiple-choice based datasets where the learner has to select\nthe right answer from a set of candidate ones including the target (\\ie the\ncorrect one) and the decoys (\\ie the incorrect ones). Through careful analysis\nof the results attained by state-of-the-art learning models and human\nannotators on existing datasets, we show that the design of the decoy answers\nhas a significant impact on how and what the learning models learn from the\ndatasets. In particular, the resulting learner can ignore the visual\ninformation, the question, or both while still doing well on the task. Inspired\nby this, we propose automatic procedures to remedy such design deficiencies. We\napply the procedures to re-construct decoy answers for two popular Visual QA\ndatasets as well as to create a new Visual QA dataset from the Visual Genome\nproject, resulting in the largest dataset for this task. Extensive empirical\nstudies show that the design deficiencies have been alleviated in the remedied\ndatasets and the performance on them is likely a more faithful indicator of the\ndifference among learning models. The datasets are released and publicly\navailable via http://www.teds.usc.edu/website_vqa/.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:05:19 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 20:34:21 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chao", "Wei-Lun", ""], ["Hu", "Hexiang", ""], ["Sha", "Fei", ""]]}, {"id": "1704.07129", "submitter": "Spandana Gella", "authors": "Spandana Gella, Frank Keller", "title": "An Analysis of Action Recognition Datasets for Language and Vision Tasks", "comments": "To appear in Proceedings of ACL 2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of recent research has focused on tasks that combine language\nand vision, resulting in a proliferation of datasets and methods. One such task\nis action recognition, whose applications include image annotation, scene\nunder- standing and image retrieval. In this survey, we categorize the existing\nap- proaches based on how they conceptualize this problem and provide a\ndetailed review of existing datasets, highlighting their di- versity as well as\nadvantages and disad- vantages. We focus on recently devel- oped datasets which\nlink visual informa- tion with linguistic resources and provide a fine-grained\nsyntactic and semantic anal- ysis of actions in images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:38:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Gella", "Spandana", ""], ["Keller", "Frank", ""]]}, {"id": "1704.07130", "submitter": "He He", "authors": "He He and Anusha Balakrishnan and Mihail Eric and Percy Liang", "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge\n  Graph Embeddings", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a symmetric collaborative dialogue setting in which two agents, each\nwith private knowledge, must strategically communicate to achieve a common\ngoal. The open-ended dialogue state in this setting poses new challenges for\nexisting dialogue systems. We collected a dataset of 11K human-human dialogues,\nwhich exhibits interesting lexical, semantic, and strategic elements. To model\nboth structured knowledge and unstructured language, we propose a neural model\nwith dynamic knowledge graph embeddings that evolve as the dialogue progresses.\nAutomatic and human evaluations show that our model is both more effective at\nachieving the goal and more human-like than baseline neural and rule-based\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:38:24 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["He", "He", ""], ["Balakrishnan", "Anusha", ""], ["Eric", "Mihail", ""], ["Liang", "Percy", ""]]}, {"id": "1704.07138", "submitter": "Chris Hokamp", "authors": "Chris Hokamp and Qun Liu", "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam\n  Search", "comments": "Accepted as a long paper at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Grid Beam Search (GBS), an algorithm which extends beam search to\nallow the inclusion of pre-specified lexical constraints. The algorithm can be\nused with any model that generates a sequence $ \\mathbf{\\hat{y}} =\n\\{y_{0}\\ldots y_{T}\\} $, by maximizing $ p(\\mathbf{y} | \\mathbf{x}) =\n\\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) $. Lexical\nconstraints take the form of phrases or words that must be present in the\noutput sequence. This is a very general way to incorporate additional knowledge\ninto a model's output without requiring any modification of the model\nparameters or training data. We demonstrate the feasibility and flexibility of\nLexically Constrained Decoding by conducting experiments on Neural\nInteractive-Predictive Translation, as well as Domain Adaptation for Neural\nMachine Translation. Experiments show that GBS can provide large improvements\nin translation quality in interactive scenarios, and that, even without any\nuser input, GBS can be used to achieve significant gains in performance in\ndomain adaptation scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 10:55:20 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 13:52:08 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Hokamp", "Chris", ""], ["Liu", "Qun", ""]]}, {"id": "1704.07146", "submitter": "Ella Rabinovich", "authors": "Ella Rabinovich, Noam Ordan, Shuly Wintner", "title": "Found in Translation: Reconstructing Phylogenetic Language Trees from\n  Translations", "comments": "ACL2017, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translation has played an important role in trade, law, commerce, politics,\nand literature for thousands of years. Translators have always tried to be\ninvisible; ideal translations should look as if they were written originally in\nthe target language. We show that traces of the source language remain in the\ntranslation product to the extent that it is possible to uncover the history of\nthe source language by looking only at the translation. Specifically, we\nautomatically reconstruct phylogenetic language trees from monolingual texts\n(translated from several source languages). The signal of the source language\nis so powerful that it is retained even after two phases of translation. This\nstrongly indicates that source language interference is the most dominant\ncharacteristic of translated texts, overshadowing the more subtle signals of\nuniversal properties of translation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:14:20 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Rabinovich", "Ella", ""], ["Ordan", "Noam", ""], ["Wintner", "Shuly", ""]]}, {"id": "1704.07156", "submitter": "Marek Rei", "authors": "Marek Rei", "title": "Semi-supervised Multitask Learning for Sequence Labeling", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequence labeling framework with a secondary training objective,\nlearning to predict surrounding words for every word in the dataset. This\nlanguage modeling objective incentivises the system to learn general-purpose\npatterns of semantic and syntactic composition, which are also useful for\nimproving accuracy on different sequence labeling tasks. The architecture was\nevaluated on a range of datasets, covering the tasks of error detection in\nlearner texts, named entity recognition, chunking and POS-tagging. The novel\nlanguage modeling objective provided consistent performance improvements on\nevery benchmark, without requiring any additional annotated or unannotated\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:47:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Rei", "Marek", ""]]}, {"id": "1704.07157", "submitter": "Dmitry Ustalov", "authors": "Dmitry Ustalov, Alexander Panchenko and Chris Biemann", "title": "Watset: Automatic Induction of Synsets from a Graph of Synonyms", "comments": "12 pages, 3 figures, 6 tables, accepted to ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1145", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new graph-based approach that induces synsets using\nsynonymy dictionaries and word embeddings. First, we build a weighted graph of\nsynonyms extracted from commonly available resources, such as Wiktionary.\nSecond, we apply word sense induction to deal with ambiguous words. Finally, we\ncluster the disambiguated version of the ambiguous input graph into synsets.\nOur meta-clustering approach lets us use an efficient hard clustering algorithm\nto perform a fuzzy clustering of the graph. Despite its simplicity, our\napproach shows excellent results, outperforming five competitive\nstate-of-the-art methods in terms of F-score on three gold standard datasets\nfor English and Russian derived from large-scale manually constructed lexical\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 11:49:08 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Ustalov", "Dmitry", ""], ["Panchenko", "Alexander", ""], ["Biemann", "Chris", ""]]}, {"id": "1704.07203", "submitter": "Johannes Daxenberger", "authors": "Johannes Daxenberger, Steffen Eger, Ivan Habernal, Christian Stab,\n  Iryna Gurevych", "title": "What is the Essence of a Claim? Cross-Domain Claim Identification", "comments": "Published at EMNLP 2017:\n  http://www.aclweb.org/anthology/D/D17/D17-1217.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argument mining has become a popular research area in NLP. It typically\nincludes the identification of argumentative components, e.g. claims, as the\ncentral component of an argument. We perform a qualitative analysis across six\ndifferent datasets and show that these appear to conceptualize claims quite\ndifferently. To learn about the consequences of such different\nconceptualizations of claim for practical applications, we carried out\nextensive experiments using state-of-the-art feature-rich and deep learning\nsystems, to identify claims in a cross-domain fashion. While the divergent\nperception of claims in different datasets is indeed harmful to cross-domain\nclassification, we show that there are shared properties on the lexical level\nas well as system configurations that can help to overcome these gaps.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:13:30 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:25:36 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 10:22:33 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Daxenberger", "Johannes", ""], ["Eger", "Steffen", ""], ["Habernal", "Ivan", ""], ["Stab", "Christian", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1704.07221", "submitter": "Elena Kochkina", "authors": "Elena Kochkina, Maria Liakata, Isabelle Augenstein", "title": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance\n  Classification with Branch-LSTM", "comments": "SemEval 2017 RumourEval: Determining rumour veracity and support for\n  rumours (SemEval 2017 Task 8, Subtask A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes team Turing's submission to SemEval 2017 RumourEval:\nDetermining rumour veracity and support for rumours (SemEval 2017 Task 8,\nSubtask A). Subtask A addresses the challenge of rumour stance classification,\nwhich involves identifying the attitude of Twitter users towards the\ntruthfulness of the rumour they are discussing. Stance classification is\nconsidered to be an important step towards rumour verification, therefore\nperforming well in this task is expected to be useful in debunking false\nrumours. In this work we classify a set of Twitter posts discussing rumours\ninto either supporting, denying, questioning or commenting on the underlying\nrumours. We propose a LSTM-based sequential model that, through modelling the\nconversational structure of tweets, which achieves an accuracy of 0.784 on the\nRumourEval test set outperforming all other systems in Subtask A.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:41:25 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Kochkina", "Elena", ""], ["Liakata", "Maria", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "1704.07287", "submitter": "Shubham Toshniwal", "authors": "Trang Tran, Shubham Toshniwal, Mohit Bansal, Kevin Gimpel, Karen\n  Livescu, Mari Ostendorf", "title": "Parsing Speech: A Neural Approach to Integrating Lexical and\n  Acoustic-Prosodic Information", "comments": "Accepted in NAACL HLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conversational speech, the acoustic signal provides cues that help\nlisteners disambiguate difficult parses. For automatically parsing spoken\nutterances, we introduce a model that integrates transcribed text and\nacoustic-prosodic features using a convolutional neural network over energy and\npitch trajectories coupled with an attention-based recurrent neural network\nthat accepts text and prosodic features. We find that different types of\nacoustic-prosodic features are individually helpful, and together give\nstatistically significant improvements in parse and disfluency detection F1\nscores over a strong text-only baseline. For this study with known sentence\nboundaries, error analyses show that the main benefit of acoustic-prosodic\nfeatures is in sentences with disfluencies, attachment decisions are most\nimproved, and transcription errors obscure gains from prosody.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:33:26 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 23:02:57 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Tran", "Trang", ""], ["Toshniwal", "Shubham", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1704.07329", "submitter": "Murahtan Kurfal{\\i}", "authors": "Murathan Kurfal{\\i}, Ahmet \\\"Ust\\\"un, Burcu Can", "title": "A Trie-Structured Bayesian Model for Unsupervised Morphological\n  Segmentation", "comments": "12 pages, accepted and presented at the CICLING 2017 - 18th\n  International Conference on Intelligent Text Processing and Computational\n  Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a trie-structured Bayesian model for unsupervised\nmorphological segmentation. We adopt prior information from different sources\nin the model. We use neural word embeddings to discover words that are\nmorphologically derived from each other and thereby that are semantically\nsimilar. We use letter successor variety counts obtained from tries that are\nbuilt by neural word embeddings. Our results show that using different\ninformation sources such as neural word embeddings and letter successor variety\nas prior information improves morphological segmentation in a Bayesian model.\nOur model outperforms other unsupervised morphological segmentation models on\nTurkish and gives promising results on English and German for scarce resources.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:07:26 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Kurfal\u0131", "Murathan", ""], ["\u00dcst\u00fcn", "Ahmet", ""], ["Can", "Burcu", ""]]}, {"id": "1704.07398", "submitter": "Yevgeni Berzak", "authors": "Yevgeni Berzak, Chie Nakamura, Suzanne Flynn and Boris Katz", "title": "Predicting Native Language from Gaze", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in language learning concerns the role of a speaker's\nfirst language in second language acquisition. We present a novel methodology\nfor studying this question: analysis of eye-movement patterns in second\nlanguage reading of free-form text. Using this methodology, we demonstrate for\nthe first time that the native language of English learners can be predicted\nfrom their gaze fixations when reading English. We provide analysis of\nclassifier uncertainty and learned features, which indicates that differences\nin English reading are likely to be rooted in linguistic divergences across\nnative languages. The presented framework complements production studies and\noffers new ground for advancing research on multilingualism.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 18:04:17 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 21:40:35 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Nakamura", "Chie", ""], ["Flynn", "Suzanne", ""], ["Katz", "Boris", ""]]}, {"id": "1704.07415", "submitter": "Yichen Gong", "authors": "Yichen Gong, Samuel R. Bowman", "title": "Ruminating Reader: Reasoning with Gated Multi-Hop Attention", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To answer the question in machine comprehension (MC) task, the models need to\nestablish the interaction between the question and the context. To tackle the\nproblem that the single-pass model cannot reflect on and correct its answer, we\npresent Ruminating Reader. Ruminating Reader adds a second pass of attention\nand a novel information fusion component to the Bi-Directional Attention Flow\nmodel (BiDAF). We propose novel layer structures that construct an query-aware\ncontext vector representation and fuse encoding representation with\nintermediate representation on top of BiDAF model. We show that a multi-hop\nattention mechanism can be applied to a bi-directional attention structure. In\nexperiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by\na substantial margin, and matches or surpasses the performance of all other\npublished systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 18:49:38 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Gong", "Yichen", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1704.07427", "submitter": "Yanqing Chen", "authors": "Yanqing Chen, Steven Skiena", "title": "Recognizing Descriptive Wikipedia Categories for Historical Figures", "comments": "9 pages, 6 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is a useful knowledge source that benefits many applications in\nlanguage processing and knowledge representation. An important feature of\nWikipedia is that of categories. Wikipedia pages are assigned different\ncategories according to their contents as human-annotated labels which can be\nused in information retrieval, ad hoc search improvements, entity ranking and\ntag recommendations. However, important pages are usually assigned too many\ncategories, which makes it difficult to recognize the most important ones that\ngive the best descriptions.\n  In this paper, we propose an approach to recognize the most descriptive\nWikipedia categories. We observe that historical figures in a precise category\npresumably are mutually similar and such categorical coherence could be\nevaluated via texts or Wikipedia links of corresponding members in the\ncategory. We rank descriptive level of Wikipedia categories according to their\ncoherence and our ranking yield an overall agreement of 88.27% compared with\nhuman wisdom.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 19:28:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Yanqing", ""], ["Skiena", "Steven", ""]]}, {"id": "1704.07431", "submitter": "George Foster", "authors": "Pierre Isabelle, Colin Cherry, and George Foster", "title": "A Challenge Set Approach to Evaluating Machine Translation", "comments": "EMNLP 2017. 28 pages, including appendix. Machine readable data\n  included in a separate file. This version corrects typos in the challenge set", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation represents an exciting leap forward in translation\nquality. But what longstanding weaknesses does it resolve, and which remain? We\naddress these questions with a challenge set approach to translation evaluation\nand error analysis. A challenge set consists of a small set of sentences, each\nhand-designed to probe a system's capacity to bridge a particular structural\ndivergence between languages. To exemplify this approach, we present an\nEnglish-French challenge set, and use it to analyze phrase-based and neural\nsystems. The resulting analysis provides not only a more fine-grained picture\nof the strengths of neural systems, but also insight into which linguistic\nphenomena remain out of reach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 19:34:38 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 16:28:52 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 00:03:19 GMT"}, {"version": "v4", "created": "Sun, 6 Aug 2017 17:13:11 GMT"}, {"version": "v5", "created": "Tue, 29 Aug 2017 02:08:33 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Isabelle", "Pierre", ""], ["Cherry", "Colin", ""], ["Foster", "George", ""]]}, {"id": "1704.07441", "submitter": "Yanqing Chen", "authors": "Yanging Chen, Rami Al-Rfou', Yejin Choi", "title": "Detecting English Writing Styles For Non Native Speakers", "comments": "9 figures, 5 tables, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first attempt, up to our knowledge, to classify\nEnglish writing styles on this scale with the challenge of classifying day to\nday language written by writers with different backgrounds covering various\nareas of topics.The paper proposes simple machine learning algorithms and\nsimple to generate features to solve hard problems. Relying on the scale of the\ndata available from large sources of knowledge like Wikipedia. We believe such\nsources of data are crucial to generate robust solutions for the web with high\naccuracy and easy to deploy in practice. The paper achieves 74\\% accuracy\nclassifying native versus non native speakers writing styles.\n  Moreover, the paper shows some interesting observations on the similarity\nbetween different languages measured by the similarity of their users English\nwriting styles. This technique could be used to show some well known facts\nabout languages as in grouping them into families, which our experiments\nsupport.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 20:04:25 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Yanging", ""], ["Al-Rfou'", "Rami", ""], ["Choi", "Yejin", ""]]}, {"id": "1704.07463", "submitter": "Kevin Duh", "authors": "Chandler May, Kevin Duh, Benjamin Van Durme, Ashwin Lall", "title": "Streaming Word Embeddings with the Space-Saving Algorithm", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a streaming (one-pass, bounded-memory) word embedding algorithm\nbased on the canonical skip-gram with negative sampling algorithm implemented\nin word2vec. We compare our streaming algorithm to word2vec empirically by\nmeasuring the cosine similarity between word pairs under each algorithm and by\napplying each algorithm in the downstream task of hashtag prediction on a\ntwo-month interval of the Twitter sample stream. We then discuss the results of\nthese experiments, concluding they provide partial validation of our approach\nas a streaming replacement for word2vec. Finally, we discuss potential failure\nmodes and suggest directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 20:55:33 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["May", "Chandler", ""], ["Duh", "Kevin", ""], ["Van Durme", "Benjamin", ""], ["Lall", "Ashwin", ""]]}, {"id": "1704.07468", "submitter": "Yanjun  Qi Dr.", "authors": "Ritambhara Singh, Arshdeep Sekhon, Kamran Kowsari, Jack Lanchantin,\n  Beilun Wang and Yanjun Qi", "title": "GaKCo: a Fast GApped k-mer string Kernel using COunting", "comments": "@ECML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String Kernel (SK) techniques, especially those using gapped $k$-mers as\nfeatures (gk), have obtained great success in classifying sequences like DNA,\nprotein, and text. However, the state-of-the-art gk-SK runs extremely slow when\nwe increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This\nis because current gk-SK uses a trie-based algorithm to calculate co-occurrence\nof mismatched substrings resulting in a time cost proportional to\n$O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating\n\\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting\n(GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of\nsubstrings using cumulative counting. This algorithm is fast, scalable to\nlarger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous\nasymptotic analysis that compares GaKCo with the state-of-the-art gk-SK.\nTheoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term\nthat slows down the trie-based approach. Experimentally, we observe that GaKCo\nachieves the same accuracy as the state-of-the-art and outperforms its speed by\nfactors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein\n(12 datasets), and character-based English text (2 datasets), respectively.\n  GaKCo is shared as an open source tool at\n\\url{https://github.com/QData/GaKCo-SVM}\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 21:43:21 GMT"}, {"version": "v2", "created": "Sun, 30 Apr 2017 20:12:01 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 17:25:17 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Singh", "Ritambhara", ""], ["Sekhon", "Arshdeep", ""], ["Kowsari", "Kamran", ""], ["Lanchantin", "Jack", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1704.07489", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Mohit Bansal", "title": "Multi-Task Video Captioning with Video and Entailment Generation", "comments": "ACL 2017 (14 pages w/ supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning, the task of describing the content of a video, has seen\nsome promising improvements in recent years with sequence-to-sequence models,\nbut accurately learning the temporal and logical dynamics involved in the task\nstill remains a challenge, especially given the lack of sufficient annotated\ndata. We improve video captioning by sharing knowledge with two related\ndirected-generation tasks: a temporally-directed unsupervised video prediction\ntask to learn richer context-aware video encoder representations, and a\nlogically-directed language entailment generation task to learn better\nvideo-entailed caption decoder representations. For this, we present a\nmany-to-many multi-task learning model that shares parameters across the\nencoders and decoders of the three tasks. We achieve significant improvements\nand the new state-of-the-art on several standard video captioning datasets\nusing diverse automatic and human evaluations. We also show mutual multi-task\nimprovements on the entailment generation task.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 23:07:32 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 17:08:58 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1704.07535", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Mitchell Stern, Dan Klein", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "comments": "ACL 2017. MR and MS contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks like code generation and semantic parsing require mapping unstructured\n(or partially structured) inputs to well-formed, executable outputs. We\nintroduce abstract syntax networks, a modeling framework for these problems.\nThe outputs are represented as abstract syntax trees (ASTs) and constructed by\na decoder with a dynamically-determined modular structure paralleling the\nstructure of the output tree. On the benchmark Hearthstone dataset for code\ngeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,\ncompared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we\nperform competitively on the Atis, Jobs, and Geo semantic parsing datasets with\nno task-specific engineering.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 04:37:35 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Stern", "Mitchell", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07556", "submitter": "Xinchi Chen", "authors": "Xinchi Chen, Zhan Shi, Xipeng Qiu, Xuanjing Huang", "title": "Adversarial Multi-Criteria Learning for Chinese Word Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different linguistic perspectives causes many diverse segmentation criteria\nfor Chinese word segmentation (CWS). Most existing methods focus on improve the\nperformance for each single criterion. However, it is interesting to exploit\nthese different criteria and mining their common underlying knowledge. In this\npaper, we propose adversarial multi-criteria learning for CWS by integrating\nshared knowledge from multiple heterogeneous segmentation criteria. Experiments\non eight corpora with heterogeneous segmentation criteria show that the\nperformance of each corpus obtains a significant improvement, compared to\nsingle-criterion learning. Source codes of this paper are available on Github.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 06:42:24 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Xinchi", ""], ["Shi", "Zhan", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1704.07616", "submitter": "Liner Yang", "authors": "Liner Yang, Meishan Zhang, Yang Liu, Nan Yu, Maosong Sun, Guohong Fu", "title": "Joint POS Tagging and Dependency Parsing with Transition-based Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While part-of-speech (POS) tagging and dependency parsing are observed to be\nclosely related, existing work on joint modeling with manually crafted feature\ntemplates suffers from the feature sparsity and incompleteness problems. In\nthis paper, we propose an approach to joint POS tagging and dependency parsing\nusing transition-based neural networks. Three neural network based classifiers\nare designed to resolve shift/reduce, tagging, and labeling conflicts.\nExperiments show that our approach significantly outperforms previous methods\nfor joint POS tagging and dependency parsing across a variety of natural\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:22:57 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Yang", "Liner", ""], ["Zhang", "Meishan", ""], ["Liu", "Yang", ""], ["Yu", "Nan", ""], ["Sun", "Maosong", ""], ["Fu", "Guohong", ""]]}, {"id": "1704.07624", "submitter": "Amit Gupta", "authors": "Amit Gupta and R\\'emi Lebret and Hamza Harkous and Karl Aberer", "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from\n  Wikipedia using Character-level Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, yet effective, approach towards inducing multilingual\ntaxonomies from Wikipedia. Given an English taxonomy, our approach leverages\nthe interlanguage links of Wikipedia followed by character-level classifiers to\ninduce high-precision, high-coverage taxonomies in other languages. Through\nexperiments, we demonstrate that our approach significantly outperforms the\nstate-of-the-art, heuristics-heavy approaches for six languages. As a\nconsequence of our work, we release presumably the largest and the most\naccurate multilingual taxonomic resource spanning over 280 languages.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:45:43 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 09:23:40 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Gupta", "Amit", ""], ["Lebret", "R\u00e9mi", ""], ["Harkous", "Hamza", ""], ["Aberer", "Karl", ""]]}, {"id": "1704.07626", "submitter": "Amit Gupta", "authors": "Amit Gupta, R\\'emi Lebret, Hamza Harkous and Karl Aberer", "title": "Taxonomy Induction using Hypernym Subsequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, semi-supervised approach towards domain taxonomy\ninduction from an input vocabulary of seed terms. Unlike all previous\napproaches, which typically extract direct hypernym edges for terms, our\napproach utilizes a novel probabilistic framework to extract hypernym\nsubsequences. Taxonomy induction from extracted subsequences is cast as an\ninstance of the minimumcost flow problem on a carefully designed directed\ngraph. Through experiments, we demonstrate that our approach outperforms\nstateof- the-art taxonomy induction approaches across four languages.\nImportantly, we also show that our approach is robust to the presence of noise\nin the input vocabulary. To the best of our knowledge, no previous approaches\nhave been empirically proven to manifest noise-robustness in the input\nvocabulary.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:49:53 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 17:03:17 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 14:42:59 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 20:34:26 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Gupta", "Amit", ""], ["Lebret", "R\u00e9mi", ""], ["Harkous", "Hamza", ""], ["Aberer", "Karl", ""]]}, {"id": "1704.07734", "submitter": "Xiaodong Gu", "authors": "Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim", "title": "DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning", "comments": "Accepted at IJCAI 2017 (The 26th International Joint Conference on\n  Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer programs written in one language are often required to be ported to\nother languages to support multiple devices and environments. When programs use\nlanguage specific APIs (Application Programming Interfaces), it is very\nchallenging to migrate these APIs to the corresponding APIs written in other\nlanguages. Existing approaches mine API mappings from projects that have\ncorresponding versions in two languages. They rely on the sparse availability\nof bilingual projects, thus producing a limited number of API mappings. In this\npaper, we propose an intelligent system called DeepAM for automatically mining\nAPI mappings from a large-scale code corpus without bilingual projects. The key\ncomponent of DeepAM is based on the multimodal sequence to sequence learning\narchitecture that aims to learn joint semantic representations of bilingual API\nsequences from big source code data. Experimental results indicate that DeepAM\nsignificantly increases the accuracy of API mappings as well as the number of\nAPI mappings, when compared with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:09:51 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Gu", "Xiaodong", ""], ["Zhang", "Hongyu", ""], ["Zhang", "Dongmei", ""], ["Kim", "Sunghun", ""]]}, {"id": "1704.07751", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Dan Klein", "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:52:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07759", "submitter": "Gianluca Stringhini", "authors": "Emeric Bernard-Jones, Jeremiah Onaolapo, Gianluca Stringhini", "title": "Email Babel: Does Language Affect Criminal Activity in Compromised\n  Webmail Accounts?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We set out to understand the effects of differing language on the ability of\ncybercriminals to navigate webmail accounts and locate sensitive information in\nthem. To this end, we configured thirty Gmail honeypot accounts with English,\nRomanian, and Greek language settings. We populated the accounts with email\nmessages in those languages by subscribing them to selected online newsletters.\nWe hid email messages about fake bank accounts in fifteen of the accounts to\nmimic real-world webmail users that sometimes store sensitive information in\ntheir accounts. We then leaked credentials to the honey accounts via paste\nsites on the Surface Web and the Dark Web, and collected data for fifteen days.\nOur statistical analyses on the data show that cybercriminals are more likely\nto discover sensitive information (bank account information) in the Greek\naccounts than the remaining accounts, contrary to the expectation that Greek\nought to constitute a barrier to the understanding of non-Greek visitors to the\nGreek accounts. We also extracted the important words among the emails that\ncybercriminals accessed (as an approximation of the keywords that they searched\nfor within the honey accounts), and found that financial terms featured among\nthe top words. In summary, we show that language plays a significant role in\nthe ability of cybercriminals to access sensitive information hidden in\ncompromised webmail accounts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 16:07:34 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Bernard-Jones", "Emeric", ""], ["Onaolapo", "Jeremiah", ""], ["Stringhini", "Gianluca", ""]]}, {"id": "1704.07828", "submitter": "Chenhao Tan", "authors": "Chenhao Tan, Dallas Card, Noah A. Smith", "title": "Friendships, Rivalries, and Trysts: Characterizing Relations between\n  Ideas in Texts", "comments": "11 pages, 9 figures, to appear in Proceedings of ACL 2017, code and\n  data available at https://chenhaot.com/pages/idea-relations.html (fixed a\n  typo)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how ideas relate to each other is a fundamental question in\nmany domains, ranging from intellectual history to public communication.\nBecause ideas are naturally embedded in texts, we propose the first framework\nto systematically characterize the relations between ideas based on their\noccurrence in a corpus of documents, independent of how these ideas are\nrepresented. Combining two statistics --- cooccurrence within documents and\nprevalence correlation over time --- our approach reveals a number of different\nways in which ideas can cooperate and compete. For instance, two ideas can\nclosely track each other's prevalence over time, and yet rarely cooccur, almost\nlike a \"cold war\" scenario. We observe that pairwise cooccurrence and\nprevalence correlation exhibit different distributions. We further demonstrate\nthat our approach is able to uncover intriguing relations between ideas through\nin-depth case studies on news articles and research papers.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 18:00:01 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 16:34:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tan", "Chenhao", ""], ["Card", "Dallas", ""], ["Smith", "Noah A.", ""]]}, {"id": "1704.07875", "submitter": "Maria Ryskina", "authors": "Maria Ryskina, Hannah Alpert-Abrams, Dan Garrette, Taylor\n  Berg-Kirkpatrick", "title": "Automatic Compositor Attribution in the First Folio of Shakespeare", "comments": "Short paper (6 pages) accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositor attribution, the clustering of pages in a historical printed\ndocument by the individual who set the type, is a bibliographic task that\nrelies on analysis of orthographic variation and inspection of visual details\nof the printed page. In this paper, we introduce a novel unsupervised model\nthat jointly describes the textual and visual features needed to distinguish\ncompositors. Applied to images of Shakespeare's First Folio, our model predicts\nattributions that agree with the manual judgements of bibliographers with an\naccuracy of 87%, even on text that is the output of OCR.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 19:26:36 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Ryskina", "Maria", ""], ["Alpert-Abrams", "Hannah", ""], ["Garrette", "Dan", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "1704.07986", "submitter": "Akira Sasaki", "authors": "Akira Sasaki, Kazuaki Hanawa, Naoaki Okazaki, Kentaro Inui", "title": "Other Topics You May Also Agree or Disagree: Modeling Inter-Topic\n  Preferences using Tweets and Matrix Factorization", "comments": "To appear in ACL2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper our approach for modeling inter-topic preferences of\nTwitter users: for example, those who agree with the Trans-Pacific Partnership\n(TPP) also agree with free trade. This kind of knowledge is useful not only for\nstance detection across multiple topics but also for various real-world\napplications including public opinion surveys, electoral predictions, electoral\ncampaigns, and online debates. In order to extract users' preferences on\nTwitter, we design linguistic patterns in which people agree and disagree about\nspecific topics (e.g., \"A is completely wrong\"). By applying these linguistic\npatterns to a collection of tweets, we extract statements agreeing and\ndisagreeing with various topics. Inspired by previous work on item\nrecommendation, we formalize the task of modeling inter-topic preferences as\nmatrix factorization: representing users' preferences as a user-topic matrix\nand mapping both users and topics onto a latent feature space that abstracts\nthe preferences. Our experimental results demonstrate both that our proposed\napproach is useful in predicting missing preferences of users and that the\nlatent vector representations of topics successfully encode inter-topic\npreferences.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 07:04:46 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Sasaki", "Akira", ""], ["Hanawa", "Kazuaki", ""], ["Okazaki", "Naoaki", ""], ["Inui", "Kentaro", ""]]}, {"id": "1704.08012", "submitter": "Jey Han Lau", "authors": "Jey Han Lau and Timothy Baldwin and Trevor Cohn", "title": "Topically Driven Neural Language Model", "comments": "11 pages, Proceedings of the 55th Annual Meeting of the Association\n  for Computational Linguistics (ACL 2017) (to appear)", "journal-ref": "In Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2017), pp. 355--365", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models are typically applied at the sentence level, without access\nto the broader document context. We present a neural language model that\nincorporates document context in the form of a topic model-like architecture,\nthus providing a succinct representation of the broader document context\noutside of the current sentence. Experiments over a range of datasets\ndemonstrate that our model outperforms a pure sentence-based model in terms of\nlanguage model perplexity, and leads to topics that are potentially more\ncoherent than those produced by a standard LDA topic model. Our model also has\nthe ability to generate related sentences for a topic, providing another way to\ninterpret topics.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 08:33:14 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 07:05:44 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""], ["Cohn", "Trevor", ""]]}, {"id": "1704.08059", "submitter": "Oleksii Hrinchuk", "authors": "Alexander Fonarev, Oleksii Hrinchuk, Gleb Gusev, Pavel Serdyukov, and\n  Ivan Oseledets", "title": "Riemannian Optimization for Skip-Gram Negative Sampling", "comments": "9 pages, 4 figures, ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its\nimplementation in \"word2vec\" software, is usually optimized by stochastic\ngradient descent. However, the optimization of SGNS objective can be viewed as\na problem of searching for a good matrix with the low-rank constraint. The most\nstandard way to solve this type of problems is to apply Riemannian optimization\nframework to optimize the SGNS objective over the manifold of required low-rank\nmatrices. In this paper, we propose an algorithm that optimizes SGNS objective\nusing Riemannian optimization and demonstrates its superiority over popular\ncompetitors, such as the original method to train SGNS and SVD over SPPMI\nmatrix.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 11:17:51 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Fonarev", "Alexander", ""], ["Hrinchuk", "Oleksii", ""], ["Gusev", "Gleb", ""], ["Serdyukov", "Pavel", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1704.08088", "submitter": "Leandro Dos Santos", "authors": "Leandro B. dos Santos, Edilson A. Corr\\^ea Jr, Osvaldo N. Oliveira Jr,\n  Diego R. Amancio, Let\\'icia L. Mansur and Sandra M. Alu\\'isio", "title": "Enriching Complex Networks with Word Embeddings for Detecting Mild\n  Cognitive Impairment from Speech Transcripts", "comments": "Published in Annual Meeting of the Association for Computational\n  Linguist 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose.\nLinguistic features, mainly from parsers, have been used to detect MCI, but\nthis is not suitable for large-scale assessments. MCI disfluencies produce\nnon-grammatical speech that requires manual or high precision automatic\ncorrection of transcripts. In this paper, we modeled transcripts into complex\nnetworks and enriched them with word embedding (CNE) to better represent short\ntexts produced in neuropsychological assessments. The network measurements were\napplied with well-known classifiers to automatically identify MCI in\ntranscripts, in a binary classification task. A comparison was made with the\nperformance of traditional approaches using Bag of Words (BoW) and linguistic\nfeatures for three datasets: DementiaBank in English, and Cinderella and\nArizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using\nonly complex networks, while Support Vector Machine was superior to other\nclassifiers. CNE provided the highest accuracies for DementiaBank and\nCinderella, but BoW was more efficient for the Arizona-Battery dataset probably\nowing to its short narratives. The approach using linguistic features yielded\nhigher accuracy if the transcriptions of the Cinderella dataset were manually\nrevised. Taken together, the results indicate that complex networks enriched\nwith embedding is promising for detecting MCI in large-scale assessments\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:06:25 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Santos", "Leandro B. dos", ""], ["Corr\u00eaa", "Edilson A.", "Jr"], ["Oliveira", "Osvaldo N.", "Jr"], ["Amancio", "Diego R.", ""], ["Mansur", "Let\u00edcia L.", ""], ["Alu\u00edsio", "Sandra M.", ""]]}, {"id": "1704.08092", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Niko Schenk, Christian Chiarcos", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese\n  Implicit Discourse Relations", "comments": "To appear at ACL2017, code available at\n  https://github.com/sronnqvist/discourse-ablstm", "journal-ref": null, "doi": "10.18653/v1/P17-2040", "report-no": "Proceedings of the 55th Annual Meeting of the Association for\n  Computational Linguistics (ACL'17)", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an attention-based Bi-LSTM for Chinese implicit discourse\nrelations and demonstrate that modeling argument pairs as a joint sequence can\noutperform word order-agnostic approaches. Our model benefits from a partial\nsampling scheme and is conceptually simple, yet achieves state-of-the-art\nperformance on the Chinese Discourse Treebank. We also visualize its attention\nactivity to illustrate the model's ability to selectively focus on the relevant\nparts of an input sequence.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:10:12 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Schenk", "Niko", ""], ["Chiarcos", "Christian", ""]]}, {"id": "1704.08224", "submitter": "Arjun Chandrasekaran", "authors": "Arjun Chandrasekaran and Devi Parikh and Mohit Bansal", "title": "Punny Captions: Witty Wordplay in Image Descriptions", "comments": "NAACL 2018 (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wit is a form of rich interaction that is often grounded in a specific\nsituation (e.g., a comment in response to an event). In this work, we attempt\nto build computational models that can produce witty descriptions for a given\nimage. Inspired by a cognitive account of humor appreciation, we employ\nlinguistic wordplay, specifically puns, in image descriptions. We develop two\napproaches which involve retrieving witty descriptions for a given image from a\nlarge corpus of sentences, or generating them via an encoder-decoder neural\nnetwork architecture. We compare our approach against meaningful baseline\napproaches via human studies and show substantial improvements. We find that\nwhen a human is subject to similar constraints as the model regarding word\nusage and style, people vote the image descriptions generated by our model to\nbe slightly wittier than human-written witty descriptions. Unsurprisingly,\nhumans are almost always wittier than the model when they are free to choose\nthe vocabulary, style, etc.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:22:53 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 17:45:50 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Parikh", "Devi", ""], ["Bansal", "Mohit", ""]]}, {"id": "1704.08243", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, Devi Parikh", "title": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has received a lot of attention over the past\ncouple of years. A number of deep learning models have been proposed for this\ntask. However, it has been shown that these models are heavily driven by\nsuperficial correlations in the training data and lack compositionality -- the\nability to answer questions about unseen compositions of seen concepts. This\ncompositionality is desirable and central to intelligence. In this paper, we\npropose a new setting for Visual Question Answering where the test\nquestion-answer pairs are compositionally novel compared to training\nquestion-answer pairs. To facilitate developing models under this setting, we\npresent a new compositional split of the VQA v1.0 dataset, which we call\nCompositional VQA (C-VQA). We analyze the distribution of questions and answers\nin the C-VQA splits. Finally, we evaluate several existing VQA models under\nthis new setting and show that the performances of these models degrade by a\nsignificant amount compared to the original VQA setting.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:57:59 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Kembhavi", "Aniruddha", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1704.08300", "submitter": "Anirban Laha", "authors": "Preksha Nema, Mitesh Khapra, Anirban Laha, Balaraman Ravindran", "title": "Diversity driven Attention Model for Query-based Abstractive\n  Summarization", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization aims to generate a shorter version of the document\ncovering all the salient points in a compact and coherent fashion. On the other\nhand, query-based summarization highlights those points that are relevant in\nthe context of a given query. The encode-attend-decode paradigm has achieved\nnotable success in machine translation, extractive summarization, dialog\nsystems, etc. But it suffers from the drawback of generation of repeated\nphrases. In this work we propose a model for the query-based summarization task\nbased on the encode-attend-decode paradigm with two key additions (i) a query\nattention model (in addition to document attention model) which learns to focus\non different portions of the query at different time steps (instead of using a\nstatic representation for the query) and (ii) a new diversity based attention\nmodel which aims to alleviate the problem of repeating phrases in the summary.\nIn order to enable the testing of this model we introduce a new query-based\nsummarization dataset building on debatepedia. Our experiments show that with\nthese two additions the proposed model clearly outperforms vanilla\nencode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 19:06:37 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 05:44:05 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Nema", "Preksha", ""], ["Khapra", "Mitesh", ""], ["Laha", "Anirban", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1704.08352", "submitter": "Clara Vania", "authors": "Clara Vania and Adam Lopez", "title": "From Characters to Words to in Between: Do We Capture Morphology?", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words can be represented by composing the representations of subword units\nsuch as word segments, characters, and/or character n-grams. While such\nrepresentations are effective and may capture the morphological regularities of\nwords, they have not been systematically compared, and it is not understood how\nthey interact with different morphological typologies. On a language modeling\ntask, we present experiments that systematically vary (1) the basic unit of\nrepresentation, (2) the composition of these representations, and (3) the\nmorphological typology of the language modeled. Our results extend previous\nfindings that character representations are effective across typologies, and we\nfind that a previously unstudied combination of character trigram\nrepresentations composed with bi-LSTMs outperforms most others. But we also\nfind room for improvement: none of the character-level models match the\npredictive accuracy of a model with access to true morphological analyses, even\nwhen learned from an order of magnitude more data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:10:53 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Vania", "Clara", ""], ["Lopez", "Adam", ""]]}, {"id": "1704.08381", "submitter": "Ioannis Konstas", "authors": "Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi and Luke\n  Zettlemoyer", "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation", "comments": "Accepted in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have shown strong performance across a broad\nrange of applications. However, their application to parsing and generating\ntext usingAbstract Meaning Representation (AMR)has been limited, due to the\nrelatively limited amount of labeled data and the non-sequential nature of the\nAMR graphs. We present a novel training procedure that can lift this limitation\nusing millions of unlabeled sentences and careful preprocessing of the AMR\ngraphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH,\nthe current best score reported without significant use of external semantic\nresources. For AMR generation, our model establishes a new state-of-the-art\nperformance of BLEU 33.8. We present extensive ablative and qualitative\nanalysis including strong evidence that sequence-based AMR models are robust\nagainst ordering variations of graph-to-sequence conversions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 23:53:34 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 19:17:32 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 11:28:05 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Konstas", "Ioannis", ""], ["Iyer", "Srinivasan", ""], ["Yatskar", "Mark", ""], ["Choi", "Yejin", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1704.08384", "submitter": "Rajarshi Das", "authors": "Rajarshi Das, Manzil Zaheer, Siva Reddy, Andrew McCallum", "title": "Question Answering on Knowledge Bases and Text using Universal Schema\n  and Memory Networks", "comments": "ACL 2017 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing question answering methods infer answers either from a knowledge\nbase or from raw text. While knowledge base (KB) methods are good at answering\ncompositional questions, their performance is often affected by the\nincompleteness of the KB. Au contraire, web text contains millions of facts\nthat are absent in the KB, however in an unstructured form. {\\it Universal\nschema} can support reasoning on the union of both structured KBs and\nunstructured text by aligning them in a common embedded space. In this paper we\nextend universal schema to natural language question answering, employing\n\\emph{memory networks} to attend to the large body of facts in the combination\nof text and KB. Our models can be trained in an end-to-end fashion on\nquestion-answer pairs. Evaluation results on \\spades fill-in-the-blank question\nanswering dataset show that exploiting universal schema for question answering\nis better than using either a KB or text alone. This model also outperforms the\ncurrent state-of-the-art by 8.5 $F_1$ points.\\footnote{Code and data available\nin \\url{https://rajarshd.github.io/TextKBQA}}\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 00:03:02 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Das", "Rajarshi", ""], ["Zaheer", "Manzil", ""], ["Reddy", "Siva", ""], ["McCallum", "Andrew", ""]]}, {"id": "1704.08387", "submitter": "Jianpeng Cheng J", "authors": "Jianpeng Cheng, Siva Reddy, Vijay Saraswat, Mirella Lapata", "title": "Learning Structured Natural Language Representations for Semantic\n  Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural semantic parser that converts natural language\nutterances to intermediate representations in the form of predicate-argument\nstructures, which are induced with a transition system and subsequently mapped\nto target domains. The semantic parser is trained end-to-end using annotated\nlogical forms or their denotations. We obtain competitive results on various\ndatasets. The induced predicate-argument structures shed light on the types of\nrepresentations useful for semantic parsing and how these are different from\nlinguistically motivated ones.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 00:24:20 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 09:57:29 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 04:18:26 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Reddy", "Siva", ""], ["Saraswat", "Vijay", ""], ["Lapata", "Mirella", ""]]}, {"id": "1704.08388", "submitter": "Ted Pedersen", "authors": "Ted Pedersen", "title": "Duluth at Semeval-2017 Task 7 : Puns upon a midnight dreary, Lexical\n  Semantics for the weak and weary", "comments": "5 pages, to Appear in the Proceedings of the 11th International\n  Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Duluth systems that participated in SemEval-2017\nTask 7 : Detection and Interpretation of English Puns. The Duluth systems\nparticipated in all three subtasks, and relied on methods that included word\nsense disambiguation and measures of semantic relatedness.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 00:29:17 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 01:16:07 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Pedersen", "Ted", ""]]}, {"id": "1704.08390", "submitter": "Ted Pedersen", "authors": "Xinru Yan and Ted Pedersen", "title": "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "comments": "5 pages, to Appear in the Proceedings of the 11th International\n  Workshop on Semantic Evaluation (SemEval 2017), August 2017, Vancouver, BC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Duluth system that participated in SemEval-2017 Task\n6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks\nA and B using N-gram language models, ranking highly in the task evaluation.\nThis paper discusses the results of our system in the development and\nevaluation stages and from two post-evaluation runs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 00:40:33 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Yan", "Xinru", ""], ["Pedersen", "Ted", ""]]}, {"id": "1704.08424", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Andrew Gordon Wilson", "title": "Multimodal Word Distributions", "comments": "This paper also appears at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings provide point representations of words containing useful\nsemantic information. We introduce multimodal word distributions formed from\nGaussian mixtures, for multiple word meanings, entailment, and rich uncertainty\ninformation. To learn these distributions, we propose an energy-based\nmax-margin objective. We show that the resulting approach captures uniquely\nexpressive semantic information, and outperforms alternatives, such as word2vec\nskip-grams, and Gaussian embeddings, on benchmark datasets such as word\nsimilarity and entailment.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 03:59:54 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 17:56:33 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1704.08430", "submitter": "Biao Zhang", "authors": "Biao Zhang, Deyi Xiong, Jinsong Su", "title": "A GRU-Gated Attention Model for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) heavily relies on an attention network to\nproduce a context vector for each target word prediction. In practice, we find\nthat context vectors for different target words are quite similar to one\nanother and therefore are insufficient in discriminatively predicting target\nwords. The reason for this might be that context vectors produced by the\nvanilla attention network are just a weighted sum of source representations\nthat are invariant to decoder states. In this paper, we propose a novel\nGRU-gated attention model (GAtt) for NMT which enhances the degree of\ndiscrimination of context vectors by enabling source representations to be\nsensitive to the partial translation generated by the decoder. GAtt uses a\ngated recurrent unit (GRU) to combine two types of information: treating a\nsource annotation vector originally produced by the bidirectional encoder as\nthe history state while the corresponding previous decoder state as the input\nto the GRU. The GRU-combined information forms a new source annotation vector.\nIn this way, we can obtain translation-sensitive source representations which\nare then feed into the attention network to generate discriminative context\nvectors. We further propose a variant that regards a source annotation vector\nas the current input while the previous decoder state as the history.\nExperiments on NIST Chinese-English translation tasks show that both GAtt-based\nmodels achieve significant improvements over the vanilla attentionbased NMT.\nFurther analyses on attention weights and context vectors demonstrate the\neffectiveness of GAtt in improving the discrimination power of representations\nand handling the challenging issue of over-translation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 04:25:41 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 17:23:49 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhang", "Biao", ""], ["Xiong", "Deyi", ""], ["Su", "Jinsong", ""]]}, {"id": "1704.08531", "submitter": "Vineet John", "authors": "Vineet John", "title": "A Survey of Neural Network Techniques for Feature Extraction from Text", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to catalyze the discussions about text feature extraction\ntechniques using neural network architectures. The research questions discussed\nin the paper focus on the state-of-the-art neural network techniques that have\nproven to be useful tools for language processing, language generation, text\nclassification and other computational linguistics tasks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 12:27:25 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["John", "Vineet", ""]]}, {"id": "1704.08619", "submitter": "Panagiotis Tzirakis", "authors": "Panagiotis Tzirakis, George Trigeorgis, Mihalis A. Nicolaou, Bj\\\"orn\n  Schuller, and Stefanos Zafeiriou", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2017.2764438", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic affect recognition is a challenging task due to the various\nmodalities emotions can be expressed with. Applications can be found in many\ndomains including multimedia retrieval and human computer interaction. In\nrecent years, deep neural networks have been used with great success in\ndetermining emotional states. Inspired by this success, we propose an emotion\nrecognition system using auditory and visual modalities. To capture the\nemotional content for various styles of speaking, robust features need to be\nextracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to\nextract features from the speech, while for the visual modality a deep residual\nnetwork (ResNet) of 50 layers. In addition to the importance of feature\nextraction, a machine learning algorithm needs also to be insensitive to\noutliers while being able to model the context. To tackle this problem, Long\nShort-Term Memory (LSTM) networks are utilized. The system is then trained in\nan end-to-end fashion where - by also taking advantage of the correlations of\nthe each of the streams - we manage to significantly outperform the traditional\napproaches based on auditory and visual handcrafted features for the prediction\nof spontaneous and natural emotions on the RECOLA database of the AVEC 2016\nresearch challenge on emotion recognition.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 15:14:33 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Tzirakis", "Panagiotis", ""], ["Trigeorgis", "George", ""], ["Nicolaou", "Mihalis A.", ""], ["Schuller", "Bj\u00f6rn", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1704.08760", "submitter": "Srinivasan Iyer", "authors": "Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy,\n  Luke Zettlemoyer", "title": "Learning a Neural Semantic Parser from User Feedback", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to rapidly and easily build natural language\ninterfaces to databases for new domains, whose performance improves over time\nbased on user feedback, and requires minimal intervention. To achieve this, we\nadapt neural sequence models to map utterances directly to SQL with its full\nexpressivity, bypassing any intermediate meaning representations. These models\nare immediately deployed online to solicit feedback from real users to flag\nincorrect queries. Finally, the popularity of SQL facilitates gathering\nannotations for incorrect predictions using the crowd, which is directly used\nto improve our models. This complete feedback loop, without intermediate\nrepresentations or database specific engineering, opens up new ways of building\nhigh quality semantic parsers. Experiments suggest that this approach can be\ndeployed quickly for any new target domain, as we show by learning a semantic\nparser for an online academic database from scratch.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 22:05:06 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Iyer", "Srinivasan", ""], ["Konstas", "Ioannis", ""], ["Cheung", "Alvin", ""], ["Krishnamurthy", "Jayant", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1704.08795", "submitter": "Yoav Artzi", "authors": "Dipendra Misra and John Langford and Yoav Artzi", "title": "Mapping Instructions and Visual Observations to Actions with\n  Reinforcement Learning", "comments": "In Proceedings of the Conference on Empirical Methods in Natural\n  Language Processing (EMNLP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to directly map raw visual observations and text input to actions\nfor instruction execution. While existing approaches assume access to\nstructured environment representations or use a pipeline of separately trained\nmodels, we learn a single model to jointly reason about linguistic and visual\ninput. We use reinforcement learning in a contextual bandit setting to train a\nneural network agent. To guide the agent's exploration, we use reward shaping\nwith different forms of supervision. Our approach does not require intermediate\nrepresentations, planning procedures, or training different models. We evaluate\nin a simulated environment, and show significant improvements over supervised\nlearning and common reinforcement learning variants.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 03:12:57 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 15:10:11 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Misra", "Dipendra", ""], ["Langford", "John", ""], ["Artzi", "Yoav", ""]]}, {"id": "1704.08798", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Word Affect Intensities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words often convey affect -- emotions, feelings, and attitudes. Lexicons of\nword-affect association have applications in automatic emotion analysis and\nnatural language generation. However, existing lexicons indicate only coarse\ncategories of affect association. Here, for the first time, we create an affect\nintensity lexicon with real-valued scores of association. We use a technique\ncalled best-worst scaling that improves annotation consistency and obtains\nreliable fine-grained scores. The lexicon includes terms common from both\ngeneral English and terms specific to social media communications. It has close\nto 6,000 entries for four basic emotions. We will be adding entries for other\naffect dimensions shortly.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 03:33:16 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "1704.08803", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, W. Bruce\n  Croft", "title": "Neural Ranking Models with Weak Supervision", "comments": "In proceedings of The 40th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 04:08:47 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 11:58:34 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Zamani", "Hamed", ""], ["Severyn", "Aliaksei", ""], ["Kamps", "Jaap", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1704.08893", "submitter": "Merel Scholman", "authors": "Vera Demberg, Fatemeh Torabi Asr, Merel Scholman", "title": "How compatible are our discourse annotations? Insights from mapping\n  RST-DT and PDTB annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse-annotated corpora are an important resource for the community, but\nthey are often annotated according to different frameworks. This makes\ncomparison of the annotations difficult, thereby also preventing researchers\nfrom searching the corpora in a unified way, or using all annotated data\njointly to train computational systems. Several theoretical proposals have\nrecently been made for mapping the relational labels of different frameworks to\neach other, but these proposals have so far not been validated against existing\nannotations. The two largest discourse relation annotated resources, the Penn\nDiscourse Treebank and the Rhetorical Structure Theory Discourse Treebank, have\nhowever been annotated on the same text, allowing for a direct comparison of\nthe annotation layers. We propose a method for automatically aligning the\ndiscourse segments, and then evaluate existing mapping proposals by comparing\nthe empirically observed against the proposed mappings. Our analysis highlights\nthe influence of segmentation on subsequent discourse relation labeling, and\nshows that while agreement between frameworks is reasonable for explicit\nrelations, agreement on implicit relations is low. We identify several sources\nof systematic discrepancies between the two annotation schemes and discuss\nconsequences of these discrepancies for future annotation and for the training\nof automatic discourse relation labellers.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 12:09:31 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 12:39:50 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Demberg", "Vera", ""], ["Asr", "Fatemeh Torabi", ""], ["Scholman", "Merel", ""]]}, {"id": "1704.08914", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Hinrich Sch\\\"utze", "title": "Past, Present, Future: A Computational Investigation of the Typology of\n  Tense in 1000 Languages", "comments": null, "journal-ref": "Extended version of EMNLP 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SuperPivot, an analysis method for low-resource languages that\noccur in a superparallel corpus, i.e., in a corpus that contains an order of\nmagnitude more languages than parallel corpora currently in use. We show that\nSuperPivot performs well for the crosslingual analysis of the linguistic\nphenomenon of tense. We produce analysis results for more than 1000 languages,\nconducting - to the best of our knowledge - the largest crosslingual\ncomputational study performed to date. We extend existing methodology for\nleveraging parallel corpora for typological analysis by overcoming a limiting\nassumption of earlier work: We only require that a linguistic feature is\novertly marked in a few of thousands of languages as opposed to requiring that\nit be marked in all languages under investigation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 13:11:09 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 17:20:41 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1704.08960", "submitter": "Jie Yang", "authors": "Jie Yang and Yue Zhang and Fei Dong", "title": "Neural Word Segmentation with Rich Pretraining", "comments": "Accepted by ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural word segmentation research has benefited from large-scale raw texts by\nleveraging them for pretraining character and word embeddings. On the other\nhand, statistical segmentation research has exploited richer sources of\nexternal information, such as punctuation, automatic segmentation and POS. We\ninvestigate the effectiveness of a range of external training sources for\nneural word segmentation by building a modular segmentation model, pretraining\nthe most important submodule using rich external sources. Results show that\nsuch pretraining significantly improves the model, leading to accuracies\ncompetitive to the best methods on six benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 14:46:25 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Yang", "Jie", ""], ["Zhang", "Yue", ""], ["Dong", "Fei", ""]]}, {"id": "1704.08966", "submitter": "Pierre Lison", "authors": "Pierre Lison, Serge Bibauw", "title": "Not All Dialogues are Created Equal: Instance Weighting for Neural\n  Conversational Models", "comments": "Accepted to SIGDIAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural conversational models require substantial amounts of dialogue data for\ntheir parameter estimation and are therefore usually learned on large corpora\nsuch as chat forums or movie subtitles. These corpora are, however, often\nchallenging to work with, notably due to their frequent lack of turn\nsegmentation and the presence of multiple references external to the dialogue\nitself. This paper shows that these challenges can be mitigated by adding a\nweighting model into the architecture. The weighting model, which is itself\nestimated from dialogue data, associates each training example to a numerical\nweight that reflects its intrinsic quality for dialogue modelling. At training\ntime, these sample weights are included into the empirical loss to be\nminimised. Evaluation results on retrieval-based models trained on movie and TV\nsubtitles demonstrate that the inclusion of such a weighting model improves the\nmodel performance on unsupervised metrics.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 14:57:29 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 17:27:13 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lison", "Pierre", ""], ["Bibauw", "Serge", ""]]}]