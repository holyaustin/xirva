[{"id": "0905.0740", "submitter": "Ignacio Vega-Paez M en C", "authors": "Gerardo Cisneros", "title": "A FORTRAN coded regular expression Compiler for IBM 1130 Computing\n  System", "comments": "This version of REC is archaeological reconstruction of REC/A\n  language on IBM1130 Simulator (SIMH IBM 1130 Emulator and Disk Monitor System\n  R2V12) from Computer History Simulation Project (www.ibm1130.org), also see\n  REC language is a live for Ignacio Vega-Paez", "journal-ref": "Acta Mexicana de Ciencia y Tecnologia Vol. IV No. 1, page 30-86,\n  1970", "doi": null, "report-no": "IBP-Memo 2008-12", "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  REC (Regular Expression Compiler) is a concise programming language which\nallows students to write programs without knowledge of the complicated syntax\nof languages like FORTRAN and ALGOL. The language is recursive and contains\nonly four elements for control. This paper describes an interpreter of REC\nwritten in FORTRAN.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2009 04:29:51 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Cisneros", "Gerardo", ""]]}, {"id": "0905.1130", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Florian Boudin, Patricia Velazquez-Morales and Juan-Manuel\n  Torres-Moreno", "title": "Statistical Automatic Summarization in Organic Chemistry", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an oriented numerical summarizer algorithm, applied to producing\nautomatic summaries of scientific documents in Organic Chemistry. We present\nits implementation named Yachs (Yet Another Chemistry Summarizer) that combines\na specific document pre-processing with a sentence scoring method relying on\nthe statistical properties of documents. We show that Yachs achieves the best\nresults among several other summarizers on a corpus of Organic Chemistry\narticles.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2009 20:21:45 GMT"}], "update_date": "2009-05-11", "authors_parsed": [["Boudin", "Florian", ""], ["Velazquez-Morales", "Patricia", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "0905.1235", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov, Stephen Sinclair, Ian Cl\\'ement, Dimitrios\n  Nicolacopoulos (for the MARF R&D Group)", "title": "The Modular Audio Recognition Framework (MARF) and its Applications:\n  Scientific and Software Engineering Notes", "comments": "v2: add missing .ind file for index; 224 pages, 40 figures, 19\n  tables; index. A comprehensive description of AI and PR algorithms and data\n  structures, software engineering design and implementation, and experiments.\n  Source revision is maintained in the CVS at http://marf.sf.net", "journal-ref": null, "doi": "10.1007/978-1-4020-8741-7_84 10.1007/978-3-540-68825-9_21\n  10.1145/1370256.1370262", "report-no": null, "categories": "cs.SD cs.CL cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MARF is an open-source research platform and a collection of\nvoice/sound/speech/text and natural language processing (NLP) algorithms\nwritten in Java and arranged into a modular and extensible framework\nfacilitating addition of new algorithms. MARF can run distributively over the\nnetwork and may act as a library in applications or be used as a source for\nlearning and extension. A few example applications are provided to show how to\nuse the framework. There is an API reference in the Javadoc format as well as\nthis set of accompanying notes with the detailed description of the\narchitectural design, algorithms, and applications. MARF and its applications\nare released under a BSD-style license and is hosted at SourceForge.net. This\ndocument provides the details and the insight on the internals of MARF and some\nof the mentioned applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2009 14:42:03 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2009 04:13:29 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Mokhov", "Serguei A.", "", "for the MARF R&D Group"], ["Sinclair", "Stephen", "", "for the MARF R&D Group"], ["Cl\u00e9ment", "Ian", "", "for the MARF R&D Group"], ["Nicolacopoulos", "Dimitrios", "", "for the MARF R&D Group"]]}, {"id": "0905.1609", "submitter": "Nabil Hathout", "authors": "Nabil Hathout (CLLE)", "title": "Acquisition of morphological families and derivational series from a\n  machine readable dictionary", "comments": "proceedings of the 6th D\\'ecembrettes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a linguistic and computational model aiming at making the\nmorphological structure of the lexicon emerge from the formal and semantic\nregularities of the words it contains. The model is word-based. The proposed\nmorphological structure consists of (1) binary relations that connect each\nheadword with words that are morphologically related, and especially with the\nmembers of its morphological family and its derivational series, and of (2) the\nanalogies that hold between the words. The model has been tested on the lexicon\nof French using the TLFi machine readable dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2009 12:17:36 GMT"}], "update_date": "2009-05-12", "authors_parsed": [["Hathout", "Nabil", "", "CLLE"]]}, {"id": "0905.2990", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno and Pier-Luc St-Onge and Michel Gagnon and\n  Marc El-B\\`eze and Patrice Bellot", "title": "Automatic Summarization System coupled with a Question-Answering System\n  (QAAS)", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To select the most relevant sentences of a document, it uses an optimal\ndecision algorithm that combines several metrics. The metrics processes,\nweighting and extract pertinence sentences by statistical and informational\nalgorithms. This technique might improve a Question-Answering system, whose\nfunction is to provide an exact answer to a question in natural language. In\nthis paper, we present the results obtained by coupling the Cortex summarizer\nwith a Question-Answering system (QAAS). Two configurations have been\nevaluated. In the first one, a low compression level is selected and the\nsummarization system is only used as a noise filter. In the second\nconfiguration, the system actually functions as a summarizer, with a very high\nlevel of compression. Our results on French corpus demonstrate that the\ncoupling of Automatic Summarization system with a Question-Answering system is\npromising. Then the system has been adapted to generate a customized summary\ndepending on the specific question. Tests on a french multi-document corpus\nhave been realized, and the personalized QAAS system obtains the best\nperformances.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 21:35:52 GMT"}], "update_date": "2009-05-20", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["St-Onge", "Pier-Luc", ""], ["Gagnon", "Michel", ""], ["El-B\u00e8ze", "Marc", ""], ["Bellot", "Patrice", ""]]}, {"id": "0905.3318", "submitter": "Maarten Hijzelendoorn", "authors": "Maarten Hijzelendoorn and Crit Cremers", "title": "An Object-Oriented and Fast Lexicon for Semantic Generation", "comments": "Paper presented at the 18th Computational Linguistics In the\n  Netherlands Meeting (CLIN), Nijmegen, 10 December 2007, 15pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.DS cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the technical design of a large computational lexicon,\nits storage, and its access from a Prolog environment. Traditionally, efficient\naccess and storage of data structures is implemented by a relational database\nmanagement system. In Delilah, a lexicon-based NLP system, efficient access to\nthe lexicon by the semantic generator is vital. We show that our highly\ndetailed HPSG-style lexical specifications do not fit well in the Relational\nModel, and that they cannot be efficiently retrieved. We argue that they fit\nmore naturally in the Object-Oriented Model. Although storage of objects is\nredundant, we claim that efficient access is still possible by applying\nindexing, and compression techniques from the Relational Model to the\nObject-Oriented Model. We demonstrate that it is possible to implement\nobject-oriented storage and fast access in ISO Prolog.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 14:12:42 GMT"}], "update_date": "2009-05-21", "authors_parsed": [["Hijzelendoorn", "Maarten", ""], ["Cremers", "Crit", ""]]}, {"id": "0905.4039", "submitter": "Paul Vitanyi", "authors": "Rudi L. Cilibrasi (software consultant Oakland, CA) and Paul M.B.\n  Vitanyi (CWI, Amsterdam)", "title": "Normalized Web Distance and Word Similarity", "comments": "Latex, 20 pages, 7 figures, to appear in: Handbook of Natural\n  Language Processing, Second Edition, Nitin Indurkhya and Fred J. Damerau\n  Eds., CRC Press, Taylor and Francis Group, Boca Raton, FL, 2010, ISBN\n  978-1420085921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great deal of work in cognitive psychology, linguistics, and\ncomputer science, about using word (or phrase) frequencies in context in text\ncorpora to develop measures for word similarity or word association, going back\nto at least the 1960s. The goal of this chapter is to introduce the\nnormalizedis a general way to tap the amorphous low-grade knowledge available\nfor free on the Internet, typed in by local users aiming at personal\ngratification of diverse objectives, and yet globally achieving what is\neffectively the largest semantic electronic database in the world. Moreover,\nthis database is available for all by using any search engine that can return\naggregate page-count estimates for a large range of search-queries. In the\npaper introducing the NWD it was called `normalized Google distance (NGD),' but\nsince Google doesn't allow computer searches anymore, we opt for the more\nneutral and descriptive NWD. web distance (NWD) method to determine similarity\nbetween words and phrases. It\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2009 15:42:40 GMT"}], "update_date": "2009-05-26", "authors_parsed": [["Cilibrasi", "Rudi L.", "", "software consultant Oakland, CA"], ["Vitanyi", "Paul M. B.", "", "CWI, Amsterdam"]]}]