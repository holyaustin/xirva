[{"id": "1003.0206", "submitter": "Steven Wegmann", "authors": "Steven Wegmann and Larry Gillick", "title": "Why has (reasonably accurate) Automatic Speech Recognition been so hard\n  to achieve?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) have been successfully applied to automatic\nspeech recognition for more than 35 years in spite of the fact that a key HMM\nassumption -- the statistical independence of frames -- is obviously violated\nby speech data. In fact, this data/model mismatch has inspired many attempts to\nmodify or replace HMMs with alternative models that are better able to take\ninto account the statistical dependence of frames. However it is fair to say\nthat in 2010 the HMM is the consensus model of choice for speech recognition\nand that HMMs are at the heart of both commercially available products and\ncontemporary research systems. In this paper we present a preliminary\nexploration aimed at understanding how speech data depart from HMMs and what\neffect this departure has on the accuracy of HMM-based speech recognition. Our\nanalysis uses standard diagnostic tools from the field of statistics --\nhypothesis testing, simulation and resampling -- which are rarely used in the\nfield of speech recognition. Our main result, obtained by novel manipulations\nof real and resampled data, demonstrates that real data have statistical\ndependency and that this dependency is responsible for significant numbers of\nrecognition errors. We also demonstrate, using simulation and resampling, that\nif we `remove' the statistical dependency from data, then the resulting\nrecognition error rates become negligible. Taken together, these results\nsuggest that a better understanding of the structure of the statistical\ndependency in speech data is a crucial first step towards improving HMM-based\nspeech recognition.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 19:00:12 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Wegmann", "Steven", ""], ["Gillick", "Larry", ""]]}, {"id": "1003.0337", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov", "title": "Change of word types to word tokens ratio in the course of translation\n  (based on Russian translations of K. Vonnegut novels)", "comments": "11 pages, 5 figures, to be reported at International Computational\n  Linguistic Conference \"Dialog-21\"-2010 (http://dialog-21.ru)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The article provides lexical statistical analysis of K. Vonnegut's two novels\nand their Russian translations. It is found out that there happen some changes\nbetween the speed of word types and word tokens ratio change in the source and\ntarget texts. The author hypothesizes that these changes are typical for\nEnglish-Russian translations, and moreover, they represent an example of\nBaker's translation feature of levelling out.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 18:04:39 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Kutuzov", "Andrey", ""]]}, {"id": "1003.0628", "submitter": "Krishnakumar Balasubramanian", "authors": "Yi Mao, Krishnakumar Balasubramanian, Guy Lebanon", "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text documents are complex high dimensional objects. To effectively visualize\nsuch data it is important to reduce its dimensionality and visualize the low\ndimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore\ndimensionality reduction methods that draw upon domain knowledge in order to\nachieve a better low dimensional embedding and visualization of documents. We\nconsider the use of geometries specified manually by an expert, geometries\nderived automatically from corpus statistics, and geometries computed from\nlinguistic resources.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 16:52:32 GMT"}], "update_date": "2010-03-03", "authors_parsed": [["Mao", "Yi", ""], ["Balasubramanian", "Krishnakumar", ""], ["Lebanon", "Guy", ""]]}, {"id": "1003.1141", "submitter": "Peter Turney", "authors": "Peter D. Turney and Patrick Pantel", "title": "From Frequency to Meaning: Vector Space Models of Semantics", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research, (2010), 37, 141-188", "doi": "10.1613/jair.2934", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers understand very little of the meaning of human language. This\nprofoundly limits our ability to give instructions to computers, the ability of\ncomputers to explain their actions to us, and the ability of computers to\nanalyse and process text. Vector space models (VSMs) of semantics are beginning\nto address these limits. This paper surveys the use of VSMs for semantic\nprocessing of text. We organize the literature on VSMs according to the\nstructure of the matrix in a VSM. There are currently three broad classes of\nVSMs, based on term-document, word-context, and pair-pattern matrices, yielding\nthree classes of applications. We survey a broad range of applications in these\nthree categories and we take a detailed look at a specific open source project\nin each category. Our goal in this survey is to show the breadth of\napplications of VSMs for semantics, to provide a new perspective on VSMs for\nthose who are already familiar with the area, and to provide pointers into the\nliterature for those who are less familiar with the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 21:07:18 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Turney", "Peter D.", ""], ["Pantel", "Patrick", ""]]}, {"id": "1003.1399", "submitter": "Zolt\\'an K\\'asa", "authors": "Peter Vaclavik, Jaroslav Poruban, Marek Mezei", "title": "Automatic derivation of domain terms and concept location based on the\n  analysis of the identifiers", "comments": null, "journal-ref": "Acta Univ. Sapientiae, Informatica, 2,1 (2010) 40-50", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers express the meaning of the domain ideas in specifically selected\nidentifiers and comments that form the target implemented code. Software\nmaintenance requires knowledge and understanding of the encoded ideas. This\npaper presents a way how to create automatically domain vocabulary. Knowledge\nof domain vocabulary supports the comprehension of a specific domain for later\ncode maintenance or evolution. We present experiments conducted in two selected\ndomains: application servers and web frameworks. Knowledge of domain terms\nenables easy localization of chunks of code that belong to a certain term. We\nconsider these chunks of code as \"concepts\" and their placement in the code as\n\"concept location\". Application developers may also benefit from the obtained\ndomain terms. These terms are parts of speech that characterize a certain\nconcept. Concepts are encoded in \"classes\" (OO paradigm) and the obtained\nvocabulary of terms supports the selection and the comprehension of the class'\nappropriate identifiers. We measured the following software products with our\ntool: JBoss, JOnAS, GlassFish, Tapestry, Google Web Toolkit and Echo2.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2010 16:34:40 GMT"}], "update_date": "2010-03-13", "authors_parsed": [["Vaclavik", "Peter", ""], ["Poruban", "Jaroslav", ""], ["Mezei", "Marek", ""]]}, {"id": "1003.1410", "submitter": "Seungyeon Kim", "authors": "Seungyeon Kim, Guy Lebanon", "title": "Local Space-Time Smoothing for Version Controlled Documents", "comments": "9 pages, 6 figures", "journal-ref": "Proceedings of the 23rd International Conference on Computational\n  Linguistics (Coling 2010); 2010 Aug 23-27; Beijing, CN", "doi": null, "report-no": null, "categories": "cs.GR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike static documents, version controlled documents are continuously edited\nby one or more authors. Such collaborative revision process makes traditional\nmodeling and visualization techniques inappropriate. In this paper we propose a\nnew representation based on local space-time smoothing that captures important\nrevision patterns. We demonstrate the applicability of our framework using\nexperiments on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2010 18:08:12 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 18:05:00 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Kim", "Seungyeon", ""], ["Lebanon", "Guy", ""]]}, {"id": "1003.1455", "submitter": "Rdv Ijcsis", "authors": "Rama N., Meenakshi Lakshmanan", "title": "A Computational Algorithm based on Empirical Analysis, that Composes\n  Sanskrit Poetry", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS February 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 2, pp. 056-062, February 2010, USA", "doi": null, "report-no": "Computer Science ISSN 19475500", "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Poetry-writing in Sanskrit is riddled with problems for even those who know\nthe language well. This is so because the rules that govern Sanskrit prosody\nare numerous and stringent. We propose a computational algorithm that converts\nprose given as E-text into poetry in accordance with the metrical rules of\nSanskrit prosody, simultaneously taking care to ensure that sandhi or euphonic\nconjunction, which is compulsory in verse, is handled. The algorithm is\nconsiderably speeded up by a novel method of reducing the target search\ndatabase. The algorithm further gives suggestions to the poet in case what\nhe/she has given as the input prose is impossible to fit into any allowed\nmetrical format. There is also an interactive component of the algorithm by\nwhich the algorithm interacts with the poet to resolve ambiguities. In\naddition, this unique work, which provides a solution to a problem that has\nnever been addressed before, provides a simple yet effective speech recognition\ninterface that would help the visually impaired dictate words in E-text, which\nis in turn versified by our Poetry Composer Engine.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 11:28:08 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["N.", "Rama", ""], ["Lakshmanan", "Meenakshi", ""]]}, {"id": "1003.4065", "submitter": "William Jackson", "authors": "Chien-Ying Chen, Jen-Yuan Yeh, Hao-Ren Ke", "title": "Plagiarism Detection using ROUGE and WordNet", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010", "doi": null, "report-no": null, "categories": "cs.OH cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the arrival of digital era and Internet, the lack of information control\nprovides an incentive for people to freely use any content available to them.\nPlagiarism occurs when users fail to credit the original owner for the content\nreferred to, and such behavior leads to violation of intellectual property. Two\nmain approaches to plagiarism detection are fingerprinting and term occurrence;\nhowever, one common weakness shared by both approaches, especially\nfingerprinting, is the incapability to detect modified text plagiarism. This\nstudy proposes adoption of ROUGE and WordNet to plagiarism detection. The\nformer includes ngram co-occurrence statistics, skip-bigram, and longest common\nsubsequence (LCS), while the latter acts as a thesaurus and provides semantic\ninformation. N-gram co-occurrence statistics can detect verbatim copy and\ncertain sentence modification, skip-bigram and LCS are immune from text\nmodification such as simple addition or deletion of words, and WordNet may\nhandle the problem of word substitution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 05:20:31 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Chen", "Chien-Ying", ""], ["Yeh", "Jen-Yuan", ""], ["Ke", "Hao-Ren", ""]]}, {"id": "1003.4149", "submitter": "Claude Martineau", "authors": "Claude Martineau (IGM-LabInfo), Elsa Tolone (IGM-LabInfo), Stavroula\n  Voyatzi (IGM-LabInfo)", "title": "Les Entit\\'es Nomm\\'ees : usage et degr\\'es de pr\\'ecision et de\n  d\\'esambigu\\\"isation", "comments": null, "journal-ref": "26\\`eme Colloque international sur le Lexique et la Grammaire\n  (LGC'07), Bonifacio : France (2007)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition and classification of Named Entities (NER) are regarded as an\nimportant component for many Natural Language Processing (NLP) applications.\nThe classification is usually made by taking into account the immediate context\nin which the NE appears. In some cases, this immediate context does not allow\ngetting the right classification. We show in this paper that the use of an\nextended syntactic context and large-scale resources could be very useful in\nthe NER task.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 13:09:57 GMT"}], "update_date": "2010-04-01", "authors_parsed": [["Martineau", "Claude", "", "IGM-LabInfo"], ["Tolone", "Elsa", "", "IGM-LabInfo"], ["Voyatzi", "Stavroula", "", "IGM-LabInfo"]]}, {"id": "1003.4394", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Bob Coecke, Mehrnoosh Sadrzadeh, Stephen Clark", "title": "Mathematical Foundations for a Compositional Distributional Model of\n  Meaning", "comments": "to appear", "journal-ref": "Lambek Festschirft, special issue of Linguistic Analysis, 2010.", "doi": null, "report-no": null, "categories": "cs.CL cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mathematical framework for a unification of the distributional\ntheory of meaning in terms of vector space models, and a compositional theory\nfor grammatical types, for which we rely on the algebra of Pregroups,\nintroduced by Lambek. This mathematical framework enables us to compute the\nmeaning of a well-typed sentence from the meanings of its constituents.\nConcretely, the type reductions of Pregroups are `lifted' to morphisms in a\ncategory, a procedure that transforms meanings of constituents into a meaning\nof the (well-typed) whole. Importantly, meanings of whole sentences live in a\nsingle space, independent of the grammatical structure of the sentence. Hence\nthe inner-product can be used to compare meanings of arbitrary sentences, as it\nis for comparing the meanings of words in the distributional model. The\nmathematical structure we employ admits a purely diagrammatic calculus which\nexposes how the information flows between the words in a sentence in order to\nmake up the meaning of the whole sentence. A variation of our `categorical\nmodel' which involves constraining the scalars of the vector spaces to the\nsemiring of Booleans results in a Montague-style Boolean-valued semantics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2010 12:32:01 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Coecke", "Bob", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Clark", "Stephen", ""]]}, {"id": "1003.4894", "submitter": "Michel Aurnague", "authors": "Michel Aurnague (CLLE), Laure Vieu (IRIT), Andr\\'ee Borillo (CLLE)", "title": "La repr\\'esentation formelle des concepts spatiaux dans la langue", "comments": null, "journal-ref": "Langage et cognition spatiale, Michel Denis (Ed.) (1997) 69-102", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we assume that systematically studying spatial markers\nsemantics in language provides a means to reveal fundamental properties and\nconcepts characterizing conceptual representations of space. We propose a\nformal system accounting for the properties highlighted by the linguistic\nanalysis, and we use these tools for representing the semantic content of\nseveral spatial relations of French. The first part presents a semantic\nanalysis of the expression of space in French aiming at describing the\nconstraints that formal representations have to take into account. In the\nsecond part, after presenting the structure of our formal system, we set out\nits components. A commonsense geometry is sketched out and several functional\nand pragmatic spatial concepts are formalized. We take a special attention in\nshowing that these concepts are well suited to representing the semantic\ncontent of several prepositions of French ('sur' (on), 'dans' (in), 'devant'\n(in front of), 'au-dessus' (above)), and in illustrating the inferential\nadequacy of these representations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 14:03:51 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Aurnague", "Michel", "", "CLLE"], ["Vieu", "Laure", "", "IRIT"], ["Borillo", "Andr\u00e9e", "", "CLLE"]]}, {"id": "1003.4898", "submitter": "Michel Aurnague", "authors": "Michel Aurnague (CLLE), Maya Hickmann (SFLTAMP), Laure Vieu (IRIT)", "title": "Les entit\\'es spatiales dans la langue : \\'etude descriptive, formelle\n  et exp\\'erimentale de la cat\\'egorisation", "comments": null, "journal-ref": "Agir dans l'espace, Catherine Thinus-Blanc & Jean Bullier (Ed.)\n  (2005) 217-232", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While previous linguistic and psycholinguistic research on space has mainly\nanalyzed spatial relations, the studies reported in this paper focus on how\nlanguage distinguishes among spatial entities. Descriptive and experimental\nstudies first propose a classification of entities, which accounts for both\nstatic and dynamic space, has some cross-linguistic validity, and underlies\nadults' cognitive processing. Formal and computational analyses then introduce\ntheoretical elements aiming at modelling these categories, while fulfilling\nvarious properties of formal ontologies (generality, parsimony, coherence...).\nThis formal framework accounts, in particular, for functional dependences among\nentities underlying some part-whole descriptions. Finally, developmental\nresearch shows that language-specific properties have a clear impact on how\nchildren talk about space. The results suggest some cross-linguistic\nvariability in children's spatial representations from an early age onwards,\nbringing into question models in which general cognitive capacities are the\nonly determinants of spatial cognition during the course of development.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 14:08:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Aurnague", "Michel", "", "CLLE"], ["Hickmann", "Maya", "", "SFLTAMP"], ["Vieu", "Laure", "", "IRIT"]]}, {"id": "1003.5372", "submitter": "Stergos Afantenos", "authors": "Stergos Afantenos, Pascal Denis, Philippe Muller, Laurence Danlos", "title": "Learning Recursive Segments for Discourse Parsing", "comments": "published at LREC 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting discourse segments is an important preliminary step\ntowards full discourse parsing. Previous research on discourse segmentation\nhave relied on the assumption that elementary discourse units (EDUs) in a\ndocument always form a linear sequence (i.e., they can never be nested).\nUnfortunately, this assumption turns out to be too strong, for some theories of\ndiscourse like SDRT allows for nested discourse units. In this paper, we\npresent a simple approach to discourse segmentation that is able to produce\nnested EDUs. Our approach builds on standard multi-class classification\ntechniques combined with a simple repairing heuristic that enforces global\ncoherence. Our system was developed and evaluated on the first round of\nannotations provided by the French Annodis project (an ongoing effort to create\na discourse bank for French). Cross-validated on only 47 documents (1,445\nEDUs), our system achieves encouraging performance results with an F-score of\n73% for finding EDUs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2010 15:17:22 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Afantenos", "Stergos", ""], ["Denis", "Pascal", ""], ["Muller", "Philippe", ""], ["Danlos", "Laurence", ""]]}, {"id": "1003.5749", "submitter": "Sylvie Billot", "authors": "Iris Eshkol (CORAL), Isabelle Tellier (LIFO), Taalab Samer (LIFO),\n  Sylvie Billot (LIFO)", "title": "Etiqueter un corpus oral par apprentissage automatique \\`a l'aide de\n  connaissances linguistiques", "comments": null, "journal-ref": "10\\`emes Journ\\'ees Internationales d'Analyse statistique des\n  Donn\\'ees Textuelles JADT'2010, Rome : Italie (2010)", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the Eslo1 (\"Enqu\\^ete sociolinguistique d'Orl\\'eans\", i.e.\n\"Sociolinguistic Inquiery of Orl\\'eans\") campain, a large oral corpus has been\ngathered and transcribed in a textual format. The purpose of the work presented\nhere is to associate a morpho-syntactic label to each unit of this corpus. To\nthis aim, we have first studied the specificities of the necessary labels, and\ntheir various possible levels of description. This study has led to a new\noriginal hierarchical structuration of labels. Then, considering that our new\nset of labels was different from the one used in every available software, and\nthat these softwares usually do not fit for oral data, we have built a new\nlabeling tool by a Machine Learning approach, from data labeled by Cordial and\ncorrected by hand. We have applied linear CRF (Conditional Random Fields)\ntrying to take the best possible advantage of the linguistic knowledge that was\nused to define the set of labels. We obtain an accuracy between 85 and 90%,\ndepending of the parameters used.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 07:04:46 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Eshkol", "Iris", "", "CORAL"], ["Tellier", "Isabelle", "", "LIFO"], ["Samer", "Taalab", "", "LIFO"], ["Billot", "Sylvie", "", "LIFO"]]}]