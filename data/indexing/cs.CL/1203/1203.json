[{"id": "1203.0145", "submitter": "Peter beim Graben", "authors": "Peter beim Graben", "title": "The Horse Raced Past: Gardenpath Processing in Dynamical Systems", "comments": "The paper has been withdrawn by the author due to some\n  inconsistencies in the argumentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I pinpoint an interesting similarity between a recent account to rational\nparsing and the treatment of sequential decisions problems in a dynamical\nsystems approach. I argue that expectation-driven search heuristics aiming at\nfast computation resembles a high-risk decision strategy in favor of large\ntransition velocities. Hale's rational parser, combining generalized\nleft-corner parsing with informed $\\mathrm{A}^*$ search to resolve processing\nconflicts, explains gardenpath effects in natural sentence processing by\nmisleading estimates of future processing costs that are to be minimized. On\nthe other hand, minimizing the duration of cognitive computations in\ntime-continuous dynamical systems can be described by combining vector space\nrepresentations of cognitive states by means of filler/role decompositions and\nsubsequent tensor product representations with the paradigm of stable\nheteroclinic sequences. Maximizing transition velocities according to a\nhigh-risk decision strategy could account for a fast race even between states\nthat are apparently remote in representation space.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 11:06:32 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2012 09:07:49 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Graben", "Peter beim", ""]]}, {"id": "1203.0504", "submitter": "Martin Bachwerk", "authors": "Martin Bachwerk and Carl Vogel", "title": "Modelling Social Structures and Hierarchies in Language Evolution", "comments": "14 pages, 3 figures, 1 table. In proceedings of AI-2010, The\n  Thirtieth SGAI International Conference on Innovative Techniques and\n  Applications of Artificial Intelligence, Cambridge, England, UK, 14-16\n  December 2010", "journal-ref": "Research and Development in Intelligent Systems XXVII, 2011, pp.\n  49-62", "doi": "10.1007/978-0-85729-130-1_4", "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language evolution might have preferred certain prior social configurations\nover others. Experiments conducted with models of different social structures\n(varying subgroup interactions and the role of a dominant interlocutor) suggest\nthat having isolated agent groups rather than an interconnected agent is more\nadvantageous for the emergence of a social communication system. Distinctive\ngroups that are closely connected by communication yield systems less like\nnatural language than fully isolated groups inhabiting the same world.\nFurthermore, the addition of a dominant male who is asymmetrically favoured as\na hearer, and equally likely to be a speaker has no positive influence on the\ndisjoint groups.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 16:03:41 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Bachwerk", "Martin", ""], ["Vogel", "Carl", ""]]}, {"id": "1203.0512", "submitter": "Martin Bachwerk", "authors": "Martin Bachwerk and Carl Vogel", "title": "Establishing linguistic conventions in task-oriented primeval dialogue", "comments": "8 pages, 5 figures. In proceedings of the COST 2102 International\n  Conference, Budapest, Hungary, September 7-10, 2010, Revised Selected Papers", "journal-ref": "Analysis of Verbal and Nonverbal Communication and Enactment. The\n  Processing Issues. Lecture Notes in Computer Science, Volume 6800/2011, 2011,\n  pp. 48-55", "doi": "10.1007/978-3-642-25775-9_4", "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we claim that language is likely to have emerged as a\nmechanism for coordinating the solution of complex tasks. To confirm this\nthesis, computer simulations are performed based on the coordination task\npresented by Garrod & Anderson (1987). The role of success in task-oriented\ndialogue is analytically evaluated with the help of performance measurements\nand a thorough lexical analysis of the emergent communication system.\nSimulation results confirm a strong effect of success mattering on both\nreliability and dispersion of linguistic conventions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 16:25:13 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Bachwerk", "Martin", ""], ["Vogel", "Carl", ""]]}, {"id": "1203.1685", "submitter": "Win Win  Thant", "authors": "Win Win Thant, Tin Myat Htwe and Ni Lar Thein", "title": "Statistical Function Tagging and Grammatical Relations of Myanmar\n  Sentences", "comments": "16 pages, 7 figures, 8 tables, AIAA-2011 (India). arXiv admin note:\n  text overlap with arXiv:0912.1820 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a context free grammar (CFG) based grammatical relations\nfor Myanmar sentences which combine corpus-based function tagging system. Part\nof the challenge of statistical function tagging for Myanmar sentences comes\nfrom the fact that Myanmar has free-phrase-order and a complex morphological\nsystem. Function tagging is a pre-processing step to show grammatical relations\nof Myanmar sentences. In the task of function tagging, which tags the function\nof Myanmar sentences with correct segmentation, POS (part-of-speech) tagging\nand chunking information, we use Naive Bayesian theory to disambiguate the\npossible function tags of a word. We apply context free grammar (CFG) to find\nout the grammatical relations of the function tags. We also create a functional\nannotated tagged corpus for Myanmar and propose the grammar rules for Myanmar\nsentences. Experiments show that our analysis achieves a good result with\nsimple sentences and complex sentences.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 03:06:29 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Thant", "Win Win", ""], ["Htwe", "Tin Myat", ""], ["Thein", "Ni Lar", ""]]}, {"id": "1203.1743", "submitter": "Christian Retore", "authors": "Christian Retor\\'e (LaBRI)", "title": "Variable types for meaning assembly: a logical syntax for generic noun\n  phrases introduced by most", "comments": null, "journal-ref": "Recherches linguistiques de Vincennes 41 (2012) 18", "doi": null, "report-no": null, "categories": "math.LO cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a way to compute the meanings associated with sentences\nwith generic noun phrases corresponding to the generalized quantifier most. We\ncall these generics specimens and they resemble stereotypes or prototypes in\nlexical semantics. The meanings are viewed as logical formulae that can\nthereafter be interpreted in your favourite models. To do so, we depart\nsignificantly from the dominant Fregean view with a single untyped universe.\nIndeed, our proposal adopts type theory with some hints from Hilbert\n\\epsilon-calculus (Hilbert, 1922; Avigad and Zach, 2008) and from medieval\nphilosophy, see e.g. de Libera (1993, 1996). Our type theoretic analysis bears\nsome resemblance with ongoing work in lexical semantics (Asher 2011; Bassac et\nal. 2010; Moot, Pr\\'evot and Retor\\'e 2011). Our model also applies to\nclassical examples involving a class, or a generic element of this class, which\nis not uttered but provided by the context. An outcome of this study is that,\nin the minimalism-contextualism debate, see Conrad (2011), if one adopts a type\ntheoretical view, terms encode the purely semantic meaning component while\ntheir typing is pragmatically determined.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 10:47:07 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Retor\u00e9", "Christian", "", "LaBRI"]]}, {"id": "1203.1858", "submitter": "Saif Mohammad", "authors": "Saif M. Mohammad and Graeme Hirst", "title": "Distributional Measures of Semantic Distance: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to mimic human notions of semantic distance has widespread\napplications. Some measures rely only on raw text (distributional measures) and\nsome rely on knowledge sources such as WordNet. Although extensive studies have\nbeen performed to compare WordNet-based measures with human judgment, the use\nof distributional measures as proxies to estimate semantic distance has\nreceived little attention. Even though they have traditionally performed poorly\nwhen compared to WordNet-based measures, they lay claim to certain uniquely\nattractive features, such as their applicability in resource-poor languages and\ntheir ability to mimic both semantic similarity and semantic relatedness.\nTherefore, this paper presents a detailed study of distributional measures.\nParticular attention is paid to flesh out the strengths and limitations of both\nWordNet-based and distributional measures, and how distributional measures of\ndistance can be brought more in line with human notions of semantic distance.\nWe conclude with a brief discussion of recent work on hybrid measures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 17:29:33 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Mohammad", "Saif M.", ""], ["Hirst", "Graeme", ""]]}, {"id": "1203.1889", "submitter": "Saif Mohammad", "authors": "Saif M Mohammad and Graeme Hirst", "title": "Distributional Measures as Proxies for Semantic Relatedness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic ranking of word pairs as per their semantic relatedness and\nability to mimic human notions of semantic relatedness has widespread\napplications. Measures that rely on raw data (distributional measures) and\nthose that use knowledge-rich ontologies both exist. Although extensive studies\nhave been performed to compare ontological measures with human judgment, the\ndistributional measures have primarily been evaluated by indirect means. This\npaper is a detailed study of some of the major distributional measures; it\nlists their respective merits and limitations. New measures that overcome these\ndrawbacks, that are more in line with the human notions of semantic\nrelatedness, are suggested. The paper concludes with an exhaustive comparison\nof the distributional and ontology-based measures. Along the way, significant\nresearch problems are identified. Work on these problems may lead to a better\nunderstanding of how semantic relatedness is to be measured.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 19:03:37 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Mohammad", "Saif M", ""], ["Hirst", "Graeme", ""]]}, {"id": "1203.2293", "submitter": "Jose Fontanari", "authors": "Sergey Petrov, Jose F. Fontanari and Leonid I. Perlovsky", "title": "Categories of Emotion names in Web retrieved texts", "comments": null, "journal-ref": "International Journal of Psychology and Behavioral Sciences 2\n  (2012) 173-184", "doi": "10.5923/j.ijpbs.20120205.08", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorization of emotion names, i.e., the grouping of emotion words that\nhave similar emotional connotations together, is a key tool of Social\nPsychology used to explore people's knowledge about emotions. Without\nexception, the studies following that research line were based on the gauging\nof the perceived similarity between emotion names by the participants of the\nexperiments. Here we propose and examine a new approach to study the categories\nof emotion names - the similarities between target emotion names are obtained\nby comparing the contexts in which they appear in texts retrieved from the\nWorld Wide Web. This comparison does not account for any explicit semantic\ninformation; it simply counts the number of common words or lexical items used\nin the contexts. This procedure allows us to write the entries of the\nsimilarity matrix as dot products in a linear vector space of contexts. The\nproperties of this matrix were then explored using Multidimensional Scaling\nAnalysis and Hierarchical Clustering. Our main findings, namely, the underlying\ndimension of the emotion space and the categories of emotion names, were\nconsistent with those based on people's judgments of emotion names\nsimilarities.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 00:04:57 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Petrov", "Sergey", ""], ["Fontanari", "Jose F.", ""], ["Perlovsky", "Leonid I.", ""]]}, {"id": "1203.2299", "submitter": "Maxim Makatchev", "authors": "Maxim Makatchev, Reid Simmons, Majd Sakr", "title": "A Cross-cultural Corpus of Annotated Verbal and Nonverbal Behaviors in\n  Receptionist Encounters", "comments": "7 pages, 3 figures, presented at the Workshop on Gaze in HRI: From\n  Modeling to Communication (a Workshop of International Conference on\n  Human-Robot Interaction), March 5, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first annotated corpus of nonverbal behaviors in receptionist\ninteractions, and the first nonverbal corpus (excluding the original video and\naudio data) of service encounters freely available online. Native speakers of\nAmerican English and Arabic participated in a naturalistic role play at\nreception desks of university buildings in Doha, Qatar and Pittsburgh, USA.\nTheir manually annotated nonverbal behaviors include gaze direction, hand and\nhead gestures, torso positions, and facial expressions. We discuss possible\nuses of the corpus and envision it to become a useful tool for the human-robot\ninteraction community.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 00:45:01 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Makatchev", "Maxim", ""], ["Simmons", "Reid", ""], ["Sakr", "Majd", ""]]}, {"id": "1203.2498", "submitter": "Riadh Bouslimi", "authors": "Riadh Bouslimi, Houda Amraoui", "title": "Fault detection system for Arabic language", "comments": "4th International Workshop on ICTs and Amazigh, February 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of natural language, especially Arabic, and mechanisms for the\nimplementation of automatic processing is a fascinating field of study, with\nvarious potential applications. The importance of tools for natural language\nprocessing is materialized by the need to have applications that can\neffectively treat the vast mass of information available nowadays on electronic\nforms. Among these tools, mainly driven by the necessity of a fast writing in\nalignment to the actual daily life speed, our interest is on the writing\nauditors. The morphological and syntactic properties of Arabic make it a\ndifficult language to master, and explain the lack in the processing tools for\nthat language. Among these properties, we can mention: the complex structure of\nthe Arabic word, the agglutinative nature, lack of vocalization, the\nsegmentation of the text, the linguistic richness, etc.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 13:28:03 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2013 06:31:32 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Bouslimi", "Riadh", ""], ["Amraoui", "Houda", ""]]}, {"id": "1203.3023", "submitter": "Mehrez Boulares", "authors": "Mehrez Boulares and Mohamed Jemni", "title": "Toward an example-based machine translation from written text to ASL\n  using virtual agent animation", "comments": "10pages, 11 figures; ISSN (Online): 1694-0814", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 1, January 2012", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Modern computational linguistic software cannot produce important aspects of\nsign language translation. Using some researches we deduce that the majority of\nautomatic sign language translation systems ignore many aspects when they\ngenerate animation; therefore the interpretation lost the truth information\nmeaning. Our goals are: to translate written text from any language to ASL\nanimation; to model maximum raw information using machine learning and\ncomputational techniques; and to produce a more adapted and expressive form to\nnatural looking and understandable ASL animations. Our methods include\nlinguistic annotation of initial text and semantic orientation to generate the\nfacial expression. We use the genetic algorithms coupled to learning/recognized\nsystems to produce the most natural form. To detect emotion we are based on\nfuzzy logic to produce the degree of interpolation between facial expressions.\nRoughly, we present a new expressive language Text Adapted Sign Modeling\nLanguage TASML that describes all maximum aspects related to a natural sign\nlanguage interpretation. This paper is organized as follow: the next section is\ndevoted to present the comprehension effect of using Space/Time/SVO form in ASL\nanimation based on experimentation. In section 3, we describe our technical\nconsiderations. We present the general approach we adopted to develop our tool\nin section 4. Finally, we give some perspectives and future works.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 08:51:14 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Boulares", "Mehrez", ""], ["Jemni", "Mohamed", ""]]}, {"id": "1203.3227", "submitter": "Anton Loss V", "authors": "Anton Loss", "title": "Generalisation of language and knowledge models for corpus analysis", "comments": "13 pages, 2 figures, slightly unconventional", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes new look on language and knowledge modelling for corpus\nlinguistics. Using ideas of Chaitin, a line of argument is made against\nlanguage/knowledge separation in Natural Language Processing. A simplistic\nmodel, that generalises approaches to language and knowledge, is proposed. One\nof hypothetical consequences of this model is Strong AI.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 22:06:42 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Loss", "Anton", ""]]}, {"id": "1203.3511", "submitter": "Sebastian Riedel", "authors": "Sebastian Riedel, David A. Smith, Andrew McCallum", "title": "Inference by Minimizing Size, Divergence, or their Sum", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-492-499", "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We speed up marginal inference by ignoring factors that do not significantly\ncontribute to overall accuracy. In order to pick a suitable subset of factors\nto ignore, we propose three schemes: minimizing the number of model factors\nunder a bound on the KL divergence between pruned and full models; minimizing\nthe KL divergence under a bound on factor count; and minimizing the weighted\nsum of KL divergence and factor count. All three problems are solved using an\napproximation of the KL divergence than can be calculated in terms of marginals\ncomputed on a simple seed graph. Applied to synthetic image denoising and to\nthree different types of NLP parsing models, this technique performs marginal\ninference up to 11 times faster than loopy BP, with graph sizes reduced up to\n98%-at comparable error in marginals and parsing accuracy. We also show that\nminimizing the weighted sum of divergence and size is substantially faster than\nminimizing either of the other objectives based on the approximation to\ndivergence presented here.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Riedel", "Sebastian", ""], ["Smith", "David A.", ""], ["McCallum", "Andrew", ""]]}, {"id": "1203.3584", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "Tarek El-Shishtawy and Fatma El-Ghannam", "title": "An Accurate Arabic Root-Based Lemmatizer for Information Retrieval\n  Purposes", "comments": "9 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 3, January 2012 ISSN (Online): 1694-0814", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma\nlevel analysis and generation does not yet focused in Arabic NLP literatures.\nIn the current research, we propose the first non-statistical accurate Arabic\nlemmatizer algorithm that is suitable for information retrieval (IR) systems.\nThe proposed lemmatizer makes use of different Arabic language knowledge\nresources to generate accurate lemma form and its relevant features that\nsupport IR purposes. As a POS tagger, the experimental results show that, the\nproposed algorithm achieves a maximum accuracy of 94.8%. For first seen\ndocuments, an accuracy of 89.15% is achieved, compared to 76.7% of up to date\nStanford accurate Arabic model, for the same, dataset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 22:49:20 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["El-Shishtawy", "Tarek", ""], ["El-Ghannam", "Fatma", ""]]}, {"id": "1203.3586", "submitter": "Mohsen Pourvali", "authors": "Mohsen Pourvali and Mohammad Saniee Abadeh", "title": "Automated Text Summarization Base on Lexicales Chain and graph Using of\n  WordNet and Wikipedia Knowledge Base", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 3, January 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The technology of automatic document summarization is maturing and may\nprovide a solution to the information overload problem. Nowadays, document\nsummarization plays an important role in information retrieval. With a large\nvolume of documents, presenting the user with a summary of each document\ngreatly facilitates the task of finding the desired documents. Document\nsummarization is a process of automatically creating a compressed version of a\ngiven document that provides useful information to users, and multi-document\nsummarization is to produce a summary delivering the majority of information\ncontent from a set of documents about an explicit or implicit main topic. The\nlexical cohesion structure of the text can be exploited to determine the\nimportance of a sentence/phrase. Lexical chains are useful tools to analyze the\nlexical cohesion structure in a text .In this paper we consider the effect of\nthe use of lexical cohesion features in Summarization, And presenting a\nalgorithm base on the knowledge base. Ours algorithm at first find the correct\nsense of any word, Then constructs the lexical chains, remove Lexical chains\nthat less score than other, detects topics roughly from lexical chains,\nsegments the text with respect to the topics and selects the most important\nsentences. The experimental results on an open benchmark datasets from DUC01\nand DUC02 show that our proposed approach can improve the performance compared\nto sate-of-the-art summarization approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 22:56:29 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Pourvali", "Mohsen", ""], ["Abadeh", "Mohammad Saniee", ""]]}, {"id": "1203.4176", "submitter": "Samaa Shohieb", "authors": "A.M. Riad, Hamdy K.Elmonier, Samaa. M. Shohieb, and A.S. Asem", "title": "SignsWorld; Deeping Into the Silence World and Hearing Its Signs (State\n  of the Art)", "comments": "20 pages, A state of art paper so it contains many references", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 4, No 1, Feb 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech processing systems are employed more and more often in real\nenvironments. Although the underlying speech technology is mostly language\nindependent, differences between languages with respect to their structure and\ngrammar have substantial effect on the recognition systems performance. In this\npaper, we present a review of the latest developments in the sign language\nrecognition research in general and in the Arabic sign language (ArSL) in\nspecific. This paper also presents a general framework for improving the deaf\ncommunity communication with the hearing people that is called SignsWorld. The\noverall goal of the SignsWorld project is to develop a vision-based technology\nfor recognizing and translating continuous Arabic sign language ArSL.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2012 10:16:10 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Riad", "A. M.", ""], ["Elmonier", "Hamdy K.", ""], ["Shohieb", "Samaa. M.", ""], ["Asem", "A. S.", ""]]}, {"id": "1203.4238", "submitter": "Alberto Pepe", "authors": "Marco Guerini, Alberto Pepe, Bruno Lepri", "title": "Do Linguistic Style and Readability of Scientific Abstracts affect their\n  Virality?", "comments": "Proceedings of the Sixth International AAAI Conference on Weblogs and\n  Social Media (ICWSM 2012), 4-8 June 2012, Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactions to textual content posted in an online social network show\ndifferent dynamics depending on the linguistic style and readability of the\nsubmitted content. Do similar dynamics exist for responses to scientific\narticles? Our intuition, supported by previous research, suggests that the\nsuccess of a scientific article depends on its content, rather than on its\nlinguistic style. In this article, we examine a corpus of scientific abstracts\nand three forms of associated reactions: article downloads, citations, and\nbookmarks. Through a class-based psycholinguistic analysis and readability\nindices tests, we show that certain stylistic and readability features of\nabstracts clearly concur in determining the success and viral capability of a\nscientific article.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 20:01:00 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Guerini", "Marco", ""], ["Pepe", "Alberto", ""], ["Lepri", "Bruno", ""]]}, {"id": "1203.4605", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "Tarek El-shishtawy and Abdulwahab Al-sammak", "title": "Arabic Keyphrase Extraction using Linguistic knowledge and Machine\n  Learning Techniques", "comments": "Proceedings of the Second International Conference on Arabic Language\n  Resources and Tools, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a supervised learning technique for extracting keyphrases of\nArabic documents is presented. The extractor is supplied with linguistic\nknowledge to enhance its efficiency instead of relying only on statistical\ninformation such as term frequency and distance. During analysis, an annotated\nArabic corpus is used to extract the required lexical features of the document\nwords. The knowledge also includes syntactic rules based on part of speech tags\nand allowed word sequences to extract the candidate keyphrases. In this work,\nthe abstract form of Arabic words is used instead of its stem form to represent\nthe candidate terms. The Abstract form hides most of the inflections found in\nArabic words. The paper introduces new features of keyphrases based on\nlinguistic knowledge, to capture titles and subtitles of a document. A simple\nANOVA test is used to evaluate the validity of selected features. Then, the\nlearning model is built using the LDA - Linear Discriminant Analysis - and\ntraining documents. Although, the presented system is trained using documents\nin the IT domain, experiments carried out show that it has a significantly\nbetter performance than the existing Arabic extractor systems, where precision\nand recall values reach double their corresponding values in the other systems\nespecially for lengthy and non-scientific articles.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 21:52:35 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["El-shishtawy", "Tarek", ""], ["Al-sammak", "Abdulwahab", ""]]}, {"id": "1203.4933", "submitter": "Kishorjit Nongmeikapam Mr.", "authors": "Kishorjit Nongmeikapam, Lairenlakpam Nonglenjaoba, Yumnam Nirmal and\n  Sivaji Bandyopadhyay", "title": "Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS\n  Tagger", "comments": "15 pages, 4 tables, 2 figures, the link\n  http://airccse.org/journal/jcsit/1011csit05.pdf. arXiv admin note: text\n  overlap with arXiv:1111.2399", "journal-ref": null, "doi": "10.5121/ijitcs.2012.210", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a detail overview about the modified features selection in\nCRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging.\nSelection of features is so important in CRF that the better are the features\nthen the better are the outputs. This work is an attempt or an experiment to\nmake the previous work more efficient. Multiple new features are tried to run\nthe CRF and again tried with the Reduplicated Multiword Expression (RMWE) as\nanother feature. The CRF run with RMWE because Manipuri is rich of RMWE and\nidentification of RMWE becomes one of the necessities to bring up the result of\nPOS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15%\nand F-measure of 75.60%. With the identification of RMWE and considering it as\na feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and\nF-measure of 77.14%.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 09:50:51 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Nongmeikapam", "Kishorjit", ""], ["Nonglenjaoba", "Lairenlakpam", ""], ["Nirmal", "Yumnam", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "1203.5051", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and Robert Gaizauskas", "title": "Analysing Temporally Annotated Corpora with CAVaT", "comments": null, "journal-ref": "Proc. LREC (2010) 398-404", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present CAVaT, a tool that performs Corpus Analysis and Validation for\nTimeML. CAVaT is an open source, modular checking utility for statistical\nanalysis of features specific to temporally-annotated natural language corpora.\nIt provides reporting, highlights salient links between a variety of general\nand time-specific linguistic features, and also validates a temporal annotation\nto ensure that it is logically consistent and sufficiently annotated. Uniquely,\nCAVaT provides analysis specific to TimeML-annotated temporal information.\nTimeML is a standard for annotating temporal information in natural language\ntext. In this paper, we present the reporting part of CAVaT, and then its\nerror-checking ability, including the workings of several novel TimeML document\nverification methods. This is followed by the execution of some example tasks\nusing the tool to show relations between times, events, signals and links. We\nalso demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been\ndetected with CAVaT.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 17:45:39 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Derczynski", "Leon", ""], ["Gaizauskas", "Robert", ""]]}, {"id": "1203.5055", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and Robert Gaizauskas", "title": "Using Signals to Improve Automatic Classification of Temporal Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Temporal information conveyed by language describes how the world around us\nchanges through time. Events, durations and times are all temporal elements\nthat can be viewed as intervals. These intervals are sometimes temporally\nrelated in text. Automatically determining the nature of such relations is a\ncomplex and unsolved problem. Some words can act as \"signals\" which suggest a\ntemporal ordering between intervals. In this paper, we use these signal words\nto improve the accuracy of a recent approach to classification of temporal\nlinks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 17:50:08 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Derczynski", "Leon", ""], ["Gaizauskas", "Robert", ""]]}, {"id": "1203.5060", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and Robert Gaizauskas", "title": "USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2", "comments": "Part of TempEval-2", "journal-ref": "Proc. 5th International Workshop on Semantic Evaluation (2010)\n  337-340", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe the University of Sheffield system used in the TempEval-2\nchallenge, USFD2. The challenge requires the automatic identification of\ntemporal entities and relations in text. USFD2 identifies and anchors temporal\nexpressions, and also attempts two of the four temporal relation assignment\ntasks. A rule-based system picks out and anchors temporal expressions, and a\nmaximum entropy classifier assigns temporal link labels, based on features that\ninclude descriptions of associated temporal signal words. USFD2 identified\ntemporal expressions successfully, and correctly classified their type in 90%\nof cases. Determining the relation between an event and time expression in the\nsame sentence was performed at 63% accuracy, the second highest score in this\npart of the challenge.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 17:59:22 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Derczynski", "Leon", ""], ["Gaizauskas", "Robert", ""]]}, {"id": "1203.5062", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and Robert Gaizauskas", "title": "An Annotation Scheme for Reichenbach's Verbal Tense Structure", "comments": null, "journal-ref": "Proc. 6th Joint ACL-ISO Workshop on Interoperable Semantic\n  Annotation (2011) 10-17", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we present RTMML, a markup language for the tenses of verbs and\ntemporal relations between verbs. There is a richness to tense in language that\nis not fully captured by existing temporal annotation schemata. Following\nReichenbach we present an analysis of tense in terms of abstract time points,\nwith the aim of supporting automated processing of tense and temporal relations\nin language. This allows for precise reasoning about tense in documents, and\nthe deduction of temporal relations between the times and verbal events in a\ndiscourse. We define the syntax of RTMML, and demonstrate the markup in a range\nof situations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 18:05:26 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Derczynski", "Leon", ""], ["Gaizauskas", "Robert", ""]]}, {"id": "1203.5066", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and Robert Gaizauskas", "title": "A Corpus-based Study of Temporal Signals", "comments": "Proc. Corpus Linguistics (2011)", "journal-ref": "Proceedings of the 6th Conference on Corpus Linguistics (2011),\n  No. 197, pp. 1--8", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Automatic temporal ordering of events described in discourse has been of\ngreat interest in recent years. Event orderings are conveyed in text via va\nrious linguistic mechanisms including the use of expressions such as \"before\",\n\"after\" or \"during\" that explicitly assert a temporal relation -- temporal\nsignals. In this paper, we investigate the role of temporal signals in temporal\nrelation extraction and provide a quantitative analysis of these expres sions\nin the TimeBank annotated corpus.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 18:08:47 GMT"}], "update_date": "2013-01-25", "authors_parsed": [["Derczynski", "Leon", ""], ["Gaizauskas", "Robert", ""]]}, {"id": "1203.5073", "submitter": "Leon Derczynski", "authors": "Amev Burman, Arun Jayapal, Sathish Kannan, Madhu Kavilikatta, Ayman\n  Alhelbawy, Leon Derczynski, Robert Gaizauskas", "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding", "comments": "Proc. Text Analysis Conference (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes the University of Sheffield's entry in the 2011 TAC KBP\nentity linking and slot filling tasks. We chose to participate in the\nmonolingual entity linking task, the monolingual slot filling task and the\ntemporal slot filling tasks. We set out to build a framework for\nexperimentation with knowledge base population. This framework was created, and\napplied to multiple KBP tasks. We demonstrated that our proposed framework is\neffective and suitable for collaborative development efforts, as well as useful\nin a teaching environment. Finally we present results that, while very modest,\nprovide improvements an order of magnitude greater than our 2010 attempt.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 18:34:19 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Burman", "Amev", ""], ["Jayapal", "Arun", ""], ["Kannan", "Sathish", ""], ["Kavilikatta", "Madhu", ""], ["Alhelbawy", "Ayman", ""], ["Derczynski", "Leon", ""], ["Gaizauskas", "Robert", ""]]}, {"id": "1203.5076", "submitter": "Leon Derczynski", "authors": "Leon Derczynski and H\\'ector Llorens and Estela Saquete", "title": "Massively Increasing TIMEX3 Resources: A Transduction Approach", "comments": "Proc. LREC (2012)", "journal-ref": "Proceedings of the 8th international conference on Language\n  Resources and Evaluation (2012), pp. 3754-3761", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. Gold standard\ntemporally-annotated resources are limited in size, which makes research using\nthem difficult. Standards have also evolved over the past decade, so not all\ntemporally annotated data is in the same format. We vastly increase available\nhuman-annotated temporal expression resources by converting older format\nresources to TimeML/TIMEX3. This task is difficult due to differing annotation\nmethods. We present a robust conversion tool and a new, large temporal\nexpression resource. Using this, we evaluate our conversion process by using it\nas training data for an existing TimeML annotation tool, achieving a 0.87 F1\nmeasure -- better than any system in the TempEval-2 timex recognition exercise.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 18:45:07 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Derczynski", "Leon", ""], ["Llorens", "H\u00e9ctor", ""], ["Saquete", "Estela", ""]]}, {"id": "1203.5084", "submitter": "Leon Derczynski", "authors": "Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood", "title": "A Data Driven Approach to Query Expansion in Question Answering", "comments": null, "journal-ref": "Proc. IR4QA Workshop (2008) 34-41", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Automated answering of natural language questions is an interesting and\nuseful problem to solve. Question answering (QA) systems often perform\ninformation retrieval at an initial stage. Information retrieval (IR)\nperformance, provided by engines such as Lucene, places a bound on overall\nsystem performance. For example, no answer bearing documents are retrieved at\nlow ranks for almost 40% of questions.\n  In this paper, answer texts from previous QA evaluations held as part of the\nText REtrieval Conferences (TREC) are paired with queries and analysed in an\nattempt to identify performance-enhancing words. These words are then used to\nevaluate the performance of a query expansion method.\n  Data driven extension words were found to help in over 70% of difficult\nquestions. These words can be used to improve and evaluate query expansion\nmethods. Simple blind relevance feedback (RF) was correctly predicted as\nunlikely to help overall performance, and an possible explanation is provided\nfor its low value in IR for QA.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 19:19:02 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Derczynski", "Leon", ""], ["Wang", "Jun", ""], ["Gaizauskas", "Robert", ""], ["Greenwood", "Mark A.", ""]]}, {"id": "1203.5188", "submitter": "Martin Monperrus", "authors": "Stefan Hen{\\ss}, Martin Monperrus (INRIA Lille - Nord Europe), Mira\n  Mezini", "title": "Semi-Automatically Extracting FAQs to Improve Accessibility of Software\n  Development Knowledge", "comments": "ICSE - 34th International Conference on Software Engineering (2012)", "journal-ref": "ICSE - 34th International Conference on Software Engineering, 2012", "doi": "10.1109/ICSE.2012.6227139", "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently asked questions (FAQs) are a popular way to document software\ndevelopment knowledge. As creating such documents is expensive, this paper\npresents an approach for automatically extracting FAQs from sources of software\ndevelopment discussion, such as mailing lists and Internet forums, by combining\ntechniques of text mining and natural language processing. We apply the\napproach to popular mailing lists and carry out a survey among software\ndevelopers to show that it is able to extract high-quality FAQs that may be\nfurther improved by experts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 07:13:06 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Hen\u00df", "Stefan", "", "INRIA Lille - Nord Europe"], ["Monperrus", "Martin", "", "INRIA Lille - Nord Europe"], ["Mezini", "Mira", ""]]}, {"id": "1203.5255", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Mohammad Alwani", "title": "Post-Editing Error Correction Algorithm for Speech Recognition using\n  Bing Spelling Suggestion", "comments": "LACSC - Lebanese Association for Computational Sciences -\n  http://www.lacsc.org", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Vol.3, No.2, February 2012", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ASR short for Automatic Speech Recognition is the process of converting a\nspoken speech into text that can be manipulated by a computer. Although ASR has\nseveral applications, it is still erroneous and imprecise especially if used in\na harsh surrounding wherein the input speech is of low quality. This paper\nproposes a post-editing ASR error correction method and algorithm based on\nBing's online spelling suggestion. In this approach, the ASR recognized output\ntext is spell-checked using Bing's spelling suggestion technology to detect and\ncorrect misrecognized words. More specifically, the proposed algorithm breaks\ndown the ASR output text into several word-tokens that are submitted as search\nqueries to Bing search engine. A returned spelling suggestion implies that a\nquery is misspelled; and thus it is replaced by the suggested correction;\notherwise, no correction is performed and the algorithm continues with the next\ntoken until all tokens get validated. Experiments carried out on various\nspeeches in different languages indicated a successful decrease in the number\nof ASR errors and an improvement in the overall error correction rate. Future\nresearch can improve upon the proposed algorithm so much so that it can be\nparallelized to take advantage of multiprocessor computers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 14:32:50 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Bassil", "Youssef", ""], ["Alwani", "Mohammad", ""]]}, {"id": "1203.5262", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Paul Semaan", "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset", "comments": "LACSC - Lebanese Association for Computational Sciences -\n  http://www.lacsc.org", "journal-ref": "Journal of Computing, Vol.4, No.1, January 2012", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the present time, computers are employed to solve complex tasks and\nproblems ranging from simple calculations to intensive digital image processing\nand intricate algorithmic optimization problems to computationally-demanding\nweather forecasting problems. ASR short for Automatic Speech Recognition is yet\nanother type of computational problem whose purpose is to recognize human\nspoken speech and convert it into text that can be processed by a computer.\nDespite that ASR has many versatile and pervasive real-world applications,it is\nstill relatively erroneous and not perfectly solved as it is prone to produce\nspelling errors in the recognized text, especially if the ASR system is\noperating in a noisy environment, its vocabulary size is limited, and its input\nspeech is of bad or low quality. This paper proposes a post-editing ASR error\ncorrection method based on MicrosoftN-Gram dataset for detecting and correcting\nspelling errors generated by ASR systems. The proposed method comprises an\nerror detection algorithm for detecting word errors; a candidate corrections\ngeneration algorithm for generating correction suggestions for the detected\nword errors; and a context-sensitive error correction algorithm for selecting\nthe best candidate for correction. The virtue of using the Microsoft N-Gram\ndataset is that it contains real-world data and word sequences extracted from\nthe web which canmimica comprehensive dictionary of words having a large and\nall-inclusive vocabulary. Experiments conducted on numerous speeches, performed\nby different speakers, showed a remarkable reduction in ASR errors. Future\nresearch can improve upon the proposed algorithm so much so that it can be\nparallelized to take advantage of multiprocessor and distributed systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 14:51:05 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Bassil", "Youssef", ""], ["Semaan", "Paul", ""]]}, {"id": "1203.5502", "submitter": "Marco Guerini", "authors": "Marco Guerini, Carlo Strapparava and Gozde Ozbal", "title": "Exploring Text Virality in Social Networks", "comments": null, "journal-ref": "Proceedings of the Fifth International AAAI Conference on Weblogs\n  and Social Media (ICWSM 2011), 17-21 July 2011, Barcelona, Spain", "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to shed some light on the concept of virality - especially in\nsocial networks - and to provide new insights on its structure. We argue that:\n(a) virality is a phenomenon strictly connected to the nature of the content\nbeing spread, rather than to the influencers who spread it, (b) virality is a\nphenomenon with many facets, i.e. under this generic term several different\neffects of persuasive communication are comprised and they only partially\noverlap. To give ground to our claims, we provide initial experiments in a\nmachine learning framework to show how various aspects of virality can be\nindependently predicted according to content features.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 14:56:04 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Guerini", "Marco", ""], ["Strapparava", "Carlo", ""], ["Ozbal", "Gozde", ""]]}, {"id": "1203.6136", "submitter": "Alex Rudnick", "authors": "Alex Rudnick", "title": "Tree Transducers, Machine Translation, and Cross-Language Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree transducers are formal automata that transform trees into other trees.\nMany varieties of tree transducers have been explored in the automata theory\nliterature, and more recently, in the machine translation literature. In this\npaper I review T and xT transducers, situate them among related formalisms, and\nshow how they can be used to implement rules for machine translation systems\nthat cover all of the cross-language structural divergences described in Bonnie\nDorr's influential article on the topic. I also present an implementation of xT\ntransduction, suitable and convenient for experimenting with translation rules.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 02:13:39 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Rudnick", "Alex", ""]]}, {"id": "1203.6339", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Intelligent Interface Architectures for Folksonomy Driven Structure\n  Network", "comments": "*** This paper has been accepted to the 5th International Workshop on\n  Intelligent Interfaces for Human-Computer Interaction (IIHCI 2012) - Palermo\n  Italy, 4-6 July 2012 *** 7 pages, 7 figures; for details see:\n  http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The folksonomy is the result of free personal information or assignment of\ntags to an object (determined by the URI) in order to find them. The practice\nof tagging is done in a collective environment. Folksonomies are self\nconstructed, based on co-occurrence of definitions, rather than a hierarchical\nstructure of the data. The downside of this was that a few sites and\napplications are able to successfully exploit the sharing of bookmarks. The\nneed for tools that are able to resolve the ambiguity of the definitions is\nbecoming urgent as the need of simple instruments for their visualization,\nediting and exploitation in web applications still hinders their diffusion and\nwide adoption. An intelligent interactive interface design for folksonomies\nshould consider the contextual design and inquiry based on a concurrent\ninteraction for a perceptual user interfaces. To represent folksonomies a new\nconcept structure called \"Folksodriven\" is used in this paper. While it is\npresented the Folksodriven Structure Network (FSN) to resolve the ambiguity of\ndefinitions of folksonomy tags suggestions for the user. On this base a\nHuman-Computer Interactive (HCI) systems is developed for the visualization,\nnavigation, updating and maintenance of folksonomies Knowledge Bases - the FSN\n- through the web. System functionalities as well as its internal architecture\nwill be introduced.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 18:59:40 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "1203.6360", "submitter": "Lillian Lee", "authors": "Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon Kleinberg, Lillian\n  Lee", "title": "You had me at hello: How phrasing affects memorability", "comments": "Final version of paper to appear at ACL 2012. 10pp, 1 fig. Data, demo\n  memorability test and other info available at\n  http://www.cs.cornell.edu/~cristian/memorability.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the ways in which information achieves widespread public\nawareness is a research question of significant interest. We consider whether,\nand how, the way in which the information is phrased --- the choice of words\nand sentence structure --- can affect this process. To this end, we develop an\nanalysis framework and build a corpus of movie quotes, annotated with\nmemorability information, in which we are able to control for both the speaker\nand the setting of the quotes. We find that there are significant differences\nbetween memorable and non-memorable quotes in several key dimensions, even\nafter controlling for situational and contextual factors. One is lexical\ndistinctiveness: in aggregate, memorable quotes use less common word choices,\nbut at the same time are built upon a scaffolding of common syntactic patterns.\nAnother is that memorable quotes tend to be more general in ways that make them\neasy to apply in new contexts --- that is, more portable. We also show how the\nconcept of \"memorable language\" can be extended across domains.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 20:00:20 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2012 20:14:40 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Danescu-Niculescu-Mizil", "Cristian", ""], ["Cheng", "Justin", ""], ["Kleinberg", "Jon", ""], ["Lee", "Lillian", ""]]}, {"id": "1203.6845", "submitter": "Juli\\'an Urbano", "authors": "M\\'onica Marrero, Sonia S\\'anchez-Cuadrado, Juli\\'an Urbano, Jorge\n  Morato, Jos\\'e-Antonio Moreiro", "title": "Information Retrieval Systems Adapted to the Biomedical Domain", "comments": "6 pages, 4 tables", "journal-ref": "El Profesional de la Informaci\\'on (2010), 19-3", "doi": "10.3145/epi.2010.may.04", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The terminology used in Biomedicine shows lexical peculiarities that have\nrequired the elaboration of terminological resources and information retrieval\nsystems with specific functionalities. The main characteristics are the high\nrates of synonymy and homonymy, due to phenomena such as the proliferation of\npolysemic acronyms and their interaction with common language. Information\nretrieval systems in the biomedical domain use techniques oriented to the\ntreatment of these lexical peculiarities. In this paper we review some of the\ntechniques used in this domain, such as the application of Natural Language\nProcessing (BioNLP), the incorporation of lexical-semantic resources, and the\napplication of Named Entity Recognition (BioNER). Finally, we present the\nevaluation methods adopted to assess the suitability of these techniques for\nretrieving biomedical resources.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 16:16:43 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Marrero", "M\u00f3nica", ""], ["S\u00e1nchez-Cuadrado", "Sonia", ""], ["Urbano", "Juli\u00e1n", ""], ["Morato", "Jorge", ""], ["Moreiro", "Jos\u00e9-Antonio", ""]]}]