[{"id": "1405.0049", "submitter": "Paul Tupper", "authors": "P. F. Tupper", "title": "Exemplar Dynamics Models of the Stability of Phonological Categories", "comments": "6 pages, COGS2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model for the stability and maintenance of phonological\ncategories. Examples of phonological categories are vowel sounds such as \"i\"\nand \"e\". We model such categories as consisting of collections of labeled\nexemplars that language users store in their memory. Each exemplar is a\ndetailed memory of an instance of the linguistic entity in question. Starting\nfrom an exemplar-level model we derive integro-differential equations for the\nlong-term evolution of the density of exemplars in different portions of\nphonetic space. Using these latter equations we investigate under what\nconditions two phonological categories merge or not. Our main conclusion is\nthat for the preservation of distinct phonological categories, it is necessary\nthat anomalous speech tokens of a given category are discarded, and not merely\nstored in memory as an exemplar of another category.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 22:44:59 GMT"}, {"version": "v2", "created": "Tue, 6 May 2014 18:41:26 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Tupper", "P. F.", ""]]}, {"id": "1405.0145", "submitter": "Kais Dukes", "authors": "Kais Dukes", "title": "Contextual Semantic Parsing using Crowdsourced Spatial Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a contextual parser for the Robot Commands Treebank, a new\ncrowdsourced resource. In contrast to previous semantic parsers that select the\nmost-probable parse, we consider the different problem of parsing using\nadditional situational context to disambiguate between different readings of a\nsentence. We show that multiple semantic analyses can be searched using dynamic\nprogramming via interaction with a spatial planner, to guide the parsing\nprocess. We are able to parse sentences in near linear-time by ruling out\nanalyses early on that are incompatible with spatial context. We report a 34%\nupper bound on accuracy, as our planner correctly processes spatial context for\n3,394 out of 10,000 sentences. However, our parser achieves a 96.53%\nexact-match score for parsing within the subset of sentences recognized by the\nplanner, compared to 82.14% for a non-contextual parser.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 12:29:30 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Dukes", "Kais", ""]]}, {"id": "1405.0546", "submitter": "Antti Puurula", "authors": "Antti Puurula, Jesse Read, Albert Bifet", "title": "Kaggle LSHTC4 Winning Solution", "comments": "Kaggle LSHTC winning solution description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our winning submission to the 2014 Kaggle competition for Large Scale\nHierarchical Text Classification (LSHTC) consists mostly of an ensemble of\nsparse generative models extending Multinomial Naive Bayes. The\nbase-classifiers consist of hierarchically smoothed models combining document,\nlabel, and hierarchy level Multinomials, with feature pre-processing using\nvariants of TF-IDF and BM25. Additional diversification is introduced by\ndifferent types of folds and random search optimization for different measures.\nThe ensemble algorithm optimizes macroFscore by predicting the documents for\neach label, instead of the usual prediction of labels per document. Scores for\ndocuments are predicted by weighted voting of base-classifier outputs with a\nvariant of Feature-Weighted Linear Stacking. The number of documents per label\nis chosen using label priors and thresholding of vote scores. This document\ndescribes the models and software used to build our solution. Reproducing the\nresults for our solution can be done by running the scripts included in the\nKaggle package. A package omitting precomputed result files is also\ndistributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0\nfor Weka and Meka dependencies.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 01:41:27 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 04:57:19 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Puurula", "Antti", ""], ["Read", "Jesse", ""], ["Bifet", "Albert", ""]]}, {"id": "1405.0603", "submitter": "Aibek Makazhanov", "authors": "Aibek Makazhanov and Denilson Barbosa and Grzegorz Kondrak", "title": "Extracting Family Relationship Networks from Novels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to the extraction of family relations from literary\nnarrative, which incorporates a technique for utterance attribution proposed\nrecently by Elson and McKeown (2010). In our work this technique is used in\ncombination with the detection of vocatives - the explicit forms of address\nused by the characters in a novel. We take advantage of the fact that certain\nvocatives indicate family relations between speakers. The extracted relations\nare then propagated using a set of rules. We report the results of the\napplication of our method to Jane Austen's Pride and Prejudice.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 16:07:19 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Makazhanov", "Aibek", ""], ["Barbosa", "Denilson", ""], ["Kondrak", "Grzegorz", ""]]}, {"id": "1405.0616", "submitter": "James Brofos", "authors": "James Brofos, Ajay Kannan, Rui Shu", "title": "Automated Attribution and Intertextual Analysis", "comments": "10 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we employ quantitative methods from the realm of statistics and\nmachine learning to develop novel methodologies for author attribution and\ntextual analysis. In particular, we develop techniques and software suitable\nfor applications to Classical study, and we illustrate the efficacy of our\napproach in several interesting open questions in the field. We apply our\nnumerical analysis techniques to questions of authorship attribution in the\ncase of the Greek tragedian Euripides, to instances of intertextuality and\ninfluence in the poetry of the Roman statesman Seneca the Younger, and to cases\nof \"interpolated\" text with respect to the histories of Livy.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 19:02:44 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Brofos", "James", ""], ["Kannan", "Ajay", ""], ["Shu", "Rui", ""]]}, {"id": "1405.0701", "submitter": "Manaal Faruqui", "authors": "Manaal Faruqui", "title": "\"Translation can't change a name\": Using Multilingual Data for Named\n  Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entities (NEs) are often written with no orthographic changes across\ndifferent languages that share a common alphabet. We show that this can be\nleveraged so as to improve named entity recognition (NER) by using unsupervised\nword clusters from secondary languages as features in state-of-the-art\ndiscriminative NER systems. We observe significant increases in performance,\nfinding that person and location identification is particularly improved, and\nthat phylogenetically close languages provide more valuable features than more\ndistant languages.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 14:23:53 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Faruqui", "Manaal", ""]]}, {"id": "1405.0941", "submitter": "Serena Villata", "authors": "Elena Cabrio and Serena Villata", "title": "Towards a Benchmark of Natural Language Arguments", "comments": null, "journal-ref": "Proceedings of the 15th International Workshop on Non-Monotonic\n  Reasoning (NMR 2014)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connections among natural language processing and argumentation theory\nare becoming stronger in the latest years, with a growing amount of works going\nin this direction, in different scenarios and applying heterogeneous\ntechniques. In this paper, we present two datasets we built to cope with the\ncombination of the Textual Entailment framework and bipolar abstract\nargumentation. In our approach, such datasets are used to automatically\nidentify through a Textual Entailment system the relations among the arguments\n(i.e., attack, support), and then the resulting bipolar argumentation graphs\nare analyzed to compute the accepted arguments.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 16:03:04 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Cabrio", "Elena", ""], ["Villata", "Serena", ""]]}, {"id": "1405.0947", "submitter": "Tom\\'a\\v{s} Ko\\v{c}isk\\'y", "authors": "Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Karl Moritz Hermann, Phil Blunsom", "title": "Learning Bilingual Word Representations by Marginalizing Alignments", "comments": "Proceedings of ACL 2014 (Short Papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model that simultaneously learns alignments and\ndistributed representations for bilingual data. By marginalizing over word\nalignments the model captures a larger semantic context than prior work relying\non hard alignments. The advantage of this approach is demonstrated in a\ncross-lingual classification task, where we outperform the prior published\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 16:24:09 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Hermann", "Karl Moritz", ""], ["Blunsom", "Phil", ""]]}, {"id": "1405.1346", "submitter": "Olena Orobinska", "authors": "Olena Orobinska (ERIC)", "title": "Automatic Method Of Domain Ontology Construction based on\n  Characteristics of Corpora POS-Analysis", "comments": "XV International Conference, France (2012), in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely recognized that ontologies, are one of the fundamental\ncornerstones of knowledge-based systems. What is lacking, however, is a\ncurrently accepted strategy of how to build ontology; what kinds of the\nresources and techniques are indispensables to optimize the expenses and the\ntime on the one hand and the amplitude, the completeness, the robustness of en\nontology on the other hand. The paper offers a semi-automatic ontology\nconstruction method from text corpora in the domain of radiological protection.\nThis method is composed from next steps: 1) text annotation with part-of-speech\ntags; 2) revelation of the significant linguistic structures and forming the\ntemplates; 3) search of text fragments corresponding to these templates; 4)\nbasic ontology instantiation process\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:28:16 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Orobinska", "Olena", "", "ERIC"]]}, {"id": "1405.1359", "submitter": "Michael Kai Petersen", "authors": "Michael Kai Petersen", "title": "Latent semantics of action verbs reflect phonetic parameters of\n  intensity and emotional content", "comments": "15 pages", "journal-ref": null, "doi": "10.1371/journal.pone.0121575", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjuring up our thoughts, language reflects statistical patterns of word\nco-occurrences which in turn come to describe how we perceive the world.\nWhether counting how frequently nouns and verbs combine in Google search\nqueries, or extracting eigenvectors from term document matrices made up of\nWikipedia lines and Shakespeare plots, the resulting latent semantics capture\nnot only the associative links which form concepts, but also spatial dimensions\nembedded within the surface structure of language. As both the shape and\nmovements of objects have been found to be associated with phonetic contrasts\nalready in toddlers, this study explores whether articulatory and acoustic\nparameters may likewise differentiate the latent semantics of action verbs.\nSelecting 3 x 20 emotion, face, and hand related verbs known to activate\npremotor areas in the brain, their mutual cosine similarities were computed\nusing latent semantic analysis LSA, and the resulting adjacency matrices were\ncompared based on two different large scale text corpora; HAWIK and TASA.\nApplying hierarchical clustering to identify common structures across the two\ntext corpora, the verbs largely divide into combined mouth and hand movements\nversus emotional expressions. Transforming the verbs into their constituent\nphonemes, the clustered small and large size movements appear differentiated by\nfront versus back vowels corresponding to increasing levels of arousal. Whereas\nthe clustered emotional verbs seem characterized by sequences of close versus\nopen jaw produced phonemes, generating up- or downwards shifts in formant\nfrequencies that may influence their perceived valence. Suggesting, that the\nlatent semantics of action verbs reflect parameters of intensity and emotional\npolarity that appear correlated with the articulatory contrasts and acoustic\ncharacteristics of phonemes\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 16:52:34 GMT"}, {"version": "v2", "created": "Sun, 31 Aug 2014 14:21:10 GMT"}, {"version": "v3", "created": "Sat, 29 Nov 2014 21:14:29 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Petersen", "Michael Kai", ""]]}, {"id": "1405.1379", "submitter": "Daniele Giacobello", "authors": "Ramin Pichevar, Jason Wung, Daniele Giacobello, Joshua Atkins", "title": "Design and Optimization of a Speech Recognition Front-End for\n  Distant-Talking Control of a Music Playback Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenging scenario for the distant-talking control\nof a music playback device, a common portable speaker with four small\nloudspeakers in close proximity to one microphone. The user controls the device\nthrough voice, where the speech-to-music ratio can be as low as -30 dB during\nmusic playback. We propose a speech enhancement front-end that relies on known\nrobust methods for echo cancellation, double-talk detection, and noise\nsuppression, as well as a novel adaptive quasi-binary mask that is well suited\nfor speech recognition. The optimization of the system is then formulated as a\nlarge scale nonlinear programming problem where the recognition rate is\nmaximized and the optimal values for the system parameters are found through a\ngenetic algorithm. We validate our methodology by testing over the TIMIT\ndatabase for different music playback levels and noise types. Finally, we show\nthat the proposed front-end allows a natural interaction with the device for\nlimited-vocabulary voice commands.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 14:37:47 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Pichevar", "Ramin", ""], ["Wung", "Jason", ""], ["Giacobello", "Daniele", ""], ["Atkins", "Joshua", ""]]}, {"id": "1405.1406", "submitter": "Sallam Abualhaija", "authors": "Sallam Abualhaija, Karl-Heinz Zimmermann", "title": "D-Bees: A Novel Method Inspired by Bee Colony Optimization for Solving\n  Word Sense Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word sense disambiguation (WSD) is a problem in the field of computational\nlinguistics given as finding the intended sense of a word (or a set of words)\nwhen it is activated within a certain context. WSD was recently addressed as a\ncombinatorial optimization problem in which the goal is to find a sequence of\nsenses that maximize the semantic relatedness among the target words. In this\narticle, a novel algorithm for solving the WSD problem called D-Bees is\nproposed which is inspired by bee colony optimization (BCO)where artificial bee\nagents collaborate to solve the problem. The D-Bees algorithm is evaluated on a\nstandard dataset (SemEval 2007 coarse-grained English all-words task corpus)and\nis compared to simulated annealing, genetic algorithms, and two ant colony\noptimization techniques (ACO). It will be observed that the BCO and ACO\napproaches are on par.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 19:26:35 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Abualhaija", "Sallam", ""], ["Zimmermann", "Karl-Heinz", ""]]}, {"id": "1405.1438", "submitter": "Chenhao Tan", "authors": "Chenhao Tan, Lillian Lee, Bo Pang", "title": "The effect of wording on message propagation: Topic- and\n  author-controlled natural experiments on Twitter", "comments": "11 pages, to appear in Proceedings of ACL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a person trying to spread an important message on a social network.\nHe/she can spend hours trying to craft the message. Does it actually matter?\nWhile there has been extensive prior work looking into predicting popularity of\nsocial-media content, the effect of wording per se has rarely been studied\nsince it is often confounded with the popularity of the author and the topic.\nTo control for these confounding factors, we take advantage of the surprising\nfact that there are many pairs of tweets containing the same url and written by\nthe same user but employing different wording. Given such pairs, we ask: which\nversion attracts more retweets? This turns out to be a more difficult task than\npredicting popular topics. Still, humans can answer this question better than\nchance (but far from perfectly), and the computational methods we develop can\ndo better than both an average human and a strong competing method trained on\nnon-controlled data.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:04:44 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Tan", "Chenhao", ""], ["Lee", "Lillian", ""], ["Pang", "Bo", ""]]}, {"id": "1405.1439", "submitter": "Chenhao Tan", "authors": "Chenhao Tan, Lillian Lee", "title": "A Corpus of Sentence-level Revisions in Academic Writing: A Step towards\n  Understanding Statement Strength in Communication", "comments": "6 pages, to appear in Proceedings of ACL 2014 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strength with which a statement is made can have a significant impact on\nthe audience. For example, international relations can be strained by how the\nmedia in one country describes an event in another; and papers can be rejected\nbecause they overstate or understate their findings. It is thus important to\nunderstand the effects of statement strength. A first step is to be able to\ndistinguish between strong and weak statements. However, even this problem is\nunderstudied, partly due to a lack of data. Since strength is inherently\nrelative, revisions of texts that make claims are a natural source of data on\nstrength differences. In this paper, we introduce a corpus of sentence-level\nrevisions from academic writing. We also describe insights gained from our\nannotation efforts for this task.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 20:05:23 GMT"}, {"version": "v2", "created": "Sat, 31 May 2014 23:44:18 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Tan", "Chenhao", ""], ["Lee", "Lillian", ""]]}, {"id": "1405.1605", "submitter": "Marco Guerini", "authors": "Jacopo Staiano and Marco Guerini", "title": "DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News", "comments": "To appear at ACL 2014. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many lexica annotated with words polarity are available for sentiment\nanalysis, very few tackle the harder task of emotion analysis and are usually\nquite limited in coverage. In this paper, we present a novel approach for\nextracting - in a totally automated way - a high-coverage and high-precision\nlexicon of roughly 37 thousand terms annotated with emotion scores, called\nDepecheMood. Our approach exploits in an original way 'crowd-sourced' affective\nannotation implicitly provided by readers of news articles from rappler.com. By\nproviding new state-of-the-art performances in unsupervised settings for\nregression and classification tasks, even using a na\\\"{\\i}ve approach, our\nexperiments show the beneficial impact of harvesting social media data for\naffective lexicon building.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 13:40:47 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Staiano", "Jacopo", ""], ["Guerini", "Marco", ""]]}, {"id": "1405.1893", "submitter": "Ana Mestrovic", "authors": "Kristina Ban, Ana Me\\v{s}trovi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Initial Comparison of Linguistic Networks Measures for Parallel Texts", "comments": "In proceeding of: 5th International Conference on Information\n  Technologies and Information Society -ITIS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents preliminary results of Croatian syllable networks\nanalysis. Syllable network is a network in which nodes are syllables and links\nbetween them are constructed according to their connections within words. In\nthis paper we analyze networks of syllables generated from texts collected from\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\nanalysis methods which provide mechanisms that can reveal new patterns in a\nlanguage structure. We aim to show that syllable networks have much higher\nclustering coefficient in comparison to Erd\\\"os-Renyi random networks. The\nresults indicate that Croatian syllable networks exhibit certain properties of\na small world networks. Furthermore, we compared Croatian syllable networks\nwith Portuguese and Chinese syllable networks and we showed that they have\nsimilar properties.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 11:47:03 GMT"}, {"version": "v2", "created": "Thu, 17 Jul 2014 16:15:24 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Ban", "Kristina", ""], ["Me\u0161trovi\u0107", "Ana", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1405.1924", "submitter": "Tebbi Hanane th", "authors": "Tebbi Hanane and Azzoune Hamid", "title": "An Expert System for Automatic Reading of A Text Written in Standard\n  Arabic", "comments": null, "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  3, No.2, April 2014", "doi": "10.5121/ijnlc.2014.3201", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present our expert system of Automatic reading or speech\nsynthesis based on a text written in Standard Arabic, our work is carried out\nin two great stages: the creation of the sound data base, and the\ntransformation of the written text into speech (Text To Speech TTS). This\ntransformation is done firstly by a Phonetic Orthographical Transcription (POT)\nof any written Standard Arabic text with the aim of transforming it into his\ncorresponding phonetics sequence, and secondly by the generation of the voice\nsignal which corresponds to the chain transcribed. We spread out the different\nof conception of the system, as well as the results obtained compared to others\nworks studied to realize TTS based on Standard Arabic.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 13:47:36 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Hanane", "Tebbi", ""], ["Hamid", "Azzoune", ""]]}, {"id": "1405.2048", "submitter": "Jeffrey Sukharev", "authors": "Jeffrey Sukharev, Leonid Zhukov, Alexandrin Popescul", "title": "Learning Alternative Name Spellings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Name matching is a key component of systems for entity resolution or record\nlinkage. Alternative spellings of the same names are a com- mon occurrence in\nmany applications. We use the largest collection of genealogy person records in\nthe world together with user search query logs to build name matching models.\nThe procedure for building a crowd-sourced training set is outlined together\nwith the presentation of our method. We cast the problem of learning\nalternative spellings as a machine translation problem at the character level.\nWe use in- formation retrieval evaluation methodology to show that this method\nsubstantially outperforms on our data a number of standard well known phonetic\nand string similarity methods in terms of precision and re- call. Additionally,\nwe rigorously compare the performance of standard methods when compared with\neach other. Our result can lead to a significant practical impact in entity\nresolution applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 19:47:51 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Sukharev", "Jeffrey", ""], ["Zhukov", "Leonid", ""], ["Popescul", "Alexandrin", ""]]}, {"id": "1405.2386", "submitter": "Srayan Datta", "authors": "Srayan Datta", "title": "Predicting Central Topics in a Blog Corpus from a Networks Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's content-centric Internet, blogs are becoming increasingly popular\nand important from a data analysis perspective. According to Wikipedia, there\nwere over 156 million public blogs on the Internet as of February 2011. Blogs\nare a reflection of our contemporary society. The contents of different blog\nposts are important from social, psychological, economical and political\nperspectives. Discovery of important topics in the blogosphere is an area which\nstill needs much exploring. We try to come up with a procedure using\nprobabilistic topic modeling and network centrality measures which identifies\nthe central topics in a blog corpus.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 03:43:14 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Datta", "Srayan", ""]]}, {"id": "1405.2434", "submitter": "Lijiang Chen", "authors": "Chen Lijiang", "title": "Coordinate System Selection for Minimum Error Rate Training in\n  Statistical Machine Translation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Minimum error rate training (MERT) is a widely used training procedure for\nstatistical machine translation. A general problem of this approach is that the\nsearch space is easy to converge to a local optimum and the acquired weight set\nis not in accord with the real distribution of feature functions. This paper\nintroduces coordinate system selection (RSS) into the search algorithm for\nMERT. Contrary to previous approaches in which every dimension only corresponds\nto one independent feature function, we create several coordinate systems by\nmoving one of the dimensions to a new direction. The basic idea is quite simple\nbut critical that the training procedure of MERT should be based on a\ncoordinate system formed by search directions but not directly on feature\nfunctions. Experiments show that by selecting coordinate systems with tuning\nset results, better results can be obtained without any other language\nknowledge.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 13:55:03 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Lijiang", "Chen", ""]]}, {"id": "1405.2584", "submitter": "Rahul Tejwani", "authors": "Rahul Tejwani (University at Buffalo)", "title": "Sentiment Analysis: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (also known as opinion mining) refers to the use of\nnatural language processing, text analysis and computational linguistics to\nidentify and extract subjective information in source materials. Mining\nopinions expressed in the user generated content is a challenging yet\npractically very useful problem. This survey would cover various approaches and\nmethodology used in Sentiment Analysis and Opinion Mining in general. The focus\nwould be on Internet text like, Product review, tweets and other social media.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 21:05:28 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Tejwani", "Rahul", "", "University at Buffalo"]]}, {"id": "1405.2702", "submitter": "Ana Mestrovic", "authors": "Sabina \\v{S}i\\v{s}ovi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c and Ana\n  Me\\v{s}trovi\\'c", "title": "Comparison of the language networks from literature and blogs", "comments": null, "journal-ref": "37th IEEE International Convention on Information and\n  Communication Technology, Electronics and Microelectronics (MIPRO 2014),\n  pp.1824--1829, (2014)", "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the comparison of the linguistic networks from\nliterature and blog texts. The linguistic networks are constructed from texts\nas directed and weighted co-occurrence networks of words. Words are nodes and\nlinks are established between two nodes if they are directly co-occurring\nwithin the sentence. The comparison of the networks structure is performed at\nglobal level (network) in terms of: average node degree, average shortest path\nlength, diameter, clustering coefficient, density and number of components.\nFurthermore, we perform analysis on the local level (node) by comparing the\nrank plots of in and out degree, strength and selectivity. The\nselectivity-based results point out that there are differences between the\nstructure of the networks constructed from literature and blogs.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 10:46:06 GMT"}, {"version": "v2", "created": "Thu, 17 Jul 2014 16:13:42 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["\u0160i\u0161ovi\u0107", "Sabina", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""], ["Me\u0161trovi\u0107", "Ana", ""]]}, {"id": "1405.2874", "submitter": "EPTCS", "authors": "Dimitri Kartsaklis (University of Oxford), Mehrnoosh Sadrzadeh (Queen\n  Mary University of London)", "title": "A Study of Entanglement in a Categorical Framework of Natural Language", "comments": "In Proceedings QPL 2014, arXiv:1412.8102", "journal-ref": "EPTCS 172, 2014, pp. 249-261", "doi": "10.4204/EPTCS.172.17", "report-no": null, "categories": "cs.CL cs.AI math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both quantum mechanics and corpus linguistics based on vector spaces, the\nnotion of entanglement provides a means for the various subsystems to\ncommunicate with each other. In this paper we examine a number of\nimplementations of the categorical framework of Coecke, Sadrzadeh and Clark\n(2010) for natural language, from an entanglement perspective. Specifically,\nour goal is to better understand in what way the level of entanglement of the\nrelational tensors (or the lack of it) affects the compositional structures in\npractical situations. Our findings reveal that a number of proposals for verb\nconstruction lead to almost separable tensors, a fact that considerably\nsimplifies the interactions between the words. We examine the ramifications of\nthis fact, and we show that the use of Frobenius algebras mitigates the\npotential problems to a great extent. Finally, we briefly examine a machine\nlearning method that creates verb tensors exhibiting a sufficient level of\nentanglement.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 18:48:54 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 03:02:56 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Kartsaklis", "Dimitri", "", "University of Oxford"], ["Sadrzadeh", "Mehrnoosh", "", "Queen\n  Mary University of London"]]}, {"id": "1405.3033", "submitter": "Zeeshan Bhatti", "authors": "Zeeshan Bhatti, Ahmad Waqas, Imdad Ali Ismaili, Dil Nawaz Hakro,\n  Waseem Javaid Soomro", "title": "Phonetic based SoundEx & ShapeEx algorithm for Sindhi Spell Checker\n  System", "comments": "9 pages, 6 figures, 5 Tables, Sindhi Computing, Sindhi Language", "journal-ref": "Adv. Environ. Biol., 8(4), 1147-1155, AENSI Publisher, 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a novel combinational phonetic algorithm for Sindhi\nLanguage, to be used in developing Sindhi Spell Checker which has yet not been\ndeveloped prior to this work. The compound textual forms and glyphs of Sindhi\nlanguage presents a substantial challenge for developing Sindhi spell checker\nsystem and generating similar suggestion list for misspelled words. In order to\nimplement such a system, phonetic based Sindhi language rules and patterns must\nbe considered into account for increasing the accuracy and efficiency. The\nproposed system is developed with a blend between Phonetic based SoundEx\nalgorithm and ShapeEx algorithm for pattern or glyph matching, generating\naccurate and efficient suggestion list for incorrect or misspelled Sindhi\nwords. A table of phonetically similar sounding Sindhi characters for SoundEx\nalgorithm is also generated along with another table containing similar glyph\nor shape based character groups for ShapeEx algorithm. Both these are first\never attempt of any such type of categorization and representation for Sindhi\nLanguage.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 04:33:04 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Bhatti", "Zeeshan", ""], ["Waqas", "Ahmad", ""], ["Ismaili", "Imdad Ali", ""], ["Hakro", "Dil Nawaz", ""], ["Soomro", "Waseem Javaid", ""]]}, {"id": "1405.3272", "submitter": "Nicholas Kersting", "authors": "Nicholas Kersting", "title": "Fast and Fuzzy Private Set Intersection", "comments": "20 pages, 6 figures, plus source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private Set Intersection (PSI) is usually implemented as a sequence of\nencryption rounds between pairs of users, whereas the present work implements\nPSI in a simpler fashion: each set only needs to be encrypted once, after which\neach pair of users need only one ordinary set comparison. This is typically\norders of magnitude faster than ordinary PSI at the cost of some ``fuzziness\"\nin the matching, which may nonetheless be tolerable or even desirable. This is\ndemonstrated in the case where the sets consist of English words processed with\nWordNet.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 19:38:58 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 02:51:41 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Kersting", "Nicholas", ""]]}, {"id": "1405.3282", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Tim Althoff, Cristian Danescu-Niculescu-Mizil, Dan Jurafsky", "title": "How to Ask for a Favor: A Case Study on the Success of Altruistic\n  Requests", "comments": "To appear at ICWSM 2014. 10pp, 3 fig. Data and other info available\n  at http://www.mpi-sws.org/~cristian/How_to_Ask_for_a_Favor.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Requests are at the core of many social media systems such as question &\nanswer sites and online philanthropy communities. While the success of such\nrequests is critical to the success of the community, the factors that lead\ncommunity members to satisfy a request are largely unknown. Success of a\nrequest depends on factors like who is asking, how they are asking, when are\nthey asking, and most critically what is being requested, ranging from small\nfavors to substantial monetary donations. We present a case study of altruistic\nrequests in an online community where all requests ask for the very same\ncontribution and do not offer anything tangible in return, allowing us to\ndisentangle what is requested from textual and social factors. Drawing from\nsocial psychology literature, we extract high-level social features from text\nthat operationalize social relations between recipient and donor and\ndemonstrate that these extracted relations are predictive of success. More\nspecifically, we find that clearly communicating need through the narrative is\nessential and that that linguistic indications of gratitude, evidentiality, and\ngeneralized reciprocity, as well as high status of the asker further increase\nthe likelihood of success. Building on this understanding, we develop a model\nthat can predict the success of unseen requests, significantly improving over\nseveral baselines. We link these findings to research in psychology on helping\nbehavior, providing a basis for further analysis of success in social media\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 20:00:07 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Althoff", "Tim", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1405.3515", "submitter": "Yoon Kim", "authors": "Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, Slav Petrov", "title": "Temporal Analysis of Language through Neural Language Models", "comments": null, "journal-ref": "Proceedings of the ACL 2014 Workshop on Language Technologies and\n  Computational Social Science. June, 2014. 61--65", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method for automatically detecting change in language across\ntime through a chronologically trained neural language model. We train the\nmodel on the Google Books Ngram corpus to obtain word vector representations\nspecific to each year, and identify words that have changed significantly from\n1900 to 2009. The model identifies words such as \"cell\" and \"gay\" as having\nchanged during that time period. The model simultaneously identifies the\nspecific years during which such words underwent change.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 14:47:22 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Kim", "Yoon", ""], ["Chiu", "Yi-I", ""], ["Hanaki", "Kentaro", ""], ["Hegde", "Darshan", ""], ["Petrov", "Slav", ""]]}, {"id": "1405.3518", "submitter": "Yoon Kim", "authors": "Yoon Kim, Owen Zhang", "title": "Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme\n  for Sentiment Analysis and Text Classification", "comments": null, "journal-ref": "Proceedings of the 5th Workshop on Computational Approaches to\n  Subjectivity, Sentiment and Social Media Analysis. June, 2014. 79--83", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple but novel supervised weighting scheme for adjusting term\nfrequency in tf-idf for sentiment analysis and text classification. We compare\nour method to baseline weighting schemes and find that it outperforms them on\nmultiple benchmarks. The method is robust and works well on both snippets and\nlonger documents.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 14:50:59 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 17:18:54 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Kim", "Yoon", ""], ["Zhang", "Owen", ""]]}, {"id": "1405.3539", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh, Adam Ganz", "title": "Pattern Recognition in Narrative: Tracking Emotional Expression in\n  Context", "comments": "21 pages, 7 figures", "journal-ref": "Journal of Data Mining & Digital Humanities, 2015 (May 26, 2015)\n  jdmdh:647", "doi": "10.46298/jdmdh.647", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using geometric data analysis, our objective is the analysis of narrative,\nwith narrative of emotion being the focus in this work. The following two\nprinciples for analysis of emotion inform our work. Firstly, emotion is\nrevealed not as a quality in its own right but rather through interaction. We\nstudy the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the\n3-way relationship of Emma, Charles and Rodolphe in the novel {\\em Madame\nBovary}. Secondly, emotion, that is expression of states of mind of subjects,\nis formed and evolves within the narrative that expresses external events and\n(personal, social, physical) context. In addition to the analysis methodology\nwith key aspects that are innovative, the input data used is crucial. We use,\nfirstly, dialogue, and secondly, broad and general description that\nincorporates dialogue. In a follow-on study, we apply our unsupervised\nnarrative mapping to data streams with very low emotional expression. We map\nthe narrative of Twitter streams. Thus we demonstrate map analysis of general\nnarratives.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 15:29:48 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 14:00:50 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 22:16:39 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Murtagh", "Fionn", ""], ["Ganz", "Adam", ""]]}, {"id": "1405.3772", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Julie Sauvage-Vincent and John Puentes", "title": "INAUT, a Controlled Language for the French Coast Pilot Books\n  Instructions nautiques", "comments": "10 pages, 3 figures, accepted for publication at Fourth Workshop on\n  Controlled Natural Language (CNL 2014), 20-22 August 2014, Galway, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe INAUT, a controlled natural language dedicated to collaborative\nupdate of a knowledge base on maritime navigation and to automatic generation\nof coast pilot books (Instructions nautiques) of the French National\nHydrographic and Oceanographic Service SHOM. INAUT is based on French language\nand abundantly uses georeferenced entities. After describing the structure of\nthe overall system, giving details on the language and on its generation, and\ndiscussing the three major applications of INAUT (document production,\ninteraction with ENCs and collaborative updates of the knowledge base), we\nconclude with future extensions and open problems.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 09:11:53 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Haralambous", "Yannis", ""], ["Sauvage-Vincent", "Julie", ""], ["Puentes", "John", ""]]}, {"id": "1405.3786", "submitter": "Ana Mestrovic", "authors": "Domagoj Margan, Ana Me\\v{s}trovi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Complex Networks Measures for Differentiation between Normal and\n  Shuffled Croatian Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the properties of the Croatian texts via complex networks.\nWe present network properties of normal and shuffled Croatian texts for\ndifferent shuffling principles: on the sentence level and on the text level. In\nboth experiments we preserved the vocabulary size, word and sentence frequency\ndistributions. Additionally, in the first shuffling approach we preserved the\nsentence structure of the text and the number of words per sentence. Obtained\nresults showed that degree rank distributions exhibit no substantial deviation\nin shuffled networks, and strength rank distributions are preserved due to the\nsame word frequencies. Therefore, standard approach to study the structure of\nlinguistic co-occurrence networks showed no clear difference among the\ntopologies of normal and shuffled texts. Finally, we showed that the in- and\nout- selectivity values from shuffled texts are constantly below selectivity\nvalues calculated from normal texts. Our results corroborate that the node\nselectivity measure can capture structural differences between original and\nshuffled Croatian texts.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 09:51:11 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Margan", "Domagoj", ""], ["Me\u0161trovi\u0107", "Ana", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1405.3925", "submitter": "Laurent Romary", "authors": "Laurent Romary (IDSL, INRIA Saclay - Ile de France, CMB), Andreas Witt\n  (IDS)", "title": "M\\'ethodes pour la repr\\'esentation informatis\\'ee de donn\\'ees\n  lexicales / Methoden der Speicherung lexikalischer Daten", "comments": "This text comprises both a French and a German version", "journal-ref": "Lexicographica 30 (2014)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, new developments in the area of lexicography have altered\nnot only the management, processing and publishing of lexicographical data, but\nalso created new types of products such as electronic dictionaries and\nthesauri. These expand the range of possible uses of lexical data and support\nusers with more flexibility, for instance in assisting human translation. In\nthis article, we give a short and easy-to-understand introduction to the\nproblematic nature of the storage, display and interpretation of lexical data.\nWe then describe the main methods and specifications used to build and\nrepresent lexical data. This paper is targeted for the following groups of\npeople: linguists, lexicographers, IT specialists, computer linguists and all\nothers who wish to learn more about the modelling, representation and\nvisualization of lexical knowledge. This paper is written in two languages:\nFrench and German.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 18:10:36 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Romary", "Laurent", "", "IDSL, INRIA Saclay - Ile de France, CMB"], ["Witt", "Andreas", "", "IDS"]]}, {"id": "1405.4053", "submitter": "Quoc Le", "authors": "Quoc V. Le and Tomas Mikolov", "title": "Distributed Representations of Sentences and Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms require the input to be represented as a\nfixed-length feature vector. When it comes to texts, one of the most common\nfixed-length features is bag-of-words. Despite their popularity, bag-of-words\nfeatures have two major weaknesses: they lose the ordering of the words and\nthey also ignore semantics of the words. For example, \"powerful,\" \"strong\" and\n\"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an\nunsupervised algorithm that learns fixed-length feature representations from\nvariable-length pieces of texts, such as sentences, paragraphs, and documents.\nOur algorithm represents each document by a dense vector which is trained to\npredict words in the document. Its construction gives our algorithm the\npotential to overcome the weaknesses of bag-of-words models. Empirical results\nshow that Paragraph Vectors outperform bag-of-words models as well as other\ntechniques for text representations. Finally, we achieve new state-of-the-art\nresults on several text classification and sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 07:12:16 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 23:23:19 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Le", "Quoc V.", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1405.4097", "submitter": "Ana Mestrovic", "authors": "Kristina Ban, Ivan Ivaki\\'c and Ana Me\\v{s}trovi\\'c", "title": "A preliminary study of Croatian Language Syllable Networks", "comments": "in Proceedings MIPRO junior - Student Papers", "journal-ref": "IEEE 36th International Convention on Information and\n  Communication Technology, Electronics and Microelectronics (MIPRO 2013), pp.\n  1004-1008", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents preliminary results of Croatian syllable networks\nanalysis. Syllable network is a network in which nodes are syllables and links\nbetween them are constructed according to their connections within words. In\nthis paper we analyze networks of syllables generated from texts collected from\nthe Croatian Wikipedia and Blogs. As a main tool we use complex network\nanalysis methods which provide mechanisms that can reveal new patterns in a\nlanguage structure. We aim to show that syllable networks have much higher\nclustering coefficient in comparison to Erd\\\"os-Renyi random networks. The\nresults indicate that Croatian syllable networks exhibit certain properties of\na small world networks. Furthermore, we compared Croatian syllable networks\nwith Portuguese and Chinese syllable networks and we showed that they have\nsimilar properties.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 08:57:40 GMT"}, {"version": "v2", "created": "Thu, 17 Jul 2014 16:13:58 GMT"}], "update_date": "2014-07-18", "authors_parsed": [["Ban", "Kristina", ""], ["Ivaki\u0107", "Ivan", ""], ["Me\u0161trovi\u0107", "Ana", ""]]}, {"id": "1405.4248", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous", "title": "Les math\\'ematiques de la langue : l'approche formelle de Montague", "comments": "14 pages, in French. Will appear in the journal Quadrature\n  (http://www.quadrature.info) in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a natural language modelization method which is strongely relying\non mathematics. This method, called \"Formal Semantics,\" has been initiated by\nthe American linguist Richard M. Montague in the 1970's. It uses mathematical\ntools such as formal languages and grammars, first-order logic, type theory and\n$\\lambda$-calculus. Our goal is to have the reader discover both Montagovian\nformal semantics and the mathematical tools that he used in his method.\n  -----\n  Nous pr\\'esentons une m\\'ethode de mod\\'elisation de la langue naturelle qui\nest fortement bas\\'ee sur les math\\'ematiques. Cette m\\'ethode, appel\\'ee\n{\\guillemotleft}s\\'emantique formelle{\\guillemotright}, a \\'et\\'e initi\\'ee par\nle linguiste am\\'ericain Richard M. Montague dans les ann\\'ees 1970. Elle\nutilise des outils math\\'ematiques tels que les langages et grammaires formels,\nla logique du 1er ordre, la th\\'eorie de types et le $\\lambda$-calcul. Nous\nnous proposons de faire d\\'ecouvrir au lecteur tant la s\\'emantique formelle de\nMontague que les outils math\\'ematiques dont il s'est servi.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 17:17:19 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Haralambous", "Yannis", ""]]}, {"id": "1405.4273", "submitter": "Jan Botha", "authors": "Jan A. Botha and Phil Blunsom", "title": "Compositional Morphology for Word Representations and Language Modelling", "comments": "Proceedings of the 31st International Conference on Machine Learning\n  (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a scalable method for integrating compositional\nmorphological representations into a vector-based probabilistic language model.\nOur approach is evaluated in the context of log-bilinear language models,\nrendered suitably efficient for implementation inside a machine translation\ndecoder by factoring the vocabulary. We perform both intrinsic and extrinsic\nevaluations, presenting results on a range of languages which demonstrate that\nour model learns morphological representations that both perform well on word\nsimilarity tasks and lead to substantial reductions in perplexity. When used\nfor translation into morphologically rich languages with large vocabularies,\nour models obtain improvements of up to 1.2 BLEU points relative to a baseline\nsystem using back-off n-gram models.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 19:08:14 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Botha", "Jan A.", ""], ["Blunsom", "Phil", ""]]}, {"id": "1405.4364", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Vitaly Klyuev", "title": "Thematically Reinforced Explicit Semantic Analysis", "comments": "13 pages, 2 figures, presented at CICLing 2013", "journal-ref": "IJCLA vol. 4, no. 1, Jan.-Jun. 2013, pp. 79--94", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extended, thematically reinforced version of Gabrilovich and\nMarkovitch's Explicit Semantic Analysis (ESA), where we obtain thematic\ninformation through the category structure of Wikipedia. For this we first\ndefine a notion of categorical tfidf which measures the relevance of terms in\ncategories. Using this measure as a weight we calculate a maximal spanning tree\nof the Wikipedia corpus considered as a directed graph of pages and categories.\nThis tree provides us with a unique path of \"most related categories\" between\neach page and the top of the hierarchy. We reinforce tfidf of words in a page\nby aggregating it with categorical tfidfs of the nodes of these paths, and\ndefine a thematically reinforced ESA semantic relatedness measure which is more\nrobust than standard ESA and less sensitive to noise caused by out-of-context\nwords. We apply our method to the French Wikipedia corpus, evaluate it through\na text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a\nprecision increase of 9-10% compared with standard ESA.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 07:58:58 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Haralambous", "Yannis", ""], ["Klyuev", "Vitaly", ""]]}, {"id": "1405.4392", "submitter": "Sunny Mitra", "authors": "Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh\n  Mukherjee, Pawan Goyal", "title": "That's sick dude!: Automatic identification of word sense change across\n  different timescales", "comments": "10 pages, 2 figures, ACL-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an unsupervised method to identify noun sense\nchanges based on rigorous analysis of time-varying text data available in the\nform of millions of digitized books. We construct distributional thesauri based\nnetworks from data at different time points and cluster each of them separately\nto obtain word-centric sense clusters corresponding to the different time\npoints. Subsequently, we compare these sense clusters of two different time\npoints to find if (i) there is birth of a new sense or (ii) if an older sense\nhas got split into more than one sense or (iii) if a newer sense has been\nformed from the joining of older senses or (iv) if a particular sense has died.\nWe conduct a thorough evaluation of the proposed methodology both manually as\nwell as through comparison with WordNet. Manual evaluation indicates that the\nalgorithm could correctly identify 60.4% birth cases from a set of 48 randomly\npicked samples and 57% split/join cases from a set of 21 randomly picked\nsamples. Remarkably, in 44% cases the birth of a novel sense is attested by\nWordNet, while in 46% cases and 43% cases split and join are respectively\nconfirmed by WordNet. Our approach can be applied for lexicography, as well as\nfor applications like word sense disambiguation or semantic search.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 13:29:13 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Mitra", "Sunny", ""], ["Mitra", "Ritwik", ""], ["Riedl", "Martin", ""], ["Biemann", "Chris", ""], ["Mukherjee", "Animesh", ""], ["Goyal", "Pawan", ""]]}, {"id": "1405.4433", "submitter": "Domagoj Margan", "authors": "Domagoj Margan, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c, Ana Me\\v{s}trovi\\'c", "title": "Preliminary Report on the Structure of Croatian Linguistic Co-occurrence\n  Networks", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the structure of Croatian linguistic\nco-occurrence networks. We examine the change of network structure properties\nby systematically varying the co-occurrence window sizes, the corpus sizes and\nremoving stopwords. In a co-occurrence window of size $n$ we establish a link\nbetween the current word and $n-1$ subsequent words. The results point out that\nthe increase of the co-occurrence window size is followed by a decrease in\ndiameter, average path shortening and expectedly condensing the average\nclustering coefficient. The same can be noticed for the removal of the\nstopwords. Finally, since the size of texts is reflected in the network\nproperties, our results suggest that the corpus influence can be reduced by\nincreasing the co-occurrence window size.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 20:43:00 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Margan", "Domagoj", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""], ["Me\u0161trovi\u0107", "Ana", ""]]}, {"id": "1405.4599", "submitter": "Dalei Wu", "authors": "Dalei Wu and Haiqing Wu", "title": "Modelling Data Dispersion Degree in Automatic Robust Estimation for\n  Multivariate Gaussian Mixture Models with an Application to Noisy Speech\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The trimming scheme with a prefixed cutoff portion is known as a method of\nimproving the robustness of statistical models such as multivariate Gaussian\nmixture models (MG- MMs) in small scale tests by alleviating the impacts of\noutliers. However, when this method is applied to real- world data, such as\nnoisy speech processing, it is hard to know the optimal cut-off portion to\nremove the outliers and sometimes removes useful data samples as well. In this\npaper, we propose a new method based on measuring the dispersion degree (DD) of\nthe training data to avoid this problem, so as to realise automatic robust\nestimation for MGMMs. The DD model is studied by using two different measures.\nFor each one, we theoretically prove that the DD of the data samples in a\ncontext of MGMMs approximately obeys a specific (chi or chi-square)\ndistribution. The proposed method is evaluated on a real-world application with\na moderately-sized speaker recognition task. Experiments show that the proposed\nmethod can significantly improve the robustness of the conventional training\nmethod of GMMs for speaker recognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 04:36:38 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Wu", "Dalei", ""], ["Wu", "Haiqing", ""]]}, {"id": "1405.4918", "submitter": "Ekin Oguz", "authors": "Mishari Almishari, Ekin Oguz, Gene Tsudik", "title": "Fighting Authorship Linkability with Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amounts of contributed content -- including traditional literature,\nblogs, music, videos, reviews and tweets -- are available on the Internet\ntoday, with authors numbering in many millions. Textual information, such as\nproduct or service reviews, is an important and increasingly popular type of\ncontent that is being used as a foundation of many trendy community-based\nreviewing sites, such as TripAdvisor and Yelp. Some recent results have shown\nthat, due partly to their specialized/topical nature, sets of reviews authored\nby the same person are readily linkable based on simple stylometric features.\nIn practice, this means that individuals who author more than a few reviews\nunder different accounts (whether within one site or across multiple sites) can\nbe linked, which represents a significant loss of privacy.\n  In this paper, we start by showing that the problem is actually worse than\npreviously believed. We then explore ways to mitigate authorship linkability in\ncommunity-based reviewing. We first attempt to harness the global power of\ncrowdsourcing by engaging random strangers into the process of re-writing\nreviews. As our empirical results (obtained from Amazon Mechanical Turk)\nclearly demonstrate, crowdsourcing yields impressively sensible reviews that\nreflect sufficiently different stylometric characteristics such that prior\nstylometric linkability techniques become largely ineffective. We also consider\nusing machine translation to automatically re-write reviews. Contrary to what\nwas previously believed, our results show that translation decreases authorship\nlinkability as the number of intermediate languages grows. Finally, we explore\nthe combination of crowdsourcing and machine translation and report on the\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 23:26:44 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Almishari", "Mishari", ""], ["Oguz", "Ekin", ""], ["Tsudik", "Gene", ""]]}, {"id": "1405.5202", "submitter": "Altaf Rahman", "authors": "Altaf Rahman, Vincent Ng", "title": "Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference\n  Resolution", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  469-521, 2011", "doi": "10.1613/jair.3120", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional learning-based coreference resolvers operate by training the\nmention-pair model for determining whether two mentions are coreferent or not.\nThough conceptually simple and easy to understand, the mention-pair model is\nlinguistically rather unappealing and lags far behind the heuristic-based\ncoreference models proposed in the pre-statistical NLP era in terms of\nsophistication. Two independent lines of recent research have attempted to\nimprove the mention-pair model, one by acquiring the mention-ranking model to\nrank preceding mentions for a given anaphor, and the other by training the\nentity-mention model to determine whether a preceding cluster is coreferent\nwith a given mention. We propose a cluster-ranking approach to coreference\nresolution, which combines the strengths of the mention-ranking model and the\nentity-mention model, and is therefore theoretically more appealing than both\nof these models. In addition, we seek to improve cluster rankers via two\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\nby jointly modeling anaphoricity determination and coreference resolution.\nExperimental results on the ACE data sets demonstrate the superior performance\nof cluster rankers to competing approaches as well as the effectiveness of our\ntwo extensions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:06:09 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Rahman", "Altaf", ""], ["Ng", "Vincent", ""]]}, {"id": "1405.5208", "submitter": "Alexander M. Rush", "authors": "Alexander M. Rush, Michael Collins", "title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference\n  in Natural Language Processing", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  305-362, 2012", "doi": "10.1613/jair.3680", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual decomposition, and more generally Lagrangian relaxation, is a classical\nmethod for combinatorial optimization; it has recently been applied to several\ninference problems in natural language processing (NLP). This tutorial gives an\noverview of the technique. We describe example algorithms, describe formal\nguarantees for the method, and describe practical issues in implementing the\nalgorithms. While our examples are predominantly drawn from the NLP literature,\nthe material should be of general relevance to inference problems in machine\nlearning. A central theme of this tutorial is that Lagrangian relaxation is\nnaturally applied in conjunction with a broad class of combinatorial\nalgorithms, allowing inference in models that go significantly beyond previous\nwork on Lagrangian relaxation for inference in graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:50:15 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Rush", "Alexander M.", ""], ["Collins", "Michael", ""]]}, {"id": "1405.5447", "submitter": "Hosein Azarbonyad", "authors": "Hosein Azarbonyad, Azadeh Shakery, Heshaam Faili", "title": "Learning to Exploit Different Translation Resources for Cross Language\n  Information Retrieval", "comments": null, "journal-ref": "International Journal of Information and Communication Technology\n  Research, Volume 6, Issue 1, pp. 55-68, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important factors that affects the performance of Cross Language\nInformation Retrieval(CLIR)is the quality of translations being employed in\nCLIR. In order to improve the quality of translations, it is important to\nexploit available resources efficiently. Employing different translation\nresources with different characteristics has many challenges. In this paper, we\npropose a method for exploiting available translation resources simultaneously.\nThis method employs Learning to Rank(LTR) for exploiting different translation\nresources. To apply LTR methods for query translation, we define different\ntranslation relation based features in addition to context based features. We\nuse the contextual information contained in translation resources for\nextracting context based features.The proposed method uses LTR to construct a\ntranslation ranking model based on defined features. The constructed model is\nused for ranking translation candidates of query words. To evaluate the\nproposed method we do English-Persian CLIR, in which we employ the translation\nranking model to find translations of English queries and employ the\ntranslations to retrieve Persian documents. Experimental results show that our\napproach significantly outperforms single resource based CLIR methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 11:45:06 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Azarbonyad", "Hosein", ""], ["Shakery", "Azadeh", ""], ["Faili", "Heshaam", ""]]}, {"id": "1405.5474", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous", "title": "New Perspectives in Sinographic Language Processing Through the Use of\n  Character Structure", "comments": "17 pages, 5 figures, presented at CICLing 2013", "journal-ref": "Lecture Notes in Computer Science 7816 (2013), pp. 201--217", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese characters have a complex and hierarchical graphical structure\ncarrying both semantic and phonetic information. We use this structure to\nenhance the text model and obtain better results in standard NLP operations.\nFirst of all, to tackle the problem of graphical variation we define\nallographic classes of characters. Next, the relation of inclusion of a\nsubcharacter in a characters, provides us with a directed graph of allographic\nclasses. We provide this graph with two weights: semanticity (semantic relation\nbetween subcharacter and character) and phoneticity (phonetic relation) and\ncalculate \"most semantic subcharacter paths\" for each character. Finally,\nadding the information contained in these paths to unigrams we claim to\nincrease the efficiency of text mining methods. We evaluate our method on a\ntext classification task on two corpora (Chinese and Japanese) of a total of 18\nmillion characters and get an improvement of 3% on an already high baseline of\n89.6% precision, obtained by a linear SVM classifier. Other possible\napplications and perspectives of the system are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 16:49:50 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Haralambous", "Yannis", ""]]}, {"id": "1405.5654", "submitter": "Lijiang Chen", "authors": "Lijiang Chen", "title": "Machine Translation Model based on Non-parallel Corpus and\n  Semi-supervised Transductive Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the parallel corpus has an irreplaceable role in machine\ntranslation, its scale and coverage is still beyond the actual needs.\nNon-parallel corpus resources on the web have an inestimable potential value in\nmachine translation and other natural language processing tasks. This article\nproposes a semi-supervised transductive learning method for expanding the\ntraining corpus in statistical machine translation system by extracting\nparallel sentences from the non-parallel corpus. This method only requires a\nsmall amount of labeled corpus and a large unlabeled corpus to build a\nhigh-performance classifier, especially for when there is short of labeled\ncorpus. The experimental results show that by combining the non-parallel corpus\nalignment and the semi-supervised transductive learning method, we can more\neffectively use their respective strengths to improve the performance of\nmachine translation system.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 07:58:56 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Chen", "Lijiang", ""]]}, {"id": "1405.5674", "submitter": "Mathieu Mangeot", "authors": "Mathieu Mangeot (LIG)", "title": "Mot\\`aMot project: conversion of a French-Khmer published dictionary for\n  building a multilingual lexical system", "comments": "8 pages, Languages Resources and Evaluation Conference, Reykjavik :\n  Iceland (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic issues related to the information processing techniques are very\nimportant. The development of such technologies is a major asset for developing\ncountries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and\nThailand. The MotAMot project aims to computerize an under-resourced language:\nKhmer, spoken mainly in Cambodia. The main goal of the project is the\ndevelopment of a multilingual lexical system targeted for Khmer. The\nmacrostructure is a pivot one with each word sense of each language linked to a\npivot axi. The microstructure comes from a simplification of the explanatory\nand combinatory dictionary. The lexical system has been initialized with data\ncoming mainly from the conversion of the French-Khmer bilingual dictionary of\nDenis Richer from Word to XML format. The French part was completed with\npronunciation and parts-of-speech coming from the FeM French-english-Malay\ndictionary. The Khmer headwords noted in IPA in the Richer dictionary were\nconverted to Khmer writing with OpenFST, a finite state transducer tool. The\nresulting resource is available online for lookup, editing, download and remote\nprogramming via a REST API on a Jibiki platform.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 08:57:54 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Mangeot", "Mathieu", "", "LIG"]]}, {"id": "1405.5893", "submitter": "Mathieu Mangeot", "authors": "Chantal Enguehard (LINA), Mathieu Mangeot (LIG)", "title": "Computerization of African languages-French dictionaries", "comments": "8 pages", "journal-ref": "Iceland (2014)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper relates work done during the DiLAF project. It consists in\nconverting 5 bilingual African language-French dictionaries originally in Word\nformat into XML following the LMF model. The languages processed are Bambara,\nHausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced\nlanguages concerning Natural Language Processing tools. Once converted, the\ndictionaries are available online on the Jibiki platform for lookup and\nmodification. The DiLAF project is first presented. A description of each\ndictionary follows. Then, the conversion methodology from .doc format to XML\nfiles is presented. A specific point on the usage of Unicode follows. Then,\neach step of the conversion into XML and LMF is detailed. The last part\npresents the Jibiki lexical resources management platform used for the project.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 20:15:57 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Enguehard", "Chantal", "", "LINA"], ["Mangeot", "Mathieu", "", "LIG"]]}, {"id": "1405.6068", "submitter": "Dmitry Lande", "authors": "Dmitry Lande", "title": "Building of Networks of Natural Hierarchies of Terms Based on Analysis\n  of Texts Corpora", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of building of networks of hierarchies of terms based on the\nanalysis of chosen text corpora is offered. The technique is based on the\nmethodology of horizontal visibility graphs. Constructed and investigated\nlanguage network, formed on the basis of electronic preprints arXiv on topics\nof information retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 14:05:45 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Lande", "Dmitry", ""]]}, {"id": "1405.6103", "submitter": "Kurt Winkler", "authors": "Kurt Winkler, Tobias Kuhn, Martin Volk", "title": "Evaluating the fully automatic multi-language translation of the Swiss\n  avalanche bulletin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Swiss avalanche bulletin is produced twice a day in four languages. Due\nto the lack of time available for manual translation, a fully automated\ntranslation system is employed, based on a catalogue of predefined phrases and\npredetermined rules of how these phrases can be combined to produce sentences.\nThe system is able to automatically translate such sentences from German into\nthe target languages French, Italian and English without subsequent\nproofreading or correction. Our catalogue of phrases is limited to a small\nsublanguage. The reduction of daily translation costs is expected to offset the\ninitial development costs within a few years. After being operational for two\nwinter seasons, we assess here the quality of the produced texts based on an\nevaluation where participants rate real danger descriptions from both origins,\nthe catalogue of phrases versus the manually written and translated texts. With\na mean recognition rate of 55%, users can hardly distinguish between the two\ntypes of texts, and give similar ratings with respect to their language\nquality. Overall, the output from the catalogue system can be considered\nvirtually equivalent to a text written by avalanche forecasters and then\nmanually translated by professional translators. Furthermore, forecasters\ndeclared that all relevant situations were captured by the system with\nsufficient accuracy and within the limited time available.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 15:51:10 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Winkler", "Kurt", ""], ["Kuhn", "Tobias", ""], ["Volk", "Martin", ""]]}, {"id": "1405.6164", "submitter": "Ion Androutsopoulos", "authors": "Ion Androutsopoulos, Gerasimos Lampouras, Dimitrios Galanis", "title": "Generating Natural Language Descriptions from OWL Ontologies: the\n  NaturalOWL System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 48, pages\n  671-715, 2013", "doi": "10.1613/jair.4017", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NaturalOWL, a natural language generation system that produces\ntexts describing individuals or classes of OWL ontologies. Unlike simpler OWL\nverbalizers, which typically express a single axiom at a time in controlled,\noften not entirely fluent natural language primarily for the benefit of domain\nexperts, we aim to generate fluent and coherent multi-sentence texts for\nend-users. With a system like NaturalOWL, one can publish information in OWL on\nthe Web, along with automatically produced corresponding texts in multiple\nlanguages, making the information accessible not only to computer programs and\ndomain experts, but also end-users. We discuss the processing stages of\nNaturalOWL, the optional domain-dependent linguistic resources that the system\ncan use at each stage, and why they are useful. We also present trials showing\nthat when the domain-dependent llinguistic resources are available, NaturalOWL\nproduces significantly better texts compared to a simpler verbalizer, and that\nthe resources can be created with relatively light effort.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 02:47:37 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Androutsopoulos", "Ion", ""], ["Lampouras", "Gerasimos", ""], ["Galanis", "Dimitrios", ""]]}, {"id": "1405.6293", "submitter": "Ahmed Yousef Y", "authors": "Ahmed H. Yousef", "title": "Cross-Language Personal Name Mapping", "comments": null, "journal-ref": "International Journal of Computational Linguistics Research, vol\n  4, issue 4, December 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Name matching between multiple natural languages is an important step in\ncross-enterprise integration applications and data mining. It is difficult to\ndecide whether or not two syntactic values (names) from two heterogeneous data\nsources are alternative designation of the same semantic entity (person), this\nprocess becomes more difficult with Arabic language due to several factors\nincluding spelling and pronunciation variation, dialects and special vowel and\nconsonant distinction and other linguistic characteristics. This paper proposes\na new framework for name matching between the Arabic language and other\nlanguages. The framework uses a dictionary based on a new proposed version of\nthe Soundex algorithm to encapsulate the recognition of special features of\nArabic names. The framework proposes a new proximity matching algorithm to suit\nthe high importance of order sensitivity in Arabic name matching. New\nperformance evaluation metrics are proposed as well. The framework is\nimplemented and verified empirically in several case studies demonstrating\nsubstantial improvements compared to other well-known techniques found in\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 11:39:34 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Yousef", "Ahmed H.", ""]]}, {"id": "1405.6667", "submitter": "Puneet Singh Ludu", "authors": "Puneet Singh Ludu", "title": "Inferring gender of a Twitter user using celebrities it follows", "comments": "Submitted at CSE department, SUNY Buffalo, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of user gender classification in social media,\nwith an application to Twitter. The approach automatically predicts gender by\nleveraging observable information such as the tweet behavior, linguistic\ncontent of the user's Twitter feed and the celebrities followed by the user.\nThis paper first evaluates linguistic content based features using LIWC\ndictionary and popular neighborhood features using Wikipedia and Freebase. Then\naugments both features which yielded a significant increase in the accuracy for\ngender prediction. Results show that rich linguistic features combined with\npopular neighborhood prove valuables and promising for additional user\nclassification needs.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 18:25:35 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Ludu", "Puneet Singh", ""]]}, {"id": "1405.6678", "submitter": "Richard Moot", "authors": "Richard Moot (LaBRI)", "title": "Hybrid Type-Logical Grammars, First-Order Linear Logic and the\n  Descriptive Inadequacy of Lambda Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we show that hybrid type-logical grammars are a fragment of\nfirst-order linear logic. This embedding result has several important\nconsequences: it not only provides a simple new proof theory for the calculus,\nthereby clarifying the proof-theoretic foundations of hybrid type-logical\ngrammars, but, since the translation is simple and direct, it also provides\nseveral new parsing strategies for hybrid type-logical grammars. Second,\nNP-completeness of hybrid type-logical grammars follows immediately. The main\nembedding result also sheds new light on problems with lambda grammars/abstract\ncategorial grammars and shows lambda grammars/abstract categorial grammars\nsuffer from problems of over-generation and from problems at the\nsyntax-semantics interface unlike any other categorial grammar.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 18:48:15 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Moot", "Richard", "", "LaBRI"]]}, {"id": "1405.6682", "submitter": "Thierry Poibeau", "authors": "Thierry Poibeau (LaTTICe)", "title": "Optimality Theory as a Framework for Lexical Acquisition", "comments": null, "journal-ref": "15th International Conference on Intelligent Text Processing and\n  Computational Linguistics, Nepal (2014)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper re-investigates a lexical acquisition system initially developed\nfor French.We show that, interestingly, the architecture of the system\nreproduces and implements the main components of Optimality Theory. However, we\nformulate the hypothesis that some of its limitations are mainly due to a poor\nrepresentation of the constraints used. Finally, we show how a better\nrepresentation of the constraints used would yield better results.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 18:51:06 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Poibeau", "Thierry", "", "LaTTICe"]]}, {"id": "1405.7397", "submitter": "Kamal Sarkar", "authors": "Vivekananda Gayen, Kamal Sarkar", "title": "An HMM Based Named Entity Recognition System for Indian Languages: The\n  JU System at ICON 2013", "comments": "The ICON 2013 tools contest on Named Entity Recognition in Indian\n  languages (IL) co-located with the 10th International Conference on Natural\n  Language Processing(ICON), CDAC Noida, India,18-20 December, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named\nEntity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,\nPunjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model\nhas been used to implement our system. The system has been trained and tested\non the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of\n0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,\nEnglish, Hindi, Marathi, Punjabi, Tamil and Telugu respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 21:05:00 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Gayen", "Vivekananda", ""], ["Sarkar", "Kamal", ""]]}, {"id": "1405.7519", "submitter": "Deepali Virmani", "authors": "Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi", "title": "Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value", "comments": "IJCSIT, MAY 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion Mining and Sentiment Analysis is a process of identifying opinions in\nlarge unstructured/structured data and then analysing polarity of those\nopinions. Opinion mining and sentiment analysis have found vast application in\nanalysing online ratings, analysing product based reviews, e-governance, and\nmanaging hostile content over the internet. This paper proposes an algorithm to\nimplement aspect level sentiment analysis. The algorithm takes input from the\nremarks submitted by various teachers of a student. An aspect tree is formed\nwhich has various levels and weights are assigned to each branch to identify\nlevel of aspect. Aspect value is calculated by the algorithm by means of the\nproposed aspect tree. Dictionary based method is implemented to evaluate the\npolarity of the remark. The algorithm returns the aspect value clubbed with\nopinion value and sentiment value which helps in concluding the summarized\nvalue of remark.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 11:05:29 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Virmani", "Deepali", ""], ["Malhotra", "Vikrant", ""], ["Tyagi", "Ridhi", ""]]}, {"id": "1405.7711", "submitter": "David L. Chen", "authors": "David L. Chen, Joohyun Kim, Raymond J. Mooney", "title": "Training a Multilingual Sportscaster: Using Perceptual Context to Learn\n  Language", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 37, pages\n  397-435, 2010", "doi": "10.1613/jair.2962", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for learning to interpret and generate language\nusing only perceptual context as supervision. We demonstrate its capabilities\nby developing a system that learns to sportscast simulated robot soccer games\nin both English and Korean without any language-specific prior knowledge.\nTraining employs only ambiguous supervision consisting of a stream of\ndescriptive textual comments and a sequence of events extracted from the\nsimulation trace. The system simultaneously establishes correspondences between\nindividual comments and the events that they describe while building a\ntranslation model that supports both parsing and generation. We also present a\nnovel algorithm for learning which events are worth describing. Human\nevaluations of the generated commentaries indicate they are of reasonable\nquality and in some cases even on par with those produced by humans for our\nlimited domain.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:29:26 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Chen", "David L.", ""], ["Kim", "Joohyun", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1405.7713", "submitter": "Sophia Katrenko", "authors": "Sophia Katrenko, Pieter Adriaans, Maarten van Someren", "title": "Using Local Alignments for Relation Recognition", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  1-48, 2010", "doi": "10.1613/jair.2964", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of marrying structural similarity with\nsemantic relatedness for Information Extraction from text. Aiming at accurate\nrecognition of relations, we introduce local alignment kernels and explore\nvarious possibilities of using them for this task. We give a definition of a\nlocal alignment (LA) kernel based on the Smith-Waterman score as a sequence\nsimilarity measure and proceed with a range of possibilities for computing\nsimilarity between elements of sequences. We show how distributional similarity\nmeasures obtained from unlabeled data can be incorporated into the learning\ntask as semantic knowledge. Our experiments suggest that the LA kernel yields\npromising results on various biomedical corpora outperforming two baselines by\na large margin. Additional series of experiments have been conducted on the\ndata sets of seven general relation types, where the performance of the LA\nkernel is comparable to the current state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:51:47 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Katrenko", "Sophia", ""], ["Adriaans", "Pieter", ""], ["van Someren", "Maarten", ""]]}, {"id": "1405.7908", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Semantic Composition and Decomposition: From Recognition to Generation", "comments": "National Research Council Canada - Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic composition is the task of understanding the meaning of text by\ncomposing the meanings of the individual words in the text. Semantic\ndecomposition is the task of understanding the meaning of an individual word by\ndecomposing it into various aspects (factors, constituents, components) that\nare latent in the meaning of the word. We take a distributional approach to\nsemantics, in which a word is represented by a context vector. Much recent work\nhas considered the problem of recognizing compositions and decompositions, but\nwe tackle the more difficult generation problem. For simplicity, we focus on\nnoun-modifier bigrams and noun unigrams. A test for semantic composition is,\ngiven context vectors for the noun and modifier in a noun-modifier bigram (\"red\nsalmon\"), generate a noun unigram that is synonymous with the given bigram\n(\"sockeye\"). A test for semantic decomposition is, given a context vector for a\nnoun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous\nwith the given unigram (\"brandy glass\"). With a vocabulary of about 73,000\nunigrams from WordNet, there are 73,000 candidate unigram compositions for a\nbigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a\nunigram. We generate ranked lists of potential solutions in two passes. A fast\nunsupervised learning algorithm generates an initial list of candidates and\nthen a slower supervised learning algorithm refines the list. We evaluate the\ncandidate solutions by comparing them to WordNet synonym sets. For\ndecomposition (unigram to bigram), the top 100 most highly ranked bigrams\ninclude a WordNet synonym of the given unigram 50.7% of the time. For\ncomposition (bigram to unigram), the top 100 most highly ranked unigrams\ninclude a WordNet synonym of the given bigram 77.8% of the time.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 16:36:07 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1405.7975", "submitter": "Ercan Canhasi", "authors": "Ercan Canhasi", "title": "Multi-layered graph-based multi-document summarization model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-document summarization is a process of automatic generation of a\ncompressed version of the given collection of documents. Recently, the\ngraph-based models and ranking algorithms have been actively investigated by\nthe extractive document summarization community. While most work to date\nfocuses on homogeneous connecteness of sentences and heterogeneous connecteness\nof documents and sentences (e.g. sentence similarity weighted by document\nimportance), in this paper we present a novel 3-layered graph model that\nemphasizes not only sentence and document level relations but also the\ninfluence of under sentence level relations (e.g. a part of sentence\nsimilarity).\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 22:21:00 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Canhasi", "Ercan", ""]]}]