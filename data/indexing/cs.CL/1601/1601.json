[{"id": "1601.00025", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh", "title": "Write a Classifier: Predicting Visual Classifiers from Unstructured Text", "comments": "(TPAMI) Transactions on Pattern Analysis and Machine Intelligence\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People typically learn through exposure to visual concepts associated with\nlinguistic descriptions. For instance, teaching visual object categories to\nchildren is often accompanied by descriptions in text or speech. In a machine\nlearning context, these observations motivates us to ask whether this learning\nprocess could be computationally modeled to learn visual classifiers. More\nspecifically, the main question of this work is how to utilize purely textual\ndescription of visual classes with no training images, to learn explicit visual\nclassifiers for them. We propose and investigate two baseline formulations,\nbased on regression and domain transfer, that predict a linear classifier.\nThen, we propose a new constrained optimization formulation that combines a\nregression function and a knowledge transfer function with additional\nconstraints to predict the parameters of a linear classifier. We also propose a\ngeneric kernelized models where a kernel classifier is predicted in the form\ndefined by the representer theorem. The kernelized models allow defining and\nutilizing any two RKHS (Reproducing Kernel Hilbert Space) kernel functions in\nthe visual space and text space, respectively. We finally propose a kernel\nfunction between unstructured text descriptions that builds on distributional\nsemantics, which shows an advantage in our setting and could be useful for\nother applications. We applied all the studied models to predict visual\nclassifiers on two fine-grained and challenging categorization datasets (CU\nBirds and Flower Datasets), and the results indicate successful predictions of\nour final model over several baselines that we designed.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:23:34 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:13:59 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1601.00087", "submitter": "Mohammed Korayem", "authors": "Mohammed Korayem, Khalifeh Aljadda, and David Crandall", "title": "Sentiment/Subjectivity Analysis Survey for Languages other than English", "comments": "This is an accepted version in Social Network Analysis and Mining\n  journal. The final publication will be available at Springer via\n  http://dx.doi.org/10.1007/s13278-016-0381-6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective and sentiment analysis have gained considerable attention\nrecently. Most of the resources and systems built so far are done for English.\nThe need for designing systems for other languages is increasing. This paper\nsurveys different ways used for building systems for subjective and sentiment\nanalysis for languages other than English. There are three different types of\nsystems used for building these systems. The first (and the best) one is the\nlanguage specific systems. The second type of systems involves reusing or\ntransferring sentiment resources from English to the target language. The third\ntype of methods is based on using language independent methods. The paper\npresents a separate section devoted to Arabic sentiment analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 15:42:27 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 01:45:09 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 16:50:10 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Korayem", "Mohammed", ""], ["Aljadda", "Khalifeh", ""], ["Crandall", "David", ""]]}, {"id": "1601.00248", "submitter": "Kushal Arora", "authors": "Kushal Arora, Anand Rangarajan", "title": "Contrastive Entropy: A new evaluation metric for unnormalized language\n  models", "comments": "submitted to INTERSPEECH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perplexity (per word) is the most widely used metric for evaluating language\nmodels. Despite this, there has been no dearth of criticism for this metric.\nMost of these criticisms center around lack of correlation with extrinsic\nmetrics like word error rate (WER), dependence upon shared vocabulary for model\ncomparison and unsuitability for unnormalized language model evaluation. In\nthis paper, we address the last problem and propose a new discriminative\nentropy based intrinsic metric that works for both traditional word level\nmodels and unnormalized language models like sentence level models. We also\npropose a discriminatively trained sentence level interpretation of recurrent\nneural network based language model (RNN) as an example of unnormalized\nsentence level model. We demonstrate that for word level models, contrastive\nentropy shows a strong correlation with perplexity. We also observe that when\ntrained at lower distortion levels, sentence level RNN considerably outperforms\ntraditional RNNs on this new metric.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 05:47:42 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 13:53:47 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Arora", "Kushal", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1601.00372", "submitter": "Jiwei Li", "authors": "Jiwei Li and Dan Jurafsky", "title": "Mutual Information and Diverse Decoding Improve Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence neural translation models learn semantic and syntactic\nrelations between sentence pairs by optimizing the likelihood of the target\ngiven the source, i.e., $p(y|x)$, an objective that ignores other potentially\nuseful sources of information. We introduce an alternative objective function\nfor neural MT that maximizes the mutual information between the source and\ntarget sentences, modeling the bi-directional dependency of sources and\ntargets. We implement the model with a simple re-ranking method, and also\nintroduce a decoding algorithm that increases diversity in the N-best list\nproduced by the first pass. Applied to the WMT German/English and\nFrench/English tasks, the proposed models offers a consistent performance boost\non both standard LSTM and attention-based neural MT architectures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 03:04:05 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 21:15:30 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Li", "Jiwei", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1601.00620", "submitter": "Lidong Bing", "authors": "Lidong Bing, Mingyang Ling, Richard C. Wang, William W. Cohen", "title": "Distant IE by Bootstrapping Using Lists and Document Structure", "comments": "7 pages, to appear at AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant labeling for information extraction (IE) suffers from noisy training\ndata. We describe a way of reducing the noise associated with distant IE by\nidentifying coupling constraints between potential instance labels. As one\nexample of coupling, items in a list are likely to have the same label. A\nsecond example of coupling comes from analysis of document structure: in some\ncorpora, sections can be identified such that items in the same section are\nlikely to have the same label. Such sections do not exist in all corpora, but\nwe show that augmenting a large corpus with coupling constraints from even a\nsmall, well-structured corpus can improve performance substantially, doubling\nF1 on one task.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 19:46:00 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Bing", "Lidong", ""], ["Ling", "Mingyang", ""], ["Wang", "Richard C.", ""], ["Cohen", "William W.", ""]]}, {"id": "1601.00710", "submitter": "Barret Zoph", "authors": "Barret Zoph and Kevin Knight", "title": "Multi-Source Neural Translation", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a multi-source machine translation model and train it to maximize\nthe probability of a target English string given French and German sources.\nUsing the neural encoder-decoder framework, we explore several combination\nmethods and report up to +4.8 Bleu increases on top of a very strong\nattention-based neural translation model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 00:49:22 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Zoph", "Barret", ""], ["Knight", "Kevin", ""]]}, {"id": "1601.00770", "submitter": "Makoto Miwa", "authors": "Makoto Miwa and Mohit Bansal", "title": "End-to-End Relation Extraction using LSTMs on Sequences and Tree\n  Structures", "comments": "Accepted for publication at the Association for Computational\n  Linguistics (ACL), 2016. 13 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end neural model to extract entities and relations\nbetween them. Our recurrent neural network based model captures both word\nsequence and dependency tree substructure information by stacking bidirectional\ntree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows\nour model to jointly represent both entities and relations with shared\nparameters in a single model. We further encourage detection of entities during\ntraining and use of entity information in relation extraction via entity\npretraining and scheduled sampling. Our model improves over the\nstate-of-the-art feature-based model on end-to-end relation extraction,\nachieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and\nACE2004, respectively. We also show that our LSTM-RNN based model compares\nfavorably to the state-of-the-art CNN based model (in F1-score) on nominal\nrelation classification (SemEval-2010 Task 8). Finally, we present an extensive\nablation analysis of several model components.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 08:53:05 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 02:23:01 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 01:08:08 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Miwa", "Makoto", ""], ["Bansal", "Mohit", ""]]}, {"id": "1601.00816", "submitter": "Pierre-Yves Oudeyer", "authors": "Pierre-Yves Oudeyer (Flowers)", "title": "Open challenges in understanding development and evolution of speech\n  forms: The roles of embodied self-organization, motivation and active\n  exploration", "comments": null, "journal-ref": "Journal of Phonetics, Elsevier, 2015, 53, pp.5", "doi": "10.1016/j.wocn.2015.09.001", "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses open scientific challenges for understanding\ndevelopment and evolution of speech forms, as a commentary to Moulin-Frier et\nal. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models\nof the origins of speech forms, with a focus on their assumptions , we study\nthe fundamental question of how speech can be formed out of non--speech, at\nboth developmental and evolutionary scales. In particular, we emphasize the\nimportance of embodied self-organization , as well as the role of mechanisms of\nmotivation and active curiosity-driven exploration in speech formation. Finally\n, we discuss an evolutionary-developmental perspective of the origins of\nspeech.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 12:50:14 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Oudeyer", "Pierre-Yves", "", "Flowers"]]}, {"id": "1601.00893", "submitter": "Oren Melamud", "authors": "Oren Melamud, David McClosky, Siddharth Patwardhan, Mohit Bansal", "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings", "comments": "Accepted to NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first extensive evaluation of how using different types of\ncontext to learn skip-gram word embeddings affects performance on a wide range\nof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic\ntasks tend to exhibit a clear preference to particular types of contexts and\nhigher dimensionality, more careful tuning is required for finding the optimal\nsettings for most of the extrinsic tasks that we considered. Furthermore, for\nthese extrinsic tasks, we find that once the benefit from increasing the\nembedding dimensionality is mostly exhausted, simple concatenation of word\nembeddings, learned with different context types, can yield further performance\ngains. As an additional contribution, we propose a new variant of the skip-gram\nmodel that learns word embeddings from weighted contexts of substitute words.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 16:28:42 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 15:32:54 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Melamud", "Oren", ""], ["McClosky", "David", ""], ["Patwardhan", "Siddharth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1601.00901", "submitter": "Janez Starc", "authors": "Janez Starc and Dunja Mladeni\\'c", "title": "Joint learning of ontology and semantic parser from text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing methods are used for capturing and representing semantic\nmeaning of text. Meaning representation capturing all the concepts in the text\nmay not always be available or may not be sufficiently complete. Ontologies\nprovide a structured and reasoning-capable way to model the content of a\ncollection of texts. In this work, we present a novel approach to joint\nlearning of ontology and semantic parser from text. The method is based on\nsemi-automatic induction of a context-free grammar from semantically annotated\ntext. The grammar parses the text into semantic trees. Both, the grammar and\nthe semantic trees are used to learn the ontology on several levels -- classes,\ninstances, taxonomic and non-taxonomic relations. The approach was evaluated on\nthe first sentences of Wikipedia pages describing people.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 16:56:28 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Starc", "Janez", ""], ["Mladeni\u0107", "Dunja", ""]]}, {"id": "1601.01073", "submitter": "Orhan Firat", "authors": "Orhan Firat, Kyunghyun Cho and Yoshua Bengio", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared\n  Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose multi-way, multilingual neural machine translation. The proposed\napproach enables a single neural translation model to translate between\nmultiple languages, with a number of parameters that grows only linearly with\nthe number of languages. This is made possible by having a single attention\nmechanism that is shared across all language pairs. We train the proposed\nmulti-way, multilingual model on ten language pairs from WMT'15 simultaneously\nand observe clear performance improvements over models trained on only one\nlanguage pair. In particular, we observe that the proposed model significantly\nimproves the translation quality of low-resource language pairs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 04:00:50 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Firat", "Orhan", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1601.01085", "submitter": "Trevor Cohn", "authors": "Trevor Cohn and Cong Duy Vu Hoang and Ekaterina Vymolova and Kaisheng\n  Yao and Chris Dyer and Gholamreza Haffari", "title": "Incorporating Structural Alignment Biases into an Attentional Neural\n  Translation Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural encoder-decoder models of machine translation have achieved impressive\nresults, rivalling traditional translation models. However their modelling\nformulation is overly simplistic, and omits several key inductive biases built\ninto traditional models. In this paper we extend the attentional neural\ntranslation model to include structural biases from word based alignment\nmodels, including positional bias, Markov conditioning, fertility and agreement\nover translation directions. We show improvements over a baseline attentional\nmodel and standard phrase-based model over several language pairs, evaluating\non difficult languages in a low resource setting.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 06:03:17 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Cohn", "Trevor", ""], ["Hoang", "Cong Duy Vu", ""], ["Vymolova", "Ekaterina", ""], ["Yao", "Kaisheng", ""], ["Dyer", "Chris", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "1601.01195", "submitter": "Kamal Sarkar", "authors": "Kamal Sarkar", "title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON\n  2015", "comments": "NLP Tool Contest on \"POS Tagging For Code-mixed Indian Social Media\n  Text\" held in conjunction with International Conference on Natural Language\n  Processing(ICON 2015). arXiv admin note: text overlap with arXiv:1512.03950,\n  arXiv:1405.7397", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the experiments carried out by us at Jadavpur University\nas part of the participation in ICON 2015 task: POS Tagging for Code-mixed\nIndian Social Media Text. The tool that we have developed for the task is based\non Trigram Hidden Markov Model that utilizes information from dictionary as\nwell as some other word level features to enhance the observation probabilities\nof the known tokens as well as unknown tokens. We submitted runs for\nBengali-English, Hindi-English and Tamil-English Language pairs. Our system has\nbeen trained and tested on the datasets released for ICON 2015 shared task: POS\nTagging For Code-mixed Indian Social Media Text. In constrained mode, our\nsystem obtains average overall accuracy (averaged over all three language\npairs) of 75.60% which is very close to other participating two systems (76.79%\nfor IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. In\nunconstrained mode, our system obtains average overall accuracy of 70.65% which\nis also close to the system (72.85% for AMRITA_CEN) which obtains the highest\naverage overall accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 14:40:38 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Sarkar", "Kamal", ""]]}, {"id": "1601.01272", "submitter": "Ke Tran", "authors": "Ke Tran, Arianna Bisazza and Christof Monz", "title": "Recurrent Memory Networks for Language Modeling", "comments": "8 pages, 6 figures. Accepted at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) have obtained excellent result in many\nnatural language processing (NLP) tasks. However, understanding and\ninterpreting the source of this success remains a challenge. In this paper, we\npropose Recurrent Memory Network (RMN), a novel RNN architecture, that not only\namplifies the power of RNN but also facilitates our understanding of its\ninternal functioning and allows us to discover underlying patterns in data. We\ndemonstrate the power of RMN on language modeling and sentence completion\ntasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)\nnetwork on three large German, Italian, and English dataset. Additionally we\nperform in-depth analysis of various linguistic dimensions that RMN captures.\nOn Sentence Completion Challenge, for which it is essential to capture sentence\ncoherence, our RMN obtains 69.2% accuracy, surpassing the previous\nstate-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 18:44:07 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 11:13:11 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Tran", "Ke", ""], ["Bisazza", "Arianna", ""], ["Monz", "Christof", ""]]}, {"id": "1601.01280", "submitter": "Li Dong", "authors": "Li Dong, Mirella Lapata", "title": "Language to Logical Form with Neural Attention", "comments": "Accepted by ACL-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing aims at mapping natural language to machine interpretable\nmeaning representations. Traditional approaches rely on high-quality lexicons,\nmanually-built templates, and linguistic features which are either domain- or\nrepresentation-specific. In this paper we present a general method based on an\nattention-enhanced encoder-decoder model. We encode input utterances into\nvector representations, and generate their logical forms by conditioning the\noutput sequences or trees on the encoding vectors. Experimental results on four\ndatasets show that our approach performs competitively without using\nhand-engineered features and is easy to adapt across domains and meaning\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 19:13:12 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 21:06:55 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Dong", "Li", ""], ["Lapata", "Mirella", ""]]}, {"id": "1601.01343", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji", "title": "Joint Learning of the Embedding of Words and Entities for Named Entity\n  Disambiguation", "comments": "Accepted at CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Disambiguation (NED) refers to the task of resolving multiple\nnamed entity mentions in a document to their correct references in a knowledge\nbase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method\nspecifically designed for NED. The proposed method jointly maps words and\nentities into the same continuous vector space. We extend the skip-gram model\nby using two models. The KB graph model learns the relatedness of entities\nusing the link structure of the KB, whereas the anchor context model aims to\nalign vectors such that similar words and entities occur close to one another\nin the vector space by leveraging KB anchors and their context words. By\ncombining contexts based on the proposed embedding with standard NED features,\nwe achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset\nand 85.2% on the TAC 2010 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 22:19:20 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 07:31:47 GMT"}, {"version": "v3", "created": "Sun, 1 May 2016 06:39:19 GMT"}, {"version": "v4", "created": "Fri, 10 Jun 2016 01:51:26 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Yamada", "Ikuya", ""], ["Shindo", "Hiroyuki", ""], ["Takeda", "Hideaki", ""], ["Takefuji", "Yoshiyasu", ""]]}, {"id": "1601.01356", "submitter": "Makbule Gulcin Ozsoy", "authors": "Makbule Gulcin Ozsoy", "title": "From Word Embeddings to Item Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network platforms can use the data produced by their users to serve\nthem better. One of the services these platforms provide is recommendation\nservice. Recommendation systems can predict the future preferences of users\nusing their past preferences. In the recommendation systems literature there\nare various techniques, such as neighborhood based methods, machine-learning\nbased methods and matrix-factorization based methods. In this work, a set of\nwell known methods from natural language processing domain, namely Word2Vec, is\napplied to recommendation systems domain. Unlike previous works that use\nWord2Vec for recommendation, this work uses non-textual features, the\ncheck-ins, and it recommends venues to visit/check-in to the target users. For\nthe experiments, a Foursquare check-in dataset is used. The results show that\nuse of continuous vector space representations of items modeled by techniques\nof Word2Vec is promising for making recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 00:09:37 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 16:09:10 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 08:07:36 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Ozsoy", "Makbule Gulcin", ""]]}, {"id": "1601.01530", "submitter": "Gakuto Kurata", "authors": "Gakuto Kurata, Bing Xiang, Bowen Zhou, Mo Yu", "title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic\n  Slot Filling", "comments": "Accepted in EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) and one of its specific architectures, Long\nShort-Term Memory (LSTM), have been widely used for sequence labeling. In this\npaper, we first enhance LSTM-based sequence labeling to explicitly model label\ndependencies. Then we propose another enhancement to incorporate the global\ninformation spanning over the whole input sequence. The latter proposed method,\nencoder-labeler LSTM, first encodes the whole input sequence into a fixed\nlength vector with the encoder LSTM, and then uses this encoded vector as the\ninitial state of another LSTM for sequence labeling. Combining these methods,\nwe can predict the label sequence with considering label dependencies and\ninformation of whole input sequence. In the experiments of a slot filling task,\nwhich is an essential component of natural language understanding, with using\nthe standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 13:32:31 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 02:06:45 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 00:41:29 GMT"}, {"version": "v4", "created": "Wed, 31 Aug 2016 00:30:10 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Kurata", "Gakuto", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""], ["Yu", "Mo", ""]]}, {"id": "1601.01705", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein", "title": "Learning to Compose Neural Networks for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 21:21:59 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 18:20:37 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 01:44:25 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 23:25:51 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Klein", "Dan", ""]]}, {"id": "1601.01887", "submitter": "Rustam Tagiew", "authors": "Rustam Tagiew", "title": "Research Project: Text Engineering Tool for Ontological Scientometry", "comments": "5 pages, 2 figure", "journal-ref": null, "doi": "10.13140/RG.2.1.1619.1847", "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of scientific papers grows exponentially in many disciplines. The\nshare of online available papers grows as well. At the same time, the period of\ntime for a paper to loose at chance to be cited anymore shortens. The decay of\nthe citing rate shows similarity to ultradiffusional processes as for other\nonline contents in social networks. The distribution of papers per author shows\nsimilarity to the distribution of posts per user in social networks. The rate\nof uncited papers for online available papers grows while some papers 'go\nviral' in terms of being cited. Summarized, the practice of scientific\npublishing moves towards the domain of social networks. The goal of this\nproject is to create a text engineering tool, which can semi-automatically\ncategorize a paper according to its type of contribution and extract\nrelationships between them into an ontological database. Semi-automatic\ncategorization means that the mistakes made by automatic pre-categorization and\nrelationship-extraction will be corrected through a wikipedia-like front-end by\nvolunteers from general public. This tool should not only help researchers and\nthe general public to find relevant supplementary material and peers faster,\nbut also provide more information for research funding agencies.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:29:44 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Tagiew", "Rustam", ""]]}, {"id": "1601.02166", "submitter": "Anders S{\\o}gaard Anders S{\\o}gaard", "authors": "Anders S{\\o}gaard", "title": "Empirical Gaussian priors for cross-lingual transfer learning", "comments": "Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence model learning algorithms typically maximize log-likelihood minus\nthe norm of the model (or minimize Hamming loss + norm). In cross-lingual\npart-of-speech (POS) tagging, our target language training data consists of\nsequences of sentences with word-by-word labels projected from translations in\n$k$ languages for which we have labeled data, via word alignments. Our training\ndata is therefore very noisy, and if Rademacher complexity is high, learning\nalgorithms are prone to overfit. Norm-based regularization assumes a constant\nwidth and zero mean prior. We instead propose to use the $k$ source language\nmodels to estimate the parameters of a Gaussian prior for learning new POS\ntaggers. This leads to significantly better performance in multi-source\ntransfer set-ups. We also present a drop-out version that injects (empirical)\nGaussian noise during online learning. Finally, we note that using empirical\nGaussian priors leads to much lower Rademacher complexity, and is superior to\noptimally weighted model interpolation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 23:34:05 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["S\u00f8gaard", "Anders", ""]]}, {"id": "1601.02403", "submitter": "Ivan Habernal", "authors": "Ivan Habernal and Iryna Gurevych", "title": "Argumentation Mining in User-Generated Web Discourse", "comments": "Cite as: Habernal, I. & Gurevych, I. (2017). Argumentation Mining in\n  User-Generated Web Discourse. Computational Linguistics 43(1), pp. 125-179", "journal-ref": "Computational Linguistics 43(1), 2017, pp. 125-179", "doi": "10.1162/COLI_a_00276", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of argumentation mining, an evolving research field in computational\nlinguistics, is to design methods capable of analyzing people's argumentation.\nIn this article, we go beyond the state of the art in several ways. (i) We deal\nwith actual Web data and take up the challenges given by the variety of\nregisters, multiple domains, and unrestricted noisy user-generated Web\ndiscourse. (ii) We bridge the gap between normative argumentation theories and\nargumentation phenomena encountered in actual data by adapting an argumentation\nmodel tested in an extensive annotation study. (iii) We create a new gold\nstandard corpus (90k tokens in 340 documents) and experiment with several\nmachine learning methods to identify argument components. We offer the data,\nsource codes, and annotation guidelines to the community under free licenses.\nOur findings show that argumentation mining in user-generated Web discourse is\na feasible but challenging task.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 11:28:49 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 12:25:09 GMT"}, {"version": "v3", "created": "Fri, 29 Apr 2016 10:52:21 GMT"}, {"version": "v4", "created": "Thu, 16 Jun 2016 14:24:41 GMT"}, {"version": "v5", "created": "Mon, 27 Mar 2017 10:46:23 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Habernal", "Ivan", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1601.02431", "submitter": "Walter Daelemans", "authors": "Claudia Peersman, Walter Daelemans, Reinhild Vandekerckhove, Bram\n  Vandekerckhove, Leona Van Vaerenbergh", "title": "The Effects of Age, Gender and Region on Non-standard Linguistic\n  Variation in Online Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a corpus-based analysis of the effects of age, gender and region\nof origin on the production of both \"netspeak\" or \"chatspeak\" features and\nregional speech features in Flemish Dutch posts that were collected from a\nBelgian online social network platform. The present study shows that combining\nquantitative and qualitative approaches is essential for understanding\nnon-standard linguistic variation in a CMC corpus. It also presents a\nmethodology that enables the systematic study of this variation by including\nall non-standard words in the corpus. The analyses resulted in a convincing\nillustration of the Adolescent Peak Principle. In addition, our approach\nrevealed an intriguing correlation between the use of regional speech features\nand chatspeak features.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 13:02:59 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Peersman", "Claudia", ""], ["Daelemans", "Walter", ""], ["Vandekerckhove", "Reinhild", ""], ["Vandekerckhove", "Bram", ""], ["Van Vaerenbergh", "Leona", ""]]}, {"id": "1601.02502", "submitter": "Jean-Marc Marty", "authors": "Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, Amine Benhalloum", "title": "Trans-gram, Fast Cross-lingual Word-embeddings", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Trans-gram, a simple and computationally-efficient method to\nsimultaneously learn and align wordembeddings for a variety of languages, using\nonly monolingual data and a smaller set of sentence-aligned data. We use our\nnew method to compute aligned wordembeddings for twenty-one languages using\nEnglish as a pivot language. We show that some linguistic features are aligned\nacross languages for which we do not have aligned data, even though those\nproperties do not exist in the pivot language. We also achieve state of the art\nresults on standard cross-lingual text classification and word translation\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 16:12:32 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Coulmance", "Jocelyn", ""], ["Marty", "Jean-Marc", ""], ["Wenzek", "Guillaume", ""], ["Benhalloum", "Amine", ""]]}, {"id": "1601.02539", "submitter": "Zhizheng Wu", "authors": "Zhizheng Wu, Simon King", "title": "Investigating gated recurrent neural networks for speech synthesis", "comments": "Accepted by ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, recurrent neural networks (RNNs) as powerful sequence models have\nre-emerged as a potential acoustic model for statistical parametric speech\nsynthesis (SPSS). The long short-term memory (LSTM) architecture is\nparticularly attractive because it addresses the vanishing gradient problem in\nstandard RNNs, making them easier to train. Although recent studies have\ndemonstrated that LSTMs can achieve significantly better performance on SPSS\nthan deep feed-forward neural networks, little is known about why. Here we\nattempt to answer two questions: a) why do LSTMs work well as a sequence model\nfor SPSS; b) which component (e.g., input gate, output gate, forget gate) is\nmost important. We present a visual analysis alongside a series of experiments,\nresulting in a proposal for a simplified architecture. The simplified\narchitecture has significantly fewer parameters than an LSTM, thus reducing\ngeneration complexity considerably without degrading quality.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 17:54:53 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Wu", "Zhizheng", ""], ["King", "Simon", ""]]}, {"id": "1601.02543", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Vinod Kumar Pandey, Sunil Kumar Kopparapu", "title": "Evaluating the Performance of a Speech Recognition based System", "comments": "7 pages, 2 figure, ACC 2011", "journal-ref": "ACC (3) 2011: 230-238", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech based solutions have taken center stage with growth in the services\nindustry where there is a need to cater to a very large number of people from\nall strata of the society. While natural language speech interfaces are the\ntalk in the research community, yet in practice, menu based speech solutions\nthrive. Typically in a menu based speech solution the user is required to\nrespond by speaking from a closed set of words when prompted by the system. A\nsequence of human speech response to the IVR prompts results in the completion\nof a transaction. A transaction is deemed successful if the speech solution can\ncorrectly recognize all the spoken utterances of the user whenever prompted by\nthe system. The usual mechanism to evaluate the performance of a speech\nsolution is to do an extensive test of the system by putting it to actual\npeople use and then evaluating the performance by analyzing the logs for\nsuccessful transactions. This kind of evaluation could lead to dissatisfied\ntest users especially if the performance of the system were to result in a poor\ntransaction completion rate. To negate this the Wizard of Oz approach is\nadopted during evaluation of a speech system. Overall this kind of evaluations\nis an expensive proposition both in terms of time and cost. In this paper, we\npropose a method to evaluate the performance of a speech solution without\nactually putting it to people use. We first describe the methodology and then\nshow experimentally that this can be used to identify the performance\nbottlenecks of the speech solution even before the system is actually used thus\nsaving evaluation time and expenses.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 18:01:56 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Pandey", "Vinod Kumar", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1601.02553", "submitter": "Suyoun Kim", "authors": "Suyoun Kim, Bhiksha Raj, Ian Lane", "title": "Environmental Noise Embeddings for Robust Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep neural network architecture for speech recognition\nthat explicitly employs knowledge of the background environmental noise within\na deep neural network acoustic model. A deep neural network is used to predict\nthe acoustic environment in which the system in being used. The discriminative\nembedding generated at the bottleneck layer of this network is then\nconcatenated with traditional acoustic features as input to a deep neural\nnetwork acoustic model. Through a series of experiments on Resource Management,\nCHiME-3 task, and Aurora4, we show that the proposed approach significantly\nimproves speech recognition accuracy in noisy and highly reverberant\nenvironments, outperforming multi-condition training, noise-aware training,\ni-vector framework, and multi-task learning on both in-domain noise and unseen\nnoise.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 18:38:18 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 22:45:08 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Kim", "Suyoun", ""], ["Raj", "Bhiksha", ""], ["Lane", "Ian", ""]]}, {"id": "1601.02789", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Danijel Kor\\v{z}inek", "title": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking", "comments": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking. arXiv admin note: text overlap with\n  arXiv:1509.09088", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-speaking is a mechanism for obtaining high quality subtitles for use in\nlive broadcast and other public events. Because it relies on humans performing\nthe actual re-speaking, the task of estimating the quality of the results is\nnon-trivial. Most organisations rely on humans to perform the actual quality\nassessment, but purely automatic methods have been developed for other similar\nproblems, like Machine Translation. This paper will try to compare several of\nthese methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will\nthen be matched to the human-derived NER metric, commonly used in re-speaking.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:06:52 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Kor\u017einek", "Danijel", ""]]}, {"id": "1601.02828", "submitter": "Pawel Swietojanski", "authors": "Pawel Swietojanski and Jinyu Li and Steve Renals", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model\n  Adaptation", "comments": "14 pages, 9 Tables, 11 Figues in IEEE/ACM Transactions on Audio,\n  Speech and Language Processing, Vol. 24, Num. 8, 2016", "journal-ref": null, "doi": "10.1109/TASLP.2016.2560534", "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a broad study on the adaptation of neural network acoustic\nmodels by means of learning hidden unit contributions (LHUC) -- a method that\nlinearly re-combines hidden units in a speaker- or environment-dependent manner\nusing small amounts of unsupervised adaptation data. We also extend LHUC to a\nspeaker adaptive training (SAT) framework that leads to a more adaptable DNN\nacoustic model, working both in a speaker-dependent and a speaker-independent\nmanner, without the requirements to maintain auxiliary speaker-dependent\nfeature extractors or to introduce significant speaker-dependent changes to the\nDNN structure. Through a series of experiments on four different speech\nrecognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4)\ncomprising 270 test speakers, we show that LHUC in both its test-only and SAT\nvariants results in consistent word error rate reductions ranging from 5% to\n23% relative depending on the task and the degree of mismatch between training\nand test data. In addition, we have investigated the effect of the amount of\nadaptation data per speaker, the quality of unsupervised adaptation targets,\nthe complementarity to other adaptation techniques, one-shot adaptation, and an\nextension to adapting DNNs trained in a sequence discriminative manner.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 12:33:56 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 17:47:07 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Swietojanski", "Pawel", ""], ["Li", "Jinyu", ""], ["Renals", "Steve", ""]]}, {"id": "1601.03210", "submitter": "Ramon Ferrer i Cancho", "authors": "Carlos G\\'omez-Rodr\\'iguez and Ramon Ferrer-i-Cancho", "title": "The scarcity of crossing dependencies: a direct outcome of a specific\n  constraint?", "comments": "minor corrections; in press in Physical Review E", "journal-ref": "Phys. Rev. E 96, 062304 (2017)", "doi": "10.1103/PhysRevE.96.062304", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of a sentence can be represented as a network where vertices\nare words and edges indicate syntactic dependencies. Interestingly, crossing\nsyntactic dependencies have been observed to be infrequent in human languages.\nThis leads to the question of whether the scarcity of crossings in languages\narises from an independent and specific constraint on crossings. We provide\nstatistical evidence suggesting that this is not the case, as the proportion of\ndependency crossings of sentences from a wide range of languages can be\naccurately estimated by a simple predictor based on a null hypothesis on the\nlocal probability that two dependencies cross given their lengths. The relative\nerror of this predictor never exceeds 5% on average, whereas the error of a\nbaseline predictor assuming a random ordering of the words of a sentence is at\nleast 6 times greater. Our results suggest that the low frequency of crossings\nin natural languages is neither originated by hidden knowledge of language nor\nby the undesirability of crossings per se, but as a mere side effect of the\nprinciple of dependency length minimization.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 11:48:25 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 14:49:07 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 16:09:37 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["G\u00f3mez-Rodr\u00edguez", "Carlos", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1601.03288", "submitter": "Walter Daelemans", "authors": "Vincent Van Asch, Walter Daelemans", "title": "Predicting the Effectiveness of Self-Training: Application to Sentiment\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to investigate the connection between the\nperformance gain that can be obtained by selftraining and the similarity\nbetween the corpora used in this approach. Self-training is a semi-supervised\ntechnique designed to increase the performance of machine learning algorithms\nby automatically classifying instances of a task and adding these as additional\ntraining material to the same classifier. In the context of language processing\ntasks, this training material is mostly an (annotated) corpus. Unfortunately\nself-training does not always lead to a performance increase and whether it\nwill is largely unpredictable. We show that the similarity between corpora can\nbe used to identify those setups for which self-training can be beneficial. We\nconsider this research as a step in the process of developing a classifier that\nis able to adapt itself to each new test corpus that it is presented with.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 15:55:36 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Van Asch", "Vincent", ""], ["Daelemans", "Walter", ""]]}, {"id": "1601.03313", "submitter": "Valentin Kassarnig", "authors": "Valentin Kassarnig", "title": "Political Speech Generation", "comments": "15 pages, class project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we present a system that can generate political speeches for a\ndesired political party. Furthermore, the system allows to specify whether a\nspeech should hold a supportive or opposing opinion. The system relies on a\ncombination of several state-of-the-art NLP methods which are discussed in this\nreport. These include n-grams, Justeson & Katz POS tag filter, recurrent neural\nnetworks, and latent Dirichlet allocation. Sequences of words are generated\nbased on probabilities obtained from two underlying models: A language model\ntakes care of the grammatical correctness while a topic model aims for textual\nconsistency. Both models were trained on the Convote dataset which contains\ntranscripts from US congressional floor debates. Furthermore, we present a\nmanual and an automated approach to evaluate the quality of generated speeches.\nIn an experimental evaluation generated speeches have shown very high quality\nin terms of grammatical correctness and sentence transitions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 16:58:05 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 15:47:13 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Kassarnig", "Valentin", ""]]}, {"id": "1601.03317", "submitter": "Shi Feng", "authors": "Shi Feng, Shujie Liu, Mu Li, Ming Zhou", "title": "Implicit Distortion and Fertility Models for Attention-based\n  Encoder-Decoder NMT Model", "comments": "11 pages, updated details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation has shown very promising results lately. Most NMT\nmodels follow the encoder-decoder framework. To make encoder-decoder models\nmore flexible, attention mechanism was introduced to machine translation and\nalso other tasks like speech recognition and image captioning. We observe that\nthe quality of translation by attention-based encoder-decoder can be\nsignificantly damaged when the alignment is incorrect. We attribute these\nproblems to the lack of distortion and fertility models. Aiming to resolve\nthese problems, we propose new variations of attention-based encoder-decoder\nand compare them with other models on machine translation. Our proposed method\nachieved an improvement of 2 BLEU points over the original attention-based\nencoder-decoder.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 17:14:01 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2016 04:58:59 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2016 02:08:02 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Feng", "Shi", ""], ["Liu", "Shujie", ""], ["Li", "Mu", ""], ["Zhou", "Ming", ""]]}, {"id": "1601.03348", "submitter": "Kayhan Moharreri", "authors": "Kayhan Moharreri, Minsu Ha, Ross H Nehm", "title": "EvoGrader: an online formative assessment tool for automatically\n  evaluating written evolutionary explanations", "comments": null, "journal-ref": "Evolution: Education and Outreach, vol. 7, pp. 1-14, 2014", "doi": "10.1186/s12052-014-0015-2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EvoGrader is a free, online, on-demand formative assessment service designed\nfor use in undergraduate biology classrooms. EvoGrader's web portal is powered\nby Amazon's Elastic Cloud and run with LightSIDE Lab's open-source\nmachine-learning tools. The EvoGrader web portal allows biology instructors to\nupload a response file (.csv) containing unlimited numbers of evolutionary\nexplanations written in response to 86 different ACORNS (Assessing COntextual\nReasoning about Natural Selection) instrument items. The system automatically\nanalyzes the responses and provides detailed information about the scientific\nand naive concepts contained within each student's response, as well as overall\nstudent (and sample) reasoning model types. Graphs and visual models provided\nby EvoGrader summarize class-level responses; downloadable files of raw scores\n(in .csv format) are also provided for more detailed analyses. Although the\ncomputational machinery that EvoGrader employs is complex, using the system is\neasy. Users only need to know how to use spreadsheets to organize student\nresponses, upload files to the web, and use a web browser. A series of\nexperiments using new samples of 2,200 written evolutionary explanations\ndemonstrate that EvoGrader scores are comparable to those of trained human\nraters, although EvoGrader scoring takes 99% less time and is free. EvoGrader\nwill be of interest to biology instructors teaching large classes who seek to\nemphasize scientific practices such as generating scientific explanations, and\nto teach crosscutting ideas such as evolution and natural selection. The\nsoftware architecture of EvoGrader is described as it may serve as a template\nfor developing machine-learning portals for other core concepts within biology\nand across other disciplines.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 18:59:06 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Moharreri", "Kayhan", ""], ["Ha", "Minsu", ""], ["Nehm", "Ross H", ""]]}, {"id": "1601.03478", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri", "title": "Deep Learning Applied to Image and Text Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to describe images with natural language sentences is the\nhallmark for image and language understanding. Such a system has wide ranging\napplications such as annotating images and using natural sentences to search\nfor images.In this project we focus on the task of bidirectional image\nretrieval: such asystem is capable of retrieving an image based on a sentence\n(image search) andretrieve sentence based on an image query (image annotation).\nWe present asystem based on a global ranking objective function which uses a\ncombinationof convolutional neural networks (CNN) and multi layer perceptrons\n(MLP).It takes a pair of image and sentence and processes them in different\nchannels,finally embedding it into a common multimodal vector space. These\nembeddingsencode abstract semantic information about the two inputs and can be\ncomparedusing traditional information retrieval approaches. For each such pair,\nthe modelreturns a score which is interpretted as a similarity metric. If this\nscore is high,the image and sentence are likely to convey similar meaning, and\nif the score is low then they are likely not to.\n  The visual input is modeled via deep convolutional neural network. On\ntheother hand we explore three models for the textual module. The first one\nisbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a\ncombination of trigram & skip-grams) with an MLP. The third is morespecialized\ndeep network specific for modeling variable length sequences (SSE).We report\ncomparable performance to recent work in the field, even though ouroverall\nmodel is simpler. We also show that the training time choice of how wecan\ngenerate our negative samples has a significant impact on performance, and can\nbe used to specialize the bi-directional system in one particular task.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:19:33 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""]]}, {"id": "1601.03650", "submitter": "Vuong Van Bui", "authors": "Vuong Van Bui, Cuong Anh Le", "title": "Smoothing parameter estimation framework for IBM word alignment models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IBM models are very important word alignment models in Machine Translation.\nFollowing the Maximum Likelihood Estimation principle to estimate their\nparameters, the models will easily overfit the training data when the data are\nsparse. While smoothing is a very popular solution in Language Model, there\nstill lacks studies on smoothing for word alignment. In this paper, we propose\na framework which generalizes the notable work Moore [2004] of applying\nadditive smoothing to word alignment models. The framework allows developers to\ncustomize the smoothing amount for each pair of word. The added amount will be\nscaled appropriately by a common factor which reflects how much the framework\ntrusts the adding strategy according to the performance on data. We also\ncarefully examine various performance criteria and propose a smoothened version\nof the error count, which generally gives the best result.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 16:30:09 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 10:48:07 GMT"}, {"version": "v3", "created": "Mon, 14 Mar 2016 04:10:51 GMT"}, {"version": "v4", "created": "Wed, 27 Apr 2016 04:01:48 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Van Bui", "Vuong", ""], ["Le", "Cuong Anh", ""]]}, {"id": "1601.03651", "submitter": "Lili Mou", "authors": "Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin", "title": "Improved Relation Classification by Deep Recurrent Neural Networks with\n  Data Augmentation", "comments": "Accepted by COLING-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, neural networks play an important role in the task of relation\nclassification. By designing different neural architectures, researchers have\nimproved the performance to a large extent in comparison with traditional\nmethods. However, existing neural networks for relation classification are\nusually of shallow architectures (e.g., one-layer convolutional neural networks\nor recurrent networks). They may fail to explore the potential representation\nspace in different abstraction levels. In this paper, we propose deep recurrent\nneural networks (DRNNs) for relation classification to tackle this challenge.\nFurther, we propose a data augmentation method by leveraging the directionality\nof relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an\nF1-score of 86.1%, outperforming previous state-of-the-art recorded results.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 16:30:41 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:11:46 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Xu", "Yan", ""], ["Jia", "Ran", ""], ["Mou", "Lili", ""], ["Li", "Ge", ""], ["Chen", "Yunchuan", ""], ["Lu", "Yangyang", ""], ["Jin", "Zhi", ""]]}, {"id": "1601.03764", "submitter": "Yingyu Liang", "authors": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski", "title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "comments": "Appear in the Transactions of the Association for Computational\n  Linguistics 2018, link:\n  https://transacl.org/ojs/index.php/tacl/article/view/1346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are ubiquitous in NLP and information retrieval, but it is\nunclear what they represent when the word is polysemous. Here it is shown that\nmultiple word senses reside in linear superposition within the word embedding\nand simple sparse coding can recover vectors that approximately capture the\nsenses. The success of our approach, which applies to several embedding\nmethods, is mathematically explained using a variant of the random walk on\ndiscourses model (Arora et al., 2016). A novel aspect of our technique is that\neach extracted word sense is accompanied by one of about 2000 \"discourse atoms\"\nthat gives a succinct description of which other words co-occur with that word\nsense. Discourse atoms can be of independent interest, and make the method\npotentially more useful. Empirical tests are used to verify and support the\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 22:02:18 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 15:22:43 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 08:08:39 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 00:06:35 GMT"}, {"version": "v5", "created": "Fri, 20 Jul 2018 15:26:24 GMT"}, {"version": "v6", "created": "Fri, 7 Dec 2018 17:30:03 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Arora", "Sanjeev", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1601.03783", "submitter": "Duygu Altinok", "authors": "Duygu Altinok", "title": "Towards Turkish ASR: Anatomy of a rule-based Turkish g2p", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the architecture and implementation of a rule-based\ngrapheme to phoneme converter for Turkish. The system accepts surface form as\ninput, outputs SAMPA mapping of the all parallel pronounciations according to\nthe morphological analysis together with stress positions. The system has been\nimplemented in Python\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 00:09:52 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Altinok", "Duygu", ""]]}, {"id": "1601.03896", "submitter": "Frank Keller", "authors": "Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erkut\n  Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara Plank", "title": "Automatic Description Generation from Images: A Survey of Models,\n  Datasets, and Evaluation Measures", "comments": "Journal of Artificial Intelligence Research 55, 409-442, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic description generation from natural images is a challenging problem\nthat has recently received a large amount of interest from the computer vision\nand natural language processing communities. In this survey, we classify the\nexisting approaches based on how they conceptualize this problem, viz., models\nthat cast description as either generation problem or as a retrieval problem\nover a visual or multimodal representational space. We provide a detailed\nreview of existing models, highlighting their advantages and disadvantages.\nMoreover, we give an overview of the benchmark image datasets and the\nevaluation measures that have been developed to assess the quality of\nmachine-generated image descriptions. Finally we extrapolate future directions\nin the area of automatic image description generation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 12:50:32 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 09:47:20 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bernardi", "Raffaella", ""], ["Cakici", "Ruket", ""], ["Elliott", "Desmond", ""], ["Erdem", "Aykut", ""], ["Erdem", "Erkut", ""], ["Ikizler-Cinbis", "Nazli", ""], ["Keller", "Frank", ""], ["Muscat", "Adrian", ""], ["Plank", "Barbara", ""]]}, {"id": "1601.03916", "submitter": "Julian Hitschler", "authors": "Julian Hitschler, Shigehiko Schamoni and Stefan Riezler", "title": "Multimodal Pivots for Image Caption Translation", "comments": "Final version, accepted at ACL 2016. New section on Human Evaluation", "journal-ref": null, "doi": "10.18653/v1/p16-1227", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to improve statistical machine translation of image\ndescriptions by multimodal pivots defined in visual space. The key idea is to\nperform image retrieval over a database of images that are captioned in the\ntarget language, and use the captions of the most similar images for\ncrosslingual reranking of translation outputs. Our approach does not depend on\nthe availability of large amounts of in-domain parallel data, but only relies\non available large datasets of monolingually captioned images, and on\nstate-of-the-art convolutional neural networks to compute image similarities.\nOur experimental evaluation shows improvements of 1 BLEU point over strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 13:42:04 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 13:47:26 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 16:52:09 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hitschler", "Julian", ""], ["Schamoni", "Shigehiko", ""], ["Riezler", "Stefan", ""]]}, {"id": "1601.04012", "submitter": "Jugal Kalita", "authors": "Jugal Kalita", "title": "Detecting and Extracting Events from Text Documents", "comments": "This is work in progress. Please email jkalita@uccs.edu with any\n  comments for improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events of various kinds are mentioned and discussed in text documents,\nwhether they are books, news articles, blogs or microblog feeds. The paper\nstarts by giving an overview of how events are treated in linguistics and\nphilosophy. We follow this discussion by surveying how events and associated\ninformation are handled in computationally. In particular, we look at how\ntextual documents can be mined to extract events and ancillary information.\nThese days, it is mostly through the application of various machine learning\ntechniques. We also discuss applications of event detection and extraction\nsystems, particularly in summarization, in the medical domain and in the\ncontext of Twitter posts. We end the paper with a discussion of challenges and\nfuture directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 17:33:39 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Kalita", "Jugal", ""]]}, {"id": "1601.04075", "submitter": "Igor Podgorny", "authors": "Igor A. Podgorny", "title": "Modification of Question Writing Style Influences Content Popularity in\n  a Social Q&A System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TurboTax AnswerXchange is a social Q&A system supporting users working on\nfederal and state tax returns. Using 2015 data, we demonstrate that content\npopularity (or number of views per AnswerXchange question) can be predicted\nwith reasonable accuracy based on attributes of the question alone. We also\nemploy probabilistic topic analysis and uplift modeling to identify question\nfeatures with the highest impact on popularity. We demonstrate that content\npopularity is driven by behavioral attributes of AnswerXchange users and\ndepends on complex interactions between search ranking algorithms,\npsycholinguistic factors and question writing style. Our findings provide a\nrationale for employing popularity predictions to guide the users into\nformulating better questions and editing the existing ones. For example,\nstarting question title with a question word or adding details to the question\nincrease number of views per question. Similar approach can be applied to\npromoting AnswerXchange content indexed by Google to drive organic traffic to\nTurboTax.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 21:01:12 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Podgorny", "Igor A.", ""]]}, {"id": "1601.04468", "submitter": "Stefan Riezler", "authors": "Artem Sokolov and Stefan Riezler and Tanguy Urvoy", "title": "Bandit Structured Prediction for Learning from Partial Feedback in\n  Statistical Machine Translation", "comments": "In Proceedings of MT Summit XV, 2015. Miami, FL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to structured prediction from bandit feedback, called\nBandit Structured Prediction, where only the value of a task loss function at a\nsingle predicted point, instead of a correct structure, is observed in\nlearning. We present an application to discriminative reranking in Statistical\nMachine Translation (SMT) where the learning algorithm only has access to a\n1-BLEU loss evaluation of a predicted translation instead of obtaining a gold\nstandard reference translation. In our experiment bandit feedback is obtained\nby evaluating BLEU on reference translations without revealing them to the\nalgorithm. This can be thought of as a simulation of interactive machine\ntranslation where an SMT system is personalized by a user who provides single\npoint feedback to predicted translations. Our experiments show that our\napproach improves translation quality and is comparable to approaches that\nemploy more informative feedback in learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 11:09:02 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Sokolov", "Artem", ""], ["Riezler", "Stefan", ""], ["Urvoy", "Tanguy", ""]]}, {"id": "1601.04580", "submitter": "Jacob Eisenstein", "authors": "Vinodh Krishnan and Jacob Eisenstein", "title": "Nonparametric Bayesian Storyline Detection from Microtexts", "comments": "Appeared at the Workshop on Computing News Storylines at the 2016\n  Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News events and social media are composed of evolving storylines, which\ncapture public attention for a limited period of time. Identifying storylines\nrequires integrating temporal and linguistic information, and prior work takes\na largely heuristic approach. We present a novel online non-parametric Bayesian\nframework for storyline detection, using the distance-dependent Chinese\nRestaurant Process (dd-CRP). To ensure efficient linear-time inference, we\nemploy a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We\nevaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouraging\nresults: despite using a weak baseline retrieval model, the dd-CRP story\nclustering method is competitive with the best entries in the 2014 TTG task.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 15:46:00 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 20:27:47 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Krishnan", "Vinodh", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1601.04811", "submitter": "Zhaopeng Tu", "authors": "Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li", "title": "Modeling Coverage for Neural Machine Translation", "comments": "Add subjective evaluation on top of ACL version: 25% of source words\n  are under-translated by NMT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism has enhanced state-of-the-art Neural Machine Translation\n(NMT) by jointly learning to align and translate. It tends to ignore past\nalignment information, however, which often leads to over-translation and\nunder-translation. To address this problem, we propose coverage-based NMT in\nthis paper. We maintain a coverage vector to keep track of the attention\nhistory. The coverage vector is fed to the attention model to help adjust\nfuture attention, which lets NMT system to consider more about untranslated\nsource words. Experiments show that the proposed approach significantly\nimproves both translation quality and alignment quality over standard\nattention-based NMT.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 07:09:38 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 01:38:45 GMT"}, {"version": "v3", "created": "Mon, 21 Mar 2016 01:45:22 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 09:51:50 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2016 05:03:33 GMT"}, {"version": "v6", "created": "Sat, 6 Aug 2016 17:13:04 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Tu", "Zhaopeng", ""], ["Lu", "Zhengdong", ""], ["Liu", "Yang", ""], ["Liu", "Xiaohua", ""], ["Li", "Hang", ""]]}, {"id": "1601.04908", "submitter": "Martha Lewis", "authors": "Desislava Bankova, Bob Coecke, Martha Lewis, Daniel Marsden", "title": "Graded Entailment for Compositional Distributional Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional model of natural language\nprovides a conceptually motivated procedure to compute the meaning of\nsentences, given grammatical structure and the meanings of its words. This\napproach has outperformed other models in mainstream empirical language\nprocessing tasks. However, until recently it has lacked the crucial feature of\nlexical entailment -- as do other distributional models of meaning.\n  In this paper we solve the problem of entailment for categorical\ncompositional distributional semantics. Taking advantage of the abstract\ncategorical framework allows us to vary our choice of model. This enables the\nintroduction of a notion of entailment, exploiting ideas from the categorical\nsemantics of partial knowledge in quantum computation.\n  The new model of language uses density matrices, on which we introduce a\nnovel robust graded order capturing the entailment strength between concepts.\nThis graded measure emerges from a general framework for approximate\nentailment, induced by any commutative monoid. Quantum logic embeds in our\ngraded order.\n  Our main theorem shows that entailment strength lifts compositionally to the\nsentence level, giving a lower bound on sentence entailment. We describe the\nessential properties of graded entailment such as continuity, and provide a\nprocedure for calculating entailment strength.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 13:13:25 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 20:10:27 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Bankova", "Desislava", ""], ["Coecke", "Bob", ""], ["Lewis", "Martha", ""], ["Marsden", "Daniel", ""]]}, {"id": "1601.05194", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang", "title": "Improved Spoken Document Summarization with Coverage Modeling Techniques", "comments": "arXiv admin note: text overlap with arXiv:1506.04365", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive summarization aims at selecting a set of indicative sentences from\na source document as a summary that can express the major theme of the\ndocument. A general consensus on extractive summarization is that both\nrelevance and coverage are critical issues to address. The existing methods\ndesigned to model coverage can be characterized by either reducing redundancy\nor increasing diversity in the summary. Maximal margin relevance (MMR) is a\nwidely-cited method since it takes both relevance and redundancy into account\nwhen generating a summary for a given document. In addition to MMR, there is\nonly a dearth of research concentrating on reducing redundancy or increasing\ndiversity for the spoken document summarization task, as far as we are aware.\nMotivated by these observations, two major contributions are presented in this\npaper. First, in contrast to MMR, which considers coverage by reducing\nredundancy, we propose two novel coverage-based methods, which directly\nincrease diversity. With the proposed methods, a set of representative\nsentences, which not only are relevant to the given document but also cover\nmost of the important sub-themes of the document, can be selected\nautomatically. Second, we make a step forward to plug in several\ndocument/sentence representation methods into the proposed framework to further\nenhance the summarization performance. A series of empirical evaluations\ndemonstrate the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 08:26:07 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Liu", "Shih-Hung", ""], ["Chen", "Berlin", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1601.05403", "submitter": "Jo\\~ao Sedoc", "authors": "Jo\\~ao Sedoc, Jean Gallier, Lyle Ungar, Dean Foster", "title": "Semantic Word Clusters Using Signed Normalized Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector space representations of words capture many aspects of word\nsimilarity, but such methods tend to make vector spaces in which antonyms (as\nwell as synonyms) are close to each other. We present a new signed spectral\nnormalized graph cut algorithm, signed clustering, that overlays existing\nthesauri upon distributionally derived vector representations of words, so that\nantonym relationships between word pairs are represented by negative weights.\nOur signed clustering algorithm produces clusters of words which simultaneously\ncapture distributional and synonym relations. We evaluate these clusters\nagainst the SimLex-999 dataset (Hill et al.,2014) of human judgments of word\npair similarities, and also show the benefit of using our clusters to predict\nthe sentiment of a given text.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 20:37:47 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Sedoc", "Jo\u00e3o", ""], ["Gallier", "Jean", ""], ["Ungar", "Lyle", ""], ["Foster", "Dean", ""]]}, {"id": "1601.05472", "submitter": "Halid Ziya Yerebakan", "authors": "Halid Ziya Yerebakan, Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa", "title": "Hierarchical Latent Word Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian non-parametric model by extending the\nusage of Hierarchical Dirichlet Allocation to extract tree structured word\nclusters from text data. The inference algorithm of the model collects words in\na cluster if they share similar distribution over documents. In our\nexperiments, we observed meaningful hierarchical structures on NIPS corpus and\nradiology reports collected from public repositories.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 23:31:58 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Yerebakan", "Halid Ziya", ""], ["Reda", "Fitsum", ""], ["Zhan", "Yiqiang", ""], ["Shinagawa", "Yoshihisa", ""]]}, {"id": "1601.05647", "submitter": "Milos Cernak", "authors": "Milos Cernak, Afsaneh Asaei, Herv\\'e Bourlard", "title": "On Structured Sparsity of Phonological Posteriors for Linguistic Parsing", "comments": null, "journal-ref": "Speech Communication, Volume 84, November 2016, Pages 36-45", "doi": "10.1016/j.specom.2016.08.004", "report-no": "Idiap-RR-07-2016", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speech signal conveys information on different time scales from short\ntime scale or segmental, associated to phonological and phonetic information to\nlong time scale or supra segmental, associated to syllabic and prosodic\ninformation. Linguistic and neurocognitive studies recognize the phonological\nclasses at segmental level as the essential and invariant representations used\nin speech temporal organization. In the context of speech processing, a deep\nneural network (DNN) is an effective computational method to infer the\nprobability of individual phonological classes from a short segment of speech\nsignal. A vector of all phonological class probabilities is referred to as\nphonological posterior. There are only very few classes comprising a short term\nspeech signal; hence, the phonological posterior is a sparse vector. Although\nthe phonological posteriors are estimated at segmental level, we claim that\nthey convey supra-segmental information. Specifically, we demonstrate that\nphonological posteriors are indicative of syllabic and prosodic events.\nBuilding on findings from converging linguistic evidence on the gestural model\nof Articulatory Phonology as well as the neural basis of speech perception, we\nhypothesize that phonological posteriors convey properties of linguistic\nclasses at multiple time scales, and this information is embedded in their\nsupport (index) of active coefficients. To verify this hypothesis, we obtain a\nbinary representation of phonological posteriors at the segmental level which\nis referred to as first-order sparsity structure; the high-order structures are\nobtained by the concatenation of first-order binary vectors. It is then\nconfirmed that the classification of supra-segmental linguistic events, the\nproblem known as linguistic parsing, can be achieved with high accuracy using\nasimple binary pattern matching of first-order or high-order structures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 14:15:41 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 14:08:02 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2016 09:23:58 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Cernak", "Milos", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herv\u00e9", ""]]}, {"id": "1601.05768", "submitter": "Daniel Christen Mr.", "authors": "Daniel Christen", "title": "Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky\nissues of syntactic ambiguity reduction and word sense disambiguation as well\nas providing innovative and original solutions for constituent generation and\nconstraints management. To provide an insight into how it operates, the\nsystem's general architecture and components, as well as its lexical, syntactic\nand semantic resources are described. After that, the paper addresses the\nmechanism that performs selective parsing through an interaction between\nsyntactic and semantic information, leading the parser to a coherent and\naccurate interpretation of the input text.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 20:19:31 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Christen", "Daniel", ""]]}, {"id": "1601.05893", "submitter": "Hans De Sterck", "authors": "Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel", "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents\n  using a Natural Language Processing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location tagging, also known as geotagging or geolocation, is the process of\nassigning geographical coordinates to input data. In this paper we present an\nalgorithm for location tagging of textual documents. Our approach makes use of\nprevious work in natural language processing by using a state-of-the-art\npart-of-speech tagger and named entity recognizer to find blocks of text which\nmay refer to locations. A knowledge base (OpenStreatMap) is then used to find a\nlist of possible locations for each block. Finally, one location is chosen for\neach block by assigning distance-based scores to each location and repeatedly\nselecting the location and block with the best score. We tested our geolocation\nalgorithm with Wikipedia articles about topics with a well-defined geographical\nlocation that are geotagged by the articles' authors, where classification\napproaches have achieved median errors as low as 11 km, with attainable\naccuracy limited by the class size. Our approach achieved a 10th percentile\nerror of 490 metres and median error of 54 kilometres on the Wikipedia dataset\nwe used. When considering the five location tags with the greatest scores, 50%\nof articles were assigned at least one tag within 8.5 kilometres of the\narticle's author-assigned true location. We also tested our approach on Twitter\nmessages that are tagged with the location from which the message was sent.\nTwitter texts are challenging because they are short and unstructured and often\ndo not contain words referring to the location they were sent from, but we\nobtain potentially useful results. We explain how we use the Spark framework\nfor data analytics to collect and process our test data. In general,\nclassification-based approaches for location tagging may be reaching their\nupper accuracy limit, but our precision-focused approach has high accuracy for\nsome texts and shows significant potential for improvement overall.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 07:09:54 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Brunsting", "Shawn", ""], ["De Sterck", "Hans", ""], ["Dolman", "Remco", ""], ["van Sprundel", "Teun", ""]]}, {"id": "1601.05936", "submitter": "Pranay Dighe", "authors": "Pranay Dighe, Gil Luyet, Afsaneh Asaei and Herve Bourlard", "title": "Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic\n  Modeling in Speech Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472767", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model the acoustic space of deep neural network (DNN)\nclass-conditional posterior probabilities as a union of low-dimensional\nsubspaces. To that end, the training posteriors are used for dictionary\nlearning and sparse coding. Sparse representation of the test posteriors using\nthis dictionary enables projection to the space of training data. Relying on\nthe fact that the intrinsic dimensions of the posterior subspaces are indeed\nvery small and the matrix of all posteriors belonging to a class has a very low\nrank, we demonstrate how low-dimensional structures enable further enhancement\nof the posteriors and rectify the spurious errors due to mismatch conditions.\nThe enhanced acoustic modeling method leads to improvements in continuous\nspeech recognition task using hybrid DNN-HMM (hidden Markov model) framework in\nboth clean and noisy conditions, where upto 15.4% relative reduction in word\nerror rate (WER) is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 10:02:47 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Dighe", "Pranay", ""], ["Luyet", "Gil", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herve", ""]]}, {"id": "1601.05991", "submitter": "Milos Cernak", "authors": "Milos Cernak, Stefan Benus, Alexandros Lazaridis", "title": "Speech vocoding for laboratory phonology", "comments": null, "journal-ref": "Computer Speech & Language, Volume 42, March 2017, Pages 100-121", "doi": "10.1016/j.csl.2016.10.001", "report-no": "Idiap-RR-07-2016", "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using phonological speech vocoding, we propose a platform for exploring\nrelations between phonology and speech processing, and in broader terms, for\nexploring relations between the abstract and physical structures of a speech\nsignal. Our goal is to make a step towards bridging phonology and speech\nprocessing and to contribute to the program of Laboratory Phonology. We show\nthree application examples for laboratory phonology: compositional phonological\nspeech modelling, a comparison of phonological systems and an experimental\nphonological parametric text-to-speech (TTS) system. The featural\nrepresentations of the following three phonological systems are considered in\nthis work: (i) Government Phonology (GP), (ii) the Sound Pattern of English\n(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded\nspeech, we conclude that the latter achieves slightly better results than the\nformer. However, GP - the most compact phonological speech representation -\nperforms comparably to the systems with a higher number of phonological\nfeatures. The parametric TTS based on phonological speech representation, and\ntrained from an unlabelled audiobook in an unsupervised manner, achieves\nintelligibility of 85% of the state-of-the-art parametric speech synthesis. We\nenvision that the presented approach paves the way for researchers in both\nfields to form meaningful hypotheses that are explicitly testable using the\nconcepts developed and exemplified in this paper. On the one hand, laboratory\nphonologists might test the applied concepts of their theoretical models, and\non the other hand, the speech processing community may utilize the concepts\ndeveloped for the theoretical phonological models for improvements of the\ncurrent state-of-the-art applications.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 13:22:10 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 12:06:21 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 08:26:38 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Cernak", "Milos", ""], ["Benus", "Stefan", ""], ["Lazaridis", "Alexandros", ""]]}, {"id": "1601.06068", "submitter": "Shashi Narayan", "authors": "Shashi Narayan, Siva Reddy and Shay B. Cohen", "title": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing", "comments": "10 pages, INLG 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the limitations of semantic parsing approaches to open-domain question\nanswering is the lexicosyntactic gap between natural language questions and\nknowledge base entries -- there are many ways to ask a question, all with the\nsame answer. In this paper we propose to bridge this gap by generating\nparaphrases of the input question with the goal that at least one of them will\nbe correctly mapped to a knowledge-base query. We introduce a novel grammar\nmodel for paraphrase generation that does not require any sentence-aligned\nparaphrase corpus. Our key idea is to leverage the flexibility and scalability\nof latent-variable probabilistic context-free grammars to sample paraphrases.\nWe do an extrinsic evaluation of our paraphrases by plugging them into a\nsemantic parser for Freebase. Our evaluation experiments on the WebQuestions\nbenchmark dataset show that the performance of the semantic parser\nsignificantly improves over strong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 16:50:22 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 12:20:52 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Narayan", "Shashi", ""], ["Reddy", "Siva", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1601.06081", "submitter": "Marco Guerini", "authors": "Marco Guerini and Carlo Strapparava", "title": "Why Do Urban Legends Go Viral?", "comments": "Preprint of paper in Journal of Information Processing and Management\n  Volume 52, Issue 1, January 2016, Pages 163-172", "journal-ref": null, "doi": "10.1016/j.ipm.2015.05.003", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban legends are a genre of modern folklore, consisting of stories about\nrare and exceptional events, just plausible enough to be believed, which tend\nto propagate inexorably across communities. In our view, while urban legends\nrepresent a form of \"sticky\" deceptive text, they are marked by a tension\nbetween the credible and incredible. They should be credible like a news\narticle and incredible like a fairy tale to go viral. In particular we will\nfocus on the idea that urban legends should mimic the details of news (who,\nwhere, when) to be credible, while they should be emotional and readable like a\nfairy tale to be catchy and memorable. Using NLP tools we will provide a\nquantitative analysis of these prototypical characteristics. We also lay out\nsome machine learning experiments showing that it is possible to recognize an\nurban legend using just these simple features.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 17:33:28 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Guerini", "Marco", ""], ["Strapparava", "Carlo", ""]]}, {"id": "1601.06303", "submitter": "Stepan Kuznetsov", "authors": "Max Kanovich, Stepan Kuznetsov, Andre Scedrov", "title": "Undecidability of the Lambek calculus with a relevant modality", "comments": "17 pages", "journal-ref": "Proc. Formal Grammar 2015/2016, LNCS vol. 9804, Springer, 2016,\n  pp. 240-256", "doi": "10.1007/978-3-662-53042-9_14", "report-no": null, "categories": "math.LO cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morrill and Valentin in the paper \"Computational coverage of TLG:\nNonlinearity\" considered an extension of the Lambek calculus enriched by a\nso-called \"exponential\" modality. This modality behaves in the \"relevant\"\nstyle, that is, it allows contraction and permutation, but not weakening.\nMorrill and Valentin stated an open problem whether this system is decidable.\nHere we show its undecidability. Our result remains valid if we consider the\nfragment where all division operations have one direction. We also show that\nthe derivability problem in a restricted case, where the modality can be\napplied only to variables (primitive types), is decidable and belongs to the NP\nclass.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 19:48:54 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 20:41:34 GMT"}, {"version": "v3", "created": "Sat, 21 May 2016 21:54:02 GMT"}, {"version": "v4", "created": "Sun, 7 Aug 2016 13:39:51 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Kanovich", "Max", ""], ["Kuznetsov", "Stepan", ""], ["Scedrov", "Andre", ""]]}, {"id": "1601.06579", "submitter": "Dong Nguyen", "authors": "Dong Nguyen, Jacob Eisenstein", "title": "A Kernel Independence Test for Geographical Language Variation", "comments": "In submission. 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the degree of spatial dependence for linguistic variables is a\nkey task for analyzing dialectal variation. However, existing approaches have\nimportant drawbacks. First, they are based on parametric models of dependence,\nwhich limits their power in cases where the underlying parametric assumptions\nare violated. Second, they are not applicable to all types of linguistic data:\nsome approaches apply only to frequencies, others to boolean indicators of\nwhether a linguistic variable is present. We present a new method for measuring\ngeographical language variation, which solves both of these problems. Our\napproach builds on Reproducing Kernel Hilbert space (RKHS) representations for\nnonparametric statistics, and takes the form of a test statistic that is\ncomputed from pairs of individual geotagged observations without aggregation\ninto predefined geographical bins. We compare this test with prior work using\nsynthetic data as well as a diverse set of real datasets: a corpus of Dutch\ntweets, a Dutch syntactic atlas, and a dataset of letters to the editor in\nNorth American newspapers. Our proposed test is shown to support robust\ninferences across a broad range of scenarios and types of data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 12:45:59 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 13:16:42 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Nguyen", "Dong", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1601.06581", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Wonyong Sung", "title": "Character-Level Incremental Speech Recognition with Recurrent Neural\n  Networks", "comments": "To appear in ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472696", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-time speech recognition applications, the latency is an important\nissue. We have developed a character-level incremental speech recognition (ISR)\nsystem that responds quickly even during the speech, where the hypotheses are\ngradually improved while the speaking proceeds. The algorithm employs a\nspeech-to-character unidirectional recurrent neural network (RNN), which is\nend-to-end trained with connectionist temporal classification (CTC), and an\nRNN-based character-level language model (LM). The output values of the\nCTC-trained RNN are character-level probabilities, which are processed by beam\nsearch decoding. The RNN LM augments the decoding by providing long-term\ndependency information. We propose tree-based online beam search with\nadditional depth-pruning, which enables the system to process infinitely long\ninput speech with low latency. This system not only responds quickly on speech\nbut also can dictate out-of-vocabulary (OOV) words according to pronunciation.\nThe proposed model achieves the word error rate (WER) of 8.90% on the Wall\nStreet Journal (WSJ) Nov'92 20K evaluation set when trained on the WSJ SI-284\ntraining set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 12:51:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 11:03:05 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1601.06732", "submitter": "Martha Lewis", "authors": "Martha Lewis, Jonathan Lawry", "title": "Concept Generation in Language Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis investigates the generation of new concepts from combinations of\nexisting concepts as a language evolves. We give a method for combining\nconcepts, and will be investigating the utility of composite concepts in\nlanguage evolution and thence the utility of concept generation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 19:23:44 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Lewis", "Martha", ""], ["Lawry", "Jonathan", ""]]}, {"id": "1601.06733", "submitter": "Jianpeng Cheng J", "authors": "Jianpeng Cheng, Li Dong, Mirella Lapata", "title": "Long Short-Term Memory-Networks for Machine Reading", "comments": "Published as a conference paper at EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 19:25:48 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 20:48:02 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2016 14:29:04 GMT"}, {"version": "v4", "created": "Thu, 17 Mar 2016 13:28:16 GMT"}, {"version": "v5", "created": "Thu, 7 Apr 2016 09:53:49 GMT"}, {"version": "v6", "created": "Wed, 1 Jun 2016 12:27:42 GMT"}, {"version": "v7", "created": "Tue, 20 Sep 2016 21:20:09 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Dong", "Li", ""], ["Lapata", "Mirella", ""]]}, {"id": "1601.06738", "submitter": "Martha Lewis", "authors": "Martha Lewis, Jonathan Lawry", "title": "A Label Semantics Approach to Linguistic Hedges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for the linguistic hedges `very' and `quite' within the\nlabel semantics framework, and combined with the prototype and conceptual\nspaces theories of concepts. The proposed model emerges naturally from the\nrepresentational framework we use and as such, has a clear semantic grounding.\nWe give generalisations of these hedge models and show that they can be\ncomposed with themselves and with other functions, going on to examine their\nbehaviour in the limit of composition.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 19:38:37 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Lewis", "Martha", ""], ["Lawry", "Jonathan", ""]]}, {"id": "1601.06755", "submitter": "Martha Lewis", "authors": "Martha Lewis, Jonathan Lawry", "title": "The Utility of Hedged Assertions in the Emergence of Shared Categorical\n  Labels", "comments": "AISB 2013, updated to include cross-reference to previous work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the emergence of shared concepts in a community of language\nusers using a multi-agent simulation. We extend results showing that negated\nassertions are of use in developing shared categories, to include assertions\nmodified by linguistic hedges. Results show that using hedged assertions\npositively affects the emergence of shared categories in two distinct ways.\nFirstly, using contraction hedges like `very' gives better convergence over\ntime. Secondly, using expansion hedges such as `quite' reduces concept overlap.\nHowever, both these improvements come at a cost of slower speed of development.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:24:50 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Lewis", "Martha", ""], ["Lawry", "Jonathan", ""]]}, {"id": "1601.06763", "submitter": "Martha Lewis", "authors": "Martha Lewis, Jonathan Lawry", "title": "Emerging Dimension Weights in a Conceptual Spaces Model of Concept\n  Combination", "comments": "AISB 2014, updated to include references to previous work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generation of new concepts from combinations of properties\nas an artificial language develops. To do so, we have developed a new framework\nfor conjunctive concept combination. This framework gives a semantic grounding\nto the weighted sum approach to concept combination seen in the literature. We\nimplement the framework in a multi-agent simulation of language evolution and\nshow that shared combination weights emerge. The expected value and the\nvariance of these weights across agents may be predicted from the distribution\nof elements in the conceptual space, as determined by the underlying\nenvironment, together with the rate at which agents adopt others' concepts.\nWhen this rate is smaller, the agents are able to converge to weights with\nlower variance. However, the time taken to converge to a steady state\ndistribution of weights is longer.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:40:55 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Lewis", "Martha", ""], ["Lawry", "Jonathan", ""]]}, {"id": "1601.06971", "submitter": "Vishal Kharde", "authors": "Vishal.A.Kharde, Prof. Sheetal.Sonawane", "title": "Sentiment Analysis of Twitter Data: A Survey of Techniques", "comments": "7 figures, 10 tables", "journal-ref": "International Journal of Computer Applications 139(11): 5-15,\n  April 2016", "doi": "10.5120/ijca2016908625", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advancement of web technology and its growth, there is a huge volume\nof data present in the web for internet users and a lot of data is generated\ntoo. Internet has become a platform for online learning, exchanging ideas and\nsharing opinions. Social networking sites like Twitter, Facebook, Google+ are\nrapidly gaining popularity as they allow people to share and express their\nviews about topics,have discussion with different communities, or post messages\nacross the world. There has been lot of work in the field of sentiment analysis\nof twitter data. This survey focuses mainly on sentiment analysis of twitter\ndata which is helpful to analyze the information in the tweets where opinions\nare highly unstructured, heterogeneous and are either positive or negative, or\nneutral in some cases. In this paper, we provide a survey and a comparative\nanalyses of existing techniques for opinion mining like machine learning and\nlexicon-based approaches, together with evaluation metrics. Using various\nmachine learning algorithms like Naive Bayes, Max Entropy, and Support Vector\nMachine, we provide a research on twitter data streams.General challenges and\napplications of Sentiment Analysis on Twitter are also discussed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 10:44:30 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 04:53:56 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 09:43:11 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Kharde", "Vishal. A.", ""], ["Sonawane", "Prof. Sheetal.", ""]]}, {"id": "1601.07124", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Elvys Linhares Pontes, Juan-Manuel Torres-Moreno, Andr\\'ea Carneiro\n  Linhares", "title": "LIA-RAG: a system based on graphs and divergence of probabilities\n  applied to Speech-To-Text Summarization", "comments": "7 pages, 2 figures, CCCS Multiling 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to introduces a new algorithm for automatic speech-to-text\nsummarization based on statistical divergences of probabilities and graphs. The\ninput is a text from speech conversations with noise, and the output a compact\ntext summary. Our results, on the pilot task CCCS Multiling 2015 French corpus\nare very encouraging\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 18:19:00 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Pontes", "Elvys Linhares", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Linhares", "Andr\u00e9a Carneiro", ""]]}, {"id": "1601.07215", "submitter": "Prasanna Kumar Muthukumar", "authors": "Prasanna Kumar Muthukumar, Alan W Black", "title": "Recurrent Neural Network Postfilters for Statistical Parametric Speech\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two years, there have been numerous papers that have looked into\nusing Deep Neural Networks to replace the acoustic model in traditional\nstatistical parametric speech synthesis. However, far less attention has been\npaid to approaches like DNN-based postfiltering where DNNs work in conjunction\nwith traditional acoustic models. In this paper, we investigate the use of\nRecurrent Neural Networks as a potential postfilter for synthesis. We explore\nthe possibility of replacing existing postfilters, as well as highlight the\nease with which arbitrary new features can be added as input to the postfilter.\nWe also tried a novel approach of jointly training the Classification And\nRegression Tree and the postfilter, rather than the traditional approach of\ntraining them independently.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 22:53:45 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Muthukumar", "Prasanna Kumar", ""], ["Black", "Alan W", ""]]}, {"id": "1601.07435", "submitter": "Torsten Timm", "authors": "Torsten Timm", "title": "Co-Occurrence Patterns in the Voynich Manuscript", "comments": "19 pages; tables for sections of the VMS added; 'The Towneley plays'\n  as example for English poetry added; revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Voynich Manuscript is a medieval book written in an unknown script. This\npaper studies the distribution of similarly spelled words in the Voynich\nManuscript. It shows that the distribution of words within the manuscript is\nnot compatible with natural languages.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 16:37:08 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 08:18:24 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Timm", "Torsten", ""]]}, {"id": "1601.07969", "submitter": "Jake Williams", "authors": "Jake Ryland Williams, James P. Bagrow, Andrew J. Reagan, Sharon E.\n  Alajajian, Christopher M. Danforth, and Peter Sheridan Dodds", "title": "Zipf's law is a consequence of coherent language production", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of text segmentation may be undertaken at many levels in text\nanalysis---paragraphs, sentences, words, or even letters. Here, we focus on a\nrelatively fine scale of segmentation, hypothesizing it to be in accord with a\nstochastic model of language generation, as the smallest scale where\nindependent units of meaning are produced. Our goals in this letter include the\ndevelopment of methods for the segmentation of these minimal independent units,\nwhich produce feature-representations of texts that align with the independence\nassumption of the bag-of-terms model, commonly used for prediction and\nclassification in computational text analysis. We also propose the measurement\nof texts' association (with respect to realized segmentations) to the model of\nlanguage generation. We find (1) that our segmentations of phrases exhibit much\nbetter associations to the generation model than words and (2), that texts\nwhich are well fit are generally topically homogeneous. Because our generative\nmodel produces Zipf's law, our study further suggests that Zipf's law may be a\nconsequence of homogeneity in language production.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 02:39:56 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 22:13:18 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Williams", "Jake Ryland", ""], ["Bagrow", "James P.", ""], ["Reagan", "Andrew J.", ""], ["Alajajian", "Sharon E.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1601.08188", "submitter": "Michael Wand", "authors": "Michael Wand and Jan Koutn\\'ik and J\\\"urgen Schmidhuber", "title": "Lipreading with Long Short-Term Memory", "comments": "Accepted for publication at ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipreading, i.e. speech recognition from visual-only recordings of a\nspeaker's face, can be achieved with a processing pipeline based solely on\nneural networks, yielding significantly better accuracy than conventional\nmethods. Feed-forward and recurrent neural network layers (namely Long\nShort-Term Memory; LSTM) are stacked to form a single structure which is\ntrained by back-propagating error gradients through all the layers. The\nperformance of such a stacked network was experimentally evaluated and compared\nto a standard Support Vector Machine classifier using conventional computer\nvision features (Eigenlips and Histograms of Oriented Gradients). The\nevaluation was performed on data from 19 speakers of the publicly available\nGRID corpus. With 51 different words to classify, we report a best word\naccuracy on held-out evaluation speakers of 79.6% using the end-to-end neural\nnetwork-based solution (11.6% improvement over the best feature-based solution\nevaluated).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:48:07 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Wand", "Michael", ""], ["Koutn\u00edk", "Jan", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}]