[{"id": "2104.00041", "submitter": "Allahsera Auguste Tapo", "authors": "Allahsera Auguste Tapo, Michael Leventhal, Sarah Luger, Christopher M.\n  Homan, Marcos Zampieri", "title": "Domain-specific MT for Low-resource Languages: The case of\n  Bambara-French", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating to and from low-resource languages is a challenge for machine\ntranslation (MT) systems due to a lack of parallel data. In this paper we\naddress the issue of domain-specific MT for Bambara, an under-resourced Mande\nlanguage spoken in Mali. We present the first domain-specific parallel dataset\nfor MT of Bambara into and from French. We discuss challenges in working with\nsmall quantities of domain-specific data for a low-resource language and we\npresent the results of machine learning experiments on this data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:12:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tapo", "Allahsera Auguste", ""], ["Leventhal", "Michael", ""], ["Luger", "Sarah", ""], ["Homan", "Christopher M.", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2104.00054", "submitter": "Daniel Deutsch", "authors": "Daniel Deutsch, Rotem Dror, Dan Roth", "title": "A Statistical Analysis of Summarization Evaluation Metrics using\n  Resampling Methods", "comments": "This is a pre-MIT Press publication version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of a summarization evaluation metric is quantified by calculating\nthe correlation between its scores and human annotations across a large number\nof summaries. Currently, it is unclear how precise these correlation estimates\nare, nor whether differences between two metrics' correlations reflect a true\ndifference or if it is due to mere chance. In this work, we address these two\nproblems by proposing methods for calculating confidence intervals and running\nhypothesis tests for correlations using two resampling methods, bootstrapping\nand permutation. After evaluating which of the proposed methods is most\nappropriate for summarization through two simulation experiments, we analyze\nthe results of applying these methods to several different automatic evaluation\nmetrics across three sets of human annotations. We find that the confidence\nintervals are rather wide, demonstrating high uncertainty in the reliability of\nautomatic metrics. Further, although many metrics fail to show statistical\nimprovements over ROUGE, two recent works, QAEval and BERTScore, do in some\nevaluation settings.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:28:14 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 18:52:41 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Deutsch", "Daniel", ""], ["Dror", "Rotem", ""], ["Roth", "Dan", ""]]}, {"id": "2104.00106", "submitter": "Aviral Joshi", "authors": "Aviral Joshi, Chengzhi Huang, Har Simrat Singh", "title": "Zero-Shot Language Transfer vs Iterative Back Translation for\n  Unsupervised Machine Translation", "comments": "7 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work focuses on comparing different solutions for machine translation on\nlow resource language pairs, namely, with zero-shot transfer learning and\nunsupervised machine translation. We discuss how the data size affects the\nperformance of both unsupervised MT and transfer learning. Additionally we also\nlook at how the domain of the data affects the result of unsupervised MT. The\ncode to all the experiments performed in this project are accessible on Github.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:19 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Joshi", "Aviral", ""], ["Huang", "Chengzhi", ""], ["Singh", "Har Simrat", ""]]}, {"id": "2104.00107", "submitter": "Aviral Joshi", "authors": "Abhinav Khattar, Aviral Joshi, Har Simrat Singh, Pulkit Goel, Rohit\n  Prakash Barnwal", "title": "Analysis on Image Set Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the challenge of Visual Question Answering in multi-image setting\nfor the ISVQA dataset. Traditional VQA tasks have focused on a single-image\nsetting where the target answer is generated from a single image. Image set\nVQA, however, comprises of a set of images and requires finding connection\nbetween images, relate the objects across images based on these connections and\ngenerate a unified answer. In this report, we work with 4 approaches in a bid\nto improve the performance on the task. We analyse and compare our results with\nthree baseline models - LXMERT, HME-VideoQA and VisualBERT - and show that our\napproaches can provide a slight improvement over the baselines. In specific, we\ntry to improve on the spatial awareness of the model and help the model\nidentify color using enhanced pre-training, reduce language dependence using\nadversarial regularization, and improve counting using regression loss and\ngraph based deduplication. We further delve into an in-depth analysis on the\nlanguage bias in the ISVQA dataset and show how models trained on ISVQA\nimplicitly learn to associate language more strongly with the final answer.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Khattar", "Abhinav", ""], ["Joshi", "Aviral", ""], ["Singh", "Har Simrat", ""], ["Goel", "Pulkit", ""], ["Barnwal", "Rohit Prakash", ""]]}, {"id": "2104.00120", "submitter": "Timo Lohrenz", "authors": "Timo Lohrenz, Zhengyang Li, Tim Fingscheidt", "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based\n  End-to-End Automatic Speech Recognition", "comments": "accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream fusion, also known as system combination, is a common technique in\nautomatic speech recognition for traditional hybrid hidden Markov model\napproaches, yet mostly unexplored for modern deep neural network end-to-end\nmodel architectures. Here, we investigate various fusion techniques for the\nall-attention-based encoder-decoder architecture known as the transformer,\nstriving to achieve optimal fusion by investigating different fusion levels in\nan example single-microphone setting with fusion of standard magnitude and\nphase features. We introduce a novel multi-encoder learning method that\nperforms a weighted combination of two encoder-decoder multi-head attention\noutputs only during training. Employing then only the magnitude feature encoder\nin inference, we are able to show consistent improvement on Wall Street Journal\n(WSJ) with language model and on Librispeech, without increase in runtime or\nparameters. Combining two such multi-encoder trained models by a simple late\nfusion in inference, we achieve state-of-the-art performance for\ntransformer-based models on WSJ with a significant WER reduction of 19%\nrelative compared to the current benchmark approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 21:07:43 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:10:01 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lohrenz", "Timo", ""], ["Li", "Zhengyang", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2104.00124", "submitter": "Joyce Nakatumba-Nabende Dr.", "authors": "Peter Nabende, David Kabiito, Claire Babirye, Hewitt Tusiime, Joyce\n  Nakatumba-Nabende", "title": "Misinformation detection in Luganda-English code-mixed social media text", "comments": "Accepted at African NLP workshop @EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing occurrence, forms, and negative effects of misinformation on\nsocial media platforms has necessitated more misinformation detection tools.\nCurrently, work is being done addressing COVID-19 misinformation however, there\nare no misinformation detection tools for any of the 40 distinct indigenous\nUgandan languages. This paper addresses this gap by presenting basic language\nresources and a misinformation detection data set based on code-mixed\nLuganda-English messages sourced from the Facebook and Twitter social media\nplatforms. Several machine learning methods are applied on the misinformation\ndetection data set to develop classification models for detecting whether a\ncode-mixed Luganda-English message contains misinformation or not. A 10-fold\ncross validation evaluation of the classification methods in an experimental\nmisinformation detection task shows that a Discriminative Multinomial Naive\nBayes (DMNB) method achieves the highest accuracy and F-measure of 78.19% and\n77.90% respectively. Also, Support Vector Machine and Bagging ensemble\nclassification models achieve comparable results. These results are promising\nsince the machine learning models are based on n-gram features from only the\nmisinformation detection dataset.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 21:12:29 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 17:34:31 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nabende", "Peter", ""], ["Kabiito", "David", ""], ["Babirye", "Claire", ""], ["Tusiime", "Hewitt", ""], ["Nakatumba-Nabende", "Joyce", ""]]}, {"id": "2104.00174", "submitter": "Isa Inuwa-Dutse", "authors": "Muhammad Abubakar Alhassan, Isa Inuwa-Dutse, Bello Shehu Bello, Diane\n  Pennington", "title": "Self-harm: detection and support on Twitter", "comments": "11 pages, 6 figures, 2 tables, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the advent of online social media platforms such as Twitter and\nFacebook, useful health-related studies have been conducted using the\ninformation posted by online participants. Personal health-related issues such\nas mental health, self-harm and depression have been studied because users\noften share their stories on such platforms. Online users resort to sharing\nbecause the empathy and support from online communities are crucial in helping\nthe affected individuals. A preliminary analysis shows how contents related to\nnon-suicidal self-injury (NSSI) proliferate on Twitter. Thus, we use Twitter to\ncollect relevant data, analyse, and proffer ways of supporting users prone to\nNSSI behaviour. Our approach utilises a custom crawler to retrieve relevant\ntweets from self-reporting users and relevant organisations interested in\ncombating self-harm. Through textual analysis, we identify six major categories\nof self-harming users consisting of inflicted, anti-self-harm, support seekers,\nrecovered, pro-self-harm and at risk. The inflicted category dominates the\ncollection. From an engagement perspective, we show how online users respond to\nthe information posted by self-harm support organisations on Twitter. By noting\nthe most engaged organisations, we apply a useful technique to uncover the\norganisations' strategy. The online participants show a strong inclination\ntowards online posts associated with mental health related attributes. Our\nstudy is based on the premise that social media can be used as a tool to\nsupport proactive measures to ease the negative impact of self-harm.\nConsequently, we proffer ways to prevent potential users from engaging in\nself-harm and support affected users through a set of recommendations. To\nsupport further research, the dataset will be made available for interested\nresearchers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:39:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Alhassan", "Muhammad Abubakar", ""], ["Inuwa-Dutse", "Isa", ""], ["Bello", "Bello Shehu", ""], ["Pennington", "Diane", ""]]}, {"id": "2104.00218", "submitter": "Xu Wang", "authors": "Xu Wang, Shuai Zhao, Bo Cheng, Jiale Han, Yingting Li, Hao Yang, Ivan\n  Sekulic, Guoshun Nan", "title": "Integrating Subgraph-aware Relation and DirectionReasoning for Question\n  Answering", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) models over Knowledge Bases (KBs) are capable of\nproviding more precise answers by utilizing relation information among\nentities. Although effective, most of these models solely rely on fixed\nrelation representations to obtain answers for different question-related KB\nsubgraphs. Hence, the rich structured information of these subgraphs may be\noverlooked by the relation representation vectors. Meanwhile, the direction\ninformation of reasoning, which has been proven effective for the answer\nprediction on graphs, has not been fully explored in existing work. To address\nthese challenges, we propose a novel neural model, Relation-updated\nDirection-guided Answer Selector (RDAS), which converts relations in each\nsubgraph to additional nodes to learn structure information. Additionally, we\nutilize direction information to enhance the reasoning ability. Experimental\nresults show that our model yields substantial improvements on two widely used\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:04:36 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Xu", ""], ["Zhao", "Shuai", ""], ["Cheng", "Bo", ""], ["Han", "Jiale", ""], ["Li", "Yingting", ""], ["Yang", "Hao", ""], ["Sekulic", "Ivan", ""], ["Nan", "Guoshun", ""]]}, {"id": "2104.00235", "submitter": "Chiranjeevi Yarra Dr.", "authors": "Anuj Diwan, Rakesh Vaideeswaran, Sanket Shah, Ankita Singh, Srinivasa\n  Raghavan, Shreya Khare, Vinit Unni, Saurabh Vyas, Akash Rajpuria, Chiranjeevi\n  Yarra, Ashish Mittal, Prasanta Kumar Ghosh, Preethi Jyothi, Kalika Bali,\n  Vivek Seshadri, Sunayana Sitaram, Samarth Bharadwaj, Jai Nanavati, Raoul\n  Nanavati, Karthik Sankaranarayanan, Tejaswi Seeram and Basil Abraham", "title": "Multilingual and code-switching ASR challenges for low resource Indian\n  languages", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there is increasing interest in multilingual automatic speech\nrecognition (ASR) where a speech recognition system caters to multiple low\nresource languages by taking advantage of low amounts of labeled corpora in\nmultiple languages. With multilingualism becoming common in today's world,\nthere has been increasing interest in code-switching ASR as well. In\ncode-switching, multiple languages are freely interchanged within a single\nsentence or between sentences. The success of low-resource multilingual and\ncode-switching ASR often depends on the variety of languages in terms of their\nacoustics, linguistic characteristics as well as the amount of data available\nand how these are carefully considered in building the ASR system. In this\nchallenge, we would like to focus on building multilingual and code-switching\nASR systems through two different subtasks related to a total of seven Indian\nlanguages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati and Bengali.\nFor this purpose, we provide a total of ~600 hours of transcribed speech data,\ncomprising train and test sets, in these languages including two code-switched\nlanguage pairs, Hindi-English and Bengali-English. We also provide a baseline\nrecipe for both the tasks with a WER of 30.73% and 32.45% on the test sets of\nmultilingual and code-switching subtasks, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:37:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Diwan", "Anuj", ""], ["Vaideeswaran", "Rakesh", ""], ["Shah", "Sanket", ""], ["Singh", "Ankita", ""], ["Raghavan", "Srinivasa", ""], ["Khare", "Shreya", ""], ["Unni", "Vinit", ""], ["Vyas", "Saurabh", ""], ["Rajpuria", "Akash", ""], ["Yarra", "Chiranjeevi", ""], ["Mittal", "Ashish", ""], ["Ghosh", "Prasanta Kumar", ""], ["Jyothi", "Preethi", ""], ["Bali", "Kalika", ""], ["Seshadri", "Vivek", ""], ["Sitaram", "Sunayana", ""], ["Bharadwaj", "Samarth", ""], ["Nanavati", "Jai", ""], ["Nanavati", "Raoul", ""], ["Sankaranarayanan", "Karthik", ""], ["Seeram", "Tejaswi", ""], ["Abraham", "Basil", ""]]}, {"id": "2104.00267", "submitter": "Prabhakar Gupta", "authors": "Prabhakar Gupta, Ridha Juneja, Anil Nelakanti, Tamojit Chatterjee", "title": "Detecting over/under-translation errors for determining adequacy in\n  human translations", "comments": "6 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to detecting over and under translations (OT/UT)\nas part of adequacy error checks in translation evaluation. We do not restrict\nourselves to machine translation (MT) outputs and specifically target\napplications with human generated translation pipeline. The goal of our system\nis to identify OT/UT errors from human translated video subtitles with high\nerror recall. We achieve this without reference translations by learning a\nmodel on synthesized training data. We compare various classification networks\nthat we trained on embeddings from pre-trained language model with our best\nhybrid network of GRU + CNN achieving 89.3% accuracy on high-quality\nhuman-annotated evaluation data in 8 languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:06:36 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gupta", "Prabhakar", ""], ["Juneja", "Ridha", ""], ["Nelakanti", "Anil", ""], ["Chatterjee", "Tamojit", ""]]}, {"id": "2104.00270", "submitter": "Jivnesh Sandhan", "authors": "Jivnesh Sandhan, Om Adideva, Digumarthi Komal, Laxmidhar Behera, and\n  Pawan Goyal", "title": "Evaluating Neural Word Embeddings for Sanskrit", "comments": "14 pages, The work is submitted at WSC 2022, Canberra, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the supervised learning paradigm's surprisingly remarkable\nperformance has garnered considerable attention from Sanskrit Computational\nLinguists. As a result, the Sanskrit community has put laudable efforts to\nbuild task-specific labeled data for various downstream Natural Language\nProcessing (NLP) tasks. The primary component of these approaches comes from\nrepresentations of word embeddings. Word embedding helps to transfer knowledge\nlearned from readily available unlabelled data for improving task-specific\nperformance in low-resource setting. Last decade, there has been much\nexcitement in the field of digitization of Sanskrit. To effectively use such\nreadily available resources, it is very much essential to perform a systematic\nstudy on word embedding approaches for the Sanskrit language. In this work, we\ninvestigate the effectiveness of word embeddings. We classify word embeddings\nin broad categories to facilitate systematic experimentation and evaluate them\non four intrinsic tasks. We investigate the efficacy of embeddings approaches\n(originally proposed for languages other than Sanskrit) for Sanskrit along with\nvarious challenges posed by language.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:08:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Sandhan", "Jivnesh", ""], ["Adideva", "Om", ""], ["Komal", "Digumarthi", ""], ["Behera", "Laxmidhar", ""], ["Goyal", "Pawan", ""]]}, {"id": "2104.00290", "submitter": "Thamme Gowda", "authors": "Thamme Gowda, Zhao Zhang, Chris A Mattmann, Jonathan May", "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models", "comments": "To-appear: ACL 2021 System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:55:12 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 19:40:00 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gowda", "Thamme", ""], ["Zhang", "Zhao", ""], ["Mattmann", "Chris A", ""], ["May", "Jonathan", ""]]}, {"id": "2104.00312", "submitter": "Ningyu Zhang", "authors": "Luoqiu Li, Xiang Chen, Ningyu Zhang, Shumin Deng, Xin Xie, Chuanqi\n  Tan, Mosha Chen, Fei Huang, Huajun Chen", "title": "Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples\n  for Relation Extraction", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural-based relation extraction approaches, though achieving\npromising improvement on benchmark datasets, have reported their vulnerability\ntowards adversarial attacks. Thus far, efforts mostly focused on generating\nadversarial samples or defending adversarial attacks, but little is known about\nthe difference between normal and adversarial samples. In this work, we take\nthe first step to leverage the salience-based method to analyze those\nadversarial samples. We observe that salience tokens have a direct correlation\nwith adversarial perturbations. We further find the adversarial perturbations\nare either those tokens not existing in the training set or superficial cues\nassociated with relation labels. To some extent, our approach unveils the\ncharacters against adversarial samples. We release an open-source testbed,\n\"DiagnoseAdv\".\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:36:04 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 07:39:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Luoqiu", ""], ["Chen", "Xiang", ""], ["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Xie", "Xin", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Huang", "Fei", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.00336", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Yejin Bang, Andrea Madotto, Pascale Fung", "title": "Mitigating Media Bias through Neutral Article Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Media bias can lead to increased political polarization, and thus, the need\nfor automatic mitigation methods is growing. Existing mitigation work displays\narticles from multiple news outlets to provide diverse news coverage, but\nwithout neutralizing the bias inherent in each of the displayed articles.\nTherefore, we propose a new task, a single neutralized article generation out\nof multiple biased articles, to facilitate more efficient access to balanced\nand unbiased information. In this paper, we compile a new dataset NeuWS, define\nan automatic evaluation metric, and provide baselines and multiple analyses to\nserve as a solid starting point for the proposed task. Lastly, we obtain a\nhuman evaluation to demonstrate the alignment between our metric and human\njudgment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:37:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lee", "Nayeon", ""], ["Bang", "Yejin", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.00366", "submitter": "Evander Nyoni", "authors": "Evander Nyoni and Bruce A. Bassett", "title": "Low-Resource Neural Machine Translation for Southern African Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resource African languages have not fully benefited from the progress in\nneural machine translation because of a lack of data. Motivated by this\nchallenge we compare zero-shot learning, transfer learning and multilingual\nlearning on three Bantu languages (Shona, isiXhosa and isiZulu) and English.\nOur main target is English-to-isiZulu translation for which we have just 30,000\nsentence pairs, 28% of the average size of our other corpora. We show the\nimportance of language similarity on the performance of English-to-isiZulu\ntransfer learning based on English-to-isiXhosa and English-to-Shona parent\nmodels whose BLEU scores differ by 5.2. We then demonstrate that multilingual\nlearning surpasses both transfer learning and zero-shot learning on our\ndataset, with BLEU score improvements relative to the baseline\nEnglish-to-isiZulu model of 9.9, 6.1 and 2.0 respectively. Our best model also\nimproves the previous SOTA BLEU score by more than 10.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:48:13 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 18:49:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nyoni", "Evander", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "2104.00369", "submitter": "Linyong Nan", "authors": "Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma,\n  Rui Zhang, Wojciech Kry\\'sci\\'nski, Nick Schoelkopf, Riley Kong, Xiangru\n  Tang, Murori Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob\n  Cunningham, Caiming Xiong, Dragomir Radev", "title": "FeTaQA: Free-form Table Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Existing table question answering datasets contain abundant factual questions\nthat primarily evaluate the query and schema comprehension capability of a\nsystem, but they fail to include questions that require complex reasoning and\nintegration of information due to the constraint of the associated short-form\nanswers. To address these issues and to demonstrate the full challenge of table\nquestion answering, we introduce FeTaQA, a new dataset with 10K Wikipedia-based\n{table, question, free-form answer, supporting table cells} pairs. FeTaQA\nyields a more challenging table question answering setting because it requires\ngenerating free-form text answers after retrieval, inference, and integration\nof multiple discontinuous facts from a structured knowledge source. Unlike\ndatasets of generative QA over text in which answers are prevalent with copies\nof short text spans from the source, answers in our dataset are human-generated\nexplanations involving entities and their high-level relations. We provide two\nbenchmark methods for the proposed task: a pipeline method based on\nsemantic-parsing-based QA systems and an end-to-end method based on large\npretrained text generation models, and show that FeTaQA poses a challenge for\nboth methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:59:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Nan", "Linyong", ""], ["Hsieh", "Chiachun", ""], ["Mao", "Ziming", ""], ["Lin", "Xi Victoria", ""], ["Verma", "Neha", ""], ["Zhang", "Rui", ""], ["Kry\u015bci\u0144ski", "Wojciech", ""], ["Schoelkopf", "Nick", ""], ["Kong", "Riley", ""], ["Tang", "Xiangru", ""], ["Mutuma", "Murori", ""], ["Rosand", "Ben", ""], ["Trindade", "Isabel", ""], ["Bandaru", "Renusree", ""], ["Cunningham", "Jacob", ""], ["Xiong", "Caiming", ""], ["Radev", "Dragomir", ""]]}, {"id": "2104.00424", "submitter": "Jussi Karlgren", "authors": "Jussi Karlgren and Pentti Kanerva", "title": "High-dimensional distributed semantic spaces for utterances", "comments": null, "journal-ref": "Natural Language Engineering 25, no. 4 (2019): 503-517", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional distributed semantic spaces have proven useful and effective\nfor aggregating and processing visual, auditory, and lexical information for\nmany tasks related to human-generated data. Human language makes use of a large\nand varying number of features, lexical and constructional items as well as\ncontextual and discourse-specific data of various types, which all interact to\nrepresent various aspects of communicative information. Some of these features\nare mostly local and useful for the organisation of e.g. argument structure of\na predication; others are persistent over the course of a discourse and\nnecessary for achieving a reasonable level of understanding of the content.\nThis paper describes a model for high-dimensional representation for utterance\nand text level data including features such as constructions or contextual\ndata, based on a mathematically principled and behaviourally plausible approach\nto representing linguistic information. The implementation of the\nrepresentation is a straightforward extension of Random Indexing models\npreviously used for lexical linguistic items. The paper shows how the\nimplemented model is able to represent a broad range of linguistic features in\na common integral framework of fixed dimensionality, which is computationally\nhabitable, and which is suitable as a bridge between symbolic representations\nsuch as dependency analysis and continuous representations used e.g. in\nclassifiers or further machine-learning approaches. This is achieved with\noperations on vectors that constitute a powerful computational algebra,\naccompanied with an associative memory for the vectors. The paper provides a\ntechnical overview of the framework and a worked through implemented example of\nhow it can be applied to various types of linguistic features.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:09:47 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Karlgren", "Jussi", ""], ["Kanerva", "Pentti", ""]]}, {"id": "2104.00426", "submitter": "Mingxuan Niu", "authors": "Yuka Takeishi, Mingxuan Niu, Jing Luo, Zhong Jin, Xinyu Yang", "title": "WakaVT: A Sequential Variational Transformer for Waka Generation", "comments": "This paper has been submitted to Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poetry generation has long been a challenge for artificial intelligence. In\nthe scope of Japanese poetry generation, many researchers have paid attention\nto Haiku generation, but few have focused on Waka generation. To further\nexplore the creative potential of natural language generation systems in\nJapanese poetry creation, we propose a novel Waka generation model, WakaVT,\nwhich automatically produces Waka poems given user-specified keywords. Firstly,\nan additive mask-based approach is presented to satisfy the form constraint.\nSecondly, the structures of Transformer and variational autoencoder are\nintegrated to enhance the quality of generated content. Specifically, to obtain\nnovelty and diversity, WakaVT employs a sequence of latent variables, which\neffectively captures word-level variability in Waka data. To improve linguistic\nquality in terms of fluency, coherence, and meaningfulness, we further propose\nthe fused multilevel self-attention mechanism, which properly models the\nhierarchical linguistic structure of Waka. To the best of our knowledge, we are\nthe first to investigate Waka generation with models based on Transformer\nand/or variational autoencoder. Both objective and subjective evaluation\nresults demonstrate that our model outperforms baselines significantly.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:14:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Takeishi", "Yuka", ""], ["Niu", "Mingxuan", ""], ["Luo", "Jing", ""], ["Jin", "Zhong", ""], ["Yang", "Xinyu", ""]]}, {"id": "2104.00543", "submitter": "Tong Wu", "authors": "Tong Wu and Jorge Ortiz", "title": "RLAD: Time Series Anomaly Detection through Reinforcement Learning and\n  Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new semi-supervised, time series anomaly detection algorithm\nthat uses deep reinforcement learning (DRL) and active learning to efficiently\nlearn and adapt to anomalies in real-world time series data. Our model - called\nRLAD - makes no assumption about the underlying mechanism that produces the\nobservation sequence and continuously adapts the detection model based on\nexperience with anomalous patterns. In addition, it requires no manual tuning\nof parameters and outperforms all state-of-art methods we compare with, both\nunsupervised and semi-supervised, across several figures of merit. More\nspecifically, we outperform the best unsupervised approach by a factor of 1.58\non the F1 score, with only 1% of labels and up to around 4.4x on another\nreal-world dataset with only 0.1% of labels. We compare RLAD with seven\ndeep-learning based algorithms across two common anomaly detection datasets\nwith up to around 3M data points and between 0.28% to 2.65% anomalies.We\noutperform all of them across several important performance metrics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:21:15 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wu", "Tong", ""], ["Ortiz", "Jorge", ""]]}, {"id": "2104.00558", "submitter": "Constantine Lignos", "authors": "Jonne S\\\"alev\\\"a and Constantine Lignos", "title": "Mining Wikidata for Name Resources for African Languages", "comments": "Accepted at the EACL 2021 AfricaNLP workshop (non-archival)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work supports further development of language technology for the\nlanguages of Africa by providing a Wikidata-derived resource of name lists\ncorresponding to common entity types (person, location, and organization).\nWhile we are not the first to mine Wikidata for name lists, our approach\nemphasizes scalability and replicability and addresses data quality issues for\nlanguages that do not use Latin scripts. We produce lists containing\napproximately 1.9 million names across 28 African languages. We describe the\ndata, the process used to produce it, and its limitations, and provide the\nsoftware and data for public use. Finally, we discuss the ethical\nconsiderations of producing this resource and others of its kind.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:34:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["S\u00e4lev\u00e4", "Jonne", ""], ["Lignos", "Constantine", ""]]}, {"id": "2104.00639", "submitter": "Rafel Palliser Sans", "authors": "Rafel Palliser-Sans, Albert Rial-Farr\\`as", "title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans\n  Detection", "comments": "7 pages, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our submission to SemEval-2021 Task 5: Toxic Spans\nDetection. The purpose of this task is to detect the spans that make a text\ntoxic, which is a complex labour for several reasons. Firstly, because of the\nintrinsic subjectivity of toxicity, and secondly, due to toxicity not always\ncoming from single words like insults or offends, but sometimes from whole\nexpressions formed by words that may not be toxic individually. Following this\nidea of focusing on both single words and multi-word expressions, we study the\nimpact of using a multi-depth DistilBERT model, which uses embeddings from\ndifferent layers to estimate the final per-token toxicity. Our quantitative\nresults show that using information from multiple depths boosts the performance\nof the model. Finally, we also analyze our best model qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:37:38 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 11:05:54 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Palliser-Sans", "Rafel", ""], ["Rial-Farr\u00e0s", "Albert", ""]]}, {"id": "2104.00640", "submitter": "James Thorne", "authors": "James Thorne, Max Glockner, Gisela Vallejo, Andreas Vlachos, Iryna\n  Gurevych", "title": "Evidence-based Verification for Real World Information Needs", "comments": "Code and Data\n  https://github.com/CambridgeNLIP/verification-real-world-info-needs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Claim verification is the task of predicting the veracity of written\nstatements against evidence. Previous large-scale datasets model the task as\nclassification, ignoring the need to retrieve evidence, or are constructed for\nresearch purposes, and may not be representative of real-world needs. In this\npaper, we introduce a novel claim verification dataset with instances derived\nfrom search-engine queries, yielding 10,987 claims annotated with evidence that\nrepresent real-world information needs. For each claim, we annotate evidence\nfrom full Wikipedia articles with both section and sentence-level granularity.\nOur annotation allows comparison between two complementary approaches to\nverification: stance classification, and evidence extraction followed by\nentailment recognition. In our comprehensive evaluation, we find no significant\ndifference in accuracy between these two approaches. This enables systems to\nuse evidence extraction to summarize a rationale for an end-user while\nmaintaining the accuracy when predicting a claim's veracity. With challenging\nclaims and evidence documents containing hundreds of sentences, our dataset\npresents interesting challenges that are not captured in previous work --\nevidenced through transfer learning experiments. We release code and data to\nsupport further research on this task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:40:08 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Thorne", "James", ""], ["Glockner", "Max", ""], ["Vallejo", "Gisela", ""], ["Vlachos", "Andreas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.00660", "submitter": "Ngoc Phuoc An Vo", "authors": "Ngoc Phuoc An Vo, Irene Manotas, Octavian Popescu, Algimantas\n  Cerniauskas, Vadim Sheinin", "title": "Recognizing and Splitting Conditional Sentences for Automation of\n  Business Processes Management", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Business Process Management (BPM) is the discipline which is responsible for\nmanagement of discovering, analyzing, redesigning, monitoring, and controlling\nbusiness processes. One of the most crucial tasks of BPM is discovering and\nmodelling business processes from text documents. In this paper, we present our\nsystem that resolves an end-to-end problem consisting of 1) recognizing\nconditional sentences from technical documents, 2) finding boundaries to\nextract conditional and resultant clauses from each conditional sentence, and\n3) categorizing resultant clause as Action or Consequence which later helps to\ngenerate new steps in our business process model automatically. We created a\nnew dataset and three models solve this problem. Our best model achieved very\npromising results of 83.82, 87.84, and 85.75 for Precision, Recall, and F1,\nrespectively, for extracting Condition, Action, and Consequence clauses using\nExact Match metric.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:53:16 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Vo", "Ngoc Phuoc An", ""], ["Manotas", "Irene", ""], ["Popescu", "Octavian", ""], ["Cerniauskas", "Algimantas", ""], ["Sheinin", "Vadim", ""]]}, {"id": "2104.00664", "submitter": "Vil\\'em Zouhar", "authors": "Vil\\'em Zouhar", "title": "Sampling and Filtering of Neural Machine Translation Distillation Data", "comments": "6 pages (without references); to be published in NAACL-SRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most of neural machine translation distillation or stealing scenarios, the\ngoal is to preserve the performance of the target model (teacher). The\nhighest-scoring hypothesis of the teacher model is commonly used to train a new\nmodel (student). If reference translations are also available, then better\nhypotheses (with respect to the references) can be upsampled and poor\nhypotheses either removed or undersampled.\n  This paper explores the importance sampling method landscape (pruning,\nhypothesis upsampling and undersampling, deduplication and their combination)\nwith English to Czech and English to German MT models using standard MT\nevaluation metrics. We show that careful upsampling and combination with the\noriginal data leads to better performance when compared to training only on the\noriginal or synthesized data or their direct combination.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:54:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zouhar", "Vil\u00e9m", ""]]}, {"id": "2104.00676", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zechun Liu and Dejia Xu and Zitian Chen and\n  Kwang-Ting Cheng and Marios Savvides", "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An\n  Empirical Study", "comments": "ICLR 2021. Project page:\n  http://zhiqiangshen.com/projects/LS_and_KD/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to empirically clarify a recently discovered perspective that\nlabel smoothing is incompatible with knowledge distillation. We begin by\nintroducing the motivation behind on how this incompatibility is raised, i.e.,\nlabel smoothing erases relative information between teacher logits. We provide\na novel connection on how label smoothing affects distributions of semantically\nsimilar and dissimilar classes. Then we propose a metric to quantitatively\nmeasure the degree of erased information in sample's representation. After\nthat, we study its one-sidedness and imperfection of the incompatibility view\nthrough massive analyses, visualizations and comprehensive experiments on Image\nClassification, Binary Networks, and Neural Machine Translation. Finally, we\nbroadly discuss several circumstances wherein label smoothing will indeed lose\nits effectiveness. Project page:\nhttp://zhiqiangshen.com/projects/LS_and_KD/index.html.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:12 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Xu", "Dejia", ""], ["Chen", "Zitian", ""], ["Cheng", "Kwang-Ting", ""], ["Savvides", "Marios", ""]]}, {"id": "2104.00743", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi and Derek Hoiem", "title": "Towards General Purpose Vision Systems", "comments": "Project page: https://prior.allenai.org/projects/gpv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A special purpose learning system assumes knowledge of admissible tasks at\ndesign time. Adapting such a system to unforeseen tasks requires architecture\nmanipulation such as adding an output head for each new task or dataset. In\nthis work, we propose a task-agnostic vision-language system that accepts an\nimage and a natural language task description and outputs bounding boxes,\nconfidences, and text. The system supports a wide range of vision tasks such as\nclassification, localization, question answering, captioning, and more. We\nevaluate the system's ability to learn multiple skills simultaneously, to\nperform tasks with novel skill-concept combinations, and to learn new skills\nefficiently and without forgetting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:35:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Gupta", "Tanmay", ""], ["Kamath", "Amita", ""], ["Kembhavi", "Aniruddha", ""], ["Hoiem", "Derek", ""]]}, {"id": "2104.00764", "submitter": "Pranav Ravindra Maneriker", "authors": "Pranav Maneriker, Yuntian He, Srinivasan Parthasarathy", "title": "StyleML: Stylometry with Structure and Multitask Learning for Darkweb\n  Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Darknet market forums are frequently used to exchange illegal goods and\nservices between parties who use encryption to conceal their identities. The\nTor network is used to host these markets, which guarantees additional\nanonymization from IP and location tracking, making it challenging to link\nacross malicious users using multiple accounts (sybils). Additionally, users\nmigrate to new forums when one is closed, making it difficult to link users\nacross multiple forums. We develop a novel stylometry-based multitask learning\napproach for natural language and interaction modeling using graph embeddings\nto construct low-dimensional representations of short episodes of user activity\nfor authorship attribution. We provide a comprehensive evaluation of our\nmethods across four different darknet forums demonstrating its efficacy over\nthe state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X\non Recall@10.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:59:02 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Maneriker", "Pranav", ""], ["He", "Yuntian", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "2104.00766", "submitter": "Ranya Aloufi", "authors": "Ranya Aloufi, Hamed Haddadi, David Boyle", "title": "Configurable Privacy-Preserving Automatic Speech Recognition", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice assistive technologies have given rise to far-reaching privacy and\nsecurity concerns. In this paper we investigate whether modular automatic\nspeech recognition (ASR) can improve privacy in voice assistive systems by\ncombining independently trained separation, recognition, and discretization\nmodules to design configurable privacy-preserving ASR systems. We evaluate\nprivacy concerns and the effects of applying various state-of-the-art\ntechniques at each stage of the system, and report results using task-specific\nmetrics (i.e. WER, ABX, and accuracy). We show that overlapping speech inputs\nto ASR systems present further privacy concerns, and how these may be mitigated\nusing speech separation and optimization techniques. Our discretization module\nis shown to minimize paralinguistics privacy leakage from ASR acoustic models\nto levels commensurate with random guessing. We show that voice privacy can be\nconfigurable, and argue this presents new opportunities for privacy-preserving\napplications incorporating ASR.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:03:49 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Aloufi", "Ranya", ""], ["Haddadi", "Hamed", ""], ["Boyle", "David", ""]]}, {"id": "2104.00767", "submitter": "Jan Buys", "authors": "Tumi Moeng, Sheldon Reay, Aaron Daniels, Jan Buys", "title": "Canonical and Surface Morphological Segmentation for Nguni Languages", "comments": "AfricaNLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Morphological Segmentation involves decomposing words into morphemes, the\nsmallest meaning-bearing units of language. This is an important NLP task for\nmorphologically-rich agglutinative languages such as the Southern African Nguni\nlanguage group. In this paper, we investigate supervised and unsupervised\nmodels for two variants of morphological segmentation: canonical and surface\nsegmentation. We train sequence-to-sequence models for canonical segmentation,\nwhere the underlying morphemes may not be equal to the surface form of the\nword, and Conditional Random Fields (CRF) for surface segmentation.\nTransformers outperform LSTMs with attention on canonical segmentation,\nobtaining an average F1 score of 72.5% across 4 languages. Feature-based CRFs\noutperform bidirectional LSTM-CRFs to obtain an average of 97.1% F1 on surface\nsegmentation. In the unsupervised setting, an entropy-based approach using a\ncharacter-level LSTM language model fails to outperforms a Morfessor baseline,\nwhile on some of the languages neither approach performs much better than a\nrandom baseline. We hope that the high performance of the supervised\nsegmentation models will help to facilitate the development of better NLP tools\nfor Nguni languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:06:51 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Moeng", "Tumi", ""], ["Reay", "Sheldon", ""], ["Daniels", "Aaron", ""], ["Buys", "Jan", ""]]}, {"id": "2104.00769", "submitter": "Axel Berg", "authors": "Axel Berg, Mark O'Connor, Miguel Tairum Cruz", "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting", "comments": "Proceedings of INTERSPEECH", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top of\nconvolutional or recurrent encoders. We investigate a range of ways to adapt\nthe Transformer architecture to keyword spotting and introduce the Keyword\nTransformer (KWT), a fully self-attentional architecture that exceeds\nstate-of-the-art performance across multiple tasks without any pre-training or\nadditional data. Surprisingly, this simple architecture outperforms more\ncomplex models that mix convolutional, recurrent and attentive layers. KWT can\nbe used as a drop-in replacement for these models, setting two new benchmark\nrecords on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on\nthe 12 and 35-command tasks respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:15:30 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 14:28:41 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:06:01 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Berg", "Axel", ""], ["O'Connor", "Mark", ""], ["Cruz", "Miguel Tairum", ""]]}, {"id": "2104.00772", "submitter": "Jan Buys", "authors": "Stuart Mesham, Luc Hayward, Jared Shapiro, Jan Buys", "title": "Low-Resource Language Modelling of South African Languages", "comments": "AfricaNLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language models are the foundation of current neural network-based models for\nnatural language understanding and generation. However, research on the\nintrinsic performance of language models on African languages has been\nextremely limited, which is made more challenging by the lack of large or\nstandardised training and evaluation sets that exist for English and other\nhigh-resource languages. In this paper, we evaluate the performance of\nopen-vocabulary language models on low-resource South African languages, using\nbyte-pair encoding to handle the rich morphology of these languages. We\nevaluate different variants of n-gram models, feedforward neural networks,\nrecurrent neural networks (RNNs), and Transformers on small-scale datasets.\nOverall, well-regularized RNNs give the best performance across two isiZulu and\none Sepedi datasets. Multilingual training further improves performance on\nthese datasets. We hope that this research will open new avenues for research\ninto multilingual and low-resource language modelling for African languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:27:27 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mesham", "Stuart", ""], ["Hayward", "Luc", ""], ["Shapiro", "Jared", ""], ["Buys", "Jan", ""]]}, {"id": "2104.00773", "submitter": "Fanghua Ye", "authors": "Fanghua Ye, Jarana Manotumruksa, Emine Yilmaz", "title": "MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with\n  Essential Annotation Corrections to Improve State Tracking Evaluation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The MultiWOZ 2.0 dataset was released in 2018. It consists of more than\n10,000 task-oriented dialogues spanning 7 domains, and has greatly stimulated\nthe research of task-oriented dialogue systems. However, there is substantial\nnoise in the state annotations, which hinders a proper evaluation of dialogue\nstate tracking models. To tackle this issue, massive efforts have been devoted\nto correcting the annotations, resulting in 3 improved versions of this dataset\n(i.e., MultiWOZ 2.1-2.3). Even so, there are still lots of incorrect and\ninconsistent annotations. This work introduces MultiWOZ 2.4, in which we refine\nall annotations in the validation set and test set on top of MultiWOZ 2.1. The\nannotations in the training set remain unchanged to encourage robust and\nnoise-resilient model training. We further benchmark 8 state-of-the-art\ndialogue state tracking models. All these models achieve much higher\nperformance on MultiWOZ 2.4 than on MultiWOZ 2.1.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:31:48 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ye", "Fanghua", ""], ["Manotumruksa", "Jarana", ""], ["Yilmaz", "Emine", ""]]}, {"id": "2104.00782", "submitter": "Filipo Sharevski", "authors": "Peter Jachim, Filipo Sharevski, Emma Pieroni", "title": "\"TL;DR:\" Out-of-Context Adversarial Text Summarization and Hashtag\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents Out-of-Context Summarizer, a tool that takes arbitrary\npublic news articles out of context by summarizing them to coherently fit\neither a liberal- or conservative-leaning agenda. The Out-of-Context Summarizer\nalso suggests hashtag keywords to bolster the polarization of the summary, in\ncase one is inclined to take it to Twitter, Parler or other platforms for\ntrolling. Out-of-Context Summarizer achieved 79% precision and 99% recall when\nsummarizing COVID-19 articles, 93% precision and 93% recall when summarizing\npolitically-centered articles, and 87% precision and 88% recall when taking\nliberally-biased articles out of context. Summarizing valid sources instead of\nsynthesizing fake text, the Out-of-Context Summarizer could fairly pass the\n\"adversarial disclosure\" test, but we didn't take this easy route in our paper.\nInstead, we used the Out-of-Context Summarizer to push the debate of potential\nmisuse of automated text generation beyond the boilerplate text of responsible\ndisclosure of adversarial language models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:03:44 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Jachim", "Peter", ""], ["Sharevski", "Filipo", ""], ["Pieroni", "Emma", ""]]}, {"id": "2104.00783", "submitter": "Derek Chen", "authors": "Derek Chen, Howard Chen, Yi Yang, Alex Lin, Zhou Yu", "title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth\n  Task-Oriented Dialogue Systems", "comments": "16 pages, 5 figures. Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing goal-oriented dialogue datasets focus mainly on identifying slots\nand values. However, customer support interactions in reality often involve\nagents following multi-step procedures derived from explicitly-defined company\npolicies as well. To study customer service dialogue systems in more realistic\nsettings, we introduce the Action-Based Conversations Dataset (ABCD), a\nfully-labeled dataset with over 10K human-to-human dialogues containing 55\ndistinct user intents requiring unique sequences of actions constrained by\npolicies to achieve task success. We propose two additional dialog tasks,\nAction State Tracking and Cascading Dialogue Success, and establish a series of\nbaselines involving large-scale, pre-trained language models on this dataset.\nEmpirical results demonstrate that while more sophisticated networks outperform\nsimpler models, a considerable gap (50.8% absolute accuracy) still exists to\nreach human-level performance on ABCD.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:04:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chen", "Derek", ""], ["Chen", "Howard", ""], ["Yang", "Yi", ""], ["Lin", "Alex", ""], ["Yu", "Zhou", ""]]}, {"id": "2104.00789", "submitter": "Miikka Silfverberg", "authors": "Miikka Silfverberg, Francis Tyers, Garrett Nicolai, Mans Hulden", "title": "Do RNN States Encode Abstract Phonological Processes?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence models have delivered impressive results in word\nformation tasks such as morphological inflection, often learning to model\nsubtle morphophonological details with limited training data. Despite the\nperformance, the opacity of neural models makes it difficult to determine\nwhether complex generalizations are learned, or whether a kind of separate rote\nmemorization of each morphophonological process takes place. To investigate\nwhether complex alternations are simply memorized or whether there is some\nlevel of generalization across related sound changes in a sequence-to-sequence\nmodel, we perform several experiments on Finnish consonant gradation -- a\ncomplex set of sound changes triggered in some words by certain suffixes. We\nfind that our models often -- though not always -- encode 17 different\nconsonant gradation processes in a handful of dimensions in the RNN. We also\nshow that by scaling the activations in these dimensions we can control whether\nconsonant gradation occurs and the direction of the gradation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:24:39 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Silfverberg", "Miikka", ""], ["Tyers", "Francis", ""], ["Nicolai", "Garrett", ""], ["Hulden", "Mans", ""]]}, {"id": "2104.00814", "submitter": "Dheeraj Rajagopal", "authors": "Dheeraj Rajagopal, Aman Madaan, Niket Tandon, Yiming Yang, Shrimai\n  Prabhumoye, Abhilasha Ravichander, Peter Clark, Eduard Hovy", "title": "CURIE: An Iterative Querying Approach for Reasoning About Situations", "comments": "This paper builds upon EIGEN (arXiv:2010.11764) and proposes a\n  general framework for situational reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, models have been shown to predict the effects of unexpected\nsituations, e.g., would cloudy skies help or hinder plant growth? Given a\ncontext, the goal of such situational reasoning is to elicit the consequences\nof a new situation (st) that arises in that context. We propose a method to\niteratively build a graph of relevant consequences explicitly in a structured\nsituational graph (st-graph) using natural language queries over a finetuned\nlanguage model (M). Across multiple domains, CURIE generates st-graphs that\nhumans find relevant and meaningful in eliciting the consequences of a new\nsituation. We show that st-graphs generated by CURIE improve a situational\nreasoning end task (WIQA-QA) by 3 points on accuracy by simply augmenting their\ninput with our generated situational graphs, especially for a hard subset that\nrequires background knowledge and multi-hop reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:51:33 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 21:30:54 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Rajagopal", "Dheeraj", ""], ["Madaan", "Aman", ""], ["Tandon", "Niket", ""], ["Yang", "Yiming", ""], ["Prabhumoye", "Shrimai", ""], ["Ravichander", "Abhilasha", ""], ["Clark", "Peter", ""], ["Hovy", "Eduard", ""]]}, {"id": "2104.00824", "submitter": "David R Mortensen", "authors": "David R. Mortensen, Jordan Picone, Xinjian Li, and Kathleen Siminyu", "title": "Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered\n  Language for Universal Phone Recognition Experiments", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is growing interest in ASR systems that can recognize phones in a\nlanguage-independent fashion. There is additionally interest in building\nlanguage technologies for low-resource and endangered languages. However, there\nis a paucity of realistic data that can be used to test such systems and\ntechnologies. This paper presents a publicly available, phonetically\ntranscribed corpus of 2255 utterances (words and short phrases) in the\nendangered Tangkhulic language East Tusom (no ISO 639-3 code), a Tibeto-Burman\nlanguage variety spoken mostly in India. Because the dataset is transcribed in\nterms of phones, rather than phonemes, it is a better match for universal phone\nrecognition systems than many larger (phonemically transcribed) datasets. This\npaper describes the dataset and the methodology used to produce it. It further\npresents basic benchmarks of state-of-the-art universal phone recognition\nsystems on the dataset as baselines for future experiments.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:26:10 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mortensen", "David R.", ""], ["Picone", "Jordan", ""], ["Li", "Xinjian", ""], ["Siminyu", "Kathleen", ""]]}, {"id": "2104.00929", "submitter": "Changying Hao", "authors": "Changying Hao, Liang Pang, Yanyan Lan, Yan Wang, Jiafeng Guo, Xueqi\n  Cheng", "title": "Sketch and Customize: A Counterfactual Story Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent text generation models are easy to generate relevant and fluent text\nfor the given text, while lack of causal reasoning ability when we change some\nparts of the given text. Counterfactual story rewriting is a recently proposed\ntask to test the causal reasoning ability for text generation models, which\nrequires a model to predict the corresponding story ending when the condition\nis modified to a counterfactual one. Previous works have shown that the\ntraditional sequence-to-sequence model cannot well handle this problem, as it\noften captures some spurious correlations between the original and\ncounterfactual endings, instead of the causal relations between conditions and\nendings. To address this issue, we propose a sketch-and-customize generation\nmodel guided by the causality implicated in the conditions and endings. In the\nsketch stage, a skeleton is extracted by removing words which are conflict to\nthe counterfactual condition, from the original ending. In the customize stage,\na generation model is used to fill proper words in the skeleton under the\nguidance of the counterfactual condition. In this way, the obtained\ncounterfactual ending is both relevant to the original ending and consistent\nwith the counterfactual condition. Experimental results show that the proposed\nmodel generates much better endings, as compared with the traditional\nsequence-to-sequence model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:14:22 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hao", "Changying", ""], ["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Wang", "Yan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.00933", "submitter": "Avik Pal", "authors": "Aishwarya Gupta, Avik Pal, Bholeshwar Khurana, Lakshay Tyagi, Ashutosh\n  Modi", "title": "Humor@IITK at SemEval-2021 Task 7: Large Language Models for Quantifying\n  Humor and Offensiveness", "comments": "Accepted at SemEval 2021 Task 7, 7 Pages (6 Pages main content + 2\n  pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humor and Offense are highly subjective due to multiple word senses, cultural\nknowledge, and pragmatic competence. Hence, accurately detecting humorous and\noffensive texts has several compelling use cases in Recommendation Systems and\nPersonalized Content Moderation. However, due to the lack of an extensive\nlabeled dataset, most prior works in this domain haven't explored large neural\nmodels for subjective humor understanding. This paper explores whether large\nneural models and their ensembles can capture the intricacies associated with\nhumor/offense detection and rating. Our experiments on the SemEval-2021 Task 7:\nHaHackathon show that we can develop reasonable humor and offense detection\nsystems with such models. Our models are ranked third in subtask 1b and\nconsistently ranked around the top 33% of the leaderboard for the remaining\nsubtasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:22:02 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Gupta", "Aishwarya", ""], ["Pal", "Avik", ""], ["Khurana", "Bholeshwar", ""], ["Tyagi", "Lakshay", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.00952", "submitter": "Shaoxiong Ji", "authors": "Wei Sun and Shaoxiong Ji and Erik Cambria and Pekka Marttinen", "title": "Multitask Recalibrated Aggregation Network for Medical Code Prediction", "comments": "ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical coding translates professionally written medical reports into\nstandardized codes, which is an essential part of medical information systems\nand health insurance reimbursement. Manual coding by trained human coders is\ntime-consuming and error-prone. Thus, automated coding algorithms have been\ndeveloped, building especially on the recent advances in machine learning and\ndeep neural networks. To solve the challenges of encoding lengthy and noisy\nclinical documents and capturing code associations, we propose a multitask\nrecalibrated aggregation network. In particular, multitask learning shares\ninformation across different coding schemes and captures the dependencies\nbetween different medical codes. Feature recalibration and aggregation in\nshared modules enhance representation learning for lengthy notes. Experiments\nwith a real-world MIMIC-III dataset show significantly improved predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:22:10 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 10:22:39 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:11:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Sun", "Wei", ""], ["Ji", "Shaoxiong", ""], ["Cambria", "Erik", ""], ["Marttinen", "Pekka", ""]]}, {"id": "2104.00975", "submitter": "Gabriella Tognola", "authors": "Emma Chiaramello, Francesco Pinciroli, Alberico Bonalumi, Angelo\n  Caroli, Gabriella Tognola", "title": "Use of 'off-the-shelf' information extraction algorithms in clinical\n  informatics: a feasibility study of MetaMap annotation of Italian medical\n  notes", "comments": "This paper has been published in the Journal of biomedical\n  informatics, Volume 63, October 2016, Pages 22-32", "journal-ref": "Journal of biomedical informatics, Volume 63, October 2016, Pages\n  22-32", "doi": "10.1016/j.jbi.2016.07.017", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Information extraction from narrative clinical notes is useful for patient\ncare, as well as for secondary use of medical data, for research or clinical\npurposes. Many studies focused on information extraction from English clinical\ntexts, but less dealt with clinical notes in languages other than English. This\nstudy tested the feasibility of using 'off the shelf' information extraction\nalgorithms to identify medical concepts from Italian clinical notes. We used\nMetaMap to map medical concepts to the Unified Medical Language System (UMLS).\nThe study addressed two questions: (Q1) to understand if it would be possible\nto properly map medical terms found in clinical notes and related to the\nsemantic group of 'Disorders' to the Italian UMLS resources; (Q2) to\ninvestigate if it would be feasible to use MetaMap as it is to extract these\nmedical concepts from Italian clinical notes. Results in EXP1 showed that the\nItalian UMLS Metathesaurus sources covered 91% of the medical terms of the\n'Disorders' semantic group, as found in the studied dataset. Even if MetaMap\nwas built to analyze texts written in English, it worked properly also with\ntexts written in Italian. MetaMap identified correctly about half of the\nconcepts in the Italian clinical notes. Using MetaMap's annotation on Italian\nclinical notes instead of a simple text search improved our results of about 15\npercentage points. MetaMap showed recall, precision and F-measure of 0.53, 0.98\nand 0.69, respectively. Most of the failures were due to the impossibility for\nMetaMap to generate Italian meaningful variants. MetaMap's performance in\nannotating automatically translated English clinical notes was in line with\nfindings in the literature, with similar recall (0.75), F-measure (0.83) and\neven higher precision (0.95).\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 10:28:50 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chiaramello", "Emma", ""], ["Pinciroli", "Francesco", ""], ["Bonalumi", "Alberico", ""], ["Caroli", "Angelo", ""], ["Tognola", "Gabriella", ""]]}, {"id": "2104.00990", "submitter": "Arka Sadhu", "authors": "Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, Aniruddha\n  Kembhavi", "title": "Visual Semantic Role Labeling for Video Understanding", "comments": "CVPR21 camera-ready including appendix. Project Page at\n  https://vidsitu.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for understanding and representing related salient\nevents in a video using visual semantic role labeling. We represent videos as a\nset of related events, wherein each event consists of a verb and multiple\nentities that fulfill various roles relevant to that event. To study the\nchallenging task of semantic role labeling in videos or VidSRL, we introduce\nthe VidSitu benchmark, a large-scale video understanding data source with $29K$\n$10$-second movie clips richly annotated with a verb and semantic-roles every\n$2$ seconds. Entities are co-referenced across events within a movie clip and\nevents are connected to each other via event-event relations. Clips in VidSitu\nare drawn from a large collection of movies (${\\sim}3K$) and have been chosen\nto be both complex (${\\sim}4.2$ unique verbs within a video) as well as diverse\n(${\\sim}200$ verbs have more than $100$ annotations each). We provide a\ncomprehensive analysis of the dataset in comparison to other publicly available\nvideo understanding benchmarks, several illustrative baselines and evaluate a\nrange of standard video recognition models. Our code and dataset is available\nat vidsitu.org.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:23:22 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sadhu", "Arka", ""], ["Gupta", "Tanmay", ""], ["Yatskar", "Mark", ""], ["Nevatia", "Ram", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2104.00994", "submitter": "Siyuan Feng", "authors": "Siyuan Feng and Piotr \\.Zelasko and Laureano Moro-Vel\\'azquez and\n  Odette Scharenborg", "title": "Unsupervised Acoustic Unit Discovery by Leveraging a\n  Language-Independent Subword Discriminative Feature Representation", "comments": "Accepted for publication in INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles automatically discovering phone-like acoustic units (AUD)\nfrom unlabeled speech data. Past studies usually proposed single-step\napproaches. We propose a two-stage approach: the first stage learns a\nsubword-discriminative feature representation and the second stage applies\nclustering to the learned representation and obtains phone-like clusters as the\ndiscovered acoustic units. In the first stage, a recently proposed method in\nthe task of unsupervised subword modeling is improved by replacing a\nmonolingual out-of-domain (OOD) ASR system with a multilingual one to create a\nsubword-discriminative representation that is more language-independent. In the\nsecond stage, segment-level k-means is adopted, and two methods to represent\nthe variable-length speech segments as fixed-dimension feature vectors are\ncompared. Experiments on a very low-resource Mboshi language corpus show that\nour approach outperforms state-of-the-art AUD in both normalized mutual\ninformation (NMI) and F-score. The multilingual ASR improved upon the\nmonolingual ASR in providing OOD phone labels and in estimating the phone\nboundaries. A comparison of our systems with and without knowing the\nground-truth phone boundaries showed a 16% NMI performance gap, suggesting that\nthe current approach can significantly benefit from improved phone boundary\nestimation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:43:07 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 11:21:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Feng", "Siyuan", ""], ["\u017belasko", "Piotr", ""], ["Moro-Vel\u00e1zquez", "Laureano", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2104.01027", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko,\n  Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel\n  Synnaeve, Michael Auli", "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised\n  Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning of speech representations has been a very active\nresearch area but most work is focused on a single domain such as read audio\nbooks for which there exist large quantities of labeled and unlabeled data. In\nthis paper, we explore more general setups where the domain of the unlabeled\ndata for pre-training data differs from the domain of the labeled data for\nfine-tuning, which in turn may differ from the test data domain. Our\nexperiments show that using target domain data during pre-training leads to\nlarge performance improvements across a variety of setups. On a large-scale\ncompetitive setup, we show that pre-training on unlabeled in-domain data\nreduces the gap between models trained on in-domain and out-of-domain labeled\ndata by 66%-73%. This has obvious practical implications since it is much\neasier to obtain unlabeled target domain data than labeled data. Moreover, we\nfind that pre-training on multiple domains improves generalization performance\non domains not seen during training. Code and models will be made available at\nhttps://github.com/pytorch/fairseq.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 12:53:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Sriram", "Anuroop", ""], ["Baevski", "Alexei", ""], ["Likhomanenko", "Tatiana", ""], ["Xu", "Qiantong", ""], ["Pratap", "Vineel", ""], ["Kahn", "Jacob", ""], ["Lee", "Ann", ""], ["Collobert", "Ronan", ""], ["Synnaeve", "Gabriel", ""], ["Auli", "Michael", ""]]}, {"id": "2104.01037", "submitter": "Perceval Wajsburt", "authors": "Perceval Wajsburt, Yoann Taill\\'e, Xavier Tannier", "title": "Effect of depth order on iterative nested named entity recognition\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the effect of the order of depth of mention on nested\nnamed entity recognition (NER) models. NER is an essential task in the\nextraction of biomedical information, and nested entities are common since\nmedical concepts can assemble to form larger entities. Conventional NER systems\nonly predict disjointed entities. Thus, iterative models for nested NER use\nmultiple predictions to enumerate all entities, imposing a predefined order\nfrom largest to smallest or smallest to largest. We design an order-agnostic\niterative model and a procedure to choose a custom order during training and\nprediction. To accommodate for this task, we propose a modification of the\nTransformer architecture to take into account the entities predicted in the\nprevious steps. We provide a set of experiments to study the model's\ncapabilities and the effects of the order on performance. Finally, we show that\nthe smallest to largest order gives the best results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 13:18:52 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wajsburt", "Perceval", ""], ["Taill\u00e9", "Yoann", ""], ["Tannier", "Xavier", ""]]}, {"id": "2104.01046", "submitter": "Sagnik Mukherjee", "authors": "Neil Rajiv Shirude, Sagnik Mukherjee, Tushar Shandhilya, Ananta\n  Mukherjee, Ashutosh Modi", "title": "IITK@LCP at SemEval 2021 Task 1: Classification for Lexical Complexity\n  Regression Task", "comments": "Accepted at SemEval 2021 Task 1, 7 Pages (5 Pages main content+ 2\n  pages for reference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes our contribution to SemEval 2021 Task 1: Lexical\nComplexity Prediction. In our approach, we leverage the ELECTRA model and\nattempt to mirror the data annotation scheme. Although the task is a regression\ntask, we show that we can treat it as an aggregation of several classification\nand regression models. This somewhat counter-intuitive approach achieved an MAE\nscore of 0.0654 for Sub-Task 1 and MAE of 0.0811 on Sub-Task 2. Additionally,\nwe used the concept of weak supervision signals from Gloss-BERT in our work,\nand it significantly improved the MAE score in Sub-Task 1.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 13:40:12 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Shirude", "Neil Rajiv", ""], ["Mukherjee", "Sagnik", ""], ["Shandhilya", "Tushar", ""], ["Mukherjee", "Ananta", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01083", "submitter": "Mark Anderson", "authors": "Mark Anderson and Carlos G\\'omez-Rodr\\'iguez", "title": "What Taggers Fail to Learn, Parsers Need the Most", "comments": "Due to be published in the proceedings of the 23rd Nordic Conference\n  on Computational Linguistics (NoDaLiDa 2021). Previously rejected at the 2021\n  Conference of the European Chapter of the Association for Computational\n  Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an error analysis of neural UPOS taggers to evaluate why using\ngold standard tags has such a large positive contribution to parsing\nperformance while using predicted UPOS tags either harms performance or offers\na negligible improvement. We evaluate what neural dependency parsers implicitly\nlearn about word types and how this relates to the errors taggers make to\nexplain the minimal impact using predicted tags has on parsers. We also present\na short analysis on what contexts result in reductions in tagging performance.\nWe then mask UPOS tags based on errors made by taggers to tease away the\ncontribution of UPOS tags which taggers succeed and fail to classify correctly\nand the impact of tagging errors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:04:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Anderson", "Mark", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2104.01099", "submitter": "Julian Eisenschlos", "authors": "Thomas M\\\"uller, Julian Martin Eisenschlos, Syrine Krichene", "title": "TAPAS at SemEval-2021 Task 9: Reasoning over tables with intermediate\n  pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the TAPAS contribution to the Shared Task on Statement\nVerification and Evidence Finding with Tables (SemEval 2021 Task 9, Wang et al.\n(2021)). SEM TAB FACT Task A is a classification task of recognizing if a\nstatement is entailed, neutral or refuted by the content of a given table. We\nadopt the binary TAPAS model of Eisenschlos et al. (2020) to this task. We\nlearn two binary classification models: A first model to predict if a statement\nis neutral or non-neutral and a second one to predict if it is entailed or\nrefuted. As the shared task training set contains only entailed or refuted\nexamples, we generate artificial neutral examples to train the first model.\nBoth models are pre-trained using a MASKLM objective, intermediate\ncounter-factual and synthetic data (Eisenschlos et al., 2020) and TABFACT (Chen\net al., 2020), a large table entailment dataset. We find that the artificial\nneutral examples are somewhat effective at training the first model, achieving\n68.03 test F1 versus the 60.47 of a majority baseline. For the second stage, we\nfind that the pre-training on the intermediate data and TABFACT improves the\nresults over MASKLM pre-training (68.03 vs 57.01).\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:47:08 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["M\u00fcller", "Thomas", ""], ["Eisenschlos", "Julian Martin", ""], ["Krichene", "Syrine", ""]]}, {"id": "2104.01117", "submitter": "Sami Diaf", "authors": "Sami Diaf and Ulrich Fritsche", "title": "Topic Scaling: A Joint Document Scaling -- Topic Model Approach To Learn\n  Time-Specific Topics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new methodology to study sequential corpora by\nimplementing a two-stage algorithm that learns time-based topics with respect\nto a scale of document positions and introduces the concept of Topic Scaling\nwhich ranks learned topics within the same document scale. The first stage\nranks documents using Wordfish, a Poisson-based document scaling method, to\nestimate document positions that serve, in the second stage, as a dependent\nvariable to learn relevant topics via a supervised Latent Dirichlet Allocation.\nThis novelty brings two innovations in text mining as it explains document\npositions, whose scale is a latent variable, and ranks the inferred topics on\nthe document scale to match their occurrences within the corpus and track their\nevolution. Tested on the U.S. State Of The Union two-party addresses, this\ninductive approach reveals that each party dominates one end of the learned\nscale with interchangeable transitions that follow the parties' term of office.\nBesides a demonstrated high accuracy in predicting in-sample documents'\npositions from topic scores, this method reveals further hidden topics that\ndifferentiate similar documents by increasing the number of learned topics to\nunfold potential nested hierarchical topic structures. Compared to other\npopular topic models, Topic Scaling learns topics with respect to document\nsimilarities without specifying a time frequency to learn topic evolution, thus\ncapturing broader topic patterns than dynamic topic models and yielding more\ninterpretable outputs than a plain latent Dirichlet allocation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:35:36 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Diaf", "Sami", ""], ["Fritsche", "Ulrich", ""]]}, {"id": "2104.01131", "submitter": "Ridam Pal", "authors": "Harshita Chopra, Aniket Vashishtha, Ridam Pal, Ashima, Ananya Tyagi\n  and Tavpritesh Sethi", "title": "Mining Trends of COVID-19 Vaccine Beliefs on Twitter with Lexical\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media plays a pivotal role in disseminating news globally and acts as\na platform for people to express their opinions on various topics. A wide\nvariety of views accompanies COVID-19 vaccination drives across the globe,\noften colored by emotions, which change along with rising cases, approval of\nvaccines, and multiple factors discussed online. This study aims at analyzing\nthe temporal evolution of different Emotion categories: Hesitation, Rage,\nSorrow, Anticipation, Faith, and Contentment with Influencing Factors: Vaccine\nRollout, Misinformation, Health Effects, and Inequities as lexical categories\ncreated from Tweets belonging to five countries with vital vaccine roll-out\nprograms, namely, India, United States of America, Brazil, United Kingdom, and\nAustralia. We extracted a corpus of nearly 1.8 million Twitter posts related to\nCOVID-19 vaccination. Using cosine distance from selected seed words, we\nexpanded the vocabulary of each category and tracked the longitudinal change in\ntheir strength from June 2020 to April 2021. We used community detection\nalgorithms to find modules in positive correlation networks. Our findings\nsuggest that tweets expressing hesitancy towards vaccines contain the highest\nmentions of health-related effects in all countries. Our results indicated that\nthe patterns of hesitancy were variable across geographies and can help us\nlearn targeted interventions. We also observed a significant change in the\nlinear trends of categories like hesitation and contentment before and after\napproval of vaccines. Negative emotions like rage and sorrow gained the highest\nimportance in the alluvial diagram. They formed a significant module with all\nthe influencing factors in April 2021, when India observed the second wave of\nCOVID-19 cases. The relationship between Emotions and Influencing Factors was\nfound to be variable across the countries.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:13:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 04:06:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chopra", "Harshita", ""], ["Vashishtha", "Aniket", ""], ["Pal", "Ridam", ""], ["Ashima", "", ""], ["Tyagi", "Ananya", ""], ["Sethi", "Tavpritesh", ""]]}, {"id": "2104.01207", "submitter": "Sarthak Dash", "authors": "Sarthak Dash, Nandana Mihindukulasooriya, Alfio Gliozzo, Mustafa Canim", "title": "Type Prediction Systems", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring semantic types for entity mentions within text documents is an\nimportant asset for many downstream NLP tasks, such as Semantic Role Labelling,\nEntity Disambiguation, Knowledge Base Question Answering, etc. Prior works have\nmostly focused on supervised solutions that generally operate on relatively\nsmall-to-medium-sized type systems. In this work, we describe two systems aimed\nat predicting type information for the following two tasks, namely, a\nTypeSuggest module, an unsupervised system designed to predict types for a set\nof user-entered query terms, and an Answer Type prediction module, that\nprovides a solution for the task of determining the correct type of the answer\nexpected to a given query. Our systems generalize to arbitrary type systems of\nany sizes, thereby making it a highly appealing solution to extract type\ninformation at any granularity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 19:16:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dash", "Sarthak", ""], ["Mihindukulasooriya", "Nandana", ""], ["Gliozzo", "Alfio", ""], ["Canim", "Mustafa", ""]]}, {"id": "2104.01215", "submitter": "Lynnette Hui Xian Ng", "authors": "Lynnette Hui Xian Ng and Kathleen M. Carley", "title": "The Coronavirus is a Bioweapon: Analysing Coronavirus Fact-Checked\n  Stories", "comments": null, "journal-ref": "SBP-Brims 2020 COVID Special Track", "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The 2020 coronavirus pandemic has heightened the need to flag\ncoronavirus-related misinformation, and fact-checking groups have taken to\nverifying misinformation on the Internet. We explore stories reported by\nfact-checking groups PolitiFact, Poynter and Snopes from January to June 2020,\ncharacterising them into six story clusters before then analyse time-series and\nstory validity trends and the level of agreement across sites. We further break\ndown the story clusters into more granular story types by proposing a unique\nautomated method with a BERT classifier, which can be used to classify diverse\nstory sources, in both fact-checked stories and tweets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 19:27:53 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ng", "Lynnette Hui Xian", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "2104.01264", "submitter": "Qingyun Dou", "authors": "Qingyun Dou, Yiting Lu, Potsawee Manakul, Xixin Wu, Mark J. F. Gales", "title": "Attention Forcing for Machine Translation", "comments": "arXiv admin note: text overlap with arXiv:1909.12289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Auto-regressive sequence-to-sequence models with attention mechanisms have\nachieved state-of-the-art performance in various tasks including Text-To-Speech\n(TTS) and Neural Machine Translation (NMT). The standard training approach,\nteacher forcing, guides a model with the reference output history. At inference\nstage, the generated output history must be used. This mismatch can impact\nperformance. However, it is highly challenging to train the model using the\ngenerated output. Several approaches have been proposed to address this\nproblem, normally by selectively using the generated output history. To make\ntraining stable, these approaches often require a heuristic schedule or an\nauxiliary classifier. This paper introduces attention forcing for NMT. This\napproach guides the model with the generated output history and reference\nattention, and can reduce the training-inference mismatch without a schedule or\na classifier. Attention forcing has been successful in TTS, but its application\nto NMT is more challenging, due to the discrete and multi-modal nature of the\noutput space. To tackle this problem, this paper adds a selection scheme to\nvanilla attention forcing, which automatically selects a suitable training\napproach for each pair of training data. Experiments show that attention\nforcing can improve the overall translation quality and the diversity of the\ntranslations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:33:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dou", "Qingyun", ""], ["Lu", "Yiting", ""], ["Manakul", "Potsawee", ""], ["Wu", "Xixin", ""], ["Gales", "Mark J. F.", ""]]}, {"id": "2104.01287", "submitter": "Akshat Gupta", "authors": "Akshat Gupta, Sai Krishna Rallabandi, Alan W Black", "title": "Intent Recognition and Unsupervised Slot Identification for Low\n  Resourced Spoken Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intent Recognition and Slot Identification are crucial components in spoken\nlanguage understanding (SLU) systems. In this paper, we present a novel\napproach towards both these tasks in the context of low resourced and unwritten\nlanguages. We present an acoustic based SLU system that converts speech to its\nphonetic transcription using a universal phone recognition system. We build a\nword-free natural language understanding module that does intent recognition\nand slot identification from these phonetic transcription. Our proposed SLU\nsystem performs competitively for resource rich scenarios and significantly\noutperforms existing approaches as the amount of available data reduces. We\nobserve more than 10% improvement for intent classification in Tamil and more\nthan 5% improvement for intent classification in Sinhala. We also present a\nnovel approach towards unsupervised slot identification using normalized\nattention scores. This approach can be used for unsupervised slot labelling,\ndata augmentation and to generate data for a new slot in a one-shot way with\nonly one speech recording\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 01:58:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gupta", "Akshat", ""], ["Rallabandi", "Sai Krishna", ""], ["Black", "Alan W", ""]]}, {"id": "2104.01290", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn and Tom Coupe and Benjamin Adams", "title": "Measuring Linguistic Diversity During COVID-19", "comments": null, "journal-ref": "Proceedings of the 4th Workshop on NLP and Computational Social\n  Science (2020)", "doi": "10.18653/v1/P17", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational measures of linguistic diversity help us understand the\nlinguistic landscape using digital language data. The contribution of this\npaper is to calibrate measures of linguistic diversity using restrictions on\ninternational travel resulting from the COVID-19 pandemic. Previous work has\nmapped the distribution of languages using geo-referenced social media and web\ndata. The goal, however, has been to describe these corpora themselves rather\nthan to make inferences about underlying populations. This paper shows that a\ndifference-in-differences method based on the Herfindahl-Hirschman Index can\nidentify the bias in digital corpora that is introduced by non-local\npopulations. These methods tell us where significant changes have taken place\nand whether this leads to increased or decreased diversity. This is an\nimportant step in aligning digital corpora like social media with the\nreal-world populations that have produced them.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 02:09:37 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dunn", "Jonathan", ""], ["Coupe", "Tom", ""], ["Adams", "Benjamin", ""]]}, {"id": "2104.01291", "submitter": "Bowen Shi", "authors": "Bowen Shi, Diane Brentari, Greg Shakhnarovich, Karen Livescu", "title": "Fingerspelling Detection in American Sign Language", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerspelling, in which words are signed letter by letter, is an important\ncomponent of American Sign Language. Most previous work on automatic\nfingerspelling recognition has assumed that the boundaries of fingerspelling\nregions in signing videos are known beforehand. In this paper, we consider the\ntask of fingerspelling detection in raw, untrimmed sign language videos. This\nis an important step towards building real-world fingerspelling recognition\nsystems. We propose a benchmark and a suite of evaluation metrics, some of\nwhich reflect the effect of detection on the downstream fingerspelling\nrecognition task. In addition, we propose a new model that learns to detect\nfingerspelling via multi-task training, incorporating pose estimation and\nfingerspelling recognition (transcription) along with detection, and compare\nthis model to several alternatives. The model outperforms all alternative\napproaches across all metrics, establishing a state of the art on the\nbenchmark.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 02:11:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shi", "Bowen", ""], ["Brentari", "Diane", ""], ["Shakhnarovich", "Greg", ""], ["Livescu", "Karen", ""]]}, {"id": "2104.01294", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Representations of Language Varieties Are Reliable Given Corpus\n  Similarity Measures", "comments": null, "journal-ref": "Proceedings of the Eighth Workshop on NLP for Similar Languages,\n  Varieties, and Dialects (2021)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper measures similarity both within and between 84 language varieties\nacross nine languages. These corpora are drawn from digital sources (the web\nand tweets), allowing us to evaluate whether such geo-referenced corpora are\nreliable for modelling linguistic variation. The basic idea is that, if each\nsource adequately represents a single underlying language variety, then the\nsimilarity between these sources should be stable across all languages and\ncountries. The paper shows that there is a consistent agreement between these\nsources using frequency-based corpus similarity measures. This provides further\nevidence that digital geo-referenced corpora consistently represent local\nlanguage varieties.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 02:19:46 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "2104.01297", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Multi-Unit Directional Measures of Association: Moving Beyond Pairs of\n  Words", "comments": null, "journal-ref": "International Journal of Corpus Linguistics (2018)", "doi": "10.1075/ijcl.16098.dun", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper formulates and evaluates a series of multi-unit measures of\ndirectional association, building on the pairwise {\\Delta}P measure, that are\nable to quantify association in sequences of varying length and type of\nrepresentation. Multi-unit measures face an additional segmentation problem:\nonce the implicit length constraint of pairwise measures is abandoned,\nassociation measures must also identify the borders of meaningful sequences.\nThis paper takes a vector-based approach to the segmentation problem by using\n18 unique measures to describe different aspects of multi-unit association. An\nexamination of these measures across eight languages shows that they are stable\nacross languages and that each provides a unique rank of associated sequences.\nTaken together, these measures expand corpus-based approaches to association by\ngeneralizing across varying lengths and types of representation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 02:43:24 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "2104.01299", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Finding Variants for Construction-Based Dialectometry: A Corpus-Based\n  Approach to Regional CxGs", "comments": null, "journal-ref": "Cognitive Linguistics (2018)", "doi": "10.1515/cog-2017-0029", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops a construction-based dialectometry capable of identifying\npreviously unknown constructions and measuring the degree to which a given\nconstruction is subject to regional variation. The central idea is to learn a\ngrammar of constructions (a CxG) using construction grammar induction and then\nto use these constructions as features for dialectometry. This offers a method\nfor measuring the aggregate similarity between regional CxGs without limiting\nin advance the set of constructions subject to variation. The learned CxG is\nevaluated on how well it describes held-out test corpora while dialectometry is\nevaluated on how well it can model regional varieties of English. Themethod is\ntested using two distinct datasets: First, the International Corpus of English\nrepresenting eight outer circle varieties; Second, a web-crawled corpus\nrepresenting five inner circle varieties. Results show that themethod (1)\nproduces a grammar with stable quality across sub-sets of a single corpus that\nis (2) capable of distinguishing between regional varieties of Englishwith a\nhigh degree of accuracy, thus (3) supporting dialectometricmethods formeasuring\nthe similarity between varieties of English and (4) measuring the degree to\nwhich each construction is subject to regional variation. This is important for\ncognitive sociolinguistics because it operationalizes the idea that competition\nbetween constructions is organized at the functional level so that\ndialectometry needs to represent as much of the available functional space as\npossible.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 02:52:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "2104.01306", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn", "title": "Global Syntactic Variation in Seven Languages: Towards a Computational\n  Dialectology", "comments": null, "journal-ref": "Frontiers in Artificial Intelligence: Language and Computation\n  (2019)", "doi": "10.3389/frai.2019.00015", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this paper is to provide a complete representation of regional\nlinguistic variation on a global scale. To this end, the paper focuses on\nremoving three constraints that have previously limited work within\ndialectology/dialectometry. First, rather than assuming a fixed and incomplete\nset of variants, we use Computational Construction Grammar to provide a\nreplicable and falsifiable set of syntactic features. Second, rather than\nassuming a specific area of interest, we use global language mapping based on\nweb-crawled and social media datasets to determine the selection of national\nvarieties. Third, rather than looking at a single language in isolation, we\nmodel seven major languages together using the same methods: Arabic, English,\nFrench, German, Portuguese, Russian, and Spanish. Results show that models for\neach language are able to robustly predict the region-of-origin of held-out\nsamples better using Construction Grammars than using simpler syntactic\nfeatures. These global-scale experiments are used to argue that new methods in\ncomputational sociolinguistics are able to provide more generalized models of\nregional variation that are essential for understanding language variation and\nchange at scale.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 03:40:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dunn", "Jonathan", ""]]}, {"id": "2104.01363", "submitter": "Diego Krivochen", "authors": "Diego Gabriel Krivochen", "title": "From n-grams to trees in Lindenmayer systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present two approaches to Lindenmayer systems: the\nrule-based (or generative) approach, which focuses on L-systems as Thue\nrewriting systems and a constraint-based (or model-theoretic) approach, in\nwhich rules are abandoned in favour of conditions over allowable expressions in\nthe language (Pullum, 2019). We will argue that it is possible, for at least a\nsubset of L-systems and the languages they generate, to map string\nadmissibility conditions (the 'Three Laws') to local tree admissibility\nconditions (cf. Rogers, 1997). This is equivalent to defining a model for those\nlanguages. We will work out how to construct structure assuming only\nsuperficial constraints on expressions, and define a set of constraints that\nwell-formed expressions of specific L-languages must satisfy. We will see that\nL-systems that other methods distinguish turn out to satisfy the same model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:42:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Krivochen", "Diego Gabriel", ""]]}, {"id": "2104.01364", "submitter": "Sabhay Jain", "authors": "Akash Gangwar, Sabhay Jain, Shubham Sourav, Ashutosh Modi", "title": "Counts@IITK at SemEval-2021 Task 8: SciBERT Based Entity And Semantic\n  Relation Extraction For Scientific Data", "comments": "Accepted at SemEval 2021 Task 8, 7 Pages (5 Pages main content + 1\n  page for references + 1 Page Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the system for SemEval 2021 Task 8 (MeasEval). MeasEval\nis a novel span extraction, classification, and relation extraction task\nfocused on finding quantities, attributes of these quantities, and additional\ninformation, including the related measured entities, properties, and\nmeasurement contexts. Our submitted system, which placed fifth (team rank) on\nthe leaderboard, consisted of SciBERT with [CLS] token embedding and CRF layer\non top. We were also placed first in Quantity (tied) and Unit subtasks, second\nin MeasuredEntity, Modifier and Qualifies subtasks, and third in Qualifier\nsubtask.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:47:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gangwar", "Akash", ""], ["Jain", "Sabhay", ""], ["Sourav", "Shubham", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01371", "submitter": "Hayate Iso", "authors": "Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis,\n  Wang-Chiew Tan", "title": "Convex Aggregation for Opinion Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for unsupervised opinion summarization have predominantly\nused the review reconstruction training paradigm. An encoder-decoder model is\ntrained to reconstruct single reviews and learns a latent review encoding\nspace. At summarization time, the unweighted average of latent review vectors\nis decoded into a summary. In this paper, we challenge the convention of simply\naveraging the latent vector set, and claim that this simplistic approach fails\nto consider variations in the quality of input reviews or the idiosyncrasies of\nthe decoder. We propose Coop, a convex vector aggregation framework for opinion\nsummarization, that searches for better combinations of input reviews. Coop\nrequires no further supervision and uses a simple word overlap objective to\nhelp the model generate summaries that are more consistent with input reviews.\nExperimental results show that extending opinion summarizers with Coop results\nin state-of-the-art performance, with ROUGE-1 improvements of 3.7% and 2.9% on\nthe Yelp and Amazon benchmark datasets, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 10:52:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Iso", "Hayate", ""], ["Wang", "Xiaolan", ""], ["Suhara", "Yoshihiko", ""], ["Angelidis", "Stefanos", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2104.01378", "submitter": "Junbo Zhang", "authors": "Junbo Zhang, Zhiwen Zhang, Yongqing Wang, Zhiyong Yan, Qiong Song,\n  Yukai Huang, Ke Li, Daniel Povey and Yujun Wang", "title": "speechocean762: An Open-Source Non-native English Speech Corpus For\n  Pronunciation Assessment", "comments": "Accepted in INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new open-source speech corpus named \"speechocean762\"\ndesigned for pronunciation assessment use, consisting of 5000 English\nutterances from 250 non-native speakers, where half of the speakers are\nchildren. Five experts annotated each of the utterances at sentence-level,\nword-level and phoneme-level. A baseline system is released in open source to\nillustrate the phoneme-level pronunciation assessment workflow on this corpus.\nThis corpus is allowed to be used freely for commercial and non-commercial\npurposes. It is available for free download from OpenSLR, and the corresponding\nbaseline system is published in the Kaldi speech recognition toolkit.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 11:31:59 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:12:02 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhang", "Junbo", ""], ["Zhang", "Zhiwen", ""], ["Wang", "Yongqing", ""], ["Yan", "Zhiyong", ""], ["Song", "Qiong", ""], ["Huang", "Yukai", ""], ["Li", "Ke", ""], ["Povey", "Daniel", ""], ["Wang", "Yujun", ""]]}, {"id": "2104.01384", "submitter": "Hiromitsu Nishizaki", "authors": "Yu Wang, Chee Siang Leow, Akio Kobayashi, Takehito Utsuro, Hiromitsu\n  Nishizaki", "title": "ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit\n  of Kaldi", "comments": "Submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of open-source software is playing a remarkable role in\nautomatic speech recognition (ASR). Kaldi, for instance, is widely used to\ndevelop state-of-the-art offline and online ASR systems. This paper describes\nthe \"ExKaldi-RT,\" online ASR toolkit implemented based on Kaldi and Python\nlanguage. ExKaldi-RT provides tools for providing a real-time audio stream\npipeline, extracting acoustic features, transmitting packets with a remote\nconnection, estimating acoustic probabilities with a neural network, and online\ndecoding. While similar functions are available built on Kaldi, a key feature\nof ExKaldi-RT is completely working on Python language, which has an\neasy-to-use interface for online ASR system developers to exploit original\nresearch, for example, by applying neural network-based signal processing and\nacoustic model trained with deep learning frameworks. We performed benchmark\nexperiments on the minimum LibriSpeech corpus, and showed that ExKaldi-RT could\nachieve competitive ASR performance in real-time.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 12:16:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Yu", ""], ["Leow", "Chee Siang", ""], ["Kobayashi", "Akio", ""], ["Utsuro", "Takehito", ""], ["Nishizaki", "Hiromitsu", ""]]}, {"id": "2104.01393", "submitter": "Shigehiko Schamoni", "authors": "Tsz Kin Lam, Mayumi Ohta, Shigehiko Schamoni, Stefan Riezler", "title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR", "comments": "Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an on-the-fly data augmentation method for automatic speech\nrecognition (ASR) that uses alignment information to generate effective\ntraining samples. Our method, called Aligned Data Augmentation (ADA) for ASR,\nreplaces transcribed tokens and the speech representations in an aligned manner\nto generate previously unseen training pairs. The speech representations are\nsampled from an audio dictionary that has been extracted from the training\ncorpus and inject speaker variations into the training examples. The\ntranscribed tokens are either predicted by a language model such that the\naugmented data pairs are semantically close to the original data, or randomly\nsampled. Both strategies result in training pairs that improve robustness in\nASR training. Our experiments on a Seq-to-Seq architecture show that ADA can be\napplied on top of SpecAugment, and achieves about 9-23% and 4-15% relative\nimprovements in WER over SpecAugment alone on LibriSpeech 100h and LibriSpeech\n960h test datasets, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:00:00 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 19:48:46 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lam", "Tsz Kin", ""], ["Ohta", "Mayumi", ""], ["Schamoni", "Shigehiko", ""], ["Riezler", "Stefan", ""]]}, {"id": "2104.01394", "submitter": "Viraj Bagal", "authors": "Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, U Deva\n  Priyakumar, CV Jawahar", "title": "MMBERT: Multimodal BERT Pretraining for Improved Medical VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images in the medical domain are fundamentally different from the general\ndomain images. Consequently, it is infeasible to directly employ general domain\nVisual Question Answering (VQA) models for the medical domain. Additionally,\nmedical images annotation is a costly and time-consuming process. To overcome\nthese limitations, we propose a solution inspired by self-supervised\npretraining of Transformer-style architectures for NLP, Vision and Language\ntasks. Our method involves learning richer medical image and text semantic\nrepresentations using Masked Language Modeling (MLM) with image features as the\npretext task on a large medical image+caption dataset. The proposed solution\nachieves new state-of-the-art performance on two VQA datasets for radiology\nimages -- VQA-Med 2019 and VQA-RAD, outperforming even the ensemble models of\nprevious best solutions. Moreover, our solution provides attention maps which\nhelp in model interpretability. The code is available at\nhttps://github.com/VirajBagal/MMBERT\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:01:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Khare", "Yash", ""], ["Bagal", "Viraj", ""], ["Mathew", "Minesh", ""], ["Devi", "Adithi", ""], ["Priyakumar", "U Deva", ""], ["Jawahar", "CV", ""]]}, {"id": "2104.01408", "submitter": "Rui Liu", "authors": "Rui Liu, Berrak Sisman, Haizhou Li", "title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with\n  Improved Emotion Discriminability", "comments": "5 pages, 4 figures, in Proceedings of INTERSPEECH 2021 conference,\n  Speech Samples: https://ttslr.github.io/i-ETTS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional text-to-speech synthesis (ETTS) has seen much progress in recent\nyears. However, the generated voice is often not perceptually identifiable by\nits intended emotion category. To address this problem, we propose a new\ninteractive training paradigm for ETTS, denoted as i-ETTS, which seeks to\ndirectly improve the emotion discriminability by interacting with a speech\nemotion recognition (SER) model. Moreover, we formulate an iterative training\nstrategy with reinforcement learning to ensure the quality of i-ETTS\noptimization. Experimental results demonstrate that the proposed i-ETTS\noutperforms the state-of-the-art baselines by rendering speech with more\naccurate emotion style. To our best knowledge, this is the first study of\nreinforcement learning in emotional text-to-speech synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:52:47 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 02:14:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Rui", ""], ["Sisman", "Berrak", ""], ["Li", "Haizhou", ""]]}, {"id": "2104.01436", "submitter": "Samujjwal Ghosh", "authors": "Samujjwal Ghosh, Subhadeep Maji, Maunendra Sankar Desarkar", "title": "Unsupervised Domain Adaptation with Global and Local Graph Neural\n  Networks in Limited Labeled Data Scenario: Application to Disaster Management", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and categorization of social media posts generated during\ndisasters are crucial to reduce the sufferings of the affected people. However,\nlack of labeled data is a significant bottleneck in learning an effective\ncategorization system for a disaster. This motivates us to study the problem as\nunsupervised domain adaptation (UDA) between a previous disaster with labeled\ndata (source) and a current disaster (target). However, if the amount of\nlabeled data available is limited, it restricts the learning capabilities of\nthe model. To handle this challenge, we utilize limited labeled data along with\nabundantly available unlabeled data, generated during a source disaster to\npropose a novel two-part graph neural network. The first-part extracts\ndomain-agnostic global information by constructing a token level graph across\ndomains and the second-part preserves local instance-level semantics. In our\nexperiments, we show that the proposed method outperforms state-of-the-art\ntechniques by $2.74\\%$ weighted F$_1$ score on average on two standard public\ndataset in the area of disaster management. We also report experimental results\nfor granular actionable multi-label classification datasets in disaster domain\nfor the first time, on which we outperform BERT by $3.00\\%$ on average w.r.t\nweighted F$_1$. Additionally, we show that our approach can retain performance\nwhen very limited labeled data is available.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 16:01:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ghosh", "Samujjwal", ""], ["Maji", "Subhadeep", ""], ["Desarkar", "Maunendra Sankar", ""]]}, {"id": "2104.01443", "submitter": "Imane Guellil", "authors": "Imane Guellil and Ahsan Adeel and Faical Azouaou and Mohamed Boubred\n  and Yousra Houichi and Akram Abdelhaq Moumna", "title": "Sexism detection: The first corpus in Algerian dialect with a\n  code-switching in Arabic/ French and English", "comments": "This paper was accepted at the AfricanNLP workshop (EACL conference\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an approach for hate speech detection against women in Arabic\ncommunity on social media (e.g. Youtube) is proposed. In the literature,\nsimilar works have been presented for other languages such as English. However,\nto the best of our knowledge, not much work has been conducted in the Arabic\nlanguage. A new hate speech corpus (Arabic\\_fr\\_en) is developed using three\ndifferent annotators. For corpus validation, three different machine learning\nalgorithms are used, including deep Convolutional Neural Network (CNN), long\nshort-term memory (LSTM) network and Bi-directional LSTM (Bi-LSTM) network.\nSimulation results demonstrate the best performance of the CNN model, which\nachieved F1-score up to 86\\% for the unbalanced corpus as compared to LSTM and\nBi-LSTM.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 16:34:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Guellil", "Imane", ""], ["Adeel", "Ahsan", ""], ["Azouaou", "Faical", ""], ["Boubred", "Mohamed", ""], ["Houichi", "Yousra", ""], ["Moumna", "Akram Abdelhaq", ""]]}, {"id": "2104.01454", "submitter": "Mark Mazumder", "authors": "Mark Mazumder, Colby Banbury, Josh Meyer, Pete Warden, Vijay Janapa\n  Reddi", "title": "Few-Shot Keyword Spotting in Any Language", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 85.2% with a false acceptance rate of 1.2%, and observe\npromising initial results on keyword search.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 17:27:37 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 15:48:01 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 18:58:44 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mazumder", "Mark", ""], ["Banbury", "Colby", ""], ["Meyer", "Josh", ""], ["Warden", "Pete", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "2104.01477", "submitter": "Hosein Mohebbi", "authors": "Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar", "title": "Exploring the Role of BERT Token Representations to Explain Sentence\n  Probing Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies have been carried out on revealing linguistic features\ncaptured by BERT. This is usually achieved by training a diagnostic classifier\non the representations obtained from different layers of BERT. The subsequent\nclassification accuracy is then interpreted as the ability of the model in\nencoding the corresponding linguistic property. Despite providing insights,\nthese studies have left out the potential role of token representations. In\nthis paper, we provide an analysis on the representation space of BERT in\nsearch for distinct and meaningful subspaces that can explain probing results.\nBased on a set of probing tasks and with the help of attribution methods we\nshow that BERT tends to encode meaningful knowledge in specific token\nrepresentations (which are often ignored in standard classification setups),\nallowing the model to detect syntactic and semantic abnormalities, and to\ndistinctively separate grammatical number and tense subspaces.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 20:40:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mohebbi", "Hosein", ""], ["Modarressi", "Ali", ""], ["Pilehvar", "Mohammad Taher", ""]]}, {"id": "2104.01488", "submitter": "Chuan Lei", "authors": "Alina Vretinaris, Chuan Lei, Vasilis Efthymiou, Xiao Qin, Fatma\n  \\\"Ozcan", "title": "Medical Entity Disambiguation Using Graph Neural Networks", "comments": "To appear in SIGMOD 2021", "journal-ref": null, "doi": "10.1145/3448016.3457328", "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical knowledge bases (KBs), distilled from biomedical literature and\nregulatory actions, are expected to provide high-quality information to\nfacilitate clinical decision making. Entity disambiguation (also referred to as\nentity linking) is considered as an essential task in unlocking the wealth of\nsuch medical KBs. However, existing medical entity disambiguation methods are\nnot adequate due to word discrepancies between the entities in the KB and the\ntext snippets in the source documents. Recently, graph neural networks (GNNs)\nhave proven to be very effective and provide state-of-the-art results for many\nreal-world applications with graph-structured data. In this paper, we introduce\nED-GNN based on three representative GNNs (GraphSAGE, R-GCN, and MAGNN) for\nmedical entity disambiguation. We develop two optimization techniques to\nfine-tune and improve ED-GNN. First, we introduce a novel strategy to represent\nentities that are mentioned in text snippets as a query graph. Second, we\ndesign an effective negative sampling strategy that identifies hard negative\nsamples to improve the model's disambiguation capability. Compared to the best\nperforming state-of-the-art solutions, our ED-GNN offers an average improvement\nof 7.3% in terms of F1 score on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:04:15 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vretinaris", "Alina", ""], ["Lei", "Chuan", ""], ["Efthymiou", "Vasilis", ""], ["Qin", "Xiao", ""], ["\u00d6zcan", "Fatma", ""]]}, {"id": "2104.01522", "submitter": "Zhengkun Tian", "authors": "Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai Zhang, Zhengqi\n  Wen, Xuefei Liu", "title": "TSNAT: Two-Step Non-Autoregressvie Transformer Models for Speech\n  Recognition", "comments": "Submitted to Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autoregressive (AR) models, such as attention-based encoder-decoder\nmodels and RNN-Transducer, have achieved great success in speech recognition.\nThey predict the output sequence conditioned on the previous tokens and\nacoustic encoded states, which is inefficient on GPUs. The non-autoregressive\n(NAR) models can get rid of the temporal dependency between the output tokens\nand predict the entire output tokens in at least one step. However, the NAR\nmodel still faces two major problems. On the one hand, there is still a great\ngap in performance between the NAR models and the advanced AR models. On the\nother hand, it's difficult for most of the NAR models to train and converge. To\naddress these two problems, we propose a new model named the two-step\nnon-autoregressive transformer(TSNAT), which improves the performance and\naccelerating the convergence of the NAR model by learning prior knowledge from\na parameters-sharing AR model. Furthermore, we introduce the two-stage method\ninto the inference process, which improves the model performance greatly. All\nthe experiments are conducted on a public Chinese mandarin dataset ASIEHLL-1.\nThe results show that the TSNAT can achieve a competitive performance with the\nAR model and outperform many complicated NAR models.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 02:34:55 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tian", "Zhengkun", ""], ["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Bai", "Ye", ""], ["Zhang", "Shuai", ""], ["Wen", "Zhengqi", ""], ["Liu", "Xuefei", ""]]}, {"id": "2104.01523", "submitter": "Mohammad Hasan", "authors": "Md. Ahsanul Kabir, Typer Phillips, Xiao Luo, Mohammad Al Hasan", "title": "ASPER: Attention-based Approach to Extract Syntactic Patterns denoting\n  Semantic Relations in Sentential Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic relationships, such as hyponym-hypernym, cause-effect,\nmeronym-holonym etc. between a pair of entities in a sentence are usually\nreflected through syntactic patterns. Automatic extraction of such patterns\nbenefits several downstream tasks, including, entity extraction, ontology\nbuilding, and question answering. Unfortunately, automatic extraction of such\npatterns has not yet received much attention from NLP and information retrieval\nresearchers. In this work, we propose an attention-based supervised deep\nlearning model, ASPER, which extracts syntactic patterns between entities\nexhibiting a given semantic relation in the sentential context. We validate the\nperformance of ASPER on three distinct semantic relations -- hyponym-hypernym,\ncause-effect, and meronym-holonym on six datasets. Experimental results show\nthat for all these semantic relations, ASPER can automatically identify a\ncollection of syntactic patterns reflecting the existence of such a relation\nbetween a pair of entities in a sentence. In comparison to the existing\nmethodologies of syntactic pattern extraction, ASPER's performance is\nsubstantially superior.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 02:36:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kabir", "Md. Ahsanul", ""], ["Phillips", "Typer", ""], ["Luo", "Xiao", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "2104.01543", "submitter": "Esha Singh", "authors": "Esha Singh, Anu Bompelli, Ruyuan Wan, Jiang Bian, Serguei Pakhomov,\n  and Rui Zhang", "title": "A Conversational Agent System for Dietary Supplements Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dietary supplements (DS) have been widely used by consumers, but the\ninformation around the efficacy and safety of DS is disparate or incomplete,\nthus creating barriers for consumers to find information effectively.\nConversational agent (CA) systems have been applied to the healthcare domain,\nbut there is no such a system to answer consumers regarding DS use, although\nwidespread use of DS. In this study, we develop the first CA system for DS use\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:47:04 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 22:16:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Singh", "Esha", ""], ["Bompelli", "Anu", ""], ["Wan", "Ruyuan", ""], ["Bian", "Jiang", ""], ["Pakhomov", "Serguei", ""], ["Zhang", "Rui", ""]]}, {"id": "2104.01558", "submitter": "Mingjiang Liu", "authors": "Mingjiang Liu, Chengli Xiao, Chunlin Chen", "title": "Perspective-corrected Spatial Referring Expression Generation for\n  Human-Robot Interaction", "comments": "Under review, 20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent robots designed to interact with humans in real scenarios need to\nbe able to refer to entities actively by natural language. In spatial referring\nexpression generation, the ambiguity is unavoidable due to the diversity of\nreference frames, which will lead to an understanding gap between humans and\nrobots. To narrow this gap, in this paper, we propose a novel\nperspective-corrected spatial referring expression generation (PcSREG) approach\nfor human-robot interaction by considering the selection of reference frames.\nThe task of referring expression generation is simplified into the process of\ngenerating diverse spatial relation units. First, we pick out all landmarks in\nthese spatial relation units according to the entropy of preference and allow\nits updating through a stack model. Then all possible referring expressions are\ngenerated according to different reference frame strategies. Finally, we\nevaluate every expression using a probabilistic referring expression resolution\nmodel and find the best expression that satisfies both of the appropriateness\nand effectiveness. We implement the proposed approach on a robot system and\nempirical experiments show that our approach can generate more effective\nspatial referring expressions for practical applications.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:00:02 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 09:02:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Mingjiang", ""], ["Xiao", "Chengli", ""], ["Chen", "Chunlin", ""]]}, {"id": "2104.01563", "submitter": "Abhishek Mittal", "authors": "Abhishek Mittal, Ashutosh Modi", "title": "ReCAM@IITK at SemEval-2021 Task 4: BERT and ALBERT based Ensemble for\n  Abstract Word Prediction", "comments": "Accepted at SemEval 2021 Task 4, 8 Pages (7 Pages main content + 1\n  pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes our system for Task 4 of SemEval-2021: Reading\nComprehension of Abstract Meaning (ReCAM). We participated in all subtasks\nwhere the main goal was to predict an abstract word missing from a statement.\nWe fine-tuned the pre-trained masked language models namely BERT and ALBERT and\nused an Ensemble of these as our submitted system on Subtask 1\n(ReCAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3\n(ReCAM-Intersection), we submitted the ALBERT model as it gives the best\nresults. We tried multiple approaches and found that Masked Language\nModeling(MLM) based approach works the best.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:22:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mittal", "Abhishek", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01566", "submitter": "Archit Bansal", "authors": "Archit Bansal, Abhay Kaushik, Ashutosh Modi", "title": "IITK@Detox at SemEval-2021 Task 5: Semi-Supervised Learning and Dice\n  Loss for Toxic Spans Detection", "comments": "Accepted at SemEval 2021 Task 5, 9 Pages (6 Pages main content + 1\n  Page for references + 2 Pages Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we present our approach and findings for SemEval-2021 Task 5 -\nToxic Spans Detection. The task's main aim was to identify spans to which a\ngiven text's toxicity could be attributed. The task is challenging mainly due\nto two constraints: the small training dataset and imbalanced class\ndistribution. Our paper investigates two techniques, semi-supervised learning\nand learning with Self-Adjusting Dice Loss, for tackling these challenges. Our\nsubmitted system (ranked ninth on the leader board) consisted of an ensemble of\nvarious pre-trained Transformer Language Models trained using either of the\nabove-proposed techniques.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:39:55 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bansal", "Archit", ""], ["Kaushik", "Abhay", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01567", "submitter": "Rohan Gupta", "authors": "Rohan Gupta, Jay Mundra, Deepak Mahajan, Ashutosh Modi", "title": "MCL@IITK at SemEval-2021 Task 2: Multilingual and Cross-lingual\n  Word-in-Context Disambiguation using Augmented Data, Signals, and\n  Transformers", "comments": "Accepted at SemEval 2021 Task 2, 10 Pages (8 Pages main content+ 2\n  pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we present our approach for solving the SemEval 2021 Task 2:\nMultilingual and Cross-lingual Word-in-Context Disambiguation (MCL-WiC). The\ntask is a sentence pair classification problem where the goal is to detect\nwhether a given word common to both the sentences evokes the same meaning. We\nsubmit systems for both the settings - Multilingual (the pair's sentences\nbelong to the same language) and Cross-Lingual (the pair's sentences belong to\ndifferent languages). The training data is provided only in English.\nConsequently, we employ cross-lingual transfer techniques. Our approach employs\nfine-tuning pre-trained transformer-based language models, like ELECTRA and\nALBERT, for the English task and XLM-R for all other tasks. To improve these\nsystems' performance, we propose adding a signal to the word to be\ndisambiguated and augmenting our data by sentence pair reversal. We further\naugment the dataset provided to us with WiC, XL-WiC and SemCor 3.0. Using\nensembles, we achieve strong performance in the Multilingual task, placing\nfirst in the EN-EN and FR-FR sub-tasks. For the Cross-Lingual setting, we\nemployed translate-test methods and a zero-shot method, using our multilingual\nmodels, with the latter performing slightly better.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:49:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gupta", "Rohan", ""], ["Mundra", "Jay", ""], ["Mahajan", "Deepak", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01569", "submitter": "Endri Kacupaj", "authors": "Endri Kacupaj, Joan Plepi, Kuldeep Singh, Harsh Thakkar, Jens Lehmann,\n  Maria Maleshkova", "title": "Conversational Question Answering over Knowledge Graphs with Transformer\n  and Graph Attention Networks", "comments": "16th conference of the European Chapter of the Association for\n  Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the task of (complex) conversational question answering\nover a knowledge graph. For this task, we propose LASAGNE (muLti-task semAntic\nparSing with trAnsformer and Graph atteNtion nEtworks). It is the first\napproach, which employs a transformer architecture extended with Graph\nAttention Networks for multi-task neural semantic parsing. LASAGNE uses a\ntransformer model for generating the base logical forms, while the Graph\nAttention model is used to exploit correlations between (entity) types and\npredicates to produce node representations. LASAGNE also includes a novel\nentity recognition module which detects, links, and ranks all relevant entities\nin the question context. We evaluate LASAGNE on a standard dataset for complex\nsequential question answering, on which it outperforms existing baseline\naverages on all question types. Specifically, we show that LASAGNE improves the\nF1-score on eight out of ten question types; in some cases, the increase in\nF1-score is more than 20% compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 09:21:50 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 11:41:05 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kacupaj", "Endri", ""], ["Plepi", "Joan", ""], ["Singh", "Kuldeep", ""], ["Thakkar", "Harsh", ""], ["Lehmann", "Jens", ""], ["Maleshkova", "Maria", ""]]}, {"id": "2104.01572", "submitter": "Tze Yuang Chong", "authors": "Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang", "title": "TransfoRNN: Capturing the Sequential Information in Self-Attention\n  Representations for Language Modeling", "comments": "INTERSPEECH 2021 (under reviewed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe the use of recurrent neural networks to capture\nsequential information from the self-attention representations to improve the\nTransformers. Although self-attention mechanism provides a means to exploit\nlong context, the sequential information, i.e. the arrangement of tokens, is\nnot explicitly captured. We propose to cascade the recurrent neural networks to\nthe Transformers, which referred to as the TransfoRNN model, to capture the\nsequential information. We found that the TransfoRNN models which consists of\nonly shallow Transformers stack is suffice to give comparable, if not better,\nperformance than a deeper Transformer model. Evaluated on the Penn Treebank and\nWikiText-2 corpora, the proposed TransfoRNN model has shown lower model\nperplexities with fewer number of model parameters. On the Penn Treebank\ncorpus, the model perplexities were reduced up to 5.5% with the model size\nreduced up to 10.5%. On the WikiText-2 corpus, the model perplexity was reduced\nup to 2.2% with a 27.7% smaller model. Also, the TransfoRNN model was applied\non the LibriSpeech speech recognition task and has shown comparable results\nwith the Transformer models.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 09:31:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chong", "Tze Yuang", ""], ["Wang", "Xuyang", ""], ["Yang", "Lin", ""], ["Wang", "Junjie", ""]]}, {"id": "2104.01604", "submitter": "Loren Lugosch", "authors": "Loren Lugosch, Piyush Papreja, Mirco Ravanelli, Abdelwahab Heba,\n  Titouan Parcollet", "title": "Timers and Such: A Practical Benchmark for Spoken Language Understanding\n  with Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper introduces Timers and Such, a new open source dataset of spoken\nEnglish commands for common voice control use cases involving numbers. We\ndescribe the gap in existing spoken language understanding datasets that Timers\nand Such fills, the design and creation of the dataset, and experiments with a\nnumber of ASR-based and end-to-end baseline models, the code for which has been\nmade available as part of the SpeechBrain toolkit.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 12:52:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lugosch", "Loren", ""], ["Papreja", "Piyush", ""], ["Ravanelli", "Mirco", ""], ["Heba", "Abdelwahab", ""], ["Parcollet", "Titouan", ""]]}, {"id": "2104.01616", "submitter": "Heng-Jui Chang", "authors": "Heng-Jui Chang, Hung-yi Lee, Lin-shan Lee", "title": "Towards Lifelong Learning of End-to-end ASR", "comments": "Interspeech 2021. We acknowledge the support of Salesforce Research\n  Deep Learning Grant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) technologies today are primarily optimized\nfor given datasets; thus, any changes in the application environment (e.g.,\nacoustic conditions or topic domains) may inevitably degrade the performance.\nWe can collect new data describing the new environment and fine-tune the\nsystem, but this naturally leads to higher error rates for the earlier\ndatasets, referred to as catastrophic forgetting. The concept of lifelong\nlearning (LLL) aiming to enable a machine to sequentially learn new tasks from\nnew datasets describing the changing real world without forgetting the\npreviously learned knowledge is thus brought to attention. This paper reports,\nto our knowledge, the first effort to extensively consider and analyze the use\nof various approaches of LLL in end-to-end (E2E) ASR, including proposing novel\nmethods in saving data for past domains to mitigate the catastrophic forgetting\nproblem. An overall relative reduction of 28.7% in WER was achieved compared to\nthe fine-tuning baseline when sequentially learning on three very different\nbenchmark corpora. This can be the first step toward the highly desired ASR\ntechnologies capable of synchronizing with the continuously changing real\nworld.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 13:48:53 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 01:06:22 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 14:12:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chang", "Heng-Jui", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "2104.01619", "submitter": "Shashank Shailabh", "authors": "Shashank Shailabh, Sajal Chaurasia, Ashutosh Modi", "title": "KnowGraph@IITK at SemEval-2021 Task 11: Building KnowledgeGraph for NLP\n  Research", "comments": "Accepted at SemEval 2021 Task 11, 11 Pages (9 Pages main content+ 2\n  pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research in Natural Language Processing is making rapid advances, resulting\nin the publication of a large number of research papers. Finding relevant\nresearch papers and their contribution to the domain is a challenging problem.\nIn this paper, we address this challenge via the SemEval 2021 Task 11:\nNLPContributionGraph, by developing a system for a research paper\ncontributions-focused knowledge graph over Natural Language Processing\nliterature. The task is divided into three sub-tasks: extracting contribution\nsentences that show important contributions in the research article, extracting\nphrases from the contribution sentences, and predicting the information units\nin the research article together with triplet formation from the phrases. The\nproposed system is agnostic to the subject domain and can be applied for\nbuilding a knowledge graph for any area. We found that transformer-based\nlanguage models can significantly improve existing techniques and utilized the\nSciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM)\nstacked on top of SciBERT model layers, while the second sub-task uses\nConditional Random Field (CRF) on top of SciBERT with BiLSTM. The third\nsub-task uses a combined SciBERT based neural approach with heuristics for\ninformation unit prediction and triplet formation from the phrases. Our system\nachieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase\nextraction testing and triplet extraction testing respectively.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 14:33:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shailabh", "Shashank", ""], ["Chaurasia", "Sajal", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01624", "submitter": "Kathleen Siminyu", "authors": "Kathleen Siminyu, Xinjian Li, Antonios Anastasopoulos, David\n  Mortensen, Michael R. Marlo, Graham Neubig", "title": "Phoneme Recognition through Fine Tuning of Phonetic Representations: a\n  Case Study on Luhya Language Varieties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Models pre-trained on multiple languages have shown significant promise for\nimproving speech recognition, particularly for low-resource languages. In this\nwork, we focus on phoneme recognition using Allosaurus, a method for\nmultilingual recognition based on phonetic annotation, which incorporates\nphonological knowledge through a language-dependent allophone layer that\nassociates a universal narrow phone-set with the phonemes that appear in each\nlanguage. To evaluate in a challenging real-world scenario, we curate phone\nrecognition datasets for Bukusu and Saamia, two varieties of the Luhya language\ncluster of western Kenya and eastern Uganda. To our knowledge, these datasets\nare the first of their kind. We carry out similar experiments on the dataset of\nan endangered Tangkhulic language, East Tusom, a Tibeto-Burman language variety\nspoken mostly in India. We explore both zero-shot and few-shot recognition by\nfine-tuning using datasets of varying sizes (10 to 1000 utterances). We find\nthat fine-tuning of Allosaurus, even with just 100 utterances, leads to\nsignificant improvements in phone error rates.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:07:55 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Siminyu", "Kathleen", ""], ["Li", "Xinjian", ""], ["Anastasopoulos", "Antonios", ""], ["Mortensen", "David", ""], ["Marlo", "Michael R.", ""], ["Neubig", "Graham", ""]]}, {"id": "2104.01642", "submitter": "Martin Weyssow", "authors": "Martin Weyssow, Houari Sahraoui, Eugene Syriani", "title": "Recommending Metamodel Concepts during Modeling Activities with\n  Pre-Trained Language Models", "comments": "18+1 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of conceptually sound metamodels that embody proper semantics in\nrelation to the application domain is particularly tedious in Model-Driven\nEngineering. As metamodels define complex relationships between domain\nconcepts, it is crucial for a modeler to define these concepts thoroughly while\nbeing consistent with respect to the application domain. We propose an approach\nto assist a modeler in the design of a metamodel by recommending relevant\ndomain concepts in several modeling scenarios. Our approach does not require to\nextract knowledge from the domain or to hand-design completion rules. Instead,\nwe design a fully data-driven approach using a deep learning model that is able\nto abstract domain concepts by learning from both structural and lexical\nmetamodel properties in a corpus of thousands of independent metamodels. We\nevaluate our approach on a test set containing 166 metamodels, unseen during\nthe model training, with more than 5000 test samples. Our preliminary results\nshow that the trained model is able to provide accurate top-$5$ lists of\nrelevant recommendations for concept renaming scenarios. Although promising,\nthe results are less compelling for the scenario of the iterative construction\nof the metamodel, in part because of the conservative strategy we use to\nevaluate the recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 16:29:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Weyssow", "Martin", ""], ["Sahraoui", "Houari", ""], ["Syriani", "Eugene", ""]]}, {"id": "2104.01666", "submitter": "Hui Liu", "authors": "Hui Liu, Danqing Zhang, Bing Yin, Xiaodan Zhu", "title": "Improving Pretrained Models for Zero-shot Multi-label Text\n  Classification through Reinforced Label Hierarchy Reasoning", "comments": "Accepted to Main Conference of NAACL 2021 as Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploiting label hierarchies has become a promising approach to tackling the\nzero-shot multi-label text classification (ZS-MTC) problem. Conventional\nmethods aim to learn a matching model between text and labels, using a graph\nencoder to incorporate label hierarchies to obtain effective label\nrepresentations \\cite{rios2018few}. More recently, pretrained models like BERT\n\\cite{devlin2018bert} have been used to convert classification tasks into a\ntextual entailment task \\cite{yin-etal-2019-benchmarking}. This approach is\nnaturally suitable for the ZS-MTC task. However, pretrained models are\nunderexplored in the existing work because they do not generate individual\nvector representations for text or labels, making it unintuitive to combine\nthem with conventional graph encoding methods. In this paper, we explore to\nimprove pretrained models with label hierarchies on the ZS-MTC task. We propose\na Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage\ninterdependence among labels in the hierarchies during training. Meanwhile, to\novercome the weakness of flat predictions, we design a rollback algorithm that\ncan remove logical errors from predictions during inference. Experimental\nresults on three real-life datasets show that our approach achieves better\nperformance and outperforms previous non-pretrained methods on the ZS-MTC task.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:14:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Hui", ""], ["Zhang", "Danqing", ""], ["Yin", "Bing", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2104.01697", "submitter": "Tuan Manh Lai", "authors": "Tuan Lai, Heng Ji, Trung Bui, Quan Hung Tran, Franck Dernoncourt,\n  Walter Chang", "title": "A Context-Dependent Gated Module for Incorporating Symbolic Semantics\n  into Event Coreference Resolution", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event coreference resolution is an important research problem with many\napplications. Despite the recent remarkable success of pretrained language\nmodels, we argue that it is still highly beneficial to utilize symbolic\nfeatures for the task. However, as the input for coreference resolution\ntypically comes from upstream components in the information extraction\npipeline, the automatically extracted symbolic features can be noisy and\ncontain errors. Also, depending on the specific context, some features can be\nmore informative than others. Motivated by these observations, we propose a\nnovel context-dependent gated module to adaptively control the information\nflows from the input symbolic features. Combined with a simple noisy training\nmethod, our best models achieve state-of-the-art results on two datasets: ACE\n2005 and KBP 2016.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 21:15:02 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lai", "Tuan", ""], ["Ji", "Heng", ""], ["Bui", "Trung", ""], ["Tran", "Quan Hung", ""], ["Dernoncourt", "Franck", ""], ["Chang", "Walter", ""]]}, {"id": "2104.01703", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Abhay Zala, Graham Burri, Mohit Bansal", "title": "FixMyPose: Pose Correctional Captioning and Retrieval", "comments": "AAAI 2021 (18 pages, 16 figures; webpage:\n  https://fixmypose-unc.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in physical therapy and individual exercises such as yoga/dance has\nincreased alongside the well-being trend. However, such exercises are hard to\nfollow without expert guidance (which is impossible to scale for personalized\nfeedback to every trainee remotely). Thus, automated pose correction systems\nare required more than ever, and we introduce a new captioning dataset named\nFixMyPose to address this need. We collect descriptions of correcting a\n\"current\" pose to look like a \"target\" pose (in both English and Hindi). The\ncollected descriptions have interesting linguistic properties such as\negocentric relations to environment objects, analogous references, etc.,\nrequiring an understanding of spatial relations and commonsense knowledge about\npostures. Further, to avoid ML biases, we maintain a balance across characters\nwith diverse demographics, who perform a variety of movements in several\ninterior environments (e.g., homes, offices). From our dataset, we introduce\nthe pose-correctional-captioning task and its reverse target-pose-retrieval\ntask. During the correctional-captioning task, models must generate\ndescriptions of how to move from the current to target pose image, whereas in\nthe retrieval task, models should select the correct target pose given the\ninitial pose and correctional description. We present strong cross-attention\nbaseline models (uni/multimodal, RL, multilingual) and also show that our\nbaselines are competitive with other models when evaluated on other\nimage-difference datasets. We also propose new task-specific metrics\n(object-match, body-part-match, direction-match) and conduct human evaluation\nfor more reliable evaluation, and we demonstrate a large human-model\nperformance gap suggesting room for promising future work. To verify the\nsim-to-real transfer of our FixMyPose dataset, we collect a set of real images\nand show promising performance on these images.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 21:45:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kim", "Hyounghun", ""], ["Zala", "Abhay", ""], ["Burri", "Graham", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.01724", "submitter": "Shuyang Cao", "authors": "Shuyang Cao and Lu Wang", "title": "Inference Time Style Control for Summarization", "comments": "Accepted at NAACL 2021 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How to generate summaries of different styles without requiring corpora in\nthe target styles, or training separate models? We present two novel methods\nthat can be deployed during summary decoding on any pre-trained\nTransformer-based summarization model. (1) Decoder state adjustment instantly\nmodifies decoder final states with externally trained style scorers, to\niteratively refine the output against a target style. (2) Word unit prediction\nconstrains the word usage to impose strong lexical control during generation.\nIn experiments of summarizing with simplicity control, automatic evaluation and\nhuman judges both find our models producing outputs in simpler languages while\nstill informative. We also generate news headlines with various ideological\nleanings, which can be distinguished by humans with a reasonable probability.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 00:27:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Cao", "Shuyang", ""], ["Wang", "Lu", ""]]}, {"id": "2104.01726", "submitter": "Fei Liu", "authors": "Kaiqiang Song and Bingqing Wang and Zhe Feng and Fei Liu", "title": "A New Approach to Overgenerating and Scoring Abstractive Summaries", "comments": "NAACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to generate multiple variants of the target summary\nwith diverse content and varying lengths, then score and select admissible ones\naccording to users' needs. Abstractive summarizers trained on single reference\nsummaries may struggle to produce outputs that achieve multiple desirable\nproperties, i.e., capturing the most important information, being faithful to\nthe original, grammatical and fluent. In this paper, we propose a two-staged\nstrategy to generate a diverse set of candidate summaries from the source text\nin stage one, then score and select admissible ones in stage two. Importantly,\nour generator gives a precise control over the length of the summary, which is\nespecially well-suited when space is limited. Our selectors are designed to\npredict the optimal summary length and put special emphasis on faithfulness to\nthe original text. Both stages can be effectively trained, optimized and\nevaluated. Our experiments on benchmark summarization datasets suggest that\nthis paradigm can achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 00:29:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Song", "Kaiqiang", ""], ["Wang", "Bingqing", ""], ["Feng", "Zhe", ""], ["Liu", "Fei", ""]]}, {"id": "2104.01759", "submitter": "Nitish Gupta", "authors": "Nitish Gupta, Sameer Singh, Matt Gardner, Dan Roth", "title": "Paired Examples as Indirect Supervision in Latent Decision Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compositional, structured models are appealing because they explicitly\ndecompose problems and provide interpretable intermediate outputs that give\nconfidence that the model is not simply latching onto data artifacts. Learning\nthese models is challenging, however, because end-task supervision only\nprovides a weak indirect signal on what values the latent decisions should\ntake. This often results in the model failing to learn to perform the\nintermediate tasks correctly. In this work, we introduce a way to leverage\npaired examples that provide stronger cues for learning latent decisions. When\ntwo related training examples share internal substructure, we add an additional\ntraining objective to encourage consistency between their latent decisions.\nSuch an objective does not require external supervision for the values of the\nlatent output, or even the end task, yet provides an additional training signal\nto that provided by individual training examples themselves. We apply our\nmethod to improve compositional question answering using neural module networks\non the DROP dataset. We explore three ways to acquire paired questions in DROP:\n(a) discovering naturally occurring paired examples within the dataset, (b)\nconstructing paired examples using templates, and (c) generating paired\nexamples using a question generation model. We empirically demonstrate that our\nproposed approach improves both in- and out-of-distribution generalization and\nleads to correct latent decision predictions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 03:58:30 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gupta", "Nitish", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""], ["Roth", "Dan", ""]]}, {"id": "2104.01767", "submitter": "Junjie Huang", "authors": "Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming\n  Gong, Daxin Jiang, Nan Duan", "title": "WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing the embedding of a sentence in an unsupervised way is valuable to\nnatural language matching and retrieval problems in practice. In this work, we\nconduct a thorough examination of pretrained model based unsupervised sentence\nembeddings. We study on four pretrained models and conduct massive experiments\non seven datasets regarding sentence semantics. We have there main findings.\nFirst, averaging all tokens is better than only using [CLS] vector. Second,\ncombining both top andbottom layers is better than only using top layers.\nLastly, an easy whitening-based vector normalization strategy with less than 10\nlines of code consistently boosts the performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:30:28 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:37:19 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 03:06:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Huang", "Junjie", ""], ["Tang", "Duyu", ""], ["Zhong", "Wanjun", ""], ["Lu", "Shuai", ""], ["Shou", "Linjun", ""], ["Gong", "Ming", ""], ["Jiang", "Daxin", ""], ["Duan", "Nan", ""]]}, {"id": "2104.01782", "submitter": "Ishani Mondal", "authors": "Ishani Mondal", "title": "BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for\n  Text Classification", "comments": "To appear in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Healthcare predictive analytics aids medical decision-making, diagnosis\nprediction and drug review analysis. Therefore, prediction accuracy is an\nimportant criteria which also necessitates robust predictive language models.\nHowever, the models using deep learning have been proven vulnerable towards\ninsignificantly perturbed input instances which are less likely to be\nmisclassified by humans. Recent efforts of generating adversaries using\nrule-based synonyms and BERT-MLMs have been witnessed in general domain, but\nthe ever increasing biomedical literature poses unique challenges. We propose\nBBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box\nattack algorithm for biomedical text classification, leveraging the strengths\nof both domain-specific synonym replacement for biomedical named entities and\nBERTMLM predictions, spelling variation and number replacement. Through\nautomatic and human evaluation on two datasets, we demonstrate that BBAEG\nperforms stronger attack with better language fluency, semantic coherence as\ncompared to prior work.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 05:32:56 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mondal", "Ishani", ""]]}, {"id": "2104.01785", "submitter": "Yoshihiko Suhara", "authors": "Yoshihiko Suhara, Jinfeng Li, Yuliang Li, Dan Zhang, \\c{C}a\\u{g}atay\n  Demiralp, Chen Chen, Wang-Chiew Tan", "title": "Annotating Columns with Pre-trained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring meta information about tables, such as column headers or\nrelationships between columns, is an active research topic in data management\nas we find many tables are missing some of this information. In this paper, we\nstudy the problem of annotating table columns (i.e., predicting column types\nand the relationships between columns) using only information from the table\nitself. We show that a multi-task learning approach (called Doduo), trained\nusing pre-trained language models on both tasks outperforms individual learning\napproaches. Experimental results show that Doduo establishes new\nstate-of-the-art performance on two benchmarks for the column type prediction\nand column relation prediction tasks with up to 4.0% and 11.9% improvements,\nrespectively. We also establish that Doduo can already perform the previous\nstate-of-the-art performance with a minimal number of tokens, only 8 tokens per\ncolumn.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:05:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Suhara", "Yoshihiko", ""], ["Li", "Jinfeng", ""], ["Li", "Yuliang", ""], ["Zhang", "Dan", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Chen", "Chen", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2104.01791", "submitter": "Saikat Dutta", "authors": "Sourya Dipta Das, Ayan Basak, Saikat Dutta", "title": "A Heuristic-driven Uncertainty based Ensemble Framework for Fake News\n  Detection in Tweets and News Articles", "comments": "submitted to Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:2101.03545", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of social media has increased manifold in the past few\ndecades as it helps people from even the most remote corners of the world to\nstay connected. With the advent of technology, digital media has become more\nrelevant and widely used than ever before and along with this, there has been a\nresurgence in the circulation of fake news and tweets that demand immediate\nattention. In this paper, we describe a novel Fake News Detection system that\nautomatically identifies whether a news item is \"real\" or \"fake\", as an\nextension of our work in the CONSTRAINT COVID-19 Fake News Detection in English\nchallenge. We have used an ensemble model consisting of pre-trained models\nfollowed by a statistical feature fusion network , along with a novel heuristic\nalgorithm by incorporating various attributes present in news items or tweets\nlike source, username handles, URL domains and authors as statistical feature.\nOur proposed framework have also quantified reliable predictive uncertainty\nalong with proper class output confidence level for the classification task. We\nhave evaluated our results on the COVID-19 Fake News dataset and FakeNewsNet\ndataset to show the effectiveness of the proposed algorithm on detecting fake\nnews in short news content as well as in news articles. We obtained a best\nF1-score of 0.9892 on the COVID-19 dataset, and an F1-score of 0.9073 on the\nFakeNewsNet dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:35:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Basak", "Ayan", ""], ["Dutta", "Saikat", ""]]}, {"id": "2104.01799", "submitter": "Tapas Nayak", "authors": "Tapas Nayak", "title": "Deep Neural Networks for Relation Extraction", "comments": "PhD Thesis, National University of Singapore (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Relation extraction from text is an important task for automatic knowledge\nbase population. In this thesis, we first propose a syntax-focused multi-factor\nattention network model for finding the relation between two entities. Next, we\npropose two joint entity and relation extraction frameworks based on\nencoder-decoder architecture. Finally, we propose a hierarchical entity graph\nconvolutional network for relation extraction across documents.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 07:18:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Nayak", "Tapas", ""]]}, {"id": "2104.01807", "submitter": "Tadahiro Taniguchi", "authors": "Asuka Moritani, Ryo Ozaki, Shoki Sakamoto, Hirokazu Kameoka, Tadahiro\n  Taniguchi", "title": "StarGAN-based Emotional Voice Conversion for Japanese Phrases", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that StarGAN-VC, a spectral envelope transformation method\nfor non-parallel many-to-many voice conversion (VC), is capable of emotional VC\n(EVC). Although StarGAN-VC has been shown to enable speaker identity\nconversion, its capability for EVC for Japanese phrases has not been clarified.\nIn this paper, we describe the direct application of StarGAN-VC to an EVC task\nwith minimal fundamental frequency and aperiodicity processing. Through\nsubjective evaluation experiments, we evaluated the performance of our\nStarGAN-EVC system in terms of its ability to achieve EVC for Japanese phrases.\nThe subjective evaluation is conducted in terms of subjective classification\nand mean opinion score of neutrality and similarity. In addition, the\ninterdependence between the source and target emotional domains was\ninvestigated from the perspective of the quality of EVC.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 08:08:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Moritani", "Asuka", ""], ["Ozaki", "Ryo", ""], ["Sakamoto", "Shoki", ""], ["Kameoka", "Hirokazu", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2104.01853", "submitter": "Sho Takase", "authors": "Sho Takase and Shun Kiyono", "title": "Rethinking Perturbations in Encoder-Decoders for Fast Training", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We often use perturbations to regularize neural models. For neural\nencoder-decoders, previous studies applied the scheduled sampling (Bengio et\nal., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations\nbut these methods require considerable computational time. Thus, this study\naddresses the question of whether these approaches are efficient enough for\ntraining time. We compare several perturbations in sequence-to-sequence\nproblems with respect to computational time. Experimental results show that the\nsimple techniques such as word dropout (Gal and Ghahramani, 2016) and random\nreplacement of input tokens achieve comparable (or better) scores to the\nrecently proposed perturbations, even though these simple methods are faster.\nOur code is publicly available at\nhttps://github.com/takase/rethink_perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 11:06:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Takase", "Sho", ""], ["Kiyono", "Shun", ""]]}, {"id": "2104.01894", "submitter": "Ramon Sanabria", "authors": "Ramon Sanabria, Austin Waters, Jason Baldridge", "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval", "comments": "Accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:11:40 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:16:17 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 17:03:38 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sanabria", "Ramon", ""], ["Waters", "Austin", ""], ["Baldridge", "Jason", ""]]}, {"id": "2104.01935", "submitter": "Abdessamad Benlahbib", "authors": "Abdessamad Benlahbib", "title": "Mining Customers' Opinions for Online Reputation Generation and\n  Visualization in e-Commerce Platforms", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer reviews represent a very rich data source from which we can extract\nvery valuable information about different online shopping experiences. The\namount of the collected data may be very large especially for trendy items\n(products, movies, TV shows, hotels, services...), where the number of\navailable customers' opinions could easily surpass thousands. In fact, while a\ngood number of reviews could indeed give a hint about the quality of an item, a\npotential customer may not have time or effort to read all reviews for the\npurpose of making an informed decision (buying, renting, booking...). Thus, the\nneed for the right tools and technologies to help in such a task becomes a\nnecessity for the buyer as for the seller. My research goal in this thesis is\nto develop reputation systems that can automatically provide E-commerce\ncustomers with valuable information to support them during their online\ndecision-making process by mining online reviews expressed in natural language.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:46:57 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Benlahbib", "Abdessamad", ""]]}, {"id": "2104.01940", "submitter": "Einat Minkov", "authors": "Avishai Zagoury and Einat Minkov and Idan Szpektor and William W.\n  Cohen", "title": "What's the best place for an AI conference, Vancouver or ______: Why\n  completing comparative questions is difficult", "comments": "AAAI 2021; preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although large neural language models (LMs) like BERT can be finetuned to\nyield state-of-the-art results on many NLP tasks, it is often unclear what\nthese models actually learn. Here we study using such LMs to fill in entities\nin human-authored comparative questions, like ``Which country is older, India\nor ______?'' -- i.e., we study the ability of neural LMs to ask (not answer)\nreasonable questions. We show that accuracy in this fill-in-the-blank task is\nwell-correlated with human judgements of whether a question is reasonable, and\nthat these models can be trained to achieve nearly human-level performance in\ncompleting comparative questions in three different subdomains. However,\nanalysis shows that what they learn fails to model any sort of broad notion of\nwhich entities are semantically comparable or similar -- instead the trained\nmodels are very domain-specific, and performance is highly correlated with\nco-occurrences between specific entities observed in the training set. This is\ntrue both for models that are pretrained on general text corpora, as well as\nmodels trained on a large corpus of comparison questions. Our study thus\nreinforces recent results on the difficulty of making claims about a deep\nmodel's world knowledge or linguistic competence based on performance on\nspecific benchmark problems. We make our evaluation datasets publicly available\nto foster future research on complex understanding and reasoning in such models\nat standards of human interaction.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:56:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zagoury", "Avishai", ""], ["Minkov", "Einat", ""], ["Szpektor", "Idan", ""], ["Cohen", "William W.", ""]]}, {"id": "2104.01955", "submitter": "Dhivya Chandrasekaran", "authors": "Dhivya Chandrasekaran and Vijay Mago", "title": "Automating Transfer Credit Assessment in Student Mobility -- A Natural\n  Language Processing-based Approach", "comments": "13 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student mobility or academic mobility involves students moving between\ninstitutions during their post-secondary education, and one of the challenging\ntasks in this process is to assess the transfer credits to be offered to the\nincoming student. In general, this process involves domain experts comparing\nthe learning outcomes of the courses, to decide on offering transfer credits to\nthe incoming students. This manual implementation is not only labor-intensive\nbut also influenced by undue bias and administrative complexity. The proposed\nresearch article focuses on identifying a model that exploits the advancements\nin the field of Natural Language Processing (NLP) to effectively automate this\nprocess. Given the unique structure, domain specificity, and complexity of\nlearning outcomes (LOs), a need for designing a tailor-made model arises. The\nproposed model uses a clustering-inspired methodology based on knowledge-based\nsemantic similarity measures to assess the taxonomic similarity of LOs and a\ntransformer-based semantic similarity model to assess the semantic similarity\nof the LOs. The similarity between LOs is further aggregated to form course to\ncourse similarity. Due to the lack of quality benchmark datasets, a new\nbenchmark dataset containing seven course-to-course similarity measures is\nproposed. Understanding the inherent need for flexibility in the\ndecision-making process the aggregation part of the model offers tunable\nparameters to accommodate different scenarios. While providing an efficient\nmodel to assess the similarity between courses with existing resources, this\nresearch work steers future research attempts to apply NLP in the field of\narticulation in an ideal direction by highlighting the persisting research\ngaps.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:14:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chandrasekaran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2104.01989", "submitter": "Jason Pelecanos", "authors": "Jason Pelecanos and Quan Wang and Ignacio Lopez Moreno", "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker\n  Recognition", "comments": "To appear in Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neural network speaker recognition systems model each speaker using a\nfixed-dimensional embedding vector. These embeddings are generally compared\nusing either linear or 2nd-order scoring and, until recently, do not handle\nutterance-specific uncertainty. In this work we propose scoring these\nrepresentations in a way that can capture uncertainty, enroll/test asymmetry\nand additional non-linear information. This is achieved by incorporating a\n2nd-stage neural network (known as a decision network) as part of an end-to-end\ntraining regimen. In particular, we propose the concept of decision residual\nnetworks which involves the use of a compact decision network to leverage\ncosine scores and to model the residual signal that's needed. Additionally, we\npresent a modification to the generalized end-to-end softmax loss function to\ntarget the separation of same/different speaker scores. We observed significant\nperformance gains for the two techniques.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:31:04 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 15:41:38 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pelecanos", "Jason", ""], ["Wang", "Quan", ""], ["Moreno", "Ignacio Lopez", ""]]}, {"id": "2104.02014", "submitter": "Patrick O'Neill", "authors": "Patrick K. O'Neill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid\n  Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko,\n  Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, and\n  Georg Kucsko", "title": "SPGISpeech: 5,000 hours of transcribed financial audio for fully\n  formatted end-to-end speech recognition", "comments": "5 pages, 1 figure. Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the English speech-to-text (STT) machine learning task, acoustic models\nare conventionally trained on uncased Latin characters, and any necessary\northography (such as capitalization, punctuation, and denormalization of\nnon-standard words) is imputed by separate post-processing models. This adds\ncomplexity and limits performance, as many formatting tasks benefit from\nsemantic information present in the acoustic signal but absent in\ntranscription. Here we propose a new STT task: end-to-end neural transcription\nwith fully formatted text for target labels. We present baseline\nConformer-based models trained on a corpus of 5,000 hours of professionally\ntranscribed earnings calls, achieving a CER of 1.7. As a contribution to the\nSTT research community, we release the corpus free for non-commercial use at\nhttps://datasets.kensho.com/datasets/scribe.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:05:28 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 04:22:48 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["O'Neill", "Patrick K.", ""], ["Lavrukhin", "Vitaly", ""], ["Majumdar", "Somshubra", ""], ["Noroozi", "Vahid", ""], ["Zhang", "Yuekai", ""], ["Kuchaiev", "Oleksii", ""], ["Balam", "Jagadeesh", ""], ["Dovzhenko", "Yuliya", ""], ["Freyberg", "Keenan", ""], ["Shulman", "Michael D.", ""], ["Ginsburg", "Boris", ""], ["Watanabe", "Shinji", ""], ["Kucsko", "Georg", ""]]}, {"id": "2104.02021", "submitter": "Dat Quoc Nguyen", "authors": "Mai Hoang Dao, Thinh Hung Truong, Dat Quoc Nguyen", "title": "Intent Detection and Slot Filling for Vietnamese", "comments": "To appear in Proceedings of INTERSPEECH 2021; The first two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intent detection and slot filling are important tasks in spoken and natural\nlanguage understanding. However, Vietnamese is a low-resource language in these\nresearch topics. In this paper, we present the first public intent detection\nand slot filling dataset for Vietnamese. In addition, we also propose a joint\nmodel for intent detection and slot filling, that extends the recent\nstate-of-the-art JointBERT+CRF model with an intent-slot attention layer to\nexplicitly incorporate intent context information into slot filling via \"soft\"\nintent label embedding. Experimental results on our Vietnamese dataset show\nthat our proposed model significantly outperforms JointBERT+CRF. We publicly\nrelease our dataset and the implementation of our model at:\nhttps://github.com/VinAIResearch/JointIDSF\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:19:42 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:26:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dao", "Mai Hoang", ""], ["Truong", "Thinh Hung", ""], ["Nguyen", "Dat Quoc", ""]]}, {"id": "2104.02041", "submitter": "Diogo Cortiz", "authors": "Diogo Cortiz", "title": "Exploring Transformers in Emotion Recognition: a comparison of BERT,\n  DistillBERT, RoBERTa, XLNet and ELECTRA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates how Natural Language Understanding (NLU) could be\napplied in Emotion Recognition, a specific task in affective computing. We\nfinetuned different transformers language models (BERT, DistilBERT, RoBERTa,\nXLNet, and ELECTRA) using a fine-grained emotion dataset and evaluating them in\nterms of performance (f1-score) and time to complete.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:46:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Cortiz", "Diogo", ""]]}, {"id": "2104.02109", "submitter": "Liang Lu", "authors": "Liang Lu, Naoyuki Kanda, Jinyu Li and Yifan Gong", "title": "Streaming Multi-talker Speech Recognition with Joint Speaker\n  Identification", "comments": "5 pages, 2 figures, submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-talker scenarios such as meetings and conversations, speech\nprocessing systems are usually required to transcribe the audio as well as\nidentify the speakers for downstream applications. Since overlapped speech is\ncommon in this case, conventional approaches usually address this problem in a\ncascaded fashion that involves speech separation, speech recognition and\nspeaker identification that are trained independently. In this paper, we\npropose Streaming Unmixing, Recognition and Identification Transducer (SURIT)\n-- a new framework that deals with this problem in an end-to-end streaming\nfashion. SURIT employs the recurrent neural network transducer (RNN-T) as the\nbackbone for both speech recognition and speaker identification. We validate\nour idea on the LibrispeechMix dataset -- a multi-talker dataset derived from\nLibrispeech, and present encouraging results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:37:33 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lu", "Liang", ""], ["Kanda", "Naoyuki", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2104.02112", "submitter": "Shuyang Cao", "authors": "Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji and Lu Wang", "title": "Efficient Attentions for Long Document Summarization", "comments": "Accepted at NAACL 2021 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quadratic computational and memory complexities of large Transformers\nhave limited their scalability for long document summarization. In this paper,\nwe propose Hepos, a novel efficient encoder-decoder attention with head-wise\npositional strides to effectively pinpoint salient information from the source.\nWe further conduct a systematic study of existing efficient self-attentions.\nCombined with Hepos, we are able to process ten times more tokens than existing\nmodels that use full attentions. For evaluation, we present a new dataset,\nGovReport, with significantly longer documents and summaries. Results show that\nour models produce significantly higher ROUGE scores than competitive\ncomparisons, including new state-of-the-art results on PubMed. Human evaluation\nalso shows that our models generate more informative summaries with fewer\nunfaithful errors.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:45:13 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 21:04:26 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Huang", "Luyang", ""], ["Cao", "Shuyang", ""], ["Parulian", "Nikolaus", ""], ["Ji", "Heng", ""], ["Wang", "Lu", ""]]}, {"id": "2104.02115", "submitter": "Hadeel Al-Negheimish", "authors": "Hadeel Al-Negheimish, Pranava Madhyastha, Alessandra Russo", "title": "Discrete Reasoning Templates for Natural Language Understanding", "comments": "Published at EACL 2021 SRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning about information from multiple parts of a passage to derive an\nanswer is an open challenge for reading-comprehension models. In this paper, we\npresent an approach that reasons about complex questions by decomposing them to\nsimpler subquestions that can take advantage of single-span extraction\nreading-comprehension models, and derives the final answer according to\ninstructions in a predefined reasoning template. We focus on subtraction-based\narithmetic questions and evaluate our approach on a subset of the DROP dataset.\nWe show that our approach is competitive with the state-of-the-art while being\ninterpretable and requires little supervision\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:56:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Al-Negheimish", "Hadeel", ""], ["Madhyastha", "Pranava", ""], ["Russo", "Alessandra", ""]]}, {"id": "2104.02128", "submitter": "Naoyuki Kanda", "authors": "Naoyuki Kanda, Guoli Ye, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo\n  Chen, Takuya Yoshioka", "title": "End-to-End Speaker-Attributed ASR with Transformer", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our recent effort on end-to-end speaker-attributed\nautomatic speech recognition, which jointly performs speaker counting, speech\nrecognition and speaker identification for monaural multi-talker audio.\nFirstly, we thoroughly update the model architecture that was previously\ndesigned based on a long short-term memory (LSTM)-based attention encoder\ndecoder by applying transformer architectures. Secondly, we propose a speaker\ndeduplication mechanism to reduce speaker identification errors in highly\noverlapped regions. Experimental results on the LibriSpeechMix dataset shows\nthat the transformer-based architecture is especially good at counting the\nspeakers and that the proposed model reduces the speaker-attributed word error\nrate by 47% over the LSTM-based baseline. Furthermore, for the LibriCSS\ndataset, which consists of real recordings of overlapped speech, the proposed\nmodel achieves concatenated minimum-permutation word error rates of 11.9% and\n16.3% with and without target speaker profiles, respectively, both of which are\nthe state-of-the-art results for LibriCSS with the monaural setting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 19:54:15 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kanda", "Naoyuki", ""], ["Ye", "Guoli", ""], ["Gaur", "Yashesh", ""], ["Wang", "Xiaofei", ""], ["Meng", "Zhong", ""], ["Chen", "Zhuo", ""], ["Yoshioka", "Takuya", ""]]}, {"id": "2104.02133", "submitter": "William Chan", "authors": "William Chan, Daniel Park, Chris Lee, Yu Zhang, Quoc Le, Mohammad\n  Norouzi", "title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train\n  One Large Neural Network", "comments": "submitted to INTERSPEECH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SpeechStew, a speech recognition model that is trained on a\ncombination of various publicly available speech recognition datasets: AMI,\nBroadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and\nWall Street Journal. SpeechStew simply mixes all of these datasets together,\nwithout any special re-weighting or re-balancing of the datasets. SpeechStew\nachieves SoTA or near SoTA results across a variety of tasks, without the use\nof an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\%\nWER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which\nsignificantly outperforms prior work with strong external language models. We\nalso demonstrate that SpeechStew learns powerful transfer learning\nrepresentations. We fine-tune SpeechStew on a noisy low resource speech\ndataset, CHiME-6. We achieve 38.9\\% WER without a language model, which\ncompares to 38.6\\% WER to a strong HMM baseline with a language model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:13:36 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 21:28:29 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 13:23:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chan", "William", ""], ["Park", "Daniel", ""], ["Lee", "Chris", ""], ["Zhang", "Yu", ""], ["Le", "Quoc", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2104.02137", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Xin Liu, Haojie Pan, Haowen Ke, Jiefu Ou, Tianqing\n  Fang, Yangqiu Song", "title": "ASER: Towards Large-scale Commonsense Knowledge Acquisition via\n  Higher-order Selectional Preference over Eventualities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Commonsense knowledge acquisition and reasoning have long been a core\nartificial intelligence problem. However, in the past, there has been a lack of\nscalable methods to collect commonsense knowledge. In this paper, we propose to\ndevelop principles for collecting commonsense knowledge based on selectional\npreference. We generalize the definition of selectional preference from one-hop\nlinguistic syntactic relations to higher-order relations over linguistic\ngraphs. Unlike previous commonsense knowledge definition (e.g., ConceptNet),\nthe selectional preference (SP) knowledge only relies on statistical\ndistribution over linguistic graphs, which can be efficiently and accurately\nacquired from the unlabeled corpus with modern tools. Following this principle,\nwe develop a large-scale eventuality (a linguistic term covering activity,\nstate, and event)-based knowledge graph ASER, where each eventuality is\nrepresented as a dependency graph, and the relation between them is a discourse\nrelation defined in shallow discourse parsing. The higher-order selectional\npreference over collected linguistic graphs reflects various kinds of\ncommonsense knowledge. Moreover, motivated by the observation that humans\nunderstand events by abstracting the observed events to a higher level and can\nthus transferring their knowledge to new events, we propose a conceptualization\nmodule to significantly boost the coverage of ASER. In total, ASER contains 438\nmillion eventualities and 648 million edges between eventualities. After\nconceptualization with Probase, a selectional preference based concept-instance\nrelational knowledge base, our concept graph contains 15 million conceptualized\neventualities and 224 million edges between them. Detailed analysis is provided\nto demonstrate its quality. All the collected data, APIs, and tools are\navailable at https://github.com/HKUST-KnowComp/ASER.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:23:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Hongming", ""], ["Liu", "Xin", ""], ["Pan", "Haojie", ""], ["Ke", "Haowen", ""], ["Ou", "Jiefu", ""], ["Fang", "Tianqing", ""], ["Song", "Yangqiu", ""]]}, {"id": "2104.02138", "submitter": "Suyoun Kim", "authors": "Suyoun Kim, Abhinav Arora, Duc Le, Ching-Feng Yeh, Christian Fuegen,\n  Ozlem Kalinli, Michael L. Seltzer", "title": "Semantic Distance: A New Metric for ASR Performance Analysis Towards\n  Spoken Language Understanding", "comments": "submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word Error Rate (WER) has been the predominant metric used to evaluate the\nperformance of automatic speech recognition (ASR) systems. However, WER is\nsometimes not a good indicator for downstream Natural Language Understanding\n(NLU) tasks, such as intent recognition, slot filling, and semantic parsing in\ntask-oriented dialog systems. This is because WER takes into consideration only\nliteral correctness instead of semantic correctness, the latter of which is\ntypically more important for these downstream tasks. In this study, we propose\na novel Semantic Distance (SemDist) measure as an alternative evaluation metric\nfor ASR systems to address this issue. We define SemDist as the distance\nbetween a reference and hypothesis pair in a sentence-level embedding space. To\nrepresent the reference and hypothesis as a sentence embedding, we exploit\nRoBERTa, a state-of-the-art pre-trained deep contextualized language model\nbased on the transformer architecture. We demonstrate the effectiveness of our\nproposed metric on various downstream tasks, including intent recognition,\nsemantic parsing, and named entity recognition.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:25:07 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kim", "Suyoun", ""], ["Arora", "Abhinav", ""], ["Le", "Duc", ""], ["Yeh", "Ching-Feng", ""], ["Fuegen", "Christian", ""], ["Kalinli", "Ozlem", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "2104.02145", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman and George E. Dahl", "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation for many natural language understanding (NLU) tasks is broken:\nUnreliable and biased systems score so highly on standard benchmarks that there\nis little room for researchers who develop better systems to demonstrate their\nimprovements. The recent trend to abandon IID benchmarks in favor of\nadversarially-constructed, out-of-distribution test sets ensures that current\nmodels will perform poorly, but ultimately only obscures the abilities that we\nwant our benchmarks to measure. In this position paper, we lay out four\ncriteria that we argue NLU benchmarks should meet. We argue most current\nbenchmarks fail at these criteria, and that adversarial data collection does\nnot meaningfully address the causes of these failures. Instead, restoring a\nhealthy evaluation ecosystem will require significant progress in the design of\nbenchmark datasets, the reliability with which they are annotated, their size,\nand the ways they handle social bias.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:36:11 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 20:29:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Dahl", "George E.", ""]]}, {"id": "2104.02176", "submitter": "Yangyang Shi", "authors": "Yangyang Shi, Varun Nagaraja, Chunyang Wu, Jay Mahadeokar, Duc Le,\n  Rohit Prabhavalkar, Alex Xiao, Ching-Feng Yeh, Julian Chan, Christian Fuegen,\n  Ozlem Kalinli, Michael L. Seltzer", "title": "Dynamic Encoder Transducer: A Flexible Solution For Trading Off Accuracy\n  For Latency", "comments": "5 pages, 2 figures, submitted Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic encoder transducer (DET) for on-device speech\nrecognition. One DET model scales to multiple devices with different\ncomputation capacities without retraining or finetuning. To trading off\naccuracy and latency, DET assigns different encoders to decode different parts\nof an utterance. We apply and compare the layer dropout and the collaborative\nlearning for DET training. The layer dropout method that randomly drops out\nencoder layers in the training phase, can do on-demand layer dropout in\ndecoding. Collaborative learning jointly trains multiple encoders with\ndifferent depths in one single model. Experiment results on Librispeech and\nin-house data show that DET provides a flexible accuracy and latency trade-off.\nResults on Librispeech show that the full-size encoder in DET relatively\nreduces the word error rate of the same size baseline by over 8%. The\nlightweight encoder in DET trained with collaborative learning reduces the\nmodel size by 25% but still gets similar WER as the full-size baseline. DET\ngets similar accuracy as a baseline model with better latency on a large\nin-house data set by assigning a lightweight encoder for the beginning part of\none utterance and a full-size encoder for the rest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 22:32:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Shi", "Yangyang", ""], ["Nagaraja", "Varun", ""], ["Wu", "Chunyang", ""], ["Mahadeokar", "Jay", ""], ["Le", "Duc", ""], ["Prabhavalkar", "Rohit", ""], ["Xiao", "Alex", ""], ["Yeh", "Ching-Feng", ""], ["Chan", "Julian", ""], ["Fuegen", "Christian", ""], ["Kalinli", "Ozlem", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "2104.02194", "submitter": "Duc Le", "authors": "Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim, Yangyang Shi, Jay\n  Mahadeokar, Julian Chan, Yuan Shangguan, Christian Fuegen, Ozlem Kalinli,\n  Yatharth Saraf, Michael L. Seltzer", "title": "Contextualized Streaming End-to-End Speech Recognition with Trie-Based\n  Deep Biasing and Shallow Fusion", "comments": "Accepted for presentation at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to leverage dynamic contextual information in end-to-end speech\nrecognition has remained an active research area. Previous solutions to this\nproblem were either designed for specialized use cases that did not generalize\nwell to open-domain scenarios, did not scale to large biasing lists, or\nunderperformed on rare long-tail words. We address these limitations by\nproposing a novel solution that combines shallow fusion, trie-based deep\nbiasing, and neural network language model contextualization. These techniques\nresult in significant 19.5% relative Word Error Rate improvement over existing\ncontextual biasing approaches and 5.4%-9.3% improvement compared to a strong\nhybrid baseline on both open-domain and constrained contextualization tasks,\nwhere the targets consist of mostly rare long-tail words. Our final system\nremains lightweight and modular, allowing for quick modification without model\nre-training.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 23:59:43 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:10:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Le", "Duc", ""], ["Jain", "Mahaveer", ""], ["Keren", "Gil", ""], ["Kim", "Suyoun", ""], ["Shi", "Yangyang", ""], ["Mahadeokar", "Jay", ""], ["Chan", "Julian", ""], ["Shangguan", "Yuan", ""], ["Fuegen", "Christian", ""], ["Kalinli", "Ozlem", ""], ["Saraf", "Yatharth", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "2104.02205", "submitter": "Shuyang Cao", "authors": "Shuyang Cao and Lu Wang", "title": "Attention Head Masking for Inference Time Content Selection in\n  Abstractive Summarization", "comments": "Accepted at NAACL 2021 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we effectively inform content selection in Transformer-based\nabstractive summarization models? In this work, we present a\nsimple-yet-effective attention head masking technique, which is applied on\nencoder-decoder attentions to pinpoint salient content at inference time. Using\nattention head masking, we are able to reveal the relation between\nencoder-decoder attentions and content selection behaviors of summarization\nmodels. We then demonstrate its effectiveness on three document summarization\ndatasets based on both in-domain and cross-domain settings. Importantly, our\nmodels outperform prior state-of-the-art models on CNN/Daily Mail and New York\nTimes datasets. Moreover, our inference-time masking technique is also\ndata-efficient, requiring only 20% of the training samples to outperform BART\nfine-tuned on the full CNN/DailyMail dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:49:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Cao", "Shuyang", ""], ["Wang", "Lu", ""]]}, {"id": "2104.02207", "submitter": "Yuan Shangguan", "authors": "Yuan Shangguan, Rohit Prabhavalkar, Hang Su, Jay Mahadeokar, Yangyang\n  Shi, Jiatong Zhou, Chunyang Wu, Duc Le, Ozlem Kalinli, Christian Fuegen,\n  Michael L. Seltzer", "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition", "comments": "Proc. of Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As speech-enabled devices such as smartphones and smart speakers become\nincreasingly ubiquitous, there is growing interest in building automatic speech\nrecognition (ASR) systems that can run directly on-device; end-to-end (E2E)\nspeech recognition models such as recurrent neural network transducers and\ntheir variants have recently emerged as prime candidates for this task. Apart\nfrom being accurate and compact, such systems need to decode speech with low\nuser-perceived latency (UPL), producing words as soon as they are spoken. This\nwork examines the impact of various techniques - model architectures, training\ncriteria, decoding hyperparameters, and endpointer parameters - on UPL. Our\nanalyses suggest that measures of model size (parameters, input chunk sizes),\nor measures of computation (e.g., FLOPS, RTF) that reflect the model's ability\nto process input frames are not always strongly correlated with observed UPL.\nThus, conventional algorithmic latency measurements might be inadequate in\naccurately capturing latency observed when models are deployed on embedded\ndevices. Instead, we find that factors affecting token emission latency, and\nendpointing behavior have a larger impact on UPL. We achieve the best trade-off\nbetween latency and word error rate when performing ASR jointly with\nendpointing, while utilizing the recently proposed alignment regularization\nmechanism.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:55:11 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 02:22:57 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Shangguan", "Yuan", ""], ["Prabhavalkar", "Rohit", ""], ["Su", "Hang", ""], ["Mahadeokar", "Jay", ""], ["Shi", "Yangyang", ""], ["Zhou", "Jiatong", ""], ["Wu", "Chunyang", ""], ["Le", "Duc", ""], ["Kalinli", "Ozlem", ""], ["Fuegen", "Christian", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "2104.02232", "submitter": "Jay Mahadeokar", "authors": "Jay Mahadeokar, Yangyang Shi, Yuan Shangguan, Chunyang Wu, Alex Xiao,\n  Hang Su, Duc Le, Ozlem Kalinli, Christian Fuegen, Michael L. Seltzer", "title": "Flexi-Transducer: Optimizing Latency, Accuracy and Compute\n  forMulti-Domain On-Device Scenarios", "comments": "Submitted to Interspeech 2021 (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, the storage and computational constraints of embeddeddevices demand\nthat a single on-device ASR model serve multiple use-cases / domains. In this\npaper, we propose aFlexibleTransducer(FlexiT) for on-device automatic speech\nrecognition to flexibly deal with multiple use-cases / domains with different\naccuracy and latency requirements. Specifically, using a single compact model,\nFlexiT provides a fast response for voice commands, and accurate transcription\nbut with more latency for dictation. In order to achieve flexible and better\naccuracy and latency trade-offs, the following techniques are used. Firstly, we\npropose using domain-specific altering of segment size for Emformer encoder\nthat enables FlexiT to achieve flexible de-coding. Secondly, we use Alignment\nRestricted RNNT loss to achieve flexible fine-grained control on token emission\nlatency for different domains. Finally, we add a domain indicator vector as an\nadditional input to the FlexiT model. Using the combination of techniques, we\nshow that a single model can be used to improve WERs and real time factor for\ndictation scenarios while maintaining optimal latency for voice commands\nuse-cases\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:50:19 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mahadeokar", "Jay", ""], ["Shi", "Yangyang", ""], ["Shangguan", "Yuan", ""], ["Wu", "Chunyang", ""], ["Xiao", "Alex", ""], ["Su", "Hang", ""], ["Le", "Duc", ""], ["Kalinli", "Ozlem", ""], ["Fuegen", "Christian", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "2104.02242", "submitter": "Olawale Onabola", "authors": "Olawale Onabola, Zhuang Ma, Yang Xie, Benjamin Akera, Abdulrahman\n  Ibraheem, Jia Xue, Dianbo Liu, Yoshua Bengio", "title": "hBert + BiasCorp -- Fighting Racism on the Web", "comments": null, "journal-ref": "ltedi-1. 4 (2021) 26-33", "doi": null, "report-no": "2021.ltedi-1.4 2021.ltedi-1.4", "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subtle and overt racism is still present both in physical and online\ncommunities today and has impacted many lives in different segments of the\nsociety. In this short piece of work, we present how we're tackling this\nsocietal issue with Natural Language Processing. We are releasing BiasCorp, a\ndataset containing 139,090 comments and news segment from three specific\nsources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually\nannotated) is ready for publication. We are currently in the final phase of\nmanually labeling the remaining dataset using Amazon Mechanical Turk. BERT has\nbeen used widely in several downstream tasks. In this work, we present hBERT,\nwhere we modify certain layers of the pretrained BERT model with the new\nHopfield Layer. hBert generalizes well across different distributions with the\nadded advantage of a reduced model complexity. We are also releasing a\nJavaScript library and a Chrome Extension Application, to help developers make\nuse of our trained model in web applications (say chat application) and for\nusers to identify and report racially biased contents on the web respectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:17:20 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 14:23:24 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Onabola", "Olawale", ""], ["Ma", "Zhuang", ""], ["Xie", "Yang", ""], ["Akera", "Benjamin", ""], ["Ibraheem", "Abdulrahman", ""], ["Xue", "Jia", ""], ["Liu", "Dianbo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2104.02258", "submitter": "Shun-Po Chuang", "authors": "Shun-Po Chuang, Heng-Jui Chang, Sung-Feng Huang, Hung-yi Lee", "title": "Non-autoregressive Mandarin-English Code-switching Speech Recognition\n  with Pinyin Mask-CTC and Word Embedding Regularization", "comments": "5 pages, 1 figure, submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed. Therefore, in this paper, we\ntook advantage of the Mask-CTC NAR ASR framework to tackle the CS speech\nrecognition issue. We propose changing the Mandarin output target of the\nencoder to Pinyin for faster encoder training, and introduce Pinyin-to-Mandarin\ndecoder to learn contextualized information. Moreover, we propose word\nembedding label smoothing to regularize the decoder with contextualized\ninformation and projection matrix regularization to bridge that gap between the\nencoder and decoder. We evaluate the proposed methods on the SEAME corpus and\nachieved exciting results.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:01:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chuang", "Shun-Po", ""], ["Chang", "Heng-Jui", ""], ["Huang", "Sung-Feng", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2104.02284", "submitter": "Ningyu Zhang", "authors": "Luoqiu Li, Zhen Bi, Hongbin Ye, Shumin Deng, Hui Chen, Huaixiao Tou", "title": "Text-guided Legal Knowledge Graph Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 04:42:56 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 13:52:49 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 06:34:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Luoqiu", ""], ["Bi", "Zhen", ""], ["Ye", "Hongbin", ""], ["Deng", "Shumin", ""], ["Chen", "Hui", ""], ["Tou", "Huaixiao", ""]]}, {"id": "2104.02308", "submitter": "Li Bei", "authors": "Bei Li, Quan Du, Tao Zhou, Shuhan Zhou, Xin Zeng, Tong Xiao, Jingbo\n  Zhu", "title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for\n  Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been found that residual networks are an Euler discretization of\nsolutions to Ordinary Differential Equations (ODEs). In this paper, we explore\na deeper relationship between Transformer and numerical methods of ODEs. We\nshow that a residual block of layers in Transformer can be described as a\nhigher-order solution to ODEs. This leads us to design a new architecture (call\nit ODE Transformer) analogous to the Runge-Kutta method that is well motivated\nin ODEs. As a natural extension to Transformer, ODE Transformer is easy to\nimplement and parameter efficient. Our experiments on three WMT tasks\ndemonstrate the genericity of this model, and large improvements in performance\nover several strong baselines. It achieves 30.76 and 44.11 BLEU scores on the\nWMT'14 En-De and En-Fr test data. This sets a new state-of-the-art on the\nWMT'14 En-Fr task.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 06:13:02 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Bei", ""], ["Du", "Quan", ""], ["Zhou", "Tao", ""], ["Zhou", "Shuhan", ""], ["Zeng", "Xin", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""]]}, {"id": "2104.02310", "submitter": "Leshem Choshen", "authors": "Leshem Choshen, Matanel Oren, Dmitry Nikolaev, Omri Abend", "title": "SERRANT: a syntactic classifier for English Grammatical Error Types", "comments": "Code library in: https://github.com/matanel-oren/serrant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SERRANT is a system and code for automatic classification of English\ngrammatical errors that combines SErCl and ERRANT. SERRANT uses ERRANT's\nannotations when they are informative and those provided by SErCl otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 06:26:58 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:36:52 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Choshen", "Leshem", ""], ["Oren", "Matanel", ""], ["Nikolaev", "Dmitry", ""], ["Abend", "Omri", ""]]}, {"id": "2104.02443", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Wei Ding, Llion Jones, Tom Gibbs, Tamas Feher,\n  Christoph Angerer, Silvia Severini, Florian Matthes and Burkhard Rost", "title": "CodeTrans: Towards Cracking the Language of Silicon's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "comments": "28 pages, 6 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, a growing number of mature natural language processing\napplications make people's life more convenient. Such applications are built by\nsource code - the language in software engineering. However, the applications\nfor understanding source code language to ease the software engineering process\nare under-researched. Simultaneously, the transformer model, especially its\ncombination with transfer learning, has been proven to be a powerful technique\nfor natural language processing tasks. These breakthroughs point out a\npromising direction for process source code and crack software engineering\ntasks. This paper describes CodeTrans - an encoder-decoder transformer model\nfor tasks in the software engineering domain, that explores the effectiveness\nof encoder-decoder transformer models for six software engineering tasks,\nincluding thirteen sub-tasks. Moreover, we have investigated the effect of\ndifferent training strategies, including single-task learning, transfer\nlearning, multi-task learning, and multi-task learning with fine-tuning.\nCodeTrans outperforms the state-of-the-art models on all the tasks. To expedite\nfuture works in the software engineering domain, we have published our\npre-trained models of CodeTrans. https://github.com/agemagician/CodeTrans\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:57:12 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:51:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Ding", "Wei", ""], ["Jones", "Llion", ""], ["Gibbs", "Tom", ""], ["Feher", "Tamas", ""], ["Angerer", "Christoph", ""], ["Severini", "Silvia", ""], ["Matthes", "Florian", ""], ["Rost", "Burkhard", ""]]}, {"id": "2104.02484", "submitter": "Petr Marek", "authors": "Petr Marek, Vishal Ishwar Naik, Vincent Auvray, Anuj Goyal", "title": "OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation", "comments": "NAACL 2021 Industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog\nsystem. Most dialog systems are trained on a pool of annotated OOD data to\nachieve this goal. However, collecting the annotated OOD data for a given\ndomain is an expensive process. To mitigate this issue, previous works have\nproposed generative adversarial networks (GAN) based models to generate OOD\ndata for a given domain automatically. However, these proposed models do not\nwork directly with the text. They work with the text's latent space instead,\nenforcing these models to include components responsible for encoding text into\nlatent space and decoding it back, such as auto-encoder. These components\nincrease the model complexity, making it difficult to train. We propose OodGAN,\na sequential generative adversarial network (SeqGAN) based model for OOD data\ngeneration. Our proposed model works directly on the text and hence eliminates\nthe need to include an auto-encoder. OOD data generated using OodGAN model\noutperforms state-of-the-art in OOD detection metrics for ROSTD (67% relative\nimprovement in FPR 0.95) and OSQ datasets (28% relative improvement in FPR\n0.95) (Zheng et al., 2020).\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:08:39 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Marek", "Petr", ""], ["Naik", "Vishal Ishwar", ""], ["Auvray", "Vincent", ""], ["Goyal", "Anuj", ""]]}, {"id": "2104.02496", "submitter": "Matthias A{\\ss}enmacher", "authors": "P. Schulze, S. Wiegrebe, P. W. Thurner, C. Heumann, M. A{\\ss}enmacher,\n  S. Wankm\\\"uller", "title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models such as the Structural Topic Model (STM) estimate latent topical\nclusters within text. An important step in many topic modeling applications is\nto explore relationships between the discovered topical structure and metadata\nassociated with the text documents. Methods used to estimate such relationships\nmust take into account that the topical structure is not directly observed, but\ninstead being estimated itself. The authors of the STM, for instance, perform\nrepeated OLS regressions of sampled topic proportions on metadata covariates by\nusing a Monte Carlo sampling technique known as the method of composition. In\nthis paper, we propose two improvements: first, we replace OLS with more\nappropriate Beta regression. Second, we suggest a fully Bayesian approach\ninstead of the current blending of frequentist and Bayesian methods. We\ndemonstrate our improved methodology by exploring relationships between Twitter\nposts by German members of parliament (MPs) and different metadata covariates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:28:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Schulze", "P.", ""], ["Wiegrebe", "S.", ""], ["Thurner", "P. W.", ""], ["Heumann", "C.", ""], ["A\u00dfenmacher", "M.", ""], ["Wankm\u00fcller", "S.", ""]]}, {"id": "2104.02516", "submitter": "Kathleen Siminyu", "authors": "Kathleen Siminyu, Godson Kalipe, Davor Orlic, Jade Abbott, Vukosi\n  Marivate, Sackey Freshia, Prateek Sibal, Bhanu Neupane, David I. Adelani,\n  Amelia Taylor, Jamiil Toure ALI, Kevin Degila, Momboladji Balogoun, Thierno\n  Ibrahima DIOP, Davis David, Chayma Fourati, Hatem Haddad, Malek Naski", "title": "AI4D -- African Language Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in speech and language technologies enable tools such as\nvoice-search, text-to-speech, speech recognition and machine translation. These\nare however only available for high resource languages like English, French or\nChinese. Without foundational digital resources for African languages, which\nare considered low-resource in the digital context, these advanced tools remain\nout of reach. This work details the AI4D - African Language Program, a 3-part\nproject that 1) incentivised the crowd-sourcing, collection and curation of\nlanguage datasets through an online quantitative and qualitative challenge, 2)\nsupported research fellows for a period of 3-4 months to create datasets\nannotated for NLP tasks, and 3) hosted competitive Machine Learning challenges\non the basis of these datasets. Key outcomes of the work so far include 1) the\ncreation of 9+ open source, African language datasets annotated for a variety\nof ML tasks, and 2) the creation of baseline models for these datasets through\nhosting of competitive ML challenges.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:51:16 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Siminyu", "Kathleen", ""], ["Kalipe", "Godson", ""], ["Orlic", "Davor", ""], ["Abbott", "Jade", ""], ["Marivate", "Vukosi", ""], ["Freshia", "Sackey", ""], ["Sibal", "Prateek", ""], ["Neupane", "Bhanu", ""], ["Adelani", "David I.", ""], ["Taylor", "Amelia", ""], ["ALI", "Jamiil Toure", ""], ["Degila", "Kevin", ""], ["Balogoun", "Momboladji", ""], ["DIOP", "Thierno Ibrahima", ""], ["David", "Davis", ""], ["Fourati", "Chayma", ""], ["Haddad", "Hatem", ""], ["Naski", "Malek", ""]]}, {"id": "2104.02526", "submitter": "Anton Mitrofanov", "authors": "Anton Mitrofanov, Mariya Korenevskaya, Ivan Podluzhny, Yuri Khokhlov,\n  Aleksandr Laptev, Andrei Andrusenko, Aleksei Ilin, Maxim Korenevsky, Ivan\n  Medennikov, Aleksei Romanenko", "title": "LT-LM: a novel non-autoregressive language model for single-shot lattice\n  rescoring", "comments": "Submitted to InterSpeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based language models are commonly used in rescoring\napproaches to improve the quality of modern automatic speech recognition (ASR)\nsystems. Most of the existing methods are computationally expensive since they\nuse autoregressive language models. We propose a novel rescoring approach,\nwhich processes the entire lattice in a single call to the model. The key\nfeature of our rescoring policy is a novel non-autoregressive Lattice\nTransformer Language Model (LT-LM). This model takes the whole lattice as an\ninput and predicts a new language score for each arc. Additionally, we propose\nthe artificial lattices generation approach to incorporate a large amount of\ntext data in the LT-LM training process. Our single-shot rescoring performs\norders of magnitude faster than other rescoring methods in our experiments. It\nis more than 300 times faster than pruned RNNLM lattice rescoring and N-best\nrescoring while slightly inferior in terms of WER.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:06:07 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mitrofanov", "Anton", ""], ["Korenevskaya", "Mariya", ""], ["Podluzhny", "Ivan", ""], ["Khokhlov", "Yuri", ""], ["Laptev", "Aleksandr", ""], ["Andrusenko", "Andrei", ""], ["Ilin", "Aleksei", ""], ["Korenevsky", "Maxim", ""], ["Medennikov", "Ivan", ""], ["Romanenko", "Aleksei", ""]]}, {"id": "2104.02535", "submitter": "Rosanna Turrisi", "authors": "Rosanna Turrisi and Leonardo Badino", "title": "Optimal Transport-based Adaptation in Dysarthric Speech Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many real-world applications, the mismatch between distributions of\ntraining data (source) and test data (target) significantly degrades the\nperformance of machine learning algorithms. In speech data, causes of this\nmismatch include different acoustic environments or speaker characteristics. In\nthis paper, we address this issue in the challenging context of dysarthric\nspeech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we\npropose the use of an optimal-transport based approach, called MSDA via\nWeighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem\nin dysarthria detection for which the proposed approach outperforms both the\nBaseline and the state-of-the-art MSDA models, improving the detection accuracy\nof 0.9% over the best competitor method. We then employ MSDA-WJDOT for\ndysarthric speaker adaptation in command speech recognition. This provides a\nCommand Error Rate relative reduction of 16% and 7% over the baseline and the\nbest competitor model, respectively. Interestingly, MSDA-WJDOT provides a\nsimilarity score between the source and the target, i.e. between speakers in\nthis case. We leverage this similarity measure to define a Dysarthric and\nHealthy score of the target speaker and diagnose the dysarthria with an\naccuracy of 95%.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:26:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Turrisi", "Rosanna", ""], ["Badino", "Leonardo", ""]]}, {"id": "2104.02542", "submitter": "Rosanna Turrisi", "authors": "Rosanna Turrisi, Arianna Braccia, Marco Emanuele, Simone Giulietti,\n  Maura Pugliatti, Mariachiara Sensi, Luciano Fadiga, Leonardo Badino", "title": "EasyCall corpus: a dysarthric speech dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new dysarthric speech command dataset in Italian,\ncalled EasyCall corpus. The dataset consists of 21386 audio recordings from 24\nhealthy and 31 dysarthric speakers, whose individual degree of speech\nimpairment was assessed by neurologists through the Therapy Outcome Measure.\nThe corpus aims at providing a resource for the development of ASR-based\nassistive technologies for patients with dysarthria. In particular, it may be\nexploited to develop a voice-controlled contact application for commercial\nsmartphones, aiming at improving dysarthric patients' ability to communicate\nwith their family and caregivers. Before recording the dataset, participants\nwere administered a survey to evaluate which commands are more likely to be\nemployed by dysarthric individuals in a voice-controlled contact application.\nIn addition, the dataset includes a list of non-commands (i.e., words\nnear/inside commands or phonetically close to commands) that can be leveraged\nto build a more robust command recognition system. At present commercial ASR\nsystems perform poorly on the EasyCall Corpus as we report in this paper. This\nresult corroborates the need for dysarthric speech corpora for developing\neffective assistive technologies. To the best of our knowledge, this database\nrepresents the richest corpus of dysarthric speech to date.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:32:47 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Turrisi", "Rosanna", ""], ["Braccia", "Arianna", ""], ["Emanuele", "Marco", ""], ["Giulietti", "Simone", ""], ["Pugliatti", "Maura", ""], ["Sensi", "Mariachiara", ""], ["Fadiga", "Luciano", ""], ["Badino", "Leonardo", ""]]}, {"id": "2104.02667", "submitter": "Ying Lin", "authors": "Ying Lin, Han Wang, Jiangning Chen, Tong Wang, Yue Liu, Heng Ji, Yang\n  Liu, Premkumar Natarajan", "title": "Personalized Entity Resolution with Dynamic Heterogeneous Knowledge\n  Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of Virtual Assistants poses new challenges for Entity\nResolution, the task of linking mentions in text to their referent entities in\na knowledge base. Specifically, in the shopping domain, customers tend to use\nimplicit utterances (e.g., \"organic milk\") rather than explicit names, leading\nto a large number of candidate products. Meanwhile, for the same query,\ndifferent customers may expect different results. For example, with \"add milk\nto my cart\", a customer may refer to a certain organic product, while some\ncustomers may want to re-order products they regularly purchase. To address\nthese issues, we propose a new framework that leverages personalized features\nto improve the accuracy of product ranking. We first build a cross-source\nheterogeneous knowledge graph from customer purchase history and product\nknowledge graph to jointly learn customer and product embeddings. After that,\nwe incorporate product, customer, and history representations into a neural\nreranking model to predict which candidate is most likely to be purchased for a\nspecific customer. Experiments show that our model substantially improves the\naccuracy of the top ranked candidates by 24.6% compared to the state-of-the-art\nproduct search model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:58:27 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:00:42 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lin", "Ying", ""], ["Wang", "Han", ""], ["Chen", "Jiangning", ""], ["Wang", "Tong", ""], ["Liu", "Yue", ""], ["Ji", "Heng", ""], ["Liu", "Yang", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "2104.02689", "submitter": "Kun Qian", "authors": "Kun Qian, Wei Wei, Zhou Yu", "title": "A Student-Teacher Architecture for Dialog Domain Adaptation under the\n  Meta-Learning Setting", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous new dialog domains are being created every day while collecting data\nfor these domains is extremely costly since it involves human interactions.\nTherefore, it is essential to develop algorithms that can adapt to different\ndomains efficiently when building data-driven dialog models. The most recent\nresearches on domain adaption focus on giving the model a better\ninitialization, rather than optimizing the adaptation process. We propose an\nefficient domain adaptive task-oriented dialog system model, which incorporates\na meta-teacher model to emphasize the different impacts between generated\ntokens with respect to the context. We first train our base dialog model and\nmeta-teacher model adversarially in a meta-learning setting on rich-resource\ndomains. The meta-teacher learns to quantify the importance of tokens under\ndifferent contexts across different domains. During adaptation, the\nmeta-teacher guides the dialog model to focus on important tokens in order to\nachieve better adaptation efficiency. We evaluate our model on two multi-domain\ndatasets, MultiWOZ and Google Schema-Guided Dialogue, and achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:31:28 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Qian", "Kun", ""], ["Wei", "Wei", ""], ["Yu", "Zhou", ""]]}, {"id": "2104.02704", "submitter": "Canwen Xu", "authors": "Canwen Xu and Wangchunshu Zhou and Tao Ge and Ke Xu and Julian McAuley\n  and Furu Wei", "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with\n  Common Sense and World Knowledge", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cant is important for understanding advertising, comedies and dog-whistle\npolitics. However, computational research on cant is hindered by a lack of\navailable datasets. In this paper, we propose a large and diverse Chinese\ndataset for creating and understanding cant from a computational linguistics\nperspective. We formulate a task for cant understanding and provide both\nquantitative and qualitative analysis for tested word embedding similarity and\npretrained language models. Experiments suggest that such a task requires deep\nlanguage understanding, common sense, and world knowledge and thus can be a\ngood testbed for pretrained language models and help models perform better on\nother tasks. The code is available at https://github.com/JetRunner/dogwhistle.\nThe data and leaderboard are available at\nhttps://competitions.codalab.org/competitions/30451.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:55:43 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:09:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xu", "Canwen", ""], ["Zhou", "Wangchunshu", ""], ["Ge", "Tao", ""], ["Xu", "Ke", ""], ["McAuley", "Julian", ""], ["Wei", "Furu", ""]]}, {"id": "2104.02724", "submitter": "Jumon Nozaki", "authors": "Jumon Nozaki, Tatsuya Komatsu", "title": "Relaxing the Conditional Independence Assumption of CTC-based ASR by\n  Conditioning on Intermediate Predictions", "comments": "Submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to relax the conditional independence assumption\nof connectionist temporal classification (CTC)-based automatic speech\nrecognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC\nlosses in intermediate layers in addition to the original CTC loss in the last\nlayer. During both training and inference, each generated prediction in the\nintermediate layers is summed to the input of the next layer to condition the\nprediction of the last layer on those intermediate predictions. Our method is\neasy to implement and retains the merits of CTC-based ASR: a simple model\narchitecture and fast decoding speed. We conduct experiments on three different\nASR corpora. Our proposed method improves a standard CTC model significantly\n(e.g., more than 20 % relative word error rate reduction on the WSJ corpus)\nwith a little computational overhead. Moreover, for the TEDLIUM2 corpus and the\nAISHELL-1 corpus, it achieves a comparable performance to a strong\nautoregressive model with beam search, but the decoding speed is at least 30\ntimes faster.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:00:03 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Nozaki", "Jumon", ""], ["Komatsu", "Tatsuya", ""]]}, {"id": "2104.02756", "submitter": "Fran\\c{c}ois Mercier", "authors": "Fran\\c{c}ois Mercier", "title": "Efficient transfer learning for NLP with ELECTRA", "comments": "Submission for ML Reproducibility Challenge 2020", "journal-ref": "Machine Learning Reproducibility Challenge 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clark et al. [2020] claims that the ELECTRA approach is highly efficient in\nNLP performances relative to computation budget. As such, this reproducibility\nstudy focus on this claim, summarized by the following question: Can we use\nELECTRA to achieve close to SOTA performances for NLP in low-resource settings,\nin term of compute cost?\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 19:34:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mercier", "Fran\u00e7ois", ""]]}, {"id": "2104.02797", "submitter": "Archit Rathore", "authors": "Archit Rathore, Sunipa Dev, Jeff M. Phillips, Vivek Srikumar, Yan\n  Zheng, Chin-Chia Michael Yeh, Junpeng Wang, Wei Zhang, Bei Wang", "title": "VERB: Visualizing and Interpreting Bias Mitigation Techniques for Word\n  Representations", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word vector embeddings have been shown to contain and amplify biases in data\nthey are extracted from. Consequently, many techniques have been proposed to\nidentify, mitigate, and attenuate these biases in word representations. In this\npaper, we utilize interactive visualization to increase the interpretability\nand accessibility of a collection of state-of-the-art debiasing techniques. To\naid this, we present Visualization of Embedding Representations for deBiasing\nsystem (\"VERB\"), an open-source web-based visualization tool that helps the\nusers gain a technical understanding and visual intuition of the inner workings\nof debiasing techniques, with a focus on their geometric properties. In\nparticular, VERB offers easy-to-follow use cases in exploring the effects of\nthese debiasing techniques on the geometry of high-dimensional word vectors. To\nhelp understand how various debiasing techniques change the underlying\ngeometry, VERB decomposes each technique into interpretable sequences of\nprimitive transformations and highlights their effect on the word vectors using\ndimensionality reduction and interactive visual exploration. VERB is designed\nto target natural language processing (NLP) practitioners who are designing\ndecision-making systems on top of word embeddings, and also researchers working\nwith fairness and ethics of machine learning systems in NLP. It can also serve\nas a visual medium for education, which helps an NLP novice to understand and\nmitigate biases in word embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:29:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rathore", "Archit", ""], ["Dev", "Sunipa", ""], ["Phillips", "Jeff M.", ""], ["Srikumar", "Vivek", ""], ["Zheng", "Yan", ""], ["Yeh", "Chin-Chia Michael", ""], ["Wang", "Junpeng", ""], ["Zhang", "Wei", ""], ["Wang", "Bei", ""]]}, {"id": "2104.02831", "submitter": "Hassan S. Shavarani", "authors": "Hassan S. Shavarani and Anoop Sarkar", "title": "Better Neural Machine Translation by Extracting Linguistic Information\n  from BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adding linguistic information (syntax or semantics) to neural machine\ntranslation (NMT) has mostly focused on using point estimates from pre-trained\nmodels. Directly using the capacity of massive pre-trained contextual word\nembedding models such as BERT (Devlin et al., 2019) has been marginally useful\nin NMT because effective fine-tuning is difficult to obtain for NMT without\nmaking training brittle and unreliable. We augment NMT by extracting dense\nfine-tuned vector-based linguistic information from BERT instead of using point\nestimates. Experimental results show that our method of incorporating\nlinguistic information helps NMT to generalize better in a variety of training\ncontexts and is no more difficult to train than conventional Transformer-based\nNMT.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 00:03:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Shavarani", "Hassan S.", ""], ["Sarkar", "Anoop", ""]]}, {"id": "2104.02851", "submitter": "Liu Chen", "authors": "Liu Chen, Meysam Asgari", "title": "Interpreting A Pre-trained Model Is A Key For Model Architecture\n  Optimization: A Case Study On Wav2Vec 2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deep Transformer model with good evaluation score does not mean each\nsubnetwork (a.k.a transformer block) learns reasonable representation.\nDiagnosing abnormal representation and avoiding it can contribute to achieving\na better evaluation score. We propose an innovative perspective for analyzing\nattention patterns: summarize block-level patterns and assume abnormal patterns\ncontribute negative influence. We leverage Wav2Vec 2.0 as a research target and\nanalyze a pre-trained model's pattern. All experiments leverage\nLibrispeech-100-clean as training data. Through avoiding diagnosed abnormal\nones, our custom Wav2Vec 2.0 outperforms the original version about 4.8%\nabsolute word error rate (WER) on test-clean with viterbi decoding. Our version\nis still 0.9% better when decoding with a 4-gram language model. Moreover, we\nidentify that avoiding abnormal patterns is the main contributor for\nperformance boosting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:41:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Chen", "Liu", ""], ["Asgari", "Meysam", ""]]}, {"id": "2104.02882", "submitter": "Zhengkun Tian", "authors": "Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai Zhang, Zhengqi\n  Wen", "title": "FSR: Accelerating the Inference Process of Transducer-Based Models by\n  Applying Fast-Skip Regularization", "comments": "Submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transducer-based models, such as RNN-Transducer and transformer-transducer,\nhave achieved great success in speech recognition. A typical transducer model\ndecodes the output sequence conditioned on the current acoustic state and\npreviously predicted tokens step by step. Statistically, The number of blank\ntokens in the prediction results accounts for nearly 90\\% of all tokens. It\ntakes a lot of computation and time to predict the blank tokens, but only the\nnon-blank tokens will appear in the final output sequence. Therefore, we\npropose a method named fast-skip regularization, which tries to align the blank\nposition predicted by a transducer with that predicted by a CTC model. During\nthe inference, the transducer model can predict the blank tokens in advance by\na simple CTC project layer without many complicated forward calculations of the\ntransducer decoder and then skip them, which will reduce the computation and\nimprove the inference speed greatly. All experiments are conducted on a public\nChinese mandarin dataset AISHELL-1. The results show that the fast-skip\nregularization can indeed help the transducer model learn the blank position\nalignments. Besides, the inference with fast-skip can be speeded up nearly 4\ntimes with only a little performance degradation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:15:10 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tian", "Zhengkun", ""], ["Yi", "Jiangyan", ""], ["Bai", "Ye", ""], ["Tao", "Jianhua", ""], ["Zhang", "Shuai", ""], ["Wen", "Zhengqi", ""]]}, {"id": "2104.02934", "submitter": "Jiayang Cheng", "authors": "Jiayang Cheng, Haiyun Jiang, Deqing Yang, Yanghua Xiao", "title": "A Question-answering Based Framework for Relation Extraction Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction is an important task in knowledge acquisition and text\nunderstanding. Existing works mainly focus on improving relation extraction by\nextracting effective features or designing reasonable model structures.\nHowever, few works have focused on how to validate and correct the results\ngenerated by the existing relation extraction models. We argue that validation\nis an important and promising direction to further improve the performance of\nrelation extraction. In this paper, we explore the possibility of using\nquestion answering as validation. Specifically, we propose a novel\nquestion-answering based framework to validate the results from relation\nextraction models. Our proposed framework can be easily applied to existing\nrelation classifiers without any additional information. We conduct extensive\nexperiments on the popular NYT dataset to evaluate the proposed framework, and\nobserve consistent improvements over five strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:08:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cheng", "Jiayang", ""], ["Jiang", "Haiyun", ""], ["Yang", "Deqing", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2104.03006", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Andr\\'e Merboldt, Wilfried Michel, Ralf Schl\\\"uter,\n  Hermann Ney", "title": "Librispeech Transducer Model with Internal Language Model Prior\n  Correction", "comments": "accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our transducer model on Librispeech. We study variants to include\nan external language model (LM) with shallow fusion and subtract an estimated\ninternal LM. This is justified by a Bayesian interpretation where the\ntransducer model prior is given by the estimated internal LM. The subtraction\nof the internal LM gives us over 14% relative improvement over normal shallow\nfusion. Our transducer has a separate probability distribution for the\nnon-blank labels which allows for easier combination with the external LM, and\neasier estimation of the internal LM. We additionally take care of including\nthe end-of-sentence (EOS) probability of the external LM in the last blank\nprobability which further improves the performance. All our code and setups are\npublished.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:18:56 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 00:09:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zeyer", "Albert", ""], ["Merboldt", "Andr\u00e9", ""], ["Michel", "Wilfried", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.03026", "submitter": "Christian Hardmeier", "authors": "Christian Hardmeier, Marta R. Costa-juss\\`a, Kellie Webster, Will\n  Radford and Su Lin Blodgett", "title": "How to Write a Bias Statement: Recommendations for Submissions to the\n  Workshop on Gender Bias in NLP", "comments": "This document was originally published as a blog post on the web site\n  of GeBNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  At the Workshop on Gender Bias in NLP (GeBNLP), we'd like to encourage\nauthors to give explicit consideration to the wider aspects of bias and its\nsocial implications. For the 2020 edition of the workshop, we therefore\nrequested that all authors include an explicit bias statement in their work to\nclarify how their work relates to the social context in which NLP systems are\nused.\n  The programme committee of the workshops included a number of reviewers with\na background in the humanities and social sciences, in addition to NLP experts\ndoing the bulk of the reviewing. Each paper was assigned one of those\nreviewers, and they were asked to pay specific attention to the provided bias\nstatements in their reviews. This initiative was well received by the authors\nwho submitted papers to the workshop, several of whom said they received useful\nsuggestions and literature hints from the bias reviewers. We are therefore\nplanning to keep this feature of the review process in future editions of the\nworkshop.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:00:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hardmeier", "Christian", ""], ["Costa-juss\u00e0", "Marta R.", ""], ["Webster", "Kellie", ""], ["Radford", "Will", ""], ["Blodgett", "Su Lin", ""]]}, {"id": "2104.03057", "submitter": "Chenxin An", "authors": "Chenxin An, Ming Zhong, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing\n  Huang", "title": "Enhancing Scientific Papers Summarization with Citation Graph", "comments": "accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work for text summarization in scientific domain mainly focused on\nthe content of the input document, but seldom considering its citation network.\nHowever, scientific papers are full of uncommon domain-specific terms, making\nit almost impossible for the model to understand its true meaning without the\nhelp of the relevant research community. In this paper, we redefine the task of\nscientific papers summarization by utilizing their citation graph and propose a\ncitation graph-based summarization model CGSum which can incorporate the\ninformation of both the source paper and its references. In addition, we\nconstruct a novel scientific papers summarization dataset Semantic Scholar\nNetwork (SSN) which contains 141K research papers in different domains and 661K\ncitation relationships. The entire dataset constitutes a large connected\ncitation graph. Extensive experiments show that our model can achieve\ncompetitive performance when compared with the pretrained models even with a\nsimple architecture. The results also indicates the citation graph is crucial\nto better understand the content of papers and generate high-quality summaries.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:13:35 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["An", "Chenxin", ""], ["Zhong", "Ming", ""], ["Chen", "Yiran", ""], ["Wang", "Danqing", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2104.03071", "submitter": "Vijit Malik", "authors": "Aditya Jindal, Ankur Gupta, Jaya Srivastava, Preeti Menghwani, Vijit\n  Malik, Vishesh Kaushik, Ashutosh Modi", "title": "BreakingBERT@IITK at SemEval-2021 Task 9 : Statement Verification and\n  Evidence Finding with Tables", "comments": "Accepted at SemEval 2021 Task 9, 11 Pages (8 Pages main content+ 1\n  pages for references + 2 Pages Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been an interest in factual verification and prediction\nover structured data like tables and graphs. To circumvent any false news\nincident, it is necessary to not only model and predict over structured data\nefficiently but also to explain those predictions. In this paper, as part of\nthe SemEval-2021 Task 9, we tackle the problem of fact verification and\nevidence finding over tabular data. There are two subtasks. Given a table and a\nstatement/fact, subtask A determines whether the statement is inferred from the\ntabular data, and subtask B determines which cells in the table provide\nevidence for the former subtask. We make a comparison of the baselines and\nstate-of-the-art approaches over the given SemTabFact dataset. We also propose\na novel approach CellBERT to solve evidence finding as a form of the Natural\nLanguage Inference task. We obtain a 3-way F1 score of 0.69 on subtask A and an\nF1 score of 0.65 on subtask B.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:41:07 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 10:08:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jindal", "Aditya", ""], ["Gupta", "Ankur", ""], ["Srivastava", "Jaya", ""], ["Menghwani", "Preeti", ""], ["Malik", "Vijit", ""], ["Kaushik", "Vishesh", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.03090", "submitter": "Firoj Alam", "authors": "Firoj Alam, Umair Qazi, Muhammad Imran, Ferda Ofli", "title": "HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep\n  Learning Benchmarks", "comments": "Accepted in ICWSM-2021, Twitter datasets, Textual content, Natural\n  disasters, Crisis Informatics, Benchmarks, Transformers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social networks are widely used for information consumption and\ndissemination, especially during time-critical events such as natural\ndisasters. Despite its significantly large volume, social media content is\noften too noisy for direct use in any application. Therefore, it is important\nto filter, categorize, and concisely summarize the available content to\nfacilitate effective consumption and decision-making. To address such issues\nautomatic classification systems have been developed using supervised modeling\napproaches, thanks to the earlier efforts on creating labeled datasets.\nHowever, existing datasets are limited in different aspects (e.g., size,\ncontains duplicates) and less suitable to support more advanced and data-hungry\ndeep learning models. In this paper, we present a new large-scale dataset with\n~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19\ndisaster events that happened between 2016 and 2019. Moreover, we propose a\ndata collection and sampling pipeline, which is important for social media data\nsampling for human annotation. We report multiclass classification results\nusing classic and deep learning (fastText and transformer) based models to set\nthe ground for future studies. The dataset and associated resources are\npublicly available. https://crisisnlp.qcri.org/humaid_dataset.html\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:29:36 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 09:12:11 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Alam", "Firoj", ""], ["Qazi", "Umair", ""], ["Imran", "Muhammad", ""], ["Ofli", "Ferda", ""]]}, {"id": "2104.03189", "submitter": "Tunazzina Islam", "authors": "Tunazzina Islam, Dan Goldwasser", "title": "Analysis of Twitter Users' Lifestyle Choices using Joint Embedding Model", "comments": "accepted at 15th International AAAI Conference on Web and Social\n  Media (ICWSM-2021), 12 pages. Minor changes for camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview representation learning of data can help construct coherent and\ncontextualized users' representations on social media. This paper suggests a\njoint embedding model, incorporating users' social and textual information to\nlearn contextualized user representations used for understanding their\nlifestyle choices. We apply our model to tweets related to two lifestyle\nactivities, `Yoga' and `Keto diet' and use it to analyze users' activity type\nand motivation. We explain the data collection and annotation process in detail\nand provide an in-depth analysis of users from different classes based on their\nTwitter content. Our experiments show that our model results in performance\nimprovements in both domains.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:29:36 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 15:36:19 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 18:14:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Islam", "Tunazzina", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2104.03190", "submitter": "Masato Hagiwara", "authors": "Masato Hagiwara, Joshua Tanner, Keisuke Sakaguchi", "title": "GrammarTagger: A Multilingual, Minimally-Supervised Grammar Profiler for\n  Language Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GrammarTagger, an open-source grammar profiler which, given an\ninput text, identifies grammatical features useful for language education. The\nmodel architecture enables it to learn from a small amount of texts annotated\nwith spans and their labels, which 1) enables easier and more intuitive\nannotation, 2) supports overlapping spans, and 3) is less prone to error\npropagation, compared to complex hand-crafted rules defined on\nconstituency/dependency parses. We show that we can bootstrap a grammar\nprofiler model with $F_1 \\approx 0.6$ from only a couple hundred sentences both\nin English and Chinese, which can be further boosted via learning a\nmultilingual model. With GrammarTagger, we also build Octanove Learn, a search\nengine of language learning materials indexed by their reading difficulty and\ngrammatical features. The code and pretrained models are publicly available at\n\\url{https://github.com/octanove/grammartagger}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:31:20 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hagiwara", "Masato", ""], ["Tanner", "Joshua", ""], ["Sakaguchi", "Keisuke", ""]]}, {"id": "2104.03204", "submitter": "Marc-Antoine Georges", "authors": "Marc-Antoine Georges, Laurent Girin, Jean-Luc Schwartz, Thomas Hueber", "title": "Learning robust speech representation with an articulatory-regularized\n  variational autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is increasingly considered that human speech perception and production\nboth rely on articulatory representations. In this paper, we investigate\nwhether this type of representation could improve the performances of a deep\ngenerative model (here a variational autoencoder) trained to encode and decode\nacoustic speech features. First we develop an articulatory model able to\nassociate articulatory parameters describing the jaw, tongue, lips and velum\nconfigurations with vocal tract shapes and spectral features. Then we\nincorporate these articulatory parameters into a variational autoencoder\napplied on spectral features by using a regularization technique that\nconstraints part of the latent space to follow articulatory trajectories. We\nshow that this articulatory constraint improves model training by decreasing\ntime to convergence and reconstruction loss at convergence, and yields better\nperformance in a speech denoising task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:47:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Georges", "Marc-Antoine", ""], ["Girin", "Laurent", ""], ["Schwartz", "Jean-Luc", ""], ["Hueber", "Thomas", ""]]}, {"id": "2104.03236", "submitter": "Herv\\'e Le Borgne", "authors": "Omar Adjali and Romaric Besan\\c{c}on and Olivier Ferret and Herve Le\n  Borgne and Brigitte Grau", "title": "Multimodal Entity Linking for Tweets", "comments": null, "journal-ref": "In: Jose J. et al. (eds) Advances in Information Retrieval. ECIR\n  2020. Lecture Notes in Computer Science, vol 12035. Springer, Cham", "doi": "10.1007/978-3-030-45439-5_31", "report-no": null, "categories": "cs.IR cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many information extraction applications, entity linking (EL) has emerged\nas a crucial task that allows leveraging information about named entities from\na knowledge base. In this paper, we address the task of multimodal entity\nlinking (MEL), an emerging research field in which textual and visual\ninformation is used to map an ambiguous mention to an entity in a knowledge\nbase (KB). First, we propose a method for building a fully annotated Twitter\ndataset for MEL, where entities are defined in a Twitter KB. Then, we propose a\nmodel for jointly learning a representation of both mentions and entities from\ntheir textual and visual contexts. We demonstrate the effectiveness of the\nproposed model by evaluating it on the proposed dataset and highlight the\nimportance of leveraging visual information when it is available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:40:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Adjali", "Omar", ""], ["Besan\u00e7on", "Romaric", ""], ["Ferret", "Olivier", ""], ["Borgne", "Herve Le", ""], ["Grau", "Brigitte", ""]]}, {"id": "2104.03285", "submitter": "Chenghua Lin", "authors": "Rui Mao, Chenghua Lin, Frank Guerin", "title": "Combining Pre-trained Word Embeddings and Linguistic Features for\n  Sequential Metaphor Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of identifying metaphors in text, treated as a sequence\ntagging task. The pre-trained word embeddings GloVe, ELMo and BERT have\nindividually shown good performance on sequential metaphor identification.\nThese embeddings are generated by different models, training targets and\ncorpora, thus encoding different semantic and syntactic information. We show\nthat leveraging GloVe, ELMo and feature-based BERT based on a multi-channel CNN\nand a Bidirectional LSTM model can significantly outperform any single word\nembedding method and the combination of the two embeddings. Incorporating\nlinguistic features into our model can further improve model performance,\nyielding state-of-the-art performance on three public metaphor datasets. We\nalso provide in-depth analysis on the effectiveness of leveraging multiple word\nembeddings, including analysing the spatial distribution of different embedding\nmethods for metaphors and literals, and showing how well the embeddings\ncomplement each other in different genres and parts of speech.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:43:05 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mao", "Rui", ""], ["Lin", "Chenghua", ""], ["Guerin", "Frank", ""]]}, {"id": "2104.03343", "submitter": "Sravana Reddy", "authors": "Rezvaneh Rezapour and Sravana Reddy and Ann Clifton and Rosie Jones", "title": "Spotify at TREC 2020: Genre-Aware Abstractive Podcast Summarization", "comments": "The Twenty-Ninth Text REtrieval Conference (TREC 2020) Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper contains the description of our submissions to the summarization\ntask of the Podcast Track in TREC (the Text REtrieval Conference) 2020. The\ngoal of this challenge was to generate short, informative summaries that\ncontain the key information present in a podcast episode using automatically\ngenerated transcripts of the podcast audio. Since podcasts vary with respect to\ntheir genre, topic, and granularity of information, we propose two\nsummarization models that explicitly take genre and named entities into\nconsideration in order to generate summaries appropriate to the style of the\npodcasts. Our models are abstractive, and supervised using creator-provided\ndescriptions as ground truth summaries. The results of the submitted summaries\nshow that our best model achieves an aggregate quality score of 1.58 in\ncomparison to the creator descriptions and a baseline abstractive system which\nboth score 1.49 (an improvement of 9%) as assessed by human evaluators.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:27:28 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Rezapour", "Rezvaneh", ""], ["Reddy", "Sravana", ""], ["Clifton", "Ann", ""], ["Jones", "Rosie", ""]]}, {"id": "2104.03364", "submitter": "Masato Hagiwara", "authors": "Hitoshi Manabe, Masato Hagiwara", "title": "EXPATS: A Toolkit for Explainable Automated Text Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated text scoring (ATS) tasks, such as automated essay scoring and\nreadability assessment, are important educational applications of natural\nlanguage processing. Due to their interpretability of models and predictions,\ntraditional machine learning (ML) algorithms based on handcrafted features are\nstill in wide use for ATS tasks. Practitioners often need to experiment with a\nvariety of models (including deep and traditional ML ones), features, and\ntraining objectives (regression and classification), although modern deep\nlearning frameworks such as PyTorch require deep ML expertise to fully utilize.\nIn this paper, we present EXPATS, an open-source framework to allow its users\nto develop and experiment with different ATS models quickly by offering\nflexible components, an easy-to-use configuration system, and the command-line\ninterface. The toolkit also provides seamless integration with the Language\nInterpretability Tool (LIT) so that one can interpret and visualize models and\ntheir predictions. We also describe two case studies where we build ATS models\nquickly with minimal engineering efforts. The toolkit is available at\n\\url{https://github.com/octanove/expats}.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:29:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Manabe", "Hitoshi", ""], ["Hagiwara", "Masato", ""]]}, {"id": "2104.03391", "submitter": "Chenghua Lin", "authors": "Rui Mao, Chenghua Lin, Frank Guerin", "title": "Interpreting Verbal Metaphors by Paraphrasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metaphorical expressions are difficult linguistic phenomena, challenging\ndiverse Natural Language Processing tasks. Previous works showed that\nparaphrasing a metaphor as its literal counterpart can help machines better\nprocess metaphors on downstream tasks. In this paper, we interpret metaphors\nwith BERT and WordNet hypernyms and synonyms in an unsupervised manner, showing\nthat our method significantly outperforms the state-of-the-art baseline. We\nalso demonstrate that our method can help a machine translation system improve\nits accuracy in translating English metaphors to 8 target languages.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:00:23 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mao", "Rui", ""], ["Lin", "Chenghua", ""], ["Guerin", "Frank", ""]]}, {"id": "2104.03416", "submitter": "Edwin Ng", "authors": "Edwin G. Ng, Chung-Cheng Chiu, Yu Zhang, William Chan", "title": "Pushing the Limits of Non-Autoregressive Speech Recognition", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:17:20 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 15:09:29 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 19:17:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ng", "Edwin G.", ""], ["Chiu", "Chung-Cheng", ""], ["Zhang", "Yu", ""], ["Chan", "William", ""]]}, {"id": "2104.03465", "submitter": "Darsh Shah", "authors": "Darsh J Shah, Lili Yu, Tao Lei and Regina Barzilay", "title": "Nutribullets Hybrid: Multi-document Health Summarization", "comments": "NAACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for generating comparative summaries that highlights\nsimilarities and contradictions in input documents. The key challenge in\ncreating such summaries is the lack of large parallel training data required\nfor training typical summarization systems. To this end, we introduce a hybrid\ngeneration approach inspired by traditional concept-to-text systems. To enable\naccurate comparison between different sources, the model first learns to\nextract pertinent relations from input documents. The content planning\ncomponent uses deterministic operators to aggregate these relations after\nidentifying a subset for inclusion into a summary. The surface realization\ncomponent lexicalizes this information using a text-infilling language model.\nBy separately modeling content selection and realization, we can effectively\ntrain them with limited annotations. We implemented and tested the model in the\ndomain of nutrition and health -- rife with inconsistencies. Compared to\nconventional methods, our framework leads to more faithful, relevant and\naggregation-sensitive summarization -- while being equally fluent.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 01:44:29 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shah", "Darsh J", ""], ["Yu", "Lili", ""], ["Lei", "Tao", ""], ["Barzilay", "Regina", ""]]}, {"id": "2104.03474", "submitter": "Simeng Sun", "authors": "Simeng Sun, Mohit Iyyer", "title": "Revisiting Simple Neural Probabilistic Language Models", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in language modeling has been driven not only by advances in\nneural architectures, but also through hardware and optimization improvements.\nIn this paper, we revisit the neural probabilistic language model (NPLM)\nof~\\citet{Bengio2003ANP}, which simply concatenates word embeddings within a\nfixed window and passes the result through a feed-forward network to predict\nthe next word. When scaled up to modern hardware, this model (despite its many\nlimitations) performs much better than expected on word-level language model\nbenchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a\nbaseline Transformer with short input contexts but struggles to handle\nlong-term dependencies. Inspired by this result, we modify the Transformer by\nreplacing its first self-attention layer with the NPLM's local concatenation\nlayer, which results in small but consistent perplexity decreases across three\nword-level language modeling datasets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 02:18:47 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Sun", "Simeng", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2104.03506", "submitter": "Yakoob Khan", "authors": "Yakoob Khan, Weicheng Ma, Soroush Vosoughi", "title": "Lone Pine at SemEval-2021 Task 5: Fine-Grained Detection of Hate Speech\n  Using BERToxic", "comments": "7 pages, 3 figures. Accepted at SemEval-2021 Workshop, ACL-IJCNLP\n  2021", "journal-ref": null, "doi": "10.18653/v1/2021.semeval-1.132", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our approach to the Toxic Spans Detection problem\n(SemEval-2021 Task 5). We propose BERToxic, a system that fine-tunes a\npre-trained BERT model to locate toxic text spans in a given text and utilizes\nadditional post-processing steps to refine the boundaries. The post-processing\nsteps involve (1) labeling character offsets between consecutive toxic tokens\nas toxic and (2) assigning a toxic label to words that have at least one token\nlabeled as toxic. Through experiments, we show that these two post-processing\nsteps improve the performance of our model by 4.16% on the test set. We also\nstudied the effects of data augmentation and ensemble modeling strategies on\nour system. Our system significantly outperformed the provided baseline and\nachieved an F1-score of 0.683, placing Lone Pine in the 17th place out of 91\nteams in the competition. Our code is made available at\nhttps://github.com/Yakoob-Khan/Toxic-Spans-Detection\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:46:14 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Khan", "Yakoob", ""], ["Ma", "Weicheng", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2104.03514", "submitter": "Steven Cao", "authors": "Steven Cao, Victor Sanh, Alexander M. Rush", "title": "Low-Complexity Probing via Finding Subnetworks", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant approach in probing neural networks for linguistic properties is\nto train a new shallow multi-layer perceptron (MLP) on top of the model's\ninternal representations. This approach can detect properties encoded in the\nmodel, but at the cost of adding new parameters that may learn the task\ndirectly. We instead propose a subtractive pruning-based probe, where we find\nan existing subnetwork that performs the linguistic task of interest. Compared\nto an MLP, the subnetwork probe achieves both higher accuracy on pre-trained\nmodels and lower accuracy on random models, so it is both better at finding\nproperties of interest and worse at learning on its own. Next, by varying the\ncomplexity of each probe, we show that subnetwork probing Pareto-dominates MLP\nprobing in that it achieves higher accuracy given any budget of probe\ncomplexity. Finally, we analyze the resulting subnetworks across various tasks\nto locate where each task is encoded, and we find that lower-level tasks are\ncaptured in lower layers, reproducing similar findings in past work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:11:21 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Cao", "Steven", ""], ["Sanh", "Victor", ""], ["Rush", "Alexander M.", ""]]}, {"id": "2104.03521", "submitter": "Xiang Li", "authors": "Xiang Li, Changhe Song, Jingbei Li, Zhiyong Wu, Jia Jia, Helen Meng", "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis", "comments": "5 pages, 4 figures, submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a multi-scale speech style modeling method for\nend-to-end expressive speech synthesis. The proposed method employs a\nmulti-scale reference encoder to extract both the global-scale utterance-level\nand the local-scale quasi-phoneme-level style features of the target speech,\nwhich are then fed into the speech synthesis model as an extension to the input\nphoneme sequence. During training time, the multi-scale style model could be\njointly trained with the speech synthesis model in an end-to-end fashion. By\napplying the proposed method to style transfer task, experimental results\nindicate that the controllability of the multi-scale speech style model and the\nexpressiveness of the synthesized speech are greatly improved. Moreover, by\nassigning different reference speeches to extraction of style on each scale,\nthe flexibility of the proposed method is further revealed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:50:09 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Xiang", ""], ["Song", "Changhe", ""], ["Li", "Jingbei", ""], ["Wu", "Zhiyong", ""], ["Jia", "Jia", ""], ["Meng", "Helen", ""]]}, {"id": "2104.03523", "submitter": "Shohei Higashiyama", "authors": "Shohei Higashiyama, Masao Utiyama, Taro Watanabe, Eiichiro Sumita", "title": "User-Generated Text Corpus for Evaluating Japanese Morphological\n  Analysis and Lexical Normalization", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological analysis (MA) and lexical normalization (LN) are both important\ntasks for Japanese user-generated text (UGT). To evaluate and compare different\nMA/LN systems, we have constructed a publicly available Japanese UGT corpus.\nOur corpus comprises 929 sentences annotated with morphological and\nnormalization information, along with category information we classified for\nfrequent UGT-specific phenomena. Experiments on the corpus demonstrated the low\nperformance of existing MA/LN methods for non-general words and non-standard\nforms, indicating that the corpus would be a challenging benchmark for further\nresearch on UGT.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:53:46 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Higashiyama", "Shohei", ""], ["Utiyama", "Masao", ""], ["Watanabe", "Taro", ""], ["Sumita", "Eiichiro", ""]]}, {"id": "2104.03543", "submitter": "Andargachew Mekonnen Gezmu", "authors": "Andargachew Mekonnen Gezmu, Andreas N\\\"urnberger and Tesfaye Bayu Bati", "title": "Extended Parallel Corpus for Amharic-English Machine Translation", "comments": "Accepted to 2nd AfricanNLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be useful for machine\ntranslation of an under-resourced language, Amharic. The corpus is larger than\npreviously compiled corpora; it is released for research purposes. We trained\nneural machine translation and phrase-based statistical machine translation\nmodels using the corpus. In the automatic evaluation, neural machine\ntranslation models outperform phrase-based statistical machine translation\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:51:08 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 07:52:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gezmu", "Andargachew Mekonnen", ""], ["N\u00fcrnberger", "Andreas", ""], ["Bati", "Tesfaye Bayu", ""]]}, {"id": "2104.03575", "submitter": "Ruiqing Zhang", "authors": "Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang, Zhongjun He, Hua Wu, Zhi\n  Li, Haifeng Wang, Ying Chen, Qinfei Li", "title": "BSTC: A Large-Scale Chinese-English Speech Translation Dataset", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents BSTC (Baidu Speech Translation Corpus), a large-scale\nChinese-English speech translation dataset. This dataset is constructed based\non a collection of licensed videos of talks or lectures, including about 68\nhours of Mandarin data, their manual transcripts and translations into English,\nas well as automated transcripts by an automatic speech recognition (ASR)\nmodel. We have further asked three experienced interpreters to simultaneously\ninterpret the testing talks in a mock conference setting. This corpus is\nexpected to promote the research of automatic simultaneous translation as well\nas the development of practical systems. We have organized simultaneous\ntranslation tasks and used this corpus to evaluate automatic simultaneous\ntranslation systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:38:51 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 05:47:33 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 03:32:51 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 02:45:39 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Ruiqing", ""], ["Wang", "Xiyang", ""], ["Zhang", "Chuanqiang", ""], ["He", "Zhongjun", ""], ["Wu", "Hua", ""], ["Li", "Zhi", ""], ["Wang", "Haifeng", ""], ["Chen", "Ying", ""], ["Li", "Qinfei", ""]]}, {"id": "2104.03587", "submitter": "Zhichao Wang", "authors": "Zhichao Wang, Wenwen Yang, Pan Zhou, Wei Chen", "title": "WNARS: WFST based Non-autoregressive Streaming End-to-End Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, attention-based encoder-decoder (AED) end-to-end (E2E) models have\ndrawn more and more attention in the field of automatic speech recognition\n(ASR). AED models, however, still have drawbacks when deploying in commercial\napplications. Autoregressive beam search decoding makes it inefficient for\nhigh-concurrency applications. It is also inconvenient to integrate external\nword-level language models. The most important thing is that AED models are\ndifficult for streaming recognition due to global attention mechanism. In this\npaper, we propose a novel framework, namely WNARS, using hybrid CTC-attention\nAED models and weighted finite-state transducers (WFST) to solve these problems\ntogether. We switch from autoregressive beam search to CTC branch decoding,\nwhich performs first-pass decoding with WFST in chunk-wise streaming way. The\ndecoder branch then performs second-pass rescoring on the generated hypotheses\nnon-autoregressively. On the AISHELL-1 task, our WNARS achieves a character\nerror rate of 5.22% with 640ms latency, to the best of our knowledge, which is\nthe state-of-the-art performance for online ASR. Further experiments on our\n10,000-hour Mandarin task show the proposed method achieves more than 20%\nimprovements with 50% latency compared to a strong TDNN-BLSTM lattice-free MMI\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:56:03 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 03:57:44 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Zhichao", ""], ["Yang", "Wenwen", ""], ["Zhou", "Pan", ""], ["Chen", "Wei", ""]]}, {"id": "2104.03617", "submitter": "Jiangyan Yi", "authors": "Jiangyan Yi, Ye Bai, Jianhua Tao, Zhengkun Tian, Chenglong Wang, Tao\n  Wang, Ruibo Fu", "title": "Half-Truth: A Partially Fake Audio Detection Dataset", "comments": "submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse promising datasets have been designed to hold back the development of\nfake audio detection, such as ASVspoof databases. However, previous datasets\nignore an attacking situation, in which the hacker hides some small fake clips\nin real speech audio. This poses a serious threat since that it is difficult to\ndistinguish the small fake clip from the whole speech utterance. Therefore,\nthis paper develops such a dataset for half-truth audio detection (HAD).\nPartially fake audio in the HAD dataset involves only changing a few words in\nan utterance.The audio of the words is generated with the very latest\nstate-of-the-art speech synthesis technology. We can not only detect fake\nuttrances but also localize manipulated regions in a speech using this dataset.\nSome benchmark results are presented on this dataset. The results show that\npartially fake audio presents much more challenging than fully fake audio for\nfake audio detection.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:57:13 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yi", "Jiangyan", ""], ["Bai", "Ye", ""], ["Tao", "Jianhua", ""], ["Tian", "Zhengkun", ""], ["Wang", "Chenglong", ""], ["Wang", "Tao", ""], ["Fu", "Ruibo", ""]]}, {"id": "2104.03630", "submitter": "Maarten De Raedt", "authors": "Maarten De Raedt, Fr\\'ederic Godin, Pieter Buteneers, Chris Develder\n  and Thomas Demeester", "title": "A Simple Geometric Method for Cross-Lingual Linguistic Transformations\n  with Pre-trained Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful sentence encoders trained for multiple languages are on the rise.\nThese systems are capable of embedding a wide range of linguistic properties\ninto vector representations. While explicit probing tasks can be used to verify\nthe presence of specific linguistic properties, it is unclear whether the\nvector representations can be manipulated to indirectly steer such properties.\nWe investigate the use of a geometric mapping in embedding space to transform\nlinguistic properties, without any tuning of the pre-trained sentence encoder\nor decoder. We validate our approach on three linguistic properties using a\npre-trained multilingual autoencoder and analyze the results in both\nmonolingual and cross-lingual settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:33:50 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["De Raedt", "Maarten", ""], ["Godin", "Fr\u00e9deric", ""], ["Buteneers", "Pieter", ""], ["Develder", "Chris", ""], ["Demeester", "Thomas", ""]]}, {"id": "2104.03643", "submitter": "Juan Pablo Zuluaga-Gomez", "authors": "Juan Zuluaga-Gomez and Iuliia Nigmatulina and Amrutha Prasad and Petr\n  Motlicek and Karel Vesel\\'y and Martin Kocour and Igor Sz\\\"oke", "title": "Contextual Semi-Supervised Learning: An Approach To Leverage\n  Air-Surveillance and Untranscribed ATC Data in ASR Systems", "comments": "Submitted to: Interspeech conference 2021 (Brno, Czechia, August 30 -\n  September 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air traffic management and specifically air-traffic control (ATC) rely mostly\non voice communications between Air Traffic Controllers (ATCos) and pilots. In\nmost cases, these voice communications follow a well-defined grammar that could\nbe leveraged in Automatic Speech Recognition (ASR) technologies. The callsign\nused to address an airplane is an essential part of all ATCo-pilot\ncommunications. We propose a two-steps approach to add contextual knowledge\nduring semi-supervised training to reduce the ASR system error rates at\nrecognizing the part of the utterance that contains the callsign. Initially, we\nrepresent in a WFST the contextual knowledge (i.e. air-surveillance data) of an\nATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the\ncontextual knowledge is added by second-pass decoding (i.e. lattice\nre-scoring). Results show that `unseen domains' (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual SSL\nwhen compared to standalone SSL. For this task, we introduce the Callsign Word\nError Rate (CA-WER) as an evaluation metric, which only assesses ASR\nperformance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER\nrelative improvement applying SSL with an additional 17.5% CA-WER improvement\nby adding contextual knowledge during SSL on a challenging ATC-based test set\ngathered from LiveATC.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:53:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zuluaga-Gomez", "Juan", ""], ["Nigmatulina", "Iuliia", ""], ["Prasad", "Amrutha", ""], ["Motlicek", "Petr", ""], ["Vesel\u00fd", "Karel", ""], ["Kocour", "Martin", ""], ["Sz\u00f6ke", "Igor", ""]]}, {"id": "2104.03678", "submitter": "Kouji Matsui", "authors": "Kouji Matsui", "title": "A Proposal for an Interactive Shell Based on a Typed Lambda Calculus", "comments": "26 pages, 6 figures, It has been presented at Information Processing\n  Society of Japan Programming Study Group-132nd Programming Study Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents Favalon, a functional programming language built on the\npremise of a lambda calculus for use as an interactive shell replacement.\nFavalon seamlessly integrates with typed versions of existing libraries and\ncommands using type inference, flexible runtime type metadata, and the same\ntechniques employed by shells to link commands together. Much of Favalon's\nsyntax is customizable via user-defined functions, allowing it to be extended\nby anyone who is familiar with a command-line shell. Furthermore, Favalon's\ntype inference engine can be separated from its runtime library and easily\nrepurposed for other applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:46:28 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Matsui", "Kouji", ""]]}, {"id": "2104.03682", "submitter": "Xiangchen Song", "authors": "Xiangchen Song, Jiaming Shen, Jieyu Zhang, and Jiawei Han", "title": "Who Should Go First? A Self-Supervised Concept Sorting Model for\n  Improving Taxonomy Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomies have been widely used in various machine learning and text mining\nsystems to organize knowledge and facilitate downstream tasks. One critical\nchallenge is that, as data and business scope grow in real applications,\nexisting taxonomies need to be expanded to incorporate new concepts. Previous\nworks on taxonomy expansion process the new concepts independently and\nsimultaneously, ignoring the potential relationships among them and the\nappropriate order of inserting operations. However, in reality, the new\nconcepts tend to be mutually correlated and form local hypernym-hyponym\nstructures. In such a scenario, ignoring the dependencies of new concepts and\nthe order of insertion may trigger error propagation. For example, existing\ntaxonomy expansion systems may insert hyponyms to existing taxonomies before\ntheir hypernym, leading to sub-optimal expanded taxonomies. To complement\nexisting taxonomy expansion systems, we propose TaxoOrder, a novel\nself-supervised framework that simultaneously discovers the local\nhypernym-hyponym structure among new concepts and decides the order of\ninsertion. TaxoOrder can be directly plugged into any taxonomy expansion system\nand improve the quality of expanded taxonomies. Experiments on the real-world\ndataset validate the effectiveness of TaxoOrder to enhance taxonomy expansion\nsystems, leading to better-resulting taxonomies with comparison to baselines\nunder various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 11:00:43 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 08:14:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Song", "Xiangchen", ""], ["Shen", "Jiaming", ""], ["Zhang", "Jieyu", ""], ["Han", "Jiawei", ""]]}, {"id": "2104.03762", "submitter": "Arka Sadhu", "authors": "Arka Sadhu, Kan Chen, Ram Nevatia", "title": "Video Question Answering with Phrases via Semantic Roles", "comments": "NAACL21 Camera Ready including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering (VidQA) evaluation metrics have been limited to a\nsingle-word answer or selecting a phrase from a fixed set of phrases. These\nmetrics limit the VidQA models' application scenario. In this work, we leverage\nsemantic roles derived from video descriptions to mask out certain phrases, to\nintroduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable\nevaluation of answer phrases, we compute the relative improvement of the\npredicted answer compared to an empty string. To reduce the influence of\nlanguage bias in VidQA datasets, we retrieve a video having a different answer\nfor the same question. To facilitate research, we construct ActivityNet-SRL-QA\nand Charades-SRL-QA and benchmark them by extending three vision-language\nmodels. We further perform extensive analysis and ablative studies to guide\nfuture work.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:27:43 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Sadhu", "Arka", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "2104.03764", "submitter": "Christopher Clack", "authors": "Christopher D. Clack", "title": "Languages for Smart and Computable Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Smart Contracts use computer technology to automate the performance of\naspects of commercial agreements. Yet how can there be confidence that the\ncomputer code is faithful to the intentions of the parties? To understand the\ndepth and subtlety of this question requires an exploration of natural and\ncomputer languages, of the semantics of expressions in those languages, and of\nthe gap that exists between the disciplines of law and computer science. Here\nwe provide a perspective on some of the key issues, explore some current\nresearch directions, and explain the importance of language design in the\ndevelopment of reliable Smart Contracts, including the specific methodology of\nComputable Contracts.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:32:17 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Clack", "Christopher D.", ""]]}, {"id": "2104.03767", "submitter": "Huiling You", "authors": "Huiling You, Xingran Zhu and Sara Stymne", "title": "Uppsala NLP at SemEval-2021 Task 2: Multilingual Language Models for\n  Fine-tuning and Feature Extraction in Word-in-Context Disambiguation", "comments": "To appear at SemEval-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the Uppsala NLP submission to SemEval-2021 Task 2 on multilingual\nand cross-lingual word-in-context disambiguation. We explore the usefulness of\nthree pre-trained multilingual language models, XLM-RoBERTa (XLMR),\nMultilingual BERT (mBERT) and multilingual distilled BERT (mDistilBERT). We\ncompare these three models in two setups, fine-tuning and as feature\nextractors. In the second case we also experiment with using dependency-based\ninformation. We find that fine-tuning is better than feature extraction. XLMR\nperforms better than mBERT in the cross-lingual setting both with fine-tuning\nand feature extraction, whereas these two models give a similar performance in\nthe multilingual setting. mDistilBERT performs poorly with fine-tuning but\ngives similar results to the other models when used as a feature extractor. We\nsubmitted our two best systems, fine-tuned with XLMR and mBERT.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:40:41 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 04:21:24 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["You", "Huiling", ""], ["Zhu", "Xingran", ""], ["Stymne", "Sara", ""]]}, {"id": "2104.03776", "submitter": "Alan Medlar", "authors": "Yang Liu, Alan Medlar and Dorota Glowacka", "title": "Statistically significant detection of semantic shifts using contextual\n  word embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting lexical semantic shifts in smaller data sets, e.g. in historical\nlinguistics and digital humanities, is challenging due to a lack of statistical\npower. This issue is exacerbated by non-contextual word embeddings that produce\none embedding per token and therefore mask the variability present in the data.\nIn this article, we propose an approach to estimate semantic shifts by\ncombining contextual word embeddings with permutation-based statistical tests.\nMultiple comparisons are addressed using a false discovery rate procedure. We\ndemonstrate the performance of this approach in simulation, achieving\nconsistently high precision by suppressing false positives. We additionally\nanalyzed real-world data from SemEval-2020 Task 1 and the Liverpool FC\nsubreddit corpus. We show that by taking sample variation into account, we can\nimprove the robustness of individual semantic shift estimates without degrading\noverall performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:58:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Liu", "Yang", ""], ["Medlar", "Alan", ""], ["Glowacka", "Dorota", ""]]}, {"id": "2104.03780", "submitter": "Michael J. Klaiber", "authors": "Michael J. Klaiber, Axel J. Acosta, Ingo Feldner, Falk Rehm", "title": "Enabling Cross-Domain Communication: How to Bridge the Gap between AI\n  and HW Engineers", "comments": "LATTE 2021 Workshop on Languages, Tools, and Techniques for\n  Accelerator Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key issue in system design is the lack of communication between hardware,\nsoftware and domain expert. Recent research work shows progress in automatic\nHW/SW co-design flows of neural accelerators that seems to make this kind of\ncommunication obsolete. Most real-world systems, however, are a composition of\nmultiple processing units, communication networks and memories. A HW/SW\nco-design process of (reconfigurable) neural accelerators, therefore, is an\nimportant sub-problem towards a common co-design methodology. The ultimate\nchallenge is to define the constraints for the design space exploration on\nsystem level - a task which requires deep knowledge and understanding of\nhardware architectures, mapping of workloads onto hardware and the application\ndomain, e.g. artificial intelligence.\n  For most projects, these skills are distributed among several people or even\ndifferent teams which is one of the major reasons why there is no established\nend-to-end development methodology for digital systems. This position paper\ndiscusses possibilities how to establish such a methodology for systems that\ninclude (reconfigurable) dedicated accelerators and outlines the central role\nthat languages and tools play in the process.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:05:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Klaiber", "Michael J.", ""], ["Acosta", "Axel J.", ""], ["Feldner", "Ingo", ""], ["Rehm", "Falk", ""]]}, {"id": "2104.03815", "submitter": "Fengpeng Yue", "authors": "Fengpeng Yue, Yan Deng, Lei He, Tom Ko", "title": "Exploring Machine Speech Chain for Domain Adaptation and Few-Shot\n  Speaker Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Speech Chain, which integrates both end-to-end (E2E) automatic speech\nrecognition (ASR) and text-to-speech (TTS) into one circle for joint training,\nhas been proven to be effective in data augmentation by leveraging large\namounts of unpaired data. In this paper, we explore the TTS->ASR pipeline in\nspeech chain to do domain adaptation for both neural TTS and E2E ASR models,\nwith only text data from target domain. We conduct experiments by adapting from\naudiobook domain (LibriSpeech) to presentation domain (TED-LIUM), there is a\nrelative word error rate (WER) reduction of 10% for the E2E ASR model on the\nTED-LIUM test set, and a relative WER reduction of 51.5% in synthetic speech\ngenerated by neural TTS in the presentation domain. Further, we apply few-shot\nspeaker adaptation for the E2E ASR by using a few utterances from target\nspeakers in an unsupervised way, results in additional gains.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:52:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yue", "Fengpeng", ""], ["Deng", "Yan", ""], ["He", "Lei", ""], ["Ko", "Tom", ""]]}, {"id": "2104.03842", "submitter": "Samuel Thomas", "authors": "Samuel Thomas, Hong-Kwang J. Kuo, George Saon, Zolt\\'an T\\\"uske, Brian\n  Kingsbury, Gakuto Kurata, Zvi Kons, Ron Hoory", "title": "RNN Transducer Models For Spoken Language Understanding", "comments": "To appear in the proceedings of ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study on building and adapting RNN transducer\n(RNN-T) models for spoken language understanding(SLU). These end-to-end (E2E)\nmodels are constructed in three practical settings: a case where verbatim\ntranscripts are available, a constrained case where the only available\nannotations are SLU labels and their values, and a more restrictive case where\ntranscripts are available but not corresponding audio. We show how RNN-T SLU\nmodels can be developed starting from pre-trained automatic speech recognition\n(ASR) systems, followed by an SLU adaptation step. In settings where real audio\ndata is not available, artificially synthesized speech is used to successfully\nadapt various SLU models. When evaluated on two SLU data sets, the ATIS corpus\nand a customer call center data set, the proposed models closely track the\nperformance of other E2E models and achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:35:22 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Thomas", "Samuel", ""], ["Kuo", "Hong-Kwang J.", ""], ["Saon", "George", ""], ["T\u00fcske", "Zolt\u00e1n", ""], ["Kingsbury", "Brian", ""], ["Kurata", "Gakuto", ""], ["Kons", "Zvi", ""], ["Hoory", "Ron", ""]]}, {"id": "2104.03848", "submitter": "Orion Weller", "authors": "Wilson Fearn, Orion Weller, Kevin Seppi", "title": "Exploring the Relationship Between Algorithm Performance, Vocabulary,\n  and Run-Time in Text Classification", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text classification is a significant branch of natural language processing,\nand has many applications including document classification and sentiment\nanalysis. Unsurprisingly, those who do text classification are concerned with\nthe run-time of their algorithms, many of which depend on the size of the\ncorpus' vocabulary due to their bag-of-words representation. Although many\nstudies have examined the effect of preprocessing techniques on vocabulary size\nand accuracy, none have examined how these methods affect a model's run-time.\nTo fill this gap, we provide a comprehensive study that examines how\npreprocessing techniques affect the vocabulary size, model performance, and\nmodel run-time, evaluating ten techniques over four models and two datasets. We\nshow that some individual methods can reduce run-time with no loss of accuracy,\nwhile some combinations of methods can trade 2-5% of the accuracy for up to a\n65% reduction of run-time. Furthermore, some combinations of preprocessing\ntechniques can even provide a 15% reduction in run-time while simultaneously\nimproving model accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:49:59 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Fearn", "Wilson", ""], ["Weller", "Orion", ""], ["Seppi", "Kevin", ""]]}, {"id": "2104.03869", "submitter": "Yao Fu", "authors": "Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen,\n  Liping Jing", "title": "Probing BERT in Hyperbolic Spaces", "comments": "ICLR 2021 Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, a variety of probing tasks are proposed to discover linguistic\nproperties learned in contextualized word embeddings. Many of these works\nimplicitly assume these embeddings lay in certain metric spaces, typically the\nEuclidean space. This work considers a family of geometrically special spaces,\nthe hyperbolic spaces, that exhibit better inductive biases for hierarchical\nstructures and may better reveal linguistic hierarchies encoded in\ncontextualized representations. We introduce a Poincare probe, a structural\nprobe projecting these embeddings into a Poincare subspace with explicitly\ndefined hierarchies. We focus on two probing objectives: (a) dependency trees\nwhere the hierarchy is defined as head-dependent structures; (b) lexical\nsentiments where the hierarchy is defined as the polarity of words (positivity\nand negativity). We argue that a key desideratum of a probe is its sensitivity\nto the existence of linguistic structures. We apply our probes on BERT, a\ntypical contextualized embedding model. In a syntactic subspace, our probe\nbetter recovers tree structures than Euclidean probes, revealing the\npossibility that the geometry of BERT syntax may not necessarily be Euclidean.\nIn a sentiment subspace, we reveal two possible meta-embeddings for positive\nand negative sentiments and show how lexically-controlled contextualization\nwould change the geometric localization of embeddings. We demonstrate the\nfindings with our Poincare probe via extensive experiments and visualization.\nOur results can be reproduced at https://github.com/FranxYao/PoincareProbe.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:24:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Chen", "Boli", ""], ["Fu", "Yao", ""], ["Xu", "Guangwei", ""], ["Xie", "Pengjun", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Jing", "Liping", ""]]}, {"id": "2104.03879", "submitter": "Dat Quoc Nguyen", "authors": "Thinh Hung Truong, Mai Hoang Dao, Dat Quoc Nguyen", "title": "COVID-19 Named Entity Recognition for Vietnamese", "comments": "To appear in Proceedings of NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current COVID-19 pandemic has lead to the creation of many corpora that\nfacilitate NLP research and downstream applications to help fight the pandemic.\nHowever, most of these corpora are exclusively for English. As the pandemic is\na global problem, it is worth creating COVID-19 related datasets for languages\nother than English. In this paper, we present the first manually-annotated\nCOVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is\nannotated for the named entity recognition (NER) task with newly-defined entity\ntypes that can be used in other future epidemics. Our dataset also contains the\nlargest number of entities compared to existing Vietnamese NER datasets. We\nempirically conduct experiments using strong baselines on our dataset, and find\nthat: automatic Vietnamese word segmentation helps improve the NER results and\nthe highest performances are obtained by fine-tuning pre-trained language\nmodels where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen,\n2020) produces higher results than the multilingual model XLM-R (Conneau et\nal., 2020). We publicly release our dataset at:\nhttps://github.com/VinAIResearch/PhoNER_COVID19\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:35:34 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Truong", "Thinh Hung", ""], ["Dao", "Mai Hoang", ""], ["Nguyen", "Dat Quoc", ""]]}, {"id": "2104.03928", "submitter": "Vinodkumar Prabhakaran", "authors": "Vinodkumar Prabhakaran, Marek Rei, Ekaterina Shutova", "title": "How Metaphors Impact Political Discourse: A Large-Scale Topic-Agnostic\n  Study Using Neural Metaphor Detection", "comments": "Published at ICWSM 2021. Please cite that version for academic\n  publications", "journal-ref": "The International AAAI Conference on Web and Social Media (ICWSM)\n  2021", "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metaphors are widely used in political rhetoric as an effective framing\ndevice. While the efficacy of specific metaphors such as the war metaphor in\npolitical discourse has been documented before, those studies often rely on\nsmall number of hand-coded instances of metaphor use. Larger-scale\ntopic-agnostic studies are required to establish the general persuasiveness of\nmetaphors as a device, and to shed light on the broader patterns that guide\ntheir persuasiveness. In this paper, we present a large-scale data-driven study\nof metaphors used in political discourse. We conduct this study on a publicly\navailable dataset of over 85K posts made by 412 US politicians in their\nFacebook public pages, up until Feb 2017. Our contributions are threefold: we\nshow evidence that metaphor use correlates with ideological leanings in complex\nways that depend on concurrent political events such as winning or losing\nelections; we show that posts with metaphors elicit more engagement from their\naudience overall even after controlling for various socio-political factors\nsuch as gender and political party affiliation; and finally, we demonstrate\nthat metaphoricity is indeed the reason for increased engagement of posts,\nthrough a fine-grained linguistic analysis of metaphorical vs. literal usages\nof 513 words across 70K posts.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:16:31 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Prabhakaran", "Vinodkumar", ""], ["Rei", "Marek", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2104.03934", "submitter": "Thanh Dung Le", "authors": "Thanh-Dung Le, Rita Noumeir, Jerome Rambaud, Guillaume Sans, and\n  Philippe Jouvet", "title": "Machine Learning Based on Natural Language Processing to Detect Cardiac\n  Failure in Clinical Narratives", "comments": "Submitted to 2021 34th IEEE International Symposium on Computer-Based\n  Medical Systems (CBMS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of the study presented herein is to develop a machine learning\nalgorithm based on natural language processing that automatically detects\nwhether a patient has a cardiac failure or a healthy condition by using\nphysician notes in Research Data Warehouse at CHU Sainte Justine Hospital.\nFirst, a word representation learning technique was employed by using\nbag-of-word (BoW), term frequency inverse document frequency (TFIDF), and\nneural word embeddings (word2vec). Each representation technique aims to retain\nthe words semantic and syntactic analysis in critical care data. It helps to\nenrich the mutual information for the word representation and leads to an\nadvantage for further appropriate analysis steps. Second, a machine learning\nclassifier was used to detect the patients condition for either cardiac failure\nor stable patient through the created word representation vector space from the\nprevious step. This machine learning approach is based on a supervised binary\nclassification algorithm, including logistic regression (LR), Gaussian\nNaive-Bayes (GaussianNB), and multilayer perceptron neural network (MLPNN).\nTechnically, it mainly optimizes the empirical loss during training the\nclassifiers. As a result, an automatic learning algorithm would be accomplished\nto draw a high classification performance, including accuracy (acc), precision\n(pre), recall (rec), and F1 score (f1). The results show that the combination\nof TFIDF and MLPNN always outperformed other combinations with all overall\nperformance. In the case without any feature selection, the proposed framework\nyielded an overall classification performance with acc, pre, rec, and f1 of 84%\nand 82%, 85%, and 83%, respectively. Significantly, if the feature selection\nwas well applied, the overall performance would finally improve up to 4% for\neach evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:28:43 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Le", "Thanh-Dung", ""], ["Noumeir", "Rita", ""], ["Rambaud", "Jerome", ""], ["Sans", "Guillaume", ""], ["Jouvet", "Philippe", ""]]}, {"id": "2104.03945", "submitter": "Annette Rios", "authors": "Annette Rios, Chantal Amrhein, No\\\"emi Aepli, Rico Sennrich", "title": "On Biasing Transformer Attention Towards Monotonicity", "comments": "To be published in: Proceedings of the 2021 Conference of the North\n  American Chapter of the Association for Computational Linguistics: Human\n  Language Technologies (NAACL-HLT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many sequence-to-sequence tasks in natural language processing are roughly\nmonotonic in the alignment between source and target sequence, and previous\nwork has facilitated or enforced learning of monotonic attention behavior via\nspecialized attention functions or pretraining. In this work, we introduce a\nmonotonicity loss function that is compatible with standard attention\nmechanisms and test it on several sequence-to-sequence tasks:\ngrapheme-to-phoneme conversion, morphological inflection, transliteration, and\ndialect normalization. Experiments show that we can achieve largely monotonic\nbehavior. Performance is mixed, with larger gains on top of RNN baselines.\nGeneral monotonicity does not benefit transformer multihead attention, however,\nwe see isolated improvements when only a subset of heads is biased towards\nmonotonic behavior.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:42:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Rios", "Annette", ""], ["Amrhein", "Chantal", ""], ["Aepli", "No\u00ebmi", ""], ["Sennrich", "Rico", ""]]}, {"id": "2104.03952", "submitter": "Niv Cohen", "authors": "Niv Cohen and Yedid Hoshen", "title": "The Single-Noun Prior for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised clustering methods have achieved increasing accuracy in\nrecent years but do not yet perform as well as supervised classification\nmethods. This contrasts with the situation for feature learning, where\nself-supervised features have recently surpassed the performance of supervised\nfeatures on several important tasks. We hypothesize that the performance gap is\ndue to the difficulty of specifying, without supervision, which features\ncorrespond to class differences that are semantic to humans. To reduce the\nperformance gap, we introduce the \"single-noun\" prior - which states that\nsemantic clusters tend to correspond to concepts that humans label by a\nsingle-noun. By utilizing a pre-trained network that maps images and sentences\ninto a common space, we impose this prior obtaining a constrained optimization\ntask. We show that our formulation is a special case of the facility location\nproblem, and introduce a simple-yet-effective approach for solving this\noptimization task at scale. We test our approach on several commonly reported\nimage clustering datasets and obtain significant accuracy gains over the best\nexisting approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:54:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Cohen", "Niv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2104.03958", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Leshem Choshen, Eyal Shnarch, Francesca\n  Toni", "title": "GrASP: A Library for Extracting and Exploring Human-Interpretable\n  Textual Patterns", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exploration is an important step of every data science and machine\nlearning project, including those involving textual data. We provide a Python\nlibrary for GrASP, an existing algorithm for drawing patterns from textual\ndata. The library is equipped with a web-based interface empowering human users\nto conveniently explore the data and the extracted patterns. We also\ndemonstrate the use of the library in two settings (spam detection and argument\nmining) and discuss future deployments of the library, e.g., beyond textual\ndata exploration.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:58:03 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Choshen", "Leshem", ""], ["Shnarch", "Eyal", ""], ["Toni", "Francesca", ""]]}, {"id": "2104.03969", "submitter": "Thanh Dung Le", "authors": "Thanh-Dung Le, Rita Noumeir, Jerome Rambaud, Guillaume Sans, and\n  Philippe Jouvet", "title": "Detecting of a Patient's Condition From Clinical Narratives Using\n  Natural Language Representation", "comments": "Submitting to IEEE Transactions on Biomedical Engineering. arXiv\n  admin note: text overlap with arXiv:2104.03934", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nFurthermore, a generative autoencoder learning algorithm was proposed to\nleverage the sparsity reduction that achieved 91% accuracy, 91% recall, and 91%\nprecision. This study successfully applied learning representation and machine\nlearning algorithms to detect heart failure from clinical natural language in a\nsingle French institution. Further work is needed to use the same methodology\nin other institutions and other languages.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:16:04 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 15:51:43 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 07:10:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Le", "Thanh-Dung", ""], ["Noumeir", "Rita", ""], ["Rambaud", "Jerome", ""], ["Sans", "Guillaume", ""], ["Jouvet", "Philippe", ""]]}, {"id": "2104.04039", "submitter": "Zhiyu Lin", "authors": "Zhiyu Lin, Mark Riedl", "title": "Plug-and-Blend: A Framework for Controllable Story Generation with\n  Blended Control Codes", "comments": "8 pages, To appear at AIIDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:15:14 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 14:33:47 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lin", "Zhiyu", ""], ["Riedl", "Mark", ""]]}, {"id": "2104.04050", "submitter": "Gaurav Bharaj", "authors": "Mahsa Elyasi, Gaurav Bharaj", "title": "Flavored Tacotron: Conditional Learning for Prosodic-linguistic Features", "comments": "5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence text-to-speech synthesis (TTS), such as\nTacotron-2, transforms text into high-quality speech. However, generating\nspeech with natural prosody still remains a challenge. Yasuda et. al. show that\nunlike natural speech, Tacotron-2's encoder doesn't fully represent prosodic\nfeatures (e.g. syllable stress in English) from characters, and result in flat\nfundamental frequency variations.\n  In this work, we propose a novel carefully designed strategy for conditioning\nTacotron-2 on two fundamental prosodic features in English -- stress syllable\nand pitch accent, that help achieve more natural prosody. To this end, we use\nof a classifier to learn these features in an end-to-end fashion, and apply\nfeature conditioning at three parts of Tacotron-2's Text-To-Mel Spectrogram:\npre-encoder, post-encoder, and intra-decoder. Further, we show that jointly\nconditioned features at pre-encoder and intra-decoder stages result in\nprosodically natural synthesized speech (vs. Tacotron-2), and allows the model\nto produce speech with more accurate pitch accent and stress patterns.\n  Quantitative evaluations show that our formulation achieves higher\nfundamental frequency contour correlation, and lower Mel Cepstral Distortion\nmeasure between synthesized and natural speech. And subjective evaluation shows\nthat the proposed method's Mean Opinion Score of 4.14 fairs higher than\nbaseline Tacotron-2, 3.91, when compared against natural speech (LJSpeech\ncorpus), 4.28.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:50:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Elyasi", "Mahsa", ""], ["Bharaj", "Gaurav", ""]]}, {"id": "2104.04052", "submitter": "Amit Seker", "authors": "Amit Seker, Elron Bandel, Dan Bareket, Idan Brusilovsky, Refael Shaked\n  Greenfeld, Reut Tsarfaty", "title": "AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your\n  Hebrew NLP Application With", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large Pre-trained Language Models (PLMs) have become ubiquitous in the\ndevelopment of language understanding technology and lie at the heart of many\nartificial intelligence advances. While advances reported for English using\nPLMs are unprecedented, reported advances using PLMs in Hebrew are few and far\nbetween. The problem is twofold. First, Hebrew resources available for training\nNLP models are not at the same order of magnitude as their English\ncounterparts. Second, there are no accepted tasks and benchmarks to evaluate\nthe progress of Hebrew PLMs on. In this work we aim to remedy both aspects.\nFirst, we present AlephBERT, a large pre-trained language model for Modern\nHebrew, which is trained on larger vocabulary and a larger dataset than any\nHebrew PLM before. Second, using AlephBERT we present new state-of-the-art\nresults on multiple Hebrew tasks and benchmarks, including: Segmentation,\nPart-of-Speech Tagging, full Morphological Tagging, Named-Entity Recognition\nand Sentiment Analysis. We make our AlephBERT model publicly available,\nproviding a single point of entry for the development of Hebrew NLP\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:51:29 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Seker", "Amit", ""], ["Bandel", "Elron", ""], ["Bareket", "Dan", ""], ["Brusilovsky", "Idan", ""], ["Greenfeld", "Refael Shaked", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2104.04087", "submitter": "Traian Rebedea", "authors": "Nicolae-Teodor Pavel and Traian Rebedea", "title": "A Sketch-Based Neural Model for Generating Commit Messages from Diffs", "comments": "submitted at ASE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Commit messages have an important impact in software development, especially\nwhen working in large teams. Multiple developers who have a different style of\nwriting may often be involved in the same project. For this reason, it may be\ndifficult to maintain a strict pattern of writing informative commit messages,\nwith the most frequent issue being that these messages are not descriptive\nenough. In this paper we apply neural machine translation (NMT) techniques to\nconvert code diffs into commit messages and we present an improved sketch-based\nencoder for this task. We split the approach into three parts. Firstly, we\nfocus on finding a more suitable NMT baseline for this problem. Secondly, we\nshow that the performance of the NMT models can be improved by training on\nexamples containing a specific file type. Lastly, we introduce a novel\nsketch-based neural model inspired by recent approaches used for code\ngeneration and we show that the sketch-based encoder significantly outperforms\nexisting state of the art solutions. The results highlight that this\nimprovement is relevant especially for Java source code files, by examining two\ndifferent datasets introduced in recent years for this task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 21:21:28 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Pavel", "Nicolae-Teodor", ""], ["Rebedea", "Traian", ""]]}, {"id": "2104.04091", "submitter": "Gaurav Bharaj", "authors": "Eric Engelhart, Mahsa Elyasi, Gaurav Bharaj", "title": "Grapheme-to-Phoneme Transformer Model for Transfer Learning Dialects", "comments": "5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grapheme-to-Phoneme (G2P) models convert words to their phonetic\npronunciations. Classic G2P methods include rule-based systems and\npronunciation dictionaries, while modern G2P systems incorporate learning, such\nas, LSTM and Transformer-based attention models. Usually, dictionary-based\nmethods require significant manual effort to build, and have limited adaptivity\non unseen words. And transformer-based models require significant training\ndata, and do not generalize well, especially for dialects with limited data.\n  We propose a novel use of transformer-based attention model that can adapt to\nunseen dialects of English language, while using a small dictionary. We show\nthat our method has potential applications for accent transfer for\ntext-to-speech, and for building robust G2P models for dialects with limited\npronunciation dictionary size.\n  We experiment with two English dialects: Indian and British. A model trained\nfrom scratch using 1000 words from British English dictionary, with 14211 words\nheld out, leads to phoneme error rate (PER) of 26.877%, on a test set generated\nusing the full dictionary. The same model pretrained on CMUDict American\nEnglish dictionary, and fine-tuned on the same dataset leads to PER of 2.469%\non the test set.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 21:36:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Engelhart", "Eric", ""], ["Elyasi", "Mahsa", ""], ["Bharaj", "Gaurav", ""]]}, {"id": "2104.04108", "submitter": "Eleftheria Briakou", "authors": "Eleftheria Briakou, Di Lu, Ke Zhang, Joel Tetreault", "title": "XFORMAL: A Benchmark for Multilingual Formality Style Transfer", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We take the first step towards multilingual style transfer by creating and\nreleasing XFORMAL, a benchmark of multiple formal reformulations of informal\ntext in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest\nthat state-of-the-art style transfer approaches perform close to simple\nbaselines, indicating that style transfer is even more challenging when moving\nmultilingual.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 23:01:17 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Briakou", "Eleftheria", ""], ["Lu", "Di", ""], ["Zhang", "Ke", ""], ["Tetreault", "Joel", ""]]}, {"id": "2104.04125", "submitter": "Benjamin Ajibade", "authors": "Safiriyu Eludiora, Benjamin Ajibade", "title": "Design and Implementation of English To Yoruba Verb Phrase Machine\n  Translation System", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to develop an English to Yoruba machine translation system which can\ntranslate English verb phrase text to its Yoruba equivalent.Words from both\nlanguages Source Language and Target Language were collected for the verb\nphrase group in the home domain.The lexical translation is done by assigning\nvalues of the matching word in the dictionary.The syntax of the two languages\nwas realized using Context-Free Grammar,we validated the rewrite rules with\nfinite state automata.The human evaluation method was used and expert fluency\nscored.The evaluation shows the system performed better than that of sampled\nGoogle translation with over 70 percent of the response matching that of the\nsystem's output.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 00:50:57 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Eludiora", "Safiriyu", ""], ["Ajibade", "Benjamin", ""]]}, {"id": "2104.04128", "submitter": "Pouya Pezeshkpour", "authors": "Pouya Pezeshkpour, Sarthak Jain, Byron C. Wallace and Sameer Singh", "title": "An Empirical Comparison of Instance Attribution Methods for NLP", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widespread adoption of deep models has motivated a pressing need for\napproaches to interpret network outputs and to facilitate model debugging.\nInstance attribution methods constitute one means of accomplishing these goals\nby retrieving training instances that (may have) led to a particular\nprediction. Influence functions (IF; Koh and Liang 2017) provide machinery for\ndoing this by quantifying the effect that perturbing individual train instances\nwould have on a specific test prediction. However, even approximating the IF is\ncomputationally expensive, to the degree that may be prohibitive in many cases.\nMight simpler approaches (e.g., retrieving train examples most similar to a\ngiven test point) perform comparably? In this work, we evaluate the degree to\nwhich different potential instance attribution agree with respect to the\nimportance of training samples. We find that simple retrieval methods yield\ntraining instances that differ from those identified via gradient-based methods\n(such as IFs), but that nonetheless exhibit desirable characteristics similar\nto more complex attribution methods. Code for all methods and experiments in\nthis paper is available at:\nhttps://github.com/successar/instance_attributions_NLP.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:03:17 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Pezeshkpour", "Pouya", ""], ["Jain", "Sarthak", ""], ["Wallace", "Byron C.", ""], ["Singh", "Sameer", ""]]}, {"id": "2104.04140", "submitter": "Manas Gaur", "authors": "Manas Gaur, Vamsi Aribandi, Amanuel Alambo, Ugur Kursuncu,\n  Krishnaprasad Thirunarayan, Jonanthan Beich, Jyotishman Pathak, Amit Sheth", "title": "Characterization of Time-variant and Time-invariant Assessment of\n  Suicidality on Reddit using C-SSRS", "comments": "24 Pages, 8 Tables, 6 Figures; Accepted by PLoS One ; One of the two\n  mentioned Datasets in the manuscript has Closed Access. We will make it\n  public after PLoS One produces the manuscript", "journal-ref": null, "doi": "10.1371/journal.pone.0250448", "report-no": null, "categories": "cs.SI cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suicide is the 10th leading cause of death in the U.S (1999-2019). However,\npredicting when someone will attempt suicide has been nearly impossible. In the\nmodern world, many individuals suffering from mental illness seek emotional\nsupport and advice on well-known and easily-accessible social media platforms\nsuch as Reddit. While prior artificial intelligence research has demonstrated\nthe ability to extract valuable information from social media on suicidal\nthoughts and behaviors, these efforts have not considered both severity and\ntemporality of risk. The insights made possible by access to such data have\nenormous clinical potential - most dramatically envisioned as a trigger to\nemploy timely and targeted interventions (i.e., voluntary and involuntary\npsychiatric hospitalization) to save lives. In this work, we address this\nknowledge gap by developing deep learning algorithms to assess suicide risk in\nterms of severity and temporality from Reddit data based on the Columbia\nSuicide Severity Rating Scale (C-SSRS). In particular, we employ two deep\nlearning approaches: time-variant and time-invariant modeling, for user-level\nsuicide risk assessment, and evaluate their performance against a\nclinician-adjudicated gold standard Reddit corpus annotated based on the\nC-SSRS. Our results suggest that the time-variant approach outperforms the\ntime-invariant method in the assessment of suicide-related ideations and\nsupportive behaviors (AUC:0.78), while the time-invariant model performed\nbetter in predicting suicide-related behaviors and suicide attempt (AUC:0.64).\nThe proposed approach can be integrated with clinical diagnostic interviews for\nimproving suicide risk assessments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:39:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gaur", "Manas", ""], ["Aribandi", "Vamsi", ""], ["Alambo", "Amanuel", ""], ["Kursuncu", "Ugur", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Beich", "Jonanthan", ""], ["Pathak", "Jyotishman", ""], ["Sheth", "Amit", ""]]}, {"id": "2104.04167", "submitter": "Yuankai Qi", "authors": "Yuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Anton van den\n  Hengel, Qi Wu", "title": "Know What and Know Where: An Object-and-Room Informed Sequential BERT\n  for Indoor Vision-Language Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to navigate to a\nremote location on the basis of natural-language instructions and a set of\nphoto-realistic panoramas. Most existing methods take words in instructions and\ndiscrete views of each panorama as the minimal unit of encoding. However, this\nrequires a model to match different textual landmarks in instructions (e.g.,\nTV, table) against the same view feature. In this work, we propose an\nobject-informed sequential BERT to encode visual perceptions and linguistic\ninstructions at the same fine-grained level, namely objects and words, to\nfacilitate the matching between visual and textual entities and hence \"know\nwhat\". Our sequential BERT enables the visual-textual clues to be interpreted\nin light of the temporal context, which is crucial to multi-round VLN tasks.\nAdditionally, we enable the model to identify the relative direction (e.g.,\nleft/right/front/back) of each navigable location and the room type (e.g.,\nbedroom, kitchen) of its current and final navigation goal, namely \"know\nwhere\", as such information is widely mentioned in instructions implying the\ndesired next and final locations. Extensive experiments demonstrate the\neffectiveness compared against several state-of-the-art methods on three indoor\nVLN tasks: REVERIE, NDH, and R2R.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:44:39 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Qi", "Yuankai", ""], ["Pan", "Zizheng", ""], ["Hong", "Yicong", ""], ["Yang", "Ming-Hsuan", ""], ["Hengel", "Anton van den", ""], ["Wu", "Qi", ""]]}, {"id": "2104.04197", "submitter": "Long Wang", "authors": "Zhongju Wang, Long Wang, Chao Huang, Xiong Luo", "title": "BERT-based Chinese Text Classification for Emergency Domain with a Novel\n  Loss Function", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic Chinese text categorization method for\nsolving the emergency event report classification problem. Since bidirectional\nencoder representations from transformers (BERT) has achieved great success in\nnatural language processing domain, it is employed to derive emergency text\nfeatures in this study. To overcome the data imbalance problem in the\ndistribution of emergency event categories, a novel loss function is proposed\nto improve the performance of the BERT-based model. Meanwhile, to avoid the\nimpact of the extreme learning rate, the Adabound optimization algorithm that\nachieves a gradual smooth transition from Adam to SGD is employed to learn\nparameters of the model. To verify the feasibility and effectiveness of the\nproposed method, a Chinese emergency text dataset collected from the Internet\nis employed. Compared with benchmarking methods, the proposed method has\nachieved the best performance in terms of accuracy, weighted-precision,\nweighted-recall, and weighted-F1 values. Therefore, it is promising to employ\nthe proposed method for real applications in smart emergency management\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:25:00 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Zhongju", ""], ["Wang", "Long", ""], ["Huang", "Chao", ""], ["Luo", "Xiong", ""]]}, {"id": "2104.04243", "submitter": "Vivek Gupta", "authors": "J. Neeraja, Vivek Gupta, Vivek Srikumar", "title": "Incorporating External Knowledge to Enhance Tabular Reasoning", "comments": "11 pages, 1 Figure, 14 tables, To appear in NAACL 2021 (Short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning about tabular information presents unique challenges to modern NLP\napproaches which largely rely on pre-trained contextualized embeddings of text.\nIn this paper, we study these challenges through the problem of tabular natural\nlanguage inference. We propose easy and effective modifications to how\ninformation is presented to a model for this task. We show via systematic\nexperiments that these strategies substantially improve tabular inference\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:25:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Neeraja", "J.", ""], ["Gupta", "Vivek", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2104.04298", "submitter": "Peter Vieting", "authors": "Peter Vieting, Christoph L\\\"uscher, Wilfried Michel, Ralf Schl\\\"uter,\n  Hermann Ney", "title": "Feature Replacement and Combination for Hybrid ASR Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic modeling of raw waveform and learning feature extractors as part of\nthe neural network classifier has been the goal of many studies in the area of\nautomatic speech recognition (ASR). Recently, one line of research has focused\non frameworks that can be pre-trained on audio-only data in an unsupervised\nfashion and aim at improving downstream ASR tasks. In this work, we investigate\nthe usefulness of one of these front-end frameworks, namely wav2vec, for hybrid\nASR systems. In addition to deploying a pre-trained feature extractor, we\nexplore how to make use of an existing acoustic model (AM) trained on the same\ntask with different features as well. Another neural front-end which is only\ntrained together with the supervised ASR loss as well as traditional Gammatone\nfeatures are applied for comparison. Moreover, it is shown that the AM can be\nretrofitted with i-vectors for speaker adaptation. Finally, the described\nfeatures are combined in order to further advance the performance. With the\nfinal best system, we obtain a relative improvement of 4% and 6% over our\nprevious best model on the LibriSpeech test-clean and test-other sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:04:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 21:42:32 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Vieting", "Peter", ""], ["L\u00fcscher", "Christoph", ""], ["Michel", "Wilfried", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.04302", "submitter": "Tanya Goyal", "authors": "Tanya Goyal and Greg Durrett", "title": "Annotating and Modeling Fine-grained Factuality in Summarization", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent pre-trained abstractive summarization systems have started to achieve\ncredible performance, but a major barrier to their use in practice is their\npropensity to output summaries that are not faithful to the input and that\ncontain factual errors. While a number of annotated datasets and statistical\nmodels for assessing factuality have been explored, there is no clear picture\nof what errors are most important to target or where current techniques are\nsucceeding and failing. We explore both synthetic and human-labeled data\nsources for training models to identify factual errors in summarization, and\nstudy factuality at the word-, dependency-, and sentence-level. Our\nobservations are threefold. First, exhibited factual errors differ\nsignificantly across datasets, and commonly-used training sets of simple\nsynthetic errors do not reflect errors made on abstractive datasets like XSum.\nSecond, human-labeled data with fine-grained annotations provides a more\neffective training signal than sentence-level annotations or synthetic data.\nFinally, we show that our best factuality detection model enables training of\nmore factual XSum summarization models by allowing us to identify non-factual\ntokens in the training data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:20:44 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Goyal", "Tanya", ""], ["Durrett", "Greg", ""]]}, {"id": "2104.04318", "submitter": "Yao Fu", "authors": "Kun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang\n  Huang, Sheng Gao", "title": "Noisy-Labeled NER with Confidence Estimation", "comments": "NAACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies in deep learning have shown significant progress in named\nentity recognition (NER). Most existing works assume clean data annotation, yet\na fundamental challenge in real-world scenarios is the large amount of noise\nfrom a variety of sources (e.g., pseudo, weak, or distant annotations). This\nwork studies NER under a noisy labeled setting with calibrated confidence\nestimation. Based on empirical observations of different training dynamics of\nnoisy and clean labels, we propose strategies for estimating confidence scores\nbased on local and global independence assumptions. We partially marginalize\nout labels of low confidence with a CRF model. We further propose a calibration\nmethod for confidence scores based on the structure of entity labels. We\nintegrate our approach into a self-training framework for boosting performance.\nExperiments in general noisy settings with four languages and distantly labeled\nsettings demonstrate the effectiveness of our method. Our code can be found at\nhttps://github.com/liukun95/Noisy-NER-Confidence-Estimation\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:56:46 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 11:50:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Kun", ""], ["Fu", "Yao", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Zhang", "Ningyu", ""], ["Huang", "Songfang", ""], ["Gao", "Sheng", ""]]}, {"id": "2104.04369", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Linfeng Song, Lifeng Jin, Kun Xu, Dong Yu, Jiebo Luo", "title": "Video-aided Unsupervised Grammar Induction", "comments": "This paper is accepted by NAACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate video-aided grammar induction, which learns a constituency\nparser from both unlabeled text and its corresponding video. Existing methods\nof multi-modal grammar induction focus on learning syntactic grammars from\ntext-image pairs, with promising results showing that the information from\nstatic images is useful in induction. However, videos provide even richer\ninformation, including not only static objects but also actions and state\nchanges useful for inducing verb phrases. In this paper, we explore rich\nfeatures (e.g. action, object, scene, audio, face, OCR and speech) from videos,\ntaking the recent Compound PCFG model as the baseline. We further propose a\nMulti-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich\nfeatures from different modalities. Our proposed MMC-PCFG is trained end-to-end\nand outperforms each individual modality and previous state-of-the-art systems\non three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the\neffectiveness of leveraging video information for unsupervised grammar\ninduction.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:01:36 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 00:23:28 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Songyang", ""], ["Song", "Linfeng", ""], ["Jin", "Lifeng", ""], ["Xu", "Kun", ""], ["Yu", "Dong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2104.04402", "submitter": "Francesco Moramarco", "authors": "Francesco Moramarco, Alex Papadopoulos Korfiatis, Aleksandar Savkov,\n  Ehud Reiter", "title": "A preliminary study on evaluating Consultation Notes with Post-Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic summarisation has the potential to aid physicians in streamlining\nclerical tasks such as note taking. But it is notoriously difficult to evaluate\nthese systems and demonstrate that they are safe to be used in a clinical\nsetting. To circumvent this issue, we propose a semi-automatic approach whereby\nphysicians post-edit generated notes before submitting them. We conduct a\npreliminary study on the time saving of automatically generated consultation\nnotes with post-editing. Our evaluators are asked to listen to mock\nconsultations and to post-edit three generated notes. We time this and find\nthat it is faster than writing the note from scratch. We present insights and\nlessons learnt from this experiment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:42:00 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Moramarco", "Francesco", ""], ["Korfiatis", "Alex Papadopoulos", ""], ["Savkov", "Aleksandar", ""], ["Reiter", "Ehud", ""]]}, {"id": "2104.04412", "submitter": "Francesco Moramarco", "authors": "Francesco Moramarco, Damir Juric, Aleksandar Savkov, Ehud Reiter", "title": "Towards objectively evaluating the quality of generated medical\n  summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for evaluating the quality of generated text by asking\nevaluators to count facts, and computing precision, recall, f-score, and\naccuracy from the raw counts. We believe this approach leads to a more\nobjective and easier to reproduce evaluation. We apply this to the task of\nmedical report summarisation, where measuring objective quality and accuracy is\nof paramount importance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:02:56 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Moramarco", "Francesco", ""], ["Juric", "Damir", ""], ["Savkov", "Aleksandar", ""], ["Reiter", "Ehud", ""]]}, {"id": "2104.04429", "submitter": "Tanvi Dinkar", "authors": "Utku Norman, Tanvi Dinkar, Barbara Bruno, Chlo\\'e Clavel", "title": "Studying Alignment in Spontaneous Speech via Automatic Methods: How Do\n  Children Use Task-specific Referents to Succeed in a Collaborative Learning\n  Activity?", "comments": "* The authors contributed equally to this work. This article a\n  preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dialogue is successful when there is alignment between the speakers, at\ndifferent linguistic levels. In this work, we consider the dialogue occurring\nbetween interlocutors engaged in a collaborative learning task, and explore how\nperformance and learning (i.e. task success) relate to dialogue alignment\nprocesses. The main contribution of this work is to propose new measures to\nautomatically study alignment, to consider completely spontaneous spoken\ndialogues among children in the context of a collaborative learning activity.\nOur measures of alignment consider the children's use of expressions that are\nrelated to the task at hand, their follow-up actions of these expressions, and\nhow it links to task success. Focusing on expressions related to the task gives\nus insight into the way children use (potentially unfamiliar) terminology\nrelated to the task. A first finding of this work is the discovery that the\nmeasures we propose can capture elements of lexical alignment in such a\ncontext. Through these measures, we find that teams with bad performance often\naligned too late in the dialogue to achieve task success, and that they were\nlate to follow up each other's instructions with actions. We also found that\nwhile interlocutors do not exhibit hesitation phenomena (which we measure by\nlooking at fillers) in introducing expressions pertaining to the task, they do\nexhibit hesitation before accepting the expression, in the role of\nclarification. Lastly, we show that information management markers (measured by\nthe discourse marker 'oh') occur in the general vicinity of the follow up\nactions from (automatically) inferred instructions. However, good performers\ntend to have this marker closer to these actions. Our measures still reflect\nsome fine-grained aspects of learning in the dialogue, even if we cannot\nconclude that overall they are linked to the final measure of learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:26:12 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Norman", "Utku", ""], ["Dinkar", "Tanvi", ""], ["Bruno", "Barbara", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "2104.04434", "submitter": "Jinlan Fu", "authors": "Jinlan Fu, Liangjing Feng, Qi Zhang, Xuanjing Huang and Pengfei Liu", "title": "Larger-Context Tagging: When and Why Does It Work?", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of neural networks and pretraining techniques has spawned\nmany sentence-level tagging systems that achieved superior performance on\ntypical benchmarks. However, a relatively less discussed topic is what if more\ncontext information is introduced into current top-scoring tagging systems.\nAlthough several existing works have attempted to shift tagging systems from\nsentence-level to document-level, there is still no consensus conclusion about\nwhen and why it works, which limits the applicability of the larger-context\napproach in tagging tasks. In this paper, instead of pursuing a\nstate-of-the-art tagging system by architectural exploration, we focus on\ninvestigating when and why the larger-context training, as a general strategy,\ncan work.\n  To this end, we conduct a thorough comparative study on four proposed\naggregators for context information collecting and present an attribute-aided\nevaluation method to interpret the improvement brought by larger-context\ntraining. Experimentally, we set up a testbed based on four tagging tasks and\nthirteen datasets. Hopefully, our preliminary observations can deepen the\nunderstanding of larger-context training and enlighten more follow-up works on\nthe use of contextual information.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:35:30 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Fu", "Jinlan", ""], ["Feng", "Liangjing", ""], ["Zhang", "Qi", ""], ["Huang", "Xuanjing", ""], ["Liu", "Pengfei", ""]]}, {"id": "2104.04466", "submitter": "Weizhe Lin", "authors": "Weizhe Lin, Bo-Hsian Tseng, Bill Byrne", "title": "Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking", "comments": "8 pages of main content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue State Tracking is a crucial part of multi-domain task-oriented\ndialogue systems, responsible for extracting information from user utterances.\nWe present a novel architecture that utilizes the powerful generative model\nGPT-2 to generate slot values one by one causally, and at the same time\nutilizes Graph Attention Networks to enable inter-slot information exchanges,\nwhich exploits the inter-slot relations such as correlations. Our model\nachieves $54.86\\%$ joint accuracy in MultiWOZ 2.0, and it retains a performance\nof up to $50.43\\%$ in sparse supervision training, where only session-level\nannotations ($14.3\\%$ of the full training set) are used. We conduct detailed\nanalyses to demonstrate the significance of using graph models in this task,\nand show by experiments that the proposed graph modules indeed help to capture\nmore inter-slot relations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:27:34 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lin", "Weizhe", ""], ["Tseng", "Bo-Hsian", ""], ["Byrne", "Bill", ""]]}, {"id": "2104.04470", "submitter": "Elisa Ferracane", "authors": "Elisa Ferracane, Greg Durrett, Junyi Jessy Li and Katrin Erk", "title": "Did they answer? Subjective acts and intents in conversational discourse", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discourse signals are often implicit, leaving it up to the interpreter to\ndraw the required inferences. At the same time, discourse is embedded in a\nsocial context, meaning that interpreters apply their own assumptions and\nbeliefs when resolving these inferences, leading to multiple, valid\ninterpretations. However, current discourse data and frameworks ignore the\nsocial aspect, expecting only a single ground truth. We present the first\ndiscourse dataset with multiple and subjective interpretations of English\nconversation in the form of perceived conversation acts and intents. We\ncarefully analyze our dataset and create computational models to (1) confirm\nour hypothesis that taking into account the bias of the interpreters leads to\nbetter predictions of the interpretations, (2) and show disagreements are\nnuanced and require a deeper understanding of the different contextual factors.\nWe share our dataset and code at http://github.com/elisaF/subjective_discourse.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:34:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ferracane", "Elisa", ""], ["Durrett", "Greg", ""], ["Li", "Junyi Jessy", ""], ["Erk", "Katrin", ""]]}, {"id": "2104.04473", "submitter": "Deepak Narayanan", "authors": "Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,\n  Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi\n  Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia", "title": "Efficient Large-Scale Language Model Training on GPU Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these large models efficiently is challenging for\ntwo reasons: a) GPU memory capacity is limited, making it impossible to fit\nlarge models on a single GPU or even on a multi-GPU server; and b) the number\nof compute operations required to train these models can result in\nunrealistically long training times. New methods of model parallelism such as\ntensor and pipeline parallelism have been proposed to address these challenges.\nUnfortunately, naive usage leads to fundamental scaling issues at thousands of\nGPUs due to various reasons, e.g., expensive cross-node communication or idle\nperiods waiting on other devices.\n  In this work, we show how to compose different types of parallelism methods\n(tensor, pipeline, and data parallelism) to scale to thousands of GPUs,\nachieving a two-order-of-magnitude increase in the sizes of models we can\nefficiently train compared to existing systems. We survey techniques for\npipeline parallelism and propose a novel interleaved pipeline parallelism\nschedule that can improve throughput by more than 10% with comparable memory\nfootprint compared to previously-proposed approaches. We quantitatively study\nthe trade-offs between tensor, pipeline, and data parallelism, and provide\nintuition as to how to configure distributed training of a large model. Our\napproach allows us to perform training iterations on a model with 1 trillion\nparameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of\n52% of peak; previous efforts to train similar-sized models achieve much lower\nthroughput (36% of theoretical peak). Our code is open sourced at\nhttps://github.com/nvidia/megatron-lm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:43:11 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:44:52 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Narayanan", "Deepak", ""], ["Shoeybi", "Mohammad", ""], ["Casper", "Jared", ""], ["LeGresley", "Patrick", ""], ["Patwary", "Mostofa", ""], ["Korthikanti", "Vijay Anand", ""], ["Vainbrand", "Dmitri", ""], ["Kashinkunti", "Prethvi", ""], ["Bernauer", "Julie", ""], ["Catanzaro", "Bryan", ""], ["Phanishayee", "Amar", ""], ["Zaharia", "Matei", ""]]}, {"id": "2104.04487", "submitter": "Rodrigo Cabrera", "authors": "Rodrigo Cabrera, Xiaofeng Liu, Mohammadreza Ghodsi, Zebulun Matteson,\n  Eugene Weinstein, Anjuli Kannan", "title": "Language model fusion for streaming end to end speech recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Streaming processing of speech audio is required for many contemporary\npractical speech recognition tasks. Even with the large corpora of manually\ntranscribed speech data available today, it is impossible for such corpora to\ncover adequately the long tail of linguistic content that's important for tasks\nsuch as open-ended dictation and voice search. We seek to address both the\nstreaming and the tail recognition challenges by using a language model (LM)\ntrained on unpaired text data to enhance the end-to-end (E2E) model. We extend\nshallow fusion and cold fusion approaches to streaming Recurrent Neural Network\nTransducer (RNNT), and also propose two new competitive fusion approaches that\nfurther enhance the RNNT architecture. Our results on multiple languages with\nvarying training set sizes show that these fusion methods improve streaming\nRNNT performance through introducing extra linguistic features. Cold fusion\nworks consistently better on streaming RNNT with up to a 8.5% WER improvement.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:14:28 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cabrera", "Rodrigo", ""], ["Liu", "Xiaofeng", ""], ["Ghodsi", "Mohammadreza", ""], ["Matteson", "Zebulun", ""], ["Weinstein", "Eugene", ""], ["Kannan", "Anjuli", ""]]}, {"id": "2104.04488", "submitter": "Hanjie Chen", "authors": "Hanjie Chen, Song Feng, Jatin Ganhotra, Hui Wan, Chulaka Gunasekara,\n  Sachindra Joshi, Yangfeng Ji", "title": "Explaining Neural Network Predictions on Sentence Pairs via Learning\n  Word-Group Masks", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining neural network models is important for increasing their\ntrustworthiness in real-world applications. Most existing methods generate\npost-hoc explanations for neural network models by identifying individual\nfeature attributions or detecting interactions between adjacent features.\nHowever, for models with text pairs as inputs (e.g., paraphrase\nidentification), existing methods are not sufficient to capture feature\ninteractions between two texts and their simple extension of computing all\nword-pair interactions between two texts is computationally inefficient. In\nthis work, we propose the Group Mask (GMASK) method to implicitly detect word\ncorrelations by grouping correlated words from the input text pair together and\nmeasure their contribution to the corresponding NLP tasks as a whole. The\nproposed method is evaluated with two different model architectures\n(decomposable attention model and BERT) across four datasets, including natural\nlanguage inference and paraphrase identification tasks. Experiments show the\neffectiveness of GMASK in providing faithful explanations to these models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:14:34 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 13:41:27 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chen", "Hanjie", ""], ["Feng", "Song", ""], ["Ganhotra", "Jatin", ""], ["Wan", "Hui", ""], ["Gunasekara", "Chulaka", ""], ["Joshi", "Sachindra", ""], ["Ji", "Yangfeng", ""]]}, {"id": "2104.04497", "submitter": "Lifeng Han", "authors": "Lifeng Han, Gareth J. F. Jones, Alan F. Smeaton and Paolo Bolzoni", "title": "Chinese Character Decomposition for Neural MT with Multi-Word\n  Expressions", "comments": "Accepted to publish in NoDaLiDa2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chinese character decomposition has been used as a feature to enhance Machine\nTranslation (MT) models, combining radicals into character and word level\nmodels. Recent work has investigated ideograph or stroke level embedding.\nHowever, questions remain about different decomposition levels of Chinese\ncharacter representations, radical and strokes, best suited for MT. To\ninvestigate the impact of Chinese decomposition embedding in detail, i.e.,\nradical, stroke, and intermediate levels, and how well these decompositions\nrepresent the meaning of the original character sequences, we carry out\nanalysis with both automated and human evaluation of MT. Furthermore, we\ninvestigate if the combination of decomposed Multiword Expressions (MWEs) can\nenhance the model learning. MWE integration into MT has seen more than a decade\nof exploration. However, decomposed MWEs has not previously been explored.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:28:49 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Han", "Lifeng", ""], ["Jones", "Gareth J. F.", ""], ["Smeaton", "Alan F.", ""], ["Bolzoni", "Paolo", ""]]}, {"id": "2104.04515", "submitter": "Xi Ye", "authors": "Xi Ye, Rohan Nair, Greg Durrett", "title": "Evaluating Explanations for Reading Comprehension with Realistic\n  Counterfactuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Token-level attributions have been extensively studied to explain model\npredictions for a wide range of classification tasks in NLP (e.g., sentiment\nanalysis), but such explanation techniques are less explored for machine\nreading comprehension (RC) tasks. Although the transformer-based models used\nhere are identical to those used for classification, the underlying reasoning\nthese models perform is very different and different types of explanations are\nrequired. We propose a methodology to evaluate explanations: an explanation\nshould allow us to understand the RC model's high-level behavior with respect\nto a set of realistic counterfactual input scenarios. We define these\ncounterfactuals for several RC settings, and by connecting explanation\ntechniques' outputs to high-level model behavior, we can evaluate how useful\ndifferent explanations really are. Our analysis suggests that pairwise\nexplanation techniques are better suited to RC than token-level attributions,\nwhich are often unfaithful in the scenarios we consider. We additionally\npropose an improvement to an attention-based attribution technique, resulting\nin explanations which better reveal the model's behavior.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:55:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ye", "Xi", ""], ["Nair", "Rohan", ""], ["Durrett", "Greg", ""]]}, {"id": "2104.04517", "submitter": "Vaibhav Bhat", "authors": "Vaibhav Bhat, Anita Yadav, Sonal Yadav, Dhivya Chandrasekran, Vijay\n  Mago", "title": "AdCOFE: Advanced Contextual Feature Extraction in Conversations for\n  emotion classification", "comments": "12 pages, to be published in PeerJ Computer Science Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion recognition in conversations is an important step in various virtual\nchat bots which require opinion-based feedback, like in social media threads,\nonline support and many more applications. Current Emotion recognition in\nconversations models face issues like (a) loss of contextual information in\nbetween two dialogues of a conversation, (b) failure to give appropriate\nimportance to significant tokens in each utterance and (c) inability to pass on\nthe emotional information from previous utterances.The proposed model of\nAdvanced Contextual Feature Extraction (AdCOFE) addresses these issues by\nperforming unique feature extraction using knowledge graphs, sentiment lexicons\nand phrases of natural language at all levels (word and position embedding) of\nthe utterances. Experiments on the Emotion recognition in conversations dataset\nshow that AdCOFE is beneficial in capturing emotions in conversations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:58:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Bhat", "Vaibhav", ""], ["Yadav", "Anita", ""], ["Yadav", "Sonal", ""], ["Chandrasekran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2104.04549", "submitter": "Dumitru-Clementin Cercel", "authors": "Andrei-Marius Avram, George-Eduard Zaharia, Dumitru-Clementin Cercel,\n  Mihai Dascalu", "title": "UPB at SemEval-2021 Task 8: Extracting Semantic Information on\n  Measurements as Multi-Turn Question Answering", "comments": "5 pages, 3 figures, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extracting semantic information on measurements and counts is an important\ntopic in terms of analyzing scientific discourses. The 8th task of\nSemEval-2021: Counts and Measurements (MeasEval) aimed to boost research in\nthis direction by providing a new dataset on which participants train their\nmodels to extract meaningful information on measurements from scientific texts.\nThe competition is composed of five subtasks that build on top of each other:\n(1) quantity span identification, (2) unit extraction from the identified\nquantities and their value modifier classification, (3) span identification for\nmeasured entities and measured properties, (4) qualifier span identification,\nand (5) relation extraction between the identified quantities, measured\nentities, measured properties, and qualifiers. We approached these challenges\nby first identifying the quantities, extracting their units of measurement,\nclassifying them with corresponding modifiers, and afterwards using them to\njointly solve the last three subtasks in a multi-turn question answering\nmanner. Our best performing model obtained an overlapping F1-score of 36.91% on\nthe test set.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:23:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Avram", "Andrei-Marius", ""], ["Zaharia", "George-Eduard", ""], ["Cercel", "Dumitru-Clementin", ""], ["Dascalu", "Mihai", ""]]}, {"id": "2104.04552", "submitter": "Wenqian Ronny Huang", "authors": "W. Ronny Huang, Tara N. Sainath, Cal Peyser, Shankar Kumar, David\n  Rybach, Trevor Strohman", "title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition", "comments": "Presented as conference paper at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Lookup-Table Language Models (LookupLM), a method for scaling up\nthe size of RNN language models with only a constant increase in the floating\npoint operations, by increasing the expressivity of the embedding table. In\nparticular, we instantiate an (additional) embedding table which embeds the\nprevious n-gram token sequence, rather than a single token. This allows the\nembedding table to be scaled up arbitrarily -- with a commensurate increase in\nperformance -- without changing the token vocabulary. Since embeddings are\nsparsely retrieved from the table via a lookup; increasing the size of the\ntable adds neither extra operations to each forward pass nor extra parameters\nthat need to be stored on limited GPU/TPU memory. We explore scaling n-gram\nembedding tables up to nearly a billion parameters. When trained on a 3-billion\nsentence corpus, we find that LookupLM improves long tail log perplexity by\n2.44 and long tail WER by 23.4% on a downstream speech recognition task over a\nstandard RNN language model baseline, an improvement comparable to a scaling up\nthe baseline by 6.2x the number of floating point operations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:31:30 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 01:01:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huang", "W. Ronny", ""], ["Sainath", "Tara N.", ""], ["Peyser", "Cal", ""], ["Kumar", "Shankar", ""], ["Rybach", "David", ""], ["Strohman", "Trevor", ""]]}, {"id": "2104.04580", "submitter": "Jian Wu", "authors": "Jian Wu, Rajal Nivargi, Sree Sai Teja Lanka, Arjun Manoj Menon, Sai\n  Ajay Modukuri, Nishanth Nakshatri, Xin Wei, Zhuoer Wang, James Caverlee,\n  Sarah M. Rajtmajer, C. Lee Giles", "title": "Predicting the Reproducibility of Social and Behavioral Science Papers\n  Using Supervised Learning Models", "comments": "17 pages, 8 figures, a draft to be submitted to JCDL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, significant effort has been invested verifying the\nreproducibility and robustness of research claims in social and behavioral\nsciences (SBS), much of which has involved resource-intensive replication\nprojects. In this paper, we investigate prediction of the reproducibility of\nSBS papers using machine learning methods based on a set of features. We\npropose a framework that extracts five types of features from scholarly work\nthat can be used to support assessments of reproducibility of published\nresearch claims. Bibliometric features, venue features, and author features are\ncollected from public APIs or extracted using open source machine learning\nlibraries with customized parsers. Statistical features, such as p-values, are\nextracted by recognizing patterns in the body text. Semantic features, such as\nfunding information, are obtained from public APIs or are extracted using\nnatural language processing models. We analyze pairwise correlations between\nindividual features and their importance for predicting a set of human-assessed\nground truth labels. In doing so, we identify a subset of 9 top features that\nplay relatively more important roles in predicting the reproducibility of SBS\npapers in our corpus. Results are verified by comparing performances of 10\nsupervised predictive classifiers trained on different sets of features.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:45:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wu", "Jian", ""], ["Nivargi", "Rajal", ""], ["Lanka", "Sree Sai Teja", ""], ["Menon", "Arjun Manoj", ""], ["Modukuri", "Sai Ajay", ""], ["Nakshatri", "Nishanth", ""], ["Wei", "Xin", ""], ["Wang", "Zhuoer", ""], ["Caverlee", "James", ""], ["Rajtmajer", "Sarah M.", ""], ["Giles", "C. Lee", ""]]}, {"id": "2104.04584", "submitter": "Swakkhar Shatabda", "authors": "Md. Mahinur Rashid, Hasin Kawsar Jahan, Annysha Huzzat, Riyasaat Ahmed\n  Rahul, Tamim Bin Zakir, Farhana Meem, Md. Saddam Hossain Mukta and Swakkhar\n  Shatabda", "title": "Text2Chart: A Multi-Staged Chart Generator from Natural Language Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generation of scientific visualization from analytical natural language text\nis a challenging task. In this paper, we propose Text2Chart, a multi-staged\nchart generator method. Text2Chart takes natural language text as input and\nproduce visualization as two-dimensional charts. Text2Chart approaches the\nproblem in three stages. Firstly, it identifies the axis elements of a chart\nfrom the given text known as x and y entities. Then it finds a mapping of\nx-entities with its corresponding y-entities. Next, it generates a chart type\nsuitable for the given text: bar, line or pie. Combination of these three\nstages is capable of generating visualization from the given analytical text.\nWe have also constructed a dataset for this problem. Experiments show that\nText2Chart achieves best performances with BERT based encodings with LSTM\nmodels in the first stage to label x and y entities, Random Forest classifier\nfor the mapping stage and fastText embedding with LSTM for the chart type\nprediction. In our experiments, all the stages show satisfactory results and\neffectiveness considering formation of charts from analytical text, achieving a\ncommendable overall performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:42:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rashid", "Md. Mahinur", ""], ["Jahan", "Hasin Kawsar", ""], ["Huzzat", "Annysha", ""], ["Rahul", "Riyasaat Ahmed", ""], ["Zakir", "Tamim Bin", ""], ["Meem", "Farhana", ""], ["Mukta", "Md. Saddam Hossain", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "2104.04597", "submitter": "Muhao Chen", "authors": "Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang\n  Lorraine Li, Andrew McCallum", "title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases often consist of facts which are harvested from a variety of\nsources, many of which are noisy and some of which conflict, resulting in a\nlevel of uncertainty for each triple. Knowledge bases are also often\nincomplete, prompting the use of embedding methods to generalize from known\nfacts, however, existing embedding methods only model triple-level uncertainty,\nand reasoning results lack global consistency. To address these shortcomings,\nwe propose BEUrRE, a novel uncertain knowledge graph embedding method with\ncalibrated probabilistic semantics. BEUrRE models each entity as a box (i.e.\naxis-aligned hyperrectangle) and relations between two entities as affine\ntransforms on the head and tail entity boxes. The geometry of the boxes allows\nfor efficient calculation of intersections and volumes, endowing the model with\ncalibrated probabilistic semantics and facilitating the incorporation of\nrelational constraints. Extensive experiments on two benchmark datasets show\nthat BEUrRE consistently outperforms baselines on confidence prediction and\nfact ranking due to its probabilistic calibration and ability to capture\nhigh-order dependencies among facts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:01:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xuelu", ""], ["Boratko", "Michael", ""], ["Chen", "Muhao", ""], ["Dasgupta", "Shib Sankar", ""], ["Li", "Xiang Lorraine", ""], ["McCallum", "Andrew", ""]]}, {"id": "2104.04627", "submitter": "Elizabeth Combs", "authors": "Xiangyun Chu (1), Elizabeth Combs (1), Amber Wang (1), Michael Picheny\n  (2) ((1) Center for Data Science, New York University, (2) Courant Computer\n  Science and Center for Data Science, New York University)", "title": "Accented Speech Recognition Inspired by Human Perception", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While improvements have been made in automatic speech recognition performance\nover the last several years, machines continue to have significantly lower\nperformance on accented speech than humans. In addition, the most significant\nimprovements on accented speech primarily arise by overwhelming the problem\nwith hundreds or even thousands of hours of data. Humans typically require much\nless data to adapt to a new accent. This paper explores methods that are\ninspired by human perception to evaluate possible performance improvements for\nrecognition of accented speech, with a specific focus on recognizing speech\nwith a novel accent relative to that of the training data. Our experiments are\nrun on small, accessible datasets that are available to the research community.\nWe explore four methodologies: pre-exposure to multiple accents, grapheme and\nphoneme-based pronunciations, dropout (to improve generalization to a novel\naccent), and the identification of the layers in the neural network that can\nspecifically be associated with accent modeling. Our results indicate that\nmethods based on human perception are promising in reducing WER and\nunderstanding how accented speech is modeled in neural networks for novel\naccents.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 22:35:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chu", "Xiangyun", ""], ["Combs", "Elizabeth", ""], ["Wang", "Amber", ""], ["Picheny", "Michael", ""]]}, {"id": "2104.04630", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Diptanu Sarkar, Marcos Zampieri, Alexander\n  Ororbia", "title": "WLV-RIT at SemEval-2021 Task 5: A Neural Transformer Framework for\n  Detecting Toxic Spans", "comments": "Accepted to SemEval-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the widespread use of social media has led to an increase in\nthe generation of toxic and offensive content on online platforms. In response,\nsocial media platforms have worked on developing automatic detection methods\nand employing human moderators to cope with this deluge of offensive content.\nWhile various state-of-the-art statistical models have been applied to detect\ntoxic posts, there are only a few studies that focus on detecting the words or\nexpressions that make a post offensive. This motivates the organization of the\nSemEval-2021 Task 5: Toxic Spans Detection competition, which has provided\nparticipants with a dataset containing toxic spans annotation in English posts.\nIn this paper, we present the WLV-RIT entry for the SemEval-2021 Task 5. Our\nbest performing neural transformer model achieves an $0.68$ F1-Score.\nFurthermore, we develop an open-source framework for multilingual detection of\noffensive spans, i.e., MUDES, based on neural transformers that detect toxic\nspans in texts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 22:52:26 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 22:32:17 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 22:09:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Sarkar", "Diptanu", ""], ["Zampieri", "Marcos", ""], ["Ororbia", "Alexander", ""]]}, {"id": "2104.04632", "submitter": "Tharindu Ranasinghe Mr", "authors": "Hansi Hettiarachchi, Tharindu Ranasinghe", "title": "TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and\n  Cross-lingual Word-in-Context Disambiguation", "comments": "Accepted to SemEval-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying whether a word carries the same meaning or different meaning in\ntwo contexts is an important research area in natural language processing which\nplays a significant role in many applications such as question answering,\ndocument summarisation, information retrieval and information extraction. Most\nof the previous work in this area rely on language-specific resources making it\ndifficult to generalise across languages. Considering this limitation, our\napproach to SemEval-2021 Task 2 is based only on pretrained transformer models\nand does not use any language-specific processing and resources. Despite that,\nour best model achieves 0.90 accuracy for English-English subtask which is very\ncompatible compared to the best result of the subtask; 0.93 accuracy. Our\napproach also achieves satisfactory results in other monolingual and\ncross-lingual language pairs as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:06:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hettiarachchi", "Hansi", ""], ["Ranasinghe", "Tharindu", ""]]}, {"id": "2104.04670", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein", "title": "Meta-tuning Language Models to Answer Prompts Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large pretrained language models like GPT-3 have acquired a surprising\nability to perform zero-shot classification (ZSC). For example, to classify\nreview sentiments, we can \"prompt\" the language model with the review and the\nquestion \"Is the review positive?\" as the context, and ask it to predict\nwhether the next word is \"Yes\" or \"No\". However, these models are not\nspecialized for answering these prompts. To address this weakness, we propose\nmeta-tuning, which trains the model to specialize in answering prompts but\nstill generalize to unseen tasks. To create the training data, we aggregated 43\nexisting datasets, annotated 441 label descriptions in total, and unified them\ninto the above question answering (QA) format. After meta-tuning, our model\noutperforms a same-sized QA model for most labels on unseen tasks, and we\nforecast that the performance would improve for even larger models. Therefore,\nmeasuring ZSC performance on non-specialized language models might\nunderestimate their true capability, and community-wide efforts on aggregating\ndatasets and unifying their formats can help build models that understand\nprompts better.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 02:57:22 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:43:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Lee", "Kristy", ""], ["Zhang", "Zheng", ""], ["Klein", "Dan", ""]]}, {"id": "2104.04676", "submitter": "Xutan Peng", "authors": "Xutan Peng, Guanyi Chen, Chenghua Lin, Mark Stevenson", "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal\n  Procrustes Analysis", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph Embeddings (KGEs) have been intensively explored in recent\nyears due to their promise for a wide range of applications. However, existing\nstudies focus on improving the final model performance without acknowledging\nthe computational cost of the proposed approaches, in terms of execution time\nand environmental impact. This paper proposes a simple yet effective KGE\nframework which can reduce the training time and carbon footprint by orders of\nmagnitudes compared with state-of-the-art approaches, while producing\ncompetitive performance. We highlight three technical innovations: full batch\nlearning via relational matrices, closed-form Orthogonal Procrustes Analysis\nfor KGEs, and non-negative-sampling training. In addition, as the first KGE\nmethod whose entity embeddings also store full relation information, our\ntrained models encode rich semantics and are highly interpretable.\nComprehensive experiments and ablation studies involving 13 strong baselines\nand two standard datasets verify the effectiveness and efficiency of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 03:55:45 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 12:17:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Peng", "Xutan", ""], ["Chen", "Guanyi", ""], ["Lin", "Chenghua", ""], ["Stevenson", "Mark", ""]]}, {"id": "2104.04689", "submitter": "Zhi Chen", "authors": "Zhi Chen, Lu Chen, Yanbin Zhao, Ruisheng Cao, Zihan Xu, Su Zhu and Kai\n  Yu", "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser", "comments": "Accepted at NAACL2021, 11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database schema, Text-to-SQL aims to translate a natural language\nquestion into the corresponding SQL query. Under the setup of cross-domain,\ntraditional semantic parsing models struggle to adapt to unseen database\nschemas. To improve the model generalization capability for rare and unseen\nschemas, we propose a new architecture, ShadowGNN, which processes schemas at\nabstract and semantic levels. By ignoring names of semantic items in databases,\nabstract schemas are exploited in a well-designed graph projection neural\nnetwork to obtain delexicalized representation of question and schema. Based on\nthe domain-independent representations, a relation-aware transformer is\nutilized to further extract logical linking between question and schema.\nFinally, a SQL decoder with context-free grammar is applied. On the challenging\nText-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms\nstate-of-the-art models. When the annotated data is extremely limited (only\n10\\% training set), ShadowGNN gets over absolute 5\\% performance gain, which\nshows its powerful generalization ability. Our implementation will be\nopen-sourced at \\url{https://github.com/WowCZ/shadowgnn}.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 05:48:28 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 07:06:55 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chen", "Zhi", ""], ["Chen", "Lu", ""], ["Zhao", "Yanbin", ""], ["Cao", "Ruisheng", ""], ["Xu", "Zihan", ""], ["Zhu", "Su", ""], ["Yu", "Kai", ""]]}, {"id": "2104.04692", "submitter": "Hongqiu Wu", "authors": "Hongqiu Wu and Hai Zhao and Min Zhang", "title": "Not All Attention Is All You Need", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond the success story of pre-trained language models (PrLMs) in recent\nnatural language processing, they are susceptible to over-fitting due to\nunusual large model size. To this end, dropout serves as a therapy. However,\nexisting methods like random-based, knowledge-based and search-based dropout\nare more general but less effective onto self-attention based models, which are\nbroadly chosen as the fundamental architecture of PrLMs. In this paper, we\npropose a novel dropout method named AttendOut to let self-attention empowered\nPrLMs capable of more robust task-specific tuning. We demonstrate that\nstate-of-the-art models with elaborate training design may achieve much\nstronger results. We verify the universality of our approach on extensive\nnatural language processing tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 06:24:52 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 12:13:12 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 03:09:39 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wu", "Hongqiu", ""], ["Zhao", "Hai", ""], ["Zhang", "Min", ""]]}, {"id": "2104.04697", "submitter": "Cheng-Te Li", "authors": "Chih-Yao Chen, Cheng-Te Li", "title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning", "comments": "Accepted to NAACL 2021. Code is available at\n  https://github.com/dinobby/ZS-BERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While relation extraction is an essential task in knowledge acquisition and\nrepresentation, and new-generated relations are common in the real world, less\neffort is made to predict unseen relations that cannot be observed at the\ntraining stage. In this paper, we formulate the zero-shot relation extraction\nproblem by incorporating the text description of seen and unseen relations. We\npropose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to\ndirectly predict unseen relations without hand-crafted attribute labeling and\nmultiple pairwise classifications. Given training instances consisting of input\nsentences and the descriptions of their relations, ZS-BERT learns two functions\nthat project sentences and relation descriptions into an embedding space by\njointly minimizing the distances between them and classifying seen relations.\nBy generating the embeddings of unseen relations and new-coming sentences based\non such two functions, we use nearest neighbor search to obtain the prediction\nof unseen relations. Experiments conducted on two well-known datasets exhibit\nthat ZS-BERT can outperform existing methods by at least 13.54\\% improvement on\nF1 score.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 06:53:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Chih-Yao", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2104.04725", "submitter": "Julian Eisenschlos", "authors": "Julian Martin Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin\n  B\\\"orschinger, Jordan Boyd-Graber", "title": "Fool Me Twice: Entailment from Wikipedia Gamification", "comments": "Published in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We release FoolMeTwice (FM2 for short), a large dataset of challenging\nentailment pairs collected through a fun multi-player game. Gamification\nencourages adversarial examples, drastically lowering the number of examples\nthat can be solved using \"shortcuts\" compared to other popular entailment\ndatasets. Players are presented with two tasks. The first task asks the player\nto write a plausible claim based on the evidence from a Wikipedia page. The\nsecond one shows two plausible claims written by other players, one of which is\nfalse, and the goal is to identify it before the time runs out. Players \"pay\"\nto see clues retrieved from the evidence pool: the more evidence the player\nneeds, the harder the claim. Game-play between motivated players leads to\ndiverse strategies for crafting claims, such as temporal inference and\ndiverting to unrelated evidence, and results in higher quality data for the\nentailment and evidence retrieval tasks. We open source the dataset and the\ngame code.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 09:58:40 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Eisenschlos", "Julian Martin", ""], ["Dhingra", "Bhuwan", ""], ["Bulian", "Jannis", ""], ["B\u00f6rschinger", "Benjamin", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2104.04736", "submitter": "Anna Langedijk", "authors": "Anna Langedijk, Verna Dankers, Phillip Lippe, Sander Bos, Bryan\n  Cardenas Guevara, Helen Yannakoudakis, Ekaterina Shutova", "title": "Meta-learning for fast cross-lingual adaptation in dependency parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning, or learning to learn, is a technique that can help to overcome\nresource scarcity in cross-lingual NLP problems, by enabling fast adaptation to\nnew tasks. We apply model-agnostic meta-learning (MAML) to the task of\ncross-lingual dependency parsing. We train our model on a diverse set of\nlanguages to learn a parameter initialization that can adapt quickly to new\nlanguages. We find that meta-learning with pre-training can significantly\nimprove upon the performance of language transfer and standard supervised\nlearning baselines for a variety of unseen, typologically diverse, and\nlow-resource languages, in a few-shot learning setup.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 11:10:16 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:10:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Langedijk", "Anna", ""], ["Dankers", "Verna", ""], ["Lippe", "Phillip", ""], ["Bos", "Sander", ""], ["Guevara", "Bryan Cardenas", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2104.04739", "submitter": "Anna Glazkova", "authors": "Mikhail Kotyushev, Anna Glazkova, Dmitry Morozov", "title": "MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with\n  Pre-trained Language Models for Toxic Spans Detection", "comments": "Accepted at SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our system for SemEval-2021 Task 5 on Toxic Spans\nDetection. We developed ensemble models using BERT-based neural architectures\nand post-processing to combine tokens into spans. We evaluated several\npre-trained language models using various ensemble techniques for toxic span\nidentification and achieved sizable improvements over our baseline fine-tuned\nBERT models. Finally, our system obtained a F1-score of 67.55% on test data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 11:27:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kotyushev", "Mikhail", ""], ["Glazkova", "Anna", ""], ["Morozov", "Dmitry", ""]]}, {"id": "2104.04748", "submitter": "Zhengxu Hou", "authors": "Zhengxu Hou, Bang Liu, Ruihui Zhao, Zijing Ou, Yafei Liu, Xi Chen,\n  Yefeng Zheng", "title": "Imperfect also Deserves Reward: Multi-Level and Sequential Reward\n  Modeling for Better Dialog Management", "comments": "9 pages", "journal-ref": "NAACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For task-oriented dialog systems, training a Reinforcement Learning (RL)\nbased Dialog Management module suffers from low sample efficiency and slow\nconvergence speed due to the sparse rewards in RL.To solve this problem, many\nstrategies have been proposed to give proper rewards when training RL, but\ntheir rewards lack interpretability and cannot accurately estimate the\ndistribution of state-action pairs in real dialogs. In this paper, we propose a\nmulti-level reward modeling approach that factorizes a reward into a\nthree-level hierarchy: domain, act, and slot. Based on inverse adversarial\nreinforcement learning, our designed reward model can provide more accurate and\nexplainable reward signals for state-action pairs.Extensive evaluations show\nthat our approach can be applied to a wide range of reinforcement\nlearning-based dialog systems and significantly improves both the performance\nand the speed of convergence.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 12:20:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hou", "Zhengxu", ""], ["Liu", "Bang", ""], ["Zhao", "Ruihui", ""], ["Ou", "Zijing", ""], ["Liu", "Yafei", ""], ["Chen", "Xi", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2104.04751", "submitter": "Aarne Talman", "authors": "Aarne Talman, Marianna Apidianaki, Stergios Chatzikyriakidis, J\\\"org\n  Tiedemann", "title": "NLI Data Sanity Check: Assessing the Effect of Data Corruption on Model\n  Performance", "comments": "NoDaLiDa 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained neural language models give high performance on natural language\ninference (NLI) tasks. But whether they actually understand the meaning of the\nprocessed sequences remains unclear. We propose a new diagnostics test suite\nwhich allows to assess whether a dataset constitutes a good testbed for\nevaluating the models' meaning understanding capabilities. We specifically\napply controlled corruption transformations to widely used benchmarks (MNLI and\nANLI), which involve removing entire word classes and often lead to\nnon-sensical sentence pairs. If model accuracy on the corrupted data remains\nhigh, then the dataset is likely to contain statistical biases and artefacts\nthat guide prediction. Inversely, a large decrease in model accuracy indicates\nthat the original dataset provides a proper challenge to the models' reasoning\ncapabilities. Hence, our proposed controls can serve as a crash test for\ndeveloping high quality data for NLI tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 12:28:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Talman", "Aarne", ""], ["Apidianaki", "Marianna", ""], ["Chatzikyriakidis", "Stergios", ""], ["Tiedemann", "J\u00f6rg", ""]]}, {"id": "2104.04770", "submitter": "Nazanin Sabri", "authors": "Alireza Salemi, Nazanin Sabri, Emad Kebriaei, Behnam Bahrak, Azadeh\n  Shakery", "title": "UTNLP at SemEval-2021 Task 5: A Comparative Analysis of Toxic Span\n  Detection using Attention-based, Named Entity Recognition, and Ensemble\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting which parts of a sentence contribute to that sentence's toxicity --\nrather than providing a sentence-level verdict of hatefulness -- would increase\nthe interpretability of models and allow human moderators to better understand\nthe outputs of the system. This paper presents our team's, UTNLP, methodology\nand results in the SemEval-2021 shared task 5 on toxic spans detection. We test\nmultiple models and contextual embeddings and report the best setting out of\nall. The experiments start with keyword-based models and are followed by\nattention-based, named entity-based, transformers-based, and ensemble models.\nOur best approach, an ensemble model, achieves an F1 of 0.684 in the\ncompetition's evaluation phase.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:56:03 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Salemi", "Alireza", ""], ["Sabri", "Nazanin", ""], ["Kebriaei", "Emad", ""], ["Bahrak", "Behnam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "2104.04805", "submitter": "Kuan-Yu Chen", "authors": "Fu-Hao Yu and Kuan-Yu Chen", "title": "Non-autoregressive Transformer-based End-to-end ASR using BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models have led to a significant innovation in various\nclassic and practical subjects, including speech processing, natural language\nprocessing, and computer vision. On top of the transformer, the attention-based\nend-to-end automatic speech recognition (ASR) models have become a popular\nfashion in recent years. Specifically, the non-autoregressive modeling, which\ncan achieve fast inference speed and comparable performance when compared to\nconventional autoregressive methods, is an emergent research topic. In the\ncontext of natural language processing, the bidirectional encoder\nrepresentations from transformers (BERT) model has received widespread\nattention, partially due to its ability to infer contextualized word\nrepresentations and to obtain superior performances of downstream tasks by\nperforming only simple fine-tuning. In order to not only inherit the advantages\nof non-autoregressive ASR modeling, but also receive benefits from a\npre-trained language model (e.g., BERT), a non-autoregressive transformer-based\nend-to-end ASR model based on BERT is presented in this paper. A series of\nexperiments conducted on the AISHELL-1 dataset demonstrates competitive or\nsuperior results of the proposed model when compared to state-of-the-art ASR\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 16:22:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yu", "Fu-Hao", ""], ["Chen", "Kuan-Yu", ""]]}, {"id": "2104.04828", "submitter": "Radu Tudor Ionescu", "authors": "Radu Tudor Ionescu, Adrian Gabriel Chifu", "title": "FreSaDa: A French Satire Data Set for Cross-Domain Satire Detection", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce FreSaDa, a French Satire Data Set, which is\ncomposed of 11,570 articles from the news domain. In order to avoid reporting\nunreasonably high accuracy rates due to the learning of characteristics\nspecific to publication sources, we divided our samples into training,\nvalidation and test, such that the training publication sources are distinct\nfrom the validation and test publication sources. This gives rise to a\ncross-domain (cross-source) satire detection task. We employ two classification\nmethods as baselines for our new data set, one based on low-level features\n(character n-grams) and one based on high-level features (average of CamemBERT\nword embeddings). As an additional contribution, we present an unsupervised\ndomain adaptation method based on regarding the pairwise similarities (given by\nthe dot product) between the training samples and the validation samples as\nfeatures. By including these domain-specific features, we attain significant\nimprovements for both character n-grams and CamemBERT embeddings.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 18:21:53 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 07:28:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ionescu", "Radu Tudor", ""], ["Chifu", "Adrian Gabriel", ""]]}, {"id": "2104.04830", "submitter": "Aidin Zehtab-Salmasi", "authors": "Aidin Zehtab-Salmasi, Mohammad-Reza Feizi-Derakhshi, Mohamad-Ali\n  Balafar", "title": "FRAKE: Fusional Real-time Automatic Keyword Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword extraction is called identifying words or phrases that express the\nmain concepts of texts in best. There is a huge amount of texts that are\ncreated every day and at all times through electronic infrastructure. So, it is\npractically impossible for humans to study and manage this volume of documents.\nHowever, the need for efficient and effective access to these documents is\nevident in various purposes. Weblogs, News, and technical notes are almost long\ntexts, while the reader seeks to understand the concepts by topics or keywords\nto decide for reading the full text. To this aim, we use a combined approach\nthat consists of two models of graph centrality features and textural features.\nIn the following, graph centralities, such as degree, betweenness, eigenvector,\nand closeness centrality, have been used to optimally combine them to extract\nthe best keyword among the candidate keywords extracted by the proposed method.\nAlso, another approach has been introduced to distinguishing keywords among\ncandidate phrases and considering them as a separate keyword. To evaluate the\nproposed method, seven datasets named, Semeval2010, SemEval2017, Inspec, fao30,\nThesis100, pak2018 and WikiNews have been used, and results reported Precision,\nRecall, and F- measure.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 18:30:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zehtab-Salmasi", "Aidin", ""], ["Feizi-Derakhshi", "Mohammad-Reza", ""], ["Balafar", "Mohamad-Ali", ""]]}, {"id": "2104.04840", "submitter": "Alexander Jones", "authors": "Alex Jones, Derry Tanti Wijaya", "title": "Sentiment-based Candidate Selection for NMT", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The explosion of user-generated content (UGC)--e.g. social media posts,\ncomments, and reviews--has motivated the development of NLP applications\ntailored to these types of informal texts. Prevalent among these applications\nhave been sentiment analysis and machine translation (MT). Grounded in the\nobservation that UGC features highly idiomatic, sentiment-charged language, we\npropose a decoder-side approach that incorporates automatic sentiment scoring\ninto the MT candidate selection process. We train separate English and Spanish\nsentiment classifiers, then, using n-best candidates generated by a baseline MT\nmodel with beam search, select the candidate that minimizes the absolute\ndifference between the sentiment score of the source sentence and that of the\ntranslation, and perform a human evaluation to assess the produced\ntranslations. Unlike previous work, we select this minimally divergent\ntranslation by considering the sentiment scores of the source sentence and\ntranslation on a continuous interval, rather than using e.g. binary\nclassification, allowing for more fine-grained selection of translation\ncandidates. The results of human evaluations show that, in comparison to the\nopen-source MT baseline model on top of which our sentiment-based pipeline is\nbuilt, our pipeline produces more accurate translations of colloquial,\nsentiment-heavy source texts.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 19:01:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jones", "Alex", ""], ["Wijaya", "Derry Tanti", ""]]}, {"id": "2104.04844", "submitter": "Manuel Tomas Carrasco Benitez Mr.", "authors": "Manuel Tomas Carrasco Benitez", "title": "On migration to Perpetual Enterprise System", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This document describes a pragmatic approach on how to migrate an\norganisation computer system towards a new system that could evolve forever,\naddresses the whole organisation and it is integrated.\n  Governance aspects are as important, if not more, than purely technical IT\naspects: human resources, call for tenders, and similar. Migration implies that\none is not starting from a green field.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 19:20:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Benitez", "Manuel Tomas Carrasco", ""]]}, {"id": "2104.04871", "submitter": "Nikhil Oswal", "authors": "Nikhil Oswal", "title": "Identifying and Categorizing Offensive Language in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Offensive language is pervasive in social media. Individuals frequently take\nadvantage of the perceived anonymity of computer-mediated communication, using\nthis to engage in behavior that many of them would not consider in real life.\nThe automatic identification of offensive content online is an important task\nthat has gained more attention in recent years. This task can be modeled as a\nsupervised classification problem in which systems are trained using a dataset\ncontaining posts that are annotated with respect to the presence of some\nform(s) of abusive or offensive content. The objective of this study is to\nprovide a description of a classification system built for SemEval-2019 Task 6:\nOffensEval. This system classifies a tweet as either offensive or not offensive\n(Sub-task A) and further classifies offensive tweets into categories (Sub-tasks\nB \\& C). We trained machine learning and deep learning models along with data\npreprocessing and sampling techniques to come up with the best results. Models\ndiscussed include Naive Bayes, SVM, Logistic Regression, Random Forest and\nLSTM.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 22:53:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Oswal", "Nikhil", ""]]}, {"id": "2104.04886", "submitter": "Simiao Zuo", "authors": "Simiao Zuo, Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He,\n  Jianfeng Gao, Weizhu Chen, Tuo Zhao", "title": "Adversarial Training as Stackelberg Game: An Unrolled Optimization\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been shown to improve the generalization performance\nof deep learning models in various natural language processing tasks. Existing\nworks usually formulate adversarial training as a zero-sum game, which is\nsolved by alternating gradient descent/ascent algorithms. Such a formulation\ntreats the adversarial and the defending players equally, which is undesirable\nbecause only the defending player contributes to the generalization\nperformance. To address this issue, we propose Stackelberg Adversarial Training\n(SALT), which formulates adversarial training as a Stackelberg game. This\nformulation induces a competition between a leader and a follower, where the\nfollower generates perturbations, and the leader trains the model subject to\nthe perturbations. Different from conventional adversarial training, in SALT,\nthe leader is in an advantageous position. When the leader moves, it recognizes\nthe strategy of the follower and takes the anticipated follower's outcomes into\nconsideration. Such a leader's advantage enables us to improve the model\nfitting to the unperturbed data. The leader's strategic information is captured\nby the Stackelberg gradient, which is obtained using an unrolling algorithm.\nOur experimental results on a set of machine translation and natural language\nunderstanding tasks show that SALT outperforms existing adversarial training\nbaselines across all tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 00:44:57 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zuo", "Simiao", ""], ["Liang", "Chen", ""], ["Jiang", "Haoming", ""], ["Liu", "Xiaodong", ""], ["He", "Pengcheng", ""], ["Gao", "Jianfeng", ""], ["Chen", "Weizhu", ""], ["Zhao", "Tuo", ""]]}, {"id": "2104.04896", "submitter": "Boris Ginsburg", "authors": "Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg", "title": "NeMo Toolbox for Speech Dataset Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new toolbox for constructing speech datasets\nfrom long audio recording and raw reference texts. We develop tools for each\nstep of the speech dataset construction pipeline including data preprocessing,\naudio-text alignment, data post-processing and filtering. The proposed pipeline\nalso supports human-in-the-loop to address text-audio mismatch issues and\nremove samples that don't satisfy the quality requirements. We demonstrated the\ntoolbox efficiency by building the Russian LibriSpeech corpus (RuLS) from\nLibriVox audiobooks. The toolbox is opne sourced in NeMo framework. The RuLS\ncorpus is released in OpenSLR.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 01:57:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bakhturina", "Evelina", ""], ["Lavrukhin", "Vitaly", ""], ["Ginsburg", "Boris", ""]]}, {"id": "2104.04907", "submitter": "Ningyu Zhang", "authors": "Xiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang,\n  Huajun Chen", "title": "Disentangled Contrastive Learning for Learning Robust Textual\n  Representations", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the self-supervised pre-training of transformer models has resulted\nin the revolutionizing of natural language processing (NLP) applications and\nthe achievement of state-of-the-art results with regard to various benchmarks,\nthis process is still vulnerable to small and imperceptible permutations\noriginating from legitimate inputs. Intuitively, the representations should be\nsimilar in the feature space with subtle input permutations, while large\nvariations occur with different meanings. This motivates us to investigate the\nlearning of robust textual representation in a contrastive manner. However, it\nis non-trivial to obtain opposing semantic instances for textual samples. In\nthis study, we propose a disentangled contrastive learning method that\nseparately optimizes the uniformity and alignment of representations without\nnegative sampling. Specifically, we introduce the concept of momentum\nrepresentation consistency to align features and leverage power normalization\nwhile conforming the uniformity. Our experimental results for the NLP\nbenchmarks demonstrate that our approach can obtain better results compared\nwith the baselines, as well as achieve promising improvements with invariance\ntests and adversarial attacks. The code is available in\nhttps://github.com/zjunlp/DCL.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 03:32:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xiang", ""], ["Xie", "Xin", ""], ["Bi", "Zhen", ""], ["Ye", "Hongbin", ""], ["Deng", "Shumin", ""], ["Zhang", "Ningyu", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.04909", "submitter": "Saed Rezayi", "authors": "Saed Rezayi, Handong Zhao, Sungchul Kim, Ryan A. Rossi, Nedim Lipka,\n  Sheng Li", "title": "Edge: Enriching Knowledge Graph Embeddings with External Text", "comments": "Accepted in NAACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge graphs suffer from sparsity which degrades the quality of\nrepresentations generated by various methods. While there is an abundance of\ntextual information throughout the web and many existing knowledge bases,\naligning information across these diverse data sources remains a challenge in\nthe literature. Previous work has partially addressed this issue by enriching\nknowledge graph entities based on \"hard\" co-occurrence of words present in the\nentities of the knowledge graphs and external text, while we achieve \"soft\"\naugmentation by proposing a knowledge graph enrichment and embedding framework\nnamed Edge. Given an original knowledge graph, we first generate a rich but\nnoisy augmented graph using external texts in semantic and structural level. To\ndistill the relevant knowledge and suppress the introduced noise, we design a\ngraph alignment term in a shared embedding space between the original graph and\naugmented graph. To enhance the embedding learning on the augmented graph, we\nfurther regularize the locality relationship of target entity based on negative\nsampling. Experimental results on four benchmark datasets demonstrate the\nrobustness and effectiveness of Edge in link prediction and node\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 03:47:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Rezayi", "Saed", ""], ["Zhao", "Handong", ""], ["Kim", "Sungchul", ""], ["Rossi", "Ryan A.", ""], ["Lipka", "Nedim", ""], ["Li", "Sheng", ""]]}, {"id": "2104.04916", "submitter": "Xutan Peng", "authors": "Xutan Peng, Chenghua Lin, Mark Stevenson", "title": "Cross-Lingual Word Embedding Refinement by $\\ell_{1}$ Norm Optimisation", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages\nin a shared high-dimensional space in which vectors representing words with\nsimilar meaning (regardless of language) are closely located. Existing methods\nfor building high-quality CLWEs learn mappings that minimise the $\\ell_{2}$\nnorm loss function. However, this optimisation objective has been demonstrated\nto be sensitive to outliers. Based on the more robust Manhattan norm (aka.\n$\\ell_{1}$ norm) goodness-of-fit criterion, this paper proposes a simple\npost-processing step to improve CLWEs. An advantage of this approach is that it\nis fully agnostic to the training process of the original CLWEs and can\ntherefore be applied widely. Extensive experiments are performed involving ten\ndiverse languages and embeddings trained on different corpora. Evaluation\nresults based on bilingual lexicon induction and cross-lingual transfer for\nnatural language inference tasks show that the $\\ell_{1}$ refinement\nsubstantially outperforms four state-of-the-art baselines in both supervised\nand unsupervised settings. It is therefore recommended that this strategy be\nadopted as a standard for CLWE methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 04:37:54 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Peng", "Xutan", ""], ["Lin", "Chenghua", ""], ["Stevenson", "Mark", ""]]}, {"id": "2104.04923", "submitter": "Arun Babu", "authors": "Arun Babu, Akshat Shrivastava, Armen Aghajanyan, Ahmed Aly, Angela Fan\n  and Marjan Ghazvininejad", "title": "Non-Autoregressive Semantic Parsing for Compositional Task-Oriented\n  Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing using sequence-to-sequence models allows parsing of deeper\nrepresentations compared to traditional word tagging based models. In spite of\nthese advantages, widespread adoption of these models for real-time\nconversational use cases has been stymied by higher compute requirements and\nthus higher latency. In this work, we propose a non-autoregressive approach to\npredict semantic parse trees with an efficient seq2seq model architecture. By\ncombining non-autoregressive prediction with convolutional neural networks, we\nachieve significant latency gains and parameter size reduction compared to\ntraditional RNN models. Our novel architecture achieves up to an 81% reduction\nin latency on TOP dataset and retains competitive performance to non-pretrained\nmodels on three different semantic parsing datasets. Our code is available at\nhttps://github.com/facebookresearch/pytext\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 05:44:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Babu", "Arun", ""], ["Shrivastava", "Akshat", ""], ["Aghajanyan", "Armen", ""], ["Aly", "Ahmed", ""], ["Fan", "Angela", ""], ["Ghazvininejad", "Marjan", ""]]}, {"id": "2104.04946", "submitter": "Zhen Wu", "authors": "Zhen Wu, Lijun Wu, Qi Meng, Yingce Xia, Shufang Xie, Tao Qin, Xinyu\n  Dai and Tie-Yan Liu", "title": "UniDrop: A Simple yet Effective Technique to Improve Transformer without\n  Extra Cost", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer architecture achieves great success in abundant natural language\nprocessing tasks. The over-parameterization of the Transformer model has\nmotivated plenty of works to alleviate its overfitting for superior\nperformances. With some explorations, we find simple techniques such as\ndropout, can greatly boost model performance with a careful design. Therefore,\nin this paper, we integrate different dropout techniques into the training of\nTransformer models. Specifically, we propose an approach named UniDrop to\nunites three different dropout techniques from fine-grain to coarse-grain,\ni.e., feature dropout, structure dropout, and data dropout. Theoretically, we\ndemonstrate that these three dropouts play different roles from regularization\nperspectives. Empirically, we conduct experiments on both neural machine\ntranslation and text classification benchmark datasets. Extensive results\nindicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement\non IWSLT14 translation tasks, and better accuracy for the classification even\nusing strong pre-trained RoBERTa as backbone.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:43:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wu", "Zhen", ""], ["Wu", "Lijun", ""], ["Meng", "Qi", ""], ["Xia", "Yingce", ""], ["Xie", "Shufang", ""], ["Qin", "Tao", ""], ["Dai", "Xinyu", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2104.04947", "submitter": "Kun Xu", "authors": "Kun Xu, Han Wu, Linfeng Song, Haisong Zhang, Linqi Song, Dong Yu", "title": "Conversational Semantic Role Labeling", "comments": "Accepted by TASLP. arXiv admin note: text overlap with\n  arXiv:2010.01417", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic role labeling (SRL) aims to extract the arguments for each predicate\nin an input sentence. Traditional SRL can fail to analyze dialogues because it\nonly works on every single sentence, while ellipsis and anaphora frequently\noccur in dialogues. To address this problem, we propose the conversational SRL\ntask, where an argument can be the dialogue participants, a phrase in the\ndialogue history or the current sentence. As the existing SRL datasets are in\nthe sentence level, we manually annotate semantic roles for 3,000 chit-chat\ndialogues (27,198 sentences) to boost the research in this direction.\nExperiments show that while traditional SRL systems (even with the help of\ncoreference resolution or rewriting) perform poorly for analyzing dialogues,\nmodeling dialogue histories and participants greatly helps the performance,\nindicating that adapting SRL to conversations is very promising for universal\ndialogue understanding. Our initial study by applying CSRL to two mainstream\nconversational tasks, dialogue response generation and dialogue context\nrewriting, also confirms the usefulness of CSRL.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:45:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Kun", ""], ["Wu", "Han", ""], ["Song", "Linfeng", ""], ["Zhang", "Haisong", ""], ["Song", "Linqi", ""], ["Yu", "Dong", ""]]}, {"id": "2104.04950", "submitter": "Shih-Hsuan Chiu", "authors": "Shih-Hsuan Chiu and Berlin Chen", "title": "Innovative Bert-based Reranking Language Models for Speech Recognition", "comments": "6 pages, 3 figures, Published in IEEE SLT 2021", "journal-ref": null, "doi": "10.1109/SLT48900.2021.9383557", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More recently, Bidirectional Encoder Representations from Transformers (BERT)\nwas proposed and has achieved impressive success on many natural language\nprocessing (NLP) tasks such as question answering and language understanding,\ndue mainly to its effective pre-training then fine-tuning paradigm as well as\nstrong local contextual modeling ability. In view of the above, this paper\npresents a novel instantiation of the BERT-based contextualized language models\n(LMs) for use in reranking of N-best hypotheses produced by automatic speech\nrecognition (ASR). To this end, we frame N-best hypothesis reranking with BERT\nas a prediction problem, which aims to predict the oracle hypothesis that has\nthe lowest word error rate (WER) given the N-best hypotheses (denoted by\nPBERT). In particular, we also explore to capitalize on task-specific global\ntopic information in an unsupervised manner to assist PBERT in N-best\nhypothesis reranking (denoted by TPBERT). Extensive experiments conducted on\nthe AMI benchmark corpus demonstrate the effectiveness and feasibility of our\nmethods in comparison to the conventional autoregressive models like the\nrecurrent neural network (RNN) and a recently proposed method that employed\nBERT to compute pseudo-log-likelihood (PLL) scores for N-best hypothesis\nreranking.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:55:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chiu", "Shih-Hsuan", ""], ["Chen", "Berlin", ""]]}, {"id": "2104.04986", "submitter": "Junqi Dai", "authors": "Junqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, Xipeng Qiu", "title": "Does syntax matter? A strong baseline for Aspect-based Sentiment\n  Analysis with RoBERTa", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities\nfor aspects, is a fine-grained task in the field of sentiment analysis.\nPrevious work showed syntactic information, e.g. dependency trees, can\neffectively improve the ABSA performance. Recently, pre-trained models (PTMs)\nalso have shown their effectiveness on ABSA. Therefore, the question naturally\narises whether PTMs contain sufficient syntactic information for ABSA so that\nwe can obtain a good ABSA model only based on PTMs. In this paper, we firstly\ncompare the induced trees from PTMs and the dependency parsing trees on several\npopular models for the ABSA task, showing that the induced tree from fine-tuned\nRoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis\nexperiments reveal that the FT-RoBERTa Induced Tree is more\nsentiment-word-oriented and could benefit the ABSA task. The experiments also\nshow that the pure RoBERTa-based model can outperform or approximate to the\nprevious SOTA performances on six datasets across four languages since it\nimplicitly incorporates the task-oriented syntactic information.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 10:45:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dai", "Junqi", ""], ["Yan", "Hang", ""], ["Sun", "Tianxiang", ""], ["Liu", "Pengfei", ""], ["Qiu", "Xipeng", ""]]}, {"id": "2104.04989", "submitter": "Jeremy Barnes", "authors": "Jeremy Barnes and Petter M{\\ae}hlum and Samia Touileb", "title": "NorDial: A Preliminary Corpus of Written Norwegian Dialect Use", "comments": "Accepted to NoDaLiDa 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Norway has a large amount of dialectal variation, as well as a general\ntolerance to its use in the public sphere. There are, however, few available\nresources to study this variation and its change over time and in more informal\nareas, \\eg on social media. In this paper, we propose a first step to creating\na corpus of dialectal variation of written Norwegian. We collect a small corpus\nof tweets and manually annotate them as Bokm{\\aa}l, Nynorsk, any dialect, or a\nmix. We further perform preliminary experiments with state-of-the-art models,\nas well as an analysis of the data to expand this corpus in the future.\nFinally, we make the annotations and models available for future work.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 10:56:53 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Barnes", "Jeremy", ""], ["M\u00e6hlum", "Petter", ""], ["Touileb", "Samia", ""]]}, {"id": "2104.04998", "submitter": "Ayush Maheshwari", "authors": "Atul Sahay, Ayush Maheshwari, Ritesh Kumar, Ganesh Ramakrishnan,\n  Manjesh Kumar Hanawal, Kavi Arya", "title": "Unsupervised Learning of Explainable Parse Trees for Improved\n  Generalisation", "comments": "8 Pages, 5 Tables, 4 Figures. To appear at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural networks (RvNN) have been shown useful for learning sentence\nrepresentations and helped achieve competitive performance on several natural\nlanguage inference tasks. However, recent RvNN-based models fail to learn\nsimple grammar and meaningful semantics in their intermediate tree\nrepresentation. In this work, we propose an attention mechanism over Tree-LSTMs\nto learn more meaningful and explainable parse tree structures. We also\ndemonstrate the superior performance of our proposed model on natural language\ninference, semantic relatedness, and sentiment analysis tasks and compare them\nwith other state-of-the-art RvNN based methods. Further, we present a detailed\nqualitative and quantitative analysis of the learned parse trees and show that\nthe discovered linguistic structures are more explainable, semantically\nmeaningful, and grammatically correct than recent approaches. The source code\nof the paper is available at\nhttps://github.com/atul04/Explainable-Latent-Structures-Using-Attention.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 12:10:03 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sahay", "Atul", ""], ["Maheshwari", "Ayush", ""], ["Kumar", "Ritesh", ""], ["Ramakrishnan", "Ganesh", ""], ["Hanawal", "Manjesh Kumar", ""], ["Arya", "Kavi", ""]]}, {"id": "2104.05010", "submitter": "Jian Zhu", "authors": "Jian Zhu and David Jurgens", "title": "The structure of online social networks modulates the rate of lexical\n  change", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New words are regularly introduced to communities, yet not all of these words\npersist in a community's lexicon. Among the many factors contributing to\nlexical change, we focus on the understudied effect of social networks. We\nconduct a large-scale analysis of over 80k neologisms in 4420 online\ncommunities across a decade. Using Poisson regression and survival analysis,\nour study demonstrates that the community's network structure plays a\nsignificant role in lexical change. Apart from overall size, properties\nincluding dense connections, the lack of local clusters and more external\ncontacts promote lexical innovation and retention. Unlike offline communities,\nthese topic-based communities do not experience strong lexical levelling\ndespite increased contact but accommodate more niche words. Our work provides\nsupport for the sociolinguistic hypothesis that lexical change is partially\nshaped by the structure of the underlying network but also uncovers findings\nspecific to online communities.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 13:06:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhu", "Jian", ""], ["Jurgens", "David", ""]]}, {"id": "2104.05022", "submitter": "Alon Eirew", "authors": "Alon Eirew, Arie Cattan, Ido Dagan", "title": "WEC: Deriving a Large-scale Cross-document Event Coreference dataset\n  from Wikipedia", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-document event coreference resolution is a foundational task for NLP\napplications involving multi-text processing. However, existing corpora for\nthis task are scarce and relatively small, while annotating only modest-size\nclusters of documents belonging to the same topic. To complement these\nresources and enhance future research, we present Wikipedia Event Coreference\n(WEC), an efficient methodology for gathering a large-scale dataset for\ncross-document event coreference from Wikipedia, where coreference links are\nnot restricted within predefined topics. We apply this methodology to the\nEnglish Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our\ndataset creation method is generic and can be applied with relatively little\neffort to other Wikipedia languages. To set baseline results, we develop an\nalgorithm that adapts components of state-of-the-art models for within-document\ncoreference resolution to the cross-document setting. Our model is suitably\nefficient and outperforms previously published state-of-the-art results for the\ntask.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 14:54:35 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 07:47:58 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Eirew", "Alon", ""], ["Cattan", "Arie", ""], ["Dagan", "Ido", ""]]}, {"id": "2104.05055", "submitter": "Yang Zhang", "authors": "Yang Zhang, Evelina Bakhturina, Kyle Gorman, Boris Ginsburg", "title": "NeMo Inverse Text Normalization: From Development To Production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inverse text normalization (ITN) converts spoken-domain automatic speech\nrecognition (ASR) output into written-domain text to improve the readability of\nthe ASR output. Many state-of-the-art ITN systems use hand-written weighted\nfinite-state transducer(WFST) grammars since this task has extremely low\ntolerance to unrecoverable errors. We introduce an open-source Python\nWFST-based library for ITN which enables a seamless path from development to\nproduction. We describe the specification of ITN grammar rules for English, but\nthe library can be adapted for other languages. It can also be used for\nwritten-to-spoken text normalization. We evaluate the NeMo ITN library using a\nmodified version of the Google Text normalization dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 17:09:49 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 16:55:21 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Yang", ""], ["Bakhturina", "Evelina", ""], ["Gorman", "Kyle", ""], ["Ginsburg", "Boris", ""]]}, {"id": "2104.05062", "submitter": "Maor Ivgi", "authors": "Maor Ivgi and Jonathan Berant", "title": "Achieving Model Robustness through Discrete Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete adversarial attacks are symbolic perturbations to a language input\nthat preserve the output label but lead to a prediction error. While such\nattacks have been extensively explored for the purpose of evaluating model\nrobustness, their utility for improving robustness has been limited to offline\naugmentation only, i.e., given a trained model, attacks are used to generate\nperturbed (adversarial) examples, and the model is re-trained exactly once. In\nthis work, we address this gap and leverage discrete attacks for online\naugmentation, where adversarial examples are generated at every step, adapting\nto the changing nature of the model. We also consider efficient attacks based\non random sampling, that unlike prior work are not based on expensive\nsearch-based procedures. As a second contribution, we provide a general\nformulation for multiple search-based attacks from past work, and propose a new\nattack based on best-first search. Surprisingly, we find that random sampling\nleads to impressive gains in robustness, outperforming the commonly-used\noffline augmentation, while leading to a speedup at training time of ~10x.\nFurthermore, online augmentation with search-based attacks justifies the higher\ntraining cost, significantly improving robustness on three datasets. Last, we\nshow that our proposed algorithm substantially improves robustness compared to\nprior methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 17:49:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ivgi", "Maor", ""], ["Berant", "Jonathan", ""]]}, {"id": "2104.05064", "submitter": "Aaron Mueller", "authors": "Aaron Mueller, Mark Dredze", "title": "Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual\n  Neural Topic Modeling", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural topic models can augment or replace bag-of-words inputs with the\nlearned representations of deep pre-trained transformer-based word prediction\nmodels. One added benefit when using representations from multilingual models\nis that they facilitate zero-shot polylingual topic modeling. However, while it\nhas been widely observed that pre-trained embeddings should be fine-tuned to a\ngiven task, it is not immediately clear what supervision should look like for\nan unsupervised task such as topic modeling. Thus, we propose several methods\nfor fine-tuning encoders to improve both monolingual and zero-shot polylingual\nneural topic modeling. We consider fine-tuning on auxiliary tasks, constructing\na new topic classification task, integrating the topic classification objective\ndirectly into topic model training, and continued pre-training. We find that\nfine-tuning encoder representations on topic classification and integrating the\ntopic classification task directly into topic modeling improves topic quality,\nand that fine-tuning encoder representations on any task is the most important\nfactor for facilitating cross-lingual transfer.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 18:03:57 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mueller", "Aaron", ""], ["Dredze", "Mark", ""]]}, {"id": "2104.05094", "submitter": "Yangkai Du", "authors": "Yangkai Du, Tengfei Ma, Lingfei Wu, Fangli Xu, Xuhong Zhang, Shouling\n  Ji", "title": "Constructing Contrastive samples via Summarization for Text\n  Classification with limited annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive Learning has emerged as a powerful representation learning method\nand facilitates various downstream tasks especially when supervised data is\nlimited. How to construct efficient contrastive samples through data\naugmentation is key to its success. Unlike vision tasks, the data augmentation\nmethod for contrastive learning has not been investigated sufficiently in\nlanguage tasks. In this paper, we propose a novel approach to constructing\ncontrastive samples for language tasks using text summarization. We use these\nsamples for supervised contrastive learning to gain better text representations\nwhich greatly benefit text classification tasks with limited annotations. To\nfurther improve the method, we mix up samples from different classes and add an\nextra regularization, named mix-sum regularization, in addition to the\ncross-entropy-loss. Experiments on real-world text classification datasets\n(Amazon-5, Yelp-5, AG News) demonstrate the effectiveness of the proposed\ncontrastive learning framework with summarization-based data augmentation and\nmix-sum regularization.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:13:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Du", "Yangkai", ""], ["Ma", "Tengfei", ""], ["Wu", "Lingfei", ""], ["Xu", "Fangli", ""], ["Zhang", "Xuhong", ""], ["Ji", "Shouling", ""]]}, {"id": "2104.05115", "submitter": "James Y. Huang", "authors": "James Y. Huang, Kuan-Hao Huang, Kai-Wei Chang", "title": "Disentangling Semantics and Syntax in Sentence Embeddings with\n  Pre-trained Language Models", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models have achieved huge success on a wide range of NLP\ntasks. However, contextual representations from pre-trained models contain\nentangled semantic and syntactic information, and therefore cannot be directly\nused to derive useful semantic sentence embeddings for some tasks. Paraphrase\npairs offer an effective way of learning the distinction between semantics and\nsyntax, as they naturally share semantics and often vary in syntax. In this\nwork, we present ParaBART, a semantic sentence embedding model that learns to\ndisentangle semantics and syntax in sentence embeddings obtained by pre-trained\nlanguage models. ParaBART is trained to perform syntax-guided paraphrasing,\nbased on a source sentence that shares semantics with the target paraphrase,\nand a parse tree that specifies the target syntax. In this way, ParaBART learns\ndisentangled semantic and syntactic representations from their respective\ninputs with separate encoders. Experiments in English show that ParaBART\noutperforms state-of-the-art sentence embedding models on unsupervised semantic\nsimilarity tasks. Additionally, we show that our approach can effectively\nremove syntactic information from semantic sentence embeddings, leading to\nbetter robustness against syntactic variation on downstream semantic tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 21:34:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Huang", "James Y.", ""], ["Huang", "Kuan-Hao", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2104.05146", "submitter": "Sweta Agrawal", "authors": "Sweta Agrawal, George Foster, Markus Freitag, Colin Cherry", "title": "Assessing Reference-Free Peer Evaluation for Machine Translation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reference-free evaluation has the potential to make machine translation\nevaluation substantially more scalable, allowing us to pivot easily to new\nlanguages or domains. It has been recently shown that the probabilities given\nby a large, multilingual model can achieve state of the art results when used\nas a reference-free metric. We experiment with various modifications to this\nmodel and demonstrate that by scaling it up we can match the performance of\nBLEU. We analyze various potential weaknesses of the approach and find that it\nis surprisingly robust and likely to offer reasonable performance across a\nbroad spectrum of domains and different system qualities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 00:49:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Agrawal", "Sweta", ""], ["Foster", "George", ""], ["Freitag", "Markus", ""], ["Cherry", "Colin", ""]]}, {"id": "2104.05156", "submitter": "Oleg Vasilyev", "authors": "Oleg Vasilyev, John Bohannon", "title": "Estimation of Summary-to-Text Inconsistency by Mismatched Embeddings", "comments": "6 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new reference-free summary quality evaluation measure, with\nemphasis on the faithfulness. The measure is designed to find and count all\npossible minute inconsistencies of the summary with respect to the source\ndocument. The proposed ESTIME, Estimator of Summary-to-Text Inconsistency by\nMismatched Embeddings, correlates with expert scores in summary-level SummEval\ndataset stronger than other common evaluation measures not only in Consistency\nbut also in Fluency. We also introduce a method of generating subtle factual\nerrors in human summaries. We show that ESTIME is more sensitive to subtle\nerrors than other common evaluation measures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 01:58:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Vasilyev", "Oleg", ""], ["Bohannon", "John", ""]]}, {"id": "2104.05196", "submitter": "Paul Pu Liang", "authors": "Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnab\\'as P\\'oczos,\n  Ruslan Salakhutdinov, Louis-Philippe Morency", "title": "StylePTB: A Compositional Benchmark for Fine-grained Controllable Text\n  Style Transfer", "comments": "NAACL 2021, code available at https://github.com/lvyiwei1/StylePTB/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text style transfer aims to controllably generate text with targeted\nstylistic changes while maintaining core meaning from the source sentence\nconstant. Many of the existing style transfer benchmarks primarily focus on\nindividual high-level semantic changes (e.g. positive to negative), which\nenable controllability at a high level but do not offer fine-grained control\ninvolving sentence structure, emphasis, and content of the sentence. In this\npaper, we introduce a large-scale benchmark, StylePTB, with (1) paired\nsentences undergoing 21 fine-grained stylistic changes spanning atomic lexical,\nsyntactic, semantic, and thematic transfers of text, as well as (2)\ncompositions of multiple transfers which allow modeling of fine-grained\nstylistic changes as building blocks for more complex, high-level transfers. By\nbenchmarking existing methods on StylePTB, we find that they struggle to model\nfine-grained changes and have an even more difficult time composing multiple\nstyles. As a result, StylePTB brings novel challenges that we hope will\nencourage future research in controllable text style transfer, compositional\nmodels, and learning disentangled representations. Solving these challenges\nwould present important steps towards controllable text generation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:25:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lyu", "Yiwei", ""], ["Liang", "Paul Pu", ""], ["Pham", "Hai", ""], ["Hovy", "Eduard", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2104.05216", "submitter": "Yang Deng", "authors": "Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Wai Lam, Ying Shen", "title": "Contextualized Knowledge-aware Attentive Neural Network: Enhancing\n  Answer Selection with Knowledge", "comments": "Accepted by TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer selection, which is involved in many natural language processing\napplications such as dialog systems and question answering (QA), is an\nimportant yet challenging task in practice, since conventional methods\ntypically suffer from the issues of ignoring diverse real-world background\nknowledge. In this paper, we extensively investigate approaches to enhancing\nthe answer selection model with external knowledge from knowledge graph (KG).\nFirst, we present a context-knowledge interaction learning framework,\nKnowledge-aware Neural Network (KNN), which learns the QA sentence\nrepresentations by considering a tight interaction with the external knowledge\nfrom KG and the textual information. Then, we develop two kinds of\nknowledge-aware attention mechanism to summarize both the context-based and\nknowledge-based interactions between questions and answers. To handle the\ndiversity and complexity of KG information, we further propose a Contextualized\nKnowledge-aware Attentive Neural Network (CKANN), which improves the knowledge\nrepresentation learning with structure information via a customized Graph\nConvolutional Network (GCN) and comprehensively learns context-based and\nknowledge-based sentence representation via the multi-view knowledge-aware\nattention mechanism. We evaluate our method on four widely-used benchmark QA\ndatasets, including WikiQA, TREC QA, InsuranceQA and Yahoo QA. Results verify\nthe benefits of incorporating external knowledge from KG, and show the robust\nsuperiority and extensive applicability of our method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 05:52:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Yang", ""], ["Xie", "Yuexiang", ""], ["Li", "Yaliang", ""], ["Yang", "Min", ""], ["Lam", "Wai", ""], ["Shen", "Ying", ""]]}, {"id": "2104.05218", "submitter": "Kevin Yang", "authors": "Kevin Yang and Dan Klein", "title": "FUDGE: Controlled Text Generation With Future Discriminators", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG's output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor's outputs to adjust G's original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks -- couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation -- and observe\ngains in all three tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 05:59:53 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yang", "Kevin", ""], ["Klein", "Dan", ""]]}, {"id": "2104.05220", "submitter": "Zhongfen Deng", "authors": "Zhongfen Deng, Hao Peng, Dongxiao He, Jianxin Li, Philip S. Yu", "title": "HTCInfoMax: A Global Model for Hierarchical Text Classification via\n  Information Maximization", "comments": "Accepted by NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art model HiAGM for hierarchical text classification\nhas two limitations. First, it correlates each text sample with all labels in\nthe dataset which contains irrelevant information. Second, it does not consider\nany statistical constraint on the label representations learned by the\nstructure encoder, while constraints for representation learning are proved to\nbe helpful in previous work. In this paper, we propose HTCInfoMax to address\nthese issues by introducing information maximization which includes two\nmodules: text-label mutual information maximization and label prior matching.\nThe first module can model the interaction between each text sample and its\nground truth labels explicitly which filters out irrelevant information. The\nsecond one encourages the structure encoder to learn better representations\nwith desired characteristics for all labels which can better handle label\nimbalance in hierarchical text classification. Experimental results on two\nbenchmark datasets demonstrate the effectiveness of the proposed HTCInfoMax.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 06:04:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Zhongfen", ""], ["Peng", "Hao", ""], ["He", "Dongxiao", ""], ["Li", "Jianxin", ""], ["Yu", "Philip S.", ""]]}, {"id": "2104.05224", "submitter": "Jakob Nyberg", "authors": "Jakob Nyberg, Ramesh Manuvinakurike, Maike Paetzel-Pr\\\"usmann", "title": "Estimating Subjective Crowd-Evaluations as an Additional Objective to\n  Improve Natural Language Generation", "comments": "To appear at Workshop on Human Evaluation of NLP Systems (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human ratings are one of the most prevalent methods to evaluate the\nperformance of natural language processing algorithms. Similarly, it is common\nto measure the quality of sentences generated by a natural language generation\nmodel using human raters. In this paper, we argue for exploring the use of\nsubjective evaluations within the process of training language generation\nmodels in a multi-task learning setting. As a case study, we use a\ncrowd-authored dialogue corpus to fine-tune six different language generation\nmodels. Two of these models incorporate multi-task learning and use subjective\nratings of lines as part of an explicit learning goal. A human evaluation of\nthe generated dialogue lines reveals that utterances generated by the\nmulti-tasking models were subjectively rated as the most typical, most moving\nthe conversation forward, and least offensive. Based on these promising first\nresults, we discuss future research directions for incorporating subjective\nhuman evaluations into language model training and to hence keep the human user\nin the loop during the development process.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 06:33:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nyberg", "Jakob", ""], ["Manuvinakurike", "Ramesh", ""], ["Paetzel-Pr\u00fcsmann", "Maike", ""]]}, {"id": "2104.05228", "submitter": "Simon Hengchen", "authors": "Simon Hengchen and Nina Tahmasebi", "title": "SuperSim: a test set for word similarity and relatedness in Swedish", "comments": "Accepted at NoDaLiDa 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language models are notoriously difficult to evaluate. We release SuperSim, a\nlarge-scale similarity and relatedness test set for Swedish built with expert\nhuman judgments. The test set is composed of 1,360 word-pairs independently\njudged for both relatedness and similarity by five annotators. We evaluate\nthree different models (Word2Vec, fastText, and GloVe) trained on two separate\nSwedish datasets, namely the Swedish Gigaword corpus and a Swedish Wikipedia\ndump, to provide a baseline for future comparison. We release the fully\nannotated test set, code, baseline models, and data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 06:47:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hengchen", "Simon", ""], ["Tahmasebi", "Nina", ""]]}, {"id": "2104.05232", "submitter": "Chong Zhang", "authors": "Chong Zhang, Jieyu Zhao, Huan Zhang, Kai-Wei Chang, Cho-Jui Hsieh", "title": "Double Perturbation: On the Robustness of Robustness and Counterfactual\n  Bias Evaluation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness and counterfactual bias are usually evaluated on a test dataset.\nHowever, are these evaluations robust? If the test dataset is perturbed\nslightly, will the evaluation results keep the same? In this paper, we propose\na \"double perturbation\" framework to uncover model weaknesses beyond the test\ndataset. The framework first perturbs the test dataset to construct abundant\nnatural sentences similar to the test data, and then diagnoses the prediction\nchange regarding a single-word substitution. We apply this framework to study\ntwo perturbation-based approaches that are used to analyze models' robustness\nand counterfactual bias in English. (1) For robustness, we focus on synonym\nsubstitutions and identify vulnerable examples where prediction can be altered.\nOur proposed attack attains high success rates (96.0%-99.8%) in finding\nvulnerable examples on both original and robustly trained CNNs and\nTransformers. (2) For counterfactual bias, we focus on substituting demographic\ntokens (e.g., gender, race) and measure the shift of the expected prediction\namong constructed sentences. Our method is able to reveal the hidden model\nbiases not directly shown in the test dataset. Our code is available at\nhttps://github.com/chong-z/nlp-second-order-attack.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 06:57:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Chong", ""], ["Zhao", "Jieyu", ""], ["Zhang", "Huan", ""], ["Chang", "Kai-Wei", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2104.05240", "submitter": "Zexuan Zhong", "authors": "Zexuan Zhong, Dan Friedman, Danqi Chen", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "comments": "NAACL 2021. The code is publicly available at\n  https://github.com/princeton-nlp/OptiPrompt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Petroni et al. (2019) demonstrated that it is possible to retrieve world\nfacts from a pre-trained language model by expressing them as cloze-style\nprompts and interpret the model's prediction accuracy as a lower bound on the\namount of factual information it encodes. Subsequent work has attempted to\ntighten the estimate by searching for better prompts, using a disjoint set of\nfacts as training data. In this work, we make two complementary contributions\nto better understand these factual probing techniques. First, we propose\nOptiPrompt, a novel and efficient method which directly optimizes in continuous\nembedding space. We find this simple method is able to predict an additional\n6.4% of facts in the LAMA benchmark. Second, we raise a more important\nquestion: Can we really interpret these probing results as a lower bound? Is it\npossible that these prompt-search methods learn from the training data too? We\nfind, somewhat surprisingly, that the training data used by these methods\ncontains certain regularities of the underlying fact distribution, and all the\nexisting prompt methods, including ours, are able to exploit them for better\nfact prediction. We conduct a set of control experiments to disentangle\n\"learning\" from \"learning to recall\", providing a more detailed picture of what\ndifferent prompts can reveal about pre-trained language models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:11:40 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhong", "Zexuan", ""], ["Friedman", "Dan", ""], ["Chen", "Danqi", ""]]}, {"id": "2104.05243", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Belinda Z. Li, Sinong Wang, Pascale Fung, Hao Ma, Wen-tau\n  Yih, Madian Khabsa", "title": "On Unifying Misinformation Detection", "comments": "Accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce UnifiedM2, a general-purpose misinformation model\nthat jointly models multiple domains of misinformation with a single, unified\nsetup. The model is trained to handle four tasks: detecting news bias,\nclickbait, fake news, and verifying rumors. By grouping these tasks together,\nUnifiedM2learns a richer representation of misinformation, which leads to\nstate-of-the-art or comparable performance across all tasks. Furthermore, we\ndemonstrate that UnifiedM2's learned representation is helpful for few-shot\nlearning of unseen misinformation tasks/datasets and model's generalizability\nto unseen events.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:25:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lee", "Nayeon", ""], ["Li", "Belinda Z.", ""], ["Wang", "Sinong", ""], ["Fung", "Pascale", ""], ["Ma", "Hao", ""], ["Yih", "Wen-tau", ""], ["Khabsa", "Madian", ""]]}, {"id": "2104.05274", "submitter": "Yuxin Liang", "authors": "Yuxin Liang, Rui Cao, Jie Zheng, Jie Ren, Ling Gao", "title": "Learning to Remove: Towards Isotropic Pre-trained BERT Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pre-trained language models such as BERT have become a more common choice of\nnatural language processing (NLP) tasks. Research in word representation shows\nthat isotropic embeddings can significantly improve performance on downstream\ntasks. However, we measure and analyze the geometry of pre-trained BERT\nembedding and find that it is far from isotropic. We find that the word vectors\nare not centered around the origin, and the average cosine similarity between\ntwo random words is much higher than zero, which indicates that the word\nvectors are distributed in a narrow cone and deteriorate the representation\ncapacity of word embedding. We propose a simple, and yet effective method to\nfix this problem: remove several dominant directions of BERT embedding with a\nset of learnable weights. We train the weights on word similarity tasks and\nshow that processed embedding is more isotropic. Our method is evaluated on\nthree standardized tasks: word similarity, word analogy, and semantic textual\nsimilarity. In all tasks, the word embedding processed by our method\nconsistently outperforms the original embedding (with average improvement of\n13% on word analogy and 16% on semantic textual similarity) and two baseline\nmethods. Our method is also proven to be more robust to changes of\nhyperparameter.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:13:59 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liang", "Yuxin", ""], ["Cao", "Rui", ""], ["Zheng", "Jie", ""], ["Ren", "Jie", ""], ["Gao", "Ling", ""]]}, {"id": "2104.05277", "submitter": "Tobias Norlund", "authors": "Tobias Norlund and Agnes Stenbom", "title": "Building a Swedish Open-Domain Conversational Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present on-going work of evaluating the, to our knowledge, first large\ngenerative language model trained to converse in Swedish, using data from the\nonline discussion forum Flashback. We conduct a human evaluation pilot study\nthat indicates the model is often able to respond to conversations in both a\nhuman-like and informative manner, on a diverse set of topics. While data from\nonline forums can be useful to build conversational systems, we reflect on the\nnegative consequences that incautious application might have, and the need for\ntaking active measures to safeguard against them.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:18:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Norlund", "Tobias", ""], ["Stenbom", "Agnes", ""]]}, {"id": "2104.05305", "submitter": "Jordan Ivanchev", "authors": "Corvin Deboeser, Jordan Ivanchev, Thomas Braud, Alois Knoll, David\n  Eckhoff, Alberto Sangiovanni-Vincentelli", "title": "A Hierarchical State-Machine-Based Framework for Platoon Manoeuvre\n  Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces the SEAD framework that simplifies the process of\ndesigning and describing autonomous vehicle platooning manoeuvres. Although a\nlarge body of research has been formulating platooning manoeuvres, it is still\nchallenging to design, describe, read, and understand them. This difficulty\nlargely arises from missing formalisation. To fill this gap, we analysed\nexisting ways of describing manoeuvres, derived the causes of difficulty, and\ndesigned a framework that simplifies the manoeuvre design process. Alongside, a\nManoeuvre Design Language was developed to structurally describe manoeuvres in\na machine-readable format. Unlike state-of-the-art manoeuvre descriptions that\nrequire one state machine for every participating vehicle, the SEAD framework\nallows describing any manoeuvre from the single perspective of the platoon\nleader. %As a proof of concept, the proposed framework was implemented in the\nmixed traffic simulation environment BEHAVE for an autonomous highway scenario.\nUsing this framework, we implemented several manoeuvres as they were described\nin literature. To demonstrate the applicability of the framework, an experiment\nwas performed to evaluate the execution time performance of multiple\nalternatives of the Join-Middle manoeuvre. This proof-of-concept experiment\nrevealed that the manoeuvre execution time can be reduced by 28 \\% through\nparallelising various steps without considerable secondary effects. We hope\nthat the SEAD framework will pave the way for further research in the area of\nnew manoeuvre design and optimisation by largely simplifying and unifying\nplatooning manoeuvre representation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:25:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deboeser", "Corvin", ""], ["Ivanchev", "Jordan", ""], ["Braud", "Thomas", ""], ["Knoll", "Alois", ""], ["Eckhoff", "David", ""], ["Sangiovanni-Vincentelli", "Alberto", ""]]}, {"id": "2104.05316", "submitter": "Lu Xu", "authors": "Lu Xu, Zhanming Jie, Wei Lu and Lidong Bing", "title": "Better Feature Integration for Named Entity Recognition", "comments": "Accepted by NAACL 2021. 13 pages, 9 Figure, 10 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It has been shown that named entity recognition (NER) could benefit from\nincorporating the long-distance structured information captured by dependency\ntrees. We believe this is because both types of features - the contextual\ninformation captured by the linear sequences and the structured information\ncaptured by the dependency trees may complement each other. However, existing\napproaches largely focused on stacking the LSTM and graph neural networks such\nas graph convolutional networks (GCNs) for building improved NER models, where\nthe exact interaction mechanism between the two types of features is not very\nclear, and the performance gain does not appear to be significant. In this\nwork, we propose a simple and robust solution to incorporate both types of\nfeatures with our Synergized-LSTM (Syn-LSTM), which clearly captures how the\ntwo types of features interact. We conduct extensive experiments on several\nstandard datasets across four languages. The results demonstrate that the\nproposed model achieves better performance than previous approaches while\nrequiring fewer parameters. Our further analysis demonstrates that our model\ncan capture longer dependencies compared with strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:55:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Lu", ""], ["Jie", "Zhanming", ""], ["Lu", "Wei", ""], ["Bing", "Lidong", ""]]}, {"id": "2104.05320", "submitter": "Juntao Yu", "authors": "Juntao Yu, Nafise Sadat Moosavi, Silviu Paun, Massimo Poesio", "title": "Stay Together: A System for Single and Split-antecedent Anaphora\n  Resolution", "comments": "accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art on basic, single-antecedent anaphora has greatly\nimproved in recent years. Researchers have therefore started to pay more\nattention to more complex cases of anaphora such as split-antecedent anaphora,\nas in Time-Warner is considering a legal challenge to Telecommunications Inc's\nplan to buy half of Showtime Networks Inc-a move that could lead to all-out war\nbetween the two powerful companies. Split-antecedent anaphora is rarer and more\ncomplex to resolve than single-antecedent anaphora; as a result, it is not\nannotated in many datasets designed to test coreference, and previous work on\nresolving this type of anaphora was carried out in unrealistic conditions that\nassume gold mentions and/or gold split-antecedent anaphors are available. These\nsystems also focus on split-antecedent anaphors only. In this work, we\nintroduce a system that resolves both single and split-antecedent anaphors, and\nevaluate it in a more realistic setting that uses predicted mentions. We also\nstart addressing the question of how to evaluate single and split-antecedent\nanaphors together using standard coreference evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:01:08 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yu", "Juntao", ""], ["Moosavi", "Nafise Sadat", ""], ["Paun", "Silviu", ""], ["Poesio", "Massimo", ""]]}, {"id": "2104.05321", "submitter": "Rachit Bansal", "authors": "Rachit Bansal, William Scott Paka, Nidhi, Shubhashis Sengupta, Tanmoy\n  Chakraborty", "title": "Combining exogenous and endogenous signals with a semi-supervised\n  co-attention network for early detection of COVID-19 fake tweets", "comments": "Pacific-Asia Conference on Knowledge Discovery and Data Mining\n  (PAKDD) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake tweets are observed to be ever-increasing, demanding immediate\ncountermeasures to combat their spread. During COVID-19, tweets with\nmisinformation should be flagged and neutralized in their early stages to\nmitigate the damages. Most of the existing methods for early detection of fake\nnews assume to have enough propagation information for large labeled tweets --\nwhich may not be an ideal setting for cases like COVID-19 where both aspects\nare largely absent. In this work, we present ENDEMIC, a novel early detection\nmodel which leverages exogenous and endogenous signals related to tweets, while\nlearning on limited labeled data. We first develop a novel dataset, called CTF\nfor early COVID-19 Twitter fake news, with additional behavioral test sets to\nvalidate early detection. We build a heterogeneous graph with\nfollower-followee, user-tweet, and tweet-retweet connections and train a graph\nembedding model to aggregate propagation information. Graph embeddings and\ncontextual features constitute endogenous, while time-relative web-scraped\ninformation constitutes exogenous signals. ENDEMIC is trained in a\nsemi-supervised fashion, overcoming the challenge of limited labeled data. We\npropose a co-attention mechanism to fuse signal representations optimally.\nExperimental results on ECTF, PolitiFact, and GossipCop show that ENDEMIC is\nhighly reliable in detecting early fake tweets, outperforming nine\nstate-of-the-art methods significantly.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:01:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bansal", "Rachit", ""], ["Paka", "William Scott", ""], ["Nidhi", "", ""], ["Sengupta", "Shubhashis", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2104.05336", "submitter": "R\\'emi Leblond", "authors": "R\\'emi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar,\n  Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan and Oriol Vinyals", "title": "Machine Translation Decoding beyond Beam Search", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beam search is the go-to method for decoding auto-regressive machine\ntranslation models. While it yields consistent improvements in terms of BLEU,\nit is only concerned with finding outputs with high model likelihood, and is\nthus agnostic to whatever end metric or score practitioners care about. Our aim\nis to establish whether beam search can be replaced by a more powerful\nmetric-driven search technique. To this end, we explore numerous decoding\nalgorithms, including some which rely on a value function parameterised by a\nneural network, and report results on a variety of metrics. Notably, we\nintroduce a Monte-Carlo Tree Search (MCTS) based method and showcase its\ncompetitiveness. We provide a blueprint for how to use MCTS fruitfully in\nlanguage applications, which opens promising future directions. We find that\nwhich algorithm is best heavily depends on the characteristics of the goal\nmetric; we believe that our extensive experiments and analysis will inform\nfurther research in this area.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:28:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Leblond", "R\u00e9mi", ""], ["Alayrac", "Jean-Baptiste", ""], ["Sifre", "Laurent", ""], ["Pislar", "Miruna", ""], ["Lespiau", "Jean-Baptiste", ""], ["Antonoglou", "Ioannis", ""], ["Simonyan", "Karen", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2104.05361", "submitter": "Mika H\\\"am\\\"al\\\"ainen", "authors": "Mika H\\\"am\\\"al\\\"ainen and Khalid Alnajjar", "title": "The Great Misalignment Problem in Human Evaluation of NLP Methods", "comments": "Workshop on Human Evaluation of NLP Systems at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We outline the Great Misalignment Problem in natural language processing\nresearch, this means simply that the problem definition is not in line with the\nmethod proposed and the human evaluation is not in line with the definition nor\nthe method. We study this misalignment problem by surveying 10 randomly sampled\npapers published in ACL 2020 that report results with human evaluation. Our\nresults show that only one paper was fully in line in terms of problem\ndefinition, method and evaluation. Only two papers presented a human evaluation\nthat was in line with what was modeled in the method. These results highlight\nthat the Great Misalignment Problem is a major one and it affects the validity\nand reproducibility of results obtained by a human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:26:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["H\u00e4m\u00e4l\u00e4inen", "Mika", ""], ["Alnajjar", "Khalid", ""]]}, {"id": "2104.05379", "submitter": "Nick Rossenbach", "authors": "Nick Rossenbach, Mohammad Zeineldeen, Benedikt Hilmes, Ralf\n  Schl\\\"uter, Hermann Ney", "title": "Comparing the Benefit of Synthetic Training Data for Various Automatic\n  Speech Recognition Architectures", "comments": "Submitted to ASRU 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent publications on automatic-speech-recognition (ASR) have a strong focus\non attention encoder-decoder (AED) architectures which tend to suffer from\nover-fitting in low resource scenarios. One solution to tackle this issue is to\ngenerate synthetic data with a trained text-to-speech system (TTS) if\nadditional text is available. This was successfully applied in many\npublications with AED systems, but only very limited in the context of other\nASR architectures. We investigate the effect of varying pre-processing, the\nspeaker embedding and input encoding of the TTS system w.r.t. the effectiveness\nof the synthesized data for AED-ASR training. Additionally, we also consider\ninternal language model subtraction for the first time, resulting in up to 38%\nrelative improvement. We compare the AED results to a state-of-the-art hybrid\nASR system, a monophone based system using\nconnectionist-temporal-classification (CTC) and a monotonic transducer based\nsystem. We show that for the later systems the addition of synthetic data has\nno relevant effect, but they still outperform the AED systems on\nLibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a\nhybrid system on the clean/noisy test-sets, surpassing any previous\nstate-of-the-art systems on Librispeech-100h that do not include unlabeled\naudio data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:59:23 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 21:37:06 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 15:02:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Rossenbach", "Nick", ""], ["Zeineldeen", "Mohammad", ""], ["Hilmes", "Benedikt", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.05409", "submitter": "Chengzhi Zhang", "authors": "Qingqing Zhou and Chengzhi Zhang", "title": "Breaking Community Boundary: Comparing Academic and Social Communication\n  Preferences regarding Global Pandemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The global spread of COVID-19 has caused pandemics to be widely discussed.\nThis is evident in the large number of scientific articles and the amount of\nuser-generated content on social media. This paper aims to compare academic\ncommunication and social communication about the pandemic from the perspective\nof communication preference differences. It aims to provide information for the\nongoing research on global pandemics, thereby eliminating knowledge barriers\nand information inequalities between the academic and the social communities.\nFirst, we collected the full text and the metadata of pandemic-related articles\nand Twitter data mentioning the articles. Second, we extracted and analyzed the\ntopics and sentiment tendencies of the articles and related tweets. Finally, we\nconducted pandemic-related differential analysis on the academic community and\nthe social community. We mined the resulting data to generate pandemic\ncommunication preferences (e.g., information needs, attitude tendencies) of\nresearchers and the public, respectively. The research results from 50,338\narticles and 927,266 corresponding tweets mentioning the articles revealed\ncommunication differences about global pandemics between the academic and the\nsocial communities regarding the consistency of research recognition and the\npreferences for particular research topics. The analysis of large-scale\npandemic-related tweets also confirmed the communication preference differences\nbetween the two communities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:44:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Qingqing", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2104.05433", "submitter": "Nora Hollenstein", "authors": "Nora Hollenstein, Federico Pirovano, Ce Zhang, Lena J\\\"ager and Lisa\n  Beinborn", "title": "Multilingual Language Models Predict Human Reading Behavior", "comments": "accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze if large language models are able to predict patterns of human\nreading behavior. We compare the performance of language-specific and\nmultilingual pretrained transformer models to predict reading time measures\nreflecting natural human sentence processing on Dutch, English, German, and\nRussian texts. This results in accurate models of human reading behavior, which\nindicates that transformer models implicitly encode relative importance in\nlanguage in a way that is comparable to human processing mechanisms. We find\nthat BERT and XLM models successfully predict a range of eye tracking features.\nIn a series of experiments, we analyze the cross-domain and cross-language\nabilities of these models and show how they reflect human sentence processing.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:03:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hollenstein", "Nora", ""], ["Pirovano", "Federico", ""], ["Zhang", "Ce", ""], ["J\u00e4ger", "Lena", ""], ["Beinborn", "Lisa", ""]]}, {"id": "2104.05459", "submitter": "Yelena Mejova", "authors": "Fabio Poletto, Yunbai Zhang, Andre Panisson, Yelena Mejova, Daniela\n  Paolotti, Sylvain Ponserre", "title": "Developing Annotated Resources for Internal Displacement Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes in details the design and development of a novel\nannotation framework and of annotated resources for Internal Displacement, as\nthe outcome of a collaboration with the Internal Displacement Monitoring\nCentre, aimed at improving the accuracy of their monitoring platform IDETECT.\nThe schema includes multi-faceted description of the events, including cause,\nquantity of people displaced, location and date. Higher-order facets aimed at\nimproving the information extraction, such as document relevance and type, are\nproposed. We also report a case study of machine learning application to the\ndocument classification tasks. Finally, we discuss the importance of\nstandardized schema in dataset benchmark development and its impact on the\ndevelopment of reliable disaster monitoring infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:30:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Poletto", "Fabio", ""], ["Zhang", "Yunbai", ""], ["Panisson", "Andre", ""], ["Mejova", "Yelena", ""], ["Paolotti", "Daniela", ""], ["Ponserre", "Sylvain", ""]]}, {"id": "2104.05488", "submitter": "Kamini Sabu", "authors": "Kamini Sabu, Mithilesh Vaidya, Preeti Rao", "title": "Deep Learning for Prominence Detection in Children's Read Speech", "comments": "5 pages, 2 figures, 6 tables, Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Expressive reading, considered the defining attribute of oral reading\nfluency, comprises the prosodic realization of phrasing and prominence. In the\ncontext of evaluating oral reading, it helps to establish the speaker's\ncomprehension of the text. We consider a labeled dataset of children's reading\nrecordings for the speaker-independent detection of prominent words using\nacoustic-prosodic and lexico-syntactic features. A previous well-tuned random\nforest ensemble predictor is replaced by an RNN sequence classifier to exploit\npotential context dependency across the longer utterance. Further, deep\nlearning is applied to obtain word-level features from low-level acoustic\ncontours of fundamental frequency, intensity and spectral shape in an\nend-to-end fashion. Performance comparisons are presented across the different\nfeature types and across different feature learning architectures for prominent\nword prediction to draw insights wherever possible.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:15:08 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 04:31:35 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sabu", "Kamini", ""], ["Vaidya", "Mithilesh", ""], ["Rao", "Preeti", ""]]}, {"id": "2104.05489", "submitter": "Yanzhe Zhang", "authors": "Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang and Diyi Yang", "title": "Continual Learning for Text Classification with Information\n  Disentanglement Based Regularization", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning has become increasingly important as it enables NLP models\nto constantly learn and gain knowledge over time. Previous continual learning\nmethods are mainly designed to preserve knowledge from previous tasks, without\nmuch emphasis on how to well generalize models to new tasks. In this work, we\npropose an information disentanglement based regularization method for\ncontinual learning on text classification. Our proposed method first\ndisentangles text hidden spaces into representations that are generic to all\ntasks and representations specific to each individual task, and further\nregularizes these representations differently to better constrain the knowledge\nrequired to generalize. We also introduce two simple auxiliary tasks: next\nsentence prediction and task-id prediction, for learning better generic and\nspecific representation spaces. Experiments conducted on large-scale benchmarks\ndemonstrate the effectiveness of our method in continual text classification\ntasks with various sequences and lengths over state-of-the-art baselines. We\nhave publicly released our code at https://github.com/GT-SALT/IDBR.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:17:43 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 00:33:19 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Huang", "Yufan", ""], ["Zhang", "Yanzhe", ""], ["Chen", "Jiaao", ""], ["Wang", "Xuezhi", ""], ["Yang", "Diyi", ""]]}, {"id": "2104.05500", "submitter": "Arseny Moskvichev", "authors": "Arseny Moskvichev, James A. Liu", "title": "Updater-Extractor Architecture for Inductive World State Representations", "comments": "15 pages (12 main content, 3 references and appendix), 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing NLP models traditionally involves two stages - training and\napplication. Retention of information acquired after training (at application\ntime) is architecturally limited by the size of the model's context window (in\nthe case of transformers), or by the practical difficulties associated with\nlong sequences (in the case of RNNs). In this paper, we propose a novel\ntransformer-based Updater-Extractor architecture and a training procedure that\ncan work with sequences of arbitrary length and refine its knowledge about the\nworld based on linguistic inputs. We explicitly train the model to incorporate\nincoming information into its world state representation, obtaining strong\ninductive generalization and the ability to handle extremely long-range\ndependencies. We prove a lemma that provides a theoretical basis for our\napproach. The result also provides insight into success and failure modes of\nmodels trained with variants of Truncated Back-Propagation Through Time (such\nas Transformer XL). Empirically, we investigate the model performance on three\ndifferent tasks, demonstrating its promise. This preprint is still a work in\nprogress. At present, we focused on easily interpretable tasks, leaving the\napplication of the proposed ideas to practical NLP applications for the future.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:30:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Moskvichev", "Arseny", ""], ["Liu", "James A.", ""]]}, {"id": "2104.05501", "submitter": "Adam Poliak", "authors": "Max Fleming, Priyanka Dondeti, Caitlin N. Dreisbach, Adam Poliak", "title": "Fine-Tuning Transformers for Identifying Self-Reporting Potential Cases\n  and Symptoms of COVID-19 in Tweets", "comments": "Social Media Mining for Health Applications 2021 Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe our straight-forward approach for Tasks 5 and 6 of 2021 Social\nMedia Mining for Health Applications (SMM4H) shared tasks. Our system is based\non fine-tuning Distill- BERT on each task, as well as first fine-tuning the\nmodel on the other task. We explore how much fine-tuning is necessary for\naccurately classifying tweets as containing self-reported COVID-19 symptoms\n(Task 5) or whether a tweet related to COVID-19 is self-reporting, non-personal\nreporting, or a literature/news mention of the virus (Task 6).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:31:51 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Fleming", "Max", ""], ["Dondeti", "Priyanka", ""], ["Dreisbach", "Caitlin N.", ""], ["Poliak", "Adam", ""]]}, {"id": "2104.05504", "submitter": "Xiquan Cui", "authors": "Rebecca West, Khalifeh Al Jadda, Unaiza Ahsan, Huiming Qu, Xiquan Cui", "title": "Interpretable Methods for Identifying Product Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For e-commerce companies with large product selections, the organization and\ngrouping of products in meaningful ways is important for creating great\ncustomer shopping experiences and cultivating an authoritative brand image. One\nimportant way of grouping products is to identify a family of product variants,\nwhere the variants are mostly the same with slight and yet distinct differences\n(e.g. color or pack size). In this paper, we introduce a novel approach to\nidentifying product variants. It combines both constrained clustering and\ntailored NLP techniques (e.g. extraction of product family name from\nunstructured product title and identification of products with similar model\nnumbers) to achieve superior performance compared with an existing baseline\nusing a vanilla classification approach. In addition, we design the algorithm\nto meet certain business criteria, including meeting high accuracy requirements\non a wide range of categories (e.g. appliances, decor, tools, and building\nmaterials, etc.) as well as prioritizing the interpretability of the model to\nmake it accessible and understandable to all business partners.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:37:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["West", "Rebecca", ""], ["Jadda", "Khalifeh Al", ""], ["Ahsan", "Unaiza", ""], ["Qu", "Huiming", ""], ["Cui", "Xiquan", ""]]}, {"id": "2104.05507", "submitter": "Yun Zhao", "authors": "Yun Zhao, Xuerui Yang, Jinchao Wang, Yongyu Gao, Chao Yan, Yuanfu Zhou", "title": "BART based semantic correction for Mandarin automatic speech recognition\n  system", "comments": "submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although automatic speech recognition (ASR) systems achieved significantly\nimprovements in recent years, spoken language recognition error occurs which\ncan be easily spotted by human beings. Various language modeling techniques\nhave been developed on post recognition tasks like semantic correction. In this\npaper, we propose a Transformer based semantic correction method with\npretrained BART initialization, Experiments on 10000 hours Mandarin speech\ndataset show that character error rate (CER) can be effectively reduced by\n21.7% relatively compared to our baseline ASR system. Expert evaluation\ndemonstrates that actual improvement of our model surpasses what CER indicates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 06:41:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhao", "Yun", ""], ["Yang", "Xuerui", ""], ["Wang", "Jinchao", ""], ["Gao", "Yongyu", ""], ["Yan", "Chao", ""], ["Zhou", "Yuanfu", ""]]}, {"id": "2104.05514", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng and Ahmed\n  Hassan Awadallah", "title": "Self-Training with Weak Supervision", "comments": "Accepted to NAACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art deep neural networks require large-scale labeled training\ndata that is often expensive to obtain or not available for many tasks. Weak\nsupervision in the form of domain-specific rules has been shown to be useful in\nsuch settings to automatically generate weakly labeled training data. However,\nlearning with weak rules is challenging due to their inherent heuristic and\nnoisy nature. An additional challenge is rule coverage and overlap, where prior\nwork on weak supervision only considers instances that are covered by weak\nrules, thus leaving valuable unlabeled data behind.\n  In this work, we develop a weak supervision framework (ASTRA) that leverages\nall the available data for a given task. To this end, we leverage task-specific\nunlabeled data through self-training with a model (student) that considers\ncontextualized representations and predicts pseudo-labels for instances that\nmay not be covered by weak rules. We further develop a rule attention network\n(teacher) that learns how to aggregate student pseudo-labels with weak rule\nlabels, conditioned on their fidelity and the underlying context of an\ninstance. Finally, we construct a semi-supervised learning objective for\nend-to-end training with unlabeled data, domain-specific rules, and a small\namount of labeled data. Extensive experiments on six benchmark datasets for\ntext classification demonstrate the effectiveness of our approach with\nsignificant improvements over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:45:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Mukherjee", "Subhabrata", ""], ["Zheng", "Guoqing", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2104.05544", "submitter": "Mohammad Zeineldeen", "authors": "Mohammad Zeineldeen, Aleksandr Glushko, Wilfried Michel, Albert Zeyer,\n  Ralf Schl\\\"uter, Hermann Ney", "title": "Investigating Methods to Improve Language Model Integration for\n  Attention-based Encoder-Decoder ASR Models", "comments": "accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder-decoder (AED) models learn an implicit internal\nlanguage model (ILM) from the training transcriptions. The integration with an\nexternal LM trained on much more unpaired text usually leads to better\nperformance. A Bayesian interpretation as in the hybrid autoregressive\ntransducer (HAT) suggests dividing by the prior of the discriminative acoustic\nmodel, which corresponds to this implicit LM, similarly as in the hybrid hidden\nMarkov model approach. The implicit LM cannot be calculated efficiently in\ngeneral and it is yet unclear what are the best methods to estimate it. In this\nwork, we compare different approaches from the literature and propose several\nnovel methods to estimate the ILM directly from the AED model. Our proposed\nmethods outperform all previous approaches. We also investigate other methods\nto suppress the ILM mainly by decreasing the capacity of the AED model,\nlimiting the label context, and also by training the AED model together with a\npre-existing LM.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:16:03 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 07:27:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zeineldeen", "Mohammad", ""], ["Glushko", "Aleksandr", ""], ["Michel", "Wilfried", ""], ["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.05547", "submitter": "Nan Bai", "authors": "Nan Bai, Renqian Luo, Pirouz Nourian, Ana Pereira Roders", "title": "WHOSe Heritage: Classification of UNESCO World Heritage \"Outstanding\n  Universal Value\" Documents with Smoothed Labels", "comments": "15 pages, 4 figures, The data and models presented in this paper can\n  be found in the following GitHub link:\n  https://github.com/zzbn12345/WHOSe_Heritage or\n  http://doi.org/10.5281/zenodo.4680508", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UNESCO World Heritage List (WHL) is to identify the exceptionally\nvaluable cultural and natural heritage to be preserved for mankind as a whole.\nEvaluating and justifying the Outstanding Universal Value (OUV) of each\nnomination in WHL is essentially important for a property to be inscribed, and\nyet a complex task even for experts since the criteria are not mutually\nexclusive. Furthermore, manual annotation of heritage values, which is\ncurrently dominant in the field, is knowledge-demanding and time-consuming,\nimpeding systematic analysis of such authoritative documents in terms of their\nimplications on heritage management. This study applies state-of-the-art NLP\nmodels to build a classifier on a new real-world dataset containing official\nOUV justification statements, seeking an explainable, scalable, and less biased\nautomation tool to facilitate the nomination, evaluation, and monitoring\nprocesses of World Heritage properties. Label smoothing is innovatively adapted\nto transform the task smoothly between multi-class and multi-label\nclassification by adding prior inter-class relationship knowledge into the\nlabels, improving the performance of most baselines. The study shows that the\nbest models fine-tuned from BERT and ULMFiT can reach 94.3% top-3 accuracy,\nwhich is promising to be further developed and applied in heritage research and\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:18:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bai", "Nan", ""], ["Luo", "Renqian", ""], ["Nourian", "Pirouz", ""], ["Roders", "Ana Pereira", ""]]}, {"id": "2104.05565", "submitter": "Victor Uc-Cetina", "authors": "Victor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez,\n  Cornelius Weber, Stefan Wermter", "title": "Survey on reinforcement learning for language processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years some researchers have explored the use of reinforcement\nlearning (RL) algorithms as key components in the solution of various natural\nlanguage processing tasks. For instance, some of these algorithms leveraging\ndeep neural learning have found their way into conversational systems. This\npaper reviews the state of the art of RL methods for their possible use for\ndifferent problems of natural language processing, focusing primarily on\nconversational systems, mainly due to their growing relevance. We provide\ndetailed descriptions of the problems as well as discussions of why RL is\nwell-suited to solve them. Also, we analyze the advantages and limitations of\nthese methods. Finally, we elaborate on promising research directions in\nnatural language processing that might benefit from reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:33:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Uc-Cetina", "Victor", ""], ["Navarro-Guerrero", "Nicolas", ""], ["Martin-Gonzalez", "Anabel", ""], ["Weber", "Cornelius", ""], ["Wermter", "Stefan", ""]]}, {"id": "2104.05591", "submitter": "Andrei Manolache", "authors": "Andrei Manolache and Florin Brad and Elena Burceanu", "title": "DATE: Detecting Anomalies in Text via Self-Supervision of Transformers", "comments": "conference paper at NAACL-HLT 2021, 11 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging deep learning models for Anomaly Detection (AD) has seen\nwidespread use in recent years due to superior performances over traditional\nmethods. Recent deep methods for anomalies in images learn better features of\nnormality in an end-to-end self-supervised setting. These methods train a model\nto discriminate between different transformations applied to visual data and\nthen use the output to compute an anomaly score. We use this approach for AD in\ntext, by introducing a novel pretext task on text sequences. We learn our DATE\nmodel end-to-end, enforcing two independent and complementary self-supervision\nsignals, one at the token-level and one at the sequence-level. Under this new\ntask formulation, we show strong quantitative and qualitative results on the\n20Newsgroups and AG News datasets. In the semi-supervised setting, we\noutperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC).\nIn the unsupervised configuration, DATE surpasses all other methods even when\n10% of its training data is contaminated with outliers (compared with 0% for\nthe others).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:08:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Manolache", "Andrei", ""], ["Brad", "Florin", ""], ["Burceanu", "Elena", ""]]}, {"id": "2104.05596", "submitter": "Mitesh M. Khapra", "authors": "Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank\n  Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee,\n  Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Kumar Deepak,\n  Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh Shantadevi Khapra", "title": "Samanantar: The Largest Publicly Available Parallel Corpora Collection\n  for 11 Indic Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Samanantar, the largest publicly available parallel corpora\ncollection for Indic languages. The collection contains a total of 46.9 million\nsentence pairs between English and 11 Indic languages (from two language\nfamilies). In particular, we compile 12.4 million sentence pairs from existing,\npublicly-available parallel corpora, and we additionally mine 34.6 million\nsentence pairs from the web, resulting in a 2.8X increase in publicly available\nsentence pairs. We mine the parallel sentences from the web by combining many\ncorpora, tools, and methods. In particular, we use (a) web-crawled monolingual\ncorpora, (b) document OCR for extracting sentences from scanned documents (c)\nmultilingual representation models for aligning sentences, and (d) approximate\nnearest neighbor search for searching in a large collection of sentences. Human\nevaluation of samples from the newly mined corpora validate the high quality of\nthe parallel sentences across 11 language pairs. Further, we extracted 82.7\nmillion sentence pairs between all 55 Indic language pairs from the\nEnglish-centric parallel corpus using English as the pivot language. We trained\nmultilingual NMT models spanning all these languages on Samanantar and compared\nwith other baselines and previously reported results on publicly available\nbenchmarks. Our models outperform existing models on these benchmarks,\nestablishing the utility of Samanantar. Our data\n(https://indicnlp.ai4bharat.org/samanantar) and models\n(https://github.com/AI4Bharat/IndicTrans) will be available publicly and we\nhope they will help advance research in Indic NMT and multilingual NLP for\nIndic languages.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:18:20 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 16:24:26 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ramesh", "Gowtham", ""], ["Doddapaneni", "Sumanth", ""], ["Bheemaraj", "Aravinth", ""], ["Jobanputra", "Mayank", ""], ["AK", "Raghavan", ""], ["Sharma", "Ajitesh", ""], ["Sahoo", "Sujit", ""], ["Diddee", "Harshita", ""], ["J", "Mahalakshmi", ""], ["Kakwani", "Divyanshu", ""], ["Kumar", "Navneet", ""], ["Pradeep", "Aswin", ""], ["Deepak", "Kumar", ""], ["Raghavan", "Vivek", ""], ["Kunchukuttan", "Anoop", ""], ["Kumar", "Pratyush", ""], ["Khapra", "Mitesh Shantadevi", ""]]}, {"id": "2104.05604", "submitter": "Chieh-Yang Huang", "authors": "Chieh-Yang Huang and Ting-Hao 'Kenneth' Huang", "title": "Semantic Frame Forecast", "comments": "9 pages, NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces semantic frame forecast, a task that predicts the\nsemantic frames that will occur in the next 10, 100, or even 1,000 sentences in\na running story. Prior work focused on predicting the immediate future of a\nstory, such as one to a few sentences ahead. However, when novelists write long\nstories, generating a few sentences is not enough to help them gain high-level\ninsight to develop the follow-up story. In this paper, we formulate a long\nstory as a sequence of \"story blocks,\" where each block contains a fixed number\nof sentences (e.g., 10, 100, or 200). This formulation allows us to predict the\nfollow-up story arc beyond the scope of a few sentences. We represent a story\nblock using the term frequencies (TF) of semantic frames in it, normalized by\neach frame's inverse document frequency (IDF). We conduct semantic frame\nforecast experiments on 4,794 books from the Bookcorpus and 7,962 scientific\nabstracts from CODA-19, with block sizes ranging from 5 to 1,000 sentences. The\nresults show that automated models can forecast the follow-up story blocks\nbetter than the random, prior, and replay baselines, indicating the task's\nfeasibility. We also learn that the models using the frame representation as\nfeatures outperform all the existing approaches when the block size is over 150\nsentences. The human evaluation also shows that the proposed frame\nrepresentation, when visualized as word clouds, is comprehensible,\nrepresentative, and specific to humans. Our code is available at\nhttps://github.com/appleternity/FrameForecasting.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:23:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Huang", "Chieh-Yang", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "2104.05688", "submitter": "Vil\\'em Zouhar", "authors": "Vil\\'em Zouhar, Michal Nov\\'ak, Mat\\'u\\v{s} \\v{Z}ilinec, Ond\\v{r}ej\n  Bojar, Mateo Obreg\\'on, Robin L. Hill, Fr\\'ed\\'eric Blain, Marina Fomicheva,\n  Lucia Specia, Lisa Yankovskaya", "title": "Backtranslation Feedback Improves User Confidence in MT, Not Quality", "comments": "9 pages (excluding references); to appear at NAACL-HWT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Translating text into a language unknown to the text's author, dubbed\noutbound translation, is a modern need for which the user experience has\nsignificant room for improvement, beyond the basic machine translation\nfacility. We demonstrate this by showing three ways in which user confidence in\nthe outbound translation, as well as its overall final quality, can be\naffected: backward translation, quality estimation (with alignment) and source\nparaphrasing. In this paper, we describe an experiment on outbound translation\nfrom English to Czech and Estonian. We examine the effects of each proposed\nfeedback module and further focus on how the quality of machine translation\nsystems influence these findings and the user perception of success. We show\nthat backward translation feedback has a mixed effect on the whole process: it\nincreases user confidence in the produced translation, but not the objective\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:50:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zouhar", "Vil\u00e9m", ""], ["Nov\u00e1k", "Michal", ""], ["\u017dilinec", "Mat\u00fa\u0161", ""], ["Bojar", "Ond\u0159ej", ""], ["Obreg\u00f3n", "Mateo", ""], ["Hill", "Robin L.", ""], ["Blain", "Fr\u00e9d\u00e9ric", ""], ["Fomicheva", "Marina", ""], ["Specia", "Lucia", ""], ["Yankovskaya", "Lisa", ""]]}, {"id": "2104.05694", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang and Tatsunori Hashimoto", "title": "On the Inductive Bias of Masked Language Modeling: From Statistical to\n  Syntactic Dependencies", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how masking and predicting tokens in an unsupervised fashion can\ngive rise to linguistic structures and downstream performance gains. Recent\ntheories have suggested that pretrained language models acquire useful\ninductive biases through masks that implicitly act as cloze reductions for\ndownstream tasks. While appealing, we show that the success of the random\nmasking strategy used in practice cannot be explained by such cloze-like masks\nalone. We construct cloze-like masks using task-specific lexicons for three\ndifferent classification datasets and show that the majority of pretrained\nperformance gains come from generic masks that are not associated with the\nlexicon. To explain the empirical success of these generic masks, we\ndemonstrate a correspondence between the Masked Language Model (MLM) objective\nand existing methods for learning statistical dependencies in graphical models.\nUsing this, we derive a method for extracting these learned statistical\ndependencies in MLMs and show that these dependencies encode useful inductive\nbiases in the form of syntactic structures. In an unsupervised parsing\nevaluation, simply forming a minimum spanning tree on the implied statistical\ndependence structure outperforms a classic method for unsupervised parsing\n(58.74 vs. 55.91 UUAS).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:55:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Tianyi", ""], ["Hashimoto", "Tatsunori", ""]]}, {"id": "2104.05696", "submitter": "Elias Stengel-Eskin", "authors": "Elias Stengel-Eskin, Kenton Murray, Sheng Zhang, Aaron Steven White,\n  Benjamin Van Durme", "title": "Joint Universal Syntactic and Semantic Parsing", "comments": "To appear: TACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While numerous attempts have been made to jointly parse syntax and semantics,\nhigh performance in one domain typically comes at the price of performance in\nthe other. This trade-off contradicts the large body of research focusing on\nthe rich interactions at the syntax-semantics interface. We explore multiple\nmodel architectures which allow us to exploit the rich syntactic and semantic\nannotations contained in the Universal Decompositional Semantics (UDS) dataset,\njointly parsing Universal Dependencies and UDS to obtain state-of-the-art\nresults in both formalisms. We analyze the behaviour of a joint model of syntax\nand semantics, finding patterns supported by linguistic theory at the\nsyntax-semantics interface. We then investigate to what degree joint modeling\ngeneralizes to a multilingual setting, where we find similar trends across 8\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:56:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Stengel-Eskin", "Elias", ""], ["Murray", "Kenton", ""], ["Zhang", "Sheng", ""], ["White", "Aaron Steven", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2104.05700", "submitter": "Thamme Gowda", "authors": "Thamme Gowda, Weiqiu You, Constantine Lignos, Jonathan May", "title": "Macro-Average: Rare Types Are Important Too", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2021.naacl-main.90", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While traditional corpus-level evaluation metrics for machine translation\n(MT) correlate well with fluency, they struggle to reflect adequacy.\nModel-based MT metrics trained on segment-level human judgments have emerged as\nan attractive replacement due to strong correlation results. These models,\nhowever, require potentially expensive re-training for new domains and\nlanguages. Furthermore, their decisions are inherently non-transparent and\nappear to reflect unwelcome biases. We explore the simple type-based classifier\nmetric, MacroF1, and study its applicability to MT evaluation. We find that\nMacroF1 is competitive on direct assessment, and outperforms others in\nindicating downstream cross-lingual information retrieval task performance.\nFurther, we show that MacroF1 can be used to effectively compare supervised and\nunsupervised neural machine translation, and reveal significant qualitative\ndifferences in the methods' outputs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:57:42 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gowda", "Thamme", ""], ["You", "Weiqiu", ""], ["Lignos", "Constantine", ""], ["May", "Jonathan", ""]]}, {"id": "2104.05740", "submitter": "Xueguang Ma", "authors": "Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin", "title": "A Replication Study of Dense Passage Retriever", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text retrieval using learned dense representations has recently emerged as a\npromising alternative to \"traditional\" text retrieval using sparse bag-of-words\nrepresentations. One recent work that has garnered much attention is the dense\npassage retriever (DPR) technique proposed by Karpukhin et al. (2020) for\nend-to-end open-domain question answering. We present a replication study of\nthis work, starting with model checkpoints provided by the authors, but\notherwise from an independent implementation in our group's Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results\nlargely verify the claims of the original paper, we arrived at two important\nadditional findings that contribute to a better understanding of DPR: First, it\nappears that the original authors under-report the effectiveness of the BM25\nbaseline and hence also dense--sparse hybrid retrieval results. Second, by\nincorporating evidence from the retriever and an improved answer span scoring\ntechnique, we are able to improve end-to-end question answering effectiveness\nusing exactly the same models as in the original work.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:10:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ma", "Xueguang", ""], ["Sun", "Kai", ""], ["Pradeep", "Ronak", ""], ["Lin", "Jimmy", ""]]}, {"id": "2104.05741", "submitter": "Martha Dais Ferreira", "authors": "Martha Dais Ferreira, Michal Malyska, Nicola Sahar, Riccardo Miotto,\n  Fernando Paulovich, Evangelos Milios", "title": "Active learning for medical code assignment", "comments": "It was accepted in the ACM CHIL 2021 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is widely used to automatically extract meaningful\ninformation from Electronic Health Records (EHR) to support operational,\nclinical, and financial decision-making. However, ML models require a large\nnumber of annotated examples to provide satisfactory results, which is not\npossible in most healthcare scenarios due to the high cost of clinician-labeled\ndata. Active Learning (AL) is a process of selecting the most informative\ninstances to be labeled by an expert to further train a supervised algorithm.\nWe demonstrate the effectiveness of AL in multi-label text classification in\nthe clinical domain. In this context, we apply a set of well-known AL methods\nto help automatically assign ICD-9 codes on the MIMIC-III dataset. Our results\nshow that the selection of informative instances provides satisfactory\nclassification with a significantly reduced training set (8.3\\% of the total\ninstances). We conclude that AL methods can significantly reduce the manual\nannotation cost while preserving model performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:11:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ferreira", "Martha Dais", ""], ["Malyska", "Michal", ""], ["Sahar", "Nicola", ""], ["Miotto", "Riccardo", ""], ["Paulovich", "Fernando", ""], ["Milios", "Evangelos", ""]]}, {"id": "2104.05745", "submitter": "Konstantinos Kogkalidis", "authors": "Giorgos Tziafas, Konstantinos Kogkalidis, Tommaso Caselli", "title": "Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble", "comments": "4 pages, NLP4IF 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the TOKOFOU system, an ensemble model for misinformation\ndetection tasks based on six different transformer-based pre-trained encoders,\nimplemented in the context of the COVID-19 Infodemic Shared Task for English.\nWe fine tune each model on each of the task's questions and aggregate their\nprediction scores using a majority voting approach. TOKOFOU obtains an overall\nF1 score of 89.7%, ranking first.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:13:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tziafas", "Giorgos", ""], ["Kogkalidis", "Konstantinos", ""], ["Caselli", "Tommaso", ""]]}, {"id": "2104.05752", "submitter": "My Phung", "authors": "Sujeong Cha, Wangrui Hou, Hyun Jung, My Phung, Michael Picheny,\n  Hong-Kwang Kuo, Samuel Thomas, Edmilson Morais", "title": "Speak or Chat with Me: End-to-End Spoken Language Understanding System\n  with Flexible Inputs", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major focus of recent research in spoken language understanding (SLU) has\nbeen on the end-to-end approach where a single model can predict intents\ndirectly from speech inputs without intermediate transcripts. However, this\napproach presents some challenges. First, since speech can be considered as\npersonally identifiable information, in some cases only automatic speech\nrecognition (ASR) transcripts are accessible. Second, intent-labeled speech\ndata is scarce. To address the first challenge, we propose a novel system that\ncan predict intents from flexible types of inputs: speech, ASR transcripts, or\nboth. We demonstrate strong performance for either modality separately, and\nwhen both speech and ASR transcripts are available, through system combination,\nwe achieve better results than using a single input modality. To address the\nsecond challenge, we leverage a semantically robust pre-trained BERT model and\nadopt a cross-modal system that co-trains text embeddings and acoustic\nembeddings in a shared latent space. We further enhance this system by\nutilizing an acoustic module pre-trained on LibriSpeech and domain-adapting the\ntext module on our target datasets. Our experiments show significant advantages\nfor these pre-training and fine-tuning strategies, resulting in a system that\nachieves competitive intent-classification performance on Snips SLU and Fluent\nSpeech Commands datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 20:48:08 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 13:55:41 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cha", "Sujeong", ""], ["Hou", "Wangrui", ""], ["Jung", "Hyun", ""], ["Phung", "My", ""], ["Picheny", "Michael", ""], ["Kuo", "Hong-Kwang", ""], ["Thomas", "Samuel", ""], ["Morais", "Edmilson", ""]]}, {"id": "2104.05753", "submitter": "Felermino Ali", "authors": "Felermino D. M. A. Ali, Andrew Caines, Jaimito L. A. Malavi", "title": "Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa\n  of Mozambique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major advancement in the performance of machine translation models has been\nmade possible in part thanks to the availability of large-scale parallel\ncorpora. But for most languages in the world, the existence of such corpora is\nrare. Emakhuwa, a language spoken in Mozambique, is like most African languages\nlow-resource in NLP terms. It lacks both computational and linguistic resources\nand, to the best of our knowledge, few parallel corpora including Emakhuwa\nalready exist. In this paper we describe the creation of the\nEmakhuwa-Portuguese parallel corpus, which is a collection of texts from the\nJehovah's Witness website and a variety of other sources including the African\nStory Book website, the Universal Declaration of Human Rights and Mozambican\nlegal documents. The dataset contains 47,415 sentence pairs, amounting to\n699,976 word tokens of Emakhuwa and 877,595 word tokens in Portuguese. After\nnormalization processes which remain to be completed, the corpus will be made\nfreely available for research use.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:31:56 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ali", "Felermino D. M. A.", ""], ["Caines", "Andrew", ""], ["Malavi", "Jaimito L. A.", ""]]}, {"id": "2104.05763", "submitter": "Dian Yu", "authors": "Dian Yu and Luheng He and Yuan Zhang and Xinya Du and Panupong Pasupat\n  and Qi Li", "title": "Few-shot Intent Classification and Slot Filling with Retrieved Examples", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning arises in important practical scenarios, such as when a\nnatural language understanding system needs to learn new semantic labels for an\nemerging, resource-scarce domain. In this paper, we explore retrieval-based\nmethods for intent classification and slot filling tasks in few-shot settings.\nRetrieval-based methods make predictions based on labeled examples in the\nretrieval index that are similar to the input, and thus can adapt to new\ndomains simply by changing the index without having to retrain the model.\nHowever, it is non-trivial to apply such methods on tasks with a complex label\nspace like slot filling. To this end, we propose a span-level retrieval method\nthat learns similar contextualized representations for spans with the same\nlabel via a novel batch-softmax objective. At inference time, we use the labels\nof the retrieved spans to construct the final structure with the highest\naggregated score. Our method outperforms previous systems in various few-shot\nsettings on the CLINC and SNIPS benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:50:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yu", "Dian", ""], ["He", "Luheng", ""], ["Zhang", "Yuan", ""], ["Du", "Xinya", ""], ["Pasupat", "Panupong", ""], ["Li", "Qi", ""]]}, {"id": "2104.05767", "submitter": "Ashwin Devaraj", "authors": "Ashwin Devaraj, Iain J. Marshall, Byron C. Wallace, Junyi Jessy Li", "title": "Paragraph-level Simplification of Medical Texts", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to simplify medical texts. This is\nimportant because most reliable, up-to-date information in biomedicine is dense\nwith jargon and thus practically inaccessible to the lay audience. Furthermore,\nmanual simplification does not scale to the rapidly growing body of biomedical\nliterature, motivating the need for automated approaches. Unfortunately, there\nare no large-scale resources available for this task. In this work we introduce\na new corpus of parallel texts in English comprising technical and lay\nsummaries of all published evidence pertaining to different clinical topics. We\nthen propose a new metric based on likelihood scores from a masked language\nmodel pretrained on scientific texts. We show that this automated measure\nbetter differentiates between technical and lay summaries than existing\nheuristics. We introduce and evaluate baseline encoder-decoder Transformer\nmodels for simplification and propose a novel augmentation to these in which we\nexplicitly penalize the decoder for producing \"jargon\" terms; we find that this\nyields improvements over baselines in terms of readability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:56:05 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Devaraj", "Ashwin", ""], ["Marshall", "Iain J.", ""], ["Wallace", "Byron C.", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2104.05801", "submitter": "Sarik Ghazarian", "authors": "Sarik Ghazarian, Zixi Liu, Akash SM, Ralph Weischedel, Aram Galstyan,\n  Nanyun Peng", "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain\n  Story Generation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances of open-domain story generation, the lack of\nreliable automatic evaluation metrics becomes an increasingly imperative issue\nthat hinders the fast development of story generation. According to conducted\nresearches in this regard, learnable evaluation metrics have promised more\naccurate assessments by having higher correlations with human judgments. A\ncritical bottleneck of obtaining a reliable learnable evaluation metric is the\nlack of high-quality training data for classifiers to efficiently distinguish\nplausible and implausible machine-generated stories. Previous works relied on\n\\textit{heuristically manipulated} plausible examples to mimic possible system\ndrawbacks such as repetition, contradiction, or irrelevant content in the text\nlevel, which can be \\textit{unnatural} and \\textit{oversimplify} the\ncharacteristics of implausible machine-generated stories. We propose to tackle\nthese issues by generating a more comprehensive set of implausible stories\nusing {\\em plots}, which are structured representations of controllable factors\nused to generate stories. Since these plots are compact and structured, it is\neasier to manipulate them to generate text with targeted undesirable\nproperties, while at the same time maintain the grammatical correctness and\nnaturalness of the generated sentences. To improve the quality of generated\nimplausible stories, we further apply the adversarial filtering procedure\npresented by \\citet{zellers2018swag} to select a more nuanced set of\nimplausible texts. Experiments show that the evaluation metrics trained on our\ngenerated data result in more reliable automatic assessments that correlate\nremarkably better with human judgments compared to the baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 20:19:24 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 20:01:43 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Ghazarian", "Sarik", ""], ["Liu", "Zixi", ""], ["SM", "Akash", ""], ["Weischedel", "Ralph", ""], ["Galstyan", "Aram", ""], ["Peng", "Nanyun", ""]]}, {"id": "2104.05807", "submitter": "Marco Valentino", "authors": "Deborah Ferreira, Julia Rozanova, Mokanarangan Thayaparan, Marco\n  Valentino, Andr\\'e Freitas", "title": "Does My Representation Capture X? Probe-Ably", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probing (or diagnostic classification) has become a popular strategy for\ninvestigating whether a given set of intermediate features is present in the\nrepresentations of neural models. Naive probing studies may have misleading\nresults, but various recent works have suggested more reliable methodologies\nthat compensate for the possible pitfalls of probing. However, these best\npractices are numerous and fast-evolving. To simplify the process of running a\nset of probing experiments in line with suggested methodologies, we introduce\nProbe-Ably: an extendable probing framework which supports and automates the\napplication of probing methods to the user's inputs\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 20:43:10 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ferreira", "Deborah", ""], ["Rozanova", "Julia", ""], ["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2104.05819", "submitter": "Bailin Wang", "authors": "Bailin Wang, Mirella Lapata and Ivan Titov", "title": "Learning from Executions for Semantic Parsing", "comments": "NAACL 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic parsing aims at translating natural language (NL) utterances onto\nmachine-interpretable programs, which can be executed against a real-world\nenvironment. The expensive annotation of utterance-program pairs has long been\nacknowledged as a major bottleneck for the deployment of contemporary neural\nmodels to real-life applications. In this work, we focus on the task of\nsemi-supervised learning where a limited amount of annotated data is available\ntogether with many unlabeled NL utterances. Based on the observation that\nprograms which correspond to NL utterances must be always executable, we\npropose to encourage a parser to generate executable programs for unlabeled\nutterances. Due to the large search space of executable programs, conventional\nmethods that use approximations based on beam-search such as self-training and\ntop-k marginal likelihood training, do not perform as well. Instead, we view\nthe problem of learning from executions from the perspective of posterior\nregularization and propose a set of new training objectives. Experimental\nresults on Overnight and GeoQuery show that our new objectives outperform\nconventional methods, bridging the gap between semi-supervised and supervised\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:07:53 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Bailin", ""], ["Lapata", "Mirella", ""], ["Titov", "Ivan", ""]]}, {"id": "2104.05824", "submitter": "Shuoyang Ding", "authors": "Shuoyang Ding, Philipp Koehn", "title": "Evaluating Saliency Methods for Neural Language Models", "comments": "19 pages, 2 figures, Accepted for NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency methods are widely used to interpret neural network predictions, but\ndifferent variants of saliency methods often disagree even on the\ninterpretations of the same prediction made by the same model. In these cases,\nhow do we identify when are these interpretations trustworthy enough to be used\nin analyses? To address this question, we conduct a comprehensive and\nquantitative evaluation of saliency methods on a fundamental category of NLP\nmodels: neural language models. We evaluate the quality of prediction\ninterpretations from two perspectives that each represents a desirable property\nof these interpretations: plausibility and faithfulness. Our evaluation is\nconducted on four different datasets constructed from the existing human\nannotation of syntactic and semantic agreements, on both sentence-level and\ndocument-level. Through our evaluation, we identified various ways saliency\nmethods could yield interpretations of low quality. We recommend that future\nwork deploying such methods to neural language models should carefully validate\ntheir interpretations before drawing insights.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:19:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ding", "Shuoyang", ""], ["Koehn", "Philipp", ""]]}, {"id": "2104.05827", "submitter": "Bailin Wang", "authors": "Bailin Wang, Wenpeng Yin, Xi Victoria Lin and Caiming Xiong", "title": "Learning to Synthesize Data for Semantic Parsing", "comments": "NAACL 2021 short paper, fixed citation issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthesizing data for semantic parsing has gained increasing attention\nrecently. However, most methods require handcrafted (high-precision) rules in\ntheir generative process, hindering the exploration of diverse unseen data. In\nthis work, we propose a generative model which features a (non-neural) PCFG\nthat models the composition of programs (e.g., SQL), and a BART-based\ntranslation model that maps a program to an utterance. Due to the simplicity of\nPCFG and pre-trained BART, our generative model can be efficiently learned from\nexisting data at hand. Moreover, explicitly modeling compositions using PCFG\nleads to a better exploration of unseen programs, thus generate more diverse\ndata. We evaluate our method in both in-domain and out-of-domain settings of\ntext-to-SQL parsing on the standard benchmarks of GeoQuery and Spider,\nrespectively. Our empirical results show that the synthesized data generated\nfrom our model can substantially help a semantic parser achieve better\ncompositional and domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:24:02 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 11:49:16 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Bailin", ""], ["Yin", "Wenpeng", ""], ["Lin", "Xi Victoria", ""], ["Xiong", "Caiming", ""]]}, {"id": "2104.05832", "submitter": "Roshanak Mirzaee", "authors": "Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, Parisa\n  Kordjmashidi", "title": "SpartQA: : A Textual Question Answering Benchmark for Spatial Reasoning", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a question-answering (QA) benchmark for spatial reasoning\non natural language text which contains more realistic spatial phenomena not\ncovered by prior work and is challenging for state-of-the-art language models\n(LM). We propose a distant supervision method to improve on this task.\nSpecifically, we design grammar and reasoning rules to automatically generate a\nspatial description of visual scenes and corresponding QA pairs. Experiments\nshow that further pretraining LMs on these automatically generated data\nsignificantly improves LMs' capability on spatial understanding, which in turn\nhelps to better solve two external datasets, bAbI, and boolQ. We hope that this\nwork can foster investigations into more sophisticated models for spatial\nreasoning over text.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:37:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mirzaee", "Roshanak", ""], ["Faghihi", "Hossein Rajaby", ""], ["Ning", "Qiang", ""], ["Kordjmashidi", "Parisa", ""]]}, {"id": "2104.05837", "submitter": "Tara Safavi", "authors": "Tara Safavi, Danai Koutra", "title": "Relational world knowledge representation in contextual language models:\n  A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relational knowledge bases (KBs) are established tools for world knowledge\nrepresentation in machines. While they are advantageous for their precision and\ninterpretability, they usually sacrifice some data modeling flexibility for\nthese advantages because they adhere to a manually engineered schema. In this\nreview, we take a natural language processing perspective to the limitations of\nKBs, examining how they may be addressed in part by training neural contextual\nlanguage models (LMs) to internalize and express relational knowledge in\nfree-text form. We propose a novel taxonomy for relational knowledge\nrepresentation in contextual LMs based on the level of KB supervision provided,\nconsidering both works that probe LMs for implicit relational knowledge\nacquired during self-supervised pretraining on unstructured text alone, and\nworks that explicitly supervise LMs at the level of KB entities and/or\nrelations. We conclude that LMs and KBs are complementary representation tools,\nas KBs provide a high standard of factual precision which can in turn be\nflexibly and expressively modeled by LMs, and provide suggestions for future\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:50:55 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Safavi", "Tara", ""], ["Koutra", "Danai", ""]]}, {"id": "2104.05845", "submitter": "Yue Yang", "authors": "Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar,\n  Chris Callison-Burch", "title": "Visual Goal-Step Inference using wikiHow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural events can often be thought of as a high level goal composed of a\nsequence of steps. Inferring the sub-sequence of steps of a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task where a model\nis given a textual goal and must choose a plausible step towards that goal from\namong four candidate images. Our task is challenging for state-of-the-art\nmuitimodal models. We introduce a novel dataset harvested from wikiHow that\nconsists of 772,294 images representing human actions. We show that the\nknowledge learned from our data can effectively transfer to other datasets like\nHowTo100M, increasing the multiple-choice accuracy by 15% to 20%. Our task will\nfacilitate multi-modal reasoning about procedural events.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:20:09 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yang", "Yue", ""], ["Panagopoulou", "Artemis", ""], ["Lyu", "Qing", ""], ["Zhang", "Li", ""], ["Yatskar", "Mark", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2104.05847", "submitter": "Xiaodong Liu", "authors": "Lis Pereira, Xiaodong Liu, Hao Cheng, Hoifung Poon, Jianfeng Gao,\n  Ichiro Kobayashi", "title": "Targeted Adversarial Training for Natural Language Understanding", "comments": "9 pages, 4 tables, 3 figurers, NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple yet effective Targeted Adversarial Training (TAT)\nalgorithm to improve adversarial training for natural language understanding.\nThe key idea is to introspect current mistakes and prioritize adversarial\ntraining steps to where the model errs the most. Experiments show that TAT can\nsignificantly improve accuracy over standard adversarial training on GLUE and\nattain new state-of-the-art zero-shot results on XNLI. Our code will be\nreleased at: https://github.com/namisan/mt-dnn.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:31:41 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Pereira", "Lis", ""], ["Liu", "Xiaodong", ""], ["Cheng", "Hao", ""], ["Poon", "Hoifung", ""], ["Gao", "Jianfeng", ""], ["Kobayashi", "Ichiro", ""]]}, {"id": "2104.05848", "submitter": "Zhong Zhou", "authors": "Zhong Zhou, Alex Waibel", "title": "Family of Origin and Family of Choice: Massively Parallel Lexiconized\n  Iterative Pretraining for Severely Low Resource Machine Translation", "comments": null, "journal-ref": "In Proceedings of the 3rd Workshop on Research in Computational\n  Typology and Multilingual NLP of the 20th Conference of the North American\n  Chapter of the Association for Computational Linguistics on Human Language\n  Technologies in 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We translate a closed text that is known in advance into a severely low\nresource language by leveraging massive source parallelism. In other words,\ngiven a text in 124 source languages, we translate it into a severely low\nresource language using only ~1,000 lines of low resource data without any\nexternal help. Firstly, we propose a systematic method to rank and choose\nsource languages that are close to the low resource language. We call the\nlinguistic definition of language family Family of Origin (FAMO), and we call\nthe empirical definition of higher-ranked languages using our metrics Family of\nChoice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual\nOrder-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines\n(~3.5%) of low resource data. To translate named entities correctly, we build a\nmassive lexicon table for 2,939 Bible named entities in 124 source languages,\nand include many that occur once and covers more than 66 severely low resource\nlanguages. Moreover, we also build a novel method of combining translations\nfrom different source languages into one. Using English as a hypothetical low\nresource language, we get a +23.9 BLEU increase over a multilingual baseline,\nand a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We\nget a 42.8 BLEU score for Portuguese-English translation on the medical EMEA\ndataset. We also have good results for a real severely low resource Mayan\nlanguage, Eastern Pokomchi.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:32:58 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 19:54:42 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 14:12:27 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 17:48:05 GMT"}, {"version": "v5", "created": "Mon, 24 May 2021 12:56:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhou", "Zhong", ""], ["Waibel", "Alex", ""]]}, {"id": "2104.05857", "submitter": "Robert Hawkins", "authors": "Robert D. Hawkins, Michael Franke, Michael C. Frank, Kenny Smith,\n  Thomas L. Griffiths, Noah D. Goodman", "title": "From partners to populations: A hierarchical Bayesian account of\n  coordination and convention", "comments": "Draft version, 4/12/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages are powerful solutions to coordination problems: they provide\nstable, shared expectations about how the words we say correspond to the\nbeliefs and intentions in our heads. Yet language use in a variable and\nnon-stationary social environment requires linguistic representations to be\nflexible: old words acquire new ad hoc or partner-specific meanings on the fly.\nIn this paper, we introduce a hierarchical Bayesian theory of convention\nformation that aims to reconcile the long-standing tension between these two\nbasic observations. More specifically, we argue that the central computational\nproblem of communication is not simply transmission, as in classical\nformulations, but learning and adaptation over multiple timescales. Under our\naccount, rapid learning within dyadic interactions allows for coordination on\npartner-specific common ground, while social conventions are stable priors that\nhave been abstracted away from interactions with multiple partners. We present\nnew empirical data alongside simulations showing how our model provides a\ncognitive foundation for explaining several phenomena that have posed a\nchallenge for previous accounts: (1) the convergence to more efficient\nreferring expressions across repeated interaction with the same partner, (2)\nthe gradual transfer of partner-specific common ground to novel partners, and\n(3) the influence of communicative context on which conventions eventually\nform.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 23:00:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hawkins", "Robert D.", ""], ["Franke", "Michael", ""], ["Frank", "Michael C.", ""], ["Smith", "Kenny", ""], ["Griffiths", "Thomas L.", ""], ["Goodman", "Noah D.", ""]]}, {"id": "2104.05882", "submitter": "Fajri Koto", "authors": "Fajri Koto and Jey Han Lau and Timothy Baldwin", "title": "Discourse Probing of Pretrained Language Models", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing work on probing of pretrained language models (LMs) has\npredominantly focused on sentence-level syntactic tasks. In this paper, we\nintroduce document-level discourse probing to evaluate the ability of\npretrained LMs to capture document-level relations. We experiment with 7\npretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be\noverall the best model at capturing discourse -- but only in its encoder, with\nBERT performing surprisingly well as the baseline model. Across the different\nmodels, there are substantial differences in which layers best capture\ndiscourse information, and large disparities between models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:04:31 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Koto", "Fajri", ""], ["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""]]}, {"id": "2104.05883", "submitter": "Chen Zhao", "authors": "Chen Zhao, Chenyan Xiong, Jordan Boyd-Graber, Hal Daum\\'e III", "title": "Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex question answering often requires finding a reasoning chain that\nconsists of multiple evidence pieces. Current approaches incorporate the\nstrengths of structured knowledge and unstructured text, assuming text corpora\nis semi-structured. Building on dense retrieval methods, we propose a new\nmulti-step retrieval approach (BeamDR) that iteratively forms an evidence chain\nthrough beam search in dense representations. When evaluated on multi-hop\nquestion answering, BeamDR is competitive to state-of-the-art systems, without\nusing any semi-structured information. Through query composition in dense\nspace, BeamDR captures the implicit relationships between evidence in the\nreasoning chain. The code is available at https://github.com/\nhenryzhao5852/BeamDR.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:16:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhao", "Chen", ""], ["Xiong", "Chenyan", ""], ["Boyd-Graber", "Jordan", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2104.05893", "submitter": "Anna Rohrbach", "authors": "Grace Luo, Trevor Darrell, Anna Rohrbach", "title": "NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The threat of online misinformation is hard to overestimate, with adversaries\nrelying on a range of tools, from cheap fakes to sophisticated deep fakes. We\nare motivated by a threat scenario where an image is being used out of context\nto support a certain narrative expressed in a caption. While some prior\ndatasets for detecting image-text inconsistency can be solved with blind models\ndue to linguistic cues introduced by text manipulation, we propose a dataset\nwhere both image and text are unmanipulated but mismatched. We introduce\nseveral strategies for automatic retrieval of suitable images for the given\ncaptions, capturing cases with related semantics but inconsistent entities as\nwell as matching entities but inconsistent semantic context. Our large-scale\nautomatically generated NewsCLIPpings Dataset requires models to jointly\nanalyze both modalities and to reason about entity mismatch as well as semantic\nmismatch between text and images in news media.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:53:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Luo", "Grace", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "2104.05904", "submitter": "Yichu Zhou", "authors": "Yichu Zhou and Vivek Srikumar", "title": "DirectProbe: Studying Representations without Classifiers", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how linguistic structures are encoded in contextualized\nembedding could help explain their impressive performance across NLP@. Existing\napproaches for probing them usually call for training classifiers and use the\naccuracy, mutual information, or complexity as a proxy for the representation's\ngoodness. In this work, we argue that doing so can be unreliable because\ndifferent representations may need different classifiers. We develop a\nheuristic, DirectProbe, that directly studies the geometry of a representation\nby building upon the notion of a version space for a task. Experiments with\nseveral linguistic tasks and contextualized embeddings show that, even without\ntraining classifiers, DirectProbe can shine light into how an embedding space\nrepresents labels, and also anticipate classifier performance for the\nrepresentation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 02:40:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhou", "Yichu", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2104.05919", "submitter": "Sha Li", "authors": "Sha Li, Heng Ji, Jiawei Han", "title": "Document-Level Event Argument Extraction by Conditional Generation", "comments": "11 pages. Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Event extraction has long been treated as a sentence-level task in the IE\ncommunity. We argue that this setting does not match human information-seeking\nbehavior and leads to incomplete and uninformative extraction results. We\npropose a document-level neural event argument extraction model by formulating\nthe task as conditional generation following event templates. We also compile a\nnew document-level event extraction benchmark dataset WikiEvents which includes\ncomplete event and coreference annotation. On the task of argument extraction,\nwe achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on\nthe RAMS and WikiEvents datasets respectively. On the more challenging task of\ninformative argument extraction, which requires implicit coreference reasoning,\nwe achieve a 9.3% F1 gain over the best baseline. To demonstrate the\nportability of our model, we also create the first end-to-end zero-shot event\nextraction framework and achieve 97% of fully supervised model's trigger\nextraction performance and 82% of the argument extraction performance given\nonly access to 10 out of the 33 types on ACE.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 03:36:38 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Sha", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "2104.05928", "submitter": "James Evans", "authors": "Brendan Chambers and James Evans", "title": "Semantic maps and metrics for science Semantic maps and metrics for\n  science using deep transformer encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growing deluge of scientific publications demands text analysis tools\nthat can help scientists and policy-makers navigate, forecast and beneficially\nguide scientific research. Recent advances in natural language understanding\ndriven by deep transformer networks offer new possibilities for mapping\nscience. Because the same surface text can take on multiple and sometimes\ncontradictory specialized senses across distinct research communities,\nsensitivity to context is critical for infometric applications. Transformer\nembedding models such as BERT capture shades of association and connotation\nthat vary across the different linguistic contexts of any particular word or\nspan of text. Here we report a procedure for encoding scientific documents with\nthese tools, measuring their improvement over static word embeddings in a\nnearest-neighbor retrieval task. We find discriminability of contextual\nrepresentations is strongly influenced by choice of pooling strategy for\nsummarizing the high-dimensional network activations. Importantly, we note that\nfundamentals such as domain-matched training data are more important than\nstate-of-the-art NLP tools. Yet state-of-the-art models did offer significant\ngains. The best approach we investigated combined domain-matched pretraining,\nsound pooling, and state-of-the-art deep transformer network encoders. Finally,\nwith the goal of leveraging contextual representations from deep encoders, we\npresent a range of measurements for understanding and forecasting research\ncommunities in science.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:12:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chambers", "Brendan", ""], ["Evans", "James", ""]]}, {"id": "2104.05938", "submitter": "Ming Zhong", "authors": "Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha,\n  Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir\n  Radev", "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting\n  Summarization", "comments": "Accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meetings are a key component of human collaboration. As increasing numbers of\nmeetings are recorded and transcribed, meeting summaries have become essential\nto remind those who may or may not have attended the meetings about the key\ndecisions made and the tasks to be completed. However, it is hard to create a\nsingle short summary that covers all the content of a long meeting involving\nmultiple people and topics. In order to satisfy the needs of different types of\nusers, we define a new query-based multi-domain meeting summarization task,\nwhere models have to select and summarize relevant spans of meetings in\nresponse to a query, and we introduce QMSum, a new benchmark for this task.\nQMSum consists of 1,808 query-summary pairs over 232 meetings in multiple\ndomains. Besides, we investigate a locate-then-summarize method and evaluate a\nset of strong summarization baselines on the task. Experimental results and\nmanual analysis reveal that QMSum presents significant challenges in long\nmeeting summarization for future research. Dataset is available at\n\\url{https://github.com/Yale-LILY/QMSum}.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 05:00:35 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhong", "Ming", ""], ["Yin", "Da", ""], ["Yu", "Tao", ""], ["Zaidi", "Ahmad", ""], ["Mutuma", "Mutethia", ""], ["Jha", "Rahul", ""], ["Awadallah", "Ahmed Hassan", ""], ["Celikyilmaz", "Asli", ""], ["Liu", "Yang", ""], ["Qiu", "Xipeng", ""], ["Radev", "Dragomir", ""]]}, {"id": "2104.05947", "submitter": "Mohit Chandra", "authors": "Mohit Chandra, Dheeraj Pailla, Himanshu Bhatia, Aadilmehdi Sanchawala,\n  Manish Gupta, Manish Shrivastava, Ponnurangam Kumaraguru", "title": "\"Subverting the Jewtocracy\": Online Antisemitism Detection Using\n  Multimodal Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The exponential rise of online social media has enabled the creation,\ndistribution, and consumption of information at an unprecedented rate. However,\nit has also led to the burgeoning of various forms of online abuse. Increasing\ncases of online antisemitism have become one of the major concerns because of\nits socio-political consequences. Unlike other major forms of online abuse like\nracism, sexism, etc., online antisemitism has not been studied much from a\nmachine learning perspective. To the best of our knowledge, we present the\nfirst work in the direction of automated multimodal detection of online\nantisemitism. The task poses multiple challenges that include extracting\nsignals across multiple modalities, contextual references, and handling\nmultiple aspects of antisemitism. Unfortunately, there does not exist any\npublicly available benchmark corpus for this critical task. Hence, we collect\nand label two datasets with 3,102 and 3,509 social media posts from Twitter and\nGab respectively. Further, we present a multimodal deep learning system that\ndetects the presence of antisemitic content and its specific antisemitism\ncategory using text and images from posts. We perform an extensive set of\nexperiments on the two datasets to evaluate the efficacy of the proposed\nsystem. Finally, we also present a qualitative analysis of our study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 05:22:55 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:05:21 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 19:30:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chandra", "Mohit", ""], ["Pailla", "Dheeraj", ""], ["Bhatia", "Himanshu", ""], ["Sanchawala", "Aadilmehdi", ""], ["Gupta", "Manish", ""], ["Shrivastava", "Manish", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2104.05964", "submitter": "Kyeongpil Kang", "authors": "Kyeongpil Kang, Kyohoon Jin, Soyoung Yang, Sujin Jang, Jaegul Choo,\n  Youngbin Kim", "title": "Restoring and Mining the Records of the Joseon Dynasty via Neural\n  Language Modeling and Machine Translation", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding voluminous historical records provides clues on the past in\nvarious aspects, such as social and political issues and even natural science\nfacts. However, it is generally difficult to fully utilize the historical\nrecords, since most of the documents are not written in a modern language and\npart of the contents are damaged over time. As a result, restoring the damaged\nor unrecognizable parts as well as translating the records into modern\nlanguages are crucial tasks. In response, we present a multi-task learning\napproach to restore and translate historical documents based on a\nself-attention mechanism, specifically utilizing two Korean historical records,\nones of the most voluminous historical records in the world. Experimental\nresults show that our approach significantly improves the accuracy of the\ntranslation task than baselines without multi-task learning. In addition, we\npresent an in-depth exploratory analysis on our translated results via topic\nmodeling, uncovering several significant historical events.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:40:25 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 06:18:25 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 03:25:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kang", "Kyeongpil", ""], ["Jin", "Kyohoon", ""], ["Yang", "Soyoung", ""], ["Jang", "Sujin", ""], ["Choo", "Jaegul", ""], ["Kim", "Youngbin", ""]]}, {"id": "2104.05965", "submitter": "Jae Won Cho", "authors": "Jae Won Cho, Dong-Jin Kim, Jinsoo Choi, Yunjae Jung, In So Kweon", "title": "Dealing with Missing Modalities in the Visual Question Answer-Difference\n  Prediction Task through Knowledge Distillation", "comments": "To appear in CVPR MULA Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the issues of missing modalities that have arisen\nfrom the Visual Question Answer-Difference prediction task and find a novel\nmethod to solve the task at hand. We address the missing modality-the ground\ntruth answers-that are not present at test time and use a privileged knowledge\ndistillation scheme to deal with the issue of the missing modality. In order to\nefficiently do so, we first introduce a model, the \"Big\" Teacher, that takes\nthe image/question/answer triplet as its input and outperforms the baseline,\nthen use a combination of models to distill knowledge to a target network\n(student) that only takes the image/question pair as its inputs. We experiment\nour models on the VizWiz and VQA-V2 Answer Difference datasets and show through\nextensive experimentation and ablation the performances of our method and a\ndiverse possibility for future research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:41:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cho", "Jae Won", ""], ["Kim", "Dong-Jin", ""], ["Choi", "Jinsoo", ""], ["Jung", "Yunjae", ""], ["Kweon", "In So", ""]]}, {"id": "2104.05980", "submitter": "Nina Hosseini-Kivanani", "authors": "Nina Hosseini-Kivanani, Roberto Gretter, Marco Matassoni, and Giuseppe\n  Daniele Falavigna", "title": "Experiments of ASR-based mispronunciation detection for children and\n  adult English learners", "comments": "Submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pronunciation is one of the fundamentals of language learning, and it is\nconsidered a primary factor of spoken language when it comes to an\nunderstanding and being understood by others. The persistent presence of high\nerror rates in speech recognition domains resulting from mispronunciations\nmotivates us to find alternative techniques for handling mispronunciations. In\nthis study, we develop a mispronunciation assessment system that checks the\npronunciation of non-native English speakers, identifies the commonly\nmispronounced phonemes of Italian learners of English, and presents an\nevaluation of the non-native pronunciation observed in phonetically annotated\nspeech corpora. In this work, to detect mispronunciations, we used a\nphone-based ASR implemented using Kaldi. We used two non-native English labeled\ncorpora; (i) a corpus of Italian adults contains 5,867 utterances from 46\nspeakers, and (ii) a corpus of Italian children consists of 5,268 utterances\nfrom 78 children. Our results show that the selected error model can\ndiscriminate correct sounds from incorrect sounds in both native and nonnative\nspeech, and therefore can be used to detect pronunciation errors in non-native\nspeech. The phone error rates show improvement in using the error language\nmodel. The ASR system shows better accuracy after applying the error model on\nour selected corpora.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:24:05 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hosseini-Kivanani", "Nina", ""], ["Gretter", "Roberto", ""], ["Matassoni", "Marco", ""], ["Falavigna", "Giuseppe Daniele", ""]]}, {"id": "2104.06001", "submitter": "Marco Gaido", "authors": "Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, Marco\n  Turchi", "title": "Gender Bias in Machine Translation", "comments": "Accepted for publication in Transaction of the Association for\n  Computational Linguistics (TACL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine translation (MT) technology has facilitated our daily tasks by\nproviding accessible shortcuts for gathering, elaborating and communicating\ninformation. However, it can suffer from biases that harm users and society at\nlarge. As a relatively new field of inquiry, gender bias in MT still lacks\ninternal cohesion, which advocates for a unified framework to ease future\nresearch. To this end, we: i) critically review current conceptualizations of\nbias in light of theoretical insights from related disciplines, ii) summarize\nprevious analyses aimed at assessing gender bias in MT, iii) discuss the\nmitigating strategies proposed so far, and iv) point toward potential\ndirections for future work.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:09:03 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 13:15:52 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 15:22:11 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Savoldi", "Beatrice", ""], ["Gaido", "Marco", ""], ["Bentivogli", "Luisa", ""], ["Negri", "Matteo", ""], ["Turchi", "Marco", ""]]}, {"id": "2104.06008", "submitter": "Zongshen Mu", "authors": "Zongshen Mu, Siliang Tang, Jie Tan, Qiang Yu, Yueting Zhuang", "title": "Disentangled Motif-aware Graph Learning for Phrase Grounding", "comments": "10 pages, 6 figures, AAAI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel graph learning framework for phrase\ngrounding in the image. Developing from the sequential to the dense graph\nmodel, existing works capture coarse-grained context but fail to distinguish\nthe diversity of context among phrases and image regions. In contrast, we pay\nspecial attention to different motifs implied in the context of the scene graph\nand devise the disentangled graph network to integrate the motif-aware\ncontextual information into representations. Besides, we adopt interventional\nstrategies at the feature and the structure levels to consolidate and\ngeneralize representations. Finally, the cross-modal attention network is\nutilized to fuse intra-modal features, where each phrase can be computed\nsimilarity with regions to select the best-grounded one. We validate the\nefficiency of disentangled and interventional graph network (DIGN) through a\nseries of ablation studies, and our model achieves state-of-the-art performance\non Flickr30K Entities and ReferIt Game benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:20:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mu", "Zongshen", ""], ["Tang", "Siliang", ""], ["Tan", "Jie", ""], ["Yu", "Qiang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2104.06022", "submitter": "Sho Takase", "authors": "Sho Takase and Shun Kiyono", "title": "Lessons on Parameter Sharing across Layers in Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:41:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Takase", "Sho", ""], ["Kiyono", "Shun", ""]]}, {"id": "2104.06039", "submitter": "Alon Talmor", "authors": "Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari\n  Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant", "title": "MultiModalQA: Complex Question Answering over Text, Tables and Images", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When answering complex questions, people can seamlessly combine information\nfrom visual, textual and tabular sources. While interest in models that reason\nover multiple pieces of evidence has surged in recent years, there has been\nrelatively little work on question answering models that reason across multiple\nmodalities. In this paper, we present MultiModalQA(MMQA): a challenging\nquestion answering dataset that requires joint reasoning over text, tables and\nimages. We create MMQA using a new framework for generating complex multi-modal\nquestions at scale, harvesting tables from Wikipedia, and attaching images and\ntext paragraphs using entities that appear in each table. We then define a\nformal language that allows us to take questions that can be answered from a\nsingle modality, and combine them to generate cross-modal questions. Last,\ncrowdsourcing workers take these automatically-generated questions and rephrase\nthem into more fluent language. We create 29,918 questions through this\nprocedure, and empirically demonstrate the necessity of a multi-modal multi-hop\napproach to solve our task: our multi-hop model, ImplicitDecomp, achieves an\naverage F1of 51.7 over cross-modal questions, substantially outperforming a\nstrong baseline that achieves 38.2 F1, but still lags significantly behind\nhuman performance, which is at 90.1 F1\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:14:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Talmor", "Alon", ""], ["Yoran", "Ori", ""], ["Catav", "Amnon", ""], ["Lahav", "Dan", ""], ["Wang", "Yizhong", ""], ["Asai", "Akari", ""], ["Ilharco", "Gabriel", ""], ["Hajishirzi", "Hannaneh", ""], ["Berant", "Jonathan", ""]]}, {"id": "2104.06045", "submitter": "Vincent Micheli", "authors": "Vincent Micheli, Quentin Heinrich, Fran\\c{c}ois Fleuret, Wacim\n  Belblidia", "title": "Structural analysis of an all-purpose question answering model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Attention is a key component of the now ubiquitous pre-trained language\nmodels. By learning to focus on relevant pieces of information, these\nTransformer-based architectures have proven capable of tackling several tasks\nat once and sometimes even surpass their single-task counterparts. To better\nunderstand this phenomenon, we conduct a structural analysis of a new\nall-purpose question answering model that we introduce. Surprisingly, this\nmodel retains single-task performance even in the absence of a strong transfer\neffect between tasks. Through attention head importance scoring, we observe\nthat attention heads specialize in a particular task and that some heads are\nmore conducive to learning than others in both the multi-task and single-task\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:20:44 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Micheli", "Vincent", ""], ["Heinrich", "Quentin", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Belblidia", "Wacim", ""]]}, {"id": "2104.06048", "submitter": "Emanuela Boros", "authors": "Emanuela Boros and Antoine Doucet", "title": "Transformer-based Methods for Recognizing Ultra Fine-grained Entities\n  (RUFES)", "comments": null, "journal-ref": "https://tac.nist.gov/2020/KBP/RUFES/index.html", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper summarizes the participation of the Laboratoire Informatique,\nImage et Interaction (L3i laboratory) of the University of La Rochelle in the\nRecognizing Ultra Fine-grained Entities (RUFES) track within the Text Analysis\nConference (TAC) series of evaluation workshops. Our participation relies on\ntwo neural-based models, one based on a pre-trained and fine-tuned language\nmodel with a stack of Transformer layers for fine-grained entity extraction and\none out-of-the-box model for within-document entity coreference. We observe\nthat our approach has great potential in increasing the performance of\nfine-grained entity recognition. Thus, the future work envisioned is to enhance\nthe ability of the models following additional experiments and a deeper\nanalysis of the results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:23:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Boros", "Emanuela", ""], ["Doucet", "Antoine", ""]]}, {"id": "2104.06063", "submitter": "Dumitru-Clementin Cercel", "authors": "R\\u{a}zvan-Alexandru Sm\\u{a}du, Dumitru-Clementin Cercel, Mihai\n  Dascalu", "title": "UPB at SemEval-2021 Task 7: Adversarial Multi-Task Learning for\n  Detecting and Rating Humor and Offense", "comments": "7 pages, 2 figures, Accepted at SemEval-2021 Workshop, ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting humor is a challenging task since words might share multiple\nvalences and, depending on the context, the same words can be even used in\noffensive expressions. Neural network architectures based on Transformer obtain\nstate-of-the-art results on several Natural Language Processing tasks,\nespecially text classification. Adversarial learning, combined with other\ntechniques such as multi-task learning, aids neural models learn the intrinsic\nproperties of data. In this work, we describe our adversarial multi-task\nnetwork, AMTL-Humor, used to detect and rate humor and offensive texts from\nTask 7 at SemEval-2021. Each branch from the model is focused on solving a\nrelated task, and consists of a BiLSTM layer followed by Capsule layers, on top\nof BERTweet used for generating contextualized embeddings. Our best model\nconsists of an ensemble of all tested configurations, and achieves a 95.66%\nF1-score and 94.70% accuracy for Task 1a, while obtaining RMSE scores of 0.6200\nand 0.5318 for Tasks 1b and 2, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:59:05 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sm\u0103du", "R\u0103zvan-Alexandru", ""], ["Cercel", "Dumitru-Clementin", ""], ["Dascalu", "Mihai", ""]]}, {"id": "2104.06104", "submitter": "Wei Zhou", "authors": "Wei Zhou, Albert Zeyer, Andr\\'e Merboldt, Ralf Schl\\\"uter, Hermann Ney", "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of\n  Concept", "comments": "accepted at Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of direct models in automatic speech recognition (ASR), the\nformerly prevalent frame-wise acoustic modeling based on hidden Markov models\n(HMM) diversified into a number of modeling architectures like encoder-decoder\nattention models, transducer models and segmental models (direct HMM). While\ntransducer models stay with a frame-level model definition, segmental models\nare defined on the level of label segments directly. While\n(soft-)attention-based models avoid explicit alignment, transducer and\nsegmental approach internally do model alignment, either by segment hypotheses\nor, more implicitly, by emitting so-called blank symbols. In this work, we\nprove that the widely used class of RNN-Transducer models and segmental models\n(direct HMM) are equivalent and therefore show equal modeling power. It is\nshown that blank probabilities translate into segment length probabilities and\nvice versa. In addition, we provide initial experiments investigating decoding\nand beam-pruning, comparing time-synchronous and label-/segment-synchronous\nsearch strategies and their properties using the same underlying model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:20:48 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 17:17:34 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhou", "Wei", ""], ["Zeyer", "Albert", ""], ["Merboldt", "Andr\u00e9", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.06129", "submitter": "Mor Geva", "authors": "Mor Geva, Uri Katz, Aviv Ben-Arie, Jonathan Berant", "title": "What's in your Head? Emergent Behaviour in Multi-Task Transformer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary paradigm for multi-task training in natural language processing\nis to represent the input with a shared pre-trained language model, and add a\nsmall, thin network (head) per task. Given an input, a target head is the head\nthat is selected for outputting the final prediction. In this work, we examine\nthe behaviour of non-target heads, that is, the output of heads when given\ninput that belongs to a different task than the one they were trained for. We\nfind that non-target heads exhibit emergent behaviour, which may either explain\nthe target task, or generalize beyond their original task. For example, in a\nnumerical reasoning task, a span extraction head extracts from the input the\narguments to a computation that results in a number generated by a target\ngenerative head. In addition, a summarization head that is trained with a\ntarget question answering head, outputs query-based summaries when given a\nquestion and a context from which the answer is to be extracted. This emergent\nbehaviour suggests that multi-task training leads to non-trivial extrapolation\nof skills, which can be harnessed for interpretability and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 12:04:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Geva", "Mor", ""], ["Katz", "Uri", ""], ["Ben-Arie", "Aviv", ""], ["Berant", "Jonathan", ""]]}, {"id": "2104.06182", "submitter": "Jose Manuel Gomez-Perez", "authors": "Andres Garcia-Silva, Cristian Berrio, Jose Manuel Gomez-Perez", "title": "Understanding Transformers for Bot Detection in Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we shed light on the impact of fine-tuning over social media\ndata in the internal representations of neural language models. We focus on bot\ndetection in Twitter, a key task to mitigate and counteract the automatic\nspreading of disinformation and bias in social media. We investigate the use of\npre-trained language models to tackle the detection of tweets generated by a\nbot or a human account based exclusively on its content. Unlike the general\ntrend in benchmarks like GLUE, where BERT generally outperforms generative\ntransformers like GPT and GPT-2 for most classification tasks on regular text,\nwe observe that fine-tuning generative transformers on a bot detection task\nproduces higher accuracies. We analyze the architectural components of each\ntransformer and study the effect of fine-tuning on their hidden states and\noutput representations. Among our findings, we show that part of the\nsyntactical information and distributional properties captured by BERT during\npre-training is lost upon fine-tuning while the generative pre-training\napproach manage to preserve these properties.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:32:55 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Garcia-Silva", "Andres", ""], ["Berrio", "Cristian", ""], ["Gomez-Perez", "Jose Manuel", ""]]}, {"id": "2104.06200", "submitter": "Jose Manuel Gomez-Perez", "authors": "Andres Garcia-Silva, Ronald Denaux, Jose Manuel Gomez-Perez", "title": "On the Impact of Knowledge-based Linguistic Annotations in the Quality\n  of Scientific Embeddings", "comments": "Accepted for publication in Future Generation Computer Systems", "journal-ref": null, "doi": "10.1016/j.future.2021.02.019", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In essence, embedding algorithms work by optimizing the distance between a\nword and its usual context in order to generate an embedding space that encodes\nthe distributional representation of words. In addition to single words or word\npieces, other features which result from the linguistic analysis of text,\nincluding lexical, grammatical and semantic information, can be used to improve\nthe quality of embedding spaces. However, until now we did not have a precise\nunderstanding of the impact that such individual annotations and their possible\ncombinations may have in the quality of the embeddings. In this paper, we\nconduct a comprehensive study on the use of explicit linguistic annotations to\ngenerate embeddings from a scientific corpus and quantify their impact in the\nresulting representations. Our results show how the effect of such annotations\nin the embeddings varies depending on the evaluation task. In general, we\nobserve that learning embeddings using linguistic annotations contributes to\nachieve better evaluation results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:51:22 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Garcia-Silva", "Andres", ""], ["Denaux", "Ronald", ""], ["Gomez-Perez", "Jose Manuel", ""]]}, {"id": "2104.06230", "submitter": "Xinyan Zhao", "authors": "Xinyan Zhao, Haibo Ding, Zhe Feng", "title": "GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised\n  Named Entity Recognition", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instead of using expensive manual annotations, researchers have proposed to\ntrain named entity recognition (NER) systems using heuristic labeling rules.\nHowever, devising labeling rules is challenging because it often requires a\nconsiderable amount of manual effort and domain expertise. To alleviate this\nproblem, we propose \\textsc{GLaRA}, a graph-based labeling rule augmentation\nframework, to learn new labeling rules from unlabeled data. We first create a\ngraph with nodes representing candidate rules extracted from unlabeled data.\nThen, we design a new graph neural network to augment labeling rules by\nexploring the semantic relations between rules. We finally apply the augmented\nrules on unlabeled data to generate weak labels and train a NER model using the\nweakly labeled data. We evaluate our method on three NER datasets and find that\nwe can achieve an average improvement of +20\\% F1 score over the best baseline\nwhen given a small set of seed rules.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:20:58 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhao", "Xinyan", ""], ["Ding", "Haibo", ""], ["Feng", "Zhe", ""]]}, {"id": "2104.06239", "submitter": "Daniel Fern\\'andez-Gonz\\'alez", "authors": "Daniel Fern\\'andez-Gonz\\'alez and Carlos G\\'omez-Rodr\\'iguez", "title": "Reducing Discontinuous to Continuous Parsing with Pointer Network\n  Reordering", "comments": "8 pages (incl. appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discontinuous constituent parsers have always lagged behind continuous\napproaches in terms of accuracy and speed, as the presence of constituents with\ndiscontinuous yield introduces extra complexity to the task. However, a\ndiscontinuous tree can be converted into a continuous variant by reordering\ntokens. Based on that, we propose to reduce discontinuous parsing to a\ncontinuous problem, which can then be directly solved by any off-the-shelf\ncontinuous parser. To that end, we develop a Pointer Network capable of\naccurately generating the continuous token arrangement for a given input\nsentence and define a bijective function to recover the original order.\nExperiments on the main benchmarks with two continuous parsers prove that our\napproach is on par in accuracy with purely discontinuous state-of-the-art\nalgorithms, but considerably faster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:32:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fern\u00e1ndez-Gonz\u00e1lez", "Daniel", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2104.06245", "submitter": "Karl Stratos", "authors": "Wenzheng Zhang and Karl Stratos", "title": "Understanding Hard Negatives in Noise Contrastive Estimation", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of negative examples is important in noise contrastive estimation.\nRecent works find that hard negatives -- highest-scoring incorrect examples\nunder the model -- are effective in practice, but they are used without a\nformal justification. We develop analytical tools to understand the role of\nhard negatives. Specifically, we view the contrastive loss as a biased\nestimator of the gradient of the cross-entropy loss, and show both\ntheoretically and empirically that setting the negative distribution to be the\nmodel distribution results in bias reduction. We also derive a general form of\nthe score function that unifies various architectures used in text retrieval.\nBy combining hard negatives with appropriate score functions, we obtain strong\nresults on the challenging task of zero-shot entity linking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:42:41 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Wenzheng", ""], ["Stratos", "Karl", ""]]}, {"id": "2104.06268", "submitter": "Genta Indra Winata", "authors": "Genta Indra Winata", "title": "Multilingual Transfer Learning for Code-Switched Language and Speech\n  Neural Modeling", "comments": "HKUST PhD Thesis. 120 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we address the data scarcity and limitations of linguistic\ntheory by proposing language-agnostic multi-task training methods. First, we\nintroduce a meta-learning-based approach, meta-transfer learning, in which\ninformation is judiciously extracted from high-resource monolingual speech data\nto the code-switching domain. The meta-transfer learning quickly adapts the\nmodel to the code-switching task from a number of monolingual tasks by learning\nto learn in a multi-task learning fashion. Second, we propose a novel\nmultilingual meta-embeddings approach to effectively represent code-switching\ndata by acquiring useful knowledge learned in other languages, learning the\ncommonalities of closely related languages and leveraging lexical composition.\nThe method is far more efficient compared to contextualized pre-trained\nmultilingual models. Third, we introduce multi-task learning to integrate\nsyntactic information as a transfer learning strategy to a language model and\nlearn where to code-switch. To further alleviate the aforementioned issues, we\npropose a data augmentation method using Pointer-Gen, a neural network using a\ncopy mechanism to teach the model the code-switch points from monolingual\nparallel sentences. We disentangle the need for linguistic theory, and the\nmodel captures code-switching points by attending to input words and aligning\nthe parallel words, without requiring any word alignments or constituency\nparsers. More importantly, the model can be effectively used for languages that\nare syntactically different, and it outperforms the linguistic theory-based\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:49:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Winata", "Genta Indra", ""]]}, {"id": "2104.06271", "submitter": "Enes Avcu", "authors": "Enes Avcu, Olivia Newman, David Gow", "title": "A Tale of Two Lexica Testing Computational Hypotheses with Deep\n  Convolutional Neural Networks", "comments": "8 pages, 3 figures, Presented as a talk at the Psychonomic Society's\n  61st Annual Meeting and poster at the SNL 2020 Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gow's (2012) dual lexicon model suggests that the primary purpose of words is\nto mediate the mappings between acoustic-phonetic input and other forms of\nlinguistic representation. Motivated by evidence from functional imaging,\naphasia, and behavioral results, the model argues for the existence of two\nparallel wordform stores: the dorsal and ventral processing streams. In this\npaper, we tested the hypothesis that the complex, but systematic mapping\nbetween sound and articulation in the dorsal stream poses different\ncomputational pressures on feature sets than the more arbitrary mapping between\nsound and meaning. To test this hypothesis, we created two deep convolutional\nneural networks (CNNs). While the dorsal network was trained to identify\nindividual spoken words, the ventral network was trained to map them onto\nsemantic classes. We then extracted patterns of network activation from the\npenultimate level of each network and tested how well features generated by the\nnetwork supported generalization to linguistic categorization associated with\nthe dorsal versus ventral processing streams. Our preliminary results showed\nboth models successfully learned their tasks. Secondary generalization testing\nshowed the ventral CNN outperformed the dorsal CNN on a semantic task:\nconcreteness classification, while the dorsal CNN outperformed the ventral CNN\non articulation tasks: classification by onset phoneme class and syllable\nlength. These results are consistent with the hypothesis that the divergent\nprocessing demands of the ventral and dorsal processing streams impose\ncomputational pressures for the development of multiple lexica.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 15:03:14 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Avcu", "Enes", ""], ["Newman", "Olivia", ""], ["Gow", "David", ""]]}, {"id": "2104.06324", "submitter": "Maciej Eder", "authors": "Rafa{\\l} L. G\\'orski and Maciej Eder", "title": "Modeling the dynamics of language change: logistic regression,\n  Piotrowski's law, and a handful of examples in Polish", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study discusses modeling diachronic processes by logistic regression.\nSuch an approach was suggested by Raimund Piotrowski (hence labelled as\nPiotrowski's law), even if actual linguistic evidence usually speaks against\nusing the notion of a \"law\" in this context. In our study, we apply logistic\nregression models to 9 changes which occurred between 15th and 18th century in\nthe Polish language. The attested course of the majority of these changes\nclosely follow the expected values, which proves that the language change might\nindeed resemble a nonlinear phase change scenario. We also extend the original\nPiotrowski's approach by proposing polynomial logistic regression for these\ncases which can hardly be described by its standard version. Also, we propose\nto consider individual language change cases jointly, in order to inspect their\npossible collinearity or, more likely, their different dynamics in the function\nof time. Last but not least, we evaluate our results by testing the influence\nof the subcorpus size on the model's goodness-of-fit.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:03:36 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 08:53:54 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["G\u00f3rski", "Rafa\u0142 L.", ""], ["Eder", "Maciej", ""]]}, {"id": "2104.06325", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Brian Roark, S{\\o}ren Wichmann, Ryan Cotterell,\n  Dami\\'an Blasi", "title": "Finding Concept-specific Biases in Form--Meaning Associations", "comments": "Accepted at NAACL 2021. This is the camera ready version. Code is\n  available in https://github.com/rycolab/form-meaning-associations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an information-theoretic operationalisation of\ncross-linguistic non-arbitrariness. It is not a new idea that there are small,\ncross-linguistic associations between the forms and meanings of words. For\ninstance, it has been claimed (Blasi et al., 2016) that the word for \"tongue\"\nis more likely than chance to contain the phone [l]. By controlling for the\ninfluence of language family and geographic proximity within a very large\nconcept-aligned, cross-lingual lexicon, we extend methods previously used to\ndetect within language non-arbitrariness (Pimentel et al., 2019) to measure\ncross-linguistic associations. We find that there is a significant effect of\nnon-arbitrariness, but it is unsurprisingly small (less than 0.5% on average\naccording to our information-theoretic estimate). We also provide a\nconcept-level analysis which shows that a quarter of the concepts considered in\nour work exhibit a significant level of cross-linguistic non-arbitrariness. In\nsum, the paper provides new methods to detect cross-linguistic associations at\nscale, and confirms their effects are minor.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:07:54 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:05:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Pimentel", "Tiago", ""], ["Roark", "Brian", ""], ["Wichmann", "S\u00f8ren", ""], ["Cotterell", "Ryan", ""], ["Blasi", "Dami\u00e1n", ""]]}, {"id": "2104.06335", "submitter": "Ian Berlot-Attwell", "authors": "Ian Berlot-Attwell and Frank Rudzicz", "title": "On the Use of Linguistic Features for the Evaluation of Generative\n  Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically evaluating text-based, non-task-oriented dialogue systems\n(i.e., `chatbots') remains an open problem. Previous approaches have suffered\nchallenges ranging from poor correlation with human judgment to poor\ngeneralization and have often required a gold standard reference for comparison\nor human-annotated data. Extending existing evaluation methods, we propose that\na metric based on linguistic features may be able to maintain good correlation\nwith human judgment and be interpretable, without requiring a gold-standard\nreference or human-annotated data. To support this proposition, we measure and\nanalyze various linguistic features on dialogues produced by multiple dialogue\nmodels. We find that the features' behaviour is consistent with the known\nproperties of the models tested, and is similar across domains. We also\ndemonstrate that this approach exhibits promising properties such as zero-shot\ngeneralization to new domains on the related task of evaluating response\nrelevance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:28:00 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Berlot-Attwell", "Ian", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2104.06338", "submitter": "Silvio Amir", "authors": "Silvio Amir and Jan-Willem van de Meent and Byron C. Wallace", "title": "On the Impact of Random Seeds on the Fairness of Clinical Classifiers", "comments": "Accepted for publication at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent work has shown that fine-tuning large networks is surprisingly\nsensitive to changes in random seed(s). We explore the implications of this\nphenomenon for model fairness across demographic groups in clinical prediction\ntasks over electronic health records (EHR) in MIMIC-III -- the standard dataset\nin clinical NLP research. Apparent subgroup performance varies substantially\nfor seeds that yield similar overall performance, although there is no evidence\nof a trade-off between overall and subgroup performance. However, we also find\nthat the small sample sizes inherent to looking at intersections of minority\ngroups and somewhat rare conditions limit our ability to accurately estimate\ndisparities. Further, we find that jointly optimizing for high overall\nperformance and low disparities does not yield statistically significant\nimprovements. Our results suggest that fairness work using MIMIC-III should\ncarefully account for variations in apparent differences that may arise from\nstochasticity and small sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:30:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Amir", "Silvio", ""], ["van de Meent", "Jan-Willem", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2104.06378", "submitter": "Michihiro Yasunaga", "authors": "Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang and Jure\n  Leskovec", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question\n  Answering", "comments": "NAACL 2021. Code & data available at\n  https://github.com/michiyasunaga/qagnn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of answering questions using knowledge from pre-trained language\nmodels (LMs) and knowledge graphs (KGs) presents two challenges: given a QA\ncontext (question and answer choice), methods need to (i) identify relevant\nknowledge from large KGs, and (ii) perform joint reasoning over the QA context\nand KG. In this work, we propose a new model, QA-GNN, which addresses the above\nchallenges through two key innovations: (i) relevance scoring, where we use LMs\nto estimate the importance of KG nodes relative to the given QA context, and\n(ii) joint reasoning, where we connect the QA context and KG to form a joint\ngraph, and mutually update their representations through graph neural networks.\nWe evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its\nimprovement over existing LM and LM+KG models, as well as its capability to\nperform interpretable and structured reasoning, e.g., correctly handling\nnegation in questions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:32:51 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 23:30:14 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yasunaga", "Michihiro", ""], ["Ren", "Hongyu", ""], ["Bosselut", "Antoine", ""], ["Liang", "Percy", ""], ["Leskovec", "Jure", ""]]}, {"id": "2104.06387", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan, Shuaicheng Chang,\n  Junqi Dai, Yixin Liu, Zihuiwen Ye, Zi-Yi Dou, Graham Neubig", "title": "ExplainaBoard: An Explainable Leaderboard for NLP", "comments": "ACL2021 Demo Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{http://explainaboard.nlpedia.ai/} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:45:50 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 18:50:23 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Pengfei", ""], ["Fu", "Jinlan", ""], ["Xiao", "Yang", ""], ["Yuan", "Weizhe", ""], ["Chang", "Shuaicheng", ""], ["Dai", "Junqi", ""], ["Liu", "Yixin", ""], ["Ye", "Zihuiwen", ""], ["Dou", "Zi-Yi", ""], ["Neubig", "Graham", ""]]}, {"id": "2104.06390", "submitter": "Eric Wallace", "authors": "Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten\n  Sap, Dan Klein", "title": "Detoxifying Language Models Risks Marginalizing Minority Voices", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models (LMs) must be both safe and equitable to be responsibly\ndeployed in practice. With safety in mind, numerous detoxification techniques\n(e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to\nmitigate toxic LM generations. In this work, we show that current\ndetoxification techniques hurt equity: they decrease the utility of LMs on\nlanguage used by marginalized groups (e.g., African-American English and\nminority identity mentions). In particular, we perform automatic and human\nevaluations of text generation quality when LMs are conditioned on inputs with\ndifferent dialects and group identifiers. We find that detoxification makes LMs\nmore brittle to distribution shift, especially on language used by marginalized\ngroups. We identify that these failures stem from detoxification methods\nexploiting spurious correlations in toxicity datasets. Overall, our results\nhighlight the tension between the controllability and distributional robustness\nof LMs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:52:01 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Albert", ""], ["Pathak", "Eshaan", ""], ["Wallace", "Eric", ""], ["Gururangan", "Suchin", ""], ["Sap", "Maarten", ""], ["Klein", "Dan", ""]]}, {"id": "2104.06393", "submitter": "Liang Ding", "authors": "Di Wu, Yiren Chen, Liang Ding, Dacheng Tao", "title": "Bridging the Gap Between Clean Data Training and Real-World Inference\n  for Spoken Language Understanding", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU) system usually consists of various\npipeline components, where each component heavily relies on the results of its\nupstream ones. For example, Intent detection (ID), and slot filling (SF)\nrequire its upstream automatic speech recognition (ASR) to transform the voice\ninto text. In this case, the upstream perturbations, e.g. ASR errors,\nenvironmental noise and careless user speaking, will propagate to the ID and SF\nmodels, thus deteriorating the system performance. Therefore, the\nwell-performing SF and ID models are expected to be noise resistant to some\nextent. However, existing models are trained on clean data, which causes a\n\\textit{gap between clean data training and real-world inference.} To bridge\nthe gap, we propose a method from the perspective of domain adaptation, by\nwhich both high- and low-quality samples are embedding into similar vector\nspace. Meanwhile, we design a denoising generation model to reduce the impact\nof the low-quality samples. Experiments on the widely-used dataset, i.e. Snips,\nand large scale in-house dataset (10 million training examples) demonstrate\nthat this method not only outperforms the baseline models on real-world (noisy)\ncorpus but also enhances the robustness, that is, it produces high-quality\nresults under a noisy environment. The source code will be released.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:54:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wu", "Di", ""], ["Chen", "Yiren", ""], ["Ding", "Liang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.06400", "submitter": "Aviv Slobodkin", "authors": "Aviv Slobodkin, Leshem Choshen, Omri Abend", "title": "Mediators in Determining what Processing BERT Performs First", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Probing neural models for the ability to perform downstream tasks using their\nactivation patterns is often used to localize what parts of the network\nspecialize in performing what tasks. However, little work addressed potential\nmediating factors in such comparisons. As a test-case mediating factor, we\nconsider the prediction's context length, namely the length of the span whose\nprocessing is minimally required to perform the prediction. We show that not\ncontrolling for context length may lead to contradictory conclusions as to the\nlocalization patterns of the network, depending on the distribution of the\nprobing dataset. Indeed, when probing BERT with seven tasks, we find that it is\npossible to get 196 different rankings between them when manipulating the\ndistribution of context lengths in the probing dataset. We conclude by\npresenting best practices for conducting such comparisons in the future.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:58:52 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Slobodkin", "Aviv", ""], ["Choshen", "Leshem", ""], ["Abend", "Omri", ""]]}, {"id": "2104.06439", "submitter": "Maria Ponomareva", "authors": "Boris Zhestiankin and Maria Ponomareva", "title": "Zhestyatsky at SemEval-2021 Task 2: ReLU over Cosine Similarity for BERT\n  Fine-tuning", "comments": "Accepted to SemEval-2021 at ACL-IJCNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our contribution to SemEval-2021 Task 2: Multilingual and\nCross-lingual Word-in-Context Disambiguation (MCL-WiC). Our experiments cover\nEnglish (EN-EN) sub-track from the multilingual setting of the task. We\nexperiment with several pre-trained language models and investigate an impact\nof different top-layers on fine-tuning. We find the combination of Cosine\nSimilarity and ReLU activation leading to the most effective fine-tuning\nprocedure. Our best model results in accuracy 92.7%, which is the fourth-best\nscore in EN-EN sub-track.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 18:28:58 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhestiankin", "Boris", ""], ["Ponomareva", "Maria", ""]]}, {"id": "2104.06443", "submitter": "Julia Mendelsohn", "authors": "Julia Mendelsohn, Ceren Budak, David Jurgens", "title": "Modeling Framing in Immigration Discourse on Social Media", "comments": "Accepted at NAACL 2021 (camera-ready), Annotation codebook, data,\n  models, and code available at https://github.com/juliamendelsohn/framing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The framing of political issues can influence policy and public opinion. Even\nthough the public plays a key role in creating and spreading frames, little is\nknown about how ordinary people on social media frame political issues. By\ncreating a new dataset of immigration-related tweets labeled for multiple\nframing typologies from political communication theory, we develop supervised\nmodels to detect frames. We demonstrate how users' ideology and region impact\nframing choices, and how a message's framing influences audience responses. We\nfind that the more commonly-used issue-generic frames obscure important\nideological and regional patterns that are only revealed by\nimmigration-specific frames. Furthermore, frames oriented towards human\ninterests, culture, and politics are associated with higher user engagement.\nThis large-scale analysis of a complex social and linguistic phenomenon\ncontributes to both NLP and social science research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 18:35:44 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mendelsohn", "Julia", ""], ["Budak", "Ceren", ""], ["Jurgens", "David", ""]]}, {"id": "2104.06457", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Tatsuya Kawahara, Shinji Watanabe", "title": "Source and Target Bidirectional Knowledge Distillation for End-to-end\n  Speech Translation", "comments": "Accepted at NAACL-HLT 2021 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conventional approach to improving the performance of end-to-end speech\ntranslation (E2E-ST) models is to leverage the source transcription via\npre-training and joint training with automatic speech recognition (ASR) and\nneural machine translation (NMT) tasks. However, since the input modalities are\ndifferent, it is difficult to leverage source language text successfully. In\nthis work, we focus on sequence-level knowledge distillation (SeqKD) from\nexternal text-based NMT models. To leverage the full potential of the source\nlanguage information, we propose backward SeqKD, SeqKD from a target-to-source\nbackward NMT model. To this end, we train a bilingual E2E-ST model to predict\nparaphrased transcriptions as an auxiliary task with a single decoder. The\nparaphrases are generated from the translations in bitext via back-translation.\nWe further propose bidirectional SeqKD in which SeqKD from both forward and\nbackward NMT models is combined. Experimental evaluations on both\nautoregressive and non-autoregressive models show that SeqKD in each direction\nconsistently improves the translation performance, and the effectiveness is\ncomplementary regardless of the model capacity.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:00:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Kawahara", "Tatsuya", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2104.06474", "submitter": "Edgar Altszyler", "authors": "Francisco Valentini, Germ\\'an Rosati, Dami\\'an Blasi, Diego Fernandez\n  Slezak, and Edgar Altszyler", "title": "On the interpretation and significance of bias metrics in texts: a\n  PMI-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the use of word embeddings has become popular to measure the\npresence of biases in texts. Despite the fact that these measures have been\nshown to be effective in detecting a wide variety of biases, metrics based on\nword embeddings lack transparency, explainability and interpretability. In this\nstudy, we propose a PMI-based metric to quantify biases in texts. We show that\nthis metric can be approximated by an odds ratio, which allows estimating the\nconfidence interval and statistical significance of textual bias. We also show\nthat this PMI-based measure can be expressed as a function of conditional\nprobabilities, providing a simple interpretation in terms of word\nco-occurrences. Our approach produces a performance comparable to GloVe-based\nand Skip-gram-based metrics in experiments of gender-occupation and gender-name\nassociations. We discuss the advantages and disadvantages of using methods\nbased on first-order vs second-order co-occurrences, from the point of view of\nthe interpretability of the metric and the sparseness of the data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:34:17 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Valentini", "Francisco", ""], ["Rosati", "Germ\u00e1n", ""], ["Blasi", "Dami\u00e1n", ""], ["Slezak", "Diego Fernandez", ""], ["Altszyler", "Edgar", ""]]}, {"id": "2104.06483", "submitter": "Ling Liu", "authors": "Ling Liu and Mans Hulden", "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural\n  Morphological Inflection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning sequence models have been successfully applied to the task of\nmorphological inflection. The results of the SIGMORPHON shared tasks in the\npast several years indicate that such models can perform well, but only if the\ntraining data cover a good amount of different lemmata, or if the lemmata that\nare inflected at test time have also been seen in training, as has indeed been\nlargely the case in these tasks. Surprisingly, standard models such as the\nTransformer almost completely fail at generalizing inflection patterns when\nasked to inflect previously unseen lemmata -- i.e. under \"wug test\"-like\ncircumstances. While established data augmentation techniques can be employed\nto alleviate this shortcoming by introducing a copying bias through\nhallucinating synthetic new word forms using the alphabet in the language at\nhand, we show that, to be more effective, the hallucination process needs to\npay attention to substrings of syllable-like length rather than individual\ncharacters or stems. We report a significant performance improvement with our\nsubstring-based hallucination model over previous data hallucination methods\nwhen training and test data do not overlap in their lemmata.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:51:21 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Liu", "Ling", ""], ["Hulden", "Mans", ""]]}, {"id": "2104.06486", "submitter": "Jay DeYoung", "authors": "Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, Lucy Lu\n  Wang", "title": "MS2: Multi-Document Summarization of Medical Studies", "comments": "8 pages of content, 20 pages including references and appendix. See\n  https://github.com/allenai/ms2/ for code,\n  https://ai2-s2-ms2.s3-us-west-2.amazonaws.com/ms_data_2021-04-12.zip for data\n  (1.8G, zipped)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To assess the effectiveness of any medical intervention, researchers must\nconduct a time-intensive and highly manual literature review. NLP systems can\nhelp to automate or assist in parts of this expensive process. In support of\nthis goal, we release MS^2 (Multi-Document Summarization of Medical Studies), a\ndataset of over 470k documents and 20k summaries derived from the scientific\nliterature. This dataset facilitates the development of systems that can assess\nand aggregate contradictory evidence across multiple studies, and is the first\nlarge-scale, publicly available multi-document summarization dataset in the\nbiomedical domain. We experiment with a summarization system based on BART,\nwith promising early results. We formulate our summarization inputs and targets\nin both free text and structured forms and modify a recently proposed metric to\nassess the quality of our system's generated summaries. Data and models are\navailable at https://github.com/allenai/ms2\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:59:34 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 16:09:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["DeYoung", "Jay", ""], ["Beltagy", "Iz", ""], ["van Zuylen", "Madeleine", ""], ["Kuehl", "Bailey", ""], ["Wang", "Lucy Lu", ""]]}, {"id": "2104.06511", "submitter": "Liwei Jiang", "authors": "Liwei Jiang, Antoine Bosselut, Chandra Bhagavatula, Yejin Choi", "title": "\"I'm Not Mad\": Commonsense Implications of Negation and Contradiction", "comments": "Camera Ready Version for NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language inference requires reasoning about contradictions,\nnegations, and their commonsense implications. Given a simple premise (e.g.,\n\"I'm mad at you\"), humans can reason about the varying shades of contradictory\nstatements ranging from straightforward negations (\"I'm not mad at you\") to\ncommonsense contradictions (\"I'm happy\"). Moreover, these negated or\ncontradictory statements shift the commonsense implications of the original\npremise in nontrivial ways. For example, while \"I'm mad\" implies \"I'm unhappy\nabout something,\" negating the premise (i.e., \"I'm not mad\") does not\nnecessarily negate the corresponding commonsense implications.\n  In this paper, we present the first comprehensive study focusing on\ncommonsense implications of negated statements and contradictions. We introduce\nANION1, a new commonsense knowledge graph with 624K if-then rules focusing on\nnegated and contradictory events. We then present joint generative and\ndiscriminative inference models for this new resource, providing novel\nempirical insights on how logical negations and commonsense contradictions\nreshape the commonsense implications of their original premises.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:51:46 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 17:22:58 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jiang", "Liwei", ""], ["Bosselut", "Antoine", ""], ["Bhagavatula", "Chandra", ""], ["Choi", "Yejin", ""]]}, {"id": "2104.06529", "submitter": "Rafael Ferreira", "authors": "Rafael Ferreira, David Semedo, Joao Magalhaes", "title": "BERT Embeddings Can Track Context in Conversational Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of conversational assistants to search for information is becoming\nincreasingly more popular among the general public, pushing the research\ntowards more advanced and sophisticated techniques. In the last few years, in\nparticular, the interest in conversational search is increasing, not only\nbecause of the generalization of conversational assistants but also because\nconversational search is a step forward in allowing a more natural interaction\nwith the system.\n  In this work, the focus is on exploring the context present of the\nconversation via the historical utterances and respective embeddings with the\naim of developing a conversational search system that helps people search for\ninformation in a natural way. In particular, this system must be able to\nunderstand the context where the question is posed, tracking the current state\nof the conversation and detecting mentions to previous questions and answers.\nWe achieve this by using a context-tracking component based on neural\nquery-rewriting models. Another crucial aspect of the system is to provide the\nmost relevant answers given the question and the conversational history. To\nachieve this objective, we used a Transformer-based re-ranking method and\nexpanded this architecture to use the conversational context.\n  The results obtained with the system developed showed the advantages of using\nthe context present in the natural language utterances and in the neural\nembeddings generated throughout the conversation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 22:02:24 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ferreira", "Rafael", ""], ["Semedo", "David", ""], ["Magalhaes", "Joao", ""]]}, {"id": "2104.06541", "submitter": "Jianing Zhou", "authors": "Jianing Zhou, Hongyu Gong, Srihari Nanniyur, Suma Bhat", "title": "From Solving a Problem Boldly to Cutting the Gordian Knot: Idiomatic\n  Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new application for text generation -- idiomatic sentence\ngeneration -- which aims to transfer literal phrases in sentences into their\nidiomatic counterparts. Inspired by psycholinguistic theories of idiom use in\none's native language, we propose a novel approach for this task, which\nretrieves the appropriate idiom for a given literal sentence, extracts the span\nof the sentence to be replaced by the idiom, and generates the idiomatic\nsentence by using a neural model to combine the retrieved idiom and the\nremainder of the sentence. Experiments on a novel dataset created for this task\nshow that our model is able to effectively transfer literal sentences into\nidiomatic ones. Furthermore, automatic and human evaluations show that for this\ntask, the proposed model outperforms a series of competitive baseline models\nfor text generation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 22:57:25 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 01:05:06 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 01:53:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhou", "Jianing", ""], ["Gong", "Hongyu", ""], ["Nanniyur", "Srihari", ""], ["Bhat", "Suma", ""]]}, {"id": "2104.06546", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov, Jeremy Barnes, Erik Velldal, Lilja {\\O}vrelid, Stephan\n  Oepen", "title": "Large-Scale Contextualised Language Modelling for Norwegian", "comments": "Accepted to NoDaLiDa'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the ongoing NorLM initiative to support the creation and use of\nvery large contextualised language models for Norwegian (and in principle other\nNordic languages), including a ready-to-use software environment, as well as an\nexperience report for data preparation and training. This paper introduces the\nfirst large-scale monolingual language models for Norwegian, based on both the\nELMo and BERT frameworks. In addition to detailing the training process, we\npresent contrastive benchmark results on a suite of NLP tasks for Norwegian.\nFor additional background and access to the data, models, and software, please\nsee http://norlm.nlpl.eu\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:18:04 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Barnes", "Jeremy", ""], ["Velldal", "Erik", ""], ["\u00d8vrelid", "Lilja", ""], ["Oepen", "Stephan", ""]]}, {"id": "2104.06552", "submitter": "Jennifer Healey", "authors": "Victor S. Bursztyn (1), Jennifer Healey (2), Eunyee Koh (2), Nedim\n  Lipka (2), Larry Birnbaum (1) ((1) Northwestern University, (2) Adobe)", "title": "Developing a Conversational Recommendation System for Navigating Limited\n  Options", "comments": "7 pages, 4 figures, to appear in CHI 2021 as a Late Breaking Work,\n  see \"https://chi2021.acm.org/\"", "journal-ref": null, "doi": "10.1145/3411763.3451596", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a conversational recommendation system designed to help\nusers navigate through a set of limited options to find the best choice. Unlike\nmany internet scale systems that use a singular set of search terms and return\na ranked list of options from amongst thousands, our system uses multi-turn\nuser dialog to deeply understand the users preferences. The system responds in\ncontext to the users specific and immediate feedback to make sequential\nrecommendations. We envision our system would be highly useful in situations\nwith intrinsic constraints, such as finding the right restaurant within walking\ndistance or the right retail item within a limited inventory. Our research\nprototype instantiates the former use case, leveraging real data from Google\nPlaces, Yelp, and Zomato. We evaluated our system against a similar system that\ndid not incorporate user feedback in a 16 person remote study, generating 64\nscenario-based search journeys. When our recommendation system was successfully\ntriggered, we saw both an increase in efficiency and a higher confidence rating\nwith respect to final user choice. We also found that users preferred our\nsystem (75%) compared with the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:46:10 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bursztyn", "Victor S.", "", "Northwestern University"], ["Healey", "Jennifer", "", "Adobe"], ["Koh", "Eunyee", "", "Adobe"], ["Lipka", "Nedim", "", "Adobe"], ["Birnbaum", "Larry", "", "Northwestern University"]]}, {"id": "2104.06555", "submitter": "Dominic Widdows", "authors": "Dominic Widdows, Kristen Howell, Trevor Cohen", "title": "Should Semantic Vector Composition be Explicit? Can it be Linear?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector representations have become a central element in semantic language\nmodelling, leading to mathematical overlaps with many fields including quantum\ntheory. Compositionality is a core goal for such representations: given\nrepresentations for 'wet' and 'fish', how should the concept 'wet fish' be\nrepresented?\n  This position paper surveys this question from two points of view. The first\nconsiders the question of whether an explicit mathematical representation can\nbe successful using only tools from within linear algebra, or whether other\nmathematical tools are needed. The second considers whether semantic vector\ncomposition should be explicitly described mathematically, or whether it can be\na model-internal side-effect of training a neural network.\n  A third and newer question is whether a compositional model can be\nimplemented on a quantum computer. Given the fundamentally linear nature of\nquantum mechanics, we propose that these questions are related, and that this\nsurvey may help to highlight candidate operations for future quantum\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:58:26 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 19:14:03 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 02:44:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Widdows", "Dominic", ""], ["Howell", "Kristen", ""], ["Cohen", "Trevor", ""]]}, {"id": "2104.06591", "submitter": "Muhammad Khalifa", "authors": "Muhammad Khalifa and Hesham Hassan and Aly Fahmy", "title": "Zero-Resource Multi-Dialectal Arabic Natural Language Understanding", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2021.0120369", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reasonable amount of annotated data is required for fine-tuning pre-trained\nlanguage models (PLM) on downstream tasks. However, obtaining labeled examples\nfor different language varieties can be costly. In this paper, we investigate\nthe zero-shot performance on Dialectal Arabic (DA) when fine-tuning a PLM on\nmodern standard Arabic (MSA) data only -- identifying a significant performance\ndrop when evaluating such models on DA. To remedy such performance drop, we\npropose self-training with unlabeled DA data and apply it in the context of\nnamed entity recognition (NER), part-of-speech (POS) tagging, and sarcasm\ndetection (SRD) on several DA varieties. Our results demonstrate the\neffectiveness of self-training with unlabeled DA data: improving zero-shot\nMSA-to-DA transfer by as large as \\texttildelow 10\\% F$_1$ (NER), 2\\% accuracy\n(POS tagging), and 4.5\\% F$_1$ (SRD). We conduct an ablation experiment and\nshow that the performance boost observed directly results from the unlabeled DA\nexamples used for self-training. Our work opens up opportunities for leveraging\nthe relatively abundant labeled MSA datasets to develop DA models for zero and\nlow-resource dialects. We also report new state-of-the-art performance on all\nthree tasks and open-source our fine-tuned models for the research community.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:29:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Khalifa", "Muhammad", ""], ["Hassan", "Hesham", ""], ["Fahmy", "Aly", ""]]}, {"id": "2104.06598", "submitter": "Wanjun Zhong", "authors": "Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang,\n  Jian Yin, Ming Zhou, Nan Duan", "title": "AR-LSAT: Investigating Analytical Reasoning of Text", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytical reasoning is an essential and challenging task that requires a\nsystem to analyze a scenario involving a set of particular circumstances and\nperform reasoning over it to make conclusions. In this paper, we study the\nchallenge of analytical reasoning of text and introduce a new dataset\nconsisting of questions from the Law School Admission Test from 1991 to 2016.\nWe analyze what knowledge understanding and reasoning abilities are required to\ndo well on this task. Furthermore, to address this reasoning challenge, we\ndesign two different baselines: (1) a Transformer-based method which leverages\nthe state-of-the-art pre-trained language models and (2) Analytical Reasoning\nMachine (ARM), a logical-level reasoning framework extracting symbolic\nknowledge (e.g, participants, facts, logical functions) to deduce legitimate\nsolutions. In our experiments, we find that the Transformer-based models\nstruggle to solve this task as their performance is close to random guess and\nARM achieves better performance by leveraging symbolic knowledge and\ninterpretable reasoning steps. Results show that both methods still lag far\nbehind human performance, which leave further space for future research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:53:32 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 02:21:45 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhong", "Wanjun", ""], ["Wang", "Siyuan", ""], ["Tang", "Duyu", ""], ["Xu", "Zenan", ""], ["Guo", "Daya", ""], ["Wang", "Jiahai", ""], ["Yin", "Jian", ""], ["Zhou", "Ming", ""], ["Duan", "Nan", ""]]}, {"id": "2104.06599", "submitter": "Guanghui Qin", "authors": "Guanghui Qin, Jason Eisner", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "comments": "NAACL-HLT 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural-language prompts have recently been used to coax pretrained language\nmodels into performing other AI tasks, using a fill-in-the-blank paradigm\n(Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,\n2020). For example, language models retain factual knowledge from their\ntraining corpora that can be extracted by asking them to \"fill in the blank\" in\na sentential prompt. However, where does this prompt come from? We explore the\nidea of learning prompts by gradient descent -- either fine-tuning prompts\ntaken from previous work, or starting from random initialization. Our prompts\nconsist of \"soft words,\" i.e., continuous vectors that are not necessarily word\ntype embeddings from the language model. Furthermore, for each task, we\noptimize a mixture of prompts, learning which prompts are most effective and\nhow to ensemble them. Across multiple English LMs and tasks, our approach\nhugely outperforms previous methods, showing that the implicit factual\nknowledge in language models was previously underestimated. Moreover, this\nknowledge is cheap to elicit: random initialization is nearly as good as\ninformed initialization.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:56:14 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Qin", "Guanghui", ""], ["Eisner", "Jason", ""]]}, {"id": "2104.06644", "submitter": "Koustuv Sinha", "authors": "Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina\n  Williams, Douwe Kiela", "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word\n  Matters Pre-training for Little", "comments": "12 pages + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks -- including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 06:30:36 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sinha", "Koustuv", ""], ["Jia", "Robin", ""], ["Hupkes", "Dieuwke", ""], ["Pineau", "Joelle", ""], ["Williams", "Adina", ""], ["Kiela", "Douwe", ""]]}, {"id": "2104.06645", "submitter": "Leon Bergen", "authors": "Leon Bergen, Dzmitry Bahdanau, Timothy J. O'Donnell", "title": "Jointly Learning Truth-Conditional Denotations and Groundings using\n  Parallel Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a model that jointly learns the denotations of words together with\ntheir groundings using a truth-conditional semantics. Our model builds on the\nneurosymbolic approach of Mao et al. (2019), learning to ground objects in the\nCLEVR dataset (Johnson et al., 2017) using a novel parallel attention\nmechanism. The model achieves state of the art performance on visual question\nanswering, learning to detect and ground objects with question performance as\nthe only training signal. We also show that the model is able to learn flexible\nnon-canonical groundings just by adjusting answers to questions in the training\nset.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 06:33:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bergen", "Leon", ""], ["Bahdanau", "Dzmitry", ""], ["O'Donnell", "Timothy J.", ""]]}, {"id": "2104.06669", "submitter": "Varun Gangal", "authors": "Varun Gangal, Steven Y. Feng, Eduard Hovy, Teruko Mitamura", "title": "NAREOR: The Narrative Reordering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the task of Narrative Reordering(NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot,\nsemantic, and temporal aspects. We present a dataset, NAREORC, with over 1000\nhuman rewritings of stories within ROCStories in non-linear orders, and conduct\na detailed analysis of it. Further, we propose novel initial task-specific\ntraining methods and evaluation metrics. We perform experiments on NAREORC\nusing GPT-2 and Transformer models and conduct an extensive human evaluation.\nWe demonstrate that NAREOR is a challenging task with potential for further\nexploration.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:33:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Gangal", "Varun", ""], ["Feng", "Steven Y.", ""], ["Hovy", "Eduard", ""], ["Mitamura", "Teruko", ""]]}, {"id": "2104.06678", "submitter": "Alexis Conneau", "authors": "Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli,\n  Alexis Conneau", "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we improve speech translation (ST) through effectively\nleveraging large quantities of unlabeled speech and text data in different and\ncomplementary ways. We explore both pretraining and self-training by using the\nlarge Libri-Light speech audio corpus and language modeling with CommonCrawl.\nOur experiments improve over the previous state of the art by 2.6 BLEU on\naverage on all four considered CoVoST 2 language pairs via a simple recipe of\ncombining wav2vec 2.0 pretraining, a single iteration of self-training and\ndecoding with a language model. Different to existing work, our approach does\nnot leverage any other supervision than ST data. Code and models will be\npublicly released.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:44:52 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Changhan", ""], ["Wu", "Anne", ""], ["Pino", "Juan", ""], ["Baevski", "Alexei", ""], ["Auli", "Michael", ""], ["Conneau", "Alexis", ""]]}, {"id": "2104.06683", "submitter": "Vikas Raunak", "authors": "Vikas Raunak, Arul Menezes and Marcin Junczys-Dowmunt", "title": "The Curious Case of Hallucinations in Neural Machine Translation", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study hallucinations in Neural Machine Translation (NMT),\nwhich lie at an extreme end on the spectrum of NMT pathologies. Firstly, we\nconnect the phenomenon of hallucinations under source perturbation to the\nLong-Tail theory of Feldman (2020), and present an empirically validated\nhypothesis that explains hallucinations under source perturbation. Secondly, we\nconsider hallucinations under corpus-level noise (without any source\nperturbation) and demonstrate that two prominent types of natural\nhallucinations (detached and oscillatory outputs) could be generated and\nexplained through specific corpus-level noise patterns. Finally, we elucidate\nthe phenomenon of hallucination amplification in popular data-generation\nprocesses such as Backtranslation and sequence-level Knowledge Distillation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:09:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Raunak", "Vikas", ""], ["Menezes", "Arul", ""], ["Junczys-Dowmunt", "Marcin", ""]]}, {"id": "2104.06709", "submitter": "Damian Pascual", "authors": "Damian Pascual, Sandro Luck, Roger Wattenhofer", "title": "Towards BERT-based Automatic ICD Coding: Limitations and Opportunities", "comments": "Accepted at BioNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic ICD coding is the task of assigning codes from the International\nClassification of Diseases (ICD) to medical notes. These codes describe the\nstate of the patient and have multiple applications, e.g., computer-assisted\ndiagnosis or epidemiological studies. ICD coding is a challenging task due to\nthe complexity and length of medical notes. Unlike the general trend in\nlanguage processing, no transformer model has been reported to reach high\nperformance on this task. Here, we investigate in detail ICD coding using\nPubMedBERT, a state-of-the-art transformer model for biomedical language\nunderstanding. We find that the difficulty of fine-tuning the model on long\npieces of text is the main limitation for BERT-based models on ICD coding. We\nrun extensive experiments and show that despite the gap with current\nstate-of-the-art, pretrained transformers can reach competitive performance\nusing relatively small portions of text. We point at better methods to\naggregate information from long texts as the main need for improving BERT-based\nICD coding.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:12:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Pascual", "Damian", ""], ["Luck", "Sandro", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2104.06719", "submitter": "Fredrik Carlsson", "authors": "Fredrik Carlsson Magnus Sahlgren", "title": "Sentence Embeddings by Ensemble Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper contributes a new State Of The Art (SOTA) for Semantic Textual\nSimilarity (STS). We compare and combine a number of recently proposed sentence\nembedding methods for STS, and propose a novel and simple ensemble knowledge\ndistillation scheme that improves on previous approaches. Our experiments\ndemonstrate that a model trained to learn the average embedding space from\nmultiple ensemble students outperforms all the other individual models with\nhigh robustness. Utilizing our distillation method in combination with previous\nmethods, we significantly improve on the SOTA unsupervised STS, and by proper\nhyperparameter tuning of previous methods we improve the supervised SOTA\nscores.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:23:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sahlgren", "Fredrik Carlsson Magnus", ""]]}, {"id": "2104.06722", "submitter": "Oishik Chatterjee", "authors": "Oishik Chatterjee, Aashish Waikar, Vishwajeet Kumar, Ganesh\n  Ramakrishnan, Kavi Arya", "title": "A Weakly Supervised Model for Solving Math word Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving math word problems (MWPs) is an important and challenging problem in\nnatural language processing. Existing approaches to solve MWPs require full\nsupervision in the form of intermediate equations. However, labeling every math\nword problem with its corresponding equations is a time-consuming and expensive\ntask. In order to address this challenge of equation annotation, we propose a\nweakly supervised model for solving math word problems by requiring only the\nfinal answer as supervision. We approach this problem by first learning to\ngenerate the equation using the problem description and the final answer, which\nwe then use to train a supervised MWP solver. We propose and compare various\nweakly supervised techniques to learn to generate equations directly from the\nproblem description and answer. Through extensive experiment, we demonstrate\nthat even without using equations for supervision, our approach achieves an\naccuracy of 56.0 on the standard Math23K dataset. We also curate and release a\nnew dataset for MWPs in English consisting of 10227 instances suitable for\ntraining weakly supervised models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:25:38 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chatterjee", "Oishik", ""], ["Waikar", "Aashish", ""], ["Kumar", "Vishwajeet", ""], ["Ramakrishnan", "Ganesh", ""], ["Arya", "Kavi", ""]]}, {"id": "2104.06737", "submitter": "Gregor Betz", "authors": "Gregor Betz", "title": "Natural-Language Multi-Agent Simulations of Argumentative Opinion\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.MA cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper develops a natural-language agent-based model of argumentation\n(ABMA). Its artificial deliberative agents (ADAs) are constructed with the help\nof so-called neural language models recently developed in AI and computational\nlinguistics. ADAs are equipped with a minimalist belief system and may generate\nand submit novel contributions to a conversation. The natural-language ABMA\nallows us to simulate collective deliberation in English, i.e. with arguments,\nreasons, and claims themselves -- rather than with their mathematical\nrepresentations (as in formal models). This paper uses the natural-language\nABMA to test the robustness of formal reason-balancing models of argumentation\n[Maes & Flache 2013, Singer et al. 2019]: First of all, as long as ADAs remain\npassive, confirmation bias and homophily updating trigger polarization, which\nis consistent with results from formal models. However, once ADAs start to\nactively generate new contributions, the evolution of a conservation is\ndominated by properties of the agents *as authors*. This suggests that the\ncreation of new arguments, reasons, and claims critically affects a\nconversation and is of pivotal importance for understanding the dynamics of\ncollective deliberation. The paper closes by pointing out further fruitful\napplications of the model and challenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:45:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Betz", "Gregor", ""]]}, {"id": "2104.06793", "submitter": "Tomoki Hayashi", "authors": "Tomoki Hayashi, Wen-Chin Huang, Kazuhiro Kobayashi, Tomoki Toda", "title": "Non-autoregressive sequence-to-sequence voice conversion", "comments": "Accepted to ICASSP2021. Demo HP:\n  https://kan-bayashi.github.io/NonARSeq2SeqVC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel voice conversion (VC) method based on\nnon-autoregressive sequence-to-sequence (NAR-S2S) models. Inspired by the great\nsuccess of NAR-S2S models such as FastSpeech in text-to-speech (TTS), we extend\nthe FastSpeech2 model for the VC problem. We introduce the\nconvolution-augmented Transformer (Conformer) instead of the Transformer,\nmaking it possible to capture both local and global context information from\nthe input sequence. Furthermore, we extend variance predictors to variance\nconverters to explicitly convert the source speaker's prosody components such\nas pitch and energy into the target speaker. The experimental evaluation with\nthe Japanese speaker dataset, which consists of male and female speakers of\n1,000 utterances, demonstrates that the proposed model enables us to perform\nmore stable, faster, and better conversion than autoregressive S2S (AR-S2S)\nmodels such as Tacotron2 and Transformer.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:53:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hayashi", "Tomoki", ""], ["Huang", "Wen-Chin", ""], ["Kobayashi", "Kazuhiro", ""], ["Toda", "Tomoki", ""]]}, {"id": "2104.06828", "submitter": "Bodhisattwa Prasad Majumder", "authors": "Bodhisattwa Prasad Majumder, Sudha Rao, Michel Galley, Julian McAuley", "title": "Ask what's missing and what's useful: Improving Clarification Question\n  Generation using Global Knowledge", "comments": "Accepted in NAACL 2021, Code is available at\n  https://github.com/microsoft/clarification-qgen-globalinfo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to generate clarification questions i.e., questions that identify\nuseful missing information in a given context, is important in reducing\nambiguity. Humans use previous experience with similar contexts to form a\nglobal view and compare it to the given context to ascertain what is missing\nand what is useful in the context. Inspired by this, we propose a model for\nclarification question generation where we first identify what is missing by\ntaking a difference between the global and the local view and then train a\nmodel to identify what is useful and generate a question about it. Our model\noutperforms several baselines as judged by both automatic metrics and humans.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:59:08 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Majumder", "Bodhisattwa Prasad", ""], ["Rao", "Sudha", ""], ["Galley", "Michel", ""], ["McAuley", "Julian", ""]]}, {"id": "2104.06835", "submitter": "Yixuan Zhou", "authors": "Yixuan Zhou, Changhe Song, Jingbei Li, Zhiyong Wu, Helen Meng", "title": "Dependency Parsing based Semantic Representation Learning with Graph\n  Neural Network for Enhancing Expressiveness of Text-to-Speech", "comments": "5 pages, submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic information of a sentence is crucial for improving the\nexpressiveness of a text-to-speech (TTS) system, but can not be well learned\nfrom the limited training TTS dataset just by virtue of the nowadays encoder\nstructures. As large scale pre-trained text representation develops,\nbidirectional encoder representations from transformers (BERT) has been proven\nto embody text-context semantic information and applied to TTS as additional\ninput. However BERT can not explicitly associate semantic tokens from point of\ndependency relations in a sentence. In this paper, to enhance expressiveness,\nwe propose a semantic representation learning method based on graph neural\nnetwork, considering dependency relations of a sentence. Dependency graph of\ninput text is composed of edges from dependency tree structure considering both\nthe forward and the reverse directions. Semantic representations are then\nextracted at word level by the relational gated graph network (RGGN) fed with\nfeatures from BERT as nodes input. Upsampled semantic representations and\ncharacter-level embeddings are concatenated to serve as the encoder input of\nTacotron-2. Experimental results show that our proposed method outperforms the\nbaseline using vanilla BERT features both in LJSpeech and Blizzard Challenge\n2013 datasets, and semantic representations learned from the reverse direction\nare more effective for enhancing expressiveness.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:09:51 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 10:17:32 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhou", "Yixuan", ""], ["Song", "Changhe", ""], ["Li", "Jingbei", ""], ["Wu", "Zhiyong", ""], ["Meng", "Helen", ""]]}, {"id": "2104.06892", "submitter": "David Semedo", "authors": "Mariana Leite, Rafael Ferreira, David Semedo, Jo\\~ao Magalh\\~aes", "title": "Knowledge-driven Answer Generation for Conversational Search", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conversational search paradigm introduces a step change over the\ntraditional search paradigm by allowing users to interact with search agents in\na multi-turn and natural fashion. The conversation flows naturally and is\nusually centered around a target field of knowledge. In this work, we propose a\nknowledge-driven answer generation approach for open-domain conversational\nsearch, where a conversation-wide entities' knowledge graph is used to bias\nsearch-answer generation. First, a conversation-specific knowledge graph is\nextracted from the top passages retrieved with a Transformer-based re-ranker.\nThe entities knowledge-graph is then used to bias a search-answer generator\nTransformer towards information rich and concise answers. This conversation\nspecific bias is computed by identifying the most relevant passages according\nto the most salient entities of that particular conversation. Experiments show\nthat the proposed approach successfully exploits entities knowledge along the\nconversation, and outperforms a set of baselines on the search-answer\ngeneration task.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:35:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Leite", "Mariana", ""], ["Ferreira", "Rafael", ""], ["Semedo", "David", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}, {"id": "2104.06893", "submitter": "Danushka Bollegala", "authors": "James O'Neill and Polina Rozenshtein and Ryuichi Kiryo and Motoko\n  Kubota and Danushka Bollegala", "title": "I Wish I Would Have Loved This One, But I Didn't -- A Multilingual\n  Dataset for Counterfactual Detection in Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Counterfactual statements describe events that did not or cannot take place.\nWe consider the problem of counterfactual detection (CFD) in product reviews.\nFor this purpose, we annotate a multilingual CFD dataset from Amazon product\nreviews covering counterfactual statements written in English, German, and\nJapanese languages. The dataset is unique as it contains counterfactuals in\nmultiple languages, covers a new application area of e-commerce reviews, and\nprovides high quality professional annotations. We train CFD models using\ndifferent text representation methods and classifiers. We find that these\nmodels are robust against the selectional biases introduced due to cue\nphrase-based sentence selection. Moreover, our CFD dataset is compatible with\nprior datasets and can be merged to learn accurate CFD models. Applying machine\ntranslation on English counterfactual examples to create multilingual data\nperforms poorly, demonstrating the language-specificity of this problem, which\nhas been ignored so far.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:38:36 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["O'Neill", "James", ""], ["Rozenshtein", "Polina", ""], ["Kiryo", "Ryuichi", ""], ["Kubota", "Motoko", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2104.06901", "submitter": "Rohan Kumar Yadav", "authors": "Rohan Kumar Yadav, Lei Jiao, Ole-Christoffer Granmo, and Morten\n  Goodwin", "title": "Distributed Word Representation in Tsetlin Machine", "comments": "9 pages, 13 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic. The algorithm has demonstrated competitive performance\nin many Natural Language Processing (NLP) tasks, including sentiment analysis,\ntext classification, and Word Sense Disambiguation (WSD). To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the TM performance and maintains\ninterpretability at the same time. We achieve this by extracting semantically\nrelated words from pre-trained word representations as input features to the\nTM. Our experiments show that the accuracy of the proposed approach is\nsignificantly higher than the previous BOW-based TM, reaching the level of\nDNN-based models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:48:41 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yadav", "Rohan Kumar", ""], ["Jiao", "Lei", ""], ["Granmo", "Ole-Christoffer", ""], ["Goodwin", "Morten", ""]]}, {"id": "2104.06924", "submitter": "Jiaying Lu", "authors": "Jiaying Lu, Jinho D. Choi", "title": "Evaluation of Unsupervised Entity and Event Salience Estimation", "comments": null, "journal-ref": "Proceedings of the 34rd International Florida Artificial\n  Intelligence Research Society Conference, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salience Estimation aims to predict term importance in documents. Due to few\nexisting human-annotated datasets and the subjective notion of salience,\nprevious studies typically generate pseudo-ground truth for evaluation.\nHowever, our investigation reveals that the evaluation protocol proposed by\nprior work is difficult to replicate, thus leading to few follow-up studies\nexisting. Moreover, the evaluation process is problematic: the entity linking\ntool used for entity matching is very noisy, while the ignorance of event\nargument for event evaluation leads to boosted performance. In this work, we\npropose a light yet practical entity and event salience estimation evaluation\nprotocol, which incorporates the more reliable syntactic dependency parser.\nFurthermore, we conduct a comprehensive analysis among popular entity and event\ndefinition standards, and present our own definition for the Salience\nEstimation task to reduce noise during the pseudo-ground truth generation\nprocess. Furthermore, we construct dependency-based heterogeneous graphs to\ncapture the interactions of entities and events. The empirical results show\nthat both baseline methods and the novel GNN method utilizing the heterogeneous\ngraph consistently outperform the previous SOTA model in all proposed metrics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:23:08 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lu", "Jiaying", ""], ["Choi", "Jinho D.", ""]]}, {"id": "2104.06951", "submitter": "Danielle Saunders", "authors": "Danielle Saunders", "title": "Domain Adaptation and Multi-Domain Adaptation for Neural Machine\n  Translation: A Survey", "comments": "39 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of deep learning techniques has allowed Neural Machine\nTranslation (NMT) models to become extremely powerful, given sufficient\ntraining data and training time. However, systems struggle when translating\ntext from a new domain with a distinct style or vocabulary. Tuning on a\nrepresentative training corpus allows good in-domain translation, but such\ndata-centric approaches can cause over-fitting to new data and `catastrophic\nforgetting' of previously learned behaviour.\n  We concentrate on more robust approaches to domain adaptation for NMT,\nparticularly the case where a system may need to translate sentences from\nmultiple domains. We divide techniques into those relating to data selection,\nmodel architecture, parameter adaptation procedure, and inference procedure. We\nfinally highlight the benefits of domain adaptation and multi-domain adaptation\ntechniques to other lines of NMT research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:21:37 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Saunders", "Danielle", ""]]}, {"id": "2104.06952", "submitter": "Kellin Pelrine", "authors": "Kellin Pelrine, Jacob Danovitch, Reihaneh Rabbany", "title": "The Surprising Performance of Simple Baselines for Misinformation\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As social media becomes increasingly prominent in our day to day lives, it is\nincreasingly important to detect informative content and prevent the spread of\ndisinformation and unverified rumours. While many sophisticated and successful\nmodels have been proposed in the literature, they are often compared with older\nNLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the\nperformance of a broad set of modern transformer-based language models and show\nthat with basic fine-tuning, these models are competitive with and can even\nsignificantly outperform recently proposed state-of-the-art methods. We present\nour framework as a baseline for creating and evaluating new methods for\nmisinformation detection. We further study a comprehensive set of benchmark\ndatasets, and discuss potential data leakage and the need for careful design of\nthe experiments and understanding of datasets to account for confounding\nvariables. As an extreme case example, we show that classifying only based on\nthe first three digits of tweet ids, which contain information on the date,\ngives state-of-the-art performance on a commonly used benchmark dataset for\nfake news detection --Twitter16. We provide a simple tool to detect this\nproblem and suggest steps to mitigate it in future datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:25:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Pelrine", "Kellin", ""], ["Danovitch", "Jacob", ""], ["Rabbany", "Reihaneh", ""]]}, {"id": "2104.06960", "submitter": "Haoran Li", "authors": "Song Xu, Haoran Li, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He,\n  Ying Liu, Bowen Zhou", "title": "K-PLUG: Knowledge-injected Pre-trained Language Model for Natural\n  Language Understanding and Generation in E-Commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing pre-trained language models (PLMs) have demonstrated the\neffectiveness of self-supervised learning for a broad range of natural language\nprocessing (NLP) tasks. However, most of them are not explicitly aware of\ndomain-specific knowledge, which is essential for downstream tasks in many\ndomains, such as tasks in e-commerce scenarios. In this paper, we propose\nK-PLUG, a knowledge-injected pre-trained language model based on the\nencoder-decoder transformer that can be transferred to both natural language\nunderstanding and generation tasks. We verify our method in a diverse range of\ne-commerce scenarios that require domain-specific knowledge. Specifically, we\npropose five knowledge-aware self-supervised pre-training objectives to\nformulate the learning of domain-specific knowledge, including e-commerce\ndomain-specific knowledge-bases, aspects of product entities, categories of\nproduct entities, and unique selling propositions of product entities. K-PLUG\nachieves new state-of-the-art results on a suite of domain-specific NLP tasks,\nincluding product knowledge base completion, abstractive product summarization,\nand multi-turn dialogue, significantly outperforms baselines across the board,\nwhich demonstrates that the proposed method effectively learns a diverse set of\ndomain-specific knowledge for both language understanding and generation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:37:31 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Xu", "Song", ""], ["Li", "Haoran", ""], ["Yuan", "Peng", ""], ["Wang", "Yujia", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""], ["Liu", "Ying", ""], ["Zhou", "Bowen", ""]]}, {"id": "2104.06967", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin,\n  Allan Hanbury", "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling", "comments": "Accepted at SIGIR 2021 (Full Paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital step towards the widespread adoption of neural retrieval models is\ntheir resource efficiency throughout the training, indexing and query\nworkflows. The neural IR community made great advancements in training\neffective dual-encoder dense retrieval (DR) models recently. A dense text\nretrieval model uses a single vector representation per query and passage to\nscore a match, which enables low-latency first stage retrieval with a nearest\nneighbor search. Increasingly common, training approaches require enormous\ncompute power, as they either conduct negative passage sampling out of a\ncontinuously updating refreshing index or require very large batch sizes for\nin-batch negative sampling. Instead of relying on more compute capability, we\nintroduce an efficient topic-aware query and balanced margin sampling\ntechnique, called TAS-Balanced. We cluster queries once before training and\nsample queries out of a cluster per batch. We train our lightweight 6-layer DR\nmodel with a novel dual-teacher supervision that combines pairwise and in-batch\nnegative teachers. Our method is trainable on a single consumer-grade GPU in\nunder 48 hours (as opposed to a common configuration of 8x V100s). We show that\nour TAS-Balanced training method achieves state-of-the-art low-latency (64ms\nper query) results on two TREC Deep Learning Track query sets. Evaluated on\nNDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by\n11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces\nthe first dense retriever that outperforms every other method on recall at any\ncutoff on TREC-DL and allows more resource intensive re-ranking models to\noperate on fewer passages to improve results further.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:49:18 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 12:04:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Lin", "Jimmy", ""], ["Hanbury", "Allan", ""]]}, {"id": "2104.06969", "submitter": "Emanuela Boros", "authors": "Emanuela Boros, Jose G. Moreno, Antoine Doucet", "title": "Event Detection as Question Answering with Entity Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a recent and under-researched paradigm for the task\nof event detection (ED) by casting it as a question-answering (QA) problem with\nthe possibility of multiple answers and the support of entities. The extraction\nof event triggers is, thus, transformed into the task of identifying answer\nspans from a context, while also focusing on the surrounding entities. The\narchitecture is based on a pre-trained and fine-tuned language model, where the\ninput context is augmented with entities marked at different levels, their\npositions, their types, and, finally, the argument roles. Experiments on the\nACE~2005 corpus demonstrate that the proposed paradigm is a viable solution for\nthe ED task and it significantly outperforms the state-of-the-art models.\nMoreover, we prove that our methods are also able to extract unseen event\ntypes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:53:11 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Boros", "Emanuela", ""], ["Moreno", "Jose G.", ""], ["Doucet", "Antoine", ""]]}, {"id": "2104.06973", "submitter": "Sugam Garg", "authors": "Haswanth Aekula, Sugam Garg, Animesh Gupta", "title": "[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias\n  Mitigation", "comments": "Under review at ML Reproducibility Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite widespread use in natural language processing (NLP) tasks, word\nembeddings have been criticized for inheriting unintended gender bias from\ntraining corpora. programmer is more closely associated with man and homemaker\nis more closely associated with woman. Such gender bias has also been shown to\npropagate in downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:56:14 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Aekula", "Haswanth", ""], ["Garg", "Sugam", ""], ["Gupta", "Animesh", ""]]}, {"id": "2104.06979", "submitter": "Kexin Wang", "authors": "Kexin Wang, Nils Reimers, Iryna Gurevych", "title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for\n  Unsupervised Sentence Embedding Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning sentence embeddings often requires large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong pre-training method for\nlearning sentence embeddings, significantly outperforming other approaches like\nMasked Language Model.\n  A crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:02:18 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Kexin", ""], ["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.06983", "submitter": "Dumitru-Clementin Cercel", "authors": "George-Eduard Zaharia, Dumitru-Clementin Cercel, Mihai Dascalu", "title": "UPB at SemEval-2021 Task 1: Combining Deep Learning and Hand-Crafted\n  Features for Lexical Complexity Prediction", "comments": "8 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading is a complex process which requires proper understanding of texts in\norder to create coherent mental representations. However, comprehension\nproblems may arise due to hard-to-understand sections, which can prove\ntroublesome for readers, while accounting for their specific language skills.\nAs such, steps towards simplifying these sections can be performed, by\naccurately identifying and evaluating difficult structures. In this paper, we\ndescribe our approach for the SemEval-2021 Task 1: Lexical Complexity\nPrediction competition that consists of a mixture of advanced NLP techniques,\nnamely Transformer-based language models, pre-trained word embeddings, Graph\nConvolutional Networks, Capsule Networks, as well as a series of hand-crafted\ntextual complexity features. Our models are applicable on both subtasks and\nachieve good performance results, with a MAE below 0.07 and a Person\ncorrelation of .73 for single word identification, as well as a MAE below 0.08\nand a Person correlation of .79 for multiple word targets. Our results are just\n5.46% and 6.5% lower than the top scores obtained in the competition on the\nfirst and the second subtasks, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:05:46 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zaharia", "George-Eduard", ""], ["Cercel", "Dumitru-Clementin", ""], ["Dascalu", "Mihai", ""]]}, {"id": "2104.06987", "submitter": "Brett Drury", "authors": "Brett Drury and Samuel Morais Drury", "title": "An Update to the Minho Quotation Resource", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Minho Quotation Resource was originally released in 2012. It provided\napproximately 500,000 quotes from business leaders, analysts and politicians\nthat spanned the period from 2008 to 2012. The original resource had several\nfailings which include a large number of missing job titles and affiliations as\nwell as unnormalised job titles which produced a large variation in spellings\nand formats of the same employment position. Also, there were numerous\nduplicate posts. This update has standardised the job title text as well as the\nimputation of missing job titles and affiliations. Duplicate quotes have been\ndeleted. This update also provides some metaphor and simile extraction as well\nas an emotion distribution of the quotes. This update has also replaced an\nantiquated version of Lucene index with a JSONL format as well as a rudimentary\ninterface that can query the data supplied with the resource. It is hoped that\nthis update will encourage the study of business communication in a time of a\nfinancial crisis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:14:39 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Drury", "Brett", ""], ["Drury", "Samuel Morais", ""]]}, {"id": "2104.06999", "submitter": "Sayan Ghosh", "authors": "Sayan Ghosh, Dylan Baker, David Jurgens, Vinodkumar Prabhakaran", "title": "Cross-geographic Bias Detection in Toxicity Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online social media platforms increasingly rely on Natural Language\nProcessing (NLP) techniques to detect abusive content at scale in order to\nmitigate the harms it causes to their users. However, these techniques suffer\nfrom various sampling and association biases present in training data, often\nresulting in sub-par performance on content relevant to marginalized groups,\npotentially furthering disproportionate harms towards them. Studies on such\nbiases so far have focused on only a handful of axes of disparities and\nsubgroups that have annotations/lexicons available. Consequently, biases\nconcerning non-Western contexts are largely ignored in the literature. In this\npaper, we introduce a weakly supervised method to robustly detect lexical\nbiases in broader geocultural contexts. Through a case study on\ncross-geographic toxicity detection, we demonstrate that our method identifies\nsalient groups of errors, and, in a follow up, demonstrate that these groupings\nreflect human judgments of offensive and inoffensive language in those\ngeographic contexts.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:32:05 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ghosh", "Sayan", ""], ["Baker", "Dylan", ""], ["Jurgens", "David", ""], ["Prabhakaran", "Vinodkumar", ""]]}, {"id": "2104.07000", "submitter": "Simeng Sun", "authors": "Simeng Sun, Wenlong Zhao, Varun Manjunatha, Rajiv Jain, Vlad Morariu,\n  Franck Dernoncourt, Balaji Vasan Srinivasan, Mohit Iyyer", "title": "IGA : An Intent-Guided Authoring Assistant", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large-scale pretrained language models have significantly improved\nwriting assistance functionalities such as autocomplete, more complex and\ncontrollable writing assistants have yet to be explored. We leverage advances\nin language modeling to build an interactive writing assistant that generates\nand rephrases text according to fine-grained author specifications. Users\nprovide input to our Intent-Guided Assistant (IGA) in the form of text\ninterspersed with tags that correspond to specific rhetorical directives (e.g.,\nadding description or contrast, or rephrasing a particular sentence). We\nfine-tune a language model on a dataset heuristically-labeled with author\nintent, which allows IGA to fill in these tags with generated text that users\ncan subsequently edit to their liking. A series of automatic and crowdsourced\nevaluations confirm the quality of IGA's generated outputs, while a small-scale\nuser study demonstrates author preference for IGA over baseline methods in a\ncreative writing task. We release our dataset, code, and demo to spur further\nresearch into AI-assisted writing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:32:21 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sun", "Simeng", ""], ["Zhao", "Wenlong", ""], ["Manjunatha", "Varun", ""], ["Jain", "Rajiv", ""], ["Morariu", "Vlad", ""], ["Dernoncourt", "Franck", ""], ["Srinivasan", "Balaji Vasan", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2104.07010", "submitter": "Adri\\'an Bazaga", "authors": "Adri\\'an Bazaga and Nupur Gunwant and Gos Micklem", "title": "Translating synthetic natural language to database queries: a polyglot\n  deep learning framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The number of databases as well as their size and complexity is increasing.\nThis creates a barrier to use especially for non-experts, who have to come to\ngrips with the nature of the data, the way it has been represented in the\ndatabase, and the specific query languages or user interfaces by which data are\naccessed. These difficulties worsen in research settings, where it is common to\nwork with many different databases. One approach to improving this situation is\nto allow users to pose their queries in natural language.\n  In this work we describe a machine learning framework, Polyglotter, that in a\ngeneral way supports the mapping of natural language searches to database\nqueries. Importantly, it does not require the creation of manually annotated\ndata for training and therefore can be applied easily to multiple domains. The\nframework is polyglot in the sense that it supports multiple different database\nengines that are accessed with a variety of query languages, including SQL and\nCypher. Furthermore Polyglotter also supports multi-class queries.\n  Our results indicate that our framework performs well on both synthetic and\nreal databases, and may provide opportunities for database maintainers to\nimprove accessibility to their resources.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:43:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bazaga", "Adri\u00e1n", ""], ["Gunwant", "Nupur", ""], ["Micklem", "Gos", ""]]}, {"id": "2104.07012", "submitter": "Biao Zhang", "authors": "Biao Zhang, Ivan Titov, Rico Sennrich", "title": "Sparse Attention with Linear Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:52:38 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhang", "Biao", ""], ["Titov", "Ivan", ""], ["Sennrich", "Rico", ""]]}, {"id": "2104.07058", "submitter": "Wen Xiao", "authors": "Wen Xiao, Patrick Huber, Giuseppe Carenini", "title": "Predicting Discourse Trees from Transformer-based Neural Summarizers", "comments": "14 pages, accepted by NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work indicates that discourse information benefits summarization. In\nthis paper, we explore whether this synergy between discourse and summarization\nis bidirectional, by inferring document-level discourse trees from pre-trained\nneural summarizers. In particular, we generate unlabeled RST-style discourse\ntrees from the self-attention matrices of the transformer model. Experiments\nacross models and datasets reveal that the summarizer learns both, dependency-\nand constituency-style discourse information, which is typically encoded in a\nsingle head, covering long- and short-distance discourse dependencies. Overall,\nthe experimental results suggest that the learned discourse information is\ngeneral and transferable inter-domain.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:10:05 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Xiao", "Wen", ""], ["Huber", "Patrick", ""], ["Carenini", "Giuseppe", ""]]}, {"id": "2104.07064", "submitter": "Somnath Basu Roy Chowdhury", "authors": "Somnath Basu Roy Chowdhury, Faeze Brahman, Snigdha Chaturvedi", "title": "Reformulating Sentence Ordering as Conditional Text Generation", "comments": "Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of organizing a shuffled set of sentences into a coherent text is\nimportant in NLP and has been used to evaluate a machine's understanding of\ncausal and temporal relations. We present Reorder-BART (RE-BART), a sentence\nordering framework which leverages a pre-trained transformer-based model to\nidentify a coherent order for a given set of shuffled sentences. We reformulate\nthe task as a conditional text-to-marker generation setup where the input is a\nset of shuffled sentences with sentence-specific markers and output is a\nsequence of position markers of the ordered text. Our framework achieves the\nstate-of-the-art performance across six datasets in Perfect Match Ratio (PMR)\nand Kendall's tau ($\\tau$) metric. We perform evaluations in a zero-shot\nsetting, showcasing that our model is able to generalize well across other\ndatasets. We additionally perform a series of experiments to understand the\nfunctioning and explore the limitations of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:16:47 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chowdhury", "Somnath Basu Roy", ""], ["Brahman", "Faeze", ""], ["Chaturvedi", "Snigdha", ""]]}, {"id": "2104.07078", "submitter": "Georgios Paraskevopoulos", "authors": "Constantinos Karouzos, Georgios Paraskevopoulos and Alexandros\n  Potamianos", "title": "UDALM: Unsupervised Domain Adaptation through Language Modeling", "comments": "Accepted for publication in 2021 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained\nlanguage models for downstream tasks. We introduce UDALM, a fine-tuning\nprocedure, using a mixed classification and Masked Language Model loss, that\ncan adapt to the target domain distribution in a robust and sample efficient\nmanner. Our experiments show that performance of models trained with the mixed\nloss scales with the amount of available target data and the mixed loss can be\neffectively used as a stopping criterion during UDA training. Furthermore, we\ndiscuss the relationship between A-distance and the target error and explore\nsome limitations of the Domain Adversarial Training approach. Our method is\nevaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset,\nyielding $91.74\\%$ accuracy, which is an $1.11\\%$ absolute improvement over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:05:01 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Karouzos", "Constantinos", ""], ["Paraskevopoulos", "Georgios", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "2104.07079", "submitter": "Maria Leonor Pacheco", "authors": "I-Ta Lee, Maria Leonor Pacheco and Dan Goldwasser", "title": "Modeling Human Mental States with an Entity-based Narrative Graph", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding narrative text requires capturing characters' motivations,\ngoals, and mental states. This paper proposes an Entity-based Narrative Graph\n(ENG) to model the internal-states of characters in a story. We explicitly\nmodel entities, their interactions and the context in which they appear, and\nlearn rich representations for them. We experiment with different task-adaptive\npre-training objectives, in-domain training, and symbolic inference to capture\ndependencies between different decisions in the output space. We evaluate our\nmodel on two narrative understanding tasks: predicting character mental states,\nand desire fulfillment, and conduct a qualitative analysis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:05:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lee", "I-Ta", ""], ["Pacheco", "Maria Leonor", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2104.07081", "submitter": "Nils Reimers", "authors": "Gregor Geigle and Nils Reimers and Andreas R\\\"uckl\\'e and Iryna\n  Gurevych", "title": "TWEAC: Transformer with Extendable QA Agent Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Question answering systems should help users to access knowledge on a broad\nrange of topics and to answer a wide array of different questions. Most systems\nfall short of this expectation as they are only specialized in one particular\nsetting, e.g., answering factual questions with Wikipedia data. To overcome\nthis limitation, we propose composing multiple QA agents within a meta-QA\nsystem. We argue that there exist a wide range of specialized QA agents in\nliterature. Thus, we address the central research question of how to\neffectively and efficiently identify suitable QA agents for any given question.\nWe study both supervised and unsupervised approaches to address this challenge,\nshowing that TWEAC - Transformer with Extendable Agent Classifiers - achieves\nthe best performance overall with 94% accuracy. We provide extensive insights\non the scalability of TWEAC, demonstrating that it scales robustly to over 100\nQA agents with each providing just 1000 examples of questions they can answer.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:06:11 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Geigle", "Gregor", ""], ["Reimers", "Nils", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.07091", "submitter": "Mingda Chen", "authors": "Mingda Chen, Zewei Chu, Sam Wiseman, Kevin Gimpel", "title": "SummScreen: A Dataset for Abstractive Screenplay Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce SummScreen, a summarization dataset comprised of pairs of TV\nseries transcripts and human written recaps. The dataset provides a challenging\ntestbed for abstractive summarization for several reasons. Plot details are\noften expressed indirectly in character dialogues and may be scattered across\nthe entirety of the transcript. These details must be found and integrated to\nform the succinct plot descriptions in the recaps. Also, TV scripts contain\ncontent that does not directly pertain to the central plot but rather serves to\ndevelop characters or provide comic relief. This information is rarely\ncontained in recaps. Since characters are fundamental to TV series, we also\npropose two entity-centric evaluation metrics. Empirically, we characterize the\ndataset by evaluating several methods, including neural models and those based\non nearest neighbors. An oracle extractive approach outperforms all benchmarked\nmodels according to automatic metrics, showing that the neural models are\nunable to fully exploit the input transcripts. Human evaluation and qualitative\nanalysis reveal that our non-oracle models are competitive with their oracle\ncounterparts in terms of generating faithful plot events and can benefit from\nbetter content selectors. Both oracle and non-oracle models generate unfaithful\nfacts, suggesting future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:37:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chen", "Mingda", ""], ["Chu", "Zewei", ""], ["Wiseman", "Sam", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2104.07094", "submitter": "Philipp Dufter", "authors": "Philipp Dufter, Nora Kassner, Hinrich Sch\\\"utze", "title": "Static Embeddings as Efficient Knowledge Bases?", "comments": "NAACL2021 CRV; first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research investigates factual knowledge stored in large pretrained\nlanguage models (PLMs). Instead of structural knowledge base (KB) queries,\nmasked sentences such as \"Paris is the capital of [MASK]\" are used as probes.\nThe good performance on this analysis task has been interpreted as PLMs\nbecoming potential repositories of factual knowledge. In experiments across ten\nlinguistically diverse languages, we study knowledge contained in static\nembeddings. We show that, when restricting the output space to a candidate set,\nsimple nearest neighbor matching using static embeddings performs better than\nPLMs. E.g., static embeddings perform 1.6% points better than BERT while just\nusing 0.3% of energy for training. One important factor in their good\ncomparative performance is that static embeddings are standardly learned for a\nlarge vocabulary. In contrast, BERT exploits its more sophisticated, but\nexpensive ability to compose meaningful representations from a much smaller\nsubword vocabulary.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:42:20 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Dufter", "Philipp", ""], ["Kassner", "Nora", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2104.07112", "submitter": "Georgios Rizos", "authors": "Panagiotis Fytas, Georgios Rizos, Lucia Specia", "title": "What Makes a Scientific Paper be Accepted for Publication?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite peer-reviewing being an essential component of academia since the\n1600s, it has repeatedly received criticisms for lack of transparency and\nconsistency. We posit that recent work in machine learning and explainable AI\nprovide tools that enable insights into the decisions from a given peer review\nprocess. We start by extracting global explanations in the form of linguistic\nfeatures that affect the acceptance of a scientific paper for publication on an\nopen peer-review dataset. Second, since such global explanations do not justify\ncausal interpretations, we provide a methodology for detecting confounding\neffects in natural language in order to generate causal explanations, under\nassumptions, in the form of lexicons. Our proposed linguistic explanation\nmethodology indicates the following on a case dataset of ICLR submissions: a)\nthe organising committee follows, for the most part, the recommendations of\nreviewers, and, b) the paper's main characteristics that led to reviewers\nrecommending acceptance for publication are originality, clarity and substance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:26:51 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Fytas", "Panagiotis", ""], ["Rizos", "Georgios", ""], ["Specia", "Lucia", ""]]}, {"id": "2104.07123", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin\n  Sertolli, Eva-Maria Messner, Erik Cambria, Guoying Zhao, and Bj\\\"orn W.\n  Schuller", "title": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment,\n  Emotion, Physiological-Emotion, and Stress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the\ntasks of sentiment and emotion, as well as physiological-emotion and\nemotion-based stress recognition through more comprehensively integrating the\naudio-visual, language, and biological signal modalities. The purpose of MuSe\n2021 is to bring together communities from different disciplines; mainly, the\naudio-visual emotion recognition community (signal-based), the sentiment\nanalysis community (symbol-based), and the health informatics community. We\npresent four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus\non continuous emotion (valence and arousal) prediction; MuSe-Sent, in which\nparticipants recognise five classes each for valence and arousal; and\nMuSe-Physio, in which the novel aspect of `physiological-emotion' is to be\npredicted. For this years' challenge, we utilise the MuSe-CaR dataset focusing\non user-generated reviews and introduce the Ulm-TSST dataset, which displays\npeople in stressful depositions. This paper also provides detail on the\nstate-of-the-art feature sets extracted from these datasets for utilisation by\nour baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each\nsub-challenge, a competitive baseline for participants is set; namely, on test,\nwe report a Concordance Correlation Coefficient (CCC) of .4616 CCC for\nMuSe-Wilder; .4717 CCC for MuSe-Stress, and .4606 CCC for MuSe-Physio. For\nMuSe-Sent an F1 score of 32.82 % is obtained.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:56:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Christ", "Lukas", ""], ["Schumann", "Lea", ""], ["Sertolli", "Benjamin", ""], ["Messner", "Eva-Maria", ""], ["Cambria", "Erik", ""], ["Zhao", "Guoying", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2104.07143", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif,\n  Fernanda Vi\\'egas, Martin Wattenberg", "title": "An Interpretability Illusion for BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an \"interpretability illusion\" that arises when analyzing the\nBERT model. Activations of individual neurons in the network may spuriously\nappear to encode a single, simple concept, when in fact they are encoding\nsomething far more complex. The same effect holds for linear combinations of\nactivations. We trace the source of this illusion to geometric properties of\nBERT's embedding space as well as the fact that common text corpora represent\nonly narrow slices of possible English sentences. We provide a taxonomy of\nmodel-learned concepts and discuss methodological implications for\ninterpretability research, especially the importance of testing hypotheses on\nmultiple data sets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:04:48 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Pearce", "Adam", ""], ["Yuan", "Ann", ""], ["Coenen", "Andy", ""], ["Reif", "Emily", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Wattenberg", "Martin", ""]]}, {"id": "2104.07149", "submitter": "Sailik Sengupta", "authors": "Jason Krone, Sailik Sengupta, Saab Mansoor", "title": "On the Robustness of Goal Oriented Dialogue Systems to Real-world Noise", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal oriented dialogue systems, that interact in real-word environments,\noften encounter noisy data. In this work, we investigate how robust goal\noriented dialogue systems are to noisy data. Specifically, our analysis\nconsiders intent classification (IC) and slot labeling (SL) models that form\nthe basis of most dialogue systems. We collect a test-suite for six common\nphenomena found in live human-to-bot conversations (abbreviations, casing,\nmisspellings, morphological variants, paraphrases, and synonyms) and show that\nthese phenomena can degrade the IC/SL performance of state-of-the-art BERT\nbased models. Through the use of synthetic data augmentation, we are improve\nIC/SL model's robustness to real-world noise by +11.5 for IC and +17.3 points\nfor SL on average across noise types. We make our suite of noisy test data\npublic to enable further research into the robustness of dialog systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:14:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Krone", "Jason", ""], ["Sengupta", "Sailik", ""], ["Mansoor", "Saab", ""]]}, {"id": "2104.07155", "submitter": "Xiongyi Zhang", "authors": "Xiongyi Zhang, Jan-Willem van de Meent, Byron C. Wallace", "title": "Disentangling Representations of Text by Masking Transformers", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Representations from large pretrained models such as BERT encode a range of\nfeatures into monolithic vectors, affording strong predictive accuracy across a\nmultitude of downstream tasks. In this paper we explore whether it is possible\nto learn disentangled representations by identifying existing subnetworks\nwithin pretrained models that encode distinct, complementary aspect\nrepresentations. Concretely, we learn binary masks over transformer weights or\nhidden units to uncover subsets of features that correlate with a specific\nfactor of variation; this eliminates the need to train a disentangled model\nfrom scratch for a particular task. We evaluate this method with respect to its\nability to disentangle representations of sentiment from genre in movie\nreviews, \"toxicity\" from dialect in Tweets, and syntax from semantics.\n  By combining masking with magnitude pruning we find that we can identify\nsparse subnetworks within BERT that strongly encode particular aspects (e.g.,\ntoxicity) while only weakly encoding others (e.g., race). Moreover, despite\nonly learning masks, we find that disentanglement-via-masking performs as well\nas -- and often better than -- previously proposed methods based on variational\nautoencoders and adversarial training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:45:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhang", "Xiongyi", ""], ["van de Meent", "Jan-Willem", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2104.07163", "submitter": "Mehdi Rezagholizadeh", "authors": "Aref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, Ali Ghodsi", "title": "Annealing Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Significant memory and computational requirements of large deep neural\nnetworks restrict their application on edge devices. Knowledge distillation\n(KD) is a prominent model compression technique for deep neural networks in\nwhich the knowledge of a trained large teacher model is transferred to a\nsmaller student model. The success of knowledge distillation is mainly\nattributed to its training objective function, which exploits the soft-target\ninformation (also known as \"dark knowledge\") besides the given regular hard\nlabels in a training set. However, it is shown in the literature that the\nlarger the gap between the teacher and the student networks, the more difficult\nis their training using knowledge distillation. To address this shortcoming, we\npropose an improved knowledge distillation method (called Annealing-KD) by\nfeeding the rich information provided by the teacher's soft-targets\nincrementally and more efficiently. Our Annealing-KD technique is based on a\ngradual transition over annealed soft-targets generated by the teacher at\ndifferent temperatures in an iterative process, and therefore, the student is\ntrained to follow the annealed teacher output in a step-by-step manner. This\npaper includes theoretical and empirical evidence as well as practical\nexperiments to support the effectiveness of our Annealing-KD method. We did a\ncomprehensive set of experiments on different tasks such as image\nclassification (CIFAR-10 and 100) and NLP language inference with BERT-based\nmodels on the GLUE benchmark and consistently got superior results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 23:45:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jafari", "Aref", ""], ["Rezagholizadeh", "Mehdi", ""], ["Sharma", "Pranav", ""], ["Ghodsi", "Ali", ""]]}, {"id": "2104.07179", "submitter": "Alicia Parrish", "authors": "Alicia Parrish, William Huang, Omar Agha, Soo-Hwan Lee, Nikita Nangia,\n  Alex Warstadt, Karmanya Aggarwal, Emily Allaway, Tal Linzen and Samuel R.\n  Bowman", "title": "Does Putting a Linguist in the Loop Improve NLU Data Collection?", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many crowdsourced NLP datasets contain systematic gaps and biases that are\nidentified only after data collection is complete. Identifying these issues\nfrom early data samples during crowdsourcing should make mitigation more\nefficient, especially when done iteratively. We take natural language inference\nas a test case and ask whether it is beneficial to put a linguist `in the loop'\nduring data collection to dynamically identify and address gaps in the data by\nintroducing novel constraints on the task. We directly compare three data\ncollection protocols: (i) a baseline protocol, (ii) a linguist-in-the-loop\nintervention with iteratively-updated constraints on the task, and (iii) an\nextension of linguist-in-the-loop that provides direct interaction between\nlinguists and crowdworkers via a chatroom. The datasets collected with linguist\ninvolvement are more reliably challenging than baseline, without loss of\nquality. But we see no evidence that using this data in training leads to\nbetter out-of-domain model performance, and the addition of a chat platform has\nno measurable effect on the resulting dataset. We suggest integrating expert\nanalysis \\textit{during} data collection so that the expert can dynamically\naddress gaps and biases in the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:31:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Parrish", "Alicia", ""], ["Huang", "William", ""], ["Agha", "Omar", ""], ["Lee", "Soo-Hwan", ""], ["Nangia", "Nikita", ""], ["Warstadt", "Alex", ""], ["Aggarwal", "Karmanya", ""], ["Allaway", "Emily", ""], ["Linzen", "Tal", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2104.07186", "submitter": "Luyu Gao", "authors": "Luyu Gao, Zhuyun Dai, Jamie Callan", "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with\n  Contextualized Inverted List", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical information retrieval systems such as BM25 rely on exact lexical\nmatch and carry out search efficiently with inverted list index. Recent neural\nIR models shifts towards soft semantic matching all query document terms, but\nthey lose the computation efficiency of exact match systems. This paper\npresents COIL, a contextualized exact match retrieval architecture that brings\nsemantic lexical matching. COIL scoring is based on overlapping query document\ntokens' contextualized representations. The new architecture stores\ncontextualized token representations in inverted lists, bringing together the\nefficiency of exact match and the representation power of deep language models.\nOur experimental results show COIL outperforms classical lexical retrievers and\nstate-of-the-art deep LM retrievers with similar or smaller latency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:53:54 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gao", "Luyu", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2104.07190", "submitter": "Weishun Song", "authors": "Liying Zheng, Yue Deng, Weishun Song, Liang Xu, Jing Xiao", "title": "An Alignment-Agnostic Model for Chinese Text Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to correct Chinese text errors with types of\nmistaken, missing and redundant characters, which is common for Chinese native\nspeakers. Most existing models based on detect-correct framework can correct\nmistaken characters errors, but they cannot deal with missing or redundant\ncharacters. The reason is that lengths of sentences before and after correction\nare not the same, leading to the inconsistence between model inputs and\noutputs. Although the Seq2Seq-based or sequence tagging methods provide\nsolutions to the problem and achieved relatively good results on English\ncontext, but they do not perform well in Chinese context according to our\nexperimental results. In our work, we propose a novel detect-correct framework\nwhich is alignment-agnostic, meaning that it can handle both text aligned and\nnon-aligned occasions, and it can also serve as a cold start model when there\nare no annotated data provided. Experimental results on three datasets\ndemonstrate that our method is effective and achieves the best performance\namong existing published models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 01:17:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zheng", "Liying", ""], ["Deng", "Yue", ""], ["Song", "Weishun", ""], ["Xu", "Liang", ""], ["Xiao", "Jing", ""]]}, {"id": "2104.07198", "submitter": "Kyoung-Rok Jang", "authors": "Kyoung-Rok Jang, Junmo Kang, Giwon Hong, Sung-Hyon Myaeng, Joohee\n  Park, Taewon Yoon, Heecheol Seo", "title": "UHD-BERT: Bucketed Ultra-High Dimensional Sparse Representations for\n  Full Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural information retrieval (IR) models are promising mainly because their\nsemantic matching capabilities can ameliorate the well-known synonymy and\npolysemy problems of word-based symbolic approaches. However, the power of\nneural models' dense representations comes at the cost of inefficiency,\nlimiting it to be used as a re-ranker. Sparse representations, on the other\nhand, can help enhance symbolic or latent-term representations and yet take\nadvantage of an inverted index for efficiency, being amenable to symbolic IR\ntechniques that have been around for decades. In order to transcend the\ntrade-off between sparse representations (symbolic or latent-term based) and\ndense representations, we propose an ultra-high dimensional (UHD)\nrepresentation scheme equipped with directly controllable sparsity. With the\nhigh dimensionality, we attempt to make the meaning of each dimension less\nentangled and polysemous than dense embeddings. The sparsity allows for not\nonly efficiency for vector calculations but also the possibility of making\nindividual dimensions attributable to interpretable concepts. Our model,\nUHD-BERT, maximizes the benefits of ultra-high dimensional (UHD) sparse\nrepresentations based on BERT language modeling, by adopting a bucketing\nmethod. With this method, different segments of an embedding (horizontal\nbuckets) or the embeddings from multiple layers of BERT (vertical buckets) can\nbe selected and merged so that diverse linguistic aspects can be represented.\nAn additional and important benefit of our highly disentangled\n(high-dimensional) and efficient (sparse) representations is that this neural\napproach can be harmonized with well-studied symbolic IR techniques (e.g.,\ninverted index, pseudo-relevance feedback, BM25), enabling us to build a\npowerful and efficient neuro-symbolic information retrieval system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 02:00:01 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jang", "Kyoung-Rok", ""], ["Kang", "Junmo", ""], ["Hong", "Giwon", ""], ["Myaeng", "Sung-Hyon", ""], ["Park", "Joohee", ""], ["Yoon", "Taewon", ""], ["Seo", "Heecheol", ""]]}, {"id": "2104.07204", "submitter": "Yuxuan Lai", "authors": "Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang and Dongyan Zhao", "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese\n  Pre-trained Language Models", "comments": "Accepted at NAACL 2021, 16 pages,\n  https://github.com/alibaba/AliceMind/tree/main/LatticeBERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese pre-trained language models usually process text as a sequence of\ncharacters, while ignoring more coarse granularity, e.g., words. In this work,\nwe propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which\nexplicitly incorporates word representations along with characters, thus can\nmodel a sentence in a multi-granularity manner. Specifically, we construct a\nlattice graph from the characters and words in a sentence and feed all these\ntext units into transformers. We design a lattice position attention mechanism\nto exploit the lattice structures in self-attention layers. We further propose\na masked segment prediction task to push the model to learn from rich but\nredundant information inherent in lattices, while avoiding learning unexpected\ntricks. Experiments on 11 Chinese natural language understanding tasks show\nthat our model can bring an average increase of 1.5% under the 12-layer\nsetting, which achieves new state-of-the-art among base-size models on the CLUE\nbenchmarks. Further analysis shows that Lattice-BERT can harness the lattice\nstructures, and the improvement comes from the exploration of redundant\ninformation and multi-granularity representations. Our code will be available\nat https://github.com/alibaba/pretrained-language-models/LatticeBERT.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 02:36:49 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 11:41:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lai", "Yuxuan", ""], ["Liu", "Yijia", ""], ["Feng", "Yansong", ""], ["Huang", "Songfang", ""], ["Zhao", "Dongyan", ""]]}, {"id": "2104.07210", "submitter": "Yixin Liu", "authors": "Yixin Liu, Zi-Yi Dou, Pengfei Liu", "title": "RefSum: Refactoring Neural Summarization", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although some recent works show potential complementarity among different\nstate-of-the-art systems, few works try to investigate this problem in text\nsummarization. Researchers in other areas commonly refer to the techniques of\nreranking or stacking to approach this problem. In this work, we highlight\nseveral limitations of previous methods, which motivates us to present a new\nframework Refactor that provides a unified view of text summarization and\nsummaries combination. Experimentally, we perform a comprehensive evaluation\nthat involves twenty-two base systems, four datasets, and three different\napplication scenarios. Besides new state-of-the-art results on CNN/DailyMail\ndataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses\nthe limitations of the traditional methods and the effectiveness of the\nRefactor model sheds light on insight for performance improvement. Our system\ncan be directly used by other researchers as an off-the-shelf tool to achieve\nfurther performance improvements. We open-source all the code and provide a\nconvenient interface to use it:\nhttps://github.com/yixinL7/Refactoring-Summarization. We have also made the\ndemo of this work available at:\nhttp://explainaboard.nlpedia.ai/leaderboard/task-summ/index.php.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 02:58:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Liu", "Yixin", ""], ["Dou", "Zi-Yi", ""], ["Liu", "Pengfei", ""]]}, {"id": "2104.07217", "submitter": "Yangming Li", "authors": "Yangming Li, Lemao Liu, Kaisheng Yao", "title": "Neural Sequence Segmentation as Determining the Leftmost Segments", "comments": "A full paper accepted at NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior methods to text segmentation are mostly at token level. Despite the\nadequacy, this nature limits their full potential to capture the long-term\ndependencies among segments. In this work, we propose a novel framework that\nincrementally segments natural language sentences at segment level. For every\nstep in segmentation, it recognizes the leftmost segment of the remaining\nsequence. Implementations involve LSTM-minus technique to construct the phrase\nrepresentations and recurrent neural networks (RNN) to model the iterations of\ndetermining the leftmost segments. We have conducted extensive experiments on\nsyntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets,\ndemonstrating that our methods have significantly outperformed previous all\nbaselines and achieved new state-of-the-art results. Moreover, qualitative\nanalysis and the study on segmenting long-length sentences verify its\neffectiveness in modeling long-term dependencies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:35:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Yangming", ""], ["Liu", "Lemao", ""], ["Yao", "Kaisheng", ""]]}, {"id": "2104.07219", "submitter": "Athul Paul Jacob", "authors": "Athul Paul Jacob, Mike Lewis, Jacob Andreas", "title": "Multitasking Inhibits Semantic Drift", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When intelligent agents communicate to accomplish shared goals, how do these\ngoals shape the agents' language? We study the dynamics of learning in latent\nlanguage policies (LLPs), in which instructor agents generate natural-language\nsubgoal descriptions and executor agents map these descriptions to low-level\nactions. LLPs can solve challenging long-horizon reinforcement learning\nproblems and provide a rich model for studying task-oriented language use. But\nprevious work has found that LLP training is prone to semantic drift (use of\nmessages in ways inconsistent with their original natural language meanings).\nHere, we demonstrate theoretically and empirically that multitask training is\nan effective counter to this problem: we prove that multitask training\neliminates semantic drift in a well-studied family of signaling games, and show\nthat multitask training of neural LLPs in a complex strategy game reduces drift\nand while improving sample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:42:17 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jacob", "Athul Paul", ""], ["Lewis", "Mike", ""], ["Andreas", "Jacob", ""]]}, {"id": "2104.07221", "submitter": "Qixuan Sun", "authors": "Qixuan Sun, Yaqi Yin and Hong Yu", "title": "A Dual-Questioning Attention Network for Emotion-Cause Pair Extraction\n  with Context Awareness", "comments": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion-cause pair extraction (ECPE), an emerging task in sentiment analysis,\naims at extracting pairs of emotions and their corresponding causes in\ndocuments. This is a more challenging problem than emotion cause extraction\n(ECE), since it requires no emotion signals which are demonstrated as an\nimportant role in the ECE task. Existing work follows a two-stage pipeline\nwhich identifies emotions and causes at the first step and pairs them at the\nsecond step. However, error propagation across steps and pair combining without\ncontextual information limits the effectiveness. Therefore, we propose a\nDual-Questioning Attention Network to alleviate these limitations.\nSpecifically, we question candidate emotions and causes to the context\nindependently through attention networks for a contextual and semantical\nanswer. Also, we explore how weighted loss functions in controlling error\npropagation between steps. Empirical results show that our method performs\nbetter than baselines in terms of multiple evaluation metrics. The source code\ncan be obtained at https://github.com/QixuanSun/DQAN.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:47:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sun", "Qixuan", ""], ["Yin", "Yaqi", ""], ["Yu", "Hong", ""]]}, {"id": "2104.07224", "submitter": "Shrey Desai", "authors": "Shrey Desai and Akshat Shrivastava and Alexander Zotov and Ahmed Aly", "title": "Low-Resource Task-Oriented Semantic Parsing via Intrinsic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented semantic parsing models typically have high resource\nrequirements: to support new ontologies (i.e., intents and slots),\npractitioners crowdsource thousands of samples for supervised fine-tuning.\nPartly, this is due to the structure of de facto copy-generate parsers; these\nmodels treat ontology labels as discrete entities, relying on parallel data to\nextrinsically derive their meaning. In our work, we instead exploit what we\nintrinsically know about ontology labels; for example, the fact that\nSL:TIME_ZONE has the categorical type \"slot\" and language-based span \"time\nzone\". Using this motivation, we build our approach with offline and online\nstages. During preprocessing, for each ontology label, we extract its intrinsic\nproperties into a component, and insert each component into an inventory as a\ncache of sorts. During training, we fine-tune a seq2seq, pre-trained\ntransformer to map utterances and inventories to frames, parse trees comprised\nof utterance and ontology tokens. Our formulation encourages the model to\nconsider ontology labels as a union of its intrinsic properties, therefore\nsubstantially bootstrapping learning in low-resource settings. Experiments show\nour model is highly sample efficient: using a low-resource benchmark derived\nfrom TOPv2, our inventory parser outperforms a copy-generate parser by +15 EM\nabsolute (44% relative) when fine-tuning on 10 samples from an unseen domain.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:01:02 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Desai", "Shrey", ""], ["Shrivastava", "Akshat", ""], ["Zotov", "Alexander", ""], ["Aly", "Ahmed", ""]]}, {"id": "2104.07228", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo, Meng Jiang", "title": "Sentence-Permuted Paragraph Generation", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating paragraphs of diverse contents is important in many applications.\nExisting generation models produce similar contents from homogenized contexts\ndue to the fixed left-to-right sentence order. Our idea is permuting the\nsentence orders to improve the content diversity of multi-sentence paragraph.\nWe propose a novel framework PermGen whose objective is to maximize the\nexpected log-likelihood of output paragraph distributions with respect to all\npossible sentence orders. PermGen uses hierarchical positional embedding and\ndesigns new procedures for training, decoding, and candidate ranking in the\nsentence-permuted generation. Experiments on three paragraph generation\nbenchmarks demonstrate PermGen generates more diverse outputs with a higher\nquality than existing models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:17:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yu", "Wenhao", ""], ["Zhu", "Chenguang", ""], ["Zhao", "Tong", ""], ["Guo", "Zhichun", ""], ["Jiang", "Meng", ""]]}, {"id": "2104.07242", "submitter": "Sohee Yang", "authors": "Sohee Yang, Minjoon Seo", "title": "Designing a Minimal Retrieve-and-Read System for Open-Domain Question\n  Answering", "comments": "NAACL 2021 (short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open-domain question answering (QA), retrieve-and-read mechanism has the\ninherent benefit of interpretability and the easiness of adding, removing, or\nediting knowledge compared to the parametric approaches of closed-book QA\nmodels. However, it is also known to suffer from its large storage footprint\ndue to its document corpus and index. Here, we discuss several orthogonal\nstrategies to drastically reduce the footprint of a retrieve-and-read\nopen-domain QA system by up to 160x. Our results indicate that\nretrieve-and-read can be a viable option even in a highly constrained serving\nenvironment such as edge devices, as we show that it can achieve better\naccuracy than a purely parametric model with comparable docker-level system\nsize.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:24:04 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 18:33:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Sohee", ""], ["Seo", "Minjoon", ""]]}, {"id": "2104.07244", "submitter": "Bai Li", "authors": "Bai Li, Frank Rudzicz", "title": "TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning\n  for Eye-Tracking Prediction", "comments": "Cognitive Modeling and Computational Linguistics Workshop (CMCL) at\n  NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movement data during reading is a useful source of information for\nunderstanding language comprehension processes. In this paper, we describe our\nsubmission to the CMCL 2021 shared task on predicting human reading patterns.\nOur model uses RoBERTa with a regression layer to predict 5 eye-tracking\nfeatures. We train the model in two stages: we first fine-tune on the Provo\ncorpus (another eye-tracking dataset), then fine-tune on the task data. We\ncompare different Transformer models and apply ensembling methods to improve\nthe performance. Our final submission achieves a MAE score of 3.929, ranking\n3rd place out of 13 teams that participated in this shared task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:29:13 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Bai", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2104.07249", "submitter": "Minbyul Jeong", "authors": "Minbyul Jeong and Jaewoo Kang", "title": "Regularizing Models via Pointwise Mutual Information for Named Entity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Named Entity Recognition (NER), pre-trained language models have been\noverestimated by focusing on dataset biases to solve current benchmark\ndatasets. However, these biases hinder generalizability which is necessary to\naddress real-world situations such as weak name regularity and plenty of unseen\nmentions. To alleviate the use of dataset biases and make the models fully\nexploit data, we propose a debiasing method that our bias-only model can be\nreplaced with a Pointwise Mutual Information (PMI) to enhance generalization\nability while outperforming an in-domain performance. Our approach enables to\ndebias highly correlated word and labels in the benchmark datasets; reflect\ninformative statistics via subword frequency; alleviates a class imbalance\nbetween positive and negative examples. For long-named and complex-structure\nentities, our method can predict these entities through debiasing on\nconjunction or special characters. Extensive experiments on both general and\nbiomedical domains demonstrate the effectiveness and generalization\ncapabilities of the PMI.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:47:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jeong", "Minbyul", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2104.07252", "submitter": "Haiqin Yang", "authors": "Haiqin Yang and Jianping Shen", "title": "Emotion Dynamics Modeling via BERT", "comments": "9 pages, 9 figures, 5 tables, in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emotion dynamics modeling is a significant task in emotion recognition in\nconversation. It aims to predict conversational emotions when building\nempathetic dialogue systems. Existing studies mainly develop models based on\nRecurrent Neural Networks (RNNs). They cannot benefit from the power of the\nrecently-developed pre-training strategies for better token representation\nlearning in conversations. More seriously, it is hard to distinguish the\ndependency of interlocutors and the emotional influence among interlocutors by\nsimply assembling the features on top of RNNs. In this paper, we develop a\nseries of BERT-based models to specifically capture the inter-interlocutor and\nintra-interlocutor dependencies of the conversational emotion dynamics.\nConcretely, we first substitute BERT for RNNs to enrich the token\nrepresentations. Then, a Flat-structured BERT (F-BERT) is applied to link up\nutterances in a conversation directly, and a Hierarchically-structured BERT\n(H-BERT) is employed to distinguish the interlocutors when linking up\nutterances. More importantly, a Spatial-Temporal-structured BERT, namely\nST-BERT, is proposed to further determine the emotional influence among\ninterlocutors. Finally, we conduct extensive experiments on two popular emotion\nrecognition in conversation benchmark datasets and demonstrate that our\nproposed models can attain around 5\\% and 10\\% improvement over the\nstate-of-the-art baselines, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:58:48 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 03:05:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yang", "Haiqin", ""], ["Shen", "Jianping", ""]]}, {"id": "2104.07253", "submitter": "Donghyun Kwak", "authors": "Seunghyun Seo, Donghyun Kwak, Bowon Lee", "title": "Integration of Pre-trained Networks with Continuous Token Interface for\n  End-to-End Spoken Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most End-to-End (E2E) SLU networks leverage the pre-trained ASR networks but\nstill lack the capability to understand the semantics of utterances, crucial\nfor the SLU task. To solve this, recently proposed studies use pre-trained NLU\nnetworks. However, it is not trivial to fully utilize both pre-trained\nnetworks; many solutions were proposed, such as Knowledge Distillation,\ncross-modal shared embedding, and network integration with Interface. We\npropose a simple and robust integration method for the E2E SLU network with\nnovel Interface, Continuous Token Interface (CTI), the junctional\nrepresentation of the ASR and NLU networks when both networks are pre-trained\nwith the same vocabulary. Because the only difference is the noise level, we\ndirectly feed the ASR network's output to the NLU network. Thus, we can train\nour SLU network in an E2E manner without additional modules, such as\nGumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset\nand achieve state-of-the-art scores on both intent classification and slot\nfilling tasks. We also verify the NLU network, pre-trained with Masked Language\nModel, can utilize a noisy textual representation of CTI. Moreover, we show our\nmodel can be trained with multi-task learning from heterogeneous data even\nafter integration with CTI.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:59:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Seo", "Seunghyun", ""], ["Kwak", "Donghyun", ""], ["Lee", "Bowon", ""]]}, {"id": "2104.07275", "submitter": "Akshat Shrivastava", "authors": "Akshat Shrivastava, Pierce Chuang, Arun Babu, Shrey Desai, Abhinav\n  Arora, Alexander Zotov, Ahmed Aly", "title": "Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic\n  Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective recipe for building seq2seq, non-autoregressive, task-oriented\nparsers to map utterances to semantic frames proceeds in three steps: encoding\nan utterance $x$, predicting a frame's length |y|, and decoding a |y|-sized\nframe with utterance and ontology tokens. Though empirically strong, these\nmodels are typically bottlenecked by length prediction, as even small\ninaccuracies change the syntactic and semantic characteristics of resulting\nframes. In our work, we propose span pointer networks, non-autoregressive\nparsers which shift the decoding task from text generation to span prediction;\nthat is, when imputing utterance spans into frame slots, our model produces\nendpoints (e.g., [i, j]) as opposed to text (e.g., \"6pm\"). This natural\nquantization of the output space reduces the variability of gold frames,\ntherefore improving length prediction and, ultimately, exact match.\nFurthermore, length prediction is now responsible for frame syntax and the\ndecoder is responsible for frame semantics, resulting in a coarse-to-fine\nmodel. We evaluate our approach on several task-oriented semantic parsing\ndatasets. Notably, we bridge the quality gap between non-autogressive and\nautoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020).\nFurthermore, due to our more consistent gold frames, we show strong\nimprovements in model generalization in both cross-domain and cross-lingual\ntransfer in low-resource settings. Finally, due to our diminished output\nvocabulary, we observe 70% reduction in latency and 83% reduction in memory at\nbeam size 5 compared to prior non-autoregressive parsers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:02:35 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 18:33:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shrivastava", "Akshat", ""], ["Chuang", "Pierce", ""], ["Babu", "Arun", ""], ["Desai", "Shrey", ""], ["Arora", "Abhinav", ""], ["Zotov", "Alexander", ""], ["Aly", "Ahmed", ""]]}, {"id": "2104.07284", "submitter": "Jungsoo Park", "authors": "Jungsoo Park, Gyuwan Kim, Jaewoo Kang", "title": "Consistency Training with Virtual Adversarial Discrete Perturbation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an effective consistency training framework that enforces a\ntraining model's predictions given original and perturbed inputs to be similar\nby adding a discrete noise that would incur the highest divergence between\npredictions. This virtual adversarial discrete noise obtained by replacing a\nsmall portion of tokens while keeping original semantics as much as possible\nefficiently pushes a training model's decision boundary. Moreover, we perform\nan iterative refinement process to alleviate the degraded fluency of the\nperturbed sentence due to the conditional independence assumption. Experimental\nresults show that our proposed method outperforms other consistency training\nbaselines with text editing, paraphrasing, or a continuous noise on\nsemi-supervised text classification tasks and a robustness benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:49:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Park", "Jungsoo", ""], ["Kim", "Gyuwan", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2104.07302", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, Hanwang Zhang", "title": "TransferNet: An Effective and Transparent Framework for Multi-hop\n  Question Answering over Relation Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-hop Question Answering (QA) is a challenging task because it requires\nprecise reasoning with entity relations at every step towards the answer. The\nrelations can be represented in terms of labels in knowledge graph (e.g.,\n\\textit{spouse}) or text in text corpus (e.g., \\textit{they have been married\nfor 26 years}). Existing models usually infer the answer by predicting the\nsequential relation path or aggregating the hidden graph features. The former\nis hard to optimize, and the latter lacks interpretability. In this paper, we\npropose TransferNet, an effective and transparent model for multi-hop QA, which\nsupports both label and text relations in a unified framework. TransferNet\njumps across entities at multiple steps. At each step, it attends to different\nparts of the question, computes activated scores for relations, and then\ntransfer the previous entity scores along activated relations in a\ndifferentiable way. We carry out extensive experiments on three datasets and\ndemonstrate that TransferNet surpasses the state-of-the-art models by a large\nmargin. In particular, on MetaQA, it achieves 100\\% accuracy in 2-hop and 3-hop\nquestions. By qualitative analysis, we show that TransferNet has transparent\nand interpretable intermediate results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:23:05 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shi", "Jiaxin", ""], ["Cao", "Shulin", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2104.07307", "submitter": "Peng-Jian Yang", "authors": "Peng-Jian Yang, Ying Ting Chen, Yuechan Chen, Daniel Cer", "title": "NT5?! Training T5 to Perform Numerical Reasoning", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical reasoning over text (NRoT) presents unique challenges that are not\nwell addressed by existing pre-training objectives. We explore five sequential\ntraining schedules that adapt a pre-trained T5 model for NRoT. Our final model\nis adapted from T5, but further pre-trained on three datasets designed to\nstrengthen skills necessary for NRoT and general reading comprehension before\nbeing fine-tuned on the Discrete Reasoning over Text (DROP) dataset. The\ntraining improves DROP's adjusted F1 performance (a numeracy-focused score)\nfrom 45.90 to 70.83. Our model closes in on GenBERT (72.4), a custom BERT-Base\nmodel using the same datasets with significantly more parameters. We show that\ntraining the T5 multitasking framework with multiple numerical reasoning\ndatasets of increasing difficulty, good performance on DROP can be achieved\nwithout manually engineering partitioned functionality between distributed and\nsymbol modules.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:34:44 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 10:58:39 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yang", "Peng-Jian", ""], ["Chen", "Ying Ting", ""], ["Chen", "Yuechan", ""], ["Cer", "Daniel", ""]]}, {"id": "2104.07358", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Xian Li, Dmitriy Genzel", "title": "Adaptive Sparse Transformer for Multilingual Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual machine translation has attracted much attention recently due to\nits support of knowledge transfer among languages and the low cost of training\nand deployment compared with numerous bilingual models. A known challenge of\nmultilingual models is the negative language interference. In order to enhance\nthe translation quality, deeper and wider architectures are applied to\nmultilingual modeling for larger model capacity, which suffers from the\nincreased inference cost at the same time. It has been pointed out in recent\nstudies that parameters shared among languages are the cause of interference\nwhile they may also enable positive transfer. Based on these insights, we\npropose an adaptive and sparse architecture for multilingual modeling, and\ntrain the model to learn shared and language-specific parameters to improve the\npositive transfer and mitigate the interference. The sparse architecture only\nactivates a subnetwork which preserves inference efficiency, and the adaptive\ndesign selects different subnetworks based on the input languages. Evaluated on\nmultilingual translation across multiple public datasets, our model outperforms\nstrong baselines in terms of translation quality without increasing the\ninference cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:31:07 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gong", "Hongyu", ""], ["Li", "Xian", ""], ["Genzel", "Dmitriy", ""]]}, {"id": "2104.07367", "submitter": "Sidharth R", "authors": "Sidharth R, Abhiraj Tiwari, Parthivi Choubey, Saisha Kashyap, Sahil\n  Khose, Kumud Lakara, Nishesh Singh, Ujjwal Verma", "title": "BERT based Transformers lead the way in Extraction of Health Information\n  from Social Media", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submissions for the Social Media Mining for Health\n(SMM4H)2021 shared tasks. We participated in 2 tasks:(1) Classification,\nextraction and normalization of adverse drug effect (ADE) mentions in English\ntweets (Task-1) and (2) Classification of COVID-19 tweets containing\nsymptoms(Task-6). Our approach for the first task uses the language\nrepresentation model RoBERTa with a binary classification head. For the second\ntask, we use BERTweet, based on RoBERTa. Fine-tuning is performed on the\npre-trained models for both tasks. The models are placed on top of a custom\ndomain-specific processing pipeline. Our system ranked first among all the\nsubmissions for subtask-1(a) with an F1-score of 61%. For subtask-1(b), our\nsystem obtained an F1-score of 50% with improvements up to +8% F1 over the\nscore averaged across all submissions. The BERTweet model achieved an F1 score\nof 94% on SMM4H 2021 Task-6.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:50:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["R", "Sidharth", ""], ["Tiwari", "Abhiraj", ""], ["Choubey", "Parthivi", ""], ["Kashyap", "Saisha", ""], ["Khose", "Sahil", ""], ["Lakara", "Kumud", ""], ["Singh", "Nishesh", ""], ["Verma", "Ujjwal", ""]]}, {"id": "2104.07376", "submitter": "Luan Thanh Nguyen", "authors": "Phu Gia Hoang, Luan Thanh Nguyen, Kiet Van Nguyen", "title": "UIT-E10dot3 at SemEval-2021 Task 5: Toxic Spans Detection with Named\n  Entity Recognition and Question-Answering Approaches", "comments": "Accepted at SemEval-2021 Task 5: Toxic Spans Detection, ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increment of toxic comments on online space is causing tremendous effects\non other vulnerable users. For this reason, considerable efforts are made to\ndeal with this, and SemEval-2021 Task 5: Toxic Spans Detection is one of those.\nThis task asks competitors to extract spans that have toxicity from the given\ntexts, and we have done several analyses to understand its structure before\ndoing experiments. We solve this task by two approaches, Named Entity\nRecognition with spaCy library and Question-Answering with RoBERTa combining\nwith ToxicBERT, and the former gains the highest F1-score of 66.99%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:07:56 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hoang", "Phu Gia", ""], ["Nguyen", "Luan Thanh", ""], ["Van Nguyen", "Kiet", ""]]}, {"id": "2104.07378", "submitter": "Maya Ramanath", "authors": "Saransh Goyal, Pratyush Pandey, Garima Gaur, Subhalingam D, Srikanta\n  Bedathur, Maya Ramanath", "title": "Tracking entities in technical procedures -- a new dataset and baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce TechTrack, a new dataset for tracking entities in technical\nprocedures. The dataset, prepared by annotating open domain articles from\nWikiHow, consists of 1351 procedures, e.g., \"How to connect a printer\",\nidentifies more than 1200 unique entities with an average of 4.7 entities per\nprocedure. We evaluate the performance of state-of-the-art models on the\nentity-tracking task and find that they are well below the human annotation\nperformance. We describe how TechTrack can be used to take forward the research\non understanding procedures from temporal texts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:16:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Goyal", "Saransh", ""], ["Pandey", "Pratyush", ""], ["Gaur", "Garima", ""], ["D", "Subhalingam", ""], ["Bedathur", "Srikanta", ""], ["Ramanath", "Maya", ""]]}, {"id": "2104.07396", "submitter": "Dai Quoc Nguyen", "authors": "Dai Quoc Nguyen and Vinh Tong and Dinh Phung and Dat Quoc Nguyen", "title": "Node Co-occurrence based Dual Quaternion Graph Neural Networks for\n  Knowledge Graph Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:51:52 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 05:41:26 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nguyen", "Dai Quoc", ""], ["Tong", "Vinh", ""], ["Phung", "Dinh", ""], ["Nguyen", "Dat Quoc", ""]]}, {"id": "2104.07398", "submitter": "Hao Jia", "authors": "Hao Jia, Shuqin Gu, Yangbin Shi, Xiangyu Duan, Zhongkai Hu, Yuqi\n  Zhang, Weihua Luo", "title": "Bilingual Terminology Extraction from Non-Parallel E-Commerce Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual terminologies are important resources for natural language\nprocessing (NLP) applications. The acquisition of bilingual terminology pairs\nis either human translation or automatic extraction from parallel data. We\nnotice that comparable corpora could also be a good resource for extracting\nbilingual terminology pairs, especially for e-commerce domain. The parallel\ncorpora are particularly scarce in e-commerce settings, but the non-parallel\ncorpora in different languages from the same domain are easily available. In\nthis paper, we propose a novel framework of extracting bilingual terminologies\nfrom non-parallel comparable corpus in e-commerce. Benefiting from\ncross-lingual pre-training in e-commerce, our framework can extract the\ncorresponding target terminology by fully utilizing the deep semantic\nrelationship between source-side terminology and target-side sentence.\nExperimental results on various language pairs show that our approaches achieve\nsignificantly better performance than various strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:59:33 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jia", "Hao", ""], ["Gu", "Shuqin", ""], ["Shi", "Yangbin", ""], ["Duan", "Xiangyu", ""], ["Hu", "Zhongkai", ""], ["Zhang", "Yuqi", ""], ["Luo", "Weihua", ""]]}, {"id": "2104.07410", "submitter": "Prasanna Raj Noel Dabre", "authors": "Raj Dabre, Aizhan Imankulova, Masahiro Kaneko, Abhisek Chakrabarty", "title": "Simultaneous Multi-Pivot Neural Machine Translation", "comments": "preliminary work. pardon the messy writing and mistakes. will be\n  submitted to emnlp after major overhaul", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parallel corpora are indispensable for training neural machine translation\n(NMT) models, and parallel corpora for most language pairs do not exist or are\nscarce. In such cases, pivot language NMT can be helpful where a pivot language\nis used such that there exist parallel corpora between the source and pivot and\npivot and target languages. Naturally, the quality of pivot language\ntranslation is more inferior to what could be achieved with a direct parallel\ncorpus of a reasonable size for that pair. In a real-time simultaneous\ntranslation setting, the quality of pivot language translation deteriorates\neven further given that the model has to output translations the moment a few\nsource words become available. To solve this issue, we propose multi-pivot\ntranslation and apply it to a simultaneous translation setting involving pivot\nlanguages. Our approach involves simultaneously translating a source language\ninto multiple pivots, which are then simultaneously translated together into\nthe target language by leveraging multi-source NMT. Our experiments in a\nlow-resource setting using the N-way parallel UN corpus for Arabic to English\nNMT via French and Spanish as pivots reveals that in a simultaneous pivot NMT\nsetting, using two pivot languages can lead to an improvement of up to 5.8\nBLEU.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:19:52 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Dabre", "Raj", ""], ["Imankulova", "Aizhan", ""], ["Kaneko", "Masahiro", ""], ["Chakrabarty", "Abhisek", ""]]}, {"id": "2104.07412", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan\n  Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson", "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has brought striking advances in multilingual natural\nlanguage processing capabilities over the past year. For example, the latest\ntechniques have improved the state-of-the-art performance on the XTREME\nmultilingual benchmark by more than 13 points. While a sizeable gap to\nhuman-level performance remains, improvements have been easier to achieve in\nsome tasks than in others. This paper analyzes the current state of\ncross-lingual transfer learning and summarizes some lessons learned. In order\nto catalyze meaningful progress, we extend XTREME to XTREME-R, which consists\nof an improved set of ten natural language understanding tasks, including\nchallenging language-agnostic retrieval tasks, and covers 50 typologically\ndiverse languages. In addition, we provide a massively multilingual diagnostic\nsuite and fine-grained multi-dataset evaluation capabilities through an\ninteractive public leaderboard to gain a better understanding of such models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:26:12 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ruder", "Sebastian", ""], ["Constant", "Noah", ""], ["Botha", "Jan", ""], ["Siddhant", "Aditya", ""], ["Firat", "Orhan", ""], ["Fu", "Jinlan", ""], ["Liu", "Pengfei", ""], ["Hu", "Junjie", ""], ["Neubig", "Graham", ""], ["Johnson", "Melvin", ""]]}, {"id": "2104.07423", "submitter": "Preslav Nakov", "authors": "Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov", "title": "The Role of Context in Detecting Previously Fact-Checked Claims", "comments": "detecting previously fact-checked claims, fact-checking,\n  disinformation, fake news, social media, political debates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the proliferation of disinformation and misinformation\nonline, thanks to the freedom of expression on the Internet and to the rise of\nsocial media. Two solutions were proposed to address the problem: (i) manual\nfact-checking, which is accurate and credible, but slow and non-scalable, and\n(ii) automatic fact-checking, which is fast and scalable, but lacks\nexplainability and credibility. With the accumulation of enough manually\nfact-checked claims, a middle-ground approach has emerged: checking whether a\ngiven claim has previously been fact-checked. This can be made automatically,\nand thus fast, while also offering credibility and explainability, thanks to\nthe human fact-checking and explanations in the associated fact-checking\narticle. This is a relatively new and understudied research direction, and here\nwe focus on claims made in a political debate, where context really matters.\nThus, we study the impact of modeling the context of the claim: both on the\nsource side, i.e., in the debate, as well as on the target side, i.e., in the\nfact-checking explanation document. We do this by modeling the local context,\nthe global context, as well as by means of co-reference resolution, and\nreasoning over the target text using Transformer-XH. The experimental results\nshow that each of these represents a valuable information source, but that\nmodeling the source-side context is more important, and can yield 10+ points of\nabsolute improvement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:39:37 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shaar", "Shaden", ""], ["Alam", "Firoj", ""], ["Martino", "Giovanni Da San", ""], ["Nakov", "Preslav", ""]]}, {"id": "2104.07425", "submitter": "Ryuto Konno", "authors": "Ryuto Konno, Shun Kiyono, Yuichiroh Matsubayashi, Hiroki Ouchi,\n  Kentaro Inui", "title": "Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of pretrained masked language models (MLMs) has drastically improved\nthe performance of zero anaphora resolution (ZAR). We further expand this\napproach with a novel pretraining task and finetuning method for Japanese ZAR.\nOur pretraining task aims to acquire anaphoric relational knowledge necessary\nfor ZAR from a large-scale raw corpus. The ZAR model is finetuned in the same\nmanner as pretraining. Our experiments show that combining the proposed methods\nsurpasses previous state-of-the-art performance with large margins, providing\ninsight on the remaining challenges.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:43:57 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Konno", "Ryuto", ""], ["Kiyono", "Shun", ""], ["Matsubayashi", "Yuichiroh", ""], ["Ouchi", "Hiroki", ""], ["Inui", "Kentaro", ""]]}, {"id": "2104.07429", "submitter": "Danielle Saunders", "authors": "Danielle Saunders and Rosie Sallis and Bill Byrne", "title": "First the worst: Finding better gender translations during beam search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation inference procedures like beam search generate the\nmost likely output under the model. This can exacerbate any demographic biases\nexhibited by the model. We focus on gender bias resulting from systematic\nerrors in grammatical gender translation, which can lead to human referents\nbeing misrepresented or misgendered.\n  Most approaches to this problem adjust the training data or the model. By\ncontrast, we experiment with simply adjusting the inference procedure. We\nexperiment with reranking nbest lists using gender features obtained\nautomatically from the source sentence, and applying gender constraints while\ndecoding to improve nbest list gender diversity. We find that a combination of\nthese techniques allows large gains in WinoMT accuracy without requiring\nadditional bilingual data or an additional NMT model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:53:30 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Saunders", "Danielle", ""], ["Sallis", "Rosie", ""], ["Byrne", "Bill", ""]]}, {"id": "2104.07456", "submitter": "Hassan Sajjad", "authors": "Hassan Sajjad and Firoj Alam and Fahim Dalvi and Nadir Durrani", "title": "Effect of Post-processing on Contextualized Word Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-processing of static embedding has beenshown to improve their\nperformance on both lexical and sequence-level tasks. However, post-processing\nfor contextualized embeddings is an under-studied problem. In this work, we\nquestion the usefulness of post-processing for contextualized embeddings\nobtained from different layers of pre-trained language models. More\nspecifically, we standardize individual neuron activations using z-score,\nmin-max normalization, and by removing top principle components using the\nall-but-the-top method. Additionally, we apply unit length normalization to\nword representations. On a diverse set of pre-trained models, we show that\npost-processing unwraps vital information present in the representations for\nboth lexical tasks (such as word similarity and analogy)and sequence\nclassification tasks. Our findings raise interesting points in relation to\ntheresearch studies that use contextualized representations, and suggest\nz-score normalization as an essential step to consider when using them in an\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:40:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sajjad", "Hassan", ""], ["Alam", "Firoj", ""], ["Dalvi", "Fahim", ""], ["Durrani", "Nadir", ""]]}, {"id": "2104.07467", "submitter": "Momchil Hardalov", "authors": "Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein", "title": "Cross-Domain Label-Adaptive Stance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stance detection concerns the classification of a writer's viewpoint towards\na target. There are different task variants, e.g., stance of a tweet vs. a full\narticle, or stance with respect to a claim vs. an (implicit) topic. Moreover,\ntask definitions vary, which includes the label inventory, the data collection,\nand the annotation protocol. All these aspects hinder cross-domain studies, as\nthey require changes to standard domain adaptation approaches. In this paper,\nwe perform an in-depth analysis of 16 stance detection datasets, and we explore\nthe possibility for cross-domain learning from them. Moreover, we propose an\nend-to-end unsupervised framework for out-of-domain prediction of unseen,\nuser-defined labels. In particular, we combine domain adaptation techniques\nsuch as mixture of experts and domain-adversarial training with label\nembeddings, and we demonstrate sizable performance gains over strong baselines\n-- both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e.,\nfor unseen targets. Finally, we perform an exhaustive analysis of the\ncross-domain results, and we highlight the important factors influencing the\nmodel performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:04:29 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hardalov", "Momchil", ""], ["Arora", "Arnav", ""], ["Nakov", "Preslav", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2104.07472", "submitter": "Louis Castricato", "authors": "Louis Castricato, Spencer Frazier, Jonathan Balloch, Mark Riedl", "title": "Fabula Entropy Indexing: Objective Measures of Story Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated story generation remains a difficult area of research because it\nlacks strong objective measures. Generated stories may be linguistically sound,\nbut in many cases suffer poor narrative coherence required for a compelling,\nlogically-sound story. To address this, we present Fabula Entropy Indexing\n(FEI), an evaluation method to assess story coherence by measuring the degree\nto which human participants agree with each other when answering true/false\nquestions about stories. We devise two theoretically grounded measures of\nreader question-answering entropy, the entropy of world coherence (EWC), and\nthe entropy of transitional coherence (ETC), focusing on global and local\ncoherence, respectively. We evaluate these metrics by testing them on\nhuman-written stories and comparing against the same stories that have been\ncorrupted to introduce incoherencies. We show that in these controlled studies,\nour entropy indices provide a reliable objective measure of story coherence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:29:37 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Castricato", "Louis", ""], ["Frazier", "Spencer", ""], ["Balloch", "Jonathan", ""], ["Riedl", "Mark", ""]]}, {"id": "2104.07478", "submitter": "Jonathan Herzig", "authors": "Jonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin Guu, Panupong\n  Pasupat, Yuan Zhang", "title": "Unlocking Compositional Generalization in Pre-trained Models Using\n  Intermediate Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence (seq2seq) models are prevalent in semantic parsing, but\nhave been found to struggle at out-of-distribution compositional\ngeneralization. While specialized model architectures and pre-training of\nseq2seq models have been proposed to address this issue, the former often comes\nat the cost of generality and the latter only shows limited success. In this\npaper, we study the impact of intermediate representations on compositional\ngeneralization in pre-trained seq2seq models, without changing the model\narchitecture at all, and identify key aspects for designing effective\nrepresentations. Instead of training to directly map natural language to an\nexecutable form, we map to a reversible or lossy intermediate representation\nthat has stronger structural correspondence with natural language. The\ncombination of our proposed intermediate representations and pre-trained models\nis surprisingly effective, where the best combinations obtain a new\nstate-of-the-art on CFQ (+14.8 accuracy points) and on the template-splits of\nthree text-to-SQL datasets (+15.0 to +19.4 accuracy points). This work\nhighlights that intermediate representations provide an important and\npotentially overlooked degree of freedom for improving the compositional\ngeneralization abilities of pre-trained seq2seq models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:15:14 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Herzig", "Jonathan", ""], ["Shaw", "Peter", ""], ["Chang", "Ming-Wei", ""], ["Guu", "Kelvin", ""], ["Pasupat", "Panupong", ""], ["Zhang", "Yuan", ""]]}, {"id": "2104.07483", "submitter": "El Moatez Billah Nagoudi", "authors": "El Moatez Billah Nagoudi, Wei-Rui Chen, Muhammad Abdul-Mageed and\n  Hasan Cavusogl", "title": "IndT5: A Text-to-Text Transformer for 10 Indigenous Languages", "comments": "Accepted in AmericasNLP 2021, co-located with NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer language models have become fundamental components of natural\nlanguage processing based pipelines. Although several Transformer models have\nbeen introduced to serve many languages, there is a shortage of models\npre-trained for low-resource and Indigenous languages. In this work, we\nintroduce IndT5, the first Transformer language model for Indigenous languages.\nTo train IndT5, we build IndCorpus--a new dataset for ten Indigenous languages\nand Spanish. We also present the application of IndT5 to machine translation by\ninvestigating different approaches to translate between Spanish and the\nIndigenous languages as part of our contribution to the AmericasNLP 2021 Shared\nTask on Open Machine Translation. IndT5 and IndCorpus are publicly available\nfor research\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 07:09:09 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 09:07:50 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Nagoudi", "El Moatez Billah", ""], ["Chen", "Wei-Rui", ""], ["Abdul-Mageed", "Muhammad", ""], ["Cavusogl", "Hasan", ""]]}, {"id": "2104.07496", "submitter": "Masahiro Kaneko", "authors": "Masahiro Kaneko and Danushka Bollegala", "title": "Unmasking the Mask -- Evaluating Social Biases in Masked Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Masked Language Models (MLMs) have shown superior performances in numerous\ndownstream NLP tasks when used as text encoders. Unfortunately, MLMs also\ndemonstrate significantly worrying levels of social biases. We show that the\npreviously proposed evaluation metrics for quantifying the social biases in\nMLMs are problematic due to following reasons: (1) prediction accuracy of the\nmasked tokens itself tend to be low in some MLMs, which raises questions\nregarding the reliability of the evaluation metrics that use the (pseudo)\nlikelihood of the predicted tokens, and (2) the correlation between the\nprediction accuracy of the mask and the performance in downstream NLP tasks is\nnot taken into consideration, and (3) high frequency words in the training data\nare masked more often, introducing noise due to this selection bias in the test\ncases. To overcome the above-mentioned disfluencies, we propose All Unmasked\nLikelihood (AUL), a bias evaluation measure that predicts all tokens in a test\ncase given the MLM embedding of the unmasked input. We find that AUL accurately\ndetects different types of biases in MLMs. We also propose AUL with attention\nweights (AULA) to evaluate tokens based on their importance in a sentence.\nHowever, unlike AUL and AULA, previously proposed bias evaluation measures for\nMLMs systematically overestimate the measured biases, and are heavily\ninfluenced by the unmasked tokens in the context.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:40:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Kaneko", "Masahiro", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2104.07500", "submitter": "Hassan Shahmohammadi", "authors": "Hassan Shahmohammadi, Hendrik P. A. Lensch, R. Harald Baayen", "title": "Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via\n  Multi-Task Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language grounding aims at linking the symbolic representation of language\n(e.g., words) into the rich perceptual knowledge of the outside world. The\ngeneral approach is to embed both textual and visual information into a common\nspace -the grounded space-confined by an explicit relationship between both\nmodalities. We argue that this approach sacrifices the abstract knowledge\nobtained from linguistic co-occurrence statistics in the process of acquiring\nperceptual information. The focus of this paper is to solve this issue by\nimplicitly grounding the word embeddings. Rather than learning two mappings\ninto a joint space, our approach integrates modalities by determining a\nreversible grounded mapping between the textual and the grounded space by means\nof multi-task learning. Evaluations on intrinsic and extrinsic tasks show that\nour embeddings are highly beneficial for both abstract and concrete words. They\nare strongly correlated with human judgments and outperform previous works on a\nwide range of benchmarks. Our grounded embeddings are publicly available here.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:49:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Shahmohammadi", "Hassan", ""], ["Lensch", "Hendrik P. A.", ""], ["Baayen", "R. Harald", ""]]}, {"id": "2104.07504", "submitter": "Chen Qu", "authors": "Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky and\n  Marc Najork", "title": "Privacy-Adaptive BERT for Natural Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trying to apply the recent advance of Natural Language Understanding\n(NLU) technologies to real-world applications, privacy preservation imposes a\ncrucial challenge, which, unfortunately, has not been well resolved. To address\nthis issue, we study how to improve the effectiveness of NLU models under a\nLocal Privacy setting, using BERT, a widely-used pretrained Language Model\n(LM), as an example. We systematically study the strengths and weaknesses of\nimposing dx-privacy, a relaxed variant of Local Differential Privacy, at\ndifferent stages of language modeling: input text, token embeddings, and\nsequence representations. We then focus on the former two with\nprivacy-constrained fine-tuning experiments to reveal the utility of BERT under\nlocal privacy constraints. More importantly, to the best of our knowledge, we\nare the first to propose privacy-adaptive LM pretraining methods and\ndemonstrate that they can significantly improve model performance on privatized\ntext input. We also interpret the level of privacy preservation and provide our\nguidance on privacy parameter selections.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:01:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Qu", "Chen", ""], ["Kong", "Weize", ""], ["Yang", "Liu", ""], ["Zhang", "Mingyang", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2104.07505", "submitter": "Karolina Stanczak", "authors": "Karolina Sta\\'nczak, Sagnik Ray Choudhury, Tiago Pimentel, Ryan\n  Cotterell, Isabelle Augenstein", "title": "Quantifying Gender Bias Towards Politicians in Cross-Lingual Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the prevalence of large pre-trained language models has led to\nsignificant improvements in the performance of NLP systems, recent research has\ndemonstrated that these models inherit societal biases extant in natural\nlanguage. In this paper, we explore a simple method to probe pre-trained\nlanguage models for gender bias, which we use to effect a multi-lingual study\nof gender bias towards politicians. We construct a dataset of 250k politicians\nfrom most countries in the world and quantify adjective and verb usage around\nthose politicians' names as a function of their gender. We conduct our study in\n7 languages across 6 different language modeling architectures. Our results\ndemonstrate that stance towards politicians in pre-trained language models is\nhighly dependent on the language used. Finally, contrary to previous findings,\nour study suggests that larger language models do not tend to be significantly\nmore gender-biased than smaller ones.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:03:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sta\u0144czak", "Karolina", ""], ["Choudhury", "Sagnik Ray", ""], ["Pimentel", "Tiago", ""], ["Cotterell", "Ryan", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2104.07511", "submitter": "Idan Schwartz", "authors": "Idan Schwartz", "title": "Ensemble of MRR and NDCG models for Visual Dialog", "comments": "Accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:09:32 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:52:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schwartz", "Idan", ""]]}, {"id": "2104.07512", "submitter": "Mehrdad Nasser", "authors": "Mehrdad Nasser, Mohamad Bagher Sajadi, Behrouz Minaei-Bidgoli", "title": "A Sample-Based Training Method for Distantly Supervised Relation\n  Extraction with Pre-Trained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) has become the standard learning paradigm\nfor distantly supervised relation extraction (DSRE). However, due to relation\nextraction being performed at bag level, MIL has significant hardware\nrequirements for training when coupled with large sentence encoders such as\ndeep transformer neural networks. In this paper, we propose a novel sampling\nmethod for DSRE that relaxes these hardware requirements. In the proposed\nmethod, we limit the number of sentences in a batch by randomly sampling\nsentences from the bags in the batch. However, this comes at the cost of losing\nvalid sentences from bags. To alleviate the issues caused by random sampling,\nwe use an ensemble of trained models for prediction. We demonstrate the\neffectiveness of our approach by using our proposed learning setting to\nfine-tuning BERT on the widely NYT dataset. Our approach significantly\noutperforms previous state-of-the-art methods in terms of AUC and P@N metrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:09:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Nasser", "Mehrdad", ""], ["Sajadi", "Mohamad Bagher", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2104.07535", "submitter": "Wonjin Yoon", "authors": "Wonjin Yoon, Richard Jackson, Jaewoo Kang, Aron Lagerberg", "title": "Sequence Tagging for Biomedical Extractive Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current studies in extractive question answering (EQA) have modeled\nsingle-span extraction setting, where a single answer span is a label to\npredict for a given question-passage pair. This setting is natural for general\ndomain EQA as the majority of the questions in the general domain can be\nanswered with a single span. Following general domain EQA models, current\nbiomedical EQA (BioEQA) models utilize single-span extraction setting with\npost-processing steps. In this paper, we investigate the difference of the\nquestion distribution across the general and biomedical domains and discover\nbiomedical questions are more likely to require list-type answers (multiple\nanswers) than factoid-type answers (single answer). In real-world use cases,\nthis emphasizes the need for Biomedical EQA models able to handle multiple\nquestion types. Based on this preliminary study, we propose a multi-span\nextraction setting, namely sequence tagging approach for BioEQA, which directly\ntackles questions with a variable number of phrases as their answer. Our\napproach can learn to decide the number of answers for a question from training\ndata. Our experimental result on the BioASQ 7b and 8b list-type questions\noutperformed the best-performing existing models without requiring\npost-processing steps.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:42:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yoon", "Wonjin", ""], ["Jackson", "Richard", ""], ["Kang", "Jaewoo", ""], ["Lagerberg", "Aron", ""]]}, {"id": "2104.07540", "submitter": "Timo Schick", "authors": "Timo Schick and Hinrich Sch\\\"utze", "title": "Generating Datasets with Pretrained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain high-quality sentence embeddings from pretrained language models\n(PLMs), they must either be augmented with additional pretraining objectives or\nfinetuned on a large set of labeled text pairs. While the latter approach\ntypically outperforms the former, it requires great human effort to generate\nsuitable datasets of sufficient size. In this paper, we show how large PLMs can\nbe leveraged to obtain high-quality embeddings without requiring any labeled\ndata, finetuning or modifications to the pretraining objective: We utilize the\ngenerative abilities of PLMs to generate entire datasets of labeled text pairs\nfrom scratch, which can then be used for regular finetuning of much smaller\nmodels. Our fully unsupervised approach outperforms strong baselines on several\nEnglish semantic textual similarity datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:51:41 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 16:05:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Schick", "Timo", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2104.07541", "submitter": "Raphael Shu", "authors": "Raphael Shu, Kang Min Yoo, Jung-Woo Ha", "title": "Reward Optimization for Neural Machine Translation with Learned Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural machine translation (NMT) models are conventionally trained with\ntoken-level negative log-likelihood (NLL), which does not guarantee that the\ngenerated translations will be optimized for a selected sequence-level\nevaluation metric. Multiple approaches are proposed to train NMT with BLEU as\nthe reward, in order to directly improve the metric. However, it was reported\nthat the gain in BLEU does not translate to real quality improvement, limiting\nthe application in industry. Recently, it became clear to the community that\nBLEU has a low correlation with human judgment when dealing with\nstate-of-the-art models. This leads to the emerging of model-based evaluation\nmetrics. These new metrics are shown to have a much higher human correlation.\nIn this paper, we investigate whether it is beneficial to optimize NMT models\nwith the state-of-the-art model-based metric, BLEURT. We propose a\ncontrastive-margin loss for fast and stable reward optimization suitable for\nlarge NMT models. In experiments, we perform automatic and human evaluations to\ncompare models trained with smoothed BLEU and BLEURT to the baseline models.\nResults show that the reward optimization with BLEURT is able to increase the\nmetric scores by a large margin, in contrast to limited gain when training with\nsmoothed BLEU. The human evaluation shows that models trained with BLEURT\nimprove adequacy and coverage of translations. Code is available via\nhttps://github.com/naver-ai/MetricMT.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:53:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shu", "Raphael", ""], ["Yoo", "Kang Min", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "2104.07545", "submitter": "Tobias Rohde", "authors": "Tobias Rohde, Xiaoxia Wu, Yinhan Liu", "title": "Hierarchical Learning for Generation with Long Source Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. In particular, our\nmodel achieves stateof-the-art results on four summarization tasks, including\nArXiv, CNN/DM, SAMSum, and AMI, and we push PubMed R1 & R2 SOTA further. Our\nmodel significantly outperforms our document-level machine translation baseline\nby 28 BLEU on the WMT19 EN-DE document translation task. We also investigate\nwhat the hierarchical layers learn by visualizing the hierarchical\nencoder-decoder attention. Finally, we study hierarchical learning on\nencoder-only pre-training and analyze its performance on classification\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:57:32 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rohde", "Tobias", ""], ["Wu", "Xiaoxia", ""], ["Liu", "Yinhan", ""]]}, {"id": "2104.07554", "submitter": "Tom Sherborne", "authors": "Tom Sherborne, Mirella Lapata", "title": "Zero-Shot Cross-lingual Semantic Parsing", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work in crosslingual semantic parsing has successfully applied machine\ntranslation to localize accurate parsing to new languages. However, these\nadvances assume access to high-quality machine translation systems, and tools\nsuch as word aligners, for all test languages. We remove these assumptions and\nstudy cross-lingual semantic parsing as a zero-shot problem without parallel\ndata for 7 test languages (DE, ZH, FR, ES, PT, HI, TR). We propose a multi-task\nencoder-decoder model to transfer parsing knowledge to additional languages\nusing only English-Logical form paired data and unlabeled, monolingual\nutterances in each test language. We train an encoder to generate\nlanguage-agnostic representations jointly optimized for generating logical\nforms or utterance reconstruction and against language discriminability. Our\nsystem frames zero-shot parsing as a latent-space alignment problem and finds\nthat pre-trained models can be improved to generate logical forms with minimal\ncross-lingual transfer penalty. Experimental results on Overnight and a new\nexecutable version of MultiATIS++ find that our zero-shot approach performs\nabove back-translation baselines and, in some cases, approaches the supervised\nupper bound.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:08:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sherborne", "Tom", ""], ["Lapata", "Mirella", ""]]}, {"id": "2104.07555", "submitter": "Jacopo Staiano", "authors": "Cl\\'ement Rebuffel, Thomas Scialom, Laure Soulier, Benjamin\n  Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, Patrick\n  Gallinari", "title": "Data-QuestEval: A Referenceless Metric for Data to Text Semantic\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore how QuestEval, which is a Text-vs-Text metric, can\nbe adapted for the evaluation of Data-to-Text Generation systems. QuestEval is\na reference-less metric that compares the predictions directly to the\nstructured input data by automatically asking and answering questions. Its\nadaptation to Data-to-Text is not straightforward as it requires multi-modal\nQuestion Generation and Answering (QG \\& QA) systems. To this purpose, we\npropose to build synthetic multi-modal corpora that enables to train\nmulti-modal QG/QA. The resulting metric is reference-less, multi-modal; it\nobtains state-of-the-art correlations with human judgement on the E2E and\nWebNLG benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:10:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rebuffel", "Cl\u00e9ment", ""], ["Scialom", "Thomas", ""], ["Soulier", "Laure", ""], ["Piwowarski", "Benjamin", ""], ["Lamprier", "Sylvain", ""], ["Staiano", "Jacopo", ""], ["Scoutheeten", "Geoffrey", ""], ["Gallinari", "Patrick", ""]]}, {"id": "2104.07560", "submitter": "Jacopo Staiano", "authors": "Thomas Scialom, Louis Martin, Jacopo Staiano, \\'Eric Villemonte de la\n  Clergerie, Beno\\^it Sagot", "title": "Rethinking Automatic Evaluation in Sentence Simplification", "comments": "updated affiliation and link to data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluation remains an open research question in Natural Language\nGeneration. In the context of Sentence Simplification, this is particularly\nchallenging: the task requires by nature to replace complex words with simpler\nones that shares the same meaning. This limits the effectiveness of n-gram\nbased metrics like BLEU. Going hand in hand with the recent advances in NLG,\nnew metrics have been proposed, such as BERTScore for Machine Translation. In\nsummarization, the QuestEval metric proposes to automatically compare two texts\nby questioning them.\n  In this paper, we first propose a simple modification of QuestEval allowing\nit to tackle Sentence Simplification. We then extensively evaluate the\ncorrelations w.r.t. human judgement for several metrics including the recent\nBERTScore and QuestEval, and show that the latter obtain state-of-the-art\ncorrelations, outperforming standard metrics like BLEU and SARI. More\nimportantly, we also show that a large part of the correlations are actually\nspurious for all the metrics. To investigate this phenomenon further, we\nrelease a new corpus of evaluated simplifications, this time not generated by\nsystems but instead, written by humans. This allows us to remove the spurious\ncorrelations and draw very different conclusions from the original ones,\nresulting in a better understanding of these metrics. In particular, we raise\nconcerns about very low correlations for most of traditional metrics. Our\nresults show that the only significant measure of the Meaning Preservation is\nour adaptation of QuestEval.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:13:50 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 08:37:14 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Scialom", "Thomas", ""], ["Martin", "Louis", ""], ["Staiano", "Jacopo", ""], ["de la Clergerie", "\u00c9ric Villemonte", ""], ["Sagot", "Beno\u00eet", ""]]}, {"id": "2104.07567", "submitter": "Kurt Shuster", "authors": "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston", "title": "Retrieval Augmentation Reduces Hallucination in Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite showing increasingly human-like conversational abilities,\nstate-of-the-art dialogue models often suffer from factual incorrectness and\nhallucination of knowledge (Roller et al., 2020). In this work we explore the\nuse of neural-retrieval-in-the-loop architectures - recently shown to be\neffective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) -\nfor knowledge-grounded dialogue, a task that is arguably more challenging as it\nrequires querying based on complex multi-turn dialogue context and generating\nconversationally coherent responses. We study various types of architectures\nwith multiple components - retrievers, rankers, and encoder-decoders - with the\ngoal of maximizing knowledgeability while retaining conversational ability. We\ndemonstrate that our best models obtain state-of-the-art performance on two\nknowledge-grounded conversational tasks. The models exhibit open-domain\nconversational capabilities, generalize effectively to scenarios not within the\ntraining data, and, as verified by human evaluations, substantially reduce the\nwell-known problem of knowledge hallucination in state-of-the-art chatbots.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:24:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shuster", "Kurt", ""], ["Poff", "Spencer", ""], ["Chen", "Moya", ""], ["Kiela", "Douwe", ""], ["Weston", "Jason", ""]]}, {"id": "2104.07571", "submitter": "Maharshi Gor", "authors": "Maharshi Gor, Kellie Webster, and Jordan Boyd-Graber", "title": "Towards Deconfounding the Influence of Subject's Demographic\n  Characteristics in Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question Answering (QA) tasks are used as benchmarks of general machine\nintelligence. Therefore, robust QA evaluation is critical, and metrics should\nindicate how models will answer any question. However, major QA datasets have\nskewed distributions over gender, profession, and nationality. Despite that\nskew, models generalize -- we find little evidence that accuracy is lower for\npeople based on gender or nationality. Instead, there is more variation in\nquestion topic and question ambiguity. Adequately accessing the generalization\nof QA systems requires more representative datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:26:54 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gor", "Maharshi", ""], ["Webster", "Kellie", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2104.07578", "submitter": "Matteo Alleman", "authors": "Matteo Alleman, Jonathan Mamou, Miguel A Del Rio, Hanlin Tang, Yoon\n  Kim, SueYeon Chung", "title": "Syntactic Perturbations Reveal Representational Correlates of\n  Hierarchical Phrase Structure in Pretrained Language Models", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While vector-based language representations from pretrained language models\nhave set a new standard for many NLP tasks, there is not yet a complete\naccounting of their inner workings. In particular, it is not entirely clear\nwhat aspects of sentence-level syntax are captured by these representations,\nnor how (if at all) they are built along the stacked layers of the network. In\nthis paper, we aim to address such questions with a general class of\ninterventional, input perturbation-based analyses of representations from\npretrained language models. Importing from computational and cognitive\nneuroscience the notion of representational invariance, we perform a series of\nprobes designed to test the sensitivity of these representations to several\nkinds of structure in sentences. Each probe involves swapping words in a\nsentence and comparing the representations from perturbed sentences against the\noriginal. We experiment with three different perturbations: (1) random\npermutations of n-grams of varying width, to test the scale at which a\nrepresentation is sensitive to word position; (2) swapping of two spans which\ndo or do not form a syntactic phrase, to test sensitivity to global phrase\nstructure; and (3) swapping of two adjacent words which do or do not break\napart a syntactic phrase, to test sensitivity to local phrase structure.\n  Results from these probes collectively suggest that Transformers build\nsensitivity to larger parts of the sentence along their layers, and that\nhierarchical phrase structure plays a role in this process. More broadly, our\nresults also indicate that structured input perturbations widens the scope of\nanalyses that can be performed on often-opaque deep learning systems, and can\nserve as a complement to existing tools (such as supervised linear probes) for\ninterpreting complex black-box models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:30:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Alleman", "Matteo", ""], ["Mamou", "Jonathan", ""], ["Del Rio", "Miguel A", ""], ["Tang", "Hanlin", ""], ["Kim", "Yoon", ""], ["Chung", "SueYeon", ""]]}, {"id": "2104.07605", "submitter": "Jesse Vig", "authors": "Jesse Vig, Wojciech Kry\\'sci\\'nski, Karan Goel, Nazneen Fatema Rajani", "title": "SummVis: Interactive Visual Analysis of Models, Data, and Evaluation for\n  Text Summarization", "comments": "Accepted to ACL 2021 System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel neural architectures, training strategies, and the availability of\nlarge-scale corpora haven been the driving force behind recent progress in\nabstractive text summarization. However, due to the black-box nature of neural\nmodels, uninformative evaluation metrics, and scarce tooling for model and data\nanalysis, the true performance and failure modes of summarization models remain\nlargely unknown. To address this limitation, we introduce SummVis, an\nopen-source tool for visualizing abstractive summaries that enables\nfine-grained analysis of the models, data, and evaluation metrics associated\nwith text summarization. Through its lexical and semantic visualizations, the\ntools offers an easy entry point for in-depth model prediction exploration\nacross important dimensions such as factual consistency or abstractiveness. The\ntool together with several pre-computed model outputs is available at\nhttps://github.com/robustness-gym/summvis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:13:00 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 12:37:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Vig", "Jesse", ""], ["Kry\u015bci\u0144ski", "Wojciech", ""], ["Goel", "Karan", ""], ["Rajani", "Nazneen Fatema", ""]]}, {"id": "2104.07606", "submitter": "Shashi Narayan", "authors": "Shashi Narayan, Yao Zhao, Joshua Maynez, Gon\\c{c}alo Simoes, Ryan\n  McDonald", "title": "Planning with Entity Chains for Abstractive Summarization", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained transformer-based sequence-to-sequence models have become the\ngo-to solution for many text generation tasks, including summarization.\nHowever, the results produced by these models tend to contain significant\nissues such as hallucinations and irrelevant passages. One solution to mitigate\nthese problems is to incorporate better content planning in neural\nsummarization. We propose to use entity chains (i.e., chains of entities\nmentioned in the summary) to better plan and ground the generation of\nabstractive summaries. In particular, we augment the target by prepending it\nwith its entity chain. We experimented with both pre-training and finetuning\nwith this content planning objective. When evaluated on CNN/DailyMail, SAMSum\nand XSum, models trained with this objective improved on entity correctness and\nsummary conciseness, and achieved state-of-the-art performance on ROUGE for\nSAMSum and XSum.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:16:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Narayan", "Shashi", ""], ["Zhao", "Yao", ""], ["Maynez", "Joshua", ""], ["Simoes", "Gon\u00e7alo", ""], ["McDonald", "Ryan", ""]]}, {"id": "2104.07611", "submitter": "Michelle Yuan", "authors": "Michelle Yuan, Patrick Xia, Benjamin Van Durme, Jordan Boyd-Graber", "title": "Adaptive Active Learning for Coreference Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Training coreference resolution models require comprehensively labeled data.\nA model trained on one dataset may not successfully transfer to new domains.\nThis paper investigates an approach to active learning for coreference\nresolution that feeds discrete annotations to an incremental clustering model.\nThe recent developments in incremental coreference resolution allow for a novel\napproach to active learning in this setting. Through this new framework, we\nanalyze important factors in data acquisition, like sources of model\nuncertainty and balancing reading and labeling costs. We explore different\nsettings through simulated labeling with gold data. By lowering the data\nbarrier for coreference, coreference resolvers can rapidly adapt to a series of\npreviously unconsidered domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:21:51 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yuan", "Michelle", ""], ["Xia", "Patrick", ""], ["Van Durme", "Benjamin", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2104.07613", "submitter": "Nasrin Taghizadeh", "authors": "Nasrin Taghizadeh and Ehsan Doostmohammadi and Elham Seifossadat and\n  Hamid R. Rabiee and Maedeh S. Tahaei", "title": "SINA-BERT: A pre-trained Language Model for Analysis of Medical Texts in\n  Persian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have released Sina-BERT, a language model pre-trained on BERT (Devlin et\nal., 2018) to address the lack of a high-quality Persian language model in the\nmedical domain. SINA-BERT utilizes pre-training on a large-scale corpus of\nmedical contents including formal and informal texts collected from a variety\nof online resources in order to improve the performance on health-care related\ntasks. We employ SINA-BERT to complete following representative tasks:\ncategorization of medical questions, medical sentiment analysis, and medical\nquestion retrieval. For each task, we have developed Persian annotated data\nsets for training and evaluation and learnt a representation for the data of\neach task especially complex and long medical questions. With the same\narchitecture being used across tasks, SINA-BERT outperforms BERT-based models\nthat were previously made available in the Persian language.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:22:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Taghizadeh", "Nasrin", ""], ["Doostmohammadi", "Ehsan", ""], ["Seifossadat", "Elham", ""], ["Rabiee", "Hamid R.", ""], ["Tahaei", "Maedeh S.", ""]]}, {"id": "2104.07623", "submitter": "Prasanna Parthasarathi", "authors": "Prasanna Parthasarathi, Koustuv Sinha, Joelle Pineau and Adina\n  Williams", "title": "Sometimes We Want Translationese", "comments": "16 pages, 11 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid progress in Neural Machine Translation (NMT) systems over the last few\nyears has been driven primarily towards improving translation quality, and as a\nsecondary focus, improved robustness to input perturbations (e.g. spelling and\ngrammatical mistakes). While performance and robustness are important\nobjectives, by over-focusing on these, we risk overlooking other important\nproperties. In this paper, we draw attention to the fact that for some\napplications, faithfulness to the original (input) text is important to\npreserve, even if it means introducing unusual language patterns in the\n(output) translation. We propose a simple, novel way to quantify whether an NMT\nsystem exhibits robustness and faithfulness, focusing on the case of word-order\nperturbations. We explore a suite of functions to perturb the word order of\nsource sentences without deleting or injecting tokens, and measure the effects\non the target side in terms of both robustness and faithfulness. Across several\nexperimental conditions, we observe a strong tendency towards robustness rather\nthan faithfulness. These results allow us to better understand the trade-off\nbetween faithfulness and robustness in NMT, and opens up the possibility of\ndeveloping systems where users have more autonomy and control in selecting\nwhich property is best suited for their use case.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:39:47 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Parthasarathi", "Prasanna", ""], ["Sinha", "Koustuv", ""], ["Pineau", "Joelle", ""], ["Williams", "Adina", ""]]}, {"id": "2104.07635", "submitter": "Hossein Rajaby Faghihi", "authors": "Hossein Rajaby Faghihi and Parisa Kordjamshidi", "title": "Time-Stamped Language Model: Teaching Language Models to Understand the\n  Flow of Events", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking entities throughout a procedure described in a text is challenging\ndue to the dynamic nature of the world described in the process. Firstly, we\npropose to formulate this task as a question answering problem. This enables us\nto use pre-trained transformer-based language models on other QA benchmarks by\nadapting those to the procedural text understanding. Secondly, since the\ntransformer-based language models cannot encode the flow of events by\nthemselves, we propose a Time-Stamped Language Model~(TSLM model) to encode\nevent information in LMs architecture by introducing the timestamp encoding.\nOur model evaluated on the Propara dataset shows improvements on the published\nstate-of-the-art results with a $3.1\\%$ increase in F1 score. Moreover, our\nmodel yields better results on the location prediction task on the NPN-Cooking\ndataset. This result indicates that our approach is effective for procedural\ntext understanding in general.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:50:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Faghihi", "Hossein Rajaby", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2104.07637", "submitter": "Yuchen Lian", "authors": "Yuchen Lian, Arianna Bisazza and Tessa Verhoef", "title": "The Effect of Efficient Messaging and Input Variability on Neural-Agent\n  Iterated Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural languages commonly display a trade-off among different strategies to\nconvey constituent roles. A similar trade-off, however, has not been observed\nin recent simulations of iterated language learning with neural network based\nagents (Chaabouni et al., 2019b). In this work, we re-evaluate this result in\nthe light of two important factors, namely: the lack of effort-based pressure\nin the agents and the lack of variability in the initial input language.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:50:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lian", "Yuchen", ""], ["Bisazza", "Arianna", ""], ["Verhoef", "Tessa", ""]]}, {"id": "2104.07639", "submitter": "Xian Li", "authors": "Xian Li, Hongyu Gong", "title": "Robust Optimization for Multilingual Translation with Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual models are parameter-efficient with the prospect improving\nlow-resource languages by leveraging crosslingual transfer. Despite recent\nadvance in massive multilingual translation with ever-growing model and data,\nhow to effectively train multilingual models has not been well understood. In\nthis paper, we show that a common situation in multilingual training, data\nimbalance among languages, poses optimization tension between high resource and\nlow resource languages where the found multilingual solution is often\nsub-optimal for low resources. We show that common training method which\nupsamples low resources can not robustly optimize population loss with risks of\neither underfitting high resource languages or overfitting low resource ones.\nDrawing on recent findings on the geometry of loss landscape and its effect on\ngeneralization, we propose a principled optimization algorithm, Curvature Aware\nTask Scaling (CATS), which adaptively rescales gradients from different tasks\nwith a meta objective of guiding multilingual training to low-curvature\nneighborhoods with uniformly low loss for all languages. We ran experiments on\ncommon benchmarks (TED, WMT and OPUS-100) with varying degrees of data\nimbalance. CATS effectively improved multilingual optimization and as a result\ndemonstrated consistent gains on low resources ( to BLEU) without hurting high\nresources. In addition, CATS is robust to overparameterization and large batch\nsize training, making it a promising training method for massive multilingual\nmodels that truly improve low resource languages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:51:03 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 06:40:27 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 00:05:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Xian", ""], ["Gong", "Hongyu", ""]]}, {"id": "2104.07642", "submitter": "Chih-Chan Tien", "authors": "Chih-chan Tien, Shane Steinert-Threlkeld", "title": "Bilingual alignment transfers to multilingual alignment for unsupervised\n  parallel text mining", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents methods for learning cross-lingual sentence\nrepresentations using paired or unpaired bilingual texts. We hypothesize that\nthe cross-lingual alignment strategy is transferable, and therefore a model\ntrained to align only two languages can encode multilingually more aligned\nrepresentations. And such transfer from bilingual alignment to multilingual\nalignment is a dual-pivot transfer from two pivot languages to other language\npairs. To study this theory, we train an unsupervised model with unpaired\nsentences and another single-pair supervised model with bitexts, both based on\nthe unsupervised language model XLM-R. The experiments evaluate the models as\nuniversal sentence encoders on the task of unsupervised bitext mining on two\ndatasets, where the unsupervised model reaches the state of the art of\nunsupervised retrieval, and the alternative single-pair supervised model\napproaches the performance of multilingually supervised models. The results\nsuggest that bilingual training techniques as proposed can be applied to get\nsentence representations with higher multilingual alignment.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:51:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Tien", "Chih-chan", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "2104.07644", "submitter": "Swarnadeep Saha", "authors": "Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal", "title": "ExplaGraphs: An Explanation Graph Generation Task for Structured\n  Commonsense Reasoning", "comments": "25 pages, 10 tables, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent commonsense-reasoning tasks are typically discriminative in nature,\nwhere a model answers a multiple-choice question for a certain context.\nDiscriminative tasks are limiting because they fail to adequately evaluate the\nmodel's ability to reason and explain predictions with underlying commonsense\nknowledge. They also allow such models to use reasoning shortcuts and not be\n\"right for the right reasons\". In this work, we present ExplaGraphs, a new\ngenerative and structured commonsense-reasoning task (and an associated\ndataset) of explanation graph generation for stance prediction. Specifically,\ngiven a belief and an argument, a model has to predict whether the argument\nsupports or counters the belief and also generate a commonsense-augmented graph\nthat serves as non-trivial, complete, and unambiguous explanation for the\npredicted stance. The explanation graphs for our dataset are collected via\ncrowdsourcing through a novel Collect-Judge-And-Refine graph collection\nframework that improves the graph quality via multiple rounds of verification\nand refinement. A significant 83% of our graphs contain external commonsense\nnodes with diverse structures and reasoning depths. We also propose a\nmulti-level evaluation framework that checks for the structural and semantic\ncorrectness of the generated graphs and their plausibility with human-written\ngraphs. We experiment with state-of-the-art text generation models like BART\nand T5 to generate explanation graphs and observe that there is a large gap\nwith human performance, thereby encouraging useful future work for this new\ncommonsense graph-based explanation generation task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:51:36 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 23:34:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Saha", "Swarnadeep", ""], ["Yadav", "Prateek", ""], ["Bauer", "Lisa", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.07646", "submitter": "Sara Rosenthal", "authors": "Sara Rosenthal, Mihaela Bornea, Avirup Sil", "title": "Are Multilingual BERT models robust? A Case Study on Adversarial Attacks\n  for Multilingual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches have exploited weaknesses in monolingual question answering\n(QA) models by adding adversarial statements to the passage. These attacks\ncaused a reduction in state-of-the-art performance by almost 50%. In this\npaper, we are the first to explore and successfully attack a multilingual QA\n(MLQA) system pre-trained on multilingual BERT using several attack strategies\nfor the adversarial statement reducing performance by as much as 85%. We show\nthat the model gives priority to English and the language of the question\nregardless of the other languages in the QA pair. Further, we also show that\nadding our attack strategies during training helps alleviate the attacks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:55:09 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rosenthal", "Sara", ""], ["Bornea", "Mihaela", ""], ["Sil", "Avirup", ""]]}, {"id": "2104.07650", "submitter": "Ningyu Zhang", "authors": "Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi\n  Tan, Fei Huang, Luo Si, Huajun Chen", "title": "AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reformulate the relation extraction task as mask language\nmodeling and propose a novel adaptive prompt-based finetuning approach. We\npropose an adaptive label words selection mechanism that scatters the relation\nlabel into variable number of label tokens to handle the complex multiple label\nspace. We further introduce an auxiliary entity discriminator object to\nencourage the model to focus on context representation learning. Extensive\nexperiments on benchmark datasets demonstrate that our approach can achieve\nbetter performance on both the few-shot and supervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:57:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chen", "Xiang", ""], ["Xie", "Xin", ""], ["Zhang", "Ningyu", ""], ["Yan", "Jiahuan", ""], ["Deng", "Shumin", ""], ["Tan", "Chuanqi", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.07695", "submitter": "Georgiana Dinu", "authors": "Prafulla Kumar Choubey, Anna Currey, Prashant Mathur, Georgiana Dinu", "title": "Improving Gender Translation Accuracy with Filtered Self-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Targeted evaluations have found that machine translation systems often output\nincorrect gender, even when the gender is clear from context. Furthermore,\nthese incorrectly gendered translations have the potential to reflect or\namplify social biases. We propose a gender-filtered self-training technique to\nimprove gender translation accuracy on unambiguously gendered inputs. This\napproach uses a source monolingual corpus and an initial model to generate\ngender-specific pseudo-parallel corpora which are then added to the training\ndata. We filter the gender-specific corpora on the source and target sides to\nensure that sentence pairs contain and correctly translate the specified\ngender. We evaluate our approach on translation from English into five\nlanguages, finding that our models improve gender translation accuracy without\nany cost to generic translation quality. In addition, we show the viability of\nour approach on several settings, including re-training from scratch,\nfine-tuning, controlling the balance of the training data, forward translation,\nand back-translation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:05:29 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Choubey", "Prafulla Kumar", ""], ["Currey", "Anna", ""], ["Mathur", "Prashant", ""], ["Dinu", "Georgiana", ""]]}, {"id": "2104.07704", "submitter": "Alireza Mohammadshahi", "authors": "Alireza Mohammadshahi, James Henderson", "title": "Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of semantic role labelling (SRL) is to recognise the\npredicate-argument structure of a sentence. Recent models have shown that\nsyntactic information can enhance the SRL performance, but other\nsyntax-agnostic approaches achieved reasonable performance. The best way to\nencode syntactic information for the SRL task is still an open question. In\nthis paper, we propose the Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr)\narchitecture, which encodes the syntactic structure with a novel way to input\ngraph relations as embeddings directly into the self-attention mechanism of\nTransformer. This approach adds a soft bias towards attention patterns that\nfollow the syntactic structure but also allows the model to use this\ninformation to learn alternative patterns. We evaluate our model on both\ndependency-based and span-based SRL datasets, and outperform all previous\nsyntax-aware and syntax-agnostic models in both in-domain and out-of-domain\nsettings, on the CoNLL 2005 and CoNLL 2009 datasets. Our architecture is\ngeneral and can be applied to encode any graph information for a desired\ndownstream task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:14:18 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mohammadshahi", "Alireza", ""], ["Henderson", "James", ""]]}, {"id": "2104.07705", "submitter": "Peter Izsak", "authors": "Peter Izsak, Moshe Berchansky, Omer Levy", "title": "How to Train BERT with an Academic Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large language models \\`a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours, using\nonly 8 low-range 12GB GPUs. We demonstrate that through a combination of\nsoftware optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:17:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Izsak", "Peter", ""], ["Berchansky", "Moshe", ""], ["Levy", "Omer", ""]]}, {"id": "2104.07762", "submitter": "Sarthak Jain", "authors": "Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C.\n  Wallace", "title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?", "comments": "NAACL Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Transformers pretrained over clinical notes from Electronic Health\nRecords (EHR) have afforded substantial gains in performance on predictive\nclinical tasks. The cost of training such models (and the necessity of data\naccess to do so) coupled with their utility motivates parameter sharing, i.e.,\nthe release of pretrained models such as ClinicalBERT. While most efforts have\nused deidentified EHR, many researchers have access to large sets of sensitive,\nnon-deidentified EHR with which they might train a BERT model (or similar).\nWould it be safe to release the weights of such a model if they did? In this\nwork, we design a battery of approaches intended to recover Personal Health\nInformation (PHI) from a trained BERT. Specifically, we attempt to recover\npatient names and conditions with which they are associated. We find that\nsimple probing methods are not able to meaningfully extract sensitive\ninformation from BERT trained over the MIMIC-III corpus of EHR. However, more\nsophisticated \"attacks\" may succeed in doing so: To facilitate such research,\nwe make our experimental setup and baseline probing models available at\nhttps://github.com/elehman16/exposing_patient_data_release\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:40:05 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 22:57:03 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lehman", "Eric", ""], ["Jain", "Sarthak", ""], ["Pichotta", "Karl", ""], ["Goldberg", "Yoav", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2104.07763", "submitter": "Iulian Vlad Serban", "authors": "Francois St-Hilaire, Nathan Burns, Robert Belfer, Muhammad Shayan,\n  Ariella Smofsky, Dung Do Vu, Antoine Frau, Joseph Potochny, Farid Faraji,\n  Vincent Pavero, Neroli Ko, Ansona Onyi Ching, Sabina Elkins, Anush Stepanyan,\n  Adela Matajova, Laurent Charlin, Yoshua Bengio, Iulian Vlad Serban and\n  Ekaterina Kochmar", "title": "Comparative Study of Learning Outcomes for Online Learning Platforms", "comments": "14 pages, 3 figures, 2 tables, accepted at AIED 2021 (2021 Conference\n  on Artificial Intelligence in Education)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization and active learning are key aspects to successful learning.\nThese aspects are important to address in intelligent educational applications,\nas they help systems to adapt and close the gap between students with varying\nabilities, which becomes increasingly important in the context of online and\ndistance learning. We run a comparative head-to-head study of learning outcomes\nfor two popular online learning platforms: Platform A, which follows a\ntraditional model delivering content over a series of lecture videos and\nmultiple-choice quizzes, and Platform B, which creates a personalized learning\nenvironment and provides problem-solving exercises and personalized feedback.\nWe report on the results of our study using pre- and post-assessment quizzes\nwith participants taking courses on an introductory data science topic on two\nplatforms. We observe a statistically significant increase in the learning\noutcomes on Platform B, highlighting the impact of well-designed and\nwell-engineered technology supporting active learning and problem-based\nlearning in online education. Moreover, the results of the self-assessment\nquestionnaire, where participants reported on perceived learning gains, suggest\nthat participants using Platform B improve their metacognition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:40:24 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["St-Hilaire", "Francois", ""], ["Burns", "Nathan", ""], ["Belfer", "Robert", ""], ["Shayan", "Muhammad", ""], ["Smofsky", "Ariella", ""], ["Vu", "Dung Do", ""], ["Frau", "Antoine", ""], ["Potochny", "Joseph", ""], ["Faraji", "Farid", ""], ["Pavero", "Vincent", ""], ["Ko", "Neroli", ""], ["Ching", "Ansona Onyi", ""], ["Elkins", "Sabina", ""], ["Stepanyan", "Anush", ""], ["Matajova", "Adela", ""], ["Charlin", "Laurent", ""], ["Bengio", "Yoshua", ""], ["Serban", "Iulian Vlad", ""], ["Kochmar", "Ekaterina", ""]]}, {"id": "2104.07777", "submitter": "Shubhi Tyagi", "authors": "Shubhi Tyagi, Antonio Bonafonte, Jaime Lorenzo-Trueba, Javier Latorre", "title": "Proteno: Text Normalization with Limited Data for Fast Deployment in\n  Text to Speech Systems", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new\nlanguages is hard. We propose a novel architecture to facilitate it for\nmultiple languages while using data less than 3% of the size of the data used\nby the state of the art results on English. We treat TN as a sequence\nclassification problem and propose a granular tokenization mechanism that\nenables the system to learn majority of the classes and their normalizations\nfrom the training data itself. This is further combined with minimal precoded\nlinguistic knowledge for other classes. We publish the first results on TN for\nTTS in Spanish and Tamil and also demonstrate that the performance of the\napproach is comparable with the previous work done on English. All annotated\ndatasets used for experimentation will be released at\nhttps://github.com/amazon-research/proteno.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:14:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tyagi", "Shubhi", ""], ["Bonafonte", "Antonio", ""], ["Lorenzo-Trueba", "Jaime", ""], ["Latorre", "Javier", ""]]}, {"id": "2104.07782", "submitter": "Ha Thanh Nguyen", "authors": "Ha-Thanh Nguyen, Le-Minh Nguyen", "title": "Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal English is a sublanguage that is important for everyone but not for\neveryone to understand. Pretrained models have become best practices among\ncurrent deep learning approaches for different problems. It would be a waste or\neven a danger if these models were applied in practice without knowledge of the\nsublanguage of the law. In this paper, we raise the issue and propose a trivial\nsolution by introducing BERTLaw a legal sublanguage pretrained model. The\npaper's experiments demonstrate the superior effectiveness of the method\ncompared to the baseline pretrained model\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:25:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nguyen", "Ha-Thanh", ""], ["Nguyen", "Le-Minh", ""]]}, {"id": "2104.07789", "submitter": "Danushka Bollegala", "authors": "Michael Abaho, Danushka Bollegala, Paula Williamson, Susanna Dodd", "title": "Detect and Classify -- Joint Span Detection and Classification for\n  Health Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. We propose a method that uses both\nword-level and sentence-level information to simultaneously perform outcome\nspan detection and outcome type classification. In addition to injecting\ncontextual information to hidden vectors, we use label attention to\nappropriately weight both word-level and sentence-level information.\nExperimental results on several benchmark datasets for health outcome detection\nshow that our model consistently outperforms decoupled methods, reporting\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:47:15 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Abaho", "Michael", ""], ["Bollegala", "Danushka", ""], ["Williamson", "Paula", ""], ["Dodd", "Susanna", ""]]}, {"id": "2104.07800", "submitter": "Revanth Reddy", "authors": "Revanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz,\n  Vittorio Castelli, Heng Ji, Avirup Sil", "title": "Towards Robust Neural Retrieval Models with Synthetic Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown that commonly available machine reading comprehension\n(MRC) datasets can be used to train high-performance neural information\nretrieval (IR) systems. However, the evaluation of neural IR has so far been\nlimited to standard supervised learning settings, where they have outperformed\ntraditional term matching baselines. We conduct in-domain and out-of-domain\nevaluations of neural IR, and seek to improve its robustness across different\nscenarios, including zero-shot settings. We show that synthetic training\nexamples generated using a sequence-to-sequence generator can be effective\ntowards this goal: in our experiments, pre-training with synthetic examples\nimproves retrieval performance in both in-domain and out-of-domain evaluation\non five different test sets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:12:01 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Reddy", "Revanth Gangi", ""], ["Yadav", "Vikas", ""], ["Sultan", "Md Arafat", ""], ["Franz", "Martin", ""], ["Castelli", "Vittorio", ""], ["Ji", "Heng", ""], ["Sil", "Avirup", ""]]}, {"id": "2104.07814", "submitter": "Zihao He", "authors": "Zihao He, Negar Mokhberian, Antonio Camara, Andres Abeliuk, Kristina\n  Lerman", "title": "Detecting Polarized Topics in COVID-19 News Using Partisanship-aware\n  Contextualized Topic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of polarization is still an open research challenge. To\naddress this gap, we propose Partisanship-aware Contextualized Topic Embeddings\n(PaCTE), a method to automatically detect polarized topics from partisan news\nsources. Specifically, we represent the ideology of a news source on a topic by\ncorpus-contextualized topic embedding utilizing a language model that has been\nfinetuned on recognizing partisanship of the news articles, and measure the\npolarization between sources using cosine similarity. We apply our method to a\ncorpus of news about COVID-19 pandemic. Extensive experiments on different news\nsources and topics demonstrate the effectiveness of our method to precisely\ncapture the topical polarization and alignment between different news sources.\nTo help clarify and validate results, we explain the polarization using the\nMoral Foundation Theory.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:05:52 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["He", "Zihao", ""], ["Mokhberian", "Negar", ""], ["Camara", "Antonio", ""], ["Abeliuk", "Andres", ""], ["Lerman", "Kristina", ""]]}, {"id": "2104.07815", "submitter": "Trung Dang", "authors": "Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Peter Chin,\n  Fran\\c{c}oise Beaufays", "title": "A Method to Reveal Speaker Identity in Distributed ASR Training, and How\n  to Counter It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  End-to-end Automatic Speech Recognition (ASR) models are commonly trained\nover spoken utterances using optimization methods like Stochastic Gradient\nDescent (SGD). In distributed settings like Federated Learning, model training\nrequires transmission of gradients over a network. In this work, we design the\nfirst method for revealing the identity of the speaker of a training utterance\nwith access only to a gradient. We propose Hessian-Free Gradients Matching, an\ninput reconstruction technique that operates without second derivatives of the\nloss function (required in prior works), which can be expensive to compute. We\nshow the effectiveness of our method using the DeepSpeech model architecture,\ndemonstrating that it is possible to reveal the speaker's identity with 34%\ntop-1 accuracy (51% top-5 accuracy) on the LibriSpeech dataset. Further, we\nstudy the effect of two well-known techniques, Differentially Private SGD and\nDropout, on the success of our method. We show that a dropout rate of 0.2 can\nreduce the speaker identity accuracy to 0% top-1 (0.5% top-5).\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:15:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Dang", "Trung", ""], ["Thakkar", "Om", ""], ["Ramaswamy", "Swaroop", ""], ["Mathews", "Rajiv", ""], ["Chin", "Peter", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "2104.07829", "submitter": "C.M. Downey", "authors": "C.M. Downey, Fei Xia, Gina-Anne Levow, Shane Steinert-Threlkeld", "title": "A Masked Segmental Language Model for Unsupervised Natural Language\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation remains an important preprocessing step both in languages where\n\"words\" or other important syntactic/semantic units (like morphemes) are not\nclearly delineated by white space, as well as when dealing with continuous\nspeech data, where there is often no meaningful pause between words.\nNear-perfect supervised methods have been developed for use in resource-rich\nlanguages such as Chinese, but many of the world's languages are both\nmorphologically complex, and have no large dataset of \"gold\" segmentations into\nmeaningful units. To solve this problem, we propose a new type of Segmental\nLanguage Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)\nfor use in both unsupervised and lightly supervised segmentation tasks. We\nintroduce a Masked Segmental Language Model (MSLM) built on a span-masking\ntransformer architecture, harnessing the power of a bi-directional masked\nmodeling context and attention. In a series of experiments, our model\nconsistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation\nquality, and performs similarly to the Recurrent model on English (PTB). We\nconclude by discussing the different challenges posed in segmenting\nphonemic-type writing systems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 00:00:05 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Downey", "C. M.", ""], ["Xia", "Fei", ""], ["Levow", "Gina-Anne", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "2104.07831", "submitter": "Ashwin Paranjape", "authors": "Ashwin Paranjape (1), Christopher D. Manning (1) ((1) Stanford\n  University)", "title": "Human-like informative conversations: Better acknowledgements using\n  conditional mutual information", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work aims to build a dialogue agent that can weave new factual content\ninto conversations as naturally as humans. We draw insights from linguistic\nprinciples of conversational analysis and annotate human-human conversations\nfrom the Switchboard Dialog Act Corpus to examine humans strategies for\nacknowledgement, transition, detail selection and presentation. When current\nchatbots (explicitly provided with new factual content) introduce facts into a\nconversation, their generated responses do not acknowledge the prior turns.\nThis is because models trained with two contexts - new factual content and\nconversational history - generate responses that are non-specific w.r.t. one of\nthe contexts, typically the conversational history. We show that specificity\nw.r.t. conversational history is better captured by Pointwise Conditional\nMutual Information ($\\text{pcmi}_h$) than by the established use of Pointwise\nMutual Information ($\\text{pmi}$). Our proposed method, Fused-PCMI, trades off\n$\\text{pmi}$ for $\\text{pcmi}_h$ and is preferred by humans for overall quality\nover the Max-PMI baseline 60% of the time. Human evaluators also judge\nresponses with higher $\\text{pcmi}_h$ better at acknowledgement 74% of the\ntime. The results demonstrate that systems mimicking human conversational\ntraits (in this case acknowledgement) improve overall quality and more broadly\nillustrate the utility of linguistic principles in improving dialogue agents.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 00:13:57 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Paranjape", "Ashwin", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2104.07836", "submitter": "Xiaonan Jing", "authors": "Xiaonan Jing, Qingyuan Hu, Yi Zhang, Julia Taylor Rayz", "title": "Tracing Topic Transitions with Temporal Graph Clusters", "comments": "Accepted as full paper by the 34th International FLAIRS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Twitter serves as a data source for many Natural Language Processing (NLP)\ntasks. It can be challenging to identify topics on Twitter due to continuous\nupdating data stream. In this paper, we present an unsupervised graph based\nframework to identify the evolution of sub-topics within two weeks of\nreal-world Twitter data. We first employ a Markov Clustering Algorithm (MCL)\nwith a node removal method to identify optimal graph clusters from temporal\nGraph-of-Words (GoW). Subsequently, we model the clustering transitions between\nthe temporal graphs to identify the topic evolution. Finally, the transition\nflows generated from both computational approach and human annotations are\ncompared to ensure the validity of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 00:55:31 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Jing", "Xiaonan", ""], ["Hu", "Qingyuan", ""], ["Zhang", "Yi", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2104.07837", "submitter": "Gong Zhang", "authors": "Gong Zhang, Yang Zhou, Sixing Wu, Zeru Zhang, Dejing Dou", "title": "Cross-lingual Entity Alignment with Adversarial Kernel Embedding and\n  Adversarial Knowledge Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-lingual entity alignment, which aims to precisely connect the same\nentities in different monolingual knowledge bases (KBs) together, often suffers\nchallenges from feature inconsistency to sequence context unawareness. This\npaper presents a dual adversarial learning framework for cross-lingual entity\nalignment, DAEA, with two original contributions. First, in order to address\nthe structural and attribute feature inconsistency between entities in two\nknowledge graphs (KGs), an adversarial kernel embedding technique is proposed\nto extract graph-invariant information in an unsupervised manner, and project\ntwo KGs into the common embedding space. Second, in order to further improve\nsuccessful rate of entity alignment, we propose to produce multiple random\nwalks through each entity to be aligned and mask these entities in random\nwalks. With the guidance of known aligned entities in the context of multiple\nrandom walks, an adversarial knowledge translation model is developed to fill\nand translate masked entities in pairwise random walks from two KGs. Extensive\nexperiments performed on real-world datasets show that DAEA can well solve the\nfeature inconsistency and sequence context unawareness issues and significantly\noutperforms thirteen state-of-the-art entity alignment methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 00:57:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhang", "Gong", ""], ["Zhou", "Yang", ""], ["Wu", "Sixing", ""], ["Zhang", "Zeru", ""], ["Dou", "Dejing", ""]]}, {"id": "2104.07838", "submitter": "Adina Williams", "authors": "Adithya Renduchintala and Adina Williams", "title": "Investigating Failures of Automatic Translation in the Case of\n  Unambiguous Gender", "comments": "10 pages, 2 figures, 4 tables, submitting to EMNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer based models are the modern work horses for neural machine\ntranslation (NMT), reaching state of the art across several benchmarks. Despite\ntheir impressive accuracy, we observe a systemic and rudimentary class of\nerrors made by transformer based models with regards to translating from a\nlanguage that doesn't mark gender on nouns into others that do. We find that\neven when the surrounding context provides unambiguous evidence of the\nappropriate grammatical gender marking, no transformer based model we tested\nwas able to accurately gender occupation nouns systematically. We release an\nevaluation scheme and dataset for measuring the ability of transformer based\nNMT models to translate gender morphology correctly in unambiguous contexts\nacross syntactically diverse sentences. Our dataset translates from an English\nsource into 20 languages from several different language families. With the\navailability of this dataset, our hope is that the NMT community can iterate on\nsolutions for this class of especially egregious errors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 00:57:36 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Renduchintala", "Adithya", ""], ["Williams", "Adina", ""]]}, {"id": "2104.07840", "submitter": "Cornelis Adam Varekamp", "authors": "Kees Varekamp", "title": "Are Classes Clusters?", "comments": "7 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentence embedding models aim to provide general purpose embeddings for\nsentences. Most of the models studied in this paper claim to perform well on\nSTS tasks - but they do not report on their suitability for clustering. This\npaper looks at four recent sentence embedding models (Universal Sentence\nEncoder (Cer et al., 2018), Sentence-BERT (Reimers and Gurevych, 2019), LASER\n(Artetxe and Schwenk, 2019), and DeCLUTR (Giorgi et al., 2020)). It gives a\nbrief overview of the ideas behind their implementations. It then investigates\nhow well topic classes in two text classification datasets (Amazon Reviews (Ni\net al., 2019) and News Category Dataset (Misra, 2018)) map to clusters in their\ncorresponding sentence embedding space. While the performance of the resulting\nclassification model is far from perfect, it is better than random. This is\ninteresting because the classification model has been constructed in an\nunsupervised way. The topic classes in these real life topic classification\ndatasets can be partly reconstructed by clustering the corresponding sentence\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 01:16:30 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Varekamp", "Kees", ""]]}, {"id": "2104.07846", "submitter": "Nick McKenna", "authors": "Nick McKenna, Liane Guillou, Mohammad Javad Hosseini, Sander Bijl de\n  Vroe, Mark Steedman", "title": "Multivalent Entailment Graphs for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Drawing inferences between open-domain natural language predicates is a\nnecessity for true language understanding. There has been much progress in\nunsupervised learning of entailment graphs for this purpose. We make three\ncontributions: (1) we reinterpret the Distributional Inclusion Hypothesis to\nmodel entailment between predicates of different valencies, like DEFEAT(Biden,\nTrump) entails WIN(Biden); (2) we actualize this theory by learning\nunsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)\nwe demonstrate the capabilities of these graphs on a novel question answering\ntask. We show that directional entailment is more helpful for inference than\nbidirectional similarity on questions of fine-grained semantics. We also show\nthat drawing on evidence across valencies answers more questions than by using\nonly the same valency evidence.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 01:45:40 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["McKenna", "Nick", ""], ["Guillou", "Liane", ""], ["Hosseini", "Mohammad Javad", ""], ["de Vroe", "Sander Bijl", ""], ["Steedman", "Mark", ""]]}, {"id": "2104.07848", "submitter": "Mamoru Komachi", "authors": "Aomi Koyama and Kengo Hotate and Masahiro Kaneko and Mamoru Komachi", "title": "Comparison of Grammatical Error Correction Using Back-Translation Models", "comments": "10 pages; camera-ready for NAACL Student Research Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Grammatical error correction (GEC) suffers from a lack of sufficient parallel\ndata. Therefore, GEC studies have developed various methods to generate pseudo\ndata, which comprise pairs of grammatical and artificially produced\nungrammatical sentences. Currently, a mainstream approach to generate pseudo\ndata is back-translation (BT). Most previous GEC studies using BT have employed\nthe same architecture for both GEC and BT models. However, GEC models have\ndifferent correction tendencies depending on their architectures. Thus, in this\nstudy, we compare the correction tendencies of the GEC models trained on pseudo\ndata generated by different BT models, namely, Transformer, CNN, and LSTM. The\nresults confirm that the correction tendencies for each error type are\ndifferent for every BT model. Additionally, we examine the correction\ntendencies when using a combination of pseudo data generated by different BT\nmodels. As a result, we find that the combination of different BT models\nimproves or interpolates the F_0.5 scores of each error type compared with that\nof single BT models with different seeds.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 01:58:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Koyama", "Aomi", ""], ["Hotate", "Kengo", ""], ["Kaneko", "Masahiro", ""], ["Komachi", "Mamoru", ""]]}, {"id": "2104.07858", "submitter": "Shitao Xiao", "authors": "Shitao Xiao, Zheng Liu, Yingxia Shao, Defu Lian, Xing Xie", "title": "Search-oriented Differentiable Product Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Product quantization (PQ) is a popular approach for maximum inner product\nsearch (MIPS), which is widely used in ad-hoc retrieval. Recent studies propose\ndifferentiable PQ, where the embedding and quantization modules can be trained\njointly. However, there is a lack of in-depth understanding of appropriate\njoint training objectives; and the improvements over non-differentiable\nbaselines are not consistently positive in reality. In this work, we propose\nSearch-oriented Product Quantization (SoPQ), where a novel training objective\nMCL is formulated. With the minimization of MCL, query and key's matching\nprobability can be maximized for the differentiable PQ. Besides, VCS protocol\nis designed to facilitate the minimization of MCL, and SQL is leveraged to\nrelax the dependency on labeled data. Extensive experiments on 4 real-world\ndatasets validate the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:25:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Xiao", "Shitao", ""], ["Liu", "Zheng", ""], ["Shao", "Yingxia", ""], ["Lian", "Defu", ""], ["Xie", "Xing", ""]]}, {"id": "2104.07868", "submitter": "David Wan", "authors": "David Wan, Chris Kedzie, Faisal Ladhak, Elsbeth Turcan, Petra\n  Galu\\v{s}\\v{c}\\'akov\\'a, Elena Zotkina, Zhengping Jiang, Peter Bell, Kathleen\n  McKeown", "title": "Segmenting Subtitles for Correcting ASR Segmentation Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typical ASR systems segment the input audio into utterances using purely\nacoustic information, which may not resemble the sentence-like units that are\nexpected by conventional machine translation (MT) systems for Spoken Language\nTranslation. In this work, we propose a model for correcting the acoustic\nsegmentation of ASR models for low-resource languages to improve performance on\ndownstream tasks. We propose the use of subtitles as a proxy dataset for\ncorrecting ASR acoustic segmentation, creating synthetic acoustic utterances by\nmodeling common error modes. We train a neural tagging model for correcting ASR\nacoustic segmentation and show that it improves downstream performance on MT\nand audio-document cross-language information retrieval (CLIR).\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:04:10 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wan", "David", ""], ["Kedzie", "Chris", ""], ["Ladhak", "Faisal", ""], ["Turcan", "Elsbeth", ""], ["Galu\u0161\u010d\u00e1kov\u00e1", "Petra", ""], ["Zotkina", "Elena", ""], ["Jiang", "Zhengping", ""], ["Bell", "Peter", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2104.07874", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis, Jill Fain Lehman, Carolyn Ros\\'e, Harry\n  Hochheiser", "title": "Translational NLP: A New Paradigm and General Principles for Natural\n  Language Processing Research", "comments": "Accepted to NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) research combines the study of universal\nprinciples, through basic science, with applied science targeting specific use\ncases and settings. However, the process of exchange between basic NLP and\napplications is often assumed to emerge naturally, resulting in many\ninnovations going unapplied and many important questions left unstudied. We\ndescribe a new paradigm of Translational NLP, which aims to structure and\nfacilitate the processes by which basic and applied NLP research inform one\nanother. Translational NLP thus presents a third research paradigm, focused on\nunderstanding the challenges posed by application needs and how these\nchallenges can drive innovation in basic science and technology design. We show\nthat many significant advances in NLP research have emerged from the\nintersection of basic principles with application needs, and present a\nconceptual framework outlining the stakeholders and key questions in\ntranslational research. Our framework provides a roadmap for developing\nTranslational NLP as a dedicated research area, and identifies general\ntranslational principles to facilitate exchange between basic and applied\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:46:10 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Lehman", "Jill Fain", ""], ["Ros\u00e9", "Carolyn", ""], ["Hochheiser", "Harry", ""]]}, {"id": "2104.07885", "submitter": "Zeyu Liu", "authors": "Leo Z. Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A.\n  Smith", "title": "Probing Across Time: What Does RoBERTa Know and When?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Models of language trained on very large corpora have been demonstrated\nuseful for NLP. As fixed artifacts, they have become the object of intense\nstudy, with many researchers \"probing\" the extent to which linguistic\nabstractions, factual and commonsense knowledge, and reasoning abilities they\nacquire and readily demonstrate. Building on this line of work, we consider a\nnew question: for types of knowledge a language model learns, when during\n(pre)training are they acquired? We plot probing performance across iterations,\nusing RoBERTa as a case study. Among our findings: linguistic knowledge is\nacquired fast, stably, and robustly across domains. Facts and commonsense are\nslower and more domain-sensitive. Reasoning abilities are, in general, not\nstably acquired. As new datasets, pretraining protocols, and probes emerge, we\nbelieve that probing-across-time analyses can help researchers understand the\ncomplex, intermingled learning that these models undergo and guide us toward\nmore efficient approaches that accomplish necessary learning faster.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 04:26:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Liu", "Leo Z.", ""], ["Wang", "Yizhong", ""], ["Kasai", "Jungo", ""], ["Hajishirzi", "Hannaneh", ""], ["Smith", "Noah A.", ""]]}, {"id": "2104.07894", "submitter": "Zach Wood-Doughty", "authors": "Zach Wood-Doughty, Isabel Cachola, and Mark Dredze", "title": "Faithful and Plausible Explanations of Medical Code Predictions", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models that offer excellent predictive performance often\nlack the interpretability necessary to support integrated human machine\ndecision-making. In clinical medicine and other high-risk settings, domain\nexperts may be unwilling to trust model predictions without explanations. Work\nin explainable AI must balance competing objectives along two different axes:\n1) Explanations must balance faithfulness to the model's decision-making with\ntheir plausibility to a domain expert. 2) Domain experts desire local\nexplanations of individual predictions and global explanations of behavior in\naggregate. We propose to train a proxy model that mimics the behavior of the\ntrained model and provides fine-grained control over these trade-offs. We\nevaluate our approach on the task of assigning ICD codes to clinical notes to\ndemonstrate that explanations from the proxy model are faithful and replicate\nthe trained model behavior.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 05:13:36 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wood-Doughty", "Zach", ""], ["Cachola", "Isabel", ""], ["Dredze", "Mark", ""]]}, {"id": "2104.07896", "submitter": "Chen Wu", "authors": "Dawn Drain, Chen Wu, Alexey Svyatkovskiy, Neel Sundaresan", "title": "Generating Bug-Fixes Using Pretrained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting and fixing bugs are two of the most important yet frustrating parts\nof the software development cycle. Existing bug detection tools are based\nmainly on static analyzers, which rely on mathematical logic and symbolic\nreasoning about the program execution to detect common types of bugs. Fixing\nbugs is typically left out to the developer. In this work we introduce\nDeepDebug: a data-driven program repair approach which learns to detect and fix\nbugs in Java methods mined from real-world GitHub repositories. We frame\nbug-patching as a sequence-to-sequence learning task consisting of two steps:\n(i) denoising pretraining, and (ii) supervised finetuning on the target\ntranslation task. We show that pretraining on source code programs improves the\nnumber of patches found by 33% as compared to supervised training from scratch,\nwhile domain-adaptive pretraining from natural language to code further\nimproves the accuracy by another 32%. We refine the standard accuracy\nevaluation metric into non-deletion and deletion-only fixes, and show that our\nbest model generates 75% more non-deletion fixes than the previous state of the\nart. In contrast to prior work, we attain our best results when generating raw\ncode, as opposed to working with abstracted code that tends to only benefit\nsmaller capacity models. Finally, we observe a subtle improvement from adding\nsyntax embeddings along with the standard positional embeddings, as well as\nwith adding an auxiliary task to predict each token's syntactic class. Despite\nfocusing on Java, our approach is language agnostic, requiring only a\ngeneral-purpose parser such as tree-sitter.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 05:27:04 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 23:20:06 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Drain", "Dawn", ""], ["Wu", "Chen", ""], ["Svyatkovskiy", "Alexey", ""], ["Sundaresan", "Neel", ""]]}, {"id": "2104.07908", "submitter": "Guoqing Zheng", "authors": "Mengzhou Xia, Guoqing Zheng, Subhabrata Mukherjee, Milad Shokouhi,\n  Graham Neubig, Ahmed Hassan Awadallah", "title": "MetaXL: Meta Representation Transformation for Low-resource\n  Cross-lingual Learning", "comments": "2021 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics (NAACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of multilingual pre-trained representations and cross-lingual\ntransfer learning is one of the most effective methods for building functional\nNLP systems for low-resource languages. However, for extremely low-resource\nlanguages without large-scale monolingual corpora for pre-training or\nsufficient annotated data for fine-tuning, transfer learning remains an\nunder-studied and challenging task. Moreover, recent work shows that\nmultilingual representations are surprisingly disjoint across languages,\nbringing additional challenges for transfer onto extremely low-resource\nlanguages. In this paper, we propose MetaXL, a meta-learning based framework\nthat learns to transform representations judiciously from auxiliary languages\nto a target one and brings their representation spaces closer for effective\ntransfer. Extensive experiments on real-world low-resource languages - without\naccess to large-scale monolingual corpora or large amounts of labeled data -\nfor tasks like cross-lingual sentiment analysis and named entity recognition\nshow the effectiveness of our approach. Code for MetaXL is publicly available\nat github.com/microsoft/MetaXL.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:15:52 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Xia", "Mengzhou", ""], ["Zheng", "Guoqing", ""], ["Mukherjee", "Subhabrata", ""], ["Shokouhi", "Milad", ""], ["Neubig", "Graham", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2104.07910", "submitter": "Aashi Jain", "authors": "Aashi Jain and Taylor Berg-Kirkpatrick", "title": "An Empirical Study of Extrapolation in Text Generation with Scalar\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an empirical evaluation of extrapolation performance when\nconditioning on scalar control inputs like desired output length, desired edit\nfrom an input sentence, and desired sentiment across three text generation\ntasks. Specifically, we examine a zero-shot setting where models are asked to\ngeneralize to ranges of control values not seen during training. We focus on\nevaluating popular embedding methods for scalar inputs, including both\nlearnable and sinusoidal embeddings, as well as simpler approaches.\nSurprisingly, our findings indicate that the simplest strategy of using scalar\ninputs directly, without further encoding, most reliably allows for successful\nextrapolation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:22:24 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Jain", "Aashi", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2104.07921", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "VGNMN: Video-grounded Neural Module Network to Video-Grounded Language\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural module networks (NMN) have achieved success in image-grounded tasks\nsuch as Visual Question Answering (VQA) on synthetic images. However, very\nlimited work on NMN has been studied in the video-grounded language tasks.\nThese tasks extend the complexity of traditional visual tasks with the\nadditional visual temporal variance. Motivated by recent NMN approaches on\nimage-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN)\nto model the information retrieval process in video-grounded language tasks as\na pipeline of neural modules. VGNMN first decomposes all language components to\nexplicitly resolve any entity references and detect corresponding action-based\ninputs from the question. The detected entities and actions are used as\nparameters to instantiate neural module networks and extract visual cues from\nthe video. Our experiments show that VGNMN can achieve promising performance on\ntwo video-grounded language tasks: video QA and video-grounded dialogues.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:47:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2104.07924", "submitter": "Keshav Singh", "authors": "Keshav Singh, Paul Reisert, Naoya Inoue, Kentaro Inui", "title": "A Comparative Study on Collecting High-Quality Implicit Reasonings at a\n  Large-scale", "comments": "2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explicating implicit reasoning (i.e. warrants) in arguments is a\nlong-standing challenge for natural language understanding systems. While\nrecent approaches have focused on explicating warrants via crowdsourcing or\nexpert annotations, the quality of warrants has been questionable due to the\nextreme complexity and subjectivity of the task. In this paper, we tackle the\ncomplex task of warrant explication and devise various methodologies for\ncollecting warrants. We conduct an extensive study with trained experts to\nevaluate the resulting warrants of each methodology and find that our\nmethodologies allow for high-quality warrants to be collected. We construct a\npreliminary dataset of 6,000 warrants annotated over 600 arguments for 3\ndebatable topics. To facilitate research in related downstream tasks, we\nrelease our guidelines and preliminary dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:03:08 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Singh", "Keshav", ""], ["Reisert", "Paul", ""], ["Inoue", "Naoya", ""], ["Inui", "Kentaro", ""]]}, {"id": "2104.07941", "submitter": "Robert West", "authors": "Roland Aydin, Lars Klein, Arnaud Miribel, Robert West", "title": "Broccoli: Sprinkling Lightweight Vocabulary Learning into Everyday\n  Information Diets", "comments": "Proceedings of The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380209", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of a new language remains to this date a cognitive task that\nrequires considerable diligence and willpower, recent advances and tools\nnotwithstanding. In this paper, we propose Broccoli, a new paradigm aimed at\nreducing the required effort by seamlessly embedding vocabulary learning into\nusers' everyday information diets. This is achieved by inconspicuously\nswitching chosen words encountered by the user for their translation in the\ntarget language. Thus, by seeing words in context, the user can assimilate new\nvocabulary without much conscious effort. We validate our approach in a careful\nuser study, finding that the efficacy of the lightweight Broccoli approach is\ncompetitive with traditional, memorization-based vocabulary learning. The low\ncognitive overhead is manifested in a pronounced decrease in learners' usage of\nmnemonic learning strategies, as compared to traditional learning. Finally, we\nestablish that language patterns in typical information diets are compatible\nwith spaced-repetition strategies, thus enabling an efficient use of the\nBroccoli paradigm. Overall, our work establishes the feasibility of a novel and\npowerful \"install-and-forget\" approach for embedded language acquisition.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:38:05 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Aydin", "Roland", ""], ["Klein", "Lars", ""], ["Miribel", "Arnaud", ""], ["West", "Robert", ""]]}, {"id": "2104.07944", "submitter": "Amir Hadifar", "authors": "Amir Hadifar, Sofie Labat, V\\'eronique Hoste, Chris Develder and\n  Thomas Demeester", "title": "A Million Tweets Are Worth a Few Points: Tuning Transformers for\n  Customer Service Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In online domain-specific customer service applications, many companies\nstruggle to deploy advanced NLP models successfully, due to the limited\navailability of and noise in their datasets. While prior research demonstrated\nthe potential of migrating large open-domain pretrained models for\ndomain-specific tasks, the appropriate (pre)training strategies have not yet\nbeen rigorously evaluated in such social media customer service settings,\nespecially under multilingual conditions. We address this gap by collecting a\nmultilingual social media corpus containing customer service conversations\n(865k tweets), comparing various pipelines of pretraining and finetuning\napproaches, applying them on 5 different end tasks. We show that pretraining a\ngeneric multilingual transformer model on our in-domain dataset, before\nfinetuning on specific end tasks, consistently boosts performance, especially\nin non-English settings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:45:04 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hadifar", "Amir", ""], ["Labat", "Sofie", ""], ["Hoste", "V\u00e9ronique", ""], ["Develder", "Chris", ""], ["Demeester", "Thomas", ""]]}, {"id": "2104.07951", "submitter": "Leon Derczynski", "authors": "Magnus Jacobsen, Mikkel H. S{\\o}rensen, Leon Derczynski", "title": "Optimal Size-Performance Tradeoffs: Weighing PoS Tagger Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Improvement in machine learning-based NLP performance are often presented\nwith bigger models and more complex code. This presents a trade-off: better\nscores come at the cost of larger tools; bigger models tend to require more\nduring training and inference time. We present multiple methods for measuring\nthe size of a model, and for comparing this with the model's performance.\n  In a case study over part-of-speech tagging, we then apply these techniques\nto taggers for eight languages and present a novel analysis identifying which\ntaggers are size-performance optimal. Results indicate that some classical\ntaggers place on the size-performance skyline across languages. Further,\nalthough the deep models have highest performance for multiple scores, it is\noften not the most complex of these that reach peak performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:02:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Jacobsen", "Magnus", ""], ["S\u00f8rensen", "Mikkel H.", ""], ["Derczynski", "Leon", ""]]}, {"id": "2104.07972", "submitter": "Vincent Micheli", "authors": "Vincent Micheli, Fran\\c{c}ois Fleuret", "title": "Language Models are Few-Shot Butlers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pretrained language models demonstrate strong performance in most NLP tasks\nwhen fine-tuned on small task-specific datasets. Hence, these autoregressive\nmodels constitute ideal agents to operate in text-based environments where\nlanguage understanding and generative capabilities are essential. Nonetheless,\ncollecting expert demonstrations in such environments is a time-consuming\nendeavour. We introduce a two-stage procedure to learn from a small set of\ndemonstrations and further improve by interacting with an environment. We show\nthat language models fine-tuned with only 1.2% of the expert demonstrations and\na simple reinforcement learning algorithm achieve a 51% absolute improvement in\nsuccess rate over existing methods in the ALFWorld environment.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:47:07 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Micheli", "Vincent", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "2104.08006", "submitter": "Weizhen Qi", "authors": "Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer Zhou, Biao\n  Cheng, Daxin Jiang, Jiusheng Chen, Ruofei Zhang, Houqiang Li, Nan Duan", "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese,\n  Multi-lingual, Dialog, and Code Generation", "comments": "Accepted by ACL 2021 demo papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now, the pre-training technique is ubiquitous in natural language processing\nfield. ProphetNet is a pre-training based natural language generation method\nwhich shows powerful performance on English text summarization and question\ngeneration tasks. In this paper, we extend ProphetNet into other domains and\nlanguages, and present the ProphetNet family pre-training models, named\nProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We\npre-train a cross-lingual generation model ProphetNet-Multi, a Chinese\ngeneration model ProphetNet-Zh, two open-domain dialog generation models\nProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG\n(Programming Language Generation) model ProphetNet-Code to show the generation\nperformance besides NLG (Natural Language Generation) tasks. In our\nexperiments, ProphetNet-X models achieve new state-of-the-art performance on 10\nbenchmarks. All the models of ProphetNet-X share the same model structure,\nwhich allows users to easily switch between different models. We make the code\nand models publicly available, and we will keep updating more pre-training\nmodels and finetuning scripts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:00:43 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 08:17:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Qi", "Weizhen", ""], ["Gong", "Yeyun", ""], ["Yan", "Yu", ""], ["Xu", "Can", ""], ["Yao", "Bolun", ""], ["Zhou", "Bartuer", ""], ["Cheng", "Biao", ""], ["Jiang", "Daxin", ""], ["Chen", "Jiusheng", ""], ["Zhang", "Ruofei", ""], ["Li", "Houqiang", ""], ["Duan", "Nan", ""]]}, {"id": "2104.08017", "submitter": "Rifat Shahriyar", "authors": "Abdullah Al Ishtiaq, Masum Hasan, Md. Mahim Anjum Haque, Kazi Sajeed\n  Mehrab, Tanveer Muttaqueen, Tahmid Hasan, Anindya Iqbal, Rifat Shahriyar", "title": "BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?", "comments": "Submitted to ICANN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of repetitive code snippets are submitted to code repositories every\nday. To search from these large codebases using simple natural language queries\nwould allow programmers to ideate, prototype, and develop easier and faster.\nAlthough the existing methods have shown good performance in searching codes\nwhen the natural language description contains keywords from the code, they are\nstill far behind in searching codes based on the semantic meaning of the\nnatural language query and semantic structure of the code. In recent years,\nboth natural language and programming language research communities have\ncreated techniques to embed them in vector spaces. In this work, we leverage\nthe efficacy of these embedding models using a simple, lightweight 2-layer\nneural network in the task of semantic code search. We show that our model\nlearns the inherent relationship between the embedding spaces and further\nprobes into the scope of improvement by empirically analyzing the embedding\nmethods. In this analysis, we show that the quality of the code embedding model\nis the bottleneck for our model's performance, and discuss future directions of\nstudy in this area.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:28:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ishtiaq", "Abdullah Al", ""], ["Hasan", "Masum", ""], ["Haque", "Md. Mahim Anjum", ""], ["Mehrab", "Kazi Sajeed", ""], ["Muttaqueen", "Tanveer", ""], ["Hasan", "Tahmid", ""], ["Iqbal", "Anindya", ""], ["Shahriyar", "Rifat", ""]]}, {"id": "2104.08027", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Ivan Vuli\\'c, Anna Korhonen, Nigel Collier", "title": "Fast, Effective and Self-Supervised: Transforming Masked LanguageModels\n  into Universal Lexical and Sentence Encoders", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in less than a\nminute without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective univeral lexical and sentence encoders.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:49:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Liu", "Fangyu", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""], ["Collier", "Nigel", ""]]}, {"id": "2104.08041", "submitter": "Wonseok Hwang", "authors": "Wonseok Hwang, Hyunji Lee, Jinyeong Yim, Geewook Kim, Minjoon Seo", "title": "Cost-effective End-to-end Information Extraction for Semi-structured\n  Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A real-world information extraction (IE) system for semi-structured document\nimages often involves a long pipeline of multiple modules, whose complexity\ndramatically increases its development and maintenance cost. One can instead\nconsider an end-to-end model that directly maps the input to the target output\nand simplify the entire process. However, such generation approach is known to\nlead to unstable performance if not designed carefully. Here we present our\nrecent effort on transitioning from our existing pipeline-based IE system to an\nend-to-end system focusing on practical challenges that are associated with\nreplacing and deploying the system in real, large-scale production. By\ncarefully formulating document IE as a sequence generation task, we show that a\nsingle end-to-end IE system can be built and still achieve competent\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:37:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hwang", "Wonseok", ""], ["Lee", "Hyunji", ""], ["Yim", "Jinyeong", ""], ["Kim", "Geewook", ""], ["Seo", "Minjoon", ""]]}, {"id": "2104.08066", "submitter": "Taichi Iki", "authors": "Taichi Iki, Akiko Aizawa", "title": "Effect of Vision-and-Language Extensions on Natural Language\n  Understanding in Vision-and-Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending language models with structural modifications and\nvision-and-language (V&L) pretraining are successful ways of making V&L models\nthat can ground vision and language. Potential applications of these advanced\nmodels include multi-modal machine reading comprehension models and multi-modal\ndialogue models, which require language ability upon grounding. Although\nlanguage capability is crucial for such applications, the impact of extending\ntheir visual capabilities on their language capabilities is not fully\nunderstood. This paper investigates how visual extension affects the language\ncapability of V&L models using the GLUE benchmark. We found that visual\nextension causes some decreases in language capability and that V&L pretraining\nhas a greater impact than structural modifications on the decreases. Our\nresults suggest the need for further study on pretraining that can maintain or,\nif possible, improve a model's language capability.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:28:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Iki", "Taichi", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2104.08078", "submitter": "Lukas Lange", "authors": "Lukas Lange, Jannik Str\\\"otgen, Heike Adel, Dietrich Klakow", "title": "To Share or not to Share: Predicting Sets of Sources for Model Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low-resource settings, model transfer can help to overcome a lack of\nlabeled data for many tasks and domains. However, predicting useful transfer\nsources is a challenging problem, as even the most similar sources might lead\nto unexpected negative transfer results. Thus, ranking methods based on task\nand text similarity may not be sufficient to identify promising sources. To\ntackle this problem, we propose a method to automatically determine which and\nhow many sources should be exploited. For this, we study the effects of model\ntransfer on sequence labeling across various domains and tasks and show that\nour methods based on model similarity and support vector machines are able to\npredict promising sources, resulting in performance increases of up to 24 F1\npoints.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:44:40 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Lange", "Lukas", ""], ["Str\u00f6tgen", "Jannik", ""], ["Adel", "Heike", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2104.08082", "submitter": "Elliot Schumacher", "authors": "Elliot Schumacher, James Mayfield, and Mark Dredze", "title": "Improving Zero-Shot Multi-Lingual Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking -- the task of identifying references in free text to relevant\nknowledge base representations -- often focuses on single languages. We\nconsider multilingual entity linking, where a single model is trained to link\nreferences to same-language knowledge bases in several languages. We propose a\nneural ranker architecture, which leverages multilingual transformer\nrepresentations of text to be easily applied to a multilingual setting. We then\nexplore how a neural ranker trained in one language (e.g. English) transfers to\nan unseen language (e.g. Chinese), and find that while there is a consistent\nbut not large drop in performance. How can this drop in performance be\nalleviated? We explore adding an adversarial objective to force our model to\nlearn language-invariant representations. We find that using this approach\nimproves recall in several datasets, often matching the in-language\nperformance, thus alleviating some of the performance loss occurring from\nzero-shot transfer.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:50:07 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Schumacher", "Elliot", ""], ["Mayfield", "James", ""], ["Dredze", "Mark", ""]]}, {"id": "2104.08087", "submitter": "Domenic Rosati", "authors": "Domenic Rosati", "title": "Citations are not opinions: a corpus linguistics approach to\n  understanding how citations are made", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Citation content analysis seeks to understand citations based on the language\nused during the making of a citation. A key issue in citation content analysis\nis looking for linguistic structures that characterize distinct classes of\ncitations for the purposes of understanding the intent and function of a\ncitation. Previous works have focused on modeling linguistic features first and\ndrawn conclusions on the language structures unique to each class of citation\nfunction based on the performance of a classification task or inter-annotator\nagreement. In this study, we start with a large sample of a pre-classified\ncitation corpus, 2 million citations from each class of the scite Smart\nCitation dataset (supporting, disputing, and mentioning citations), and analyze\nits corpus linguistics in order to reveal the unique and statistically\nsignificant language structures belonging to each type of citation. By\ngenerating comparison tables for each citation type we present a number of\ninteresting linguistic features that uniquely characterize citation type. What\nwe find is that within citation collocates, there is very low correlation\nbetween citation type and sentiment. Additionally, we find that the\nsubjectivity of citation collocates across classes is very low. These findings\nsuggest that the sentiment of collocates is not a predictor of citation\nfunction and that due to their low subjectivity, an opinion-expressing mode of\nunderstanding citations, implicit in previous citation sentiment analysis\nliterature, is inappropriate. Instead, we suggest that citations can be better\nunderstood as claims-making devices where the citation type can be explained by\nunderstanding how two claims are being compared. By presenting this approach,\nwe hope to inspire similar corpus linguistic studies on citations that derive a\nmore robust theory of citation from an empirical basis using citation corpora\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:52:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rosati", "Domenic", ""]]}, {"id": "2104.08108", "submitter": "Shir Gur", "authors": "Shir Gur, Natalia Neverova, Chris Stauffer, Ser-Nam Lim, Douwe Kiela,\n  Austin Reiter", "title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in using retrieval components over external knowledge sources\nhave shown impressive results for a variety of downstream tasks in natural\nlanguage processing. Here, we explore the use of unstructured external\nknowledge sources of images and their corresponding captions for improving\nvisual question answering (VQA). First, we train a novel alignment model for\nembedding images and captions in the same space, which achieves substantial\nimprovement in performance on image-caption retrieval w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the\ntrained alignment model improve results on VQA over strong baselines. We\nfurther conduct extensive experiments to establish the promise of this\napproach, and examine novel applications for inference time such as\nhot-swapping indices.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:27:45 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gur", "Shir", ""], ["Neverova", "Natalia", ""], ["Stauffer", "Chris", ""], ["Lim", "Ser-Nam", ""], ["Kiela", "Douwe", ""], ["Reiter", "Austin", ""]]}, {"id": "2104.08110", "submitter": "Moustafa Al-Hajj", "authors": "Moustafa Al-Hajj, Mustafa Jarrar", "title": "LU-BZU at SemEval-2021 Task 2: Word2Vec and Lemma2Vec performance in\n  Arabic Word-in-Context disambiguation", "comments": "Accepted at SemEval 2021 Task 2, 8 Pages (6 Pages main content + 2\n  pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a set of experiments to evaluate and compare between the\nperformance of using CBOW Word2Vec and Lemma2Vec models for Arabic\nWord-in-Context (WiC) disambiguation without using sense inventories or sense\nembeddings. As part of the SemEval-2021 Shared Task 2 on WiC disambiguation, we\nused the dev.ar-ar dataset (2k sentence pairs) to decide whether two words in a\ngiven sentence pair carry the same meaning. We used two Word2Vec models:\nWiki-CBOW, a pre-trained model on Arabic Wikipedia, and another model we\ntrained on large Arabic corpora of about 3 billion tokens. Two Lemma2Vec models\nwas also constructed based on the two Word2Vec models. Each of the four models\nwas then used in the WiC disambiguation task, and then evaluated on the\nSemEval-2021 test.ar-ar dataset. At the end, we reported the performance of\ndifferent models and compared between using lemma-based and word-based models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:38:35 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Al-Hajj", "Moustafa", ""], ["Jarrar", "Mustafa", ""]]}, {"id": "2104.08116", "submitter": "Paul R\\\"ottger", "authors": "Paul R\\\"ottger and Janet B. Pierrehumbert", "title": "Temporal Adaptation of BERT and Performance on Downstream Document\n  Classification: Insights from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language use differs between domains and even within a domain, language use\nchanges over time. Previous work shows that adapting pretrained language models\nlike BERT to domain through continued pretraining improves performance on\nin-domain downstream tasks. In this article, we investigate whether adapting\nBERT to time in addition to domain can increase performance even further. For\nthis purpose, we introduce a benchmark corpus of social media comments sampled\nover three years. The corpus consists of 36.36m unlabelled comments for\nadaptation and evaluation on an upstream masked language modelling task as well\nas 0.9m labelled comments for finetuning and evaluation on a downstream\ndocument classification task. We find that temporality matters for both tasks:\ntemporal adaptation improves upstream task performance and temporal finetuning\nimproves downstream task performance. However, we do not find clear evidence\nthat adapting BERT to time and domain improves downstream task performance over\njust adapting to domain. Temporal adaptation captures changes in language use\nin the downstream task, but not those changes that are actually relevant to\nperformance on it.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 13:48:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["R\u00f6ttger", "Paul", ""], ["Pierrehumbert", "Janet B.", ""]]}, {"id": "2104.08139", "submitter": "Junliang Guo", "authors": "Junliang Guo, Zhirui Zhang, Linlin Zhang, Linli Xu, Boxing Chen,\n  Enhong Chen, Weihua Luo", "title": "Towards Variable-Length Textual Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks have shown the vulnerability of machine learning models,\nhowever, it is non-trivial to conduct textual adversarial attacks on natural\nlanguage processing tasks due to the discreteness of data. Most previous\napproaches conduct attacks with the atomic \\textit{replacement} operation,\nwhich usually leads to fixed-length adversarial examples and therefore limits\nthe exploration on the decision space. In this paper, we propose\nvariable-length textual adversarial attacks~(VL-Attack) and integrate three\natomic operations, namely \\textit{insertion}, \\textit{deletion} and\n\\textit{replacement}, into a unified framework, by introducing and manipulating\na special \\textit{blank} token while attacking. In this way, our approach is\nable to more comprehensively find adversarial examples around the decision\nboundary and effectively conduct adversarial attacks. Specifically, our method\ndrops the accuracy of IMDB classification by $96\\%$ with only editing $1.3\\%$\ntokens while attacking a pre-trained BERT model. In addition, fine-tuning the\nvictim model with generated adversarial samples can improve the robustness of\nthe model without hurting the performance, especially for length-sensitive\nmodels. On the task of non-autoregressive machine translation, our method can\nachieve $33.18$ BLEU score on IWSLT14 German-English translation, achieving an\nimprovement of $1.47$ over the baseline model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:37:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Guo", "Junliang", ""], ["Zhang", "Zhirui", ""], ["Zhang", "Linlin", ""], ["Xu", "Linli", ""], ["Chen", "Boxing", ""], ["Chen", "Enhong", ""], ["Luo", "Weihua", ""]]}, {"id": "2104.08142", "submitter": "Joe Stacey", "authors": "Joe Stacey, Yonatan Belinkov and Marek Rei", "title": "Natural Language Inference with a Human Touch: Using Human Explanations\n  to Guide Model Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference (NLI) models are known to learn from biases and\nartefacts within their training data, impacting how well the models generalise\nto other unseen datasets. While previous de-biasing approaches focus on\npreventing models learning from these biases, we instead provide models with\ninformation about how a human would approach the task, with the aim of\nencouraging the model to learn features that will generalise better to\nout-of-domain datasets. Using natural language explanations, we supervise a\nmodel's attention weights to encourage more attention to be paid to the words\npresent in these explanations. For the first time, we show that training with\nhuman generated explanations can simultaneously improve performance both\nin-distribution and out-of-distribution for NLI, whereas most related work on\nrobustness involves a trade-off between the two. Training with the human\nexplanations encourages models to attend more broadly across the sentences,\npaying more attention to words in the premise and less attention to stop-words\nand punctuation. The supervised models attend to words humans believe are\nimportant, creating more robust and better performing NLI models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:45:35 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Stacey", "Joe", ""], ["Belinkov", "Yonatan", ""], ["Rei", "Marek", ""]]}, {"id": "2104.08145", "submitter": "Keyur Faldu", "authors": "Keyur Faldu, Amit Sheth, Prashant Kikani, Hemang Akabari", "title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain\n  Understanding", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized entity representations learned by state-of-the-art deep\nlearning models (BERT, GPT, T5, etc) leverage the attention mechanism to learn\nthe data context. However, these models are still blind to leverage the\nknowledge context present in the knowledge graph. Knowledge context can be\nunderstood as semantics about entities, and their relationship with neighboring\nentities in knowledge graphs. We propose a novel and effective technique to\ninfuse knowledge context from knowledge graphs for conceptual and ambiguous\nentities into models based on transformer architecture. Our novel technique\nproject knowledge graph embedding in the homogeneous vector-space, introduces\nnew token-types for entities, align entity position ids, and a selective\nattention mechanism. We take BERT as a baseline model and implement\n\"KnowledgeInfused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT over a wide range of NLP tasks\nover eight different GLUE datasets. KI-BERT-base model even outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:15:31 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Faldu", "Keyur", ""], ["Sheth", "Amit", ""], ["Kikani", "Prashant", ""], ["Akabari", "Hemang", ""]]}, {"id": "2104.08154", "submitter": "Yaoming Zhu", "authors": "Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, Lei Li", "title": "Serial or Parallel? Plug-able Adapter for multilingual machine\n  translation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing a unified multilingual translation model is a key topic in machine\ntranslation research. However, existing approaches suffer from performance\ndegradation: multilingual models yield inferior performance compared to the\nones trained separately on rich bilingual data. We attribute the performance\ndegradation to two issues: multilingual embedding conflation and multilingual\nfusion effects. To address the two issues, we propose PAM, a Transformer model\naugmented with defusion adaptation for multilingual machine translation.\nSpecifically, PAM consists of embedding and layer adapters to shift the word\nand intermediate representations towards language-specific ones. Extensive\nexperiment results on IWSLT, OPUS-100, and WMT benchmarks show that \\method\noutperforms several strong competitors, including series adapter and\nmultilingual knowledge distillation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:58:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Yaoming", ""], ["Feng", "Jiangtao", ""], ["Zhao", "Chengqi", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2104.08161", "submitter": "Yanai Elazar", "authors": "Yanai Elazar, Hongming Zhang, Yoav Goldberg, Dan Roth", "title": "Back to Square One: Bias Detection, Training and Commonsense\n  Disentanglement in the Winograd Schema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Winograd Schema (WS) has been proposed as a test for measuring\ncommonsense capabilities of models. Recently, pre-trained language model-based\napproaches have boosted performance on some WS benchmarks but the source of\nimprovement is still not clear. We begin by showing that the current evaluation\nmethod of WS is sub-optimal and propose a modification that makes use of twin\nsentences for evaluation. We also propose two new baselines that indicate the\nexistence of biases in WS benchmarks. Finally, we propose a method for\nevaluating WS-like sentences in a zero-shot setting and observe that popular\nlanguage models perform randomly in this setting. We conclude that much of the\napparent progress on WS may not necessarily reflect progress in commonsense\nreasoning, but much of it comes from supervised data, which is not likely to\naccount for all the required commonsense reasoning skills and knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:17:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Elazar", "Yanai", ""], ["Zhang", "Hongming", ""], ["Goldberg", "Yoav", ""], ["Roth", "Dan", ""]]}, {"id": "2104.08164", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Wilker Aziz, Ivan Titov", "title": "Editing Factual Knowledge in Language Models", "comments": "15 pages, 6 figures, 2 tables. Code at\n  https://github.com/nicola-decao/KnowledgeEditor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The factual knowledge acquired during pretraining and stored in the\nparameters of Language Models (LM) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod that can be used to edit this knowledge and, thus, fix 'bugs' or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditor does not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor's efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a 'probe' revealing which components of a model need to be changed to\nmanipulate factual knowledge; our analysis shows that the updates tend to be\nconcentrated on a small subset of components. Code at\nhttps://github.com/nicola-decao/KnowledgeEditor\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:24:42 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["De Cao", "Nicola", ""], ["Aziz", "Wilker", ""], ["Titov", "Ivan", ""]]}, {"id": "2104.08173", "submitter": "Gary Phua", "authors": "Gary Phua, Shaowei Lin, Dario Poletti", "title": "Word2rate: training and evaluating multiple word embeddings as\n  statistical transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using pretrained word embeddings has been shown to be a very effective way in\nimproving the performance of natural language processing tasks. In fact almost\nany natural language tasks that can be thought of has been improved by these\npretrained embeddings. These tasks range from sentiment analysis, translation,\nsequence prediction amongst many others. One of the most successful word\nembeddings is the Word2vec CBOW model proposed by Mikolov trained by the\nnegative sampling technique. Mai et al. modifies this objective to train CMOW\nembeddings that are sensitive to word order. We used a modified version of the\nnegative sampling objective for our context words, modelling the context\nembeddings as a Taylor series of rate matrices. We show that different modes of\nthe Taylor series produce different types of embeddings. We compare these\nembeddings to their similar counterparts like CBOW and CMOW and show that they\nachieve comparable performance. We also introduce a novel left-right context\nsplit objective that improves performance for tasks sensitive to word order.\nOur Word2rate model is grounded in a statistical foundation using rate matrices\nwhile being competitive in variety of language tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:31:29 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Phua", "Gary", ""], ["Lin", "Shaowei", ""], ["Poletti", "Dario", ""]]}, {"id": "2104.08197", "submitter": "Anna Ivanova", "authors": "Anna A. Ivanova, John Hewitt, Noga Zaslavsky", "title": "Probing artificial neural networks: insights from neuroscience", "comments": "ICLR 2021 Workshop: How Can Findings About The Brain Improve AI\n  Systems?", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major challenge in both neuroscience and machine learning is the\ndevelopment of useful tools for understanding complex information processing\nsystems. One such tool is probes, i.e., supervised models that relate features\nof interest to activation patterns arising in biological or artificial neural\nnetworks. Neuroscience has paved the way in using such models through numerous\nstudies conducted in recent decades. In this work, we draw insights from\nneuroscience to help guide probing research in machine learning. We highlight\ntwo important design choices for probes $-$ direction and expressivity $-$ and\nrelate these choices to research goals. We argue that specific research goals\nplay a paramount role when designing a probe and encourage future probing\nstudies to be explicit in stating these goals.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:13:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ivanova", "Anna A.", ""], ["Hewitt", "John", ""], ["Zaslavsky", "Noga", ""]]}, {"id": "2104.08200", "submitter": "Genta Indra Winata", "authors": "Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa\n  Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim,\n  Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti, Pascale Fung", "title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural\n  Language Generation", "comments": "Work in progress, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A benchmark provides an ecosystem to measure the advancement of models with\nstandard datasets and automatic and human evaluation metrics. We introduce\nIndoNLG, the first such benchmark for the Indonesian language for natural\nlanguage generation (NLG). It covers six tasks: summarization, question\nanswering, open chitchat, as well as three different language-pairs of machine\ntranslation tasks. We provide a vast and clean pre-training corpus of\nIndonesian, Sundanese, and Javanese datasets called Indo4B-Plus, which is used\nto train our pre-trained NLG model, IndoBART. We evaluate the effectiveness and\nefficiency of IndoBART by conducting extensive evaluation on all IndoNLG tasks.\nOur findings show that IndoBART achieves competitive performance on Indonesian\ntasks with five times fewer parameters compared to the largest multilingual\nmodel in our benchmark, mBART-LARGE (Liu et al., 2020), and an almost 4x and\n2.5x faster inference time on the CPU and GPU respectively. We additionally\ndemonstrate the ability of IndoBART to learn Javanese and Sundanese, and it\nachieves decent performance on machine translation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:16:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cahyawijaya", "Samuel", ""], ["Winata", "Genta Indra", ""], ["Wilie", "Bryan", ""], ["Vincentio", "Karissa", ""], ["Li", "Xiaohong", ""], ["Kuncoro", "Adhiguna", ""], ["Ruder", "Sebastian", ""], ["Lim", "Zhi Yuan", ""], ["Bahar", "Syafri", ""], ["Khodra", "Masayu Leylia", ""], ["Purwarianti", "Ayu", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.08202", "submitter": "Or Honovich", "authors": "Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor,\n  Omri Abend", "title": "$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues\n  via Question Generation and Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neural knowledge-grounded generative models for dialogue often produce\ncontent that is factually inconsistent with the source text they rely on. As a\nconsequence, such models are unreliable, limiting their real-world\napplicability. Inspired by recent work on evaluating factual consistency in\nabstractive summarization (Durmus et al., 2020; Wang et al., 2020), we propose\nan automatic evaluation metric for factual consistency in knowledge-grounded\ndialogue models using automatic question generation and question answering.\nUnlike previous works which use naive token-based comparison of answer spans,\nour metric makes use of co-reference resolution and natural language inference\ncapabilities which greatly improve its performance. To foster proper\nevaluation, we curate a novel dataset of state-of-the-art dialogue system\noutputs for the Wizard-of-Wikipedia dataset (Dinan et al., 2019), which we\nmanually annotate for factual consistency. We perform a thorough\nmeta-evaluation of our metric against other metrics using the new dataset and\ntwo others, where it greatly outperforms the baselines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:21:16 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Honovich", "Or", ""], ["Choshen", "Leshem", ""], ["Aharoni", "Roee", ""], ["Neeman", "Ella", ""], ["Szpektor", "Idan", ""], ["Abend", "Omri", ""]]}, {"id": "2104.08211", "submitter": "Elizabeth Salesky", "authors": "Elizabeth Salesky, David Etter, Matt Post", "title": "Robust Open-Vocabulary Translation from Visual Text Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation models have discrete vocabularies and commonly use\nsubword segmentation techniques to achieve an 'open-vocabulary.' This approach\nrelies on consistent and correct underlying unicode sequences, and makes models\nsusceptible to degradation from common types of noise and variation. Motivated\nby the robustness of human language processing, we propose the use of visual\ntext representations, which dispense with a finite set of text embeddings in\nfavor of continuous vocabularies created by processing visually rendered text.\nWe show that models using visual text representations approach or match\nperformance of text baselines on clean TED datasets. More importantly, models\nwith visual embeddings demonstrate significant robustness to varied types of\nnoise, achieving e.g., 25.9 BLEU on a character permuted German--English task\nwhere subword models degrade to 1.9.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:37:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Salesky", "Elizabeth", ""], ["Etter", "David", ""], ["Post", "Matt", ""]]}, {"id": "2104.08219", "submitter": "George Chrysostomou", "authors": "George Chrysostomou and Nikolaos Aletras", "title": "Variable Instance-Level Explainability for Text Classification", "comments": "NLP Interpretability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the high accuracy of pretrained transformer networks in text\nclassification, a persisting issue is their significant complexity that makes\nthem hard to interpret. Recent research has focused on developing feature\nscoring methods for identifying which parts of the input are most important for\nthe model to make a particular prediction and use it as an explanation (i.e.\nrationale). A limitation of these approaches is that they assume that a\nparticular feature scoring method should be used across all instances in a\ndataset using a predefined fixed length, which might not be optimal across all\ninstances. To address this, we propose a method for extracting variable-length\nexplanations using a set of different feature scoring methods at\ninstance-level. Our method is inspired by word erasure approaches which assume\nthat the most faithful rationale for a prediction should be the one with the\nhighest divergence between the model's output distribution using the full text\nand the text after removing the rationale for a particular instance. Evaluation\non four standard text classification datasets shows that our method\nconsistently provides more faithful explanations compared to previous\nfixed-length and fixed-feature scoring methods for rationale extraction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:53:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chrysostomou", "George", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2104.08225", "submitter": "Fenia Christopoulou", "authors": "Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou", "title": "Distantly Supervised Relation Extraction with Sentence Reconstruction\n  and Knowledge Base Priors", "comments": "16 pages, 9 figures, Accepted as a long paper at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-task, probabilistic approach to facilitate distantly\nsupervised relation extraction by bringing closer the representations of\nsentences that contain the same Knowledge Base pairs. To achieve this, we bias\nthe latent space of sentences via a Variational Autoencoder (VAE) that is\ntrained jointly with a relation classifier. The latent code guides the pair\nrepresentations and influences sentence reconstruction. Experimental results on\ntwo datasets created via distant supervision indicate that multi-task learning\nresults in performance benefits. Additional exploration of employing Knowledge\nBase priors into the VAE reveals that the sentence space can be shifted towards\nthat of the Knowledge Base, offering interpretability and further improving\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:10:19 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Christopoulou", "Fenia", ""], ["Miwa", "Makoto", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "2104.08231", "submitter": "Xiang Gao", "authors": "Xiang Gao, Yizhe Zhang, Michel Galley, Bill Dolan", "title": "An Adversarially-Learned Turing Test for Dialog Generation Models", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of better automated dialogue evaluation metrics offers the\npotential of accelerate evaluation research on conversational AI. However,\nexisting trainable dialogue evaluation models are generally restricted to\nclassifiers trained in a purely supervised manner, which suffer a significant\nrisk from adversarial attacking (e.g., a nonsensical response that enjoys a\nhigh classification score). To alleviate this risk, we propose an adversarial\ntraining approach to learn a robust model, ATT (Adversarial Turing Test), that\ndiscriminates machine-generated responses from human-written replies. In\ncontrast to previous perturbation-based methods, our discriminator is trained\nby iteratively generating unrestricted and diverse adversarial examples using\nreinforcement learning. The key benefit of this unrestricted adversarial\ntraining approach is allowing the discriminator to improve robustness in an\niterative attack-defense game. Our discriminator shows high accuracy on strong\nattackers including DialoGPT and GPT-3.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:13:14 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gao", "Xiang", ""], ["Zhang", "Yizhe", ""], ["Galley", "Michel", ""], ["Dolan", "Bill", ""]]}, {"id": "2104.08247", "submitter": "Jonas Pfeiffer", "authors": "Clifton Poth, Jonas Pfeiffer, Andreas R\\\"uckl\\'e and Iryna Gurevych", "title": "What to Pre-Train on? Efficient Intermediate Task Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:31:18 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Poth", "Clifton", ""], ["Pfeiffer", "Jonas", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.08251", "submitter": "Keisuke Sakaguchi", "authors": "Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon,\n  Peter Clark, Yejin Choi", "title": "proScript: Partially Ordered Scripts Generation via Pre-trained Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scripts - standardized event sequences describing typical everyday activities\n- have been shown to help understand narratives by providing expectations,\nresolving ambiguity, and filling in unstated information. However, to date they\nhave proved hard to author or extract from text. In this work, we demonstrate\nfor the first time that pre-trained neural language models (LMs) can be be\nfinetuned to generate high-quality scripts, at varying levels of granularity,\nfor a wide range of everyday scenarios (e.g., bake a cake). To do this, we\ncollected a large (6.4k), crowdsourced partially ordered scripts (named\nproScript), which is substantially larger than prior datasets, and developed\nmodels that generate scripts with combining language generation and structure\nprediction. We define two complementary tasks: (i) edge prediction: given a\nscenario and unordered events, organize the events into a valid (possibly\npartial-order) script, and (ii) script generation: given only a scenario,\ngenerate events and organize them into a (possibly partial-order) script. Our\nexperiments show that our models perform well (e.g., F1=75.7 in task (i)),\nillustrating a new approach to overcoming previous barriers to script\ncollection. We also show that there is still significant room for improvement\ntoward human level performance. Together, our tasks, dataset, and models offer\na new research direction for learning script knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:35:10 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sakaguchi", "Keisuke", ""], ["Bhagavatula", "Chandra", ""], ["Bras", "Ronan Le", ""], ["Tandon", "Niket", ""], ["Clark", "Peter", ""], ["Choi", "Yejin", ""]]}, {"id": "2104.08253", "submitter": "Luyu Gao", "authors": "Luyu Gao, Jamie Callan", "title": "Is Your Language Model Ready for Dense Representation Fine-tuning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (LM) have become go-to text representation\nencoders. Prior research used deep LMs to encode text sequences such as\nsentences and passages into single dense vector representations. These dense\nrepresentations have been used in efficient text comparison and embedding-based\nretrieval. However, dense encoders suffer in low resource situations. Many\ntechniques have been developed to solve this problem. Despite their success,\nnot much is known about why this happens. This paper shows that one cause lies\nin the readiness of the LM to expose its knowledge through dense representation\nin fine-tuning, which we term Optimization Readiness. To validate the theory,\nwe present Condenser, a general pre-training architecture based on Transformer\nLMs, to improve dense optimization readiness. We show that fine-tuning from\nCondenser significantly improves performance for small and/or noisy training\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:36:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gao", "Luyu", ""], ["Callan", "Jamie", ""]]}, {"id": "2104.08258", "submitter": "Xiaonan Jing", "authors": "Xiaonan Jing, Yi Zhang, Qingyuan Hu, Julia Taylor Rayz", "title": "Modeling Fuzzy Cluster Transitions for Topic Tracing", "comments": "Accepted as full paper by NAFIPS'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Twitter can be viewed as a data source for Natural Language Processing (NLP)\ntasks. The continuously updating data streams on Twitter make it challenging to\ntrace real-time topic evolution. In this paper, we propose a framework for\nmodeling fuzzy transitions of topic clusters. We extend our previous work on\ncrisp cluster transitions by incorporating fuzzy logic in order to enrich the\nunderlying structures identified by the framework. We apply the methodology to\nboth computer generated clusters of nouns from tweets and human tweet\nannotations. The obtained fuzzy transitions are compared with the crisp\ntransitions, on both computer generated clusters and human labeled topic sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:41:16 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Jing", "Xiaonan", ""], ["Zhang", "Yi", ""], ["Hu", "Qingyuan", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2104.08259", "submitter": "Linlin Zhang", "authors": "Linlin Zhang", "title": "Context-Adaptive Document-Level Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing document-level neural machine translation (NMT) models leverage\na fixed number of the previous or all global source sentences to handle the\ncontext-independent problem in standard NMT. However, the translating of each\nsource sentence benefits from various sizes of context, and inappropriate\ncontext may harm the translation performance. In this work, we introduce a\ndata-adaptive method that enables the model to adopt the necessary and useful\ncontext. Specifically, we introduce a light predictor into two document-level\ntranslation models to select the explicit context. Experiments demonstrate the\nproposed approach can significantly improve the performance over the previous\nmethods with a gain up to 1.99 BLEU points.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:43:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhang", "Linlin", ""]]}, {"id": "2104.08268", "submitter": "Akhila Yerukola", "authors": "Akhila Yerukola, Mason Bretan and Hongxia Jin", "title": "Data Augmentation for Voice-Assistant NLU using BERT-based\n  Interchangeable Rephrase", "comments": "Accepted at EACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data augmentation technique based on byte pair encoding and a\nBERT-like self-attention model to boost performance on spoken language\nunderstanding tasks. We compare and evaluate this method with a range of\naugmentation techniques encompassing generative models such as VAEs and\nperformance-boosting techniques such as synonym replacement and\nback-translation. We show our method performs strongly on domain and intent\nclassification tasks for a voice assistant and in a user-study focused on\nutterance naturalness and semantic similarity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:53:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Yerukola", "Akhila", ""], ["Bretan", "Mason", ""], ["Jin", "Hongxia", ""]]}, {"id": "2104.08273", "submitter": "Lichao Sun", "authors": "Yu Wang, Lichao Sun", "title": "Membership Inference Attacks on Knowledge Graphs", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have become increasingly popular supplemental information\nbecause they represented structural relations between entities. Knowledge graph\nembedding methods (KGE) are used for various downstream tasks, e.g., knowledge\ngraph completion, including triple classification, link prediction. However,\nthe knowledge graph also includes much sensitive information in the training\nset, which is very vulnerable to privacy attacks. In this paper, we conduct\nsuch one attack, i.e., membership inference attack, on four standard KGE\nmethods to explore the privacy vulnerabilities of knowledge graphs. Our\nexperimental results on four benchmark knowledge graph datasets show that our\nprivacy attacks can reveal the membership information leakage of KGE methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:56:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Yu", ""], ["Sun", "Lichao", ""]]}, {"id": "2104.08274", "submitter": "Matthias Hofer", "authors": "Matthias Hofer, Tuan Anh Le, Roger Levy, Josh Tenenbaum", "title": "Learning Evolved Combinatorial Symbols with a Neuro-symbolic Generative\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have the ability to rapidly understand rich combinatorial concepts\nfrom limited data. Here we investigate this ability in the context of auditory\nsignals, which have been evolved in a cultural transmission experiment to study\nthe emergence of combinatorial structure in language. We propose a\nneuro-symbolic generative model which combines the strengths of previous\napproaches to concept learning. Our model performs fast inference drawing on\nneural network methods, while still retaining the interpretability and\ngeneralization from limited data seen in structured generative approaches. This\nmodel outperforms a purely neural network-based approach on classification as\nevaluated against both ground truth and human experimental classification\npreferences, and produces superior reproductions of observed signals as well.\nOur results demonstrate the power of flexible combined neural-symbolic\narchitectures for human-like generalization in raw perceptual domains and\noffers a step towards developing precise computational models of inductive\nbiases in language evolution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:57:51 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hofer", "Matthias", ""], ["Le", "Tuan Anh", ""], ["Levy", "Roger", ""], ["Tenenbaum", "Josh", ""]]}, {"id": "2104.08296", "submitter": "Nafise Sadat Moosavi", "authors": "Nafise Sadat Moosavi, Andreas R\\\"uckl\\'e, Dan Roth, Iryna Gurevych", "title": "Learning to Reason for Text Generation from Scientific Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce SciGen, a new challenge dataset for the task of\nreasoning-aware data-to-text generation consisting of tables from scientific\narticles and their corresponding descriptions. Describing scientific tables\ngoes beyond the surface realization of the table content and requires reasoning\nover table values. The unique properties of SciGen are that (1) tables mostly\ncontain numerical values, and (2) the corresponding descriptions require\narithmetic reasoning. SciGen is therefore the first dataset that assesses the\narithmetic reasoning capabilities of generation models on complex input\nstructures, i.e., tables from scientific articles. We study the effectiveness\nof state-of-the-art data-to-text generation models on SciGen and evaluate the\nresults using common metrics as well as human evaluation. Our results and\nanalyses show that (a) while humans like to reason for describing scientific\ntables, the ability of state-of-the-art models is severely limited on this\ntask, (b) while adding more training data improves the results, it is not the\nsolution for reasoning-aware text generation, and (c) one of the main\nbottlenecks for this task is the lack of proper automatic evaluation metrics.\nThe data, code, and annotations for human evaluation will be available at\nhttps://github.com/UKPLab/SciGen. SciGen opens new avenues for future research\nin reasoning-aware text generation and evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:01:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Moosavi", "Nafise Sadat", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Roth", "Dan", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.08301", "submitter": "Rifat Shahriyar", "authors": "Masum Hasan, Kazi Sajeed Mehrab, Wasi Uddin Ahmad, Rifat Shahriyar", "title": "Text2App: A Framework for Creating Android Apps from Text Descriptions", "comments": "Submitted to EMNLP 2021 System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Text2App -- a framework that allows users to create functional\nAndroid applications from natural language specifications. The conventional\nmethod of source code generation tries to generate source code directly, which\nis impractical for creating complex software. We overcome this limitation by\ntransforming natural language into an abstract intermediate formal language\nrepresenting an application with a substantially smaller number of tokens. The\nintermediate formal representation is then compiled into target source codes.\nThis abstraction of programming details allows seq2seq networks to learn\ncomplex application structures with less overhead. In order to train sequence\nmodels, we introduce a data synthesis method grounded in a human survey. We\ndemonstrate that Text2App generalizes well to unseen combination of app\ncomponents and it is capable of handling noisy natural language instructions.\nWe explore the possibility of creating applications from highly abstract\ninstructions by coupling our system with GPT-3 -- a large pretrained language\nmodel. We perform an extensive human evaluation and identify the capabilities\nand limitations of our system. The source code, a ready-to-run demo notebook,\nand a demo video are publicly available at\n\\url{https://github.com/text2app/Text2App}.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:13:10 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 16:37:15 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hasan", "Masum", ""], ["Mehrab", "Kazi Sajeed", ""], ["Ahmad", "Wasi Uddin", ""], ["Shahriyar", "Rifat", ""]]}, {"id": "2104.08303", "submitter": "Mustafa Canim", "authors": "Michael Glass, Mustafa Canim, Alfio Gliozzo, Saneem Chemmengath,\n  Vishwajeet Kumar, Rishav Chakravarti, Avi Sil, Feifei Pan, Samarth Bharadwaj,\n  Nicolas Rodolfo Fauceglia", "title": "Capturing Row and Column Semantics in Transformer Based Question\n  Answering over Tables", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformer based architectures are recently used for the task of answering\nquestions over tables. In order to improve the accuracy on this task,\nspecialized pre-training techniques have been developed and applied on millions\nof open-domain web tables. In this paper, we propose two novel approaches\ndemonstrating that one can achieve superior performance on table QA task\nwithout even using any of these specialized pre-training techniques. The first\nmodel, called RCI interaction, leverages a transformer based architecture that\nindependently classifies rows and columns to identify relevant cells. While\nthis model yields extremely high accuracy at finding cell values on recent\nbenchmarks, a second model we propose, called RCI representation, provides a\nsignificant efficiency advantage for online QA systems over tables by\nmaterializing embeddings for existing tables. Experiments on recent benchmarks\nprove that the proposed methods can effectively locate cell values on tables\n(up to ~98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction\nmodel outperforms the state-of-the-art transformer based approaches,\npre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4% and\n~18.86% additional precision improvement on the standard WikiSQL benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:22:30 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 21:52:55 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Glass", "Michael", ""], ["Canim", "Mustafa", ""], ["Gliozzo", "Alfio", ""], ["Chemmengath", "Saneem", ""], ["Kumar", "Vishwajeet", ""], ["Chakravarti", "Rishav", ""], ["Sil", "Avi", ""], ["Pan", "Feifei", ""], ["Bharadwaj", "Samarth", ""], ["Fauceglia", "Nicolas Rodolfo", ""]]}, {"id": "2104.08305", "submitter": "Abhyuday Jagannatha", "authors": "Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, Hong Yu", "title": "Membership Inference Attack Susceptibility of Clinical Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) models have been shown to have high empirical\nprivacy leakages. Clinical language models (CLMs) trained on clinical data have\nbeen used to improve performance in biomedical natural language processing\ntasks. In this work, we investigate the risks of training-data leakage through\nwhite-box or black-box access to CLMs. We design and employ membership\ninference attacks to estimate the empirical privacy leaks for model\narchitectures like BERT and GPT2. We show that membership inference attacks on\nCLMs lead to non-trivial privacy leakages of up to 7%. Our results show that\nsmaller models have lower empirical privacy leakages than larger ones, and\nmasked LMs have lower leakages than auto-regressive LMs. We further show that\ndifferentially private CLMs can have improved model utility on clinical domain\nwhile ensuring low empirical privacy leakage. Lastly, we also study the effects\nof group-level membership inference and disease rarity on CLM privacy leakages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:29:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jagannatha", "Abhyuday", ""], ["Rawat", "Bhanu Pratap Singh", ""], ["Yu", "Hong", ""]]}, {"id": "2104.08313", "submitter": "Benjamin Devillers", "authors": "Benjamin Devillers, Bhavin Choksi, Romain Bielawski and Rufin\n  VanRullen", "title": "Does language help generalization in vision models?", "comments": "Paper accepted for presentation at the ViGIL 2021 workshop @NAACL.\n  This version: added models to the comparison (ICMLM, TSM); added tests of\n  adversarial robustness; mistake identified and corrected in the normalization\n  of image features; results and conclusions updated accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:54:14 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 17:23:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Devillers", "Benjamin", ""], ["Choksi", "Bhavin", ""], ["Bielawski", "Romain", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2104.08315", "submitter": "Peter West", "authors": "Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi, Luke Zettlemoyer", "title": "Surface Form Competition: Why the Highest Probability Answer Isn't\n  Always Right", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n  However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n  We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:57:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Holtzman", "Ari", ""], ["West", "Peter", ""], ["Schwartz", "Vered", ""], ["Choi", "Yejin", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2104.08320", "submitter": "Katerina Margatina", "authors": "Katerina Margatina, Loic Barrault, Nikolaos Aletras", "title": "Bayesian Active Learning with Pretrained Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active Learning (AL) is a method to iteratively select data for annotation\nfrom a pool of unlabeled data, aiming to achieve better model performance than\nrandom selection. Previous AL approaches in Natural Language Processing (NLP)\nhave been limited to either task-specific models that are trained from scratch\nat each iteration using only the labeled data at hand or using off-the-shelf\npretrained language models (LMs) that are not adapted effectively to the\ndownstream task. In this paper, we address these limitations by introducing\nBALM; Bayesian Active Learning with pretrained language Models. We first\npropose to adapt the pretrained LM to the downstream task by continuing\ntraining with all the available unlabeled data and then use it for AL. We also\nsuggest a simple yet effective fine-tuning method to ensure that the adapted LM\nis properly trained in both low and high resource scenarios during AL. We\nfinally apply Monte Carlo dropout to the downstream model to obtain\nwell-calibrated confidence scores for data selection with uncertainty sampling.\nOur experiments in five standard natural language understanding tasks\ndemonstrate that BALM provides substantial data efficiency improvements\ncompared to various combinations of acquisition functions, models and\nfine-tuning methods proposed in recent AL literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:07:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Margatina", "Katerina", ""], ["Barrault", "Loic", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2104.08350", "submitter": "Rujun Han", "authors": "Rujun Han, I-Hung Hsu, Jiao Sun, Julia Baylon, Qiang Ning, Dan Roth,\n  Nanyun Pen", "title": "ESTER: A Machine Reading Comprehension Dataset for Event Semantic\n  Relation Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stories and narratives are composed based on a variety of events.\nUnderstanding how these events are semantically related to each other is the\nessence of reading comprehension. Recent event-centric reading comprehension\ndatasets focus on either event arguments or event temporal commonsense.\nAlthough these tasks evaluate machines' ability of narrative understanding,\nhuman like reading comprehension requires the capability to process event-based\nsemantics beyond arguments and temporal commonsense. For example, to understand\ncausality between events, we need to infer motivations or purposes; to\nunderstand event hierarchy, we need to parse the composition of events. To\nfacilitate these tasks, we introduce ESTER, a comprehensive machine reading\ncomprehension (MRC) dataset for Event Semantic Relation Reasoning. We study\nfive most commonly used event semantic relations and formulate them as question\nanswering tasks. Experimental results show that the current SOTA systems\nachieve 60.5%, 57.8%, and 76.3% for event-based F1, token based F1 and HIT@1\nscores respectively, which are significantly below human performances.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:59:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Han", "Rujun", ""], ["Hsu", "I-Hung", ""], ["Sun", "Jiao", ""], ["Baylon", "Julia", ""], ["Ning", "Qiang", ""], ["Roth", "Dan", ""], ["Pen", "Nanyun", ""]]}, {"id": "2104.08376", "submitter": "Elisa Kreiss", "authors": "Elisa Kreiss, Noah D. Goodman, Christopher Potts", "title": "Concadia: Tackling image accessibility with context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images have become an integral part of online media. This has enhanced\nself-expression and the dissemination of knowledge, but it poses serious\naccessibility challenges. Adequate textual descriptions are rare. Captions are\nmore abundant, but they do not consistently provide the needed descriptive\ndetails, and systems trained on such texts inherit these shortcomings. To\naddress this, we introduce the publicly available Wikipedia-based corpus\nConcadia, which consists of 96,918 images with corresponding English-language\ndescriptions, captions, and surrounding context. We use Concadia to further\ncharacterize the commonalities and differences between descriptions and\ncaptions, and this leads us to the hypothesis that captions, while not\nsubstitutes for descriptions, can provide a useful signal for creating\neffective descriptions. We substantiate this hypothesis by showing that image\ncaptioning systems trained on Concadia benefit from having caption embeddings\nas part of their inputs. These experiments also begin to show how Concadia can\nbe a powerful tool in addressing the underlying accessibility issues posed by\nimage data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:25:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kreiss", "Elisa", ""], ["Goodman", "Noah D.", ""], ["Potts", "Christopher", ""]]}, {"id": "2104.08384", "submitter": "Mohammad Sadegh Rasooli", "authors": "Mohammad Sadegh Rasooli, Chris Callison-Burch, Derry Tanti Wijaya", "title": "\"Wikily\" Neural Machine Translation Tailored to Cross-Lingual Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily translation models to unsupervised image\ncaptioning and cross-lingual dependency parser transfer. In image captioning,\nwe train a multi-tasking machine translation and image captioning pipeline for\nArabic and English from which the Arabic training data is a wikily translation\nof the English captioning data. Our captioning results in Arabic are slightly\nbetter than that of its supervised model. In dependency parsing, we translate a\nlarge amount of monolingual text, and use it as an artificial training data in\nan annotation projection framework. We show that our model outperforms recent\nwork on cross-lingual transfer of dependency parsers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:49:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rasooli", "Mohammad Sadegh", ""], ["Callison-Burch", "Chris", ""], ["Wijaya", "Derry Tanti", ""]]}, {"id": "2104.08388", "submitter": "Jind\\v{r}ich Libovick\\'y", "authors": "Jind\\v{r}ich Libovick\\'y, Alexander Fraser", "title": "Neural String Edit Distance", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the neural string edit distance model for string-pair\nclassification and sequence generation based on learned string edit distance.\nWe modify the original expectation-maximization learned edit distance algorithm\ninto a differentiable loss function, allowing us to integrate it into a neural\nnetwork providing a contextual representation of the input. We test the method\non cognate detection, transliteration, and grapheme-to-phoneme conversion. We\nshow that we can trade off between performance and interpretability in a single\nframework. Using contextual representations, which are difficult to interpret,\nwe can match the performance of state-of-the-art string-pair classification\nmodels. Using static embeddings and a minor modification of the loss function,\nwe can force interpretability, at the expense of an accuracy drop.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:16:47 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Libovick\u00fd", "Jind\u0159ich", ""], ["Fraser", "Alexander", ""]]}, {"id": "2104.08392", "submitter": "Ronald Cardenas Acosta", "authors": "Ronald Cardenas and Matthias Galle and Shay B. Cohen", "title": "Unsupervised Extractive Summarization by Human Memory Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization systems face the core challenge of identifying and selecting\nimportant information. In this paper, we tackle the problem of content\nselection in unsupervised extractive summarization of long, structured\ndocuments. We introduce a wide range of heuristics that leverage cognitive\nrepresentations of content units and how these are retained or forgotten in\nhuman memory. We find that properties of these representations of human memory\ncan be exploited to capture relevance of content units in scientific articles.\nExperiments show that our proposed heuristics are effective at leveraging\ncognitive structures and the organization of the document (i.e.\\ sections of an\narticle), and automatic and human evaluations provide strong evidence that\nthese heuristics extract more summary-worthy content units.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:49:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cardenas", "Ronald", ""], ["Galle", "Matthias", ""], ["Cohen", "Shay B.", ""]]}, {"id": "2104.08398", "submitter": "George Stoica", "authors": "George Stoica, Emmanouil Antonios Platanios, Barnab\\'as P\\'oczos", "title": "Re-TACRED: Addressing Shortcomings of the TACRED Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TACRED is one of the largest and most widely used sentence-level relation\nextraction datasets. Proposed models that are evaluated using this dataset\nconsistently set new state-of-the-art performance. However, they still exhibit\nlarge error rates despite leveraging external knowledge and unsupervised\npretraining on large text corpora. A recent study suggested that this may be\ndue to poor dataset quality. The study observed that over 50% of the most\nchallenging sentences from the development and test sets are incorrectly\nlabeled and account for an average drop of 8% f1-score in model performance.\nHowever, this study was limited to a small biased sample of 5k (out of a total\nof 106k) sentences, substantially restricting the generalizability and broader\nimplications of its findings. In this paper, we address these shortcomings by:\n(i) performing a comprehensive study over the whole TACRED dataset, (ii)\nproposing an improved crowdsourcing strategy and deploying it to re-annotate\nthe whole dataset, and (iii) performing a thorough analysis to understand how\ncorrecting the TACRED annotations affects previously published results. After\nverification, we observed that 23.9% of TACRED labels are incorrect. Moreover,\nevaluating several models on our revised dataset yields an average f1-score\nimprovement of 14.3% and helps uncover significant relationships between the\ndifferent models (rather than simply offsetting or scaling their scores by a\nconstant factor). Finally, aside from our analysis we also release Re-TACRED, a\nnew completely re-annotated version of the TACRED dataset that can be used to\nperform reliable evaluation of relation extraction models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:55:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Stoica", "George", ""], ["Platanios", "Emmanouil Antonios", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "2104.08400", "submitter": "Jiaao Chen", "authors": "Jiaao Chen, Diyi Yang", "title": "Structure-Aware Abstractive Conversation Summarization via Discourse and\n  Action Graphs", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstractive conversation summarization has received much attention recently.\nHowever, these generated summaries often suffer from insufficient, redundant,\nor incorrect content, largely due to the unstructured and complex\ncharacteristics of human-human interactions. To this end, we propose to\nexplicitly model the rich structures in conversations for more precise and\naccurate conversation summarization, by first incorporating discourse relations\nbetween utterances and action triples (\"who-doing-what\") in utterances through\nstructured graphs to better encode conversations, and then designing a\nmulti-granularity decoder to generate summaries by combining all levels of\ninformation. Experiments show that our proposed models outperform\nstate-of-the-art methods and generalize well in other domains in terms of both\nautomatic evaluations and human judgments. We have publicly released our code\nat https://github.com/GT-SALT/Structure-Aware-BART.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:04:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Jiaao", ""], ["Yang", "Diyi", ""]]}, {"id": "2104.08401", "submitter": "Peter Clark", "authors": "Nora Kassner, Oyvind Tafjord, Hinrich Schutze, Peter Clark", "title": "Enriching a Model's Notion of Belief using a Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although pretrained language models (PTLMs) have been shown to contain\nsignificant amounts of world knowledge, they can still produce inconsistent\nanswers to questions when probed, even after using specialized training\ntechniques to reduce inconsistency. As a result, it can be hard to identify\nwhat the model actually \"believes\" about the world. Our goal is to reduce this\nproblem, so systems are more globally consistent and accurate in their answers.\nOur approach is to add a memory component - a BeliefBank - that records a\nmodel's answers, and two mechanisms that use it to improve consistency among\nbeliefs. First, a reasoning component - a weighted SAT solver - improves\nconsistency by flipping answers that significantly clash with others. Second, a\nfeedback component re-queries the model but using known beliefs as context. We\nshow that, in a controlled experimental setting, these two mechanisms improve\nboth accuracy and consistency. This is significant as it is a first step\ntowards endowing models with an evolving memory, allowing them to construct a\nmore coherent picture of the world.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:09:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kassner", "Nora", ""], ["Tafjord", "Oyvind", ""], ["Schutze", "Hinrich", ""], ["Clark", "Peter", ""]]}, {"id": "2104.08405", "submitter": "Te-Lin Wu", "authors": "Te-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, Spurthi Amba Hombaiah,\n  Michael Bendersky", "title": "LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document layout comprises both structural and visual (eg. font-sizes)\ninformation that is vital but often ignored by machine learning models. The few\nexisting models which do use layout information only consider textual contents,\nand overlook the existence of contents in other modalities such as images.\nAdditionally, spatial interactions of presented contents in a layout were never\nreally fully exploited. To bridge this gap, we parse a document into content\nblocks (eg. text, table, image) and propose a novel layout-aware multimodal\nhierarchical framework, LAMPreT, to model the blocks and the whole document.\nOur LAMPreT encodes each block with a multimodal transformer in the lower-level\nand aggregates the block-level representations and connections utilizing a\nspecifically designed transformer at the higher-level. We design hierarchical\npretraining objectives where the lower-level model is trained similarly to\nmultimodal grounding models, and the higher-level model is trained with our\nproposed novel layout-aware objectives. We evaluate the proposed model on two\nlayout-aware tasks -- text block filling and image suggestion and show the\neffectiveness of our proposed hierarchical architecture as well as pretraining\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:27:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Te-Lin", ""], ["Li", "Cheng", ""], ["Zhang", "Mingyang", ""], ["Chen", "Tao", ""], ["Hombaiah", "Spurthi Amba", ""], ["Bendersky", "Michael", ""]]}, {"id": "2104.08410", "submitter": "Zhengxuan Wu", "authors": "Zhengxuan Wu, Nelson F. Liu, Christopher Potts", "title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained\n  Models", "comments": "16 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is growing evidence that pretrained language models improve\ntask-specific fine-tuning not just for the languages seen in pretraining, but\nalso for new languages and even non-linguistic data. What is the nature of this\nsurprising cross-domain transfer? We offer a partial answer via a systematic\nexploration of how much transfer occurs when models are denied any information\nabout word identity via random scrambling. In four classification tasks and two\nsequence labeling tasks, we evaluate baseline models, LSTMs using GloVe\nembeddings, and BERT. We find that only BERT shows high rates of transfer into\nour scrambled domains, and for classification but not sequence labeling tasks.\nOur analyses seek to explain why transfer succeeds for some tasks but not\nothers, to isolate the separate contributions of pretraining versus\nfine-tuning, and to quantify the role of word frequency. These findings help\nexplain where and why cross-domain transfer occurs, which can guide future\nstudies and practical fine-tuning efforts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 00:14:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Liu", "Nelson F.", ""], ["Potts", "Christopher", ""]]}, {"id": "2104.08413", "submitter": "Emily Allaway", "authors": "Emily Allaway, Shuai Wang, and Miguel Ballesteros", "title": "Sequential Cross-Document Coreference Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relating entities and events in text is a key component of natural language\nunderstanding. Cross-document coreference resolution, in particular, is\nimportant for the growing interest in multi-document analysis tasks. In this\nwork we propose a new model that extends the efficient sequential prediction\nparadigm for coreference resolution to cross-document settings and achieves\ncompetitive results for both entity and event coreference while provides strong\nevidence of the efficacy of both sequential models and higher-order inference\nin cross-document settings. Our model incrementally composes mentions into\ncluster representations and predicts links between a mention and the already\nconstructed clusters, approximating a higher-order model. In addition, we\nconduct extensive ablation studies that provide new insights into the\nimportance of various inputs and representation types in coreference.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 00:46:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Allaway", "Emily", ""], ["Wang", "Shuai", ""], ["Ballesteros", "Miguel", ""]]}, {"id": "2104.08420", "submitter": "Kira Selby", "authors": "Kira A. Selby (1), Yinong Wang (1), Ruizhe Wang (1), Peyman Passban\n  (2), Ahmad Rashid (2), Mehdi Rezagholizadeh (2) and Pascal Poupart (1) ((1)\n  University of Waterloo, (2) Huawei Noah's Ark Lab)", "title": "Robust Embeddings Via Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent monumental advances in the field, many Natural Language\nProcessing (NLP) models still struggle to perform adequately on noisy domains.\nWe propose a novel probabilistic embedding-level method to improve the\nrobustness of NLP models. Our method, Robust Embeddings via Distributions\n(RED), incorporates information from both noisy tokens and surrounding context\nto obtain distributions over embedding vectors that can express uncertainty in\nsemantic space more fully than any deterministic method. We evaluate our method\non a number of downstream tasks using existing state-of-the-art models in the\npresence of both natural and synthetic noise, and demonstrate a clear\nimprovement over other embedding approaches to robustness from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 02:02:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Selby", "Kira A.", ""], ["Wang", "Yinong", ""], ["Wang", "Ruizhe", ""], ["Passban", "Peyman", ""], ["Rashid", "Ahmad", ""], ["Rezagholizadeh", "Mehdi", ""], ["Poupart", "Pascal", ""]]}, {"id": "2104.08428", "submitter": "Kaiqi Fu", "authors": "Kaiqi Fu and Jones Lin and Dengfeng Ke and Yanlu Xie and Jinsong Zhang\n  and Binghuai Lin", "title": "A Full Text-Dependent End to End Mispronunciation Detection and\n  Diagnosis with Easy Data Augmentation Techniques", "comments": "Submitted to INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end mispronunciation detection and diagnosis (MD&D) systems\nhas become a popular alternative to greatly simplify the model-building process\nof conventional hybrid DNN-HMM systems by representing complicated modules with\na single deep network architecture. In this paper, in order to utilize the\nprior text in the end-to-end structure, we present a novel text-dependent model\nwhich is difference with sed-mdd, the model achieves a fully end-to-end system\nby aligning the audio with the phoneme sequences of the prior text inside the\nmodel through the attention mechanism. Moreover, the prior text as input will\nbe a problem of imbalance between positive and negative samples in the phoneme\nsequence. To alleviate this problem, we propose three simple data augmentation\nmethods, which effectively improve the ability of model to capture\nmispronounced phonemes. We conduct experiments on L2-ARCTIC, and our best\nperformance improved from 49.29% to 56.08% in F-measure metric compared to the\nCNN-RNN-CTC model.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 03:11:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fu", "Kaiqi", ""], ["Lin", "Jones", ""], ["Ke", "Dengfeng", ""], ["Xie", "Yanlu", ""], ["Zhang", "Jinsong", ""], ["Lin", "Binghuai", ""]]}, {"id": "2104.08433", "submitter": "Angana Borah", "authors": "Angana Borah, Manash Pratim Barman, Amit Awekar", "title": "Are Word Embedding Methods Stable and Should We Care About It?", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A representation learning method is considered stable if it consistently\ngenerates similar representation of the given data across multiple runs. Word\nEmbedding Methods (WEMs) are a class of representation learning methods that\ngenerate dense vector representation for each word in the given text data. The\ncentral idea of this paper is to explore the stability measurement of WEMs\nusing intrinsic evaluation based on word similarity. We experiment with three\npopular WEMs: Word2Vec, GloVe, and fastText. For stability measurement, we\ninvestigate the effect of five parameters involved in training these models. We\nperform experiments using four real-world datasets from different domains:\nWikipedia, News, Song lyrics, and European parliament proceedings. We also\nobserve the effect of WEM stability on three downstream tasks: Clustering, POS\ntagging, and Fairness evaluation. Our experiments indicate that amongst the\nthree WEMs, fastText is the most stable, followed by GloVe and Word2Vec.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 03:29:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Borah", "Angana", ""], ["Barman", "Manash Pratim", ""], ["Awekar", "Amit", ""]]}, {"id": "2104.08443", "submitter": "Yongqi Li", "authors": "Yongqi Li, Wenjie Li, Liqiang Nie", "title": "A Graph-guided Multi-round Retrieval Method for Conversational\n  Open-domain Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, conversational agents have provided a natural and convenient\naccess to useful information in people's daily life, along with a broad and new\nresearch topic, conversational question answering (QA). Among the popular\nconversational QA tasks, conversational open-domain QA, which requires to\nretrieve relevant passages from the Web to extract exact answers, is more\npractical but less studied. The main challenge is how to well capture and fully\nexplore the historical context in conversation to facilitate effective\nlarge-scale retrieval. The current work mainly utilizes history questions to\nrefine the current question or to enhance its representation, yet the relations\nbetween history answers and the current answer in a conversation, which is also\ncritical to the task, are totally neglected. To address this problem, we\npropose a novel graph-guided retrieval method to model the relations among\nanswers across conversation turns. In particular, it utilizes a passage graph\nderived from the hyperlink-connected passages that contains history answers and\npotential current answers, to retrieve more relevant passages for subsequent\nanswer extraction. Moreover, in order to collect more complementary information\nin the historical context, we also propose to incorporate the multi-round\nrelevance feedback technique to explore the impact of the retrieval context on\ncurrent question understanding. Experimental results on the public dataset\nverify the effectiveness of our proposed method. Notably, the F1 score is\nimproved by 5% and 11% with predicted history answers and true history answers,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:39:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Yongqi", ""], ["Li", "Wenjie", ""], ["Nie", "Liqiang", ""]]}, {"id": "2104.08444", "submitter": "Yuqi Si", "authors": "Yuqi Si and Kirk Roberts", "title": "Hierarchical Transformer Networks for Longitudinal Clinical Document\n  Classification", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Hierarchical Transformer Networks for modeling long-term\ndependencies across clinical notes for the purpose of patient-level prediction.\nThe network is equipped with three levels of Transformer-based encoders to\nlearn progressively from words to sentences, sentences to notes, and finally\nnotes to patients. The first level from word to sentence directly applies a\npre-trained BERT model, and the second and third levels both implement a stack\nof 2-layer encoders before the final patient representation is fed into the\nclassification layer for clinical predictions. Compared to traditional BERT\nmodels, our model increases the maximum input length from 512 words to much\nlonger sequences that are appropriate for long sequences of clinical notes. We\nempirically examine and experiment with different parameters to identify an\noptimal trade-off given computational resource limits. Our experimental results\non the MIMIC-III dataset for different prediction tasks demonstrate that our\nproposed hierarchical model outperforms previous state-of-the-art hierarchical\nneural networks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:45:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Si", "Yuqi", ""], ["Roberts", "Kirk", ""]]}, {"id": "2104.08445", "submitter": "Sewon Min", "authors": "Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, Hannaneh\n  Hajishirzi", "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study multi-answer retrieval, an under-explored problem that requires\nretrieving passages to cover multiple distinct answers for a given question.\nThis task requires joint modeling of retrieved passages, as models should not\nrepeatedly retrieve passages containing the same answer at the cost of missing\na different valid answer. Prior work focusing on single-answer retrieval is\nlimited as it cannot reason about the set of passages jointly. In this paper,\nwe introduce JPR, a joint passage retrieval model focusing on reranking. To\nmodel the joint probability of the retrieved passages, JPR makes use of an\nautoregressive reranker that selects a sequence of passages, equipped with\nnovel training and decoding algorithms. Compared to prior approaches, JPR\nachieves significantly better answer coverage on three multi-answer datasets.\nWhen combined with downstream question answering, the improved retrieval\nenables larger answer generation models since they need to consider fewer\npassages, establishing a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:48:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Min", "Sewon", ""], ["Lee", "Kenton", ""], ["Chang", "Ming-Wei", ""], ["Toutanova", "Kristina", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2104.08448", "submitter": "Yongqi Li", "authors": "Yongqi Li, Wenjie Li", "title": "Data Distillation for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have achieved great success in many fields, while at\nthe same time deep learning models are getting more complex and expensive to\ncompute. It severely hinders the wide applications of these models. In order to\nalleviate this problem, model distillation emerges as an effective means to\ncompress a large model into a smaller one without a significant drop in\naccuracy. In this paper, we study a related but orthogonal issue, data\ndistillation, which aims to distill the knowledge from a large training dataset\ndown to a smaller and synthetic one. It has the potential to address the large\nand growing neural network training problem based on the small dataset. We\ndevelop a novel data distillation method for text classification. We evaluate\nour method on eight benchmark datasets. The results that the distilled data\nwith the size of 0.1% of the original text data achieves approximately 90%\nperformance of the original is rather impressive.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:54:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Yongqi", ""], ["Li", "Wenjie", ""]]}, {"id": "2104.08451", "submitter": "Zuohui Fu", "authors": "Zhe Hu, Zuohui Fu, Yu Yin, Gerard de Melo and Cheng Peng", "title": "Context-Aware Interaction Network for Question Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Impressive milestones have been achieved in text matching by adopting a\ncross-attention mechanism to capture pertinent semantic connections between two\nsentences. However, these cross-attention mechanisms focus on word-level links\nbetween the two inputs, neglecting the importance of contextual information. We\npropose a context-aware interaction network (COIN) to properly align two\nsequences and infer their semantic relationship. Specifically, each interaction\nblock includes (1) a context-aware cross-attention mechanism to effectively\nintegrate contextual information, and (2) a gate fusion layer to flexibly\ninterpolate aligned representations. We apply multiple stacked interaction\nblocks to produce alignments at different levels and gradually refine the\nattention results. Experiments on two question matching datasets and detailed\nanalyses confirm the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:03:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hu", "Zhe", ""], ["Fu", "Zuohui", ""], ["Yin", "Yu", ""], ["de Melo", "Gerard", ""], ["Peng", "Cheng", ""]]}, {"id": "2104.08453", "submitter": "Lei Xu", "authors": "Lei Xu, Kalyan Veeramachaneni", "title": "Attacking Text Classifiers via Sentence Rewriting Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most adversarial attack methods on text classification can change the\nclassifier's prediction by synonym substitution. We propose the adversarial\nsentence rewriting sampler (ASRS), which rewrites the whole sentence to\ngenerate more similar and higher-quality adversarial examples. Our method\nachieves a better attack success rate on 4 out of 7 datasets, as well as\nsignificantly better sentence quality on all 7 datasets. ASRS is an\nindispensable supplement to the existing attack methods, because classifiers\ncannot resist the attack from ASRS unless they are trained on adversarial\nexamples found by ASRS.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:21:35 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 11:21:51 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xu", "Lei", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "2104.08455", "submitter": "Nouha Dziri", "authors": "Nouha Dziri, Andrea Madotto, Osmar Zaiane, Avishek Joey Bose", "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path\n  Grounding", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue systems powered by large pre-trained language models (LM) exhibit an\ninnate ability to deliver fluent and natural-looking responses. Despite their\nimpressive generation performance, these models can often generate factually\nincorrect statements impeding their widespread adoption. In this paper, we\nfocus on the task of improving the faithfulness -- and thus reduce\nhallucination -- of Neural Dialogue Systems to known facts supplied by a\nKnowledge Graph (KG). We propose Neural Path Hunter which follows a\ngenerate-then-refine strategy whereby a generated response is amended using the\nk-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level\nfact critic to identify plausible sources of hallucination followed by a\nrefinement stage consisting of a chain of two neural LM's that retrieves\ncorrect entities by crafting a query signal that is propagated over the k-hop\nsubgraph. Our proposed model can easily be applied to any dialogue generated\nresponses without retraining the model. We empirically validate our proposed\napproach on the OpenDialKG dataset against a suite of metrics and report a\nrelative improvement of faithfulness over GPT2 dialogue responses by 8.4%.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:23:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dziri", "Nouha", ""], ["Madotto", "Andrea", ""], ["Zaiane", "Osmar", ""], ["Bose", "Avishek Joey", ""]]}, {"id": "2104.08457", "submitter": "Patrick Xia", "authors": "Patrick Xia, Benjamin Van Durme", "title": "Moving on from OntoNotes: Coreference Resolution Model Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic neural models for coreference resolution are typically trained on a\nsingle dataset (OntoNotes) and model improvements are then benchmarked on that\ndataset. However, real-world usages of coreference resolution models depend on\nthe annotation guidelines and the domain of the target dataset, which often\ndiffer from those of OntoNotes. We aim to quantify transferability of\ncoreference resolution models based on the number of annotated documents\navailable in the target dataset. We examine five target datasets and find that\ncontinued training is consistently effective and especially beneficial when\nthere are few target documents. We establish new benchmarks across several\ndatasets, including state-of-the-art results on LitBank and PreCo.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:35:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xia", "Patrick", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2104.08459", "submitter": "Yerbolat Khassanov", "authors": "Saida Mussakhojayeva, Aigerim Janaliyeva, Almas Mirzakhmetov, Yerbolat\n  Khassanov, Huseyin Atakan Varol", "title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset", "comments": "5 pages, 4 tables, 2 figures, accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a high-quality open-source speech synthesis dataset for\nKazakh, a low-resource language spoken by over 13 million people worldwide. The\ndataset consists of about 93 hours of transcribed audio recordings spoken by\ntwo professional speakers (female and male). It is the first publicly available\nlarge-scale dataset developed to promote Kazakh text-to-speech (TTS)\napplications in both academia and industry. In this paper, we share our\nexperience by describing the dataset development procedures and faced\nchallenges, and discuss important future directions. To demonstrate the\nreliability of our dataset, we built baseline end-to-end TTS models and\nevaluated them using the subjective mean opinion score (MOS) measure.\nEvaluation results show that the best TTS models trained on our dataset achieve\nMOS above 4 for both speakers, which makes them applicable for practical use.\nThe dataset, training recipe, and pretrained TTS models are freely available.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:49:57 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 04:39:18 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 09:36:25 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Mussakhojayeva", "Saida", ""], ["Janaliyeva", "Aigerim", ""], ["Mirzakhmetov", "Almas", ""], ["Khassanov", "Yerbolat", ""], ["Varol", "Huseyin Atakan", ""]]}, {"id": "2104.08462", "submitter": "Sitanshu Gakkhar", "authors": "Sitanshu Gakkhar, Matilde Marcolli", "title": "Syntactic structures and the general Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We further the theme of studying syntactic structures data from Longobardi\n(2017b), Collins (2010), Ceolin et al. (2020) and Koopman (2011) using general\nMarkov models initiated in Shu et al. (2017), exploring the question of how\nconsistent the data is with the idea that general Markov models. The ideas\nexplored in the present paper are more generally applicable than to the setting\nof syntactic structures, and can be used when analyzing consistency of data\nwith general Markov models. Additionally, we give an interpretation of the\nmethods of Ceolin et al. (2020) as an infinite sites evolutionary model and\ncompare it to the Markov model and explore each in the context of evolutionary\nprocesses acting on human language syntax.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:58:16 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 04:06:55 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Gakkhar", "Sitanshu", ""], ["Marcolli", "Matilde", ""]]}, {"id": "2104.08464", "submitter": "Naomi Shapiro", "authors": "Naomi Tachikawa Shapiro, Amandalynne Paullada, Shane\n  Steinert-Threlkeld", "title": "A multilabel approach to morphosyntactic probing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a multilabel probing task to assess the morphosyntactic\nrepresentations of word embeddings from multilingual language models. We\ndemonstrate this task with multilingual BERT (Devlin et al., 2018), training\nprobes for seven typologically diverse languages of varying morphological\ncomplexity: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish.\nThrough this simple but robust paradigm, we show that multilingual BERT renders\nmany morphosyntactic features easily and simultaneously extractable (e.g.,\ngender, grammatical case, pronominal type). We further evaluate the probes on\nsix \"held-out\" languages in a zero-shot transfer setting: Arabic, Chinese,\nMarathi, Slovenian, Tagalog, and Yoruba. This style of probing has the added\nbenefit of revealing the linguistic properties that language models recognize\nas being shared across languages. For instance, the probes performed well on\nrecognizing nouns in the held-out languages, suggesting that multilingual BERT\nhas a conception of noun-hood that transcends individual languages; yet, the\nsame was not true of adjectives.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 06:24:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shapiro", "Naomi Tachikawa", ""], ["Paullada", "Amandalynne", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "2104.08465", "submitter": "Kaitlyn Zhou", "authors": "Kaitlyn Zhou, Kawin Ethayarajh, Dan Jurafsky", "title": "Frequency-based Distortions in Contextualized Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does word frequency in pre-training data affect the behavior of\nsimilarity metrics in contextualized BERT embeddings? Are there systematic ways\nin which some word relationships are exaggerated or understated? In this work,\nwe explore the geometric characteristics of contextualized word embeddings with\ntwo novel tools: (1) an identity probe that predicts the identity of a word\nusing its embedding; (2) the minimal bounding sphere for a word's\ncontextualized representations. Our results reveal that words of high and low\nfrequency differ significantly with respect to their representational geometry.\nSuch differences introduce distortions: when compared to human judgments, point\nestimates of embedding similarity (e.g., cosine similarity) can over- or\nunder-estimate the semantic similarity of two words, depending on the frequency\nof those words in the training data. This has downstream societal implications:\nBERT-Base has more trouble differentiating between South American and African\ncountries than North American and European ones. We find that these distortions\npersist when using BERT-Multilingual, suggesting that they cannot be easily\nfixed with additional data, which in turn introduces new distortions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 06:35:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Kaitlyn", ""], ["Ethayarajh", "Kawin", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2104.08478", "submitter": "Mamoru Komachi", "authors": "Seiichiro Kondo and Kengo Hotate and Masahiro Kaneko and Mamoru\n  Komachi", "title": "Sentence Concatenation Approach to Data Augmentation for Neural Machine\n  Translation", "comments": "7 pages; camera-ready for NAACL Student Research Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural machine translation (NMT) has recently gained widespread attention\nbecause of its high translation accuracy. However, it shows poor performance in\nthe translation of long sentences, which is a major issue in low-resource\nlanguages. It is assumed that this issue is caused by insufficient number of\nlong sentences in the training data. Therefore, this study proposes a simple\ndata augmentation method to handle long sentences. In this method, we use only\nthe given parallel corpora as the training data and generate long sentences by\nconcatenating two sentences. Based on the experimental results, we confirm\nimprovements in long sentence translation by the proposed data augmentation\nmethod, despite its simplicity. Moreover, the translation quality is further\nimproved by the proposed method, when combined with back-translation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 08:04:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kondo", "Seiichiro", ""], ["Hotate", "Kengo", ""], ["Kaneko", "Masahiro", ""], ["Komachi", "Mamoru", ""]]}, {"id": "2104.08480", "submitter": "Jianhua Yuan", "authors": "Jianhua Yuan, Yanyan Zhao, Bing Qin, Ting Liu", "title": "Learning to Share by Masking the Non-shared for Multi-domain Sentiment\n  Classification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-domain sentiment classification deals with the scenario where labeled\ndata exists for multiple domains but insufficient for training effective\nsentiment classifiers that work across domains. Thus, fully exploiting\nsentiment knowledge shared across domains is crucial for real world\napplications. While many existing works try to extract domain-invariant\nfeatures in high-dimensional space, such models fail to explicitly distinguish\nbetween shared and private features at text-level, which to some extent lacks\ninterpretablity. Based on the assumption that removing domain-related tokens\nfrom texts would help improve their domain-invariance, we instead first\ntransform original sentences to be domain-agnostic. To this end, we propose the\nBertMasker network which explicitly masks domain-related words from texts,\nlearns domain-invariant sentiment features from these domain-agnostic texts,\nand uses those masked words to form domain-aware sentence representations.\nEmpirical experiments on a well-adopted multiple domain sentiment\nclassification dataset demonstrate the effectiveness of our proposed model on\nboth multi-domain sentiment classification and cross-domain settings, by\nincreasing the accuracy by 0.94% and 1.8% respectively. Further analysis on\nmasking proves that removing those domain-related and sentiment irrelevant\ntokens decreases texts' domain distinction, resulting in the performance\ndegradation of a BERT-based domain classifier by over 12%.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 08:15:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yuan", "Jianhua", ""], ["Zhao", "Yanyan", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""]]}, {"id": "2104.08481", "submitter": "Ofer Sabo", "authors": "Ofer Sabo, Yanai Elazar, Yoav Goldberg, Ido Dagan", "title": "Revisiting Few-shot Relation Classification: Evaluation Data and\n  Classification Schemes", "comments": "Accepted to TACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore Few-Shot Learning (FSL) for Relation Classification (RC). Focusing\non the realistic scenario of FSL, in which a test instance might not belong to\nany of the target categories (none-of-the-above, aka NOTA), we first revisit\nthe recent popular dataset structure for FSL, pointing out its unrealistic data\ndistribution. To remedy this, we propose a novel methodology for deriving more\nrealistic few-shot test data from available datasets for supervised RC, and\napply it to the TACRED dataset. This yields a new challenging benchmark for FSL\nRC, on which state of the art models show poor performance. Next, we analyze\nclassification schemes within the popular embedding-based nearest-neighbor\napproach for FSL, with respect to constraints they impose on the embedding\nspace. Triggered by this analysis we propose a novel classification scheme, in\nwhich the NOTA category is represented as learned vectors, shown empirically to\nbe an appealing option for FSL.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 08:16:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sabo", "Ofer", ""], ["Elazar", "Yanai", ""], ["Goldberg", "Yoav", ""], ["Dagan", "Ido", ""]]}, {"id": "2104.08512", "submitter": "Omer Goldman", "authors": "Omer Goldman and Reut Tsarfaty", "title": "Minimal Supervision for Morphological Inflection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural models for the various flavours of morphological inflection tasks have\nproven to be extremely accurate given ample labeled data -- data that may be\nslow and costly to obtain. In this work we aim to overcome this annotation\nbottleneck by bootstrapping labeled data from a seed as little as {\\em five}\nlabeled paradigms, accompanied by a large bulk of unlabeled text. Our approach\nexploits different kinds of regularities in morphological systems in a\ntwo-phased setup, where word tagging based on {\\em analogies} is followed by\nword pairing based on {\\em distances}. We experiment with the Paradigm Cell\nFilling Problem over eight typologically different languages, and find that, in\nlanguages with relatively simple morphology, orthographic regularities on their\nown allow inflection models to achieve respectable accuracy. Combined\northographic and semantic regularities alleviate difficulties with particularly\ncomplex morpho-phonological systems. Our results suggest that hand-crafting\nmany tagged examples might be an unnecessary effort. However, more work is\nneeded in order to address rarely used forms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 11:07:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Goldman", "Omer", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2104.08521", "submitter": "Minori Toyoda", "authors": "Minori Toyoda, Kanata Suzuki, Hiroki Mori, Yoshihiko Hayashi, Tetsuya\n  Ogata", "title": "Embodying Pre-Trained Word Embeddings Through Robot Actions", "comments": "To appear in IEEE Robotics and Automation Letters (RA-L) and IEEE\n  International Conference on Robotics and Automation (ICRA 2021)", "journal-ref": "IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.\n  4225-4232, 2021", "doi": "10.1109/LRA.2021.3067862", "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a promising neural network model with which to acquire a grounded\nrepresentation of robot actions and the linguistic descriptions thereof.\nProperly responding to various linguistic expressions, including polysemous\nwords, is an important ability for robots that interact with people via\nlinguistic dialogue. Previous studies have shown that robots can use words that\nare not included in the action-description paired datasets by using pre-trained\nword embeddings. However, the word embeddings trained under the distributional\nhypothesis are not grounded, as they are derived purely from a text corpus. In\nthis letter, we transform the pre-trained word embeddings to embodied ones by\nusing the robot's sensory-motor experiences. We extend a bidirectional\ntranslation model for actions and descriptions by incorporating non-linear\nlayers that retrofit the word embeddings. By training the retrofit layer and\nthe bidirectional translation model alternately, our proposed model is able to\ntransform the pre-trained word embeddings to adapt to a paired\naction-description dataset. Our results demonstrate that the embeddings of\nsynonyms form a semantic cluster by reflecting the experiences (actions and\nenvironments) of a robot. These embeddings allow the robot to properly generate\nactions from unseen words that are not paired with actions in a dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:04:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Toyoda", "Minori", ""], ["Suzuki", "Kanata", ""], ["Mori", "Hiroki", ""], ["Hayashi", "Yoshihiko", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "2104.08524", "submitter": "Ivan Vuli\\'c", "authors": "Daniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek Mondal, Micha{\\l}\n  Lis, Eshan Singhal, Nikola Mrk\\v{s}i\\'c, Tsung-Hsien Wen, Ivan Vuli\\'c", "title": "Multilingual and Cross-Lingual Intent Detection from Spoken Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a systematic study on multilingual and cross-lingual intent\ndetection from spoken data. The study leverages a new resource put forth in\nthis work, termed MInDS-14, a first training and evaluation resource for the\nintent detection task with spoken data. It covers 14 intents extracted from a\ncommercial system in the e-banking domain, associated with spoken examples in\n14 diverse language varieties. Our key results indicate that combining machine\ntranslation models with state-of-the-art multilingual sentence encoders (e.g.,\nLaBSE) can yield strong intent detectors in the majority of target languages\ncovered in MInDS-14, and offer comparative analyses across different axes:\ne.g., zero-shot versus few-shot learning, translation direction, and impact of\nspeech recognition. We see this work as an important step towards more\ninclusive development and evaluation of multilingual intent detectors from\nspoken data, in a much wider spectrum of languages compared to prior work.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:17:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gerz", "Daniela", ""], ["Su", "Pei-Hao", ""], ["Kusztos", "Razvan", ""], ["Mondal", "Avishek", ""], ["Lis", "Micha\u0142", ""], ["Singhal", "Eshan", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Vuli\u0107", "Ivan", ""]]}, {"id": "2104.08529", "submitter": "Yu Qiao", "authors": "Yu Qiao, Wei Zhou, Elma Kerz, Ralf Schl\\\"uter", "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and\n  Sophistication in Spontaneous L2 Speech", "comments": "accepted at Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, automated approaches to assessing linguistic complexity in\nsecond language (L2) writing have made significant progress in gauging learner\nperformance, predicting human ratings of the quality of learner productions,\nand benchmarking L2 development. In contrast, there is comparatively little\nwork in the area of speaking, particularly with respect to fully automated\napproaches to assessing L2 spontaneous speech. While the importance of a\nwell-performing ASR system is widely recognized, little research has been\nconducted to investigate the impact of its performance on subsequent automatic\ntext analysis. In this paper, we focus on this issue and examine the impact of\nusing a state-of-the-art ASR system for subsequent automatic analysis of\nlinguistic complexity in spontaneously produced L2 speech. A set of 30 selected\nmeasures were considered, falling into four categories: syntactic, lexical,\nn-gram frequency, and information-theoretic measures. The agreement between the\nscores for these measures obtained on the basis of ASR-generated vs. manual\ntranscriptions was determined through correlation analysis. A more differential\neffect of ASR performance on specific types of complexity measures when\ncontrolling for task type effects is also presented.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:45:49 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:28:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Qiao", "Yu", ""], ["Zhou", "Wei", ""], ["Kerz", "Elma", ""], ["Schl\u00fcter", "Ralf", ""]]}, {"id": "2104.08530", "submitter": "Malik Altakrori", "authors": "Malik H. Altakrori (1 and 3), Jackie Chi Kit Cheung (1 and 3),\n  Benjamin C. M. Fung (2 and 3) ((1) School of Computer Science -McGill\n  University, (2) School of Information Studies-McGill University, (3) Mila)", "title": "The Topic Confusion Task: A Novel Scenario for Authorship Attribution", "comments": "17 pages (8 + ref./appin.), 6 figures, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Authorship attribution is the problem of identifying the most plausible\nauthor of an anonymous text from a set of candidate authors. Researchers have\ninvestigated same-topic and cross-topic scenarios of authorship attribution,\nwhich differ according to whether unseen topics are used in the testing phase.\nHowever, neither scenario allows us to explain whether errors are caused by\nfailure to capture authorship style, by the topic shift or by other factors.\nMotivated by this, we propose the \\emph{topic confusion} task, where we switch\nthe author-topic configuration between training and testing set. This setup\nallows us to probe errors in the attribution process. We investigate the\naccuracy and two error measures: one caused by the models' confusion by the\nswitch because the features capture the topics, and one caused by the features'\ninability to capture the writing styles, leading to weaker models. By\nevaluating different features, we show that stylometric features with\npart-of-speech tags are less susceptible to topic variations and can increase\nthe accuracy of the attribution process. We further show that combining them\nwith word-level $n$-grams can outperform the state-of-the-art technique in the\ncross-topic scenario. Finally, we show that pretrained language models such as\nBERT and RoBERTa perform poorly on this task, and are outperformed by simple\n$n$-gram features.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:50:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Altakrori", "Malik H.", "", "1 and 3"], ["Cheung", "Jackie Chi Kit", "", "1 and 3"], ["Fung", "Benjamin C. M.", "", "2 and 3"]]}, {"id": "2104.08535", "submitter": "Kevin Stowe", "authors": "Kevin Stowe, Iryna Gurevych", "title": "Combating Temporal Drift in Crisis with Adapted Embeddings", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language usage changes over time, and this can impact the effectiveness of\nNLP systems. This work investigates methods for adapting to changing discourse\nduring crisis events. We explore social media data during crisis, for which\neffective, time-sensitive methods are necessary. We experiment with two\nseparate methods to accommodate changing data: temporal pretraining, which uses\nunlabeled data for the target time periods to train better language models, and\na model of embedding shift based on tools for analyzing semantic change. This\nshift allows us to counteract temporal drift by normalizing incoming data based\non observed patterns of language change. Simulating scenarios in which we lack\naccess to incoming labeled data, we demonstrate the effectiveness of these\nmethods for a wide variety of crises, showing we can improve performance by up\nto 8.0 F1 score for relevance classification across datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:11:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Stowe", "Kevin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.08536", "submitter": "Alexander Fabbri", "authors": "Alexander R. Fabbri, Xiaojian Wu, Srini Iyer, Mona Diab", "title": "Multi-Perspective Abstractive Answer Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Community Question Answering (CQA) forums such as Stack Overflow and Yahoo!\nAnswers contain a rich resource of answers to a wide range of questions. Each\nquestion thread can receive a large number of answers with different\nperspectives. The goal of multi-perspective answer summarization is to produce\na summary that includes all perspectives of the answer. A major obstacle for\nmulti-perspective, abstractive answer summarization is the absence of a dataset\nto provide supervision for producing such summaries. This work introduces a\nnovel dataset creation method to automatically create multi-perspective,\nbullet-point abstractive summaries from an existing CQA forum. Supervision\nprovided by this dataset trains models to inherently produce multi-perspective\nsummaries. Additionally, to train models to output more diverse, faithful\nanswer summaries while retaining multiple perspectives, we propose a\nmulti-reward optimization technique coupled with a sentence-relevance\nprediction multi-task loss. Our methods demonstrate improved coverage of\nperspectives and faithfulness as measured by automatic and human evaluations\ncompared to a strong baseline.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:15:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fabbri", "Alexander R.", ""], ["Wu", "Xiaojian", ""], ["Iyer", "Srini", ""], ["Diab", "Mona", ""]]}, {"id": "2104.08540", "submitter": "Dominik Schlechtweg", "authors": "Dominik Schlechtweg, Nina Tahmasebi, Simon Hengchen, Haim Dubossarsky,\n  Barbara McGillivray", "title": "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word meaning is notoriously difficult to capture, both synchronically and\ndiachronically. In this paper, we describe the creation of the largest resource\nof graded contextualized, diachronic word meaning annotation in four different\nlanguages, based on 100,000 human semantic proximity judgments. We thoroughly\ndescribe the multi-round incremental annotation process, the choice for a\nclustering algorithm to group usages into senses, and possible - diachronic and\nsynchronic - uses for this dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:34:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Schlechtweg", "Dominik", ""], ["Tahmasebi", "Nina", ""], ["Hengchen", "Simon", ""], ["Dubossarsky", "Haim", ""], ["McGillivray", "Barbara", ""]]}, {"id": "2104.08551", "submitter": "Yatin Chaudhary", "authors": "Pankaj Gupta, Yatin Chaudhary, Hinrich Sch\\\"utze", "title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces", "comments": "NAACL2021, 13 pages, 14 tables, 2 figures. arXiv admin note:\n  substantial text overlap with arXiv:1909.06563", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though word embeddings and topics are complementary representations, several\npast works have only used pretrained word embeddings in (neural) topic modeling\nto address data sparsity in short-text or small collection of documents. This\nwork presents a novel neural topic modeling framework using multi-view\nembedding spaces: (1) pretrained topic-embeddings, and (2) pretrained\nword-embeddings (context insensitive from Glove and context-sensitive from BERT\nmodels) jointly from one or many sources to improve topic quality and better\ndeal with polysemy. In doing so, we first build respective pools of pretrained\ntopic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify\none or more relevant source domain(s) and transfer knowledge to guide\nmeaningful learning in the sparse target domain. Within neural topic modeling,\nwe quantify the quality of topics and document representations via\ngeneralization (perplexity), interpretability (topic coherence) and information\nretrieval (IR) using short-text, long-text, small and large document\ncollections from news and medical domains. Introducing the multi-source\nmulti-view embedding spaces, we have shown state-of-the-art neural topic\nmodeling using 6 source (high-resource) and 5 target (low-resource) corpora.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:08:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gupta", "Pankaj", ""], ["Chaudhary", "Yatin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2104.08560", "submitter": "Andrea Burns", "authors": "Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate\n  Saenko, Bryan A. Plummer", "title": "Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task\n  Feasibility in Interactive Visual Environments", "comments": "Accepted at the workshop on Visually Grounded Interaction and\n  Language (ViGIL) at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, vision-language research has shifted to study tasks which\nrequire more complex reasoning, such as interactive question answering, visual\ncommon sense reasoning, and question-answer plausibility prediction. However,\nthe datasets used for these problems fail to capture the complexity of real\ninputs and multimodal environments, such as ambiguous natural language requests\nand diverse digital domains. We introduce Mobile app Tasks with Iterative\nFeedback (MoTIF), a dataset with natural language commands for the greatest\nnumber of interactive environments to date. MoTIF is the first to contain\nnatural language requests for interactive environments that are not\nsatisfiable, and we obtain follow-up questions on this subset to enable\nresearch on task uncertainty resolution. We perform initial feasibility\nclassification experiments and only reach an F1 score of 37.3, verifying the\nneed for richer vision-language representations and improved architectures to\nreason about task feasibility.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:48:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Burns", "Andrea", ""], ["Arsan", "Deniz", ""], ["Agrawal", "Sanjna", ""], ["Kumar", "Ranjitha", ""], ["Saenko", "Kate", ""], ["Plummer", "Bryan A.", ""]]}, {"id": "2104.08570", "submitter": "Evgeniia Razumovskaia", "authors": "Evgeniia Razumovskaia, Goran Glava\\v{s}, Olga Majewska, Edoardo M.\n  Ponti, Anna Korhonen, Ivan Vuli\\'c", "title": "Crossing the Conversational Chasm: A Primer on Natural Language\n  Processing for Multilingual Task-Oriented Dialogue Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In task-oriented dialogue (ToD), a user holds a conversation with an\nartificial agent to complete a concrete task. Although this technology\nrepresents one of the central objectives of AI and has been the focus of ever\nmore intense research and development efforts, it is currently limited to a few\nnarrow domains (e.g., food ordering, ticket booking) and a handful of languages\n(e.g., English, Chinese). This work provides an extensive overview of existing\nmethods and resources in multilingual ToD as an entry point to this exciting\nand emerging field. We find that the most critical factor preventing the\ncreation of truly multilingual ToD systems is the lack of datasets in most\nlanguages for both training and evaluation. In fact, acquiring annotations or\nhuman feedback for each component of modular systems or for data-hungry\nend-to-end systems is expensive and tedious. Hence, state-of-the-art approaches\nto multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer\nfrom resource-rich languages (almost exclusively English), either by means of\nmachine translation or multilingual representations. These approaches are\ncurrently viable only for typologically similar languages and languages with\nparallel / monolingual corpora available. On the other hand, their\neffectiveness beyond these boundaries is doubtful or hard to assess due to the\nlack of linguistically diverse benchmarks (especially for natural language\ngeneration and end-to-end evaluation). To overcome this limitation, we draw\nparallels between components of the ToD pipeline and other NLP tasks, which can\ninspire solutions for learning in low-resource scenarios. Finally, we list\nadditional challenges that multilinguality poses for related areas (such as\nspeech and human-centred evaluation), and indicate future directions that hold\npromise to further expand language coverage and dialogue capabilities of\ncurrent ToD systems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:19:56 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 12:57:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Razumovskaia", "Evgeniia", ""], ["Glava\u0161", "Goran", ""], ["Majewska", "Olga", ""], ["Ponti", "Edoardo M.", ""], ["Korhonen", "Anna", ""], ["Vuli\u0107", "Ivan", ""]]}, {"id": "2104.08578", "submitter": "Debanjan Mahata", "authors": "Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi,\n  Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle Lee, Anish Acharya, Rajiv\n  Ratn Shah", "title": "GupShup: An Annotated Corpus for Abstractive Summarization of\n  Open-Domain Code-Switched Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Code-switching is the communication phenomenon where speakers switch between\ndifferent languages during a conversation. With the widespread adoption of\nconversational agents and chat platforms, code-switching has become an integral\npart of written conversations in many multi-lingual communities worldwide. This\nmakes it essential to develop techniques for summarizing and understanding\nthese conversations. Towards this objective, we introduce abstractive\nsummarization of Hindi-English code-switched conversations and develop the\nfirst code-switched conversation summarization dataset - GupShup, which\ncontains over 6,831 conversations in Hindi-English and their corresponding\nhuman-annotated summaries in English and Hindi-English. We present a detailed\naccount of the entire data collection and annotation processes. We analyze the\ndataset using various code-switching statistics. We train state-of-the-art\nabstractive summarization models and report their performances using both\nautomated metrics and human evaluation. Our results show that multi-lingual\nmBART and multi-view seq2seq models obtain the best performances on the new\ndataset\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:42:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mehnaz", "Laiba", ""], ["Mahata", "Debanjan", ""], ["Gosangi", "Rakesh", ""], ["Gunturi", "Uma Sushmitha", ""], ["Jain", "Riya", ""], ["Gupta", "Gauri", ""], ["Kumar", "Amardeep", ""], ["Lee", "Isabelle", ""], ["Acharya", "Anish", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2104.08588", "submitter": "Shengxuan Luo", "authors": "Shengxuan Luo, Huaiyuan Ying, Sheng Yu", "title": "Sentence Alignment with Parallel Documents Helps Biomedical Machine\n  Translation", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing neural machine translation system has achieved near human-level\nperformance in general domain in some languages, but the lack of parallel\ncorpora poses a key problem in specific domains. In biomedical domain, the\nparallel corpus is less accessible. This work presents a new unsupervised\nsentence alignment method and explores features in training biomedical neural\nmachine translation (NMT) systems. We use a simple but effective way to build\nbilingual word embeddings (BWEs) to evaluate bilingual word similarity and\ntransferred the sentence alignment problem into an extended earth mover's\ndistance (EMD) problem. The proposed method achieved high accuracy in both\n1-to-1 and many-to-many cases. Pre-training in general domain, the larger\nin-domain dataset and n-to-m sentence pairs benefit the NMT model. Fine-tuning\nin domain corpus helps the translation model learns more terminology and fits\nthe in-domain style of text.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 16:09:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Luo", "Shengxuan", ""], ["Ying", "Huaiyuan", ""], ["Yu", "Sheng", ""]]}, {"id": "2104.08597", "submitter": "Ahmed El-Kishky", "authors": "Ahmed El-Kishky, Adi Renduchintala, James Cross, Francisco Guzm\\'an,\n  Philipp Koehn", "title": "XLEnt: Mining a Large Cross-lingual Entity Dataset with\n  Lexical-Semantic-Phonetic Word Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual named-entity lexicon are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexicon\nfrom the web. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 16:58:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["El-Kishky", "Ahmed", ""], ["Renduchintala", "Adi", ""], ["Cross", "James", ""], ["Guzm\u00e1n", "Francisco", ""], ["Koehn", "Philipp", ""]]}, {"id": "2104.08599", "submitter": "Mohcine EL Baroudi", "authors": "Mohcine El Baroudi", "title": "A Stylistic Analysis of Honest Deception: The Case of Seinfeld TV Series\n  Sitcom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language is a powerful tool if used in the correct manner. It is the major\nmode of communication, and using the correct choice of words and styles can\nserve to have a long-lasting impact. Stylistics is the study of the use of\nvarious language styles in communication to pass a message with a bigger impact\nor to communicate indirectly. Stylistic analysis, therefore, is the study of\nthe use of linguistic styles in texts to determine how a style has been used,\nwhat is communicated and how it is communicated. Honest deception is the use of\na choice of words to imply something different from the literal meaning. A\nperson listening or reading a text where honest deception has been used and\nwith a literal understanding may completely miss out on the point. This is\nbecause the issue of honesty and falsehood arises. However, it would be better\nto understand that honest deception is used with the intention of having a\nlasting impact rather than to deceive the readers, viewers or listeners. The\nmajor styles used in honest deception are hyperboles, litotes, irony and\nsarcasm. The Seinfeld Sitcom TV series was a situational TV comedy show aired\nfrom 1990 to 1998. the show attempts to bring to the understanding the daily\nlife of a comedian and how comedian views life experiences and convert them\ninto hilarious jokes. It also shows Jerry's struggle with getting the right\npartner from the many women who come into his life. Reflecting on honest\ndeception in the Seinfeld sitcom TV series, this paper is going to investigate\nhow honest deception has been used in the series, why it has been used and what\nis being communicated. The study is going to use a recapitulative form to give\na better analysis and grouping of the different styles used in honest deception\nthroughout the series.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 17:17:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Baroudi", "Mohcine El", ""]]}, {"id": "2104.08601", "submitter": "Lu Ji", "authors": "Lu Ji, Jing Li, Zhongyu Wei, Qi Zhang, Xuanjing Huang", "title": "Who Responded to Whom: The Joint Effects of Latent Topics and Discourse\n  in Conversation Structure", "comments": "10 pages, 7 figures, 3 tables submitted for emnlp2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous online conversations are produced on a daily basis, resulting in a\npressing need to conversation understanding. As a basis to structure a\ndiscussion, we identify the responding relations in the conversation discourse,\nwhich link response utterances to their initiations. To figure out who\nresponded to whom, here we explore how the consistency of topic contents and\ndependency of discourse roles indicate such interactions, whereas most prior\nwork ignore the effects of latent factors underlying word occurrences. We\npropose a model to learn latent topics and discourse in word distributions, and\npredict pairwise initiation-response links via exploiting topic consistency and\ndiscourse dependency. Experimental results on both English and Chinese\nconversations show that our model significantly outperforms the previous state\nof the arts, such as 79 vs. 73 MRR on Chinese customer service dialogues. We\nfurther probe into our outputs and shed light on how topics and discourse\nindicate conversational user interactions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 17:46:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ji", "Lu", ""], ["Li", "Jing", ""], ["Wei", "Zhongyu", ""], ["Zhang", "Qi", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2104.08610", "submitter": "Michael Glass", "authors": "Michael Glass, Gaetano Rossiello, Alfio Gliozzo", "title": "Zero-shot Slot Filling with DPR and RAG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically extract Knowledge Graphs (KG) from a given\ncollection of documents is a long-standing problem in Artificial Intelligence.\nOne way to assess this capability is through the task of slot filling. Given an\nentity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot\nby generating or extracting the missing value from a relevant passage or\npassages. This capability is crucial to create systems for automatic knowledge\nbase population, which is becoming in ever-increasing demand, especially in\nenterprise applications. Recently, there has been a promising direction in\nevaluating language models in the same way we would evaluate knowledge bases,\nand the task of slot filling is the most suitable to this intent. The recent\nadvancements in the field try to solve this task in an end-to-end fashion using\nretrieval-based language models. Models like Retrieval Augmented Generation\n(RAG) show surprisingly good performance without involving complex information\nextraction pipelines. However, the results achieved by these models on the two\nslot filling tasks in the KILT benchmark are still not at the level required by\nreal-world information extraction systems. In this paper, we describe several\nstrategies we adopted to improve the retriever and the generator of RAG in\norder to make it a better slot filler. Our KGI0 system (available at\nhttps://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position\non the KILT leaderboard on both T-REx and zsRE dataset with a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:24:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Glass", "Michael", ""], ["Rossiello", "Gaetano", ""], ["Gliozzo", "Alfio", ""]]}, {"id": "2104.08613", "submitter": "Omar Sharif", "authors": "Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, Iqbal H. Sarker", "title": "Emotion Classification in a Resource Constrained Language Using\n  Transformer-based Approach", "comments": "Accepted in NAACL-SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although research on emotion classification has significantly progressed in\nhigh-resource languages, it is still infancy for resource-constrained languages\nlike Bengali. However, unavailability of necessary language processing tools\nand deficiency of benchmark corpora makes the emotion classification task in\nBengali more challenging and complicated. This work proposes a\ntransformer-based technique to classify the Bengali text into one of the six\nbasic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali\nemotion corpus consists of 6243 texts is developed for the classification task.\nExperimentation carried out using various machine learning (LR, RF, MNB, SVM),\ndeep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT,\nm-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R\noutdoes all other techniques by achieving the highest weighted $f_1$-score of\n$69.73\\%$ on the test data. The dataset is publicly available at\nhttps://github.com/omar-sharif03/NAACL-SRW-2021.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:28:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Avishek", ""], ["Sharif", "Omar", ""], ["Hoque", "Mohammed Moshiul", ""], ["Sarker", "Iqbal H.", ""]]}, {"id": "2104.08614", "submitter": "Michael Bronstein", "authors": "Jacob Andreas, Ga\\v{s}per Begu\\v{s}, Michael M. Bronstein, Roee\n  Diamant, Denley Delaney, Shane Gero, Shafi Goldwasser, David F. Gruber, Sarah\n  de Haas, Peter Malkin, Roger Payne, Giovanni Petri, Daniela Rus, Pratyusha\n  Sharma, Dan Tchernov, Pernille T{\\o}nnesen, Antonio Torralba, Daniel Vogt,\n  Robert J. Wood", "title": "Cetacean Translation Initiative: a roadmap to deciphering the\n  communication of sperm whales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG cs.RO eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The past decade has witnessed a groundbreaking rise of machine learning for\nhuman language analysis, with current methods capable of automatically\naccurately recovering various aspects of syntax and semantics - including\nsentence structure and grounded word meaning - from large data collections.\nRecent research showed the promise of such tools for analyzing acoustic\ncommunication in nonhuman species. We posit that machine learning will be the\ncornerstone of future collection, processing, and analysis of multimodal\nstreams of data in animal communication studies, including bioacoustic,\nbehavioral, biological, and environmental data. Cetaceans are unique non-human\nmodel species as they possess sophisticated acoustic communications, but\nutilize a very different encoding system that evolved in an aquatic rather than\nterrestrial medium. Sperm whales, in particular, with their highly-developed\nneuroanatomical features, cognitive abilities, social structures, and discrete\nclick-based encoding make for an excellent starting point for advanced machine\nlearning tools that can be applied to other animals in the future. This paper\ndetails a roadmap toward this goal based on currently existing technology and\nmultidisciplinary scientific community effort. We outline the key elements\nrequired for the collection and processing of massive bioacoustic data of sperm\nwhales, detecting their basic communication units and language-like\nhigher-level structures, and validating these models through interactive\nplayback experiments. The technological capabilities developed by such an\nundertaking are likely to yield cross-applications and advancements in broader\ncommunities investigating non-human communication and animal behavioral\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:39:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Andreas", "Jacob", ""], ["Begu\u0161", "Ga\u0161per", ""], ["Bronstein", "Michael M.", ""], ["Diamant", "Roee", ""], ["Delaney", "Denley", ""], ["Gero", "Shane", ""], ["Goldwasser", "Shafi", ""], ["Gruber", "David F.", ""], ["de Haas", "Sarah", ""], ["Malkin", "Peter", ""], ["Payne", "Roger", ""], ["Petri", "Giovanni", ""], ["Rus", "Daniela", ""], ["Sharma", "Pratyusha", ""], ["Tchernov", "Dan", ""], ["T\u00f8nnesen", "Pernille", ""], ["Torralba", "Antonio", ""], ["Vogt", "Daniel", ""], ["Wood", "Robert J.", ""]]}, {"id": "2104.08620", "submitter": "Joshua Rozner", "authors": "Josh Rozner, Christopher Potts, Kyle Mahowald", "title": "Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as\n  a Target for NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryptic crosswords, the dominant English-language crossword variety in the\nUnited Kingdom, can be solved by expert humans using flexible, creative\nintelligence and knowledge of language. Cryptic clues read like fluent natural\nlanguage, but they are adversarially composed of two parts: a definition and a\nwordplay cipher requiring sub-word or character-level manipulations. As such,\nthey are a promising target for evaluating and advancing NLP systems that seek\nto process language in more creative, human-like ways. We present a dataset of\ncryptic crossword clues from a major newspaper that can be used as a benchmark\nand train a sequence-to-sequence model to solve them. We also develop related\nbenchmarks that can guide development of approaches to this challenging task.\nWe show that performance can be substantially improved using a novel curriculum\nlearning approach in which the model is pre-trained on related tasks involving,\ne.g, unscrambling words, before it is trained to solve cryptics. However, even\nthis curricular approach does not generalize to novel clue types in the way\nthat humans can, and so cryptic crosswords remain a challenge for NLP systems\nand a potential source of future innovation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:54:00 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:20:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Rozner", "Josh", ""], ["Potts", "Christopher", ""], ["Mahowald", "Kyle", ""]]}, {"id": "2104.08635", "submitter": "Andrei Paraschiv", "authors": "Andrei Paraschiv, Dumitru-Clementin Cercel, Mihai Dascalu", "title": "UPB at SemEval-2021 Task 5: Virtual Adversarial Training for Toxic Spans\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The real-world impact of polarization and toxicity in the online sphere\nmarked the end of 2020 and the beginning of this year in a negative way.\nSemeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of\na subset of the Jigsaw Unintended Bias dataset and is the first language\ntoxicity detection task dedicated to identifying the toxicity-level spans. For\nthis task, participants had to automatically detect character spans in short\ncomments that render the message as toxic. Our model considers applying Virtual\nAdversarial Training in a semi-supervised setting during the fine-tuning\nprocess of several Transformer-based models (i.e., BERT and RoBERTa), in\ncombination with Conditional Random Fields. Our approach leads to performance\nimprovements and more robust models, enabling us to achieve an F1-score of\n65.73% in the official submission and an F1-score of 66.13% after further\ntuning during post-evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:42:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Paraschiv", "Andrei", ""], ["Cercel", "Dumitru-Clementin", ""], ["Dascalu", "Mihai", ""]]}, {"id": "2104.08639", "submitter": "Qianchu Liu", "authors": "Qianchu Liu, Edoardo M. Ponti, Diana McCarthy, Ivan Vuli\\'c, Anna\n  Korhonen", "title": "AM2iCo: Evaluating Word Meaning in Context across Low-ResourceLanguages\n  with Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capturing word meaning in context and distinguishing between correspondences\nand variations across languages is key to building successful multilingual and\ncross-lingual text representation models. However, existing multilingual\nevaluation datasets that evaluate lexical semantics \"in-context\" have various\nlimitations, in particular, (1) their language coverage is restricted to\nhigh-resource languages and skewed in favor of only a few language families and\nareas, (2) a design that makes the task solvable via superficial cues, which\nresults in artificially inflated (and sometimes super-human) performances of\npretrained encoders, on many target languages, which limits their usefulness\nfor model probing and diagnostics, and (3) no support for cross-lingual\nevaluation. In order to address these gaps, we present AM2iCo, Adversarial and\nMultilingual Meaning in Context, a wide-coverage cross-lingual and multilingual\nevaluation set; it aims to faithfully assess the ability of state-of-the-art\n(SotA) representation models to understand the identity of word meaning in\ncross-lingual contexts for 14 language pairs. We conduct a series of\nexperiments in a wide range of setups and demonstrate the challenging nature of\nAM2iCo. The results reveal that current SotA pretrained encoders substantially\nlag behind human performance, and the largest gaps are observed for\nlow-resource languages and languages dissimilar to English.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 20:23:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Qianchu", ""], ["Ponti", "Edoardo M.", ""], ["McCarthy", "Diana", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""]]}, {"id": "2104.08642", "submitter": "Bogdan Lobodzinski", "authors": "Bogdan {\\L}obodzi\\'nski", "title": "Customized determination of stop words using Random Matrix Theory\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The distances between words calculated in word units are studied and compared\nwith the distributions of the Random Matrix Theory (RMT). It is found that the\ndistribution of distance between the same words can be well described by the\nsingle-parameter Brody distribution. Using the Brody distribution fit, we found\nthat the distance between given words in a set of texts can show mixed\ndynamics, coexisting regular and chaotic regimes. It is found that\ndistributions correctly fitted by the Brody distribution with a certain\ngoodness of the fit threshold can be identifid as stop words, usually\nconsidered as the uninformative part of the text. By applying various threshold\nvalues for the goodness of fit, we can extract uninformative words from the\ntexts under analysis to the desired extent. On this basis we formulate a fully\nagnostic recipe that can be used in the creation of a customized set of stop\nwords for texts in any language based on words.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 20:42:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["\u0141obodzi\u0144ski", "Bogdan", ""]]}, {"id": "2104.08645", "submitter": "Kuan-Hao Huang", "authors": "Kuan-Hao Huang, Wasi Uddin Ahmad, Nanyun Peng, Kai-Wei Chang", "title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, pre-trained multilingual language models, such as\nmultilingual BERT and XLM-R, exhibit good performance on zero-shot\ncross-lingual transfer learning. However, since their multilingual contextual\nembedding spaces for different languages are not perfectly aligned, the\ndifference between representations of different languages might cause zero-shot\ncross-lingual transfer failed in some cases. In this work, we draw connections\nbetween those failed cases and adversarial examples. We then propose to use\nrobust training methods to train a robust model that can tolerate some noise in\ninput embeddings. We study two widely used robust training methods: adversarial\ntraining and randomized smoothing. The experimental results demonstrate that\nrobust training can improve zero-shot cross-lingual transfer for text\nclassification. The performance improvements become significant when the\ndistance between the source language and the target language increases.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 21:21:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Huang", "Kuan-Hao", ""], ["Ahmad", "Wasi Uddin", ""], ["Peng", "Nanyun", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2104.08646", "submitter": "Matt Gardner", "authors": "Matt Gardner, William Merrill, Jesse Dodge, Matthew E. Peters, Alexis\n  Ross, Sameer Singh, Noah Smith", "title": "Competency Problems: On Finding and Removing Artifacts in Language Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much recent work in NLP has documented dataset artifacts, bias, and spurious\ncorrelations between input features and output labels. However, how to tell\nwhich features have \"spurious\" instead of legitimate correlations is typically\nleft unspecified. In this work we argue that for complex language understanding\ntasks, all simple feature correlations are spurious, and we formalize this\nnotion into a class of problems which we call competency problems. For example,\nthe word \"amazing\" on its own should not give information about a sentiment\nlabel independent of the context in which it appears, which could include\nnegation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of\ncreating data for competency problems when human bias is taken into account,\nshowing that realistic datasets will increasingly deviate from competency\nproblems as dataset size increases. This analysis gives us a simple statistical\ntest for dataset artifacts, which we use to show more subtle biases than were\ndescribed in prior work, including demonstrating that models are\ninappropriately affected by these less extreme biases. Our theoretical\ntreatment of this problem also allows us to analyze proposed solutions, such as\nmaking local edits to dataset instances, and to give recommendations for future\ndata collection and model design efforts that target competency problems.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 21:34:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gardner", "Matt", ""], ["Merrill", "William", ""], ["Dodge", "Jesse", ""], ["Peters", "Matthew E.", ""], ["Ross", "Alexis", ""], ["Singh", "Sameer", ""], ["Smith", "Noah", ""]]}, {"id": "2104.08647", "submitter": "Matan Hasson", "authors": "Matan Hasson and Jonathan Berant", "title": "Question Decomposition with Dependency Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QDMR is a meaning representation for complex questions, which decomposes\nquestions into a sequence of atomic steps. While state-of-the-art QDMR parsers\nuse the common sequence-to-sequence (seq2seq) approach, a QDMR structure\nfundamentally describes labeled relations between spans in the input question,\nand thus dependency-based approaches seem appropriate for this task. In this\nwork, we present a QDMR parser that is based on dependency graphs (DGs), where\nnodes in the graph are words and edges describe logical relations that\ncorrespond to the different computation steps. We propose (a) a\nnon-autoregressive graph parser, where all graph edges are computed\nsimultaneously, and (b) a seq2seq parser that uses gold graph as auxiliary\nsupervision. We find that a graph parser leads to a moderate reduction in\nperformance (0.47 to 0.44), but to a 16x speed-up in inference time due to the\nnon-autoregressive nature of the parser, and to improved sample complexity\ncompared to a seq2seq model. Second, a seq2seq model trained with auxiliary\ngraph supervision has better generalization to new domains compared to a\nseq2seq model, and also performs better on questions with long sequences of\ncomputation steps.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 21:35:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hasson", "Matan", ""], ["Berant", "Jonathan", ""]]}, {"id": "2104.08653", "submitter": "Baban Gain", "authors": "Baban Gain, Dibyanayan Bandyopadhyay, Tanik Saikh, Asif Ekbal", "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT", "comments": "5 pages. IITP@ COLIEE 2019: legal information retrieval using BM25\n  and BERT. Proceedings of the 6th Competition on Legal Information\n  Extraction/Entailment. COLIEE", "journal-ref": null, "doi": "10.13140/RG.2.2.24136.44804", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:28:15 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 19:07:25 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 08:39:42 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 14:40:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gain", "Baban", ""], ["Bandyopadhyay", "Dibyanayan", ""], ["Saikh", "Tanik", ""], ["Ekbal", "Asif", ""]]}, {"id": "2104.08655", "submitter": "Abhyuday Bhartiya", "authors": "Abhyuday Bhartiya, Kartikeya Badola, Mausam", "title": "DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision (DS) is a well established technique for creating\nlarge-scale datasets for relation extraction (RE) without using human\nannotations. However, research in DS-RE has been mostly limited to the English\nlanguage. Constraining RE to a single language inhibits utilization of large\namounts of data in other languages which could allow extraction of more diverse\nfacts. Very recently, a dataset for multilingual DS-RE has been released.\nHowever, our analysis reveals that the proposed dataset exhibits unrealistic\ncharacteristics such as 1) lack of sentences that do not express any relation,\nand 2) all sentences for a given entity pair expressing exactly one relation.\nWe show that these characteristics lead to a gross overestimation of the model\nperformance. In response, we propose a new dataset, DiS-ReX, which alleviates\nthese issues. Our dataset has more than 1.5 million sentences, spanning across\n4 languages with 36 relation classes + 1 no relation (NA) class. We also modify\nthe widely used bag attention models by encoding sentences using mBERT and\nprovide the first benchmark results on multilingual DS-RE. Unlike the competing\ndataset, we show that our dataset is challenging and leaves enough room for\nfuture research to take place in this field.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:44:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bhartiya", "Abhyuday", ""], ["Badola", "Kartikeya", ""], ["Mausam", "", ""]]}, {"id": "2104.08656", "submitter": "Wenxuan Zhou", "authors": "Wenxuan Zhou, Muhao Chen", "title": "Learning from Noisy Labels for Entity-Centric Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts for information extraction have relied on many deep neural\nmodels. However, any such models can easily overfit noisy labels and suffer\nfrom performance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with different parameter\ninitialization. These models are jointly optimized with task-specific loss, and\nare regularized to generate similar predictions based on an agreement loss,\nwhich prevents overfitting on noisy labels. In the end, we can take any of the\ntrained models for inference. Extensive experiments on two widely used but\nnoisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate\nthe effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:49:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Wenxuan", ""], ["Chen", "Muhao", ""]]}, {"id": "2104.08659", "submitter": "Zeming Chen", "authors": "Zeming Chen, Qiyue Gao", "title": "Monotonicity Marking from Universal Dependency Trees", "comments": "10 pages, 3 figures, The 14th International Conference on\n  Computational Semantics (IWCS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dependency parsing is a tool widely used in the field of Natural language\nprocessing and computational linguistics. However, there is hardly any work\nthat connects dependency parsing to monotonicity, which is an essential part of\nlogic and linguistic semantics. In this paper, we present a system that\nautomatically annotates monotonicity information based on Universal Dependency\nparse trees. Our system utilizes surface-level monotonicity facts about\nquantifiers, lexical items, and token-level polarity information. We compared\nour system's performance with existing systems in the literature, including\nNatLog and ccg2mono, on a small evaluation dataset. Results show that our\nsystem outperforms NatLog and ccg2mono.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:01:10 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 21:53:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chen", "Zeming", ""], ["Gao", "Qiyue", ""]]}, {"id": "2104.08661", "submitter": "Bhavana Dalvi Mishra", "authors": "Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah\n  Smith, Leighanna Pipatanangkura, Peter Clark", "title": "Explaining Answers with Entailment Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by not just listing supporting textual evidence\n(\"rationales\"), but also showing how such evidence leads to the answer in a\nsystematic way. If this could be done, new opportunities for understanding and\ndebugging the system's reasoning would become possible. Our approach is to\ngenerate explanations in the form of entailment trees, namely a tree of\nentailment steps from facts that are known, through intermediate conclusions,\nto the final answer. To train a model with this skill, we created\nENTAILMENTBANK, the first dataset to contain multistep entailment trees. At\neach node in the tree (typically) two or more facts compose together to produce\na new conclusion. Given a hypothesis (question + answer), we define three\nincreasingly difficult explanation tasks: generate a valid entailment tree\ngiven (a) all relevant sentences (the leaves of the gold entailment tree), (b)\nall relevant and some irrelevant sentences, or (c) a corpus. We show that a\nstrong language model only partially solves these tasks, and identify several\nnew directions to improve performance. This work is significant as it provides\na new type of dataset (multistep entailments) and baselines, offering a new\navenue for the community to generate richer, more systematic explanations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:13:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dalvi", "Bhavana", ""], ["Jansen", "Peter", ""], ["Tafjord", "Oyvind", ""], ["Xie", "Zhengnan", ""], ["Smith", "Hannah", ""], ["Pipatanangkura", "Leighanna", ""], ["Clark", "Peter", ""]]}, {"id": "2104.08663", "submitter": "Nandan Thakur", "authors": "Nandan Thakur, Nils Reimers, Andreas R\\\"uckl\\'e, Abhishek Srivastava,\n  Iryna Gurevych", "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural IR models have often been studied in homogeneous and narrow settings,\nwhich has considerably limited insights into their generalization capabilities.\nTo address this, and to allow researchers to more broadly establish the\neffectiveness of their models, we introduce BEIR (Benchmarking IR), a\nheterogeneous benchmark for information retrieval. We leverage a careful\nselection of 17 datasets for evaluation spanning diverse retrieval tasks\nincluding open-domain datasets as well as narrow expert domains. We study the\neffectiveness of nine state-of-the-art retrieval models in a zero-shot\nevaluation setup on BEIR, finding that performing well consistently across all\ndatasets is challenging. Our results show BM25 is a robust baseline and\nReranking-based models overall achieve the best zero-shot performances,\nhowever, at high computational costs. In contrast, Dense-retrieval models are\ncomputationally more efficient but often underperform other approaches,\nhighlighting the considerable room for improvement in their generalization\ncapabilities. In this work, we extensively analyze different retrieval models\nand provide several suggestions that we believe may be useful for future work.\nBEIR datasets and code are available at https://github.com/UKPLab/beir.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:29:55 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 13:59:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Thakur", "Nandan", ""], ["Reimers", "Nils", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Srivastava", "Abhishek", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.08664", "submitter": "Michaela Socolof", "authors": "Michaela Socolof, Jackie Chi Kit Cheung, Michael Wagner, Timothy J.\n  O'Donnell", "title": "Characterizing Idioms: Conventionality and Contingency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Idioms are unlike other phrases in two important ways. First, the words in an\nidiom have unconventional meanings. Second, the unconventional meaning of words\nin an idiom are contingent on the presence of the other words in the idiom.\nLinguistic theories disagree about whether these two properties depend on one\nanother, as well as whether special theoretical machinery is needed to\naccommodate idioms. We define two measures that correspond to these two\nproperties, and we show that idioms fall at the expected intersection of the\ntwo dimensions, but that the dimensions themselves are not correlated. Our\nresults suggest that idioms are no more anomalous than other types of phrases,\nand that introducing special machinery to handle idioms may not be warranted.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:46:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Socolof", "Michaela", ""], ["Cheung", "Jackie Chi Kit", ""], ["Wagner", "Michael", ""], ["O'Donnell", "Timothy J.", ""]]}, {"id": "2104.08666", "submitter": "Tejas Srinivasan", "authors": "Tejas Srinivasan, Yonatan Bisk", "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous works have analyzed biases in vision and pre-trained language models\nindividually - however, less attention has been paid to how these biases\ninteract in multimodal settings. This work extends text-based bias analysis\nmethods to investigate multimodal language models, and analyzes intra- and\ninter-modality associations and biases learned by these models. Specifically,\nwe demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often\npreferring to reinforce a stereotype over faithfully describing the visual\nscene. We demonstrate these findings on a controlled case-study and extend them\nfor a larger set of stereotypically gendered entities.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:02:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Srinivasan", "Tejas", ""], ["Bisk", "Yonatan", ""]]}, {"id": "2104.08667", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi", "title": "SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal\n  Conversations", "comments": "10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new corpus for the Situated and Interactive Multimodal\nConversations, SIMMC 2.0, aimed at building a successful multimodal assistant\nagent. Specifically, the dataset features 11K task-oriented dialogs (117K\nutterances) between a user and a virtual assistant on the shopping domain\n(fashion and furniture), grounded in situated and photo-realistic VR scenes.\nThe dialogs are collected using a two-phase pipeline, which first generates\nsimulated dialog flows via a novel multimodal dialog simulator we propose,\nfollowed by manual paraphrasing of the generated utterances. In this paper, we\nprovide an in-depth analysis of the collected dataset, and describe in detail\nthe four main benchmark tasks we propose for SIMMC 2.0. The preliminary\nanalysis with a baseline model highlights the new challenges that the SIMMC 2.0\ndataset brings, suggesting new directions for future research. Our dataset and\ncode will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:14:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kottur", "Satwik", ""], ["Moon", "Seungwhan", ""], ["Geramifard", "Alborz", ""], ["Damavandi", "Babak", ""]]}, {"id": "2104.08668", "submitter": "Darsh Shah", "authors": "Darsh J Shah and Regina Barzilay", "title": "Generating Related Work", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:19:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shah", "Darsh J", ""], ["Barzilay", "Regina", ""]]}, {"id": "2104.08671", "submitter": "Lucia Zheng", "authors": "Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, Daniel\n  E. Ho", "title": "When Does Pretraining Help? Assessing Self-Supervised Learning for Law\n  and the CaseHOLD Dataset", "comments": "ICAIL 2021. Code & data available at\n  https://github.com/reglab/casehold", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While self-supervised learning has made rapid advances in natural language\nprocessing, it remains unclear when researchers should engage in\nresource-intensive domain-specific pretraining (domain pretraining). The law,\npuzzlingly, has yielded few documented instances of substantial gains to domain\npretraining in spite of the fact that legal language is widely seen to be\nunique. We hypothesize that these existing results stem from the fact that\nexisting legal NLP tasks are too easy and fail to meet conditions for when\ndomain pretraining can help. To address this, we first present CaseHOLD (Case\nHoldings On Legal Decisions), a new dataset comprised of over 53,000+ multiple\nchoice questions to identify the relevant holding of a cited case. This dataset\npresents a fundamental task to lawyers and is both legally meaningful and\ndifficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second,\nwe assess performance gains on CaseHOLD and existing legal NLP datasets. While\na Transformer architecture (BERT) pretrained on a general corpus (Google Books\nand Wikipedia) improves performance, domain pretraining (using corpus of\napproximately 3.5M decisions across all courts in the U.S. that is larger than\nBERT's) with a custom legal vocabulary exhibits the most substantial\nperformance gains with CaseHOLD (gain of 7.2% on F1, representing a 12%\nimprovement on BERT) and consistent performance gains across two other legal\ntasks. Third, we show that domain pretraining may be warranted when the task\nexhibits sufficient similarity to the pretraining corpus: the level of\nperformance increase in three legal tasks was directly tied to the domain\nspecificity of the task. Our findings inform when researchers should engage\nresource-intensive pretraining and show that Transformer-based architectures,\ntoo, learn embeddings suggestive of distinct legal language.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:57:16 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 22:45:11 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 00:56:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zheng", "Lucia", ""], ["Guha", "Neel", ""], ["Anderson", "Brandon R.", ""], ["Henderson", "Peter", ""], ["Ho", "Daniel E.", ""]]}, {"id": "2104.08673", "submitter": "Zihan Wang", "authors": "Zihan Wang and Chengyu Dong and Jingbo Shang", "title": "\"Average\" Approximates \"First Principal Component\"? An Empirical\n  Analysis on Representations from Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contextualized representations based on neural language models have furthered\nthe state of the art in various NLP tasks. Despite its great success, the\nnature of such representations remains a mystery. In this paper, we present an\nempirical property of these representations -- \"average\" approximates \"first\nprincipal component\". Specifically, experiments show that the average of these\nrepresentations shares almost the same direction as the first principal\ncomponent of the matrix whose columns are these representations. We believe\nthis explains why the average representation is always a simple yet strong\nbaseline. Our further examinations show that this property also holds in more\nchallenging scenarios, for example, when the representations are from a model\nright after its random initialization. Therefore, we conjecture that this\nproperty is intrinsic to the distribution of representations and not\nnecessarily related to the input structure. We realize that these\nrepresentations empirically follow a normal distribution for each dimension,\nand by assuming this is true, we demonstrate that the empirical property can be\nin fact derived mathematically.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 01:15:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Zihan", ""], ["Dong", "Chengyu", ""], ["Shang", "Jingbo", ""]]}, {"id": "2104.08675", "submitter": "Xingyi Cheng", "authors": "Xingyi Cheng", "title": "Dual-View Distilled BERT for Sentence Embedding", "comments": "Accepted at SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, BERT realized significant progress for sentence matching via\nword-level cross sentence attention. However, the performance significantly\ndrops when using siamese BERT-networks to derive two sentence embeddings, which\nfall short in capturing the global semantic since the word-level attention\nbetween two sentences is absent. In this paper, we propose a Dual-view\ndistilled BERT~(DvBERT) for sentence matching with sentence embeddings. Our\nmethod deals with a sentence pair from two distinct views, i.e., Siamese View\nand Interaction View. Siamese View is the backbone where we generate sentence\nembeddings. Interaction View integrates the cross sentence interaction as\nmultiple teachers to boost the representation ability of sentence embeddings.\nExperiments on six STS tasks show that our method outperforms the\nstate-of-the-art sentence embedding methods significantly.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 01:20:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cheng", "Xingyi", ""]]}, {"id": "2104.08676", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Yixin Nie, Mohit Bansal", "title": "Distributed NLI: Learning to Predict Human Opinion Distributions for\n  Language Reasoning", "comments": "13 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce distributed NLI, a new NLU task with a goal to predict the\ndistribution of human judgements for natural language inference. We show that\nmodels can capture human judgement distribution by applying additional\ndistribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep\nEnsemble, Re-Calibration, and Distribution Distillation. All four of these\nmethods substantially outperform the softmax baseline. We show that MC Dropout\nis able to achieve decent performance without any distribution annotations\nwhile Re-Calibration can further give substantial improvements when extra\ndistribution annotations are provided, suggesting the value of multiple\nannotations for the example in modeling the distribution of human judgements.\nMoreover, MC Dropout and Re-Calibration can achieve decent transfer performance\non out-of-domain data. Despite these improvements, the best results are still\nfar below estimated human upper-bound, indicating that the task of predicting\nthe distribution of human judgements is still an open, challenging problem with\nlarge room for future improvements. We showcase the common errors for MC\nDropout and Re-Calibration. Finally, we give guidelines on the usage of these\nmethods with different levels of data availability and encourage future work on\nmodeling the human opinion distribution for language reasoning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 01:25:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Xiang", ""], ["Nie", "Yixin", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.08677", "submitter": "Mehdi Rezagholizadeh", "authors": "Krtin Kumar, Mehdi Rezagholizadeh, Yiu Sing Lau, Qun Liu", "title": "Improving Neural Machine Translation with Compact Word Embedding Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Embedding matrices are key components in neural natural language processing\n(NLP) models that are responsible to provide numerical representations of input\ntokens.\\footnote{In this paper words and subwords are referred to as\n\\textit{tokens} and the term \\textit{embedding} only refers to embeddings of\ninputs.} In this paper, we analyze the impact and utility of such matrices in\nthe context of neural machine translation (NMT). We show that detracting\nsyntactic and semantic information from word embeddings and running NMT systems\nwith random embeddings is not as damaging as it initially sounds. We also show\nhow incorporating only a limited amount of task-specific knowledge from\nfully-trained embeddings can boost the performance NMT systems. Our findings\ndemonstrate that in exchange for negligible deterioration in performance, any\nNMT model can be run with partially random embeddings. Working with such\nstructures means a minimal memory requirement as there is no longer need to\nstore large embedding tables, which is a significant gain in industrial and\non-device settings. We evaluated our embeddings in translating {English} into\n{German} and {French} and achieved a $5.3$x compression rate. Despite having a\nconsiderably smaller architecture, our models in some cases are even able to\noutperform state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 01:57:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kumar", "Krtin", ""], ["Rezagholizadeh", "Mehdi", ""], ["Lau", "Yiu Sing", ""], ["Liu", "Qun", ""]]}, {"id": "2104.08678", "submitter": "Max Bartolo", "authors": "Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus\n  Stenetorp, Douwe Kiela", "title": "Improving Question Answering Model Robustness with Synthetic Adversarial\n  Data Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the availability of very large datasets and pretrained models,\nstate-of-the-art question answering models remain susceptible to a variety of\nadversarial attacks and are still far from obtaining human-level language\nunderstanding. One proposed way forward is dynamic adversarial data collection,\nin which a human annotator attempts to create examples for which a\nmodel-in-the-loop fails. However, this approach comes at a higher cost per\nsample and slower pace of annotation, as model-adversarial data requires more\nannotator effort to generate. In this work, we investigate several answer\nselection, question generation, and filtering methods that form a synthetic\nadversarial data generation pipeline that takes human-generated adversarial\nsamples and unannotated text to create synthetic question-answer pairs. Models\ntrained on both synthetic and human-generated data outperform models not\ntrained on synthetic adversarial data, and obtain state-of-the-art results on\nthe AdversarialQA dataset with overall performance gains of 3.7F1. Furthermore,\nwe find that training on the synthetic adversarial data improves model\ngeneralisation across domains for non-adversarial data, demonstrating gains on\n9 of the 12 datasets for MRQA. Lastly, we find that our models become\nconsiderably more difficult to beat by human adversaries, with a drop in\nmacro-averaged validated model error rate from 17.6% to 8.8% when compared to\nnon-augmented models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:00:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bartolo", "Max", ""], ["Thrush", "Tristan", ""], ["Jia", "Robin", ""], ["Riedel", "Sebastian", ""], ["Stenetorp", "Pontus", ""], ["Kiela", "Douwe", ""]]}, {"id": "2104.08679", "submitter": "Shahab Raji", "authors": "Shahab Raji, Gerard de Melo", "title": "Guilt by Association: Emotion Intensities in Lexical Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  What do word vector representations reveal about the emotions associated with\nwords? In this study, we consider the task of estimating word-level emotion\nintensity scores for specific emotions, exploring unsupervised, supervised, and\nfinally a self-supervised method of extracting emotional associations from word\nvector representations. Overall, we find that word vectors carry substantial\npotential for inducing fine-grained emotion intensity scores, showing a far\nhigher correlation with human ground truth ratings than achieved by\nstate-of-the-art emotion lexicons.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:03:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Raji", "Shahab", ""], ["de Melo", "Gerard", ""]]}, {"id": "2104.08682", "submitter": "Dongkuan Xu", "authors": "Dongkuan Xu, Ian E.H. Yen, Jinxi Zhao, Zhibin Xiao", "title": "Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm", "comments": "7 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformer-based pre-trained language models have significantly improved the\nperformance of various natural language processing (NLP) tasks in the recent\nyears. While effective and prevalent, these models are usually prohibitively\nlarge for resource-limited deployment scenarios. A thread of research has thus\nbeen working on applying network pruning techniques under the\npretrain-then-finetune paradigm widely adopted in NLP. However, the existing\npruning results on benchmark transformers, such as BERT, are not as remarkable\nas the pruning results in the literature of convolutional neural networks\n(CNNs). In particular, common wisdom in pruning CNN states that sparse pruning\ntechnique compresses a model more than that obtained by reducing number of\nchannels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing\nworks on sparse pruning of BERT yields inferior results than its small-dense\ncounterparts such as TinyBERT (Jiao et al., 2020). In this work, we aim to fill\nthis gap by studying how knowledge are transferred and lost during the\npre-train, fine-tune, and pruning process, and proposing a knowledge-aware\nsparse pruning process that achieves significantly superior results than\nexisting literature. We show for the first time that sparse pruning compresses\na BERT model significantly more than reducing its number of channels and\nlayers. Experiments on multiple data sets of GLUE benchmark show that our\nmethod outperforms the leading competitors with a 20-times weight/FLOPs\ncompression and neglectable loss in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:20:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xu", "Dongkuan", ""], ["Yen", "Ian E. H.", ""], ["Zhao", "Jinxi", ""], ["Xiao", "Zhibin", ""]]}, {"id": "2104.08685", "submitter": "Jacob Louis Hoover", "authors": "Jacob Louis Hoover, Alessandro Sordoni, Wenyu Du, Timothy J. O'Donnell", "title": "Linguistic dependencies and statistical dependence", "comments": "8 pages, plus references and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the relationship between linguistic dependencies and statistical\ndependence? Building on earlier work in NLP and cognitive science, we study\nthis question. We introduce a contextualized version of pointwise mutual\ninformation (CPMI), using pretrained language models to estimate probabilities\nof words in context. Extracting dependency trees which maximize CPMI, we\ncompare the resulting structures against gold dependencies. Overall, we find\nthat these maximum-CPMI trees correspond to linguistic dependencies more often\nthan trees extracted from non-contextual PMI estimate, but only roughly as\noften as a simple baseline formed by connecting adjacent words. We also provide\nevidence that the extent to which the two kinds of dependency align cannot be\nexplained by the distance between words or by the category of the dependency\nrelation. Finally, our analysis sheds some light on the differences between\nlarge pretrained language models, specifically in the kinds of inductive biases\nthey encode.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:43:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hoover", "Jacob Louis", ""], ["Sordoni", "Alessandro", ""], ["Du", "Wenyu", ""], ["O'Donnell", "Timothy J.", ""]]}, {"id": "2104.08691", "submitter": "Brian Lester", "authors": "Brian Lester, Rami Al-Rfou, Noah Constant", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:19:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lester", "Brian", ""], ["Al-Rfou", "Rami", ""], ["Constant", "Noah", ""]]}, {"id": "2104.08692", "submitter": "Li Dong", "authors": "Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang Xian-Ling Mao, Heyan\n  Huang, Furu Wei", "title": "mT6: Multilingual Pretrained Text-to-Text Transformer with Translation\n  Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive\nmonolingual texts, which has shown promising results on many cross-lingual\ntasks. In this paper, we improve multilingual text-to-text transfer Transformer\nwith translation pairs (mT6). Specifically, we explore three cross-lingual\ntext-to-text pre-training tasks, namely, machine translation, translation pair\nspan corruption, and translation span corruption. In addition, we propose a\npartially non-autoregressive objective for text-to-text pre-training. We\nevaluate the methods on seven multilingual benchmark datasets, including\nsentence classification, named entity recognition, question answering, and\nabstractive summarization. Experimental results show that the proposed mT6\nimproves cross-lingual transferability over mT5.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:24:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chi", "Zewen", ""], ["Dong", "Li", ""], ["Ma", "Shuming", ""], ["Mao", "Shaohan Huang Xian-Ling", ""], ["Huang", "Heyan", ""], ["Wei", "Furu", ""]]}, {"id": "2104.08696", "submitter": "Li Dong", "authors": "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei", "title": "Knowledge Neurons in Pretrained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we explore\nhow implicit knowledge is stored in pretrained Transformers by introducing the\nconcept of knowledge neurons. Given a relational fact, we propose a knowledge\nattribution method to identify the neurons that express the fact. We present\nthat the activation of such knowledge neurons is highly correlated to the\nexpression of their corresponding facts. In addition, even without fine-tuning,\nwe can leverage knowledge neurons to explicitly edit (such as update, and\nerase) specific factual knowledge for pretrained Transformers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:38:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dai", "Damai", ""], ["Dong", "Li", ""], ["Hao", "Yaru", ""], ["Sui", "Zhifang", ""], ["Wei", "Furu", ""]]}, {"id": "2104.08698", "submitter": "Pu-Chin Chen", "authors": "Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung,\n  Yin-Wen Chang, Chun-Sung Ferng", "title": "Demystifying the Better Performance of Position Encoding Variants for\n  Transformer", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are state of the art models in NLP that map a given input\nsequence of vectors to an output sequence of vectors. However these models are\npermutation equivariant, and additive position embeddings to the input are used\nto supply the information about the order of the input tokens. Further, for\nsome tasks, additional additive segment embeddings are used to denote different\ntypes of input sentences. Recent works proposed variations of positional\nencodings with relative position encodings achieving better performance. In\nthis work, we do a systematic study comparing different position encodings and\nunderstanding the reasons for differences in their performance. We demonstrate\na simple yet effective way to encode position and segment into the Transformer\nmodels. The proposed method performs on par with SOTA on GLUE, XTREME and WMT\nbenchmarks while saving computation costs.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:44:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Pu-Chin", ""], ["Tsai", "Henry", ""], ["Bhojanapalli", "Srinadh", ""], ["Chung", "Hyung Won", ""], ["Chang", "Yin-Wen", ""], ["Ferng", "Chun-Sung", ""]]}, {"id": "2104.08701", "submitter": "Brian Lester", "authors": "Brian Lester, Sagnik Ray Choudhury, Rashmi Prasad, Srinivas Bangalore", "title": "Intent Features for Rich Natural Language Understanding", "comments": "Camera-ready for NAACL 2021 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex natural language understanding modules in dialog systems have a\nricher understanding of user utterances, and thus are critical in providing a\nbetter user experience. However, these models are often created from scratch,\nfor specific clients and use cases, and require the annotation of large\ndatasets. This encourages the sharing of annotated data across multiple\nclients. To facilitate this we introduce the idea of intent features: domain\nand topic agnostic properties of intents that can be learned from the syntactic\ncues only, and hence can be shared. We introduce a new neural network\narchitecture, the Global-Local model, that shows significant improvement over\nstrong baselines for identifying these features in a deployed, multi-intent\nnatural language understanding module, and, more generally, in a classification\nsetting where a part of an utterance has to be classified utilizing the whole\ncontext.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:57:02 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 16:08:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lester", "Brian", ""], ["Choudhury", "Sagnik Ray", ""], ["Prasad", "Rashmi", ""], ["Bangalore", "Srinivas", ""]]}, {"id": "2104.08704", "submitter": "Tianyu Liu", "authors": "Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu\n  Chen and Bill Dolan", "title": "A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:09:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Tianyu", ""], ["Zhang", "Yizhe", ""], ["Brockett", "Chris", ""], ["Mao", "Yi", ""], ["Sui", "Zhifang", ""], ["Chen", "Weizhu", ""], ["Dolan", "Bill", ""]]}, {"id": "2104.08710", "submitter": "Vidhisha Balachandran", "authors": "Vidhisha Balachandran, Ashish Vaswani, Yulia Tsvetkov, Niki Parmar", "title": "Simple and Efficient ways to Improve REALM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense retrieval has been shown to be effective for retrieving relevant\ndocuments for Open Domain QA, surpassing popular sparse retrieval methods like\nBM25. REALM (Guu et al., 2020) is an end-to-end dense retrieval system that\nrelies on MLM based pretraining for improved downstream QA efficiency across\nmultiple datasets. We study the finetuning of REALM on various QA tasks and\nexplore the limits of various hyperparameter and supervision choices. We find\nthat REALM was significantly undertrained when finetuning and simple\nimprovements in the training, supervision, and inference setups can\nsignificantly benefit QA results and exceed the performance of other models\npublished post it. Our best model, REALM++, incorporates all the best working\nfindings and achieves significant QA accuracy improvements over baselines\n(~5.5% absolute accuracy) without any model design changes. Additionally,\nREALM++ matches the performance of large Open Domain QA models which have 3x\nmore parameters demonstrating the efficiency of the setup.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:32:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Balachandran", "Vidhisha", ""], ["Vaswani", "Ashish", ""], ["Tsvetkov", "Yulia", ""], ["Parmar", "Niki", ""]]}, {"id": "2104.08712", "submitter": "Ehsan Qasemi", "authors": "Ehsan Qasemi, Filip Ilievski, Muhao Chen, Pedro Szekely", "title": "CoreQuisite: Circumstantial Preconditions of Common Sense Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of identifying and reasoning with circumstantial preconditions\nassociated with everyday facts is natural to humans. It is unclear whether\nstate-of-the-art language models (LMs) understand the implicit preconditions\nthat enable or invalidate commonsense facts, such as \"A glass is used for\ndrinking water\", Despite their impressive accuracy on existing commonsense\ntasks. In this paper, we propose a new problem of reasoning with circumstantial\npreconditions, and present a dataset, called CoreQuisite, which annotates\ncommonsense facts with preconditions expressed in natural language. Based on\nthis resource, we create three canonical evaluation tasks and use them to\nexamine the capability of existing LMs to understand situational\npre-conditions. Our results show that there is a 10-30%gap between machine and\nhuman performance on our tasks. We make all resources and software publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:37:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Qasemi", "Ehsan", ""], ["Ilievski", "Filip", ""], ["Chen", "Muhao", ""], ["Szekely", "Pedro", ""]]}, {"id": "2104.08718", "submitter": "Jack Hessel", "authors": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi", "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in stark contrast to the reference-free manner in which humans\nassess caption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker vs reference-based\nmetrics, e.g., news captions that require richer contextual knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:00:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hessel", "Jack", ""], ["Holtzman", "Ari", ""], ["Forbes", "Maxwell", ""], ["Bras", "Ronan Le", ""], ["Choi", "Yejin", ""]]}, {"id": "2104.08721", "submitter": "Kelly Marchisio", "authors": "Kelly Marchisio, Conghao Xiong, and Philipp Koehn", "title": "Embedding-Enhanced Giza++: Improving Alignment in Low- and High-\n  Resource Scenarios Using Embedding Space Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular natural language processing task decades ago, word alignment has\nbeen dominated until recently by GIZA++, a statistical method based on the\n30-year-old IBM models. Though recent years have finally seen Giza++\nperformance bested, the new methods primarily rely on large machine translation\nmodels, massively multilingual language models, or supervision from Giza++\nalignments itself. We introduce Embedding-Enhanced Giza++, and outperform\nGiza++ without any of the aforementioned factors. Taking advantage of\nmonolingual embedding space geometry of the source and target language only, we\nexceed Giza++'s performance in every tested scenario for three languages. In\nthe lowest-resource scenario of only 500 lines of bitext, we improve\nperformance over Giza++ by 10.9 AER. Our method scales monotonically\noutperforming Giza++ for all tested scenarios between 500 and 1.9 million lines\nof bitext. Our code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:21:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Marchisio", "Kelly", ""], ["Xiong", "Conghao", ""], ["Koehn", "Philipp", ""]]}, {"id": "2104.08723", "submitter": "Xiuwen Zheng", "authors": "Xiuwen Zheng, Dheeraj Mekala, Amarnath Gupta, Jingbo Shang", "title": "News Meets Microblog: Hashtag Annotation via Retriever-Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hashtag annotation for microblog posts has been recently formulated as a\nsequence generation problem to handle emerging hashtags that are unseen in the\ntraining set. The state-of-the-art method leverages conversations initiated by\nposts to enrich contextual information for the short posts. However, it is\nunrealistic to assume the existence of conversations before the hashtag\nannotation itself. Therefore, we propose to leverage news articles published\nbefore the microblog post to generate hashtags following a Retriever-Generator\nframework. Extensive experiments on English Twitter datasets demonstrate\nsuperior performance and significant advantages of leveraging news articles to\ngenerate hashtags.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:28:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zheng", "Xiuwen", ""], ["Mekala", "Dheeraj", ""], ["Gupta", "Amarnath", ""], ["Shang", "Jingbo", ""]]}, {"id": "2104.08724", "submitter": "Yuning Mao", "authors": "Yuning Mao, Wenchang Ma, Deren Lei, Xiang Ren", "title": "Extract, Denoise, and Enforce: Evaluating and Predicting Lexical\n  Constraints for Conditional Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained language models (PLMs) have dominated conditional text\ngeneration tasks. Given the impressive performance and prevalence of the PLMs,\nit is seemingly natural to assume that they could figure out what to attend to\nin the input and what to include in the output via seq2seq learning without\nmore guidance than the training input/output pairs. However, a rigorous study\nregarding the above assumption is still lacking. In this paper, we present a\nsystematic analysis of conditional generation to study whether current PLMs are\ngood enough for preserving important concepts in the input and to what extent\nexplicitly guiding generation with lexical constraints is beneficial. We\nconduct extensive analytical experiments on a range of conditional generation\ntasks and try to answer in what scenarios guiding generation with lexical\nconstraints works well and why. We then propose a framework for automatic\nconstraint extraction, denoising, and enforcement that is shown to perform\ncomparably or better than unconstrained generation. We hope that our findings\ncould serve as a reference when determining whether it is appropriate and\nworthwhile to use explicit constraints for a specific task or\ndataset.\\footnote{Our code is available at\n\\url{https://github.com/morningmoni/LCGen-eval}.}\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:29:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mao", "Yuning", ""], ["Ma", "Wenchang", ""], ["Lei", "Deren", ""], ["Ren", "Xiang", ""]]}, {"id": "2104.08726", "submitter": "Abteen Ebrahimi", "authors": "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis\n  Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan\n  Vladimir, Gustavo A. Gim\\'enez-Lugo, Elisabeth Mager, Graham Neubig, Alexis\n  Palmer, Rolando A. Coto Solano, Ngoc Thang Vu and Katharina Kann", "title": "AmericasNLI: Evaluating Zero-shot Natural Language Understanding of\n  Pretrained Multilingual Models in Truly Low-resource Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained multilingual models are able to perform cross-lingual transfer in\na zero-shot setting, even for languages unseen during pretraining. However,\nprior work evaluating performance on unseen languages has largely been limited\nto low-level, syntactic tasks, and it remains unclear if zero-shot learning of\nhigh-level, semantic tasks is possible for unseen languages. To explore this\nquestion, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018)\nto 10 indigenous languages of the Americas. We conduct experiments with XLM-R,\ntesting multiple zero-shot and translation-based approaches. Additionally, we\nexplore model adaptation via continued pretraining and provide an analysis of\nthe dataset by considering hypothesis-only models. We find that XLM-R's\nzero-shot performance is poor for all 10 languages, with an average performance\nof 38.62%. Continued pretraining offers improvements, with an average accuracy\nof 44.05%. Surprisingly, training on poorly translated data by far outperforms\nall other methods with an accuracy of 48.72%.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:32:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ebrahimi", "Abteen", ""], ["Mager", "Manuel", ""], ["Oncevay", "Arturo", ""], ["Chaudhary", "Vishrav", ""], ["Chiruzzo", "Luis", ""], ["Fan", "Angela", ""], ["Ortega", "John", ""], ["Ramos", "Ricardo", ""], ["Rios", "Annette", ""], ["Vladimir", "Ivan", ""], ["Gim\u00e9nez-Lugo", "Gustavo A.", ""], ["Mager", "Elisabeth", ""], ["Neubig", "Graham", ""], ["Palmer", "Alexis", ""], ["Solano", "Rolando A. Coto", ""], ["Vu", "Ngoc Thang", ""], ["Kann", "Katharina", ""]]}, {"id": "2104.08727", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh\n  Hajishirzi, Chris Callison-Burch", "title": "GooAQ: Open Question Answering with Diverse Answer Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While day-to-day questions come with a variety of answer types, the current\nquestion-answering (QA) literature has failed to adequately address the answer\ndiversity of questions. To this end, we present GooAQ, a large-scale dataset\nwith a variety of answer types. This dataset contains over 5 million questions\nand 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete\nfeature. This results in naturalistic questions of practical interest that are\nnonetheless short and expressed using simple language. GooAQ answers are mined\nfrom Google's responses to our collected questions, specifically from the\nanswer boxes in the search results. This yields a rich space of answer types,\ncontaining both textual answers (short and long) as well as more structured\nones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)\nin line with recent work, LM's strong performance on GooAQ's short-answer\nquestions heavily benefit from annotated data; however, (b) their quality in\ngenerating coherent and accurate responses for questions requiring long\nresponses (such as 'how' and 'why' questions) is less reliant on observing\nannotated data and mainly supported by their pre-training. We release GooAQ to\nfacilitate further research on improving QA with diverse response types.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:40:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khashabi", "Daniel", ""], ["Ng", "Amos", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Hajishirzi", "Hannaneh", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2104.08728", "submitter": "Emily Sheng", "authors": "Emily Sheng, Josh Arnold, Zhou Yu, Kai-Wei Chang, Nanyun Peng", "title": "Revealing Persona Biases in Dialogue Systems", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue systems in the form of chatbots and personal assistants are being\nincreasingly integrated into people's lives. These dialogue systems often have\nthe ability to adopt an anthropomorphic persona, mimicking a societal\ndemographic to appear more approachable and trustworthy to users. However, the\nadoption of a persona can result in the adoption of biases. We define persona\nbiases as harmful differences in text (e.g., varying levels of offensiveness or\naffirmations of biased statements) generated from adopting different\ndemographic personas. In this paper, we present the first large-scale study on\npersona biases in dialogue systems and conduct analyses on personas of\ndifferent social classes, sexual orientations, races, and genders. Furthermore,\nwe introduce an open-source framework, UnitPersonaBias, a tool to explore and\naggregate subtle persona biases in dialogue systems. In our studies of the\nBlender and DialoGPT dialogue systems, we show that the choice of personas can\naffect the degree of harms in generated responses. Additionally, adopting\npersonas of more diverse, historically marginalized demographics appears to\ndecrease harmful responses the most.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:44:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sheng", "Emily", ""], ["Arnold", "Josh", ""], ["Yu", "Zhou", ""], ["Chang", "Kai-Wei", ""], ["Peng", "Nanyun", ""]]}, {"id": "2104.08729", "submitter": "Xianjie Shen", "authors": "Xianjie Shen, Yinghan Wang, Rui Meng, Jingbo Shang", "title": "Unsupervised Deep Keyphrase Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase generation aims to summarize long documents with a collection of\nsalient phrases. Deep neural models have demonstrated a remarkable success in\nthis task, capable of predicting keyphrases that are even absent from a\ndocument. However, such abstractiveness is acquired at the expense of a\nsubstantial amount of annotated data. In this paper, we present a novel method\nfor keyphrase generation, AutoKeyGen, without the supervision of any human\nannotation. Motivated by the observation that an absent keyphrase in one\ndocument can appear in other places, in whole or in part, we first construct a\nphrase bank by pooling all phrases in a corpus. With this phrase bank, we then\ndraw candidate absent keyphrases for each document through a partial matching\nprocess. To rank both types of candidates, we combine their lexical- and\nsemantic-level similarities to the input document. Moreover, we utilize these\ntop-ranked candidates as to train a deep generative model for more absent\nkeyphrases. Extensive experiments demonstrate that AutoKeyGen outperforms all\nunsupervised baselines and can even beat strong supervised methods in certain\ncases.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:53:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shen", "Xianjie", ""], ["Wang", "Yinghan", ""], ["Meng", "Rui", ""], ["Shang", "Jingbo", ""]]}, {"id": "2104.08731", "submitter": "Jifan Chen", "authors": "Jifan Chen, Eunsol Choi, Greg Durrett", "title": "Can NLI Models Verify QA Systems' Predictions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build robust question answering systems, we need the ability to verify\nwhether answers to questions are truly correct, not just \"good enough\" in the\ncontext of imperfect QA datasets. We explore the use of natural language\ninference (NLI) as a way to achieve this goal, as NLI inherently requires the\npremise (document context) to contain all necessary information to support the\nhypothesis (proposed answer to the question). We leverage large pre-trained\nmodels and recent prior datasets to construct powerful question converter and\ndecontextualization modules, which can reformulate QA instances as\npremise-hypothesis pairs with very high reliability. Then, by combining\nstandard NLI datasets with NLI examples automatically derived from QA training\ndata, we can train NLI models to judge the correctness of QA models' proposed\nanswers. We show that our NLI approach can generally improve the confidence\nestimation of a QA model across different domains, evaluated in a selective QA\nsetting. Careful manual analysis over the predictions of our NLI model shows\nthat it can further identify cases where the QA model produces the right answer\nfor the wrong reason, or where the answer cannot be verified as addressing all\naspects of the question.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:03:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Jifan", ""], ["Choi", "Eunsol", ""], ["Durrett", "Greg", ""]]}, {"id": "2104.08735", "submitter": "Dheeru Dua", "authors": "Dheeru Dua, Pradeep Dasigi, Sameer Singh, Matt Gardner", "title": "Learning with Instance Bundles for Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When training most modern reading comprehension models, all the questions\nassociated with a context are treated as being independent from each other.\nHowever, closely related questions and their corresponding answers are not\nindependent, and leveraging these relationships could provide a strong\nsupervision signal to a model. Drawing on ideas from contrastive estimation, we\nintroduce several new supervision techniques that compare question-answer\nscores across multiple related instances. Specifically, we normalize these\nscores across various neighborhoods of closely contrasting questions and/or\nanswers, adding another cross entropy loss term that is used in addition to\ntraditional maximum likelihood estimation. Our techniques require bundles of\nrelated question-answer pairs, which we can either mine from within existing\ndata or create using various automated heuristics. We empirically demonstrate\nthe effectiveness of training with instance bundles on two datasets -- HotpotQA\nand ROPES -- showing up to 11% absolute gains in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:17:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dua", "Dheeru", ""], ["Dasigi", "Pradeep", ""], ["Singh", "Sameer", ""], ["Gardner", "Matt", ""]]}, {"id": "2104.08737", "submitter": "Akhil Arora", "authors": "Akhil Arora, Alberto Garcia-Duran, Robert West", "title": "Low-rank Subspaces for Unsupervised Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking is an important problem with many applications. Most previous\nsolutions were designed for settings where annotated training data is\navailable, which is, however, not the case in numerous domains. We propose a\nlight-weight and scalable entity linking method, Eigenthemes, that relies\nsolely on the availability of entity names and a referent knowledge base.\nEigenthemes exploits the fact that the entities that are truly mentioned in a\ndocument (the \"gold entities\") tend to form a semantically dense subset of the\nset of all candidate entities in the document. Geometrically speaking, when\nrepresenting entities as vectors via some given embedding, the gold entities\ntend to lie in a low-rank subspace of the full embedding space. Eigenthemes\nidentifies this subspace using the singular value decomposition and scores\ncandidate entities according to their proximity to the subspace. On the\nempirical front, we introduce multiple strong baselines that compare favorably\nto the existing state of the art. Extensive experiments on benchmark datasets\nfrom a variety of real-world domains showcase the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:24:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Arora", "Akhil", ""], ["Garcia-Duran", "Alberto", ""], ["West", "Robert", ""]]}, {"id": "2104.08741", "submitter": "Keshav Kolluru", "authors": "Keshav Kolluru, Mayank Singh Chauhan, Yatin Nandwani, Parag Singla and\n  Mausam", "title": "CEAR: Cross-Entity Aware Reranker for Knowledge Base Completion", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (LMs) like BERT have shown to store factual\nknowledge about the world. This knowledge can be used to augment the\ninformation present in Knowledge Bases, which tend to be incomplete. However,\nprior attempts at using BERT for task of Knowledge Base Completion (KBC)\nresulted in performance worse than embedding based techniques that rely only on\nthe graph structure. In this work we develop a novel model, Cross-Entity Aware\nReranker (CEAR), that uses BERT to re-rank the output of existing KBC models\nwith cross-entity attention. Unlike prior work that scores each entity\nindependently, CEAR uses BERT to score the entities together, which is\neffective for exploiting its factual knowledge. CEAR establishes a new state of\nthe art performance with 42.6 HITS@1 in FB15k-237 (32.7% relative improvement)\nand 5.3 pt improvement in HITS@1 for Open Link Prediction.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:56:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kolluru", "Keshav", ""], ["Chauhan", "Mayank Singh", ""], ["Nandwani", "Yatin", ""], ["Singla", "Parag", ""], ["Mausam", "", ""]]}, {"id": "2104.08742", "submitter": "Rik Koncel-Kedziorski", "authors": "Rik Koncel-Kedziorski and Noah A. Smith", "title": "Go Forth and Prosper: Language Modeling with Ancient Textual History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique for improving document-level language models (LM) by\nleveraging \"ancient history\": text that is outside the LM's current context\nwindow. We learn an auxiliary function to select spans from the ancient history\nwhich can help the LM to predict future text. The selected text spans are then\ncopied directly into the LM's context window, replacing less predictive spans.\nThis method can improve perplexity of pretrained LMs with no updates to the\nLM's own parameters. We further observe that an auxiliary function trained in a\nspecific textual domain like Wikipedia will also work in a substantially\ndifferent domain such as scientific publications. With this technique we see a\n7 percent perplexity reduction on Wikipedia articles, and a 12 percent\nperplexity reduction on scientific texts.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:57:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Koncel-Kedziorski", "Rik", ""], ["Smith", "Noah A.", ""]]}, {"id": "2104.08744", "submitter": "Dheeru Dua", "authors": "Dheeru Dua, Cicero Nogueira dos Santos, Patrick Ng, Ben Athiwaratkun,\n  Bing Xiang, Matt Gardner, Sameer Singh", "title": "Generative Context Pair Selection for Multi-hop Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Compositional reasoning tasks like multi-hop question answering, require\nmaking latent decisions to get the final answer, given a question. However,\ncrowdsourced datasets often capture only a slice of the underlying task\ndistribution, which can induce unanticipated biases in models performing\ncompositional reasoning. Furthermore, discriminatively trained models exploit\nsuch biases to get a better held-out performance, without learning the right\nway to reason, as they do not necessitate paying attention to the question\nrepresentation (conditioning variable) in its entirety, to estimate the answer\nlikelihood. In this work, we propose a generative context selection model for\nmulti-hop question answering that reasons about how the given question could\nhave been generated given a context pair. While being comparable to the\nstate-of-the-art answering performance, our proposed generative passage\nselection model has a better performance (4.9% higher than baseline) on\nadversarial held-out set which tests robustness of model's multi-hop reasoning\ncapabilities.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:00:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dua", "Dheeru", ""], ["Santos", "Cicero Nogueira dos", ""], ["Ng", "Patrick", ""], ["Athiwaratkun", "Ben", ""], ["Xiang", "Bing", ""], ["Gardner", "Matt", ""], ["Singh", "Sameer", ""]]}, {"id": "2104.08755", "submitter": "Zhaohao Zeng", "authors": "Zhaohao Zeng and Tetsuya Sakai", "title": "DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions\n  of Annotators' Labels", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a data set called DCH-2, which contains 4,390 real\ncustomer-helpdesk dialogues in Chinese and their English translations. DCH-2\nalso contains dialogue-level annotations and turn-level annotations obtained\nindependently from either 19 or 20 annotators. The data set was built through\nour effort as organisers of the NTCIR-14 Short Text Conversation and NTCIR-15\nDialogue Evaluation tasks, to help researchers understand what constitutes an\neffective customer-helpdesk dialogue, and thereby build efficient and helpful\nhelpdesk systems that are available to customers at all times. In addition,\nDCH-2 may be utilised for other purposes, for example, as a repository for\nretrieval-based dialogue systems, or as a parallel corpus for machine\ntranslation in the helpdesk domain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:35:15 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 15:32:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zeng", "Zhaohao", ""], ["Sakai", "Tetsuya", ""]]}, {"id": "2104.08757", "submitter": "Guanhua Chen", "authors": "Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan,\n  Wenping Wang, Furu Wei", "title": "Zero-shot Cross-lingual Transfer of Neural Machine Translation with\n  Multilingual Pretrained Encoders", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works mainly focus on improving cross-lingual transfer for NLU tasks\nwith multilingual pretrained encoder (MPE), or improving the translation\nperformance on NMT task with BERT. However, how to improve the cross-lingual\ntransfer of NMT model with multilingual pretrained encoder is under-explored.\nIn this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In\nthis task, the NMT model is trained with one parallel dataset and an\noff-the-shelf MPE, then is directly tested on zero-shot language pairs. We\npropose SixT, a simple yet effective model for this task. The SixT model\nleverages the MPE with a two-stage training schedule and gets further\nimprovement with a position disentangled encoder and a capacity-enhanced\ndecoder. The extensive experiments prove that SixT significantly improves the\ntranslation quality of the unseen languages. With much less computation cost\nand training data, our model achieves better performance on many-to-English\ntestsets than CRISS and m2m-100, two strong multilingual NMT baselines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:42:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Guanhua", ""], ["Ma", "Shuming", ""], ["Chen", "Yun", ""], ["Dong", "Li", ""], ["Zhang", "Dongdong", ""], ["Pan", "Jia", ""], ["Wang", "Wenping", ""], ["Wei", "Furu", ""]]}, {"id": "2104.08758", "submitter": "Jesse Dodge", "authors": "Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel\n  Ilharco, Dirk Groeneveld, Matt Gardner", "title": "Documenting the English Colossal Clean Crawled Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As language models are trained on ever more text, researchers are turning to\nsome of the largest corpora available. Unlike most other types of datasets in\nNLP, large unlabeled text corpora are often presented with minimal\ndocumentation, and best practices for documenting them have not been\nestablished. In this work we provide the first documentation for the Colossal\nClean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a\nset of filters to a single snapshot of Common Crawl. We begin with a high-level\nsummary of the data, including distributions of where the text came from and\nwhen it was written. We then give more detailed analysis on salient parts of\nthis data, including the most frequent sources of text (e.g.,\npatents.google.com, which contains a significant percentage of machine\ntranslated and/or OCR'd text), the effect that the filters had on the data\n(they disproportionately remove text in AAE), and evidence that some other\nbenchmark NLP dataset examples are contained in the text. We release a web\ninterface to an interactive, indexed copy of this dataset, encouraging the\ncommunity to continuously explore and report additional findings.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:42:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dodge", "Jesse", ""], ["Sap", "Maarten", ""], ["Marasovic", "Ana", ""], ["Agnew", "William", ""], ["Ilharco", "Gabriel", ""], ["Groeneveld", "Dirk", ""], ["Gardner", "Matt", ""]]}, {"id": "2104.08762", "submitter": "Rajarshi Das", "authors": "Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez,\n  Jay-Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum", "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often challenging for a system to solve a new complex problem from\nscratch, but much easier if the system can access other similar problems and\ndescription of their solutions -- a paradigm known as case-based reasoning\n(CBR). We propose a neuro-symbolic CBR approach for question answering over\nlarge knowledge bases (CBR-KBQA). While the idea of CBR is tempting, composing\na solution from cases is nontrivial, when individual cases only contain partial\nlogic to the full solution. To resolve this, CBR-KBQA consists of two modules:\na non-parametric memory that stores cases (question and logical forms) and a\nparametric model which can generate logical forms by retrieving relevant cases\nfrom memory. Through experiments, we show that CBR-KBQA can effectively derive\nnovel combination of relations not presented in case memory that is required to\nanswer compositional questions. On several KBQA datasets that test\ncompositional generalization, CBR-KBQA achieves competitive performance. For\nexample, on the challenging ComplexWebQuestions dataset, CBR-KBQA outperforms\nthe current state of the art by 11% accuracy. Furthermore, we show that\nCBR-KBQA is capable of using new cases \\emph{without} any further training.\nJust by incorporating few human-labeled examples in the non-parametric case\nmemory, CBR-KBQA is able to successfully generate queries containing unseen KB\nrelations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:50:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Rajarshi", ""], ["Zaheer", "Manzil", ""], ["Thai", "Dung", ""], ["Godbole", "Ameya", ""], ["Perez", "Ethan", ""], ["Lee", "Jay-Yoon", ""], ["Tan", "Lizhen", ""], ["Polymenakos", "Lazaros", ""], ["McCallum", "Andrew", ""]]}, {"id": "2104.08763", "submitter": "Shunsuke Kitada", "authors": "Shunsuke Kitada, Hitoshi Iyatomi", "title": "Making Attention Mechanisms More Robust and Interpretable with Virtual\n  Adversarial Training for Semi-Supervised Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new general training technique for attention mechanisms based on\nvirtual adversarial training (VAT). VAT can compute adversarial perturbations\nfrom unlabeled data in a semi-supervised setting for the attention mechanisms\nthat have been reported in previous studies to be vulnerable to perturbations.\nEmpirical experiments reveal that our technique (1) provides significantly\nbetter prediction performance compared to not only conventional adversarial\ntraining-based techniques but also VAT-based techniques in a semi-supervised\nsetting, (2) demonstrates a stronger correlation with the word importance and\nbetter agreement with evidence provided by humans, and (3) gains in performance\nwith increasing amounts of unlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:51:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kitada", "Shunsuke", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2104.08765", "submitter": "Aman Madaan", "authors": "Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Yiming Yang, Peter\n  Clark, Keisuke Sakaguchi, Ed Hovy", "title": "Improving Neural Model Performance through Natural Language Feedback on\n  Their Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A class of explainable NLP models for reasoning tasks support their decisions\nby generating free-form or structured explanations, but what happens when these\nsupporting structures contain errors? Our goal is to allow users to\ninteractively correct explanation structures through natural language feedback.\nWe introduce MERCURIE - an interactive system that refines its explanations for\na given reasoning task by getting human feedback in natural language. Our\napproach generates graphs that have 40% fewer inconsistencies as compared with\nthe off-the-shelf system. Further, simply appending the corrected explanation\nstructures to the output leads to a gain of 1.2 points on accuracy on\ndefeasible reasoning across all three domains. We release a dataset of over\n450k graphs for defeasible reasoning generated by our system at\nhttps://tinyurl.com/mercurie .\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:10:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Madaan", "Aman", ""], ["Tandon", "Niket", ""], ["Rajagopal", "Dheeraj", ""], ["Yang", "Yiming", ""], ["Clark", "Peter", ""], ["Sakaguchi", "Keisuke", ""], ["Hovy", "Ed", ""]]}, {"id": "2104.08768", "submitter": "Richard Shin", "authors": "Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro\n  Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner,\n  Benjamin Van Durme", "title": "Constrained Language Models Yield Few-Shot Semantic Parsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of large pretrained language models as few-shot semantic\nparsers. The goal in semantic parsing is to generate a structured meaning\nrepresentation given a natural language input. However, language models are\ntrained to generate natural language. To bridge the gap, we use language models\nto paraphrase inputs into a controlled sublanguage resembling English that can\nbe automatically mapped to a target meaning representation. With a small amount\nof data and very little code to convert into English-like representations, we\nprovide a blueprint for rapidly bootstrapping semantic parsers and demonstrate\ngood performance on multiple tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:13:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shin", "Richard", ""], ["Lin", "Christopher H.", ""], ["Thomson", "Sam", ""], ["Chen", "Charles", ""], ["Roy", "Subhro", ""], ["Platanios", "Emmanouil Antonios", ""], ["Pauls", "Adam", ""], ["Klein", "Dan", ""], ["Eisner", "Jason", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2104.08771", "submitter": "Mozhdeh Gheini", "authors": "Mozhdeh Gheini, Xiang Ren, Jonathan May", "title": "On the Strengths of Cross-Attention in Pretrained Transformers for\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the power of cross-attention in the Transformer architecture within\nthe context of machine translation. In transfer learning experiments, where we\nfine-tune a translation model on a dataset with one new language, we find that,\napart from the new language's embeddings, only the cross-attention parameters\nneed to be fine-tuned to obtain competitive BLEU performance. We provide\ninsights into why this is the case and further find that limiting fine-tuning\nin this manner yields cross-lingually aligned type embeddings. The implications\nof this finding include a mitigation of catastrophic forgetting in the network\nand the potential for zero-shot translation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:41:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gheini", "Mozhdeh", ""], ["Ren", "Xiang", ""], ["May", "Jonathan", ""]]}, {"id": "2104.08773", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi", "title": "Natural Instructions: Benchmarking Generalization to New Tasks from\n  Natural Language Instructions", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we enable NLP models to appropriately respond to instructional prompts\nand consequently generalize to new tasks? To study this question, we leverage\nthe existing NLP datasets and the instructions that were used to crowdsource\nthem to create NATURAL INSTRUCTIONS, a dataset of instructions and\ntask-specific input/output data. This dataset consists of 61 distinct language\ninstructions and about 600k task instances, and is used to evaluate existing\nstate-of-the-art language-models (LMs) in addressing new tasks by few-shot\nprompting of GPT3 and fine-tuning BART. Our analysis indicates that: (a) the\nexisting models indeed benefit from instructions and hence, show improved\ngeneralization to new tasks; (b) while models like GPT-3 generally benefit from\ninstructions, the extent of their gains varies across different fields of\ninstructions and also depends on the task being solved; (c) generalization to\nunseen tasks in NATURAL INSTRUCTIONS remains far from perfect for the\nstate-of-the-art, indicating significant room for more progress in this\ndirection.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:44:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mishra", "Swaroop", ""], ["Khashabi", "Daniel", ""], ["Baral", "Chitta", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2104.08775", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Andrea Madotto, Yejin Bang, Pascale Fung", "title": "Dynamically Addressing Unseen Rumor via Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rumors are often associated with newly emerging events, thus, an ability to\ndeal with unseen rumors is crucial for a rumor veracity classification model.\nPrevious works address this issue by improving the model's generalizability,\nwith an assumption that the model will stay unchanged even after the new\noutbreak of an event. In this work, we propose an alternative solution to\ncontinuously update the model in accordance with the dynamics of rumor domain\ncreations. The biggest technical challenge associated with this new approach is\nthe catastrophic forgetting of previous learnings due to new learnings. We\nadopt continual learning strategies that control the new learnings to avoid\ncatastrophic forgetting and propose an additional strategy that can jointly be\nused to strengthen the forgetting alleviation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:50:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lee", "Nayeon", ""], ["Madotto", "Andrea", ""], ["Bang", "Yejin", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.08779", "submitter": "Ziqian Zeng", "authors": "Ziqian Zeng, Yangqiu Song", "title": "Variational Weakly Supervised Sentiment Analysis with Posterior\n  Regularization", "comments": "Accepted at EACL 2021. arXiv admin note: text overlap with\n  arXiv:2008.09394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis is an important task in natural language processing (NLP).\nMost of existing state-of-the-art methods are under the supervised learning\nparadigm. However, human annotations can be scarce. Thus, we should leverage\nmore weak supervision for sentiment analysis. In this paper, we propose a\nposterior regularization framework for the variational approach to the weakly\nsupervised sentiment analysis to better control the posterior distribution of\nthe label assignment. The intuition behind the posterior regularization is that\nif extracted opinion words from two documents are semantically similar, the\nposterior distributions of two documents should be similar. Our experimental\nresults show that the posterior regularization can improve the original\nvariational approach to the weakly supervised sentiment analysis and the\nperformance is more stable with smaller prediction variance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:05:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zeng", "Ziqian", ""], ["Song", "Yangqiu", ""]]}, {"id": "2104.08782", "submitter": "Fan Yin", "authors": "Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang", "title": "On the Faithfulness Measurements for Model Interpretations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed the emergence of a variety of post-hoc\ninterpretations that aim to uncover how natural language processing (NLP)\nmodels make predictions. Despite the surge of new interpretations, it remains\nan open problem how to define and quantitatively measure the faithfulness of\ninterpretations, i.e., to what extent they conform to the reasoning process\nbehind the model. To tackle these issues, we start with three criteria: the\nremoval-based criterion, the sensitivity of interpretations, and the stability\nof interpretations, that quantify different notions of faithfulness, and\npropose novel paradigms to systematically evaluate interpretations in NLP. Our\nresults show that the performance of interpretations under different criteria\nof faithfulness could vary substantially. Motivated by the desideratum of these\nfaithfulness notions, we introduce a new class of interpretation methods that\nadopt techniques from the adversarial robustness domain. Empirical results show\nthat our proposed methods achieve top performance under all three criteria.\nAlong with experiments and analysis on both the text classification and the\ndependency parsing tasks, we come to a more comprehensive understanding of the\ndiverse set of interpretations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:19:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yin", "Fan", ""], ["Shi", "Zhouxing", ""], ["Hsieh", "Cho-Jui", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2104.08786", "submitter": "Yao Lu", "authors": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus\n  Stenetorp", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When primed with only a handful of training samples, very large pretrained\nlanguage models such as GPT-3, have shown competitive results when compared to\nfully-supervised fine-tuned large pretrained language models. We demonstrate\nthat the order in which the samples are provided can be the difference between\nnear state-of-the-art and random guess performance: Essentially some\npermutations are \"fantastic\" and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of the\nlanguage models to construct an artificial development set and based on entropy\nstatistics of the candidate permutations from this set we identify performant\nprompts. Our method improves upon GPT-family models by on average 13% relative\nacross eleven different established text classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:29:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lu", "Yao", ""], ["Bartolo", "Max", ""], ["Moore", "Alastair", ""], ["Riedel", "Sebastian", ""], ["Stenetorp", "Pontus", ""]]}, {"id": "2104.08787", "submitter": "Zhen Wang", "authors": "Zhen Wang, Xiangxie Zhang, Yicong Tan", "title": "Chinese Sentences Similarity via Cross-Attention Based Siamese Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring sentence similarity is a key research area nowadays as it allows\nmachines to better understand human languages. In this paper, we proposed a\nCross-Attention Siamese Network (CATsNet) to carry out the task of learning the\nsemantic meanings of Chinese sentences and comparing the similarity between two\nsentences. This novel model is capable of catching non-local features.\nAdditionally, we also tried to apply the long short-term memory (LSTM) network\nin the model to improve its performance. The experiments were conducted on the\nLCQMC dataset and the results showed that our model could achieve a higher\naccuracy than previous work.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:35:58 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 18:35:07 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Zhen", ""], ["Zhang", "Xiangxie", ""], ["Tan", "Yicong", ""]]}, {"id": "2104.08790", "submitter": "Saadia Gabriel", "authors": "Saadia Gabriel, Skyler Hallinan, Maarten Sap, Pemi Nguyen, Franziska\n  Roesner, Eunsol Choi, Yejin Choi", "title": "Misinfo Belief Frames: A Case Study on Covid & Climate News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior beliefs of readers impact the way in which they project meaning onto\nnews headlines. These beliefs can influence their perception of news\nreliability, as well as their reaction to news, and their likelihood of\nspreading the misinformation through social networks. However, most prior work\nfocuses on fact-checking veracity of news or stylometry rather than measuring\nimpact of misinformation. We propose Misinfo Belief Frames, a formalism for\nunderstanding how readers perceive the reliability of news and the impact of\nmisinformation. We also introduce the Misinfo Belief Frames (MBF) corpus, a\ndataset of 66k inferences over 23.5k headlines. Misinformation frames use\ncommonsense reasoning to uncover implications of real and fake news headlines\nfocused on global crises: the Covid-19 pandemic and climate change. Our results\nusing large-scale language modeling to predict misinformation frames show that\nmachine-generated inferences can influence readers' trust in news headlines\n(readers' trust in news headlines was affected in 29.3% of cases). This\ndemonstrates the potential effectiveness of using generated frames to counter\nmisinformation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:50:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gabriel", "Saadia", ""], ["Hallinan", "Skyler", ""], ["Sap", "Maarten", ""], ["Nguyen", "Pemi", ""], ["Roesner", "Franziska", ""], ["Choi", "Eunsol", ""], ["Choi", "Yejin", ""]]}, {"id": "2104.08792", "submitter": "Mimansa Jaiswal", "authors": "Mimansa Jaiswal, Emily Mower Provost", "title": "Why Should I Trust a Model is Private? Using Shifts in Model Explanation\n  for Evaluating Privacy-Preserving Emotion Recognition Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy preservation is a crucial component of any real-world application.\nYet, in applications relying on machine learning backends, this is challenging\nbecause models often capture more than a designer may have envisioned,\nresulting in the potential leakage of sensitive information. For example,\nemotion recognition models are susceptible to learning patterns between the\ntarget variable and other sensitive variables, patterns that can be maliciously\nre-purposed to obtain protected information. In this paper, we concentrate on\nusing interpretable methods to evaluate a model's efficacy to preserve privacy\nwith respect to sensitive variables. We focus on saliency-based explanations,\nexplanations that highlight regions of the input text, which allows us to\nunderstand how model explanations shift when models are trained to preserve\nprivacy. We show how certain commonly-used methods that seek to preserve\nprivacy might not align with human perception of privacy preservation. We also\nshow how some of these induce spurious correlations in the model between the\ninput and the primary as well as secondary task, even if the improvement in\nevaluation metric is significant. Such correlations can hence lead to false\nassurances about the perceived privacy of the model because especially when\nused in cross corpus conditions. We conduct crowdsourcing experiments to\nevaluate the inclination of the evaluators to choose a particular model for a\ngiven task when model explanations are provided, and find that correlation of\ninterpretation differences with sociolinguistic biases can be used as a proxy\nfor user trust.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:56:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jaiswal", "Mimansa", ""], ["Provost", "Emily Mower", ""]]}, {"id": "2104.08793", "submitter": "Aaron Chan", "authors": "Aaron Chan, Boyuan Long, Jiashu Xu, Soumya Sanyal, Tanishq Gupta,\n  Xiang Ren", "title": "SalKG: Learning From Knowledge Graph Explanations for Commonsense\n  Reasoning", "comments": "XAI Workshop at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, we\npropose SalKG, a framework for KG-augmented models to learn from coarse and/or\nfine saliency explanations. Given saliency explanations created from a task's\ntraining set, SalKG jointly trains the model to predict the explanations, then\nsolve the task by attending to KG features highlighted by the predicted\nexplanations. On two popular commonsense QA benchmarks (CSQA, OBQA), we show\nthat \\textsc{SalKG} models can yield large performance gains -- up to 3.27% on\nCSQA.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:59:46 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:53:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chan", "Aaron", ""], ["Long", "Boyuan", ""], ["Xu", "Jiashu", ""], ["Sanyal", "Soumya", ""], ["Gupta", "Tanishq", ""], ["Ren", "Xiang", ""]]}, {"id": "2104.08799", "submitter": "Yige Xu", "authors": "Yichao Luo, Yige Xu, Jiacheng Ye, Xipeng Qiu, Qi Zhang", "title": "Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a\nclassical task for capturing the central idea from a given document. Typically,\ntraditional KG evaluation metrics are only aware of the exact correctness of\npredictions on phrase-level and ignores the semantic similarities between\nsimilar predictions and targets, which inhibits the model from learning deep\nlinguistic patterns. In this paper, we propose a new fine-grained evaluation\nmetric that considers different granularity: token-level $F_1$ score, edit\ndistance, duplication, and prediction quantities. For learning more recessive\nlinguistic patterns, we use a pre-trained model (e.g., BERT) to compute the\ncontinuous similarity score between predicted keyphrases and target keyphrases.\nOn the whole, we propose a two-stage Reinforcement Learning (RL) training\nframework with two reward functions: our proposed fine-grained evaluation score\nand the vanilla $F_1$ score. This framework helps the model identifying some\npartial match phrases which can be further optimized as the exact match ones.\nExperiments on four KG benchmarks show that our proposed training framework\noutperforms the traditional RL training frameworks among all evaluation scores.\nIn addition, our method can effectively ease the synonym problem and generate a\nhigher quality prediction.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:13:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Luo", "Yichao", ""], ["Xu", "Yige", ""], ["Ye", "Jiacheng", ""], ["Qiu", "Xipeng", ""], ["Zhang", "Qi", ""]]}, {"id": "2104.08801", "submitter": "Devang Kulshreshtha", "authors": "Devang Kulshreshtha, Robert Belfer, Iulian Vlad Serban, Siva Reddy", "title": "Back-Training excels Self-Training at Unsupervised Domain Adaptation of\n  Question Generation and Passage Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new domain adaptation method called\n$\\textit{back-training}$, a superior alternative to self-training. While\nself-training results in synthetic training data of the form quality inputs\naligned with noisy outputs, back-training results in noisy inputs aligned with\nquality outputs. Our experimental results on unsupervised domain adaptation of\nquestion generation and passage retrieval models from $\\textit{Natural\nQuestions}$ domain to the machine learning domain show that back-training\noutperforms self-training by a large margin: 9.3 BLEU-1 points on generation,\nand 7.9 accuracy points on top-1 retrieval. We release $\\textit{MLQuestions}$,\na domain-adaptation dataset for the machine learning domain containing 50K\nunaligned passages and 35K unaligned questions, and 3K aligned passage and\nquestion pairs. Our data and code are available at\nhttps://github.com/McGill-NLP/MLQuestions\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:20:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kulshreshtha", "Devang", ""], ["Belfer", "Robert", ""], ["Serban", "Iulian Vlad", ""], ["Reddy", "Siva", ""]]}, {"id": "2104.08803", "submitter": "Tal Schuster", "authors": "Tal Schuster, Adam Fisch, Tommi Jaakkola, Regina Barzilay", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approach for confidently accelerating inference in the\nlarge and expensive multilayer Transformers that are now ubiquitous in natural\nlanguage processing (NLP). Amortized or approximate computational methods\nincrease efficiency, but can come with unpredictable performance costs. In this\nwork, we present CATs -- Confident Adaptive Transformers -- in which we\nsimultaneously increase computational efficiency, while guaranteeing a\nspecifiable degree of consistency with the original model with high confidence.\nOur method trains additional prediction heads on top of intermediate layers,\nand dynamically decides when to stop allocating computational effort to each\ninput using a meta consistency classifier. To calibrate our early prediction\nstopping rule, we formulate a unique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach on four classification and\nregression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:22:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Schuster", "Tal", ""], ["Fisch", "Adam", ""], ["Jaakkola", "Tommi", ""], ["Barzilay", "Regina", ""]]}, {"id": "2104.08804", "submitter": "Harkanwar Singh", "authors": "Harkanwar Singh, Prachi Jain, Mausam, Soumen Chakrabarti", "title": "Multilingual Knowledge Graph Completion with Joint Relation and Entity\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph Completion (KGC) predicts missing facts in an incomplete\nKnowledge Graph. Almost all of existing KGC research is applicable to only one\nKG at a time, and in one language only. However, different language speakers\nmay maintain separate KGs in their language and no individual KG is expected to\nbe complete. Moreover, common entities or relations in these KGs have different\nsurface forms and IDs, leading to ID proliferation. Entity alignment (EA) and\nrelation alignment (RA) tasks resolve this by recognizing pairs of entity\n(relation) IDs in different KGs that represent the same entity (relation). This\ncan further help prediction of missing facts, since knowledge from one KG is\nlikely to benefit completion of another. High confidence predictions may also\nadd valuable information for the alignment tasks. In response, we study the\nnovel task of jointly training multilingual KGC, relation alignment and entity\nalignment models. We present ALIGNKGC, which uses some seed alignments to\njointly optimize all three of KGC, EA and RA losses. A key component of\nALIGNKGC is an embedding based soft notion of asymmetric overlap defined on the\n(subject, object) set signatures of relations this aids in better predicting\nrelations that are equivalent to or implied by other relations. Extensive\nexperiments with DBPedia in five languages establish the benefits of joint\ntraining for all tasks, achieving 10-32 MRR improvements of ALIGNKGC over a\nstrong state-of-the-art single-KGC system completion model over each\nmonolingual KG . Further, ALIGNKGC achieves reasonable gains in EA and RA tasks\nover a vanilla completion model over a KG that combines all facts without\nalignment, underscoring the value of joint training for these tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:27:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Singh", "Harkanwar", ""], ["Jain", "Prachi", ""], ["Mausam", "", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2104.08808", "submitter": "Xisen Jin", "authors": "Xisen Jin, Mohammad Rostami, Xiang Ren", "title": "Lifelong Learning of Few-shot Learners across NLP Tasks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large pre-trained language models have greatly improved\nthe performance on a broad set of NLP tasks. However, adapting an existing\nmodel to new tasks often requires (repeated) re-training over enormous labeled\ndata that is prohibitively expensive to obtain. Moreover, models learned on new\ntasks may gradually \"forget\" about the knowledge learned from earlier tasks\n(i.e., catastrophic forgetting). In this paper, we study the challenge of\nlifelong learning to few-shot learn over a sequence of diverse NLP tasks,\nthrough continuously fine-tuning a language model. We investigate the model's\nability of few-shot generalization to new tasks while retaining its performance\non the previously learned tasks. We explore existing continual learning methods\nin solving this problem and propose a continual meta-learning approach which\nlearns to generate adapter weights from a few examples while regularizing\nchanges of the weights to mitigate catastrophic forgetting. We demonstrate our\napproach preserves model performance over training tasks and leads to positive\nknowledge transfer when the future tasks are learned.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:41:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jin", "Xisen", ""], ["Rostami", "Mohammad", ""], ["Ren", "Xiang", ""]]}, {"id": "2104.08809", "submitter": "Arie Cattan", "authors": "Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug\n  Downey, Tom Hope", "title": "SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts", "comments": "Data and code available at https://github.com/ariecattan/SciCo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Determining coreference of concept mentions across multiple documents is\nfundamental for natural language understanding. Work on cross-document\ncoreference resolution (CDCR) typically considers mentions of events in the\nnews, which do not often involve abstract technical concepts that are prevalent\nin science and technology. These complex concepts take diverse or ambiguous\nforms and have many hierarchical levels of granularity (e.g., tasks and\nsubtasks), posing challenges for CDCR. We present a new task of hierarchical\nCDCR for concepts in scientific papers, with the goal of jointly inferring\ncoreference clusters and hierarchy between them. We create SciCo, an\nexpert-annotated dataset for this task, which is 3X larger than the prominent\nECB+ resource. We find that tackling both coreference and hierarchy at once\noutperforms disjoint models, which we hope will spur development of joint\nmodels for SciCo.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:42:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cattan", "Arie", ""], ["Johnson", "Sophie", ""], ["Weld", "Daniel", ""], ["Dagan", "Ido", ""], ["Beltagy", "Iz", ""], ["Downey", "Doug", ""], ["Hope", "Tom", ""]]}, {"id": "2104.08811", "submitter": "Anton Belyy", "authors": "Noah Weber, Anton Belyy, Nils Holzenberger, Rachel Rudinger, Benjamin\n  Van Durme", "title": "Schema Curation via Causal Association Rule Mining", "comments": "9 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event schemas are structured knowledge sources defining typical real-world\nscenarios (e.g., going to an airport). We present a framework for efficient\nhuman-in-the-loop construction of a schema library, based on a novel mechanism\nfor schema induction and a well-crafted interface that allows non-experts to\n\"program\" complex event structures. Associated with this work we release a\nmachine readable resource (schema library) of 232 detailed event schemas, each\nof which describe a distinct typical scenario in terms of its relevant\nsub-event structure (what happens in the scenario), participants (who plays a\nrole in the scenario), fine-grained typing of each participant, and the implied\nrelational constraints between them. Our custom annotation interface,\nSchemaBlocks, and the event schemas are available online.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:48:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Weber", "Noah", ""], ["Belyy", "Anton", ""], ["Holzenberger", "Nils", ""], ["Rudinger", "Rachel", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2104.08812", "submitter": "Wenxuan Zhou", "authors": "Wenxuan Zhou, Muhao Chen", "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained transformers achieve remarkable performance when the test data\nfollows the same distribution as the training data. However, in real-world NLU\ntasks, the model often faces out-of-distribution (OoD) instances. Such\ninstances can cause the severe semantic shift problem to inference, hence they\nare supposed to be identified and rejected by the model. In this paper, we\nstudy the OoD detection problem for pretrained transformers using only\nin-distribution data in training. We observe that such instances can be found\nusing the Mahalanobis distance in the penultimate layer. We further propose a\ncontrastive loss that improves the compactness of representations, such that\nOoD instances can be better differentiated from in-distribution ones.\nExperiments on the GLUE benchmark demonstrate the effectiveness of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:51:47 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Wenxuan", ""], ["Chen", "Muhao", ""]]}, {"id": "2104.08815", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang,\n  Mahdi Soltanolkotabi, Xiang Ren, Salman Avestimehr", "title": "FedNLP: A Research Platform for Federated Learning in Natural Language\n  Processing", "comments": "Github link: https://github.com/FedML-AI/FedNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasing concerns and regulations about data privacy, necessitate the study\nof privacy-preserving methods for natural language processing (NLP)\napplications. Federated learning (FL) provides promising methods for a large\nnumber of clients (i.e., personal devices or organizations) to collaboratively\nlearn a shared global model to benefit all clients, while allowing users to\nkeep their data locally. To facilitate FL research in NLP, we present the\nFedNLP, a research platform for federated learning in NLP. FedNLP supports\nvarious popular task formulations in NLP such as text classification, sequence\ntagging, question answering, seq2seq generation, and language modeling. We also\nimplement an interface between Transformer language models (e.g., BERT) and FL\nmethods (e.g., FedAvg, FedOpt, etc.) for distributed training. The evaluation\nprotocol of this interface supports a comprehensive collection of non-IID\npartitioning strategies. Our preliminary experiments with FedNLP reveal that\nthere exists a large performance gap between learning on decentralized and\ncentralized datasets -- opening intriguing and exciting future research\ndirections aimed at developing FL methods suited to NLP tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:04:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["He", "Chaoyang", ""], ["Zeng", "Zihang", ""], ["Wang", "Hulin", ""], ["Huang", "Yufen", ""], ["Soltanolkotabi", "Mahdi", ""], ["Ren", "Xiang", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2104.08817", "submitter": "Javier Iranzo-S\\'anchez", "authors": "Javier Iranzo-S\\'anchez and Jorge Civera and Alfons Juan", "title": "Stream-level Latency Evaluation for Simultaneous Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Simultaneous machine translation has recently gained traction thanks to\nsignificant quality improvements and the advent of streaming applications.\nSimultaneous translation systems need to find a trade-off between translation\nquality and response time, and with this purpose multiple latency measures have\nbeen proposed. However, latency evaluations for simultaneous translation are\nestimated at the sentence level, not taking into account the sequential nature\nof a streaming scenario. Indeed, these sentence-level latency measures are not\nwell suited for continuous stream translation resulting in figures that are not\ncoherent with the simultaneous translation policy of the system being assessed.\nThis work proposes a stream-level adaptation of the current latency measures\nbased on a re-segmentation approach applied to the output translation, that is\nsuccessfully evaluated on streaming conditions for a reference IWSLT task.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:16:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Iranzo-S\u00e1nchez", "Javier", ""], ["Civera", "Jorge", ""], ["Juan", "Alfons", ""]]}, {"id": "2104.08821", "submitter": "Tianyu Gao", "authors": "Tianyu Gao, Xingcheng Yao, Danqi Chen", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We hypothesize that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we draw inspiration from\nthe recent success of learning sentence embeddings from natural language\ninference (NLI) datasets and incorporate annotated pairs from NLI datasets into\ncontrastive learning by using \"entailment\" pairs as positives and\n\"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard\nsemantic textual similarity (STS) tasks, and our unsupervised and supervised\nmodels using BERT-base achieve an average of 74.5% and 81.6% Spearman's\ncorrelation respectively, a 7.9 and 4.6 points improvement compared to previous\nbest results. We also show that contrastive learning theoretically regularizes\npre-trained embeddings' anisotropic space to be more uniform, and it better\naligns positive pairs when supervised signals are available.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:27:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gao", "Tianyu", ""], ["Yao", "Xingcheng", ""], ["Chen", "Danqi", ""]]}, {"id": "2104.08825", "submitter": "Kaj Bostrom", "authors": "Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett", "title": "Flexible Operations for Natural Language Deduction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interpretable system for complex, open-domain reasoning needs an\ninterpretable meaning representation. Natural language is an excellent\ncandidate -- it is both extremely expressive and easy for humans to understand.\nHowever, manipulating natural language statements in logically consistent ways\nis hard. Models have to be precise, yet robust enough to handle variation in\nhow information is expressed. In this paper, we describe ParaPattern, a method\nfor building models to generate logical transformations of diverse natural\nlanguage inputs without direct human supervision. We use a BART-based model\n(Lewis et al., 2020) to generate the result of applying a particular logical\noperation to one or more premise statements. Crucially, we have a largely\nautomated pipeline for scraping and constructing suitable training examples\nfrom Wikipedia, which are then paraphrased to give our models the ability to\nhandle lexical variation. We evaluate our models using targeted contrast sets\nas well as out-of-domain sentence compositions from the QASC dataset (Khot et\nal., 2020). Our results demonstrate that our operation models are both accurate\nand flexible.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:36:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bostrom", "Kaj", ""], ["Zhao", "Xinyu", ""], ["Chaudhuri", "Swarat", ""], ["Durrett", "Greg", ""]]}, {"id": "2104.08826", "submitter": "Kang Min Yoo", "authors": "Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, Woomyeong Park", "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "comments": "11 pages, 7 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale language models such as GPT-3 are excellent few-shot learners,\nallowing them to be controlled via natural text prompts. Recent studies report\nthat prompt-based direct classification eliminates the need for fine-tuning but\nlacks data and inference scalability. This paper proposes a novel data\naugmentation technique that leverages large-scale language models to generate\nrealistic text samples from a mixture of real samples. We also propose\nutilizing soft-labels predicted by the language models, effectively distilling\nknowledge from the large-scale language models and creating textual\nperturbations simultaneously. We perform data augmentation experiments on\ndiverse classification tasks and show that our method hugely outperforms\nexisting text augmentation methods. Ablation studies and a qualitative analysis\nprovide more insights into our approach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:39:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yoo", "Kang Min", ""], ["Park", "Dongju", ""], ["Kang", "Jaewook", ""], ["Lee", "Sang-Woo", ""], ["Park", "Woomyeong", ""]]}, {"id": "2104.08829", "submitter": "Valentin Hofmann", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\\\"utze", "title": "Modeling Ideological Agenda Setting and Framing in Polarized Online\n  Groups with Graph Neural Networks and Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing polarization of online political discourse calls for\ncomputational tools that are able to automatically detect and monitor\nideological divides in social media. Here, we introduce a minimally supervised\nmethod that directly leverages the network structure of online discussion\nforums, specifically Reddit, to detect polarized concepts. We model\npolarization along the dimensions of agenda setting and framing, drawing upon\ninsights from moral psychology. The architecture we propose combines graph\nneural networks with structured sparsity and results in representations for\nconcepts and subreddits that capture phenomena such as ideological\nradicalization and subreddit hijacking. We also create a new dataset of\npolitical discourse covering 12 years and more than 600 online groups with\ndifferent ideologies.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:48:25 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 17:02:48 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Hofmann", "Valentin", ""], ["Pierrehumbert", "Janet B.", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2104.08835", "submitter": "Qinyuan Ye", "authors": "Qinyuan Ye, Bill Yuchen Lin, Xiang Ren", "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in\n  NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humans can learn a new language task more efficiently than machines,\nconceivably by leveraging their prior experience and knowledge in learning\nother tasks. In this paper, we explore whether such cross-task generalization\nability can be acquired, and further applied to build better few-shot learners\nacross diverse NLP tasks. We introduce CrossFit, a task setup for studying\ncross-task few-shot learning ability, which standardizes seen/unseen task\nsplits, data access during different learning stages, and the evaluation\nprotocols. In addition, we present NLP Few-shot Gym, a repository of 160\nfew-shot NLP tasks, covering diverse task categories and applications, and\nconverted to a unified text-to-text format. Our empirical analysis reveals that\nthe few-shot learning ability on unseen tasks can be improved via an upstream\nlearning stage using a set of seen tasks. Additionally, the advantage lasts\ninto medium-resource scenarios when thousands of training examples are\navailable. We also observe that selection of upstream learning tasks can\nsignificantly influence few-shot performance on unseen tasks, asking further\nanalysis on task similarity and transferability.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:14:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Qinyuan", ""], ["Lin", "Bill Yuchen", ""], ["Ren", "Xiang", ""]]}, {"id": "2104.08836", "submitter": "Lei Cui", "authors": "Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei\n  Florencio, Cha Zhang, Furu Wei", "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n  Document Understanding", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUN, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUN dataset. The pre-trained LayoutXLM\nmodel and the XFUN dataset will be publicly available at\nhttps://aka.ms/layoutxlm.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:16:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xu", "Yiheng", ""], ["Lv", "Tengchao", ""], ["Cui", "Lei", ""], ["Wang", "Guoxin", ""], ["Lu", "Yijuan", ""], ["Florencio", "Dinei", ""], ["Zhang", "Cha", ""], ["Wei", "Furu", ""]]}, {"id": "2104.08840", "submitter": "Qinyuan Ye", "authors": "Qinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin Bolte, Hao Ma,\n  Wen-tau Yih, Xiang Ren, Madian Khabsa", "title": "On the Influence of Masking Policies in Intermediate Pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current NLP models are predominantly trained through a pretrain-then-finetune\npipeline, where models are first pretrained on a large text corpus with a\nmasked-language-modelling (MLM) objective, then finetuned on the downstream\ntask. Prior work has shown that inserting an intermediate pre-training phase,\nwith heuristic MLM objectives that resemble downstream tasks, can significantly\nimprove final performance. However, it is still unclear (1) in what cases such\nintermediate pre-training is helpful, (2) whether hand-crafted heuristic\nobjectives are optimal for a given task, and (3) whether a MLM policy designed\nfor one task is generalizable beyond that task. In this paper, we perform a\nlarge-scale empirical study to investigate the effect of various MLM policies\nin intermediate pre-training. Crucially, we introduce methods to automate\ndiscovery of optimal MLM policies, by learning a masking model through either\ndirect supervision or meta-learning on the downstream task. We investigate the\neffects of using heuristic, directly supervised, and meta-learned MLM policies\nfor intermediate pretraining, on eight selected tasks across three categories\n(closed-book QA, knowledge-intensive language tasks, and abstractive\nsummarization). Most notably, we show that learned masking policies outperform\nthe heuristic of masking named entities on TriviaQA, and masking policies\nlearned on one task can positively transfer to other tasks in certain cases.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:32:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Qinyuan", ""], ["Li", "Belinda Z.", ""], ["Wang", "Sinong", ""], ["Bolte", "Benjamin", ""], ["Ma", "Hao", ""], ["Yih", "Wen-tau", ""], ["Ren", "Xiang", ""], ["Khabsa", "Madian", ""]]}, {"id": "2104.08857", "submitter": "Yu-Ping Ruan", "authors": "Yu-Ping Ruan, and Zhen-Hua Ling", "title": "Emotion-Regularized Conditional Variational Autoencoder for Emotional\n  Response Generation", "comments": "Accepted by IEEE Transactions on Affective Computing", "journal-ref": null, "doi": "10.1109/TAFFC.2021.3073809", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an emotion-regularized conditional variational\nautoencoder (Emo-CVAE) model for generating emotional conversation responses.\nIn conventional CVAE-based emotional response generation, emotion labels are\nsimply used as additional conditions in prior, posterior and decoder networks.\nConsidering that emotion styles are naturally entangled with semantic contents\nin the language space, the Emo-CVAE model utilizes emotion labels to regularize\nthe CVAE latent space by introducing an extra emotion prediction network. In\nthe training stage, the estimated latent variables are required to predict the\nemotion labels and token sequences of the input responses simultaneously.\nExperimental results show that our Emo-CVAE model can learn a more informative\nand structured latent space than a conventional CVAE model and output responses\nwith better content and emotion performance than baseline CVAE and\nsequence-to-sequence (Seq2Seq) models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 13:53:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ruan", "Yu-Ping", ""], ["Ling", "Zhen-Hua", ""]]}, {"id": "2104.08874", "submitter": "Federico Bianchi", "authors": "Federico Bianchi and Ciro Greco and Jacopo Tagliabue", "title": "Language in a (Search) Box: Grounding Language Learning in Real-World\n  Human-Machine Interaction", "comments": "Published as a conference paper at NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate grounded language learning through real-world data, by\nmodelling a teacher-learner dynamics through the natural interactions occurring\nbetween users and search engines; in particular, we explore the emergence of\nsemantic generalization from unsupervised dense representations outside of\nsynthetic environments. A grounding domain, a denotation function and a\ncomposition function are learned from user data only. We show how the resulting\nsemantics for noun phrases exhibits compositional properties while being fully\nlearnable without any explicit labelling. We benchmark our grounded semantics\non compositionality and zero-shot inference tasks, and we show that it provides\nbetter results and better generalizations than SOTA non-grounded models, such\nas word2vec and BERT.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 15:03:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bianchi", "Federico", ""], ["Greco", "Ciro", ""], ["Tagliabue", "Jacopo", ""]]}, {"id": "2104.08922", "submitter": "Ken Litkowski", "authors": "Ken Litkowski and Orin Hargraves", "title": "The Preposition Project", "comments": "Proceedings of the Second ACL-SIGSEM Workshop on The Linguistic\n  Dimensions of Prepositions and their Use in Computational Linguistics\n  Formalisms and Applications, April 19-21, 2005, Colchester, England:\n  University of Essex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prepositions are an important vehicle for indicating semantic roles. Their\nmeanings are difficult to analyze and they are often discarded in processing\ntext. The Preposition Project is designed to provide a comprehensive database\nof preposition senses suitable for use in natural language processing\napplications. In the project, prepositions in the FrameNet corpus are\ndisambiguated using a sense inventory from a current dictionary, guided by a\ncomprehensive treatment of preposition meaning. The methodology provides a\nframework for identifying and characterizing semantic roles, a gold standard\ncorpus of instances for further analysis, and an account of semantic role\nalternation patterns. By adhering to this methodology, it is hoped that a\ncomprehensive and improved characterization of preposition behavior (semantic\nrole identification, and syntactic and semantic properties of the preposition\ncomplement and attachment point) will be developed. The databases generated in\nthe project are publicly available for further use by researchers and\napplication developers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 17:45:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Litkowski", "Ken", ""], ["Hargraves", "Orin", ""]]}, {"id": "2104.08928", "submitter": "Kan Xu", "authors": "Kan Xu, Xuanyi Zhao, Hamsa Bastani, Osbert Bastani", "title": "Group-Sparse Matrix Factorization for Transfer Learning of Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse regression has recently been applied to enable transfer learning from\nvery limited data. We study an extension of this approach to unsupervised\nlearning -- in particular, learning word embeddings from unstructured text\ncorpora using low-rank matrix factorization. Intuitively, when transferring\nword embeddings to a new domain, we expect that the embeddings change for only\na small number of words -- e.g., the ones with novel meanings in that domain.\nWe propose a novel group-sparse penalty that exploits this sparsity to perform\ntransfer learning when there is very little text data available in the target\ndomain -- e.g., a single article of text. We prove generalization bounds for\nour algorithm. Furthermore, we empirically evaluate its effectiveness, both in\nterms of prediction accuracy in downstream tasks as well as the\ninterpretability of the results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 18:19:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xu", "Kan", ""], ["Zhao", "Xuanyi", ""], ["Bastani", "Hamsa", ""], ["Bastani", "Osbert", ""]]}, {"id": "2104.08936", "submitter": "Vivek Khetan", "authors": "Vivek Khetan, Annervaz K M, Erin Wetherley, Elena Eneva, Shubhashis\n  Sengupta, and Andrew E. Fano", "title": "Knowledge Graph Anchored Information-Extraction for Domain-Specific\n  Insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing quantity and complexity of data pose challenges for humans to\nconsume information and respond in a timely manner. For businesses in domains\nwith rapidly changing rules and regulations, failure to identify changes can be\ncostly. In contrast to expert analysis or the development of domain-specific\nontology and taxonomies, we use a task-based approach for fulfilling specific\ninformation needs within a new domain. Specifically, we propose to extract\ntask-based information from incoming instance data. A pipeline constructed of\nstate of the art NLP technologies, including a bi-LSTM-CRF model for entity\nextraction, attention-based deep Semantic Role Labeling, and an automated\nverb-based relationship extractor, is used to automatically extract an instance\nlevel semantic structure. Each instance is then combined with a larger,\ndomain-specific knowledge graph to produce new and timely insights. Preliminary\nresults, validated manually, show the methodology to be effective for\nextracting specific information to complete end use-cases.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:28:10 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 02:44:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Khetan", "Vivek", ""], ["M", "Annervaz K", ""], ["Wetherley", "Erin", ""], ["Eneva", "Elena", ""], ["Sengupta", "Shubhashis", ""], ["Fano", "Andrew E.", ""]]}, {"id": "2104.08942", "submitter": "Neel Kanwal", "authors": "Neel Kanwal, Giuseppe Rizzo", "title": "Attention-based Clinical Note Summarization", "comments": "9 Pages, 6 Figure, 2 Tables, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend of deploying digital systems in numerous industries has induced a\nhike in recording digital information. The health sector has observed a large\nadoption of digital devices and systems generating large volumes of personal\nmedical health records. Electronic health records contain valuable information\nfor retrospective and prospective analysis that is often not entirely exploited\nbecause of the dense information storage. The crude purpose of condensing\nhealth records is to select the information that holds most characteristics of\nthe original documents based on reported disease. These summaries may boost\ndiagnosis and extend a doctor's interaction time with the patient during a high\nworkload situation like the COVID-19 pandemic. In this paper, we propose a\nmulti-head attention-based mechanism to perform extractive summarization of\nmeaningful phrases in clinical notes. This method finds major sentences for a\nsummary by correlating tokens, segments and positional embeddings. The model\noutputs attention scores that are statistically transformed to extract key\nphrases and can be used for a projection on the heat-mapping tool for visual\nand human use.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:40:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kanwal", "Neel", ""], ["Rizzo", "Giuseppe", ""]]}, {"id": "2104.08943", "submitter": "Thuy Vu", "authors": "Vivek Krishnamurthy, Thuy Vu, Alessandro Moschitti", "title": "Reference-based Weak Supervision for Answer Sentence Selection using Web\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer sentence selection (AS2) modeling requires annotated data, i.e.,\nhand-labeled question-answer pairs. We present a strategy to collect weakly\nsupervised answers for a question based on its reference to improve AS2\nmodeling. Specifically, we introduce Reference-based Weak Supervision (RWS), a\nfully automatic large-scale data pipeline that harvests high-quality\nweakly-supervised answers from abundant Web data requiring only a\nquestion-reference pair as input. We study the efficacy and robustness of RWS\nin the setting of TANDA, a recent state-of-the-art fine-tuning approach\nspecialized for AS2. Our experiments indicate that the produced data\nconsistently bolsters TANDA. We achieve the state of the art in terms of P@1,\n90.1%, and MAP, 92.9%, on WikiQA.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:41:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Krishnamurthy", "Vivek", ""], ["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2104.08962", "submitter": "Rakesh Gosangi", "authors": "Rakesh Gosangi, Ravneet Arora, Mohsen Gheisarieha, Debanjan Mahata,\n  Haimin Zhang", "title": "On the Use of Context for Predicting Citation Worthiness of Sentences in\n  Scholarly Articles", "comments": "To be published in the proceedings of NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we study the importance of context in predicting the citation\nworthiness of sentences in scholarly articles. We formulate this problem as a\nsequence labeling task solved using a hierarchical BiLSTM model. We contribute\na new benchmark dataset containing over two million sentences and their\ncorresponding labels. We preserve the sentence order in this dataset and\nperform document-level train/test splits, which importantly allows\nincorporating contextual information in the modeling process. We evaluate the\nproposed approach on three benchmark datasets. Our results quantify the\nbenefits of using context and contextual embeddings for citation worthiness.\nLastly, through error analysis, we provide insights into cases where context\nplays an essential role in predicting citation worthiness.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:47:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gosangi", "Rakesh", ""], ["Arora", "Ravneet", ""], ["Gheisarieha", "Mohsen", ""], ["Mahata", "Debanjan", ""], ["Zhang", "Haimin", ""]]}, {"id": "2104.08964", "submitter": "Luciana Benotti", "authors": "Luciana Benotti and Patrick Blackburn", "title": "A recipe for annotating grounded clarifications", "comments": "Accepted for publication at the 2021 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In order to interpret the communicative intents of an utterance, it needs to\nbe grounded in something that is outside of language; that is, grounded in\nworld modalities. In this paper, we argue that dialogue clarification\nmechanisms make explicit the process of interpreting the communicative intents\nof the speaker's utterances by grounding them in the various modalities in\nwhich the dialogue is situated. This paper frames dialogue clarification\nmechanisms as an understudied research problem and a key missing piece in the\ngiant jigsaw puzzle of natural language understanding. We discuss both the\ntheoretical background and practical challenges posed by this problem and\npropose a recipe for obtaining grounding annotations. We conclude by\nhighlighting ethical issues that need to be addressed in future work.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:47:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Benotti", "Luciana", ""], ["Blackburn", "Patrick", ""]]}, {"id": "2104.08967", "submitter": "Badrinath Jayakumar", "authors": "Minhua Chen, Badrinath Jayakumar, Padmasundari Gopalakrishnan, Qiming\n  Huang, Michael Johnston, and Patrick Haffner", "title": "Deep Clustering with Measure Propagation", "comments": "This work was presented as a poster in 14th Annual Machine Learning\n  Symposium in The New York Academy of Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models have improved state-of-the-art for both supervised and\nunsupervised learning. For example, deep embedded clustering (DEC) has greatly\nimproved the unsupervised clustering performance, by using stacked autoencoders\nfor representation learning. However, one weakness of deep modeling is that the\nlocal neighborhood structure in the original space is not necessarily preserved\nin the latent space. To preserve local geometry, various methods have been\nproposed in the supervised and semi-supervised learning literature (e.g.,\nspectral clustering and label propagation) using graph Laplacian\nregularization. In this paper, we combine the strength of deep representation\nlearning with measure propagation (MP), a KL-divergence based graph\nregularization method originally used in the semi-supervised scenario. The main\nassumption of MP is that if two data points are close in the original space,\nthey are likely to belong to the same class, measured by KL-divergence of class\nmembership distribution. By taking the same assumption in the unsupervised\nlearning scenario, we propose our Deep Embedded Clustering Aided by Measure\nPropagation (DECAMP) model. We evaluate DECAMP on short text clustering tasks.\nOn three public datasets, DECAMP performs competitively with other\nstate-of-the-art baselines, including baselines using additional data to\ngenerate word embeddings used in the clustering process. As an example, on the\nStackoverflow dataset, DECAMP achieved a clustering accuracy of 79%, which is\nabout 5% higher than all existing baselines. These empirical results suggest\nthat DECAMP is a very effective method for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 22:02:43 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 02:42:42 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 14:32:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chen", "Minhua", ""], ["Jayakumar", "Badrinath", ""], ["Gopalakrishnan", "Padmasundari", ""], ["Huang", "Qiming", ""], ["Johnston", "Michael", ""], ["Haffner", "Patrick", ""]]}, {"id": "2104.09006", "submitter": "Gati L. Martin", "authors": "Gati L. Martin, Medard E. Mswahili, Young-Seob Jeong", "title": "Sentiment Classification in Swahili Language Using Multilingual BERT", "comments": "Accepted to African NLP Workshop, EACL 2021 (non-archival)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The evolution of the Internet has increased the amount of information that is\nexpressed by people on different platforms. This information can be product\nreviews, discussions on forums, or social media platforms. Accessibility of\nthese opinions and peoples feelings open the door to opinion mining and\nsentiment analysis. As language and speech technologies become more advanced,\nmany languages have been used and the best models have been obtained. However,\ndue to linguistic diversity and lack of datasets, African languages have been\nleft behind. In this study, by using the current state-of-the-art model,\nmultilingual BERT, we perform sentiment classification on Swahili datasets. The\ndata was created by extracting and annotating 8.2k reviews and comments on\ndifferent social media platforms and the ISEAR emotion dataset. The data were\nclassified as either positive or negative. The model was fine-tuned and achieve\nthe best accuracy of 87.59%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:47:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Martin", "Gati L.", ""], ["Mswahili", "Medard E.", ""], ["Jeong", "Young-Seob", ""]]}, {"id": "2104.09011", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata", "title": "Few-shot Learning for Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been successfully used for analyzing text documents.\nHowever, with existing topic models, many documents are required for training.\nIn this paper, we propose a neural network-based few-shot learning method that\ncan learn a topic model from just a few documents. The neural networks in our\nmodel take a small number of documents as inputs, and output topic model\npriors. The proposed method trains the neural networks such that the expected\ntest likelihood is improved when topic model parameters are estimated by\nmaximizing the posterior probability using the priors based on the EM\nalgorithm. Since each step in the EM algorithm is differentiable, the proposed\nmethod can backpropagate the loss through the EM algorithm to train the neural\nnetworks. The expected test likelihood is maximized by a stochastic gradient\ndescent method using a set of multiple text corpora with an episodic training\nframework. In our experiments, we demonstrate that the proposed method achieves\nbetter perplexity than existing methods using three real-world text document\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:56:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Iwata", "Tomoharu", ""]]}, {"id": "2104.09033", "submitter": "Jonathan Dunn", "authors": "Jonathan Dunn and Andrea Nini", "title": "Production vs Perception: The Role of Individuality in Usage-Based\n  Grammar Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper asks whether a distinction between production-based and\nperception-based grammar induction influences either (i) the growth curve of\ngrammars and lexicons or (ii) the similarity between representations learned\nfrom independent sub-sets of a corpus. A production-based model is trained on\nthe usage of a single individual, thus simulating the grammatical knowledge of\na single speaker. A perception-based model is trained on an aggregation of many\nindividuals, thus simulating grammatical generalizations learned from exposure\nto many different speakers. To ensure robustness, the experiments are\nreplicated across two registers of written English, with four additional\nregisters reserved as a control. A set of three computational experiments shows\nthat production-based grammars are significantly different from\nperception-based grammars across all conditions, with a steeper growth curve\nthat can be explained by substantial inter-individual grammatical differences.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 03:38:47 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dunn", "Jonathan", ""], ["Nini", "Andrea", ""]]}, {"id": "2104.09040", "submitter": "Soroush Vosoughi Dr", "authors": "Aadil Islam, Weicheng Ma, Soroush Vosoughi", "title": "BigGreen at SemEval-2021 Task 1: Lexical Complexity Prediction with\n  Assembly Models", "comments": "In Proceedings of the 15th International Workshop on Semantic\n  Evaluation (SemEval-2021). Colocated with ACL-IJCNLP 2021", "journal-ref": null, "doi": "10.18653/v1/2021.semeval-1.86", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes a system submitted by team BigGreen to LCP 2021 for\npredicting the lexical complexity of English words in a given context. We\nassemble a feature engineering-based model with a deep neural network model\nfounded on BERT. While BERT itself performs competitively, our feature\nengineering-based model helps in extreme cases, eg. separating instances of\neasy and neutral difficulty. Our handcrafted features comprise a breadth of\nlexical, semantic, syntactic, and novel phonological measures. Visualizations\nof BERT attention maps offer insight into potential features that Transformers\nmodels may learn when fine-tuned for lexical complexity prediction. Our\nensembled predictions score reasonably well for the single word subtask, and we\ndemonstrate how they can be harnessed to perform well on the multi word\nexpression subtask too.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:05:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Islam", "Aadil", ""], ["Ma", "Weicheng", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2104.09047", "submitter": "Kashif Munir", "authors": "Kashif Munir, Hai Zhao, Zuchao Li", "title": "Neural Unsupervised Semantic Role Labeling", "comments": "This is an initial version of our recently accepted ACM TALLIP paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of semantic role labeling (SRL) is dedicated to finding the\npredicate-argument structure. Previous works on SRL are mostly supervised and\ndo not consider the difficulty in labeling each example which can be very\nexpensive and time-consuming. In this paper, we present the first neural\nunsupervised model for SRL. To decompose the task as two argument related\nsubtasks, identification and clustering, we propose a pipeline that\ncorrespondingly consists of two neural modules. First, we train a neural model\non two syntax-aware statistically developed rules. The neural model gets the\nrelevance signal for each token in a sentence, to feed into a BiLSTM, and then\nan adversarial layer for noise-adding and classifying simultaneously, thus\nenabling the model to learn the semantic structure of a sentence. Then we\npropose another neural model for argument role clustering, which is done\nthrough clustering the learned argument embeddings biased towards their\ndependency relations. Experiments on CoNLL-2009 English dataset demonstrate\nthat our model outperforms previous state-of-the-art baseline in terms of\nnon-neural models for argument identification and classification.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:50:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Munir", "Kashif", ""], ["Zhao", "Hai", ""], ["Li", "Zuchao", ""]]}, {"id": "2104.09061", "submitter": "Sihao Chen", "authors": "Sihao Chen and Fan Zhang and Kazoo Sone and Dan Roth", "title": "Improving Faithfulness in Abstractive Summarization with Contrast\n  Candidate Generation and Selection", "comments": "NAACL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite significant progress in neural abstractive summarization, recent\nstudies have shown that the current models are prone to generating summaries\nthat are unfaithful to the original context. To address the issue, we study\ncontrast candidate generation and selection as a model-agnostic post-processing\ntechnique to correct the extrinsic hallucinations (i.e. information not present\nin the source text) in unfaithful summaries. We learn a discriminative\ncorrection model by generating alternative candidate summaries where named\nentities and quantities in the generated summary are replaced with ones with\ncompatible semantic types from the source document. This model is then used to\nselect the best candidate as the final output summary. Our experiments and\nanalysis across a number of neural summarization systems show that our proposed\nmethod is effective in identifying and correcting extrinsic hallucinations. We\nanalyze the typical hallucination phenomenon by different types of neural\nsummarization systems, in hope to provide insights for future work on the\ndirection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 05:39:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Sihao", ""], ["Zhang", "Fan", ""], ["Sone", "Kazoo", ""], ["Roth", "Dan", ""]]}, {"id": "2104.09063", "submitter": "Pamela Fleischmann", "authors": "Pamela Fleischmann, Sebastian Bernhard Germann, and Dirk Nowotka", "title": "Scattered Factor Universality -- The Power of the Remainder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scattered factor (circular) universality was firstly introduced by Barker et\nal. in 2020. A word $w$ is called $k$-universal for some natural number $k$, if\nevery word of length $k$ of $w$'s alphabet occurs as a scattered factor in $w$;\nit is called circular $k$-universal if a conjugate of $w$ is $k$-universal.\nHere, a word $u=u_1\\cdots u_n$ is called a scattered factor of $w$ if $u$ is\nobtained from $w$ by deleting parts of $w$, i.e. there exists (possibly empty)\nwords $v_1,\\dots,v_{n+1}$ with $w=v_1u_1v_2\\cdots v_nu_nv_{n+1}$. In this work,\nwe prove two problems, left open in the aforementioned paper, namely a\ngeneralisation of one of their main theorems to arbitrary alphabets and a\nslight modification of another theorem such that we characterise the circular\nuniversality by the universality. On the way, we present deep insights into the\nbehaviour of the remainder of the so called arch factorisation by Hebrard when\nrepetitions of words are considered.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 05:45:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fleischmann", "Pamela", ""], ["Germann", "Sebastian Bernhard", ""], ["Nowotka", "Dirk", ""]]}, {"id": "2104.09066", "submitter": "Adeep Hande", "authors": "Karthik Puranik, Adeep Hande, Ruba Priyadharshini, Sajeetha\n  Thavareesan, Bharathi Raja Chakravarthi", "title": "IIITT@LT-EDI-EACL2021-Hope Speech Detection: There is always Hope in\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a world filled with serious challenges like climate change, religious and\npolitical conflicts, global pandemics, terrorism, and racial discrimination, an\ninternet full of hate speech, abusive and offensive content is the last thing\nwe desire for. In this paper, we work to identify and promote positive and\nsupportive content on these platforms. We work with several transformer-based\nmodels to classify social media comments as hope speech or not-hope speech in\nEnglish, Malayalam and Tamil languages. This paper portrays our work for the\nShared Task on Hope Speech Detection for Equality, Diversity, and Inclusion at\nLT-EDI 2021- EACL 2021.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:19:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Puranik", "Karthik", ""], ["Hande", "Adeep", ""], ["Priyadharshini", "Ruba", ""], ["Thavareesan", "Sajeetha", ""], ["Chakravarthi", "Bharathi Raja", ""]]}, {"id": "2104.09081", "submitter": "Adeep Hande", "authors": "Siddhanth U Hegde, Adeep Hande, Ruba Priyadharshini, Sajeetha\n  Thavareesan, Bharathi Raja Chakravarthi", "title": "UVCE-IIITT@DravidianLangTech-EACL2021: Tamil Troll Meme Classification:\n  You need to Pay more Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tamil is a Dravidian language that is commonly used and spoken in the\nsouthern part of Asia. In the era of social media, memes have been a fun moment\nin the day-to-day life of people. Here, we try to analyze the true meaning of\nTamil memes by categorizing them as troll and non-troll. We propose an\ningenious model comprising of a transformer-transformer architecture that tries\nto attain state-of-the-art by using attention as its main component. The\ndataset consists of troll and non-troll images with their captions as text. The\ntask is a binary classification task. The objective of the model is to pay more\nattention to the extracted features and to ignore the noise in both images and\ntext.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:57:43 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hegde", "Siddhanth U", ""], ["Hande", "Adeep", ""], ["Priyadharshini", "Ruba", ""], ["Thavareesan", "Sajeetha", ""], ["Chakravarthi", "Bharathi Raja", ""]]}, {"id": "2104.09088", "submitter": "Sanchit Agarwal", "authors": "Anish Acharya, Suranjit Adhikari, Sanchit Agarwal, Vincent Auvray,\n  Nehal Belgamwar, Arijit Biswas, Shubhra Chandra, Tagyoung Chung, Maryam\n  Fazel-Zarandi, Raefer Gabriel, Shuyang Gao, Rahul Goel, Dilek Hakkani-Tur,\n  Jan Jezabek, Abhay Jha, Jiun-Yu Kao, Prakash Krishnan, Peter Ku, Anuj Goyal,\n  Chien-Wei Lin, Qing Liu, Arindam Mandal, Angeliki Metallinou, Vishal Naik, Yi\n  Pan, Shachi Paul, Vittorio Perera, Abhishek Sethi, Minmin Shen, Nikko Strom,\n  Eddie Wang", "title": "Alexa Conversations: An Extensible Data-driven Approach for Building\n  Task-oriented Dialogue Systems", "comments": null, "journal-ref": "NAACL 2021 System Demonstrations Track", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional goal-oriented dialogue systems rely on various components such as\nnatural language understanding, dialogue state tracking, policy learning and\nresponse generation. Training each component requires annotations which are\nhard to obtain for every new domain, limiting scalability of such systems.\nSimilarly, rule-based dialogue systems require extensive writing and\nmaintenance of rules and do not scale either. End-to-End dialogue systems, on\nthe other hand, do not require module-specific annotations but need a large\namount of data for training. To overcome these problems, in this demo, we\npresent Alexa Conversations, a new approach for building goal-oriented dialogue\nsystems that is scalable, extensible as well as data efficient. The components\nof this system are trained in a data-driven manner, but instead of collecting\nannotated conversations for training, we generate them using a novel dialogue\nsimulator based on a few seed dialogues and specifications of APIs and entities\nprovided by the developer. Our approach provides out-of-the-box support for\nnatural conversational phenomena like entity sharing across turns or users\nchanging their mind during conversation without requiring developers to provide\nany such dialogue flows. We exemplify our approach using a simple pizza\nordering task and showcase its value in reducing the developer burden for\ncreating a robust experience. Finally, we evaluate our system using a typical\nmovie ticket booking task and show that the dialogue simulator is an essential\ncomponent of the system that leads to over $50\\%$ improvement in turn-level\naction signature prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:09:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Acharya", "Anish", ""], ["Adhikari", "Suranjit", ""], ["Agarwal", "Sanchit", ""], ["Auvray", "Vincent", ""], ["Belgamwar", "Nehal", ""], ["Biswas", "Arijit", ""], ["Chandra", "Shubhra", ""], ["Chung", "Tagyoung", ""], ["Fazel-Zarandi", "Maryam", ""], ["Gabriel", "Raefer", ""], ["Gao", "Shuyang", ""], ["Goel", "Rahul", ""], ["Hakkani-Tur", "Dilek", ""], ["Jezabek", "Jan", ""], ["Jha", "Abhay", ""], ["Kao", "Jiun-Yu", ""], ["Krishnan", "Prakash", ""], ["Ku", "Peter", ""], ["Goyal", "Anuj", ""], ["Lin", "Chien-Wei", ""], ["Liu", "Qing", ""], ["Mandal", "Arindam", ""], ["Metallinou", "Angeliki", ""], ["Naik", "Vishal", ""], ["Pan", "Yi", ""], ["Paul", "Shachi", ""], ["Perera", "Vittorio", ""], ["Sethi", "Abhishek", ""], ["Shen", "Minmin", ""], ["Strom", "Nikko", ""], ["Wang", "Eddie", ""]]}, {"id": "2104.09106", "submitter": "Wei Zhou", "authors": "Wei Zhou, Mohammad Zeineldeen, Zuoyun Zheng, Ralf Schl\\\"uter, Hermann\n  Ney", "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition", "comments": "accepted at Interspeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:54:15 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 15:18:43 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhou", "Wei", ""], ["Zeineldeen", "Mohammad", ""], ["Zheng", "Zuoyun", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.09113", "submitter": "Florian Cafiero", "authors": "Florian Cafiero, Paul Guille-Escuret, Jeremy Ward", "title": "No comments: Addressing commentary sections in websites' analyses", "comments": "6th International Conference on Computational Social Science,\n  Massachusetts Institute of Technology (MIT), Jul 2020, Cambridge, MA, United\n  States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing or extracting the commentary sections from a series of websites is a\ntedious task, as no standard way to code them is widely adopted. This operation\nis thus very rarely performed. In this paper, we show that these commentary\nsections can induce significant biases in the analyses, especially in the case\nof controversial Highlights $\\bullet$ Commentary sections can induce biases in\nthe analysis of websites' contents $\\bullet$ Analyzing these sections can be\ninteresting per se. $\\bullet$ We illustrate these points using a corpus of\nanti-vaccine websites. $\\bullet$ We provide guidelines to remove or extract\nthese sections.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:10:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cafiero", "Florian", ""], ["Guille-Escuret", "Paul", ""], ["Ward", "Jeremy", ""]]}, {"id": "2104.09243", "submitter": "Nikola Ljube\\v{s}i\\'c", "authors": "Nikola Ljube\\v{s}i\\'c, Davor Lauc", "title": "BERTi\\'c -- The Transformer Language Model for Bosnian, Croatian,\n  Montenegrin and Serbian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we describe a transformer model pre-trained on 8 billion tokens\nof crawled text from the Croatian, Bosnian, Serbian and Montenegrin web\ndomains. We evaluate the transformer model on the tasks of part-of-speech\ntagging, named-entity-recognition, geo-location prediction and commonsense\ncausal reasoning, showing improvements on all tasks over state-of-the-art\nmodels. For commonsense reasoning evaluation, we introduce COPA-HR -- a\ntranslation of the Choice of Plausible Alternatives (COPA) dataset into\nCroatian. The BERTi\\'c model is made available for free usage and further\ntask-specific fine-tuning through HuggingFace.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 12:30:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ljube\u0161i\u0107", "Nikola", ""], ["Lauc", "Davor", ""]]}, {"id": "2104.09261", "submitter": "Xu Guo", "authors": "Xu Guo, Boyang Li, Han Yu and Chunyan Miao", "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection", "comments": "14 pages, 5 figures, published at NAACL-HLT 2021 conference, see\n  https://www.aclweb.org/anthology/2021.naacl-main.425/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of multiple datasets for sarcasm detection prompts us to apply\ntransfer learning to exploit their commonality. The adversarial neural transfer\n(ANT) framework utilizes multiple loss terms that encourage the source-domain\nand the target-domain feature distributions to be similar while optimizing for\ndomain-specific performance. However, these objectives may be in conflict,\nwhich can lead to optimization difficulties and sometimes diminished transfer.\nWe propose a generalized latent optimization strategy that allows different\nlosses to accommodate each other and improves training dynamics. The proposed\nmethod outperforms transfer learning and meta-learning baselines. In\nparticular, we achieve 10.02% absolute performance gain over the previous state\nof the art on the iSarcasm dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:07:52 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 09:33:22 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Guo", "Xu", ""], ["Li", "Boyang", ""], ["Yu", "Han", ""], ["Miao", "Chunyan", ""]]}, {"id": "2104.09340", "submitter": "Sz Gao", "authors": "Shuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, Xin Xia", "title": "Code Structure Guided Transformer for Source Code Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code summarization aims at generating concise descriptions of given\nprograms' functionalities. While Transformer-based approaches achieve promising\nperformance, they do not explicitly incorporate the code structure information\nwhich is important for capturing code semantics. Besides, without explicit\nconstraints, multi-head attentions in Transformer may suffer from attention\ncollapse, leading to poor code representations for summarization. Effectively\nintegrating the code structure information into Transformer is under-explored\nin this task domain. In this paper, we propose a novel approach named SG-Trans\nto incorporate code structural properties into Transformer. Specifically, to\ncapture the hierarchical characteristics of code, we inject the local symbolic\ninformation (e.g., code tokens) and global syntactic structure (e.g., data\nflow) into the self-attention module as inductive bias. Extensive evaluation\nshows the superior performance of SG-Trans over the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:26:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gao", "Shuzheng", ""], ["Gao", "Cuiyun", ""], ["He", "Yulan", ""], ["Zeng", "Jichuan", ""], ["Nie", "Lun Yiu", ""], ["Xia", "Xin", ""]]}, {"id": "2104.09356", "submitter": "Saturnino Luz", "authors": "Saturnino Luz, Fasih Haider, Sofia de la Fuente, Davida Fromm, Brian\n  MacWhinney", "title": "Detecting cognitive decline using speech only: The ADReSSo Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on the success of the ADReSS Challenge at Interspeech 2020, which\nattracted the participation of 34 teams from across the world, the ADReSSo\nChallenge targets three difficult automatic prediction problems of societal and\nmedical relevance, namely: detection of Alzheimer's Dementia, inference of\ncognitive testing scores, and prediction of cognitive decline. This paper\npresents these prediction tasks in detail, describes the datasets used, and\nreports the results of the baseline classification and regression models we\ndeveloped for each task. A combination of acoustic and linguistic features\nextracted directly from audio recordings, without human intervention, yielded a\nbaseline accuracy of 78.87% for the AD classification task, an MMSE prediction\nroot mean squared (RMSE) error of 5.28, and 68.75% accuracy for the cognitive\ndecline prediction task.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 01:09:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Luz", "Saturnino", ""], ["Haider", "Fasih", ""], ["de la Fuente", "Sofia", ""], ["Fromm", "Davida", ""], ["MacWhinney", "Brian", ""]]}, {"id": "2104.09400", "submitter": "Onkar Pandit", "authors": "Onkar Pandit and Yufang Hou", "title": "Probing for Bridging Inference in Transformer Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We probe pre-trained transformer language models for bridging inference. We\nfirst investigate individual attention heads in BERT and observe that attention\nheads at higher layers prominently focus on bridging relations in-comparison\nwith the lower and middle layers, also, few specific attention heads\nconcentrate consistently on bridging. More importantly, we consider language\nmodels as a whole in our second approach where bridging anaphora resolution is\nformulated as a masked token prediction task (Of-Cloze test). Our formulation\nproduces optimistic results without any fine-tuning, which indicates that\npre-trained language models substantially capture bridging inference. Our\nfurther investigation shows that the distance between anaphor-antecedent and\nthe context provided to language models play an important role in the\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:42:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pandit", "Onkar", ""], ["Hou", "Yufang", ""]]}, {"id": "2104.09420", "submitter": "Xiao Liu", "authors": "Xiao Liu, Da Yin, Yansong Feng, Yuting Wu, Dongyan Zhao", "title": "Everything Has a Cause: Leveraging Causal Inference in Legal Text\n  Analysis", "comments": "Accepted by NAACL 2021. Code is available at\n  https://github.com/xxxiaol/GCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Causal inference is the process of capturing cause-effect relationship among\nvariables. Most existing works focus on dealing with structured data, while\nmining causal relationship among factors from unstructured data, like text, has\nbeen less examined, but is of great importance, especially in the legal domain.\n  In this paper, we propose a novel Graph-based Causal Inference (GCI)\nframework, which builds causal graphs from fact descriptions without much human\ninvolvement and enables causal inference to facilitate legal practitioners to\nmake proper decisions. We evaluate the framework on a challenging similar\ncharge disambiguation task. Experimental results show that GCI can capture the\nnuance from fact descriptions among multiple confusing charges and provide\nexplainable discrimination, especially in few-shot settings. We also observe\nthat the causal knowledge contained in GCI can be effectively injected into\npowerful neural networks for better performance and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:13:10 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 07:33:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "Xiao", ""], ["Yin", "Da", ""], ["Feng", "Yansong", ""], ["Wu", "Yuting", ""], ["Zhao", "Dongyan", ""]]}, {"id": "2104.09426", "submitter": "Takaaki Hori", "authors": "Takaaki Hori, Niko Moritz, Chiori Hori, Jonathan Le Roux", "title": "Advanced Long-context End-to-end Speech Recognition Using\n  Context-expanded Transformers", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses end-to-end automatic speech recognition (ASR) for long\naudio recordings such as lecture and conversational speeches. Most end-to-end\nASR models are designed to recognize independent utterances, but contextual\ninformation (e.g., speaker or topic) over multiple utterances is known to be\nuseful for ASR. In our prior work, we proposed a context-expanded Transformer\nthat accepts multiple consecutive utterances at the same time and predicts an\noutput sequence for the last utterance, achieving 5-15% relative error\nreduction from utterance-based baselines in lecture and conversational ASR\nbenchmarks. Although the results have shown remarkable performance gain, there\nis still potential to further improve the model architecture and the decoding\nprocess. In this paper, we extend our prior work by (1) introducing the\nConformer architecture to further improve the accuracy, (2) accelerating the\ndecoding process with a novel activation recycling technique, and (3) enabling\nstreaming decoding with triggered attention. We demonstrate that the extended\nTransformer provides state-of-the-art end-to-end ASR performance, obtaining a\n17.3% character error rate for the HKUST dataset and 12.0%/6.3% word error\nrates for the Switchboard-300 Eval2000 CallHome/Switchboard test sets. The new\ndecoding method reduces decoding time by more than 50% and further enables\nstreaming ASR with limited accuracy degradation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:18:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hori", "Takaaki", ""], ["Moritz", "Niko", ""], ["Hori", "Chiori", ""], ["Roux", "Jonathan Le", ""]]}, {"id": "2104.09489", "submitter": "Gasper Begus", "authors": "Ga\\v{s}per Begu\\v{s} and Alan Zhou", "title": "Interpreting intermediate convolutional layers of CNNs trained on raw\n  speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique to interpret and visualize intermediate\nlayers in CNNs trained on raw speech data in an unsupervised manner. We show\nthat averaging over feature maps after ReLU activation in each convolutional\nlayer yields interpretable time-series data. The proposed technique enables\nacoustic analysis of intermediate convolutional layers. To uncover how\nmeaningful representation in speech gets encoded in intermediate layers of\nCNNs, we manipulate individual latent variables to marginal levels outside of\nthe training range. We train and probe internal representations on two models\n-- a bare WaveGAN architecture and a ciwGAN extension which forces the\nGenerator to output informative data and results in emergence of linguistically\nmeaningful representations. Interpretation and visualization is performed for\nthree basic acoustic properties of speech: periodic vibration (corresponding to\nvowels), aperiodic noise vibration (corresponding to fricatives), and silence\n(corresponding to stops). We also argue that the proposed technique allows\nacoustic analysis of intermediate layers that parallels the acoustic analysis\nof human speech data: we can extract F0, intensity, duration, formants, and\nother acoustic properties from intermediate layers in order to test where and\nhow CNNs encode various types of information. The models are trained on two\nspeech processes with different degrees of complexity: a simple presence of [s]\nand a computationally complex presence of reduplication (copied material).\nObserving the causal effect between interpolation and the resulting changes in\nintermediate layers can reveal how individual variables get transformed into\nspikes in activation in intermediate layers. Using the proposed technique, we\ncan analyze how linguistically meaningful units in speech get encoded in\ndifferent convolutional layers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:52:06 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:43:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Begu\u0161", "Ga\u0161per", ""], ["Zhou", "Alan", ""]]}, {"id": "2104.09500", "submitter": "Arthur Bra\\v{z}inskas", "authors": "Arthur Bra\\v{z}inskas, Mengwen Liu, Ramesh Nallapati, Sujith Ravi,\n  Markus Dreyer", "title": "Transductive Learning for Abstractive News Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models have recently advanced abstractive summarization.\nThese models are further fine-tuned on human-written references before summary\ngeneration in test time. In this work, we propose the first application of\ntransductive learning to summarization. In this paradigm, a model can learn\nfrom the test set's input before inference. To perform transduction, we propose\nto utilize input document summarizing sentences to construct references for\nlearning in test time. These sentences are often compressed and fused to form\nabstractive summaries and provide omitted details and additional context to the\nreader. We show that our approach yields state-of-the-art results on CNN/DM and\nNYT datasets. For instance, we achieve over 1 ROUGE-L point improvement on\nCNN/DM. Further, we show the benefits of transduction from older to more recent\nnews. Finally, through human and automatic evaluation, we show that our\nsummaries become more abstractive and coherent.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 17:33:12 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bra\u017einskas", "Arthur", ""], ["Liu", "Mengwen", ""], ["Nallapati", "Ramesh", ""], ["Ravi", "Sujith", ""], ["Dreyer", "Markus", ""]]}, {"id": "2104.09554", "submitter": "Lior Vassertail", "authors": "Adi Haviv, Lior Vassertail and Omer Levy", "title": "Can Latent Alignments Improve Autoregressive Machine Translation?", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent alignment objectives such as CTC and AXE significantly improve\nnon-autoregressive machine translation models. Can they improve autoregressive\nmodels as well? We explore the possibility of training autoregressive machine\ntranslation models with latent alignment objectives, and observe that, in\npractice, this approach results in degenerate models. We provide a theoretical\nexplanation for these empirical results, and prove that latent alignment\nobjectives are incompatible with teacher forcing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:31:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Haviv", "Adi", ""], ["Vassertail", "Lior", ""], ["Levy", "Omer", ""]]}, {"id": "2104.09557", "submitter": "Dylan Cope", "authors": "Dylan Cope and Nandi Schoots", "title": "Learning to Communicate with Strangers via Channel Randomisation Methods", "comments": null, "journal-ref": "4th Workshop on Emergent Communication at NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce two methods for improving the performance of agents meeting for\nthe first time to accomplish a communicative task. The methods are: (1)\n`message mutation' during the generation of the communication protocol; and (2)\nrandom permutations of the communication channel. These proposals are tested\nusing a simple two-player game involving a `teacher' who generates a\ncommunication protocol and sends a message, and a `student' who interprets the\nmessage. After training multiple agents via self-play we analyse the\nperformance of these agents when they are matched with a stranger, i.e. their\nzero-shot communication performance. We find that both message mutation and\nchannel permutation positively influence performance, and we discuss their\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:42:48 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cope", "Dylan", ""], ["Schoots", "Nandi", ""]]}, {"id": "2104.09570", "submitter": "Shuaicheng Zhang", "authors": "Shuaicheng Zhang, Lifu Huang, Qiang Ning", "title": "Extracting Temporal Event Relation with Syntactic-Guided Temporal Graph\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Extracting temporal relations (e.g., before, after, concurrent) among events\nis crucial to natural language understanding. Previous studies mainly rely on\nneural networks to learn effective features or manual-crafted linguistic\nfeatures for temporal relation extraction, which usually fail when the context\nbetween two events is complex or wide. Inspired by the examination of available\ntemporal relation annotations and human-like cognitive procedures, we propose a\nnew Temporal Graph Transformer network to (1) explicitly find the connection\nbetween two events from a syntactic graph constructed from one or two\ncontinuous sentences, and (2) automatically locate the most indicative temporal\ncues from the path of the two event mentions as well as their surrounding\nconcepts in the syntactic graph with a new temporal-oriented attention\nmechanism. Experiments on MATRES and TB-Dense datasets show that our approach\nsignificantly outperforms previous state-of-the-art methods on both end-to-end\ntemporal relation extraction and temporal relation classification.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:00:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Shuaicheng", ""], ["Huang", "Lifu", ""], ["Ning", "Qiang", ""]]}, {"id": "2104.09574", "submitter": "Pei Zhou", "authors": "Pei Zhou, Pegah Jandaghi, Bill Yuchen Lin, Justin Cho, Jay Pujara,\n  Xiang Ren", "title": "Probing Causal Common Sense in Dialogue Response Generation", "comments": "This article has been withdrawn by the authors. The submitted version\n  was an early draft that has errors in the results which renders the analysis\n  invalid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication is a cooperative effort that requires reaching mutual\nunderstanding among the participants. Humans use commonsense reasoning\nimplicitly to produce natural and logically-coherent responses. As a step\ntowards fluid human-AI communication, we study if response generation (RG)\nmodels can emulate human reasoning process and use common sense to help produce\nbetter-quality responses. We aim to tackle two research questions: how to\nformalize conversational common sense and how to examine RG models capability\nto use common sense? We first propose a task, CEDAR: Causal common sEnse in\nDiAlogue Response generation, that concretizes common sense as textual\nexplanations for what might lead to the response and evaluates RG models\nbehavior by comparing the modeling loss given a valid explanation with an\ninvalid one. Then we introduce a process that automatically generates such\nexplanations and ask humans to verify them. Finally, we design two probing\nsettings for RG models targeting two reasoning capabilities using verified\nexplanations. We find that RG models have a hard time determining the logical\nvalidity of explanations but can identify grammatical naturalness of the\nexplanation easily.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:10:05 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 20:00:17 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhou", "Pei", ""], ["Jandaghi", "Pegah", ""], ["Lin", "Bill Yuchen", ""], ["Cho", "Justin", ""], ["Pujara", "Jay", ""], ["Ren", "Xiang", ""]]}, {"id": "2104.09580", "submitter": "Jialu Li", "authors": "Jialu Li, Hao Tan, Mohit Bansal", "title": "Improving Cross-Modal Alignment in Vision Language Navigation via\n  Syntactic Information", "comments": "NAACL 2021 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision language navigation is the task that requires an agent to navigate\nthrough a 3D environment based on natural language instructions. One key\nchallenge in this task is to ground instructions with the current visual\ninformation that the agent perceives. Most of the existing work employs soft\nattention over individual words to locate the instruction required for the next\naction. However, different words have different functions in a sentence (e.g.,\nmodifiers convey attributes, verbs convey actions). Syntax information like\ndependencies and phrase structures can aid the agent to locate important parts\nof the instruction. Hence, in this paper, we propose a navigation agent that\nutilizes syntax information derived from a dependency tree to enhance alignment\nbetween the instruction and the current visual scenes. Empirically, our agent\noutperforms the baseline model that does not use syntax information on the\nRoom-to-Room dataset, especially in the unseen environment. Besides, our agent\nachieves the new state-of-the-art on Room-Across-Room dataset, which contains\ninstructions in 3 languages (English, Hindi, and Telugu). We also show that our\nagent is better at aligning instructions with the current visual information\nvia qualitative visualizations. Code and models:\nhttps://github.com/jialuli-luka/SyntaxVLN\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:18:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Jialu", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.09585", "submitter": "Giacomo Miolo", "authors": "Giacomo Miolo, Giulio Mantoan, Carlotta Orsenigo", "title": "ELECTRAMed: a new pre-trained language representation model for\n  biomedical NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The overwhelming amount of biomedical scientific texts calls for the\ndevelopment of effective language models able to tackle a wide range of\nbiomedical natural language processing (NLP) tasks. The most recent dominant\napproaches are domain-specific models, initialized with general-domain textual\ndata and then trained on a variety of scientific corpora. However, it has been\nobserved that for specialized domains in which large corpora exist, training a\nmodel from scratch with just in-domain knowledge may yield better results.\nMoreover, the increasing focus on the compute costs for pre-training recently\nled to the design of more efficient architectures, such as ELECTRA. In this\npaper, we propose a pre-trained domain-specific language model, called\nELECTRAMed, suited for the biomedical field. The novel approach inherits the\nlearning framework of the general-domain ELECTRA architecture, as well as its\ncomputational advantages. Experiments performed on benchmark datasets for\nseveral biomedical NLP tasks support the usefulness of ELECTRAMed, which sets\nthe novel state-of-the-art result on the BC5CDR corpus for named entity\nrecognition, and provides the best outcome in 2 over the 5 runs of the 7th\nBioASQ-factoid Challange for the question answering task.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:38:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Miolo", "Giacomo", ""], ["Mantoan", "Giulio", ""], ["Orsenigo", "Carlotta", ""]]}, {"id": "2104.09617", "submitter": "Javier de la Rosa", "authors": "Per E Kummervold, Javier de la Rosa, Freddy Wetjen, Svein Arne\n  Brygfjeld", "title": "Operationalizing a National Digital Library: The Case for a Norwegian\n  Transformer Model", "comments": "Accepted to NoDaLiDa 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we show the process of building a large-scale training set from\ndigital and digitized collections at a national library. The resulting\nBidirectional Encoder Representations from Transformers (BERT)-based language\nmodel for Norwegian outperforms multilingual BERT (mBERT) models in several\ntoken and sequence classification tasks for both Norwegian Bokm{\\aa}l and\nNorwegian Nynorsk. Our model also improves the mBERT performance for other\nlanguages present in the corpus such as English, Swedish, and Danish. For\nlanguages not included in the corpus, the weights degrade moderately while\nkeeping strong multilingual properties. Therefore, we show that building\nhigh-quality models within a memory institution using somewhat noisy optical\ncharacter recognition (OCR) content is feasible, and we hope to pave the way\nfor other memory institutions to follow.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:36:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kummervold", "Per E", ""], ["de la Rosa", "Javier", ""], ["Wetjen", "Freddy", ""], ["Brygfjeld", "Svein Arne", ""]]}, {"id": "2104.09635", "submitter": "Benjamin Newman", "authors": "Benjamin Newman, Kai-Siang Ang, Julia Gong and John Hewitt", "title": "Refining Targeted Syntactic Evaluation of Language Models", "comments": "14 pages, 5 figures, 3 tables. To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted syntactic evaluation of subject-verb number agreement in English\n(TSE) evaluates language models' syntactic knowledge using hand-crafted minimal\npairs of sentences that differ only in the main verb's conjugation. The method\nevaluates whether language models rate each grammatical sentence as more likely\nthan its ungrammatical counterpart. We identify two distinct goals for TSE.\nFirst, evaluating the systematicity of a language model's syntactic knowledge:\ngiven a sentence, can it conjugate arbitrary verbs correctly? Second,\nevaluating a model's likely behavior: given a sentence, does the model\nconcentrate its probability mass on correctly conjugated verbs, even if only on\na subset of the possible verbs? We argue that current implementations of TSE do\nnot directly capture either of these goals, and propose new metrics to capture\neach goal separately. Under our metrics, we find that TSE overestimates\nsystematicity of language models, but that models score up to 40% better on\nverbs that they predict are likely in context.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:55:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Newman", "Benjamin", ""], ["Ang", "Kai-Siang", ""], ["Gong", "Julia", ""], ["Hewitt", "John", ""]]}, {"id": "2104.09644", "submitter": "Yanshan Wang", "authors": "Bhavani Singh Agnikula Kshatriya, Nicolas A Nunez, Manuel Gardea-\n  Resendez, Euijung Ryu, Brandon J Coombes, Sunyang Fu, Mark A Frye, Joanna M\n  Biernacka, Yanshan Wang", "title": "Neural Language Models with Distant Supervision to Identify Major\n  Depressive Disorder from Clinical Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major depressive disorder (MDD) is a prevalent psychiatric disorder that is\nassociated with significant healthcare burden worldwide. Phenotyping of MDD can\nhelp early diagnosis and consequently may have significant advantages in\npatient management. In prior research MDD phenotypes have been extracted from\nstructured Electronic Health Records (EHR) or using Electroencephalographic\n(EEG) data with traditional machine learning models to predict MDD phenotypes.\nHowever, MDD phenotypic information is also documented in free-text EHR data,\nsuch as clinical notes. While clinical notes may provide more accurate\nphenotyping information, natural language processing (NLP) algorithms must be\ndeveloped to abstract such information. Recent advancements in NLP resulted in\nstate-of-the-art neural language models, such as Bidirectional Encoder\nRepresentations for Transformers (BERT) model, which is a transformer-based\nmodel that can be pre-trained from a corpus of unsupervised text data and then\nfine-tuned on specific tasks. However, such neural language models have been\nunderutilized in clinical NLP tasks due to the lack of large training datasets.\nIn the literature, researchers have utilized the distant supervision paradigm\nto train machine learning models on clinical text classification tasks to\nmitigate the issue of lacking annotated training data. It is still unknown\nwhether the paradigm is effective for neural language models. In this paper, we\npropose to leverage the neural language models in a distant supervision\nparadigm to identify MDD phenotypes from clinical notes. The experimental\nresults indicate that our proposed approach is effective in identifying MDD\nphenotypes and that the Bio- Clinical BERT, a specific BERT model for clinical\ndata, achieved the best performance in comparison with conventional machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:11:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kshatriya", "Bhavani Singh Agnikula", ""], ["Nunez", "Nicolas A", ""], ["Resendez", "Manuel Gardea-", ""], ["Ryu", "Euijung", ""], ["Coombes", "Brandon J", ""], ["Fu", "Sunyang", ""], ["Frye", "Mark A", ""], ["Biernacka", "Joanna M", ""], ["Wang", "Yanshan", ""]]}, {"id": "2104.09647", "submitter": "Alexander Spangher", "authors": "Alexander Spangher and Jonathan May", "title": "\\textit{NewsEdits}: A Dataset of Revision Histories for News Articles\n  (Technical Report: Data Processing)", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News article revision histories have the potential to give us novel insights\nacross varied fields of linguistics and social sciences. In this work, we\npresent, to our knowledge, the first publicly available dataset of news article\nrevision histories, or \\textit{NewsEdits}.\n  Our dataset is multilingual; it contains 1,278,804 articles with 4,609,430\nversions from over 22 English- and French-language newspaper sources based in\nthree countries. Across version pairs, we count 10.9 million added sentences;\n8.9 million changed sentences and 6.8 million removed sentences. Within the\nchanged sentences, we derive 72 million atomic edits. \\textit{NewsEdits} is, to\nour knowledge, the largest corpus of revision histories of any domain.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:15:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Spangher", "Alexander", ""], ["May", "Jonathan", ""]]}, {"id": "2104.09653", "submitter": "Alexander Spangher", "authors": "Alexander Spangher, Nanyun Peng, Jonathan May and Emilio Ferrara", "title": "Modeling \"Newsworthiness\" for Lead-Generation Across Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Journalists obtain \"leads\", or story ideas, by reading large corpora of\ngovernment records: court cases, proposed bills, etc. However, only a small\npercentage of such records are interesting documents. We propose a model of\n\"newsworthiness\" aimed at surfacing interesting documents. We train models on\nautomatically labeled corpora -- published newspaper articles -- to predict\nwhether each article was a front-page article (i.e., \\textbf{newsworthy}) or\nnot (i.e., \\textbf{less newsworthy}). We transfer these models to unlabeled\ncorpora -- court cases, bills, city-council meeting minutes -- to rank\ndocuments in these corpora on \"newsworthiness\". A fine-tuned RoBERTa model\nachieves .93 AUC performance on heldout labeled documents, and .88 AUC on\nexpert-validated unlabeled corpora. We provide interpretation and visualization\nfor our models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:48:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Spangher", "Alexander", ""], ["Peng", "Nanyun", ""], ["May", "Jonathan", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2104.09656", "submitter": "Alexander Spangher", "authors": "Alexander Spangher, Nanyun Peng, Jonathan May and Emilio Ferrara", "title": "\"Don't quote me on that\": Finding Mixtures of Sources in News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Journalists publish statements provided by people, or \\textit{sources} to\ncontextualize current events, help voters make informed decisions, and hold\npowerful individuals accountable. In this work, we construct an ontological\nlabeling system for sources based on each source's \\textit{affiliation} and\n\\textit{role}. We build a probabilistic model to infer these attributes for\nnamed sources and to describe news articles as mixtures of these sources. Our\nmodel outperforms existing mixture modeling and co-clustering approaches and\ncorrectly infers source-type in 80\\% of expert-evaluated trials. Such work can\nfacilitate research in downstream tasks like opinion and argumentation mining,\nrepresenting a first step towards machine-in-the-loop \\textit{computational\njournalism} systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:57:11 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Spangher", "Alexander", ""], ["Peng", "Nanyun", ""], ["May", "Jonathan", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2104.09683", "submitter": "Pierre Lison", "authors": "Pierre Lison and Jeremy Barnes and Aliaksandr Hubin", "title": "skweak: Weak Supervision Made Easy for NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present skweak, a versatile, Python-based software toolkit enabling NLP\ndevelopers to apply weak supervision to a wide range of NLP tasks. Weak\nsupervision is an emerging machine learning paradigm based on a simple idea:\ninstead of labelling data points by hand, we use labelling functions derived\nfrom domain knowledge to automatically obtain annotations for a given dataset.\nThe resulting labels are then aggregated with a generative model that estimates\nthe accuracy (and possible confusions) of each labelling function. The skweak\ntoolkit makes it easy to implement a large spectrum of labelling functions\n(such as heuristics, gazetteers, neural models or linguistic constraints) on\ntext data, apply them on a corpus, and aggregate their results in a fully\nunsupervised fashion. skweak is especially designed to facilitate the use of\nweak supervision for NLP tasks such as text classification and sequence\nlabelling. We illustrate the use of skweak for NER and sentiment analysis.\nskweak is released under an open-source license and is available at:\nhttps://github.com/NorskRegnesentral/skweak\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 23:26:51 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lison", "Pierre", ""], ["Barnes", "Jeremy", ""], ["Hubin", "Aliaksandr", ""]]}, {"id": "2104.09691", "submitter": "V\\'it Novotn\\'y", "authors": "V\\'it Novotn\\'y and Michal \\v{S}tef\\'anik and Eniafe Festus Ayetiran\n  and Petr Sojka", "title": "When FastText Pays Attention: Efficient Estimation of Word\n  Representations using Constrained Positional Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the seminal work of Mikolov et al. (2013a) and Bojanowski et al.\n(2017), word representations of shallow log-bilinear language models have found\ntheir way into many NLP applications. Mikolov et al. (2018) introduced a\npositional log-bilinear language model, which has characteristics of an\nattention-based language model and which has reached state-of-the-art\nperformance on the intrinsic word analogy task. However, the positional model\nhas never been evaluated on qualitative criteria or extrinsic tasks and its\nspeed is impractical.\n  We outline the similarities between the attention mechanism and the\npositional model, and we propose a constrained positional model, which adapts\nthe sparse attention mechanism of Dai et al. (2018). We evaluate the positional\nand constrained positional models on three novel qualitative criteria and on\nthe extrinsic language modeling task of Botha and Blunsom (2014).\n  We show that the positional and constrained positional models contain\ninterpretable information about word order and outperform the subword model of\nBojanowski et al. (2017) on language modeling. We also show that the\nconstrained positional model outperforms the positional model on language\nmodeling and is twice as fast.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 23:52:19 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 06:43:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Novotn\u00fd", "V\u00edt", ""], ["\u0160tef\u00e1nik", "Michal", ""], ["Ayetiran", "Eniafe Festus", ""], ["Sojka", "Petr", ""]]}, {"id": "2104.09694", "submitter": "Luca Di Liello", "authors": "Luca Di Liello, Matteo Gabburo, Alessandro Moschitti", "title": "Efficient pre-training objectives for Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Transformer architecture deeply changed the natural language processing,\noutperforming all previous state-of-the-art models. However, well-known\nTransformer models like BERT, RoBERTa, and GPT-2 require a huge compute budget\nto create a high quality contextualised representation. In this paper, we study\nseveral efficient pre-training objectives for Transformers-based models. By\ntesting these objectives on different tasks, we determine which of the ELECTRA\nmodel's new features is the most relevant. We confirm that Transformers\npre-training is improved when the input does not contain masked tokens and that\nthe usage of the whole output to compute the loss reduces training time.\nMoreover, inspired by ELECTRA, we study a model composed of two blocks; a\ndiscriminator and a simple generator based on a statistical model with no\nimpact on the computational performances. Besides, we prove that eliminating\nthe MASK token and considering the whole output during the loss computation are\nessential choices to improve performance. Furthermore, we show that it is\npossible to efficiently train BERT-like models using a discriminative approach\nas in ELECTRA but without a complex generator, which is expensive. Finally, we\nshow that ELECTRA benefits heavily from a state-of-the-art hyper-parameters\nsearch.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:09:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Di Liello", "Luca", ""], ["Gabburo", "Matteo", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2104.09696", "submitter": "Meryem M'hamdi", "authors": "Meryem M'hamdi, Doo Soon Kim, Franck Dernoncourt, Trung Bui, Xiang\n  Ren, and Jonathan May", "title": "X-METRA-ADA: Cross-lingual Meta-Transfer Learning Adaptation to Natural\n  Language Understanding and Question Answering", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual models, such as M-BERT and XLM-R, have gained increasing\npopularity, due to their zero-shot cross-lingual transfer learning\ncapabilities. However, their generalization ability is still inconsistent for\ntypologically diverse languages and across different benchmarks. Recently,\nmeta-learning has garnered attention as a promising technique for enhancing\ntransfer learning under low-resource scenarios: particularly for cross-lingual\ntransfer in Natural Language Understanding (NLU). In this work, we propose\nX-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for\nNLU. Our approach adapts MAML, an optimization-based meta-learning approach, to\nlearn to adapt to new languages. We extensively evaluate our framework on two\nchallenging cross-lingual NLU tasks: multilingual task-oriented dialog and\ntypologically diverse question answering. We show that our approach outperforms\nnaive fine-tuning, reaching competitive performance on both tasks for most\nlanguages. Our analysis reveals that X-METRA-ADA can leverage limited data for\nfaster adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:13:35 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 18:00:09 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["M'hamdi", "Meryem", ""], ["Kim", "Doo Soon", ""], ["Dernoncourt", "Franck", ""], ["Bui", "Trung", ""], ["Ren", "Xiang", ""], ["May", "Jonathan", ""]]}, {"id": "2104.09712", "submitter": "Qingxiu Dong", "authors": "Qingxiu Dong, Zhifang Sui, Weidong Zhan and Baobao Chang", "title": "Problems and Countermeasures in Natural Language Processing Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Evaluation in natural language processing guides and promotes research on\nmodels and methods. In recent years, new evalua-tion data sets and evaluation\ntasks have been continuously proposed. At the same time, a series of problems\nexposed by ex-isting evaluation have also restricted the progress of natural\nlanguage processing technology. Starting from the concept, com-position,\ndevelopment and meaning of natural language evaluation, this article classifies\nand summarizes the tasks and char-acteristics of mainstream natural language\nevaluation, and then summarizes the problems and causes of natural language\npro-cessing evaluation. Finally, this article refers to the human language\nability evaluation standard, puts forward the concept of human-like machine\nlanguage ability evaluation, and proposes a series of basic principles and\nimplementation ideas for hu-man-like machine language ability evaluation from\nthe three aspects of reliability, difficulty and validity.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 01:35:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Dong", "Qingxiu", ""], ["Sui", "Zhifang", ""], ["Zhan", "Weidong", ""], ["Chang", "Baobao", ""]]}, {"id": "2104.09715", "submitter": "Yuzi Yan", "authors": "Yuzi Yan, Xu Tan, Bohan Li, Tao Qin, Sheng Zhao, Yuan Shen, Tie-Yan\n  Liu", "title": "AdaSpeech 2: Adaptive Text to Speech with Untranscribed Data", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text to speech (TTS) is widely used to synthesize personal voice for a target\nspeaker, where a well-trained source TTS model is fine-tuned with few paired\nadaptation data (speech and its transcripts) on this target speaker. However,\nin many scenarios, only untranscribed speech data is available for adaptation,\nwhich brings challenges to the previous TTS adaptation pipelines (e.g.,\nAdaSpeech). In this paper, we develop AdaSpeech 2, an adaptive TTS system that\nonly leverages untranscribed speech data for adaptation. Specifically, we\nintroduce a mel-spectrogram encoder to a well-trained TTS model to conduct\nspeech reconstruction, and at the same time constrain the output sequence of\nthe mel-spectrogram encoder to be close to that of the original phoneme\nencoder. In adaptation, we use untranscribed speech data for speech\nreconstruction and only fine-tune the TTS decoder. AdaSpeech 2 has two\nadvantages: 1) Pluggable: our system can be easily applied to existing trained\nTTS models without re-training. 2) Effective: our system achieves on-par voice\nquality with the transcribed TTS adaptation (e.g., AdaSpeech) with the same\namount of untranscribed data, and achieves better voice quality than previous\nuntranscribed adaptation methods. Synthesized speech samples can be found at\nhttps://speechresearch.github.io/adaspeech2/.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 01:53:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yan", "Yuzi", ""], ["Tan", "Xu", ""], ["Li", "Bohan", ""], ["Qin", "Tao", ""], ["Zhao", "Sheng", ""], ["Shen", "Yuan", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2104.09742", "submitter": "Shuguang Chen", "authors": "Shuguang Chen, Leonardo Neves, and Thamar Solorio", "title": "Mitigating Temporal-Drift: A Simple Approach to Keep NER Models Crisp", "comments": "Accepted to SocialNLP at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance of neural models for named entity recognition degrades over time,\nbecoming stale. This degradation is due to temporal drift, the change in our\ntarget variables' statistical properties over time. This issue is especially\nproblematic for social media data, where topics change rapidly. In order to\nmitigate the problem, data annotation and retraining of models is common.\nDespite its usefulness, this process is expensive and time-consuming, which\nmotivates new research on efficient model updating. In this paper, we propose\nan intuitive approach to measure the potential trendiness of tweets and use\nthis metric to select the most informative instances to use for training. We\nconduct experiments on three state-of-the-art models on the Temporal Twitter\nDataset. Our approach shows larger increases in prediction accuracy with less\ntraining data than the alternatives, making it an attractive, practical\nsolution.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:35:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Shuguang", ""], ["Neves", "Leonardo", ""], ["Solorio", "Thamar", ""]]}, {"id": "2104.09765", "submitter": "Yiping Jin", "authors": "Yiping Jin, Akshay Bhatia, Dittaya Wanvarie", "title": "Seed Word Selection for Weakly-Supervised Text Classification with\n  Unsupervised Error Estimation", "comments": "Accepted to NAACL SRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly-supervised text classification aims to induce text classifiers from\nonly a few user-provided seed words. The vast majority of previous work assumes\nhigh-quality seed words are given. However, the expert-annotated seed words are\nsometimes non-trivial to come up with. Furthermore, in the weakly-supervised\nlearning setting, we do not have any labeled document to measure the seed\nwords' efficacy, making the seed word selection process \"a walk in the dark\".\nIn this work, we remove the need for expert-curated seed words by first mining\n(noisy) candidate seed words associated with the category names. We then train\ninterim models with individual candidate seed words. Lastly, we estimate the\ninterim models' error rate in an unsupervised manner. The seed words that yield\nthe lowest estimated error rates are added to the final seed word set. A\ncomprehensive evaluation of six binary classification tasks on four popular\ndatasets demonstrates that the proposed method outperforms a baseline using\nonly category name seed words and obtained comparable performance as a\ncounterpart using expert-annotated seed words.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 05:10:40 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jin", "Yiping", ""], ["Bhatia", "Akshay", ""], ["Wanvarie", "Dittaya", ""]]}, {"id": "2104.09777", "submitter": "Inkyu Sa", "authors": "JongYoon Lim, Inkyu Sa, Ho Seok Ahn, Norina Gasteiger, Sanghyub John\n  Lee, Bruce MacDonald", "title": "Subsentence Extraction from Text Using Coverage-Based Deep Learning\n  Language Models", "comments": "27 pages, 16 figures", "journal-ref": "MDPI Sensors 2021, 21(8), 2712", "doi": "10.3390/s21082712", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment prediction remains a challenging and unresolved task in various\nresearch fields, including psychology, neuroscience, and computer science. This\nstems from its high degree of subjectivity and limited input sources that can\neffectively capture the actual sentiment. This can be even more challenging\nwith only text-based input. Meanwhile, the rise of deep learning and an\nunprecedented large volume of data have paved the way for artificial\nintelligence to perform impressively accurate predictions or even human-level\nreasoning. Drawing inspiration from this, we propose a coverage-based sentiment\nand subsentence extraction system that estimates a span of input text and\nrecursively feeds this information back to the networks. The predicted\nsubsentence consists of auxiliary information expressing a sentiment. This is\nan important building block for enabling vivid and epic sentiment delivery\n(within the scope of this paper) and for other natural language processing\ntasks such as text summarisation and Q&A. Our approach outperforms the\nstate-of-the-art approaches by a large margin in subsentence prediction (i.e.,\nAverage Jaccard scores from 0.72 to 0.89). For the evaluation, we designed\nrigorous experiments consisting of 24 ablation studies. Finally, our learned\nlessons are returned to the community by sharing software packages and a public\ndataset that can reproduce the results presented in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:24:49 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 03:49:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Lim", "JongYoon", ""], ["Sa", "Inkyu", ""], ["Ahn", "Ho Seok", ""], ["Gasteiger", "Norina", ""], ["Lee", "Sanghyub John", ""], ["MacDonald", "Bruce", ""]]}, {"id": "2104.09792", "submitter": "Gilad Kutiel", "authors": "Iftah Gamzu, Hila Gonen, Gilad Kutiel, Ran Levy, Eugene Agichtein", "title": "Identifying Helpful Sentences in Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years online shopping has gained momentum and became an important\nvenue for customers wishing to save time and simplify their shopping process. A\nkey advantage of shopping online is the ability to read what other customers\nare saying about products of interest. In this work, we aim to maintain this\nadvantage in situations where extreme brevity is needed, for example, when\nshopping by voice. We suggest a novel task of extracting a single\nrepresentative helpful sentence from a set of reviews for a given product. The\nselected sentence should meet two conditions: first, it should be helpful for a\npurchase decision and second, the opinion it expresses should be supported by\nmultiple reviewers. This task is closely related to the task of Multi Document\nSummarization in the product reviews domain but differs in its objective and\nits level of conciseness. We collect a dataset in English of sentence\nhelpfulness scores via crowd-sourcing and demonstrate its reliability despite\nthe inherent subjectivity involved. Next, we describe a complete model that\nextracts representative helpful sentences with positive and negative sentiment\ntowards the product and demonstrate that it outperforms several baselines.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:09:22 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 06:36:15 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 08:37:56 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gamzu", "Iftah", ""], ["Gonen", "Hila", ""], ["Kutiel", "Gilad", ""], ["Levy", "Ran", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2104.09810", "submitter": "Weiwen Xu", "authors": "Weiwen Xu, Ai Ti Aw, Yang Ding, Kui Wu, Shafiq Joty", "title": "Addressing the Vulnerability of NMT in Input Perturbations", "comments": "Accepted by NAACL 2021 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Machine Translation (NMT) has achieved significant breakthrough in\nperformance but is known to suffer vulnerability to input perturbations. As\nreal input noise is difficult to predict during training, robustness is a big\nissue for system deployment. In this paper, we improve the robustness of NMT\nmodels by reducing the effect of noisy words through a Context-Enhanced\nReconstruction (CER) approach. CER trains the model to resist noise in two\nsteps: (1) perturbation step that breaks the naturalness of input sequence with\nmade-up words; (2) reconstruction step that defends the noise propagation by\ngenerating better and more robust contextual representation. Experimental\nresults on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks\ndemonstrate robustness improvement on both news and social media text. Further\nfine-tuning experiments on social media text show our approach can converge at\na higher position and provide a better adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:52:58 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Xu", "Weiwen", ""], ["Aw", "Ai Ti", ""], ["Ding", "Yang", ""], ["Wu", "Kui", ""], ["Joty", "Shafiq", ""]]}, {"id": "2104.09827", "submitter": "Sagnik Mukherjee", "authors": "Jay Mundra, Rohan Gupta, Sagnik Mukherjee", "title": "WASSA@IITK at WASSA 2021: Multi-task Learning and Transformer Finetuning\n  for Emotion Classification and Empathy Prediction", "comments": "Accepted at WASSA-2021, 4 Pages + 1 Page (references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes our contribution to the WASSA 2021 shared task on\nEmpathy Prediction and Emotion Classification. The broad goal of this task was\nto model an empathy score, a distress score and the overall level of emotion of\nan essay written in response to a newspaper article associated with harm to\nsomeone. We have used the ELECTRA model abundantly and also advanced deep\nlearning approaches like multi-task learning. Additionally, we also leveraged\nstandard machine learning techniques like ensembling. Our system achieves a\nPearson Correlation Coefficient of 0.533 on sub-task I and a macro F1 score of\n0.5528 on sub-task II. We ranked 1st in Emotion Classification sub-task and 3rd\nin Empathy Prediction sub-task\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:24:10 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mundra", "Jay", ""], ["Gupta", "Rohan", ""], ["Mukherjee", "Sagnik", ""]]}, {"id": "2104.09833", "submitter": "Honai Ueoka", "authors": "Honai Ueoka, Yugo Murawaki and Sadao Kurohashi", "title": "Frustratingly Easy Edit-based Linguistic Steganography with a Masked\n  Language Model", "comments": "7 pages, 4 firgures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in neural language models, the focus of linguistic\nsteganography has shifted from edit-based approaches to generation-based ones.\nWhile the latter's payload capacity is impressive, generating genuine-looking\ntexts remains challenging. In this paper, we revisit edit-based linguistic\nsteganography, with the idea that a masked language model offers an\noff-the-shelf solution. The proposed method eliminates painstaking rule\nconstruction and has a high payload capacity for an edit-based model. It is\nalso shown to be more secure against automatic detection than a\ngeneration-based method while offering better control of the security/payload\ncapacity trade-off.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:35:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ueoka", "Honai", ""], ["Murawaki", "Yugo", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "2104.09864", "submitter": "Jianlin Su", "authors": "Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "comments": "Preprint. English experiments are coming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Position encoding in transformer architecture provides supervision for\ndependency modeling between elements at different positions in the sequence. We\ninvestigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named\nRotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional\ninformation with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with\nvaluable properties such as flexibility of being expand to any sequence\nlengths, decaying inter-token dependency with increasing relative distances,\nand capability of equipping the linear self-attention with relative position\nencoding. As a result, the enhanced transformer with rotary position embedding,\nor RoFormer, achieves superior performance in tasks with long texts. We release\nthe theoretical analysis along with some preliminary experiment results on\nChinese data. The undergoing experiment for English benchmark will soon be\nupdated.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:54:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Su", "Jianlin", ""], ["Lu", "Yu", ""], ["Pan", "Shengfeng", ""], ["Wen", "Bo", ""], ["Liu", "Yunfeng", ""]]}, {"id": "2104.09871", "submitter": "Theo Yan", "authors": "Shiyao Yan, Zequn Zhang, Xian Sun, Guangluan Xu, Li Jin and Shuchao Li", "title": "HYPER^2: Hyperbolic Poincare Embedding for Hyper-Relational Link\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link Prediction, addressing the issue of completing KGs with missing facts,\nhas been broadly studied. However, less light is shed on the ubiquitous\nhyper-relational KGs. Most existing hyper-relational KG embedding models still\ntear an n-ary fact into smaller tuples, neglecting the indecomposability of\nsome n-ary facts. While other frameworks work for certain arity facts only or\nignore the significance of primary triple. In this paper, we represent an n-ary\nfact as a whole, simultaneously keeping the integrity of n-ary fact and\nmaintaining the vital role that the primary triple plays. In addition, we\ngeneralize hyperbolic Poincar\\'e embedding from binary to arbitrary arity data,\nwhich has not been studied yet. To tackle the weak expressiveness and high\ncomplexity issue, we propose HYPER^2 which is qualified for capturing the\ninteraction between entities within and beyond triple through information\naggregation on the tangent space. Extensive experiments demonstrate HYPER^2\nachieves superior performance to its translational and deep analogues,\nimproving SOTA by up to 34.5\\% with relatively few dimensions. Moreover, we\nstudy the side effect of literals and we theoretically and experimentally\ncompare the computational complexity of HYPER^2 against several best performing\nbaselines, HYPER^2 is 49-61 times quicker than its counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:06:05 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yan", "Shiyao", ""], ["Zhang", "Zequn", ""], ["Sun", "Xian", ""], ["Xu", "Guangluan", ""], ["Jin", "Li", ""], ["Li", "Shuchao", ""]]}, {"id": "2104.09933", "submitter": "Mathias Creutz", "authors": "Eetu Sj\\\"oblom and Mathias Creutz and Teemu Vahtola", "title": "Grammatical Error Generation Based on Translated Fragments", "comments": "Accepted for NoDaLiDa 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We perform neural machine translation of sentence fragments in order to\ncreate large amounts of training data for English grammatical error correction.\nOur method aims at simulating mistakes made by second language learners, and\nproduces a wider range of non-native style language in comparison to\nstate-of-the-art synthetic data creation methods. In addition to purely\ngrammatical errors, our approach generates other types of errors, such as\nlexical errors. We perform grammatical error correction experiments using\nneural sequence-to-sequence models, and carry out quantitative and qualitative\nevaluation. A model trained on data created using our proposed method is shown\nto outperform a baseline model on test data with a high proportion of errors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 12:43:40 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sj\u00f6blom", "Eetu", ""], ["Creutz", "Mathias", ""], ["Vahtola", "Teemu", ""]]}, {"id": "2104.09947", "submitter": "Pieter Delobelle", "authors": "Kristen Scott and Pieter Delobelle and Bettina Berendt", "title": "Measuring Shifts in Attitudes Towards COVID-19 Measures in Belgium Using\n  Multilingual BERT", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify seven months' worth of Belgian COVID-related Tweets using\nmultilingual BERT and relate them to their governments' COVID measures. We\nclassify Tweets by their stated opinion on Belgian government curfew measures\n(too strict, ok, too loose). We examine the change in topics discussed and\nviews expressed over time and in reference to dates of related events such as\nimplementation of new measures or COVID-19 related announcements in the media.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:17:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Scott", "Kristen", ""], ["Delobelle", "Pieter", ""], ["Berendt", "Bettina", ""]]}, {"id": "2104.09978", "submitter": "Karan Jindal", "authors": "Rahul Singh, Karan Jindal, Yufei Yu, Hanyu Yang, Tarun Joshi, Matthew\n  A. Campbell, Wayne B. Shoumaker", "title": "Robustness Tests of NLP Machine Learning Models: Search and Semantically\n  Replace", "comments": "18 pages, 2 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategy to assess the robustness of different machine\nlearning models that involve natural language processing (NLP). The overall\napproach relies upon a Search and Semantically Replace strategy that consists\nof two steps: (1) Search, which identifies important parts in the text; (2)\nSemantically Replace, which finds replacements for the important parts, and\nconstrains the replaced tokens with semantically similar words. We introduce\ndifferent types of Search and Semantically Replace methods designed\nspecifically for particular types of machine learning models. We also\ninvestigate the effectiveness of this strategy and provide a general framework\nto assess a variety of machine learning models. Finally, an empirical\ncomparison is provided of robustness performance among three different model\ntypes, each with a different text representation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:05:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Singh", "Rahul", ""], ["Jindal", "Karan", ""], ["Yu", "Yufei", ""], ["Yang", "Hanyu", ""], ["Joshi", "Tarun", ""], ["Campbell", "Matthew A.", ""], ["Shoumaker", "Wayne B.", ""]]}, {"id": "2104.09995", "submitter": "Zhaoxi Mu", "authors": "Zhaoxi Mu, Xinyu Yang, Yizhuo Dong", "title": "Review of end-to-end speech synthesis technology based on deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an indispensable part of modern human-computer interaction system, speech\nsynthesis technology helps users get the output of intelligent machine more\neasily and intuitively, thus has attracted more and more attention. Due to the\nlimitations of high complexity and low efficiency of traditional speech\nsynthesis technology, the current research focus is the deep learning-based\nend-to-end speech synthesis technology, which has more powerful modeling\nability and a simpler pipeline. It mainly consists of three modules: text\nfront-end, acoustic model, and vocoder. This paper reviews the research status\nof these three parts, and classifies and compares various methods according to\ntheir emphasis. Moreover, this paper also summarizes the open-source speech\ncorpus of English, Chinese and other languages that can be used for speech\nsynthesis tasks, and introduces some commonly used subjective and objective\nspeech quality evaluation method. Finally, some attractive future research\ndirections are pointed out.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:24:05 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mu", "Zhaoxi", ""], ["Yang", "Xinyu", ""], ["Dong", "Yizhuo", ""]]}, {"id": "2104.10097", "submitter": "Boaz Shmueli", "authors": "Boaz Shmueli, Jan Fell, Soumya Ray, Lun-Wei Ku", "title": "Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing", "comments": "To be published in NAACL-HLT 2021. 12 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of crowdworkers in NLP research is growing rapidly, in tandem with\nthe exponential increase in research production in machine learning and AI.\nEthical discussion regarding the use of crowdworkers within the NLP research\ncommunity is typically confined in scope to issues related to labor conditions\nsuch as fair pay. We draw attention to the lack of ethical considerations\nrelated to the various tasks performed by workers, including labeling,\nevaluation, and production. We find that the Final Rule, the common ethical\nframework used by researchers, did not anticipate the use of online\ncrowdsourcing platforms for data collection, resulting in gaps between the\nspirit and practice of human-subjects ethics in NLP research. We enumerate\ncommon scenarios where crowdworkers performing NLP tasks are at risk of harm.\nWe thus recommend that researchers evaluate these risks by considering the\nthree ethical principles set up by the Belmont Report. We also clarify some\ncommon misconceptions regarding the Institutional Review Board (IRB)\napplication. We hope this paper will serve to reopen the discussion within our\ncommunity regarding the ethical use of crowdworkers.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:30:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Shmueli", "Boaz", ""], ["Fell", "Jan", ""], ["Ray", "Soumya", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2104.10100", "submitter": "Son T. Luu", "authors": "Son T. Luu, Ngan Luu-Thuy Nguyen", "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with\n  BiLSTM-CRF and ToxicBERT Comment Classification", "comments": "Final camera-ready version. To be appeared in Proceedings of The 15th\n  International Workshop on Semantic Evaluation (SemEval 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:32:56 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 10:47:01 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 02:09:48 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Luu", "Son T.", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2104.10117", "submitter": "Yuting Guo", "authors": "Yuting Guo and Jinho Choi", "title": "Enhancing Cognitive Models of Emotions with Representation Learning", "comments": "Accepted by the NAACL Workshop on Cognitive Modeling and\n  Computational Linguistics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel deep learning-based framework to generate embedding\nrepresentations of fine-grained emotions that can be used to computationally\ndescribe psychological models of emotions. Our framework integrates a\ncontextualized embedding encoder with a multi-head probing model that enables\nto interpret dynamically learned representations optimized for an emotion\nclassification task. Our model is evaluated on the Empathetic Dialogue dataset\nand shows the state-of-the-art result for classifying 32 emotions. Our layer\nanalysis can derive an emotion graph to depict hierarchical relations among the\nemotions. Our emotion representations can be used to generate an emotion wheel\ndirectly comparable to the one from Plutchik's\\LN model, and also augment the\nvalues of missing emotions in the PAD emotional state model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:55:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Guo", "Yuting", ""], ["Choi", "Jinho", ""]]}, {"id": "2104.10121", "submitter": "Shahin Amiriparian", "authors": "Shahin Amiriparian (1), Artem Sokolov (2,3), Ilhan Aslan (2), Lukas\n  Christ (1), Maurice Gerczuk (1), Tobias H\\\"ubner (1), Dmitry Lamanov (2),\n  Manuel Milling (1), Sandra Ottl (1), Ilya Poduremennykh (2), Evgeniy Shuranov\n  (2,4), Bj\\\"orn W. Schuller (1,5) ((1) EIHW -- Chair of Embedded Intelligence\n  for Health Care and Wellbeing, University of Augsburg, Germany, (2) Huawei\n  Technologies, (3) HSE University, Nizhniy Novgorod, Russia, (4) ITMO\n  University, Saint Petersburg, Russia)", "title": "On the Impact of Word Error Rate on Acoustic-Linguistic Speech Emotion\n  Recognition: An Update for the Deep Learning Era", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text encodings from automatic speech recognition (ASR) transcripts and audio\nrepresentations have shown promise in speech emotion recognition (SER) ever\nsince. Yet, it is challenging to explain the effect of each information stream\non the SER systems. Further, more clarification is required for analysing the\nimpact of ASR's word error rate (WER) on linguistic emotion recognition per se\nand in the context of fusion with acoustic information exploitation in the age\nof deep ASR systems. In order to tackle the above issues, we create transcripts\nfrom the original speech by applying three modern ASR systems, including an\nend-to-end model trained with recurrent neural network-transducer loss, a model\nwith connectionist temporal classification loss, and a wav2vec framework for\nself-supervised learning. Afterwards, we use pre-trained textual models to\nextract text representations from the ASR outputs and the gold standard. For\nextraction and learning of acoustic speech features, we utilise openSMILE,\nopenXBoW, DeepSpectrum, and auDeep. Finally, we conduct decision-level fusion\non both information streams -- acoustics and linguistics. Using the best\ndevelopment configuration, we achieve state-of-the-art unweighted average\nrecall values of $73.6\\,\\%$ and $73.8\\,\\%$ on the speaker-independent\ndevelopment and test partitions of IEMOCAP, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:10:01 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Amiriparian", "Shahin", ""], ["Sokolov", "Artem", ""], ["Aslan", "Ilhan", ""], ["Christ", "Lukas", ""], ["Gerczuk", "Maurice", ""], ["H\u00fcbner", "Tobias", ""], ["Lamanov", "Dmitry", ""], ["Milling", "Manuel", ""], ["Ottl", "Sandra", ""], ["Poduremennykh", "Ilya", ""], ["Shuranov", "Evgeniy", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2104.10129", "submitter": "Sunil Gandhi", "authors": "Hengxin Fun, Sunil Gandhi, Sujith Ravi", "title": "Efficient Retrieval Optimized Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been significant advances in neural methods for tackling\nknowledge-intensive tasks such as open domain question answering (QA). These\nadvances are fueled by combining large pre-trained language models with\nlearnable retrieval of documents. Majority of these models use separate\nencoders for learning query representation, passage representation for the\nretriever and an additional encoder for the downstream task. Using separate\nencoders for each stage/task occupies a lot of memory and makes it difficult to\nscale to a large number of tasks. In this paper, we propose a novel Retrieval\nOptimized Multi-task (ROM) framework for jointly training self-supervised\ntasks, knowledge retrieval, and extractive question answering. Our ROM approach\npresents a unified and generalizable framework that enables scaling efficiently\nto multiple tasks, varying levels of supervision, and optimization choices such\nas different learning schedules without changing the model architecture. It\nalso provides the flexibility of changing the encoders without changing the\narchitecture of the system. Using our framework, we achieve comparable or\nbetter performance than recent methods on QA, while drastically reducing the\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:16:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Fun", "Hengxin", ""], ["Gandhi", "Sunil", ""], ["Ravi", "Sujith", ""]]}, {"id": "2104.10130", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Heba Elfardy, Christos Christodoulopoulos, Thomas Butler,\n  Mohit Bansal", "title": "Hidden Biases in Unreliable News Detection Datasets", "comments": "EACL 2021 (11 pages, 3 figures, 8 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic unreliable news detection is a research problem with great\npotential impact. Recently, several papers have shown promising results on\nlarge-scale news datasets with models that only use the article itself without\nresorting to any fact-checking mechanism or retrieving any supporting evidence.\nIn this work, we take a closer look at these datasets. While they all provide\nvaluable resources for future research, we observe a number of problems that\nmay lead to results that do not generalize in more realistic settings.\nSpecifically, we show that selection bias during data collection leads to\nundesired artifacts in the datasets. In addition, while most systems train and\npredict at the level of individual articles, overlapping article sources in the\ntraining and evaluation data can provide a strong confounding factor that\nmodels can exploit. In the presence of this confounding factor, the models can\nachieve good performance by directly memorizing the site-label mapping instead\nof modeling the real task of unreliable news detection. We observed a\nsignificant drop (>10%) in accuracy for all models tested in a clean split with\nno train/test source overlap. Using the observations and experimental results,\nwe provide practical suggestions on how to create more reliable datasets for\nthe unreliable news detection task. We suggest future dataset creation include\na simple model as a difficulty/bias probe and future model development use a\nclean non-overlapping site and date split.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:16:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhou", "Xiang", ""], ["Elfardy", "Heba", ""], ["Christodoulopoulos", "Christos", ""], ["Butler", "Thomas", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.10139", "submitter": "Pritish Sahu", "authors": "Pritish Sahu, Karan Sikka, and Ajay Divakaran", "title": "Towards Solving Multimodal Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets the problem of procedural multimodal machine comprehension\n(M3C). This task requires an AI to comprehend given steps of multimodal\ninstructions and then answer questions. Compared to vanilla machine\ncomprehension tasks where an AI is required only to understand a textual input,\nprocedural M3C is more challenging as the AI needs to comprehend both the\ntemporal and causal factors along with multimodal inputs. Recently Yagcioglu et\nal. [35] introduced RecipeQA dataset to evaluate M3C. Our first contribution is\nthe introduction of two new M3C datasets- WoodworkQA and DecorationQA with 16K\nand 10K instructional procedures, respectively. We then evaluate M3C using a\ntextual cloze style question-answering task and highlight an inherent bias in\nthe question answer generation method from [35] that enables a naive baseline\nto cheat by learning from only answer choices. This naive baseline performs\nsimilar to a popular method used in question answering- Impatient Reader [6]\nthat uses attention over both the context and the query. We hypothesized that\nthis naturally occurring bias present in the dataset affects even the best\nperforming model. We verify our proposed hypothesis and propose an algorithm\ncapable of modifying the given dataset to remove the bias elements. Finally, we\nreport our performance on the debiased dataset with several strong baselines.\nWe observe that the performance of all methods falls by a margin of 8% - 16%\nafter correcting for the bias. We hope these datasets and the analysis will\nprovide valuable benchmarks and encourage further research in this area.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:30:27 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sahu", "Pritish", ""], ["Sikka", "Karan", ""], ["Divakaran", "Ajay", ""]]}, {"id": "2104.10166", "submitter": "Amit Moryossef", "authors": "Amit Moryossef, Ioannis Tsochantaridis, Joe Dinn, Necati Cihan\n  Camg\\\"oz, Richard Bowden, Tao Jiang, Annette Rios, Mathias M\\\"uller, Sarah\n  Ebling", "title": "Evaluating the Immediate Applicability of Pose Estimation for Sign\n  Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Signed languages are visual languages produced by the movement of the hands,\nface, and body. In this paper, we evaluate representations based on skeleton\nposes, as these are explainable, person-independent, privacy-preserving,\nlow-dimensional representations. Basically, skeletal representations generalize\nover an individual's appearance and background, allowing us to focus on the\nrecognition of motion. But how much information is lost by the skeletal\nrepresentation? We perform two independent studies using two state-of-the-art\npose estimation systems. We analyze the applicability of the pose estimation\nsystems to sign language recognition by evaluating the failure cases of the\nrecognition models. Importantly, this allows us to characterize the current\nlimitations of skeletal pose estimation approaches in sign language\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:41:45 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Moryossef", "Amit", ""], ["Tsochantaridis", "Ioannis", ""], ["Dinn", "Joe", ""], ["Camg\u00f6z", "Necati Cihan", ""], ["Bowden", "Richard", ""], ["Jiang", "Tao", ""], ["Rios", "Annette", ""], ["M\u00fcller", "Mathias", ""], ["Ebling", "Sarah", ""]]}, {"id": "2104.10193", "submitter": "Lisa Bauer", "authors": "Lisa Bauer, Mohit Bansal", "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense\n  Reasoning Tasks", "comments": "EACL 2021 (14 pages, 2 figures, 10 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating external knowledge into commonsense reasoning tasks has shown\nprogress in resolving some, but not all, knowledge gaps in these tasks. For\nknowledge integration to yield peak performance, it is critical to select a\nknowledge graph (KG) that is well-aligned with the given task's objective. We\npresent an approach to assess how well a candidate KG can correctly identify\nand accurately fill in gaps of reasoning for a task, which we call KG-to-task\nmatch. We show this KG-to-task match in 3 phases: knowledge-task\nidentification, knowledge-task alignment, and knowledge-task integration. We\nalso analyze our transformer-based KG-to-task models via commonsense probes to\nmeasure how much knowledge is captured in these models before and after KG\nintegration. Empirically, we investigate KG matches for the SocialIQA (SIQA)\n(Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0\n(Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al.,\n2019a), ConceptNet (Speer et al., 2017), and an automatically constructed\ninstructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we\nare able to demonstrate that ATOMIC, an event-inference focused KG, is the best\nmatch for SIQA and MCScript2.0, and that the taxonomic ConceptNet and\nWikiHow-based KGs are the best matches for PIQA across all 3 analysis phases.\nWe verify our methods and findings with human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:23:45 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bauer", "Lisa", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.10210", "submitter": "Richard A. Blythe", "authors": "Richard A Blythe and William Croft", "title": "How individuals change language", "comments": "50 pages, 11 figures", "journal-ref": "PLoS ONE 16(6): e0252582 (2021)", "doi": "10.1371/journal.pone.0252582", "report-no": null, "categories": "cs.CL physics.soc-ph q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Languages emerge and change over time at the population level though\ninteractions between individual speakers. It is, however, hard to directly\nobserve how a single speaker's linguistic innovation precipitates a\npopulation-wide change in the language, and many theoretical proposals exist.\nWe introduce a very general mathematical model that encompasses a wide variety\nof individual-level linguistic behaviours and provides statistical predictions\nfor the population-level changes that result from them. This model allows us to\ncompare the likelihood of empirically-attested changes in definite and\nindefinite articles in multiple languages under different assumptions on the\nway in which individuals learn and use language. We find that accounts of\nlanguage change that appeal primarily to errors in childhood language\nacquisition are very weakly supported by the historical data, whereas those\nthat allow speakers to change incrementally across the lifespan are more\nplausible, particularly when combined with social network effects.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 19:02:49 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Blythe", "Richard A", ""], ["Croft", "William", ""]]}, {"id": "2104.10213", "submitter": "George Papakostas Prof.", "authors": "N.-I. Galanis, P. Vafiadis, K.-G. Mirzaev, G.A. Papakostas", "title": "Machine Learning Meets Natural Language Processing -- The story so far", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural Language Processing (NLP) has evolved significantly over the last\ndecade. This paper highlights the most important milestones of this period\nwhile trying to pinpoint the contribution of each individual model and\nalgorithm to the overall progress. Furthermore, it focuses on issues still\nremaining to be solved, emphasizing the groundbreaking proposals of\nTransformers, BERT, and all the similar attention-based models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 16:41:34 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Galanis", "N. -I.", ""], ["Vafiadis", "P.", ""], ["Mirzaev", "K. -G.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2104.10215", "submitter": "Sopan Khosla", "authors": "Sopan Khosla, James Fiacco, Carolyn Rose", "title": "Evaluating the Impact of a Hierarchical Discourse Representation on\n  Entity Coreference Resolution Performance", "comments": "Also contains the Appendix. Accepted to NAACL 2021 as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work on entity coreference resolution (CR) follows current trends in\nDeep Learning applied to embeddings and relatively simple task-related\nfeatures. SOTA models do not make use of hierarchical representations of\ndiscourse structure. In this work, we leverage automatically constructed\ndiscourse parse trees within a neural approach and demonstrate a significant\nimprovement on two benchmark entity coreference-resolution datasets. We explore\nhow the impact varies depending upon the type of mention.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 19:14:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Khosla", "Sopan", ""], ["Fiacco", "James", ""], ["Rose", "Carolyn", ""]]}, {"id": "2104.10247", "submitter": "Ian Porada", "authors": "Ian Porada, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung", "title": "Modeling Event Plausibility with Consistent Conceptual Abstraction", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding natural language requires common sense, one aspect of which is\nthe ability to discern the plausibility of events. While distributional models\n-- most recently pre-trained, Transformer language models -- have demonstrated\nimprovements in modeling event plausibility, their performance still falls\nshort of humans'. In this work, we show that Transformer-based plausibility\nmodels are markedly inconsistent across the conceptual classes of a lexical\nhierarchy, inferring that \"a person breathing\" is plausible while \"a dentist\nbreathing\" is not, for example. We find this inconsistency persists even when\nmodels are softly injected with lexical knowledge, and we present a simple\npost-hoc method of forcing model consistency that improves correlation with\nhuman plausibility judgements.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:08:32 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Porada", "Ian", ""], ["Suleman", "Kaheer", ""], ["Trischler", "Adam", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "2104.10259", "submitter": "Philip Feldman", "authors": "Philip Feldman, Sim Tiwari, Charissa S. L. Cheah, James R. Foulds,\n  Shimei Pan", "title": "Analyzing COVID-19 Tweets with Transformer-based Language Models", "comments": "Six pages, six tables, four figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for using Transformer-based Language Models\n(TLMs) to understand public opinion from social media posts. In this approach,\nwe train a set of GPT models on several COVID-19 tweet corpora that reflect\npopulations of users with distinctive views. We then use prompt-based queries\nto probe these models to reveal insights into the biases and opinions of the\nusers. We demonstrate how this approach can be used to produce results which\nresemble polling the public on diverse social, political and public health\nissues. The results on the COVID-19 tweet data show that transformer language\nmodels are promising tools that can help us understand public opinions on\nsocial media at scale.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:45:33 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 17:06:00 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 01:03:18 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Feldman", "Philip", ""], ["Tiwari", "Sim", ""], ["Cheah", "Charissa S. L.", ""], ["Foulds", "James R.", ""], ["Pan", "Shimei", ""]]}, {"id": "2104.10263", "submitter": "Alexander Spangher", "authors": "Alexander Spangher and Jonathan May", "title": "\\textit{StateCensusLaws.org}: A Web Application for Consuming and\n  Annotating Legal Discourse Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we create a web application to highlight the output of NLP\nmodels trained to parse and label discourse segments in law text. Our system is\nbuilt primarily with journalists and legal interpreters in mind, and we focus\non state-level law that uses U.S. Census population numbers to allocate\nresources and organize government.\n  Our system exposes a corpus we collect of 6,000 state-level laws that pertain\nto the U.S. census, using 25 scrapers we built to crawl state law websites,\nwhich we release. We also build a novel, flexible annotation framework that can\nhandle span-tagging and relation tagging on an arbitrary input text document\nand be embedded simply into any webpage. This framework allows journalists and\nresearchers to add to our annotation database by correcting and tagging new\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:00:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Spangher", "Alexander", ""], ["May", "Jonathan", ""]]}, {"id": "2104.10270", "submitter": "Andrea Bruera", "authors": "Andrea Bruera and Aur\\'elie Herbelot", "title": "Novel Aficionados and Doppelg\\\"angers: a referential task for semantic\n  representations of individual entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human semantic cognition, proper names (names which refer to individual\nentities) are harder to learn and retrieve than common nouns. This seems to be\nthe case for machine learning algorithms too, but the linguistic and\ndistributional reasons for this behaviour have not been investigated in depth\nso far. To tackle this issue, we show that the semantic distinction between\nproper names and common nouns is reflected in their linguistic distributions by\nemploying an original task for distributional semantics, the Doppelg\\\"anger\ntest, an extensive set of models, and a new dataset, the Novel Aficionados\ndataset. The results indicate that the distributional representations of\ndifferent individual entities are less clearly distinguishable from each other\nthan those of common nouns, an outcome which intriguingly mirrors human\ncognition.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:24:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bruera", "Andrea", ""], ["Herbelot", "Aur\u00e9lie", ""]]}, {"id": "2104.10283", "submitter": "Weixin Liang", "authors": "Weixin Liang, Yanhao Jiang and Zixuan Liu", "title": "GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual\n  Question Answering", "comments": "NAACL 2021 MAI-Workshop. Code available at\n  https://github.com/codexxxl/GraphVQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are more than a collection of objects or attributes -- they represent\na web of relationships among interconnected objects. Scene Graph has emerged as\na new modality for a structured graphical representation of images. Scene Graph\nencodes objects as nodes connected via pairwise relations as edges. To support\nquestion answering on scene graphs, we propose GraphVQA, a language-guided\ngraph neural network framework that translates and executes a natural language\nquestion as multiple iterations of message passing among graph nodes. We\nexplore the design space of GraphVQA framework, and discuss the trade-off of\ndifferent design choices. Our experiments on GQA dataset show that GraphVQA\noutperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 23:54:41 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:29:00 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liang", "Weixin", ""], ["Jiang", "Yanhao", ""], ["Liu", "Zixuan", ""]]}, {"id": "2104.10317", "submitter": "Zhiling Zhang", "authors": "Zhiling Zhang, Kenny Q. Zhu", "title": "Diverse and Specific Clarification Question Generation with Keywords", "comments": "11 pages, 3 figures, WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449876", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Product descriptions on e-commerce websites often suffer from missing\nimportant aspects. Clarification question generation (CQGen) can be a promising\napproach to help alleviate the problem. Unlike traditional QGen assuming the\nexistence of answers in the context and generating questions accordingly, CQGen\nmimics user behaviors of asking for unstated information. The generated CQs can\nserve as a sanity check or proofreading to help e-commerce merchant to identify\npotential missing information before advertising their product, and improve\nconsumer experience consequently. Due to the variety of possible user\nbackgrounds and use cases, the information need can be quite diverse but also\nspecific to a detailed topic, while previous works assume generating one CQ per\ncontext and the results tend to be generic. We thus propose the task of Diverse\nCQGen and also tackle the challenge of specificity. We propose a new model\nnamed KPCNet, which generates CQs with Keyword Prediction and Conditioning, to\ndeal with the tasks. Automatic and human evaluation on 2 datasets (Home &\nKitchen, Office) showed that KPCNet can generate more specific questions and\npromote better group-level diversity than several competing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:29:33 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Zhiling", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2104.10336", "submitter": "Haiqin Yang", "authors": "Jian Ma, Shuyi Xie, Haiqin Yang, Lianxin Jiang, Mengyuan Zhou, Xiaoyi\n  Ruan, Yang Mo", "title": "MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor\n  Based on Multi-Task Adversarial Training", "comments": "7 pages, 1 figure, 4 tables", "journal-ref": "A system for SemEval-2021 Task 7", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes MagicPai's system for SemEval 2021 Task 7, HaHackathon:\nDetecting and Rating Humor and Offense. This task aims to detect whether the\ntext is humorous and how humorous it is. There are four subtasks in the\ncompetition. In this paper, we mainly present our solution, a multi-task\nlearning model based on adversarial examples, for task 1a and 1b. More\nspecifically, we first vectorize the cleaned dataset and add the perturbation\nto obtain more robust embedding representations. We then correct the loss via\nthe confidence level. Finally, we perform interactive joint learning on\nmultiple tasks to capture the relationship between whether the text is humorous\nand how humorous it is. The final result shows the effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:23:02 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:41:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ma", "Jian", ""], ["Xie", "Shuyi", ""], ["Yang", "Haiqin", ""], ["Jiang", "Lianxin", ""], ["Zhou", "Mengyuan", ""], ["Ruan", "Xiaoyi", ""], ["Mo", "Yang", ""]]}, {"id": "2104.10339", "submitter": "Qian Chen", "authors": "Qian Chen, Wen Wang, Mengzhe Chen, Qinglin Zhang", "title": "Discriminative Self-training for Punctuation Prediction", "comments": "Accepted by INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Punctuation prediction for automatic speech recognition (ASR) output\ntranscripts plays a crucial role for improving the readability of the ASR\ntranscripts and for improving the performance of downstream natural language\nprocessing applications. However, achieving good performance on punctuation\nprediction often requires large amounts of labeled speech transcripts, which is\nexpensive and laborious. In this paper, we propose a Discriminative\nSelf-Training approach with weighted loss and discriminative label smoothing to\nexploit unlabeled speech transcripts. Experimental results on the English\nIWSLT2011 benchmark test set and an internal Chinese spoken language dataset\ndemonstrate that the proposed approach achieves significant improvement on\npunctuation prediction accuracy over strong baselines including BERT, RoBERTa,\nand ELECTRA models. The proposed Discriminative Self-Training approach\noutperforms the vanilla self-training approach. We establish a new\nstate-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current\nSOTA model by 1.3% absolute gain on F$_1$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:32:47 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:18:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chen", "Qian", ""], ["Wang", "Wen", ""], ["Chen", "Mengzhe", ""], ["Zhang", "Qinglin", ""]]}, {"id": "2104.10343", "submitter": "Michael Hahn", "authors": "Michael Hahn, Dan Jurafsky, Richard Futrell", "title": "Sensitivity as a Complexity Measure for Sequence Classification Tasks", "comments": "Accepted by TACL. This is a pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a theoretical framework for understanding and predicting the\ncomplexity of sequence classification tasks, using a novel extension of the\ntheory of Boolean function sensitivity. The sensitivity of a function, given a\ndistribution over input sequences, quantifies the number of disjoint subsets of\nthe input sequence that can each be individually changed to change the output.\nWe argue that standard sequence classification methods are biased towards\nlearning low-sensitivity functions, so that tasks requiring high sensitivity\nare more difficult. To that end, we show analytically that simple lexical\nclassifiers can only express functions of bounded sensitivity, and we show\nempirically that low-sensitivity functions are easier to learn for LSTMs. We\nthen estimate sensitivity on 15 NLP tasks, finding that sensitivity is higher\non challenging tasks collected in GLUE than on simple text classification\ntasks, and that sensitivity predicts the performance both of simple lexical\nclassifiers and of vanilla BiLSTMs without pretrained contextualized\nembeddings. Within a task, sensitivity predicts which inputs are hard for such\nsimple models. Our results suggest that the success of massively pretrained\ncontextual representations stems in part because they provide representations\nfrom which information can be extracted by low-sensitivity decoders.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:56:59 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hahn", "Michael", ""], ["Jurafsky", "Dan", ""], ["Futrell", "Richard", ""]]}, {"id": "2104.10344", "submitter": "Zheng Yuan", "authors": "Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang, Fei Huang", "title": "Improving Biomedical Pretrained Language Models with Knowledge", "comments": "Accepted at BioNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models have shown success in many natural language\nprocessing tasks. Many works explore incorporating knowledge into language\nmodels. In the biomedical domain, experts have taken decades of effort on\nbuilding large-scale knowledge bases. For example, the Unified Medical Language\nSystem (UMLS) contains millions of entities with their synonyms and defines\nhundreds of relations among entities. Leveraging this knowledge can benefit a\nvariety of downstream tasks such as named entity recognition and relation\nextraction. To this end, we propose KeBioLM, a biomedical pretrained language\nmodel that explicitly leverages knowledge from the UMLS knowledge bases.\nSpecifically, we extract entities from PubMed abstracts and link them to UMLS.\nWe then train a knowledge-aware language model that firstly applies a text-only\nencoding layer to learn entity representation and applies a text-entity fusion\nencoding to aggregate entity representation. Besides, we add two training\nobjectives as entity detection and entity linking. Experiments on the named\nentity recognition and relation extraction from the BLURB benchmark demonstrate\nthe effectiveness of our approach. Further analysis on a collected probing\ndataset shows that our model has better ability to model medical knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:57:26 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yuan", "Zheng", ""], ["Liu", "Yijia", ""], ["Tan", "Chuanqi", ""], ["Huang", "Songfang", ""], ["Huang", "Fei", ""]]}, {"id": "2104.10355", "submitter": "Wei-Lun Chao", "authors": "Jihyung Kil, Wei-Lun Chao", "title": "Revisiting Document Representations for Large-Scale Zero-Shot Learning", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Zero-shot learning aims to recognize unseen objects using their semantic\nrepresentations. Most existing works use visual attributes labeled by humans,\nnot suitable for large-scale applications. In this paper, we revisit the use of\ndocuments as semantic representations. We argue that documents like Wikipedia\npages contain rich visual information, which however can easily be buried by\nthe vast amount of non-visual sentences. To address this issue, we propose a\nsemi-automatic mechanism for visual sentence extraction that leverages the\ndocument section headers and the clustering structure of visual sentences. The\nextracted visual sentences, after a novel weighting scheme to distinguish\nsimilar classes, essentially form semantic representations like visual\nattributes but need much less human effort. On the ImageNet dataset with over\n10,000 unseen classes, our representations lead to a 64% relative improvement\nagainst the commonly used ones.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 05:17:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kil", "Jihyung", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2104.10357", "submitter": "Qian Chen", "authors": "Qian Chen, Wen Wang, Qinglin Zhang", "title": "Pre-training for Spoken Language Understanding with Joint Textual and\n  Phonetic Representation Learning", "comments": "Accepted by INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the traditional cascading architecture for spoken language understanding\n(SLU), it has been observed that automatic speech recognition errors could be\ndetrimental to the performance of natural language understanding. End-to-end\n(E2E) SLU models have been proposed to directly map speech input to desired\nsemantic frame with a single model, hence mitigating ASR error propagation.\nRecently, pre-training technologies have been explored for these E2E models. In\nthis paper, we propose a novel joint textual-phonetic pre-training approach for\nlearning spoken language representations, aiming at exploring the full\npotentials of phonetic information to improve SLU robustness to ASR errors. We\nexplore phoneme labels as high-level speech features, and design and compare\npre-training tasks based on conditional masked language model objectives and\ninter-sentence relation objectives. We also investigate the efficacy of\ncombining textual and phonetic information during fine-tuning. Experimental\nresults on spoken language understanding benchmarks, Fluent Speech Commands and\nSNIPS, show that the proposed approach significantly outperforms strong\nbaseline models and improves robustness of spoken language understanding to ASR\nerrors.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 05:19:13 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:45:52 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chen", "Qian", ""], ["Wang", "Wen", ""], ["Zhang", "Qinglin", ""]]}, {"id": "2104.10366", "submitter": "Haiqin Yang", "authors": "Xiaoyi Ruan, Meizhi Jin, Jian Ma, Haiqin Yang, Lianxin Jiang, Yang Mo,\n  Mengyuan Zhou", "title": "Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement\n  Verification and Evidence Finding with Tables", "comments": "7 pages, 1 figure, 3 tables, the champion solution for SemEval-21\n  Task 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Question answering from semi-structured tables can be seen as a semantic\nparsing task and is significant and practical for pushing the boundary of\nnatural language understanding. Existing research mainly focuses on\nunderstanding contents from unstructured evidence, e.g., news, natural language\nsentences, and documents. The task of verification from structured evidence,\nsuch as tables, charts, and databases, is still less explored. This paper\ndescribes sattiy team's system in SemEval-2021 task 9: Statement Verification\nand Evidence Finding with Tables (SEM-TAB-FACT). This competition aims to\nverify statements and to find evidence from tables for scientific articles and\nto promote the proper interpretation of the surrounding article. In this paper,\nwe exploited ensemble models of pre-trained language models over tables, TaPas\nand TaBERT, for Task A and adjust the result based on some rules extracted for\nTask B. Finally, in the leaderboard, we attain the F1 scores of 0.8496 and\n0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1\nscore of 0.4856 in Task B.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:11:49 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:39:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ruan", "Xiaoyi", ""], ["Jin", "Meizhi", ""], ["Ma", "Jian", ""], ["Yang", "Haiqin", ""], ["Jiang", "Lianxin", ""], ["Mo", "Yang", ""], ["Zhou", "Mengyuan", ""]]}, {"id": "2104.10375", "submitter": "Haiqin Yang", "authors": "Shuyi Xie, Jian Ma, Haiqin Yang, Lianxin Jiang, Yang Mo, Jianping Shen", "title": "PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context\n  Disambiguation", "comments": "7 pages, 2 figures, 2 tables, winning solution on En-Ar, En-Fr,\n  En-Ru, and En-Zh cross-lingual tasks of SemEval'21 Task 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the PALI team's winning system for SemEval-2021 Task 2:\nMultilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune\nXLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to\ndetermine whether the target word in the two contexts contains the same meaning\nor not. In the implementation, we first specifically design an input tag to\nemphasize the target word in the contexts. Second, we construct a new vector on\nthe fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected\nnetwork to output the probability of whether the target word in the context has\nthe same meaning or not. The new vector is attained by concatenating the\nembedding of the [CLS] token and the embeddings of the target word in the\ncontexts. In training, we explore several tricks, such as the Ranger optimizer,\ndata augmentation, and adversarial training, to improve the model prediction.\nConsequently, we attain first place in all four cross-lingual tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:24:49 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:35:35 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Shuyi", ""], ["Ma", "Jian", ""], ["Yang", "Haiqin", ""], ["Jiang", "Lianxin", ""], ["Mo", "Yang", ""], ["Shen", "Jianping", ""]]}, {"id": "2104.10380", "submitter": "Rong Ye", "authors": "Rong Ye, Mingxuan Wang, Lei Li", "title": "End-to-end Speech Translation via Cross-modal Progressive Training", "comments": "Accepted at InterSpeech2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end speech translation models have become a new trend in research due\nto their potential of reducing error propagation. However, these models still\nsuffer from the challenge of data scarcity. How to effectively use unlabeled or\nother parallel corpora from machine translation is promising but still an open\nproblem. In this paper, we propose Cross Speech-Text Network (XSTNet), an\nend-to-end model for speech-to-text translation. XSTNet takes both speech and\ntext as input and outputs both transcription and translation text. The model\nbenefits from its three key design aspects: a self-supervised pre-trained\nsub-network as the audio encoder, a multi-task training objective to exploit\nadditional parallel bilingual text, and a progressive training procedure. We\nevaluate the performance of XSTNet and baselines on the MuST-C En-X and\nLibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art\nresults on all language directions with an average BLEU of 28.8, outperforming\nthe previous best method by 3.2 BLEU. Code, models, cases, and more detailed\nanalysis are available at https://github.com/ReneeYe/XSTNet.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:44:31 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 04:01:23 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ye", "Rong", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2104.10408", "submitter": "Roman Grundkiewicz", "authors": "Roman Grundkiewicz, Marcin Junczys-Dowmunt, Christian Federmann and\n  Tom Kocmi", "title": "On User Interfaces for Large-Scale Document-Level Human Evaluation of\n  Machine Translation Outputs", "comments": "Presented at HumEval, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies emphasize the need of document context in human evaluation of\nmachine translations, but little research has been done on the impact of user\ninterfaces on annotator productivity and the reliability of assessments. In\nthis work, we compare human assessment data from the last two WMT evaluation\ncampaigns collected via two different methods for document-level evaluation.\nOur analysis shows that a document-centric approach to evaluation where the\nannotator is presented with the entire document context on a screen leads to\nhigher quality segment and document level assessments. It improves the\ncorrelation between segment and document scores and increases inter-annotator\nagreement for document scores but is considerably more time consuming for\nannotators.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:40:18 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Grundkiewicz", "Roman", ""], ["Junczys-Dowmunt", "Marcin", ""], ["Federmann", "Christian", ""], ["Kocmi", "Tom", ""]]}, {"id": "2104.10424", "submitter": "Saiping Guan", "authors": "Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng", "title": "Link Prediction on N-ary Relational Data Based on Relatedness Evaluation", "comments": "Accepted to TKDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the overwhelming popularity of Knowledge Graphs (KGs), researchers have\npoured attention to link prediction to fill in missing facts for a long time.\nHowever, they mainly focus on link prediction on binary relational data, where\nfacts are usually represented as triples in the form of (head entity, relation,\ntail entity). In practice, n-ary relational facts are also ubiquitous. When\nencountering such facts, existing studies usually decompose them into triples\nby introducing a multitude of auxiliary virtual entities and additional\ntriples. These conversions result in the complexity of carrying out link\nprediction on n-ary relational data. It has even proven that they may cause\nloss of structure information. To overcome these problems, in this paper, we\nrepresent each n-ary relational fact as a set of its role and role-value pairs.\nWe then propose a method called NaLP to conduct link prediction on n-ary\nrelational data, which explicitly models the relatedness of all the role and\nrole-value pairs in an n-ary relational fact. We further extend NaLP by\nintroducing type constraints of roles and role-values without any external\ntype-specific supervision, and proposing a more reasonable negative sampling\nmechanism. Experimental results validate the effectiveness and merits of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:06:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Guan", "Saiping", ""], ["Jin", "Xiaolong", ""], ["Guo", "Jiafeng", ""], ["Wang", "Yuanzhuo", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.10441", "submitter": "Tim Isbister", "authors": "Tim Isbister, Fredrik Carlsson, Magnus Sahlgren", "title": "Should we Stop Training More Monolingual Models, and Simply Use Machine\n  Translation Instead?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most work in NLP makes the assumption that it is desirable to develop\nsolutions in the native language in question. There is consequently a strong\ntrend towards building native language models even for low-resource languages.\nThis paper questions this development, and explores the idea of simply\ntranslating the data into English, thereby enabling the use of pretrained, and\nlarge-scale, English language models. We demonstrate empirically that a large\nEnglish language model coupled with modern machine translation outperforms\nnative language models in most Scandinavian languages. The exception to this is\nFinnish, which we assume is due to inferior translation quality. Our results\nsuggest that machine translation is a mature technology, which raises a serious\ncounter-argument for training native language models for low-resource\nlanguages. This paper therefore strives to make a provocative but important\npoint. As English language models are improving at an unprecedented pace, which\nin turn improves machine translation, it is from an empirical and environmental\nstand-point more effective to translate data from low-resource languages into\nEnglish, than to build language models for such languages.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:21:24 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Isbister", "Tim", ""], ["Carlsson", "Fredrik", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "2104.10454", "submitter": "Petr Marek", "authors": "Petr Marek, \\v{S}t\\v{e}p\\'an M\\\"uller, Jakub Konr\\'ad, Petr Lorenc,\n  Jan Pichl and Jan \\v{S}ediv\\'y", "title": "Text Summarization of Czech News Articles Using Named Entities", "comments": null, "journal-ref": "The Prague Bulletin of Mathematical Linguistics 2021 116", "doi": "10.14712/00326585.012", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The foundation for the research of summarization in the Czech language was\nlaid by the work of Straka et al. (2018). They published the SumeCzech, a large\nCzech news-based summarization dataset, and proposed several baseline\napproaches. However, it is clear from the achieved results that there is a\nlarge space for improvement. In our work, we focus on the impact of named\nentities on the summarization of Czech news articles. First, we annotate\nSumeCzech with named entities. We propose a new metric ROUGE_NE that measures\nthe overlap of named entities between the true and generated summaries, and we\nshow that it is still challenging for summarization systems to reach a high\nscore in it. We propose an extractive summarization approach Named Entity\nDensity that selects a sentence with the highest ratio between a number of\nentities and the length of the sentence as the summary of the article. The\nexperiments show that the proposed approach reached results close to the solid\nbaseline in the domain of news articles selecting the first sentence. Moreover,\nwe demonstrate that the selected sentence reflects the style of reports\nconcisely identifying to whom, when, where, and what happened. We propose that\nsuch a summary is beneficial in combination with the first sentence of an\narticle in voice applications presenting news articles. We propose two\nabstractive summarization approaches based on Seq2Seq architecture. The first\napproach uses the tokens of the article. The second approach has access to the\nnamed entity annotations. The experiments show that both approaches exceed\nstate-of-the-art results previously reported by Straka et al. (2018), with the\nlatter achieving slightly better results on SumeCzech's out-of-domain testing\nset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:48:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Marek", "Petr", ""], ["M\u00fcller", "\u0160t\u011bp\u00e1n", ""], ["Konr\u00e1d", "Jakub", ""], ["Lorenc", "Petr", ""], ["Pichl", "Jan", ""], ["\u0160ediv\u00fd", "Jan", ""]]}, {"id": "2104.10493", "submitter": "Shogo Ujiie", "authors": "Shogo Ujiie, Hayate Iso, Shuntaro Yada, Shoko Wakamiya, Eiji Aramaki", "title": "End-to-end Biomedical Entity Linking with Span-based Dictionary Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease name recognition and normalization, which is generally called\nbiomedical entity linking, is a fundamental process in biomedical text mining.\nRecently, neural joint learning of both tasks has been proposed to utilize the\nmutual benefits. While this approach achieves high performance, disease\nconcepts that do not appear in the training dataset cannot be accurately\npredicted. This study introduces a novel end-to-end approach that combines span\nrepresentations with dictionary-matching features to address this problem. Our\nmodel handles unseen concepts by referring to a dictionary while maintaining\nthe performance of neural network-based models, in an end-to-end fashion.\nExperiments using two major datasets demonstrate that our model achieved\ncompetitive results with strong baselines, especially for unseen concepts\nduring training.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:24:12 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ujiie", "Shogo", ""], ["Iso", "Hayate", ""], ["Yada", "Shuntaro", ""], ["Wakamiya", "Shoko", ""], ["Aramaki", "Eiji", ""]]}, {"id": "2104.10507", "submitter": "Yingbo Gao", "authors": "Yingbo Gao, David Thulke, Alexander Gerstenberger, Khoa Viet Tran,\n  Ralf Schl\\\"uter, Hermann Ney", "title": "On Sampling-Based Training Criteria for Neural Language Modeling", "comments": "Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the vocabulary size of modern word-based language models becomes ever\nlarger, many sampling-based training criteria are proposed and investigated.\nThe essence of these sampling methods is that the softmax-related traversal\nover the entire vocabulary can be simplified, giving speedups compared to the\nbaseline. A problem we notice about the current landscape of such sampling\nmethods is the lack of a systematic comparison and some myths about preferring\none over another. In this work, we consider Monte Carlo sampling, importance\nsampling, a novel method we call compensated partial summation, and noise\ncontrastive estimation. Linking back to the three traditional criteria, namely\nmean squared error, binary cross-entropy, and cross-entropy, we derive the\ntheoretical solutions to the training problems. Contrary to some common belief,\nwe show that all these sampling methods can perform equally well, as long as we\ncorrect for the intended class posterior probabilities. Experimental results in\nlanguage modeling and automatic speech recognition on Switchboard and\nLibriSpeech support our claim, with all sampling-based methods showing similar\nperplexities and word error rates while giving the expected speedups.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:55:52 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 10:21:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Gao", "Yingbo", ""], ["Thulke", "David", ""], ["Gerstenberger", "Alexander", ""], ["Tran", "Khoa Viet", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.10513", "submitter": "Soroosh Tayebi Arasteh", "authors": "Soroosh Tayebi Arasteh, Mehrpad Monajem, Vincent Christlein, Philipp\n  Heinrich, Anguelos Nicolaou, Hamidreza Naderi Boldaji, Mahshad Lotfinia,\n  Stefan Evert", "title": "How Will Your Tweet Be Received? Predicting the Sentiment Polarity of\n  Tweet Replies", "comments": "Published in 2021 IEEE 15th International Conference on Semantic\n  Computing (ICSC)", "journal-ref": "2021 IEEE 15th International Conference on Semantic Computing\n  (ICSC), Laguna Hills, CA, USA, 2021, pp. 356-359", "doi": "10.1109/ICSC50631.2021.00068", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Twitter sentiment analysis, which often focuses on predicting the polarity of\ntweets, has attracted increasing attention over the last years, in particular\nwith the rise of deep learning (DL). In this paper, we propose a new task:\npredicting the predominant sentiment among (first-order) replies to a given\ntweet. Therefore, we created RETWEET, a large dataset of tweets and replies\nmanually annotated with sentiment labels. As a strong baseline, we propose a\ntwo-stage DL-based method: first, we create automatically labeled training data\nby applying a standard sentiment classifier to tweet replies and aggregating\nits predictions for each original tweet; our rationale is that individual\nerrors made by the classifier are likely to cancel out in the aggregation step.\nSecond, we use the automatically labeled data for supervised training of a\nneural network to predict reply sentiment from the original tweets. The\nresulting classifier is evaluated on the new RETWEET dataset, showing promising\nresults, especially considering that it has been trained without any manually\nlabeled data. Both the dataset and the baseline implementation are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:08:45 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Arasteh", "Soroosh Tayebi", ""], ["Monajem", "Mehrpad", ""], ["Christlein", "Vincent", ""], ["Heinrich", "Philipp", ""], ["Nicolaou", "Anguelos", ""], ["Boldaji", "Hamidreza Naderi", ""], ["Lotfinia", "Mahshad", ""], ["Evert", "Stefan", ""]]}, {"id": "2104.10516", "submitter": "Konstantinos Kogkalidis", "authors": "Giorgos Tziafas, Konstantinos Kogkalidis, Gijs Wijnholds, Michael\n  Moortgat", "title": "Improving BERT Pretraining with Syntactic Supervision", "comments": "4 pages, rejected by IWCS due to \"not fitting the conference theme\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bidirectional masked Transformers have become the core theme in the current\nNLP landscape. Despite their impressive benchmarks, a recurring theme in recent\nresearch has been to question such models' capacity for syntactic\ngeneralization. In this work, we seek to address this question by adding a\nsupervised, token-level supertagging objective to standard unsupervised\npretraining, enabling the explicit incorporation of syntactic biases into the\nnetwork's training dynamics. Our approach is straightforward to implement,\ninduces a marginal computational overhead and is general enough to adapt to a\nvariety of settings. We apply our methodology on Lassy Large, an automatically\nannotated corpus of written Dutch. Our experiments suggest that our\nsyntax-aware model performs on par with established baselines, despite Lassy\nLarge being one order of magnitude smaller than commonly used corpora.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:15:58 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Tziafas", "Giorgos", ""], ["Kogkalidis", "Konstantinos", ""], ["Wijnholds", "Gijs", ""], ["Moortgat", "Michael", ""]]}, {"id": "2104.10640", "submitter": "Sushant Singh", "authors": "Sushant Singh and Ausif Mahmood", "title": "The NLP Cookbook: Modern Recipes for Transformer based Deep Learning\n  Architectures", "comments": "27 pages and 29 figures", "journal-ref": "IEEE Access (Volume: 9), 2021", "doi": "10.1109/ACCESS.2021.3077350", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, Natural Language Processing (NLP) models have achieved\nphenomenal success in linguistic and semantic tasks like text classification,\nmachine translation, cognitive dialogue systems, information retrieval via\nNatural Language Understanding (NLU), and Natural Language Generation (NLG).\nThis feat is primarily attributed due to the seminal Transformer architecture,\nleading to designs such as BERT, GPT (I, II, III), etc. Although these\nlarge-size models have achieved unprecedented performances, they come at high\ncomputational costs. Consequently, some of the recent NLP architectures have\nutilized concepts of transfer learning, pruning, quantization, and knowledge\ndistillation to achieve moderate model sizes while keeping nearly similar\nperformances as achieved by their predecessors. Additionally, to mitigate the\ndata size challenge raised by language models from a knowledge extraction\nperspective, Knowledge Retrievers have been built to extricate explicit data\ndocuments from a large corpus of databases with greater efficiency and\naccuracy. Recent research has also focused on superior inference by providing\nefficient attention to longer input sequences. In this paper, we summarize and\nexamine the current state-of-the-art (SOTA) NLP models that have been employed\nfor numerous NLP tasks for optimal performance and efficiency. We provide a\ndetailed understanding and functioning of the different architectures, a\ntaxonomy of NLP designs, comparative evaluations, and future directions in NLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:38:20 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 05:38:37 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 17:31:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Singh", "Sushant", ""], ["Mahmood", "Ausif", ""]]}, {"id": "2104.10649", "submitter": "Ruiqing Yan", "authors": "Ruiqing Yan, Lanchang Sun, Fang Wang, Xiaoming Zhang", "title": "K-XLNet: A General Method for Combining Explicit Knowledge with Language\n  Model Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though pre-trained language models such as Bert and XLNet, have rapidly\nadvanced the state-of-the-art on many NLP tasks, they implicit semantics only\nrelying on surface information between words in corpus. Intuitively, background\nknowledge influences the efficacy of understanding. Inspired by this common\nsense, we focus on improving model pretraining by leveraging explicit\nknowledge. Different from recent research that optimize pretraining model by\nknowledge masking strategies, we propose a simple but general method to combine\nexplicit knowledge with pretraining. To be specific, we first match knowledge\nfacts from knowledge graph (KG) and then add a knowledge injunction layer to\ntransformer directly without changing its architecture. The present study seeks\nto find the direct impact of explicit knowledge on transformer per-training. We\nconduct experiments on various datasets for different downstream tasks. The\nexperimental results show that solely by adding external knowledge to\ntransformer can improve the learning performance on many NLP tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 06:14:18 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 10:08:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yan", "Ruiqing", ""], ["Sun", "Lanchang", ""], ["Wang", "Fang", ""], ["Zhang", "Xiaoming", ""]]}, {"id": "2104.10652", "submitter": "Biplob Biswas", "authors": "Biplob Biswas, Thai-Hoang Pham, Ping Zhang", "title": "TransICD: Transformer Based Code-wise Attention Model for Explainable\n  ICD Coding", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International Classification of Disease (ICD) coding procedure which refers\nto tagging medical notes with diagnosis codes has been shown to be effective\nand crucial to the billing system in medical sector. Currently, ICD codes are\nassigned to a clinical note manually which is likely to cause many errors.\nMoreover, training skilled coders also requires time and human resources.\nTherefore, automating the ICD code determination process is an important task.\nWith the advancement of artificial intelligence theory and computational\nhardware, machine learning approach has emerged as a suitable solution to\nautomate this process. In this project, we apply a transformer-based\narchitecture to capture the interdependence among the tokens of a document and\nthen use a code-wise attention mechanism to learn code-specific representations\nof the entire document. Finally, they are fed to separate dense layers for\ncorresponding code prediction. Furthermore, to handle the imbalance in the code\nfrequency of clinical datasets, we employ a label distribution aware margin\n(LDAM) loss function. The experimental results on the MIMIC-III dataset show\nthat our proposed model outperforms other baselines by a significant margin. In\nparticular, our best setting achieves a micro-AUC score of 0.923 compared to\n0.868 of bidirectional recurrent neural networks. We also show that by using\nthe code-wise attention mechanism, the model can provide more insights about\nits prediction, and thus it can support clinicians to make reliable decisions.\nOur code is available online (https://github.com/biplob1ly/TransICD)\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 05:34:32 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Biswas", "Biplob", ""], ["Pham", "Thai-Hoang", ""], ["Zhang", "Ping", ""]]}, {"id": "2104.10658", "submitter": "Dewayne Whitfield", "authors": "Dewayne Whitfield", "title": "Using GPT-2 to Create Synthetic Data to Improve the Prediction\n  Performance of NLP Machine Learning Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification Models use input data to predict the likelihood that the\nsubsequent input data will fall into predetermined categories. To perform\neffective classifications, these models require large datasets for training. It\nis becoming common practice to utilize synthetic data to boost the performance\nof Machine Learning Models. It is reported that Shell is using synthetic data\nto build models to detect problems that rarely occur; for example Shell created\nsynthetic data to help models to identify deteriorating oil lines. It is common\npractice for Machine Learning Practitioners to generate synthetic data by\nrotating, flipping, and cropping images to increase the volume of image data to\ntrain Convolutional Neural Networks. The purpose of this paper is to explore\ncreating and utilizing synthetic NLP data to improve the performance of Natural\nLanguage Processing Machine Learning Classification Models. In this paper I\nused a Yelp pizza restaurant reviews dataset and transfer learning to fine-tune\na pre-trained GPT-2 Transformer Model to generate synthetic pizza reviews data.\nI then combined this synthetic data with the original genuine data to create a\nnew joint dataset. The new combined model significantly outperformed the\noriginal model in accuracy and precision.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 20:20:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Whitfield", "Dewayne", ""]]}, {"id": "2104.10660", "submitter": "Sayyed Ali Hossayni", "authors": "Yousef Alizadeh-Q, Behrouz Minaei-Bidgoli, Sayyed-Ali Hossayni,\n  Mohammad-R Akbarzadeh-T, Diego Reforgiato Recupero, Mohammad-Reza Rajati,\n  Aldo Gangemi", "title": "Interval Probabilistic Fuzzy WordNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WordNet lexical-database groups English words into sets of synonyms called\n\"synsets.\" Synsets are utilized for several applications in the field of\ntext-mining. However, they were also open to criticism because although, in\nreality, not all the members of a synset represent the meaning of that synset\nwith the same degree, in practice, they are considered as members of the\nsynset, identically. Thus, the fuzzy version of synsets, called fuzzy-synsets\n(or fuzzy word-sense classes) were proposed and studied. In this study, we\ndiscuss why (type-1) fuzzy synsets (T1 F-synsets) do not properly model the\nmembership uncertainty, and propose an upgraded version of fuzzy synsets in\nwhich membership degrees of word-senses are represented by intervals, similar\nto what in Interval Type 2 Fuzzy Sets (IT2 FS) and discuss that IT2 FS\ntheoretical framework is insufficient for analysis and design of such synsets,\nand propose a new concept, called Interval Probabilistic Fuzzy (IPF) sets. Then\nwe present an algorithm for constructing the IPF synsets in any language, given\na corpus and a word-sense-disambiguation system. Utilizing our algorithm and\nthe open-American-online-corpus (OANC) and UKB word-sense-disambiguation, we\nconstructed and published the IPF synsets of WordNet for English language.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 17:28:37 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Alizadeh-Q", "Yousef", ""], ["Minaei-Bidgoli", "Behrouz", ""], ["Hossayni", "Sayyed-Ali", ""], ["Akbarzadeh-T", "Mohammad-R", ""], ["Recupero", "Diego Reforgiato", ""], ["Rajati", "Mohammad-Reza", ""], ["Gangemi", "Aldo", ""]]}, {"id": "2104.10661", "submitter": "Houjun Liu", "authors": "Houjun Liu", "title": "Towards Automated Psychotherapy via Language Modeling", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this experiment, a model was devised, trained, and evaluated to automate\npsychotherapist/client text conversations through the use of state-of-the-art,\nSeq2Seq Transformer-based Natural Language Generation (NLG) systems. Through\ntraining the model upon a mix of the Cornell Movie Dialogue Corpus for language\nunderstanding and an open-source, anonymized, and public licensed\npsychotherapeutic dataset, the model achieved statistically significant\nperformance in published, standardized qualitative benchmarks against\nhuman-written validation data - meeting or exceeding human-written responses'\nperformance in 59.7% and 67.1% of the test set for two independent test methods\nrespectively. Although the model cannot replace the work of psychotherapists\nentirely, its ability to synthesize human-appearing utterances for the majority\nof the test set serves as a promising step towards communizing and easing\nstigma at the psychotherapeutic point-of-care.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 01:53:39 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "Houjun", ""]]}, {"id": "2104.10662", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Aswin Krishna", "title": "COVID-19 sentiment analysis via deep learning during the rise of novel\n  cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social scientists and psychologists take interest in understanding how people\nexpress emotions or sentiments when dealing with catastrophic events such as\nnatural disasters, political unrest, and terrorism. The COVID-19 pandemic is a\ncatastrophic event that has raised a number of psychological issues such as\ndepression given abrupt social changes and lack of employment. During the rise\nof COVID-19 cases with stricter lock downs, people have been expressing their\nsentiments in social media which can provide a deep understanding of how people\nphysiologically react to catastrophic events. In this paper, we use deep\nlearning based language models via long short-term memory (LSTM) recurrent\nneural networks for sentiment analysis on Twitter with a focus of rise of novel\ncases in India. We use the LSTM model with a global vector (GloVe) for word\nrepresentation in building a language model. We review the sentiments expressed\nfor selective months covering the major peak of new cases in 2020. We present a\nframework that focuses on multi-label sentiment classification using LSTM model\nand GloVe embedding, where more than one sentiment can be expressed at once.\nOur results show that the majority of the tweets have been positive with high\nlevels of optimism during the rise of the COVID-19 cases in India. We find that\nthe number of tweets significantly lowered towards the peak of new cases. We\nfind that the optimistic and joking tweets mostly dominated the monthly tweets\nand there was a much lower number of negative sentiments expressed. This could\nimply that the majority were generally positive and some annoyed towards the\nway the pandemic was handled by the authorities as their peak was reached.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:31:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chandra", "Rohitash", ""], ["Krishna", "Aswin", ""]]}, {"id": "2104.10726", "submitter": "Xiaoyu Shen", "authors": "Jidong Ge, Yunyun huang, Xiaoyu Shen, Chuanyi Li, Wei Hu and Bin Luo", "title": "Learning Fine-grained Fact-Article Correspondence in Legal Cases", "comments": "Code and dataset are available at https://github.com/gjdnju/MLMN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically recommending relevant law articles to a given legal case has\nattracted much attention as it can greatly release human labor from searching\nover the large database of laws. However, current researches only support\ncoarse-grained recommendation where all relevant articles are predicted as a\nwhole without explaining which specific fact each article is relevant with.\nSince one case can be formed of many supporting facts, traversing over them to\nverify the correctness of recommendation results can be time-consuming. We\nbelieve that learning fine-grained correspondence between each single fact and\nlaw articles is crucial for an accurate and trustworthy AI system. With this\nmotivation, we perform a pioneering study and create a corpus with manually\nannotated fact-article correspondences. We treat the learning as a text\nmatching task and propose a multi-level matching network to address it. To help\nthe model better digest the content of law articles, we parse articles in form\nof premise-conclusion pairs with random forest. Experiments show that the\nparsed form yielded better performance and the resulting model surpassed other\npopular text matching baselines. Furthermore, we compare with previous\nresearches and find that establishing the fine-grained fact-article\ncorrespondences can improve the recommendation accuracy by a large margin. Our\nbest system reaches an F1 score of 96.3%, making it of great potential for\npractical use. It can also significantly boost the downstream task of legal\ndecision prediction, increasing the F1 score by up to 12.7%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:06:58 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 11:11:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ge", "Jidong", ""], ["huang", "Yunyun", ""], ["Shen", "Xiaoyu", ""], ["Li", "Chuanyi", ""], ["Hu", "Wei", ""], ["Luo", "Bin", ""]]}, {"id": "2104.10747", "submitter": "Arthur Hinsvark", "authors": "Arthur Hinsvark (1), Natalie Delworth (1), Miguel Del Rio (1), Quinten\n  McNamara (1), Joshua Dong (1), Ryan Westerman (1), Michelle Huang (1), Joseph\n  Palakapilly (1), Jennifer Drexler (1), Ilya Pirkin (1), Nishchal Bhandari\n  (1), Miguel Jette (1) ((1) Rev.com)", "title": "Accented Speech Recognition: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition (ASR) systems generalize poorly on accented\nspeech. The phonetic and linguistic variability of accents present hard\nchallenges for ASR systems today in both data collection and modeling\nstrategies. The resulting bias in ASR performance across accents comes at a\ncost to both users and providers of ASR.\n  We present a survey of current promising approaches to accented speech\nrecognition and highlight the key challenges in the space. Approaches mostly\nfocus on single model generalization and accent feature engineering. Among the\nchallenges, lack of a standard benchmark makes research and comparison\nespecially difficult.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:21:06 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 14:58:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hinsvark", "Arthur", "", "Rev.com"], ["Delworth", "Natalie", "", "Rev.com"], ["Del Rio", "Miguel", "", "Rev.com"], ["McNamara", "Quinten", "", "Rev.com"], ["Dong", "Joshua", "", "Rev.com"], ["Westerman", "Ryan", "", "Rev.com"], ["Huang", "Michelle", "", "Rev.com"], ["Palakapilly", "Joseph", "", "Rev.com"], ["Drexler", "Jennifer", "", "Rev.com"], ["Pirkin", "Ilya", "", "Rev.com"], ["Bhandari", "Nishchal", "", "Rev.com"], ["Jette", "Miguel", "", "Rev.com"]]}, {"id": "2104.10748", "submitter": "Laura O. Moraes", "authors": "Laura O. Moraes, Carlos Eduardo Pedreira", "title": "Clustering Introductory Computer Science Exercises Using Topic Modeling\n  Methods", "comments": "13 pages, 11 figures, published in IEEE Transactions on Learning\n  Technologies", "journal-ref": "IEEE Transactions on Learning Technologies, vol. 14, no. 1, pp.\n  42-54, Feb. 2021", "doi": "10.1109/TLT.2021.3056907", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manually determining concepts present in a group of questions is a\nchallenging and time-consuming process. However, the process is an essential\nstep while modeling a virtual learning environment since a mapping between\nconcepts and questions using mastery level assessment and recommendation\nengines are required. We investigated unsupervised semantic models (known as\ntopic modeling techniques) to assist computer science teachers in this task and\npropose a method to transform Computer Science 1 teacher-provided code\nsolutions into representative text documents, including the code structure\ninformation. By applying non-negative matrix factorization and latent Dirichlet\nallocation techniques, we extract the underlying relationship between questions\nand validate the results using an external dataset. We consider the\ninterpretability of the learned concepts using 14 university professors' data,\nand the results confirm six semantically coherent clusters using the current\ndataset. Moreover, the six topics comprise the main concepts present in the\ntest dataset, achieving 0.75 in the normalized pointwise mutual information\nmetric. The metric correlates with human ratings, making the proposed method\nuseful and providing semantics for large amounts of unannotated code.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:23:53 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Moraes", "Laura O.", ""], ["Pedreira", "Carlos Eduardo", ""]]}, {"id": "2104.10764", "submitter": "Lu Huang", "authors": "Lu Huang, Jingyu Sun, Yufeng Tang, Junfeng Hou, Jinkun Chen, Jun\n  Zhang, Zejun Ma", "title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer", "comments": "Accepted by Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes an encoder pre-training procedure using frame-wise label\nto improve the training of streaming recurrent neural network transducer\n(RNN-T) model. Streaming RNN-T trained from scratch usually performs worse than\nnon-streaming RNN-T. Although it is common to address this issue through\npre-training components of RNN-T with other criteria or frame-wise alignment\nguidance, the alignment is not easily available in end-to-end manner. In this\nwork, frame-wise alignment, used to pre-train streaming RNN-T's encoder, is\ngenerated without using a HMM-based system. Therefore an all-neural framework\nequipping HMM-free encoder pre-training is constructed. This is achieved by\nexpanding the spikes of CTC model to their left/right blank frames, and two\nexpanding strategies are proposed. To our best knowledge, this is the first\nwork to simulate HMM-based frame-wise label using CTC model for pre-training.\nExperiments conducted on LibriSpeech and MLS English tasks show the proposed\npre-training procedure, compared with random initialization, reduces the WER by\nrelatively 5%~11% and the emission latency by 60 ms. Besides, the method is\nlexicon-free, so it is friendly to new languages without manually designed\nlexicon.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:14:11 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 03:11:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Huang", "Lu", ""], ["Sun", "Jingyu", ""], ["Tang", "Yufeng", ""], ["Hou", "Junfeng", ""], ["Chen", "Jinkun", ""], ["Zhang", "Jun", ""], ["Ma", "Zejun", ""]]}, {"id": "2104.10769", "submitter": "Vicky Zayats", "authors": "Johann C. Rocholl, Vicky Zayats, Daniel D. Walker, Noah B. Murad,\n  Aaron Schneider, Daniel J. Liebling", "title": "Disfluency Detection with Unlabeled Data and Small BERT Models", "comments": "INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disfluency detection models now approach high accuracy on English text.\nHowever, little exploration has been done in improving the size and inference\ntime of the model. At the same time, automatic speech recognition (ASR) models\nare moving from server-side inference to local, on-device inference. Supporting\nmodels in the transcription pipeline (like disfluency detection) must follow\nsuit. In this work we concentrate on the disfluency detection task, focusing on\nsmall, fast, on-device models based on the BERT architecture. We demonstrate it\nis possible to train disfluency detection models as small as 1.3 MiB, while\nretaining high performance. We build on previous work that showed the benefit\nof data augmentation approaches such as self-training. Then, we evaluate the\neffect of domain mismatch between conversational and written text on model\nperformance. We find that domain adaptation and data augmentation strategies\nhave a more pronounced effect on these smaller models, as compared to\nconventional BERT models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 21:24:32 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 16:10:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Rocholl", "Johann C.", ""], ["Zayats", "Vicky", ""], ["Walker", "Daniel D.", ""], ["Murad", "Noah B.", ""], ["Schneider", "Aaron", ""], ["Liebling", "Daniel J.", ""]]}, {"id": "2104.10791", "submitter": "Darshini Mahendran", "authors": "Darshini Mahendran and Bridget T. McInnes", "title": "Extracting Adverse Drug Events from Clinical Notes", "comments": "9 pages, 4 figures", "journal-ref": "American Medical Informatics Association (AMIA)-2021 Virtual\n  Informatics Summit", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adverse drug events (ADEs) are unexpected incidents caused by the\nadministration of a drug or medication. To identify and extract these events,\nwe require information about not just the drug itself but attributes describing\nthe drug (e.g., strength, dosage), the reason why the drug was initially\nprescribed, and any adverse reaction to the drug. This paper explores the\nrelationship between a drug and its associated attributes using relation\nextraction techniques. We explore three approaches: a rule-based approach, a\ndeep learning-based approach, and a contextualized language model-based\napproach. We evaluate our system on the n2c2-2018 ADE extraction dataset. Our\nexperimental results demonstrate that the contextualized language model-based\napproach outperformed other models overall and obtain the state-of-the-art\nperformance in ADE extraction with a Precision of 0.93, Recall of 0.96, and an\n$F_1$ score of 0.94; however, for certain relation types, the rule-based\napproach obtained a higher Precision and Recall than either learning approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 23:10:20 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Mahendran", "Darshini", ""], ["McInnes", "Bridget T.", ""]]}, {"id": "2104.10807", "submitter": "Shayan Fazeli", "authors": "Shayan Fazeli, Davina Zamanzadeh, Anaelia Ovalle, Thu Nguyen, Gilbert\n  Gee, Majid Sarrafzadeh", "title": "COVID-19 and Big Data: Multi-faceted Analysis for Spatio-temporal\n  Understanding of the Pandemic with Social Media Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 has been devastating the world since the end of 2019 and has\ncontinued to play a significant role in major national and worldwide events,\nand consequently, the news. In its wake, it has left no life unaffected. Having\nearned the world's attention, social media platforms have served as a vehicle\nfor the global conversation about COVID-19. In particular, many people have\nused these sites in order to express their feelings, experiences, and\nobservations about the pandemic. We provide a multi-faceted analysis of\ncritical properties exhibited by these conversations on social media regarding\nthe novel coronavirus pandemic. We present a framework for analysis, mining,\nand tracking the critical content and characteristics of social media\nconversations around the pandemic. Focusing on Twitter and Reddit, we have\ngathered a large-scale dataset on COVID-19 social media conversations. Our\nanalyses cover tracking potential reports on virus acquisition, symptoms,\nconversation topics, and language complexity measures through time and by\nregion across the United States. We also present a BERT-based model for\nrecognizing instances of hateful tweets in COVID-19 conversations, which\nachieves a lower error-rate than the state-of-the-art performance. Our results\nprovide empirical validation for the effectiveness of our proposed framework\nand further demonstrate that social media data can be efficiently leveraged to\nprovide public health experts with inexpensive but thorough insight over the\ncourse of an outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 00:45:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Fazeli", "Shayan", ""], ["Zamanzadeh", "Davina", ""], ["Ovalle", "Anaelia", ""], ["Nguyen", "Thu", ""], ["Gee", "Gilbert", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "2104.10809", "submitter": "William Merrill", "authors": "William Merrill, Yoav Goldberg, Roy Schwartz, Noah A. Smith", "title": "Provable Limitations of Acquiring Meaning from Ungrounded Form: What\n  Will Future Language Models Understand?", "comments": "Updated 06/22/21 with substantive changes. Accepted at TACL; pre-MIT\n  Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models trained on billions of tokens have recently led to\nunprecedented results on many NLP tasks. This success raises the question of\nwhether, in principle, a system can ever ``understand'' raw text without access\nto some form of grounding. We formally investigate the abilities of ungrounded\nsystems to acquire meaning. Our analysis focuses on the role of ``assertions'':\ntextual contexts that provide indirect clues about the underlying semantics. We\nstudy whether assertions enable a system to emulate representations preserving\nsemantic relations like equivalence. We find that assertions enable semantic\nemulation of languages that satisfy a strong notion of semantic transparency.\nHowever, for classes of languages where the same expression can take different\nvalues in different contexts, we show that emulation can become uncomputable.\nFinally, we discuss differences between our formal model and natural language,\nexploring how our results generalize to a modal setting and other semantic\nrelations. Together, our results suggest that assertions in code or language do\nnot provide sufficient signal to fully emulate semantic representations. We\nformalize ways in which ungrounded language models appear to be fundamentally\nlimited in their ability to ``understand''.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:00:17 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 16:57:15 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Merrill", "William", ""], ["Goldberg", "Yoav", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah A.", ""]]}, {"id": "2104.10810", "submitter": "Munazza Zaib", "authors": "Munazza Zaib and Quan Z. Sheng and Wei Emma Zhang", "title": "A Short Survey of Pre-trained Language Models for Conversational AI-A\n  NewAge in NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a dialogue system that can communicate naturally with humans is a\nchallenging yet interesting problem of agent-based computing. The rapid growth\nin this area is usually hindered by the long-standing problem of data scarcity\nas these systems are expected to learn syntax, grammar, decision making, and\nreasoning from insufficient amounts of task-specific dataset. The recently\nintroduced pre-trained language models have the potential to address the issue\nof data scarcity and bring considerable advantages by generating contextualized\nword embeddings. These models are considered counterpart of ImageNet in NLP and\nhave demonstrated to capture different facets of language such as hierarchical\nrelations, long-term dependency, and sentiment. In this short survey paper, we\ndiscuss the recent progress made in the field of pre-trained language models.\nWe also deliberate that how the strengths of these language models can be\nleveraged in designing more engaging and more eloquent conversational agents.\nThis paper, therefore, intends to establish whether these pre-trained models\ncan overcome the challenges pertinent to dialogue systems, and how their\narchitecture could be exploited in order to overcome these challenges. Open\nchallenges in the field of dialogue systems have also been deliberated.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:00:56 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zaib", "Munazza", ""], ["Sheng", "Quan Z.", ""], ["Zhang", "Wei Emma", ""]]}, {"id": "2104.10813", "submitter": "Kanishka Misra", "authors": "Kanishka Misra and Julia Taylor Rayz", "title": "Finding Fuzziness in Neural Network Models of Language Processing", "comments": "To appear at NAFIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans often communicate by using imprecise language, suggesting that fuzzy\nconcepts with unclear boundaries are prevalent in language use. In this paper,\nwe test the extent to which models trained to capture the distributional\nstatistics of language show correspondence to fuzzy-membership patterns. Using\nthe task of natural language inference, we test a recent state of the art model\non the classical case of temperature, by examining its mapping of temperature\ndata to fuzzy-perceptions such as \"cool\", \"hot\", etc. We find the model to show\npatterns that are similar to classical fuzzy-set theoretic formulations of\nlinguistic hedges, albeit with a substantial amount of noise, suggesting that\nmodels trained solely on language show promise in encoding fuzziness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:06:14 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Misra", "Kanishka", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2104.10830", "submitter": "Geetanjali Bihani", "authors": "Geetanjali Bihani and Julia Taylor Rayz", "title": "Fuzzy Classification of Multi-intent Utterances", "comments": "NAFIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current intent classification approaches assign binary intent class\nmemberships to natural language utterances while disregarding the inherent\nvagueness in language and the corresponding vagueness in intent class\nboundaries. In this work, we propose a scheme to address the ambiguity in\nsingle-intent as well as multi-intent natural language utterances by creating\ndegree memberships over fuzzified intent classes. To our knowledge, this is the\nfirst work to address and quantify the impact of the fuzzy nature of natural\nlanguage utterances over intent category memberships. Additionally, our\napproach overcomes the sparsity of multi-intent utterance data to train\nclassification models by using a small database of single intent utterances to\ngenerate class memberships over multi-intent utterances. We evaluate our\napproach over two task-oriented dialog datasets, across different fuzzy\nmembership generation techniques and approximate string similarity measures.\nOur results reveal the impact of lexical overlap between utterances of\ndifferent intents, and the underlying data distributions, on the fuzzification\nof intent memberships. Moreover, we evaluate the accuracy of our approach by\ncomparing the defuzzified memberships to their binary counterparts, across\ndifferent combinations of membership functions and string similarity measures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 02:15:56 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bihani", "Geetanjali", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2104.10833", "submitter": "Geetanjali Bihani", "authors": "Geetanjali Bihani and Julia Taylor Rayz", "title": "Low Anisotropy Sense Retrofitting (LASeR) : Towards Isotropic and Sense\n  Enriched Representations", "comments": "DEELIO Workshop@NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual word representation models have shown massive improvements on a\nmultitude of NLP tasks, yet their word sense disambiguation capabilities remain\npoorly explained. To address this gap, we assess whether contextual word\nrepresentations extracted from deep pretrained language models create\ndistinguishable representations for different senses of a given word. We\nanalyze the representation geometry and find that most layers of deep\npretrained language models create highly anisotropic representations, pointing\ntowards the existence of representation degeneration problem in contextual word\nrepresentations. After accounting for anisotropy, our study further reveals\nthat there is variability in sense learning capabilities across different\nlanguage models. Finally, we propose LASeR, a 'Low Anisotropy Sense\nRetrofitting' approach that renders off-the-shelf representations isotropic and\nsemantically more meaningful, resolving the representation degeneration problem\nas a post-processing step, and conducting sense-enrichment of contextualized\nrepresentations extracted from deep neural language models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 02:44:49 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bihani", "Geetanjali", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "2104.10880", "submitter": "Shimin Di", "authors": "Shimin Di, Quanming Yao, Yongqi Zhang, Lei Chen", "title": "Efficient Relation-aware Scoring Function Search for Knowledge Graph\n  Embedding", "comments": "ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scoring function, which measures the plausibility of triplets in\nknowledge graphs (KGs), is the key to ensure the excellent performance of KG\nembedding, and its design is also an important problem in the literature.\nAutomated machine learning (AutoML) techniques have recently been introduced\ninto KG to design task-aware scoring functions, which achieve state-of-the-art\nperformance in KG embedding. However, the effectiveness of searched scoring\nfunctions is still not as good as desired. In this paper, observing that\nexisting scoring functions can exhibit distinct performance on different\nsemantic patterns, we are motivated to explore such semantics by searching\nrelation-aware scoring functions. But the relation-aware search requires a much\nlarger search space than the previous one. Hence, we propose to encode the\nspace as a supernet and propose an efficient alternative minimization algorithm\nto search through the supernet in a one-shot manner. Finally, experimental\nresults on benchmark datasets demonstrate that the proposed method can\nefficiently search relation-aware scoring functions, and achieve better\nembedding performance than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:05:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Di", "Shimin", ""], ["Yao", "Quanming", ""], ["Zhang", "Yongqi", ""], ["Chen", "Lei", ""]]}, {"id": "2104.10899", "submitter": "Heike Adel", "authors": "Heike Adel, Jannik Str\\\"otgen", "title": "Enriched Attention for Robust Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of relation extraction models has increased considerably with\nthe rise of neural networks. However, a key issue of neural relation extraction\nis robustness: the models do not scale well to long sentences with multiple\nentities and relations. In this work, we address this problem with an enriched\nattention mechanism. Attention allows the model to focus on parts of the input\nsentence that are relevant to relation extraction. We propose to enrich the\nattention function with features modeling knowledge about the relation\narguments and the shortest dependency path between them. Thus, for different\nrelation arguments, the model can pay attention to different parts of the\nsentence. Our model outperforms prior work using comparable setups on two\npopular benchmarks, and our analysis confirms that it indeed scales to long\nsentences with many entities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:17:19 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""]]}, {"id": "2104.10925", "submitter": "Junhan Yang", "authors": "Junhan Yang, Zheng Liu, Bowen Jin, Jianxun Lian, Defu Lian, Akshay\n  Soni, Eun Yong Kang, Yajun Wang, Guangzhong Sun, Xing Xie", "title": "Hybrid Encoder: Towards Efficient and Precise Native AdsRecommendation\n  via Hybrid Transformer Encoding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer encoding networks have been proved to be a powerful tool of\nunderstanding natural languages. They are playing a critical role in native ads\nservice, which facilitates the recommendation of appropriate ads based on\nuser's web browsing history. For the sake of efficient recommendation,\nconventional methods would generate user and advertisement embeddings\nindependently with a siamese transformer encoder, such that approximate nearest\nneighbour search (ANN) can be leveraged. Given that the underlying semantic\nabout user and ad can be complicated, such independently generated embeddings\nare prone to information loss, which leads to inferior recommendation quality.\nAlthough another encoding strategy, the cross encoder, can be much more\naccurate, it will lead to huge running cost and become infeasible for realtime\nservices, like native ads recommendation. In this work, we propose hybrid\nencoder, which makes efficient and precise native ads recommendation through\ntwo consecutive steps: retrieval and ranking. In the retrieval step, user and\nad are encoded with a siamese component, which enables relevant candidates to\nbe retrieved via ANN search. In the ranking step, it further represents each ad\nwith disentangled embeddings and each user with ad-related embeddings, which\ncontributes to the fine-grained selection of high-quality ads from the\ncandidate set. Both steps are light-weighted, thanks to the pre-computed and\ncached intermedia results. To optimize the hybrid encoder's performance in this\ntwo-stage workflow, a progressive training pipeline is developed, which builds\nup the model's capability in the retrieval and ranking task step-by-step. The\nhybrid encoder's effectiveness is experimentally verified: with very little\nadditional cost, it outperforms the siamese encoder significantly and achieves\ncomparable recommendation quality as the cross encoder.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 08:42:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yang", "Junhan", ""], ["Liu", "Zheng", ""], ["Jin", "Bowen", ""], ["Lian", "Jianxun", ""], ["Lian", "Defu", ""], ["Soni", "Akshay", ""], ["Kang", "Eun Yong", ""], ["Wang", "Yajun", ""], ["Sun", "Guangzhong", ""], ["Xie", "Xing", ""]]}, {"id": "2104.11030", "submitter": "Shima Khanehzar", "authors": "Shima Khanehzar, Trevor Cohn, Gosia Mikolajczak, Andrew Turpin, Lea\n  Frermann", "title": "Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of\n  Media Frames", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how news media frame political issues is important due to its\nimpact on public attitudes, yet hard to automate. Computational approaches have\nlargely focused on classifying the frame of a full news article while framing\nsignals are often subtle and local. Furthermore, automatic news analysis is a\nsensitive domain, and existing classifiers lack transparency in their\npredictions. This paper addresses both issues with a novel semi-supervised\nmodel, which jointly learns to embed local information about the events and\nrelated actors in a news article through an auto-encoding framework, and to\nleverage this signal for document-level frame classification. Our experiments\nshow that: our model outperforms previous models of frame prediction; we can\nfurther improve performance with unlabeled training data leveraging the\nsemi-supervised nature of our model; and the learnt event and actor embeddings\nintuitively corroborate the document-level predictions, providing a nuanced and\ninterpretable article frame representation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:05:53 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Khanehzar", "Shima", ""], ["Cohn", "Trevor", ""], ["Mikolajczak", "Gosia", ""], ["Turpin", "Andrew", ""], ["Frermann", "Lea", ""]]}, {"id": "2104.11032", "submitter": "Moeen Mostafavi", "authors": "Moeen Mostafavi and Michael D. Porter", "title": "How emoji and word embedding helps to unveil emotional transitions\n  during online messaging", "comments": null, "journal-ref": null, "doi": "10.1109/SysCon48628.2021.9447137", "report-no": null, "categories": "cs.HC cs.CL cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During online chats, body-language and vocal characteristics are not part of\nthe communication mechanism making it challenging to facilitate an accurate\ninterpretation of feelings, emotions, and attitudes. The use of emojis to\nexpress emotional feeling is an alternative approach in these types of\ncommunication. In this project, we focus on modeling a customer's emotion in an\nonline messaging session with a chatbot. We use Affect Control Theory (ACT) to\npredict emotional change during the interaction. To let the customer use\nemojis, we also extend the affective dictionaries used by ACT. For this\npurpose, we mapped Emoji2vec embedding to the affective space. Our framework\ncan find emotional change during messaging and how a customer's reaction is\nchanged accordingly.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:45:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mostafavi", "Moeen", ""], ["Porter", "Michael D.", ""]]}, {"id": "2104.11038", "submitter": "Aditya Vempaty", "authors": "Mohammad Niknazar and Aditya Vempaty and Ravi Kokku", "title": "Voice Privacy with Smart Digital Assistants in Educational Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.CR cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of voice-assistant devices ushers in delightful user\nexperiences not just on the smart home front, but also in diverse educational\nenvironments from classrooms to personalized-learning/tutoring. However, the\nuse of voice as an interaction modality also could result in exposure of user's\nidentity, and hinders the broader adoption of voice interfaces; this is\nespecially important in environments where children are present and their voice\nprivacy needs to be protected. To this end, building on state-of-the-art\ntechniques proposed in the literature, we design and evaluate a practical and\nefficient framework for voice privacy at the source. The approach combines\nspeaker identification (SID) and speech conversion methods to randomly disguise\nthe identity of users right on the device that records the speech, while\nensuring that the transformed utterances of users can still be successfully\ntranscribed by Automatic Speech Recognition (ASR) solutions. We evaluate the\nASR performance of the conversion in terms of word error rate and show the\npromise of this framework in preserving the content of the input speech.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:58:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Niknazar", "Mohammad", ""], ["Vempaty", "Aditya", ""], ["Kokku", "Ravi", ""]]}, {"id": "2104.11070", "submitter": "Ashish Shenoy", "authors": "Ashish Shenoy, Sravan Bodapati, Monica Sunkara, Srikanth Ronanki,\n  Katrin Kirchhoff", "title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents", "comments": "Accepted to Interspeech 2021. arXiv admin note: text overlap with\n  arXiv:2103.10325", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Language Models (NLM), when trained and evaluated with context\nspanning multiple utterances, have been shown to consistently outperform both\nconventional n-gram language models and NLMs that use limited context. In this\npaper, we investigate various techniques to incorporate turn based context\nhistory into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent\nbased NLMs, we explore context carry over mechanism and feature based\naugmentation, where we incorporate other forms of contextual information such\nas bot response and system dialogue acts as classified by a Natural Language\nUnderstanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem\nwith contextual NLM, we propose the use of attention layer over lexical\nmetadata to improve feature based augmentation. Additionally, we adapt our\ncontextual NLM towards user provided on-the-fly speech patterns by leveraging\nencodings from a large pre-trained masked language model and performing fusion\nwith a Transformer-XL based NLM. We test our proposed models using N-best\nrescoring of ASR hypotheses of task-oriented dialogues and also evaluate on\ndownstream NLU tasks such as intent classification and slot labeling. The best\nperforming model shows a relative WER between 1.6% and 9.1% and a slot labeling\nF1 score improvement of 4% over non-contextual baselines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 00:15:21 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 22:22:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shenoy", "Ashish", ""], ["Bodapati", "Sravan", ""], ["Sunkara", "Monica", ""], ["Ronanki", "Srikanth", ""], ["Kirchhoff", "Katrin", ""]]}, {"id": "2104.11127", "submitter": "Janne Pylkk\\\"onen", "authors": "Janne Pylkk\\\"onen (1), Antti Ukkonen (1 and 2), Juho Kilpikoski (1),\n  Samu Tamminen (1), Hannes Heikinheimo (1) ((1) Speechly, (2) Department of\n  Computer Science, University of Helsinki, Finland)", "title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network", "comments": "5 pages, 2 figures. Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adaption of end-to-end speech recognition systems to new tasks is known to be\nchallenging. A number of solutions have been proposed which apply external\nlanguage models with various fusion methods, possibly with a combination of\ntwo-pass decoding. Also TTS systems have been used to generate adaptation data\nfor the end-to-end models. In this paper we show that RNN-transducer models can\nbe effectively adapted to new domains using only small amounts of textual data.\nBy taking advantage of model's inherent structure, where the prediction network\nis interpreted as a language model, we can apply fast adaptation to the model.\nAdapting the model avoids the need for complicated decoding time fusions and\nexternal language models. Using appropriate regularization, the prediction\nnetwork can be adapted to new domains while still retaining good generalization\ncapabilities. We show with multiple ASR evaluation tasks how this method can\nprovide relative gains of 10-45% in target task WER. We also share insights how\nRNN-transducer prediction network performs as a language model.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:21:41 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 07:52:33 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pylkk\u00f6nen", "Janne", "", "1 and 2"], ["Ukkonen", "Antti", "", "1 and 2"], ["Kilpikoski", "Juho", ""], ["Tamminen", "Samu", ""], ["Heikinheimo", "Hannes", ""]]}, {"id": "2104.11295", "submitter": "Rishi Jha", "authors": "Rishi Jha and Kai Mihata", "title": "On Geodesic Distances and Contextual Embedding Compression for Text\n  Classification", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In some memory-constrained settings like IoT devices and over-the-network\ndata pipelines, it can be advantageous to have smaller contextual embeddings.\nWe investigate the efficacy of projecting contextual embedding data (BERT) onto\na manifold, and using nonlinear dimensionality reduction techniques to compress\nthese embeddings. In particular, we propose a novel post-processing approach,\napplying a combination of Isomap and PCA. We find that the geodesic distance\nestimations, estimates of the shortest path on a Riemannian manifold, from\nIsomap's k-Nearest Neighbors graph bolstered the performance of the compressed\nembeddings to be comparable to the original BERT embeddings. On one dataset, we\nfind that despite a 12-fold dimensionality reduction, the compressed embeddings\nperformed within 0.1% of the original BERT embeddings on a downstream\nclassification task. In addition, we find that this approach works particularly\nwell on tasks reliant on syntactic data, when compared with linear\ndimensionality reduction. These results show promise for a novel geometric\napproach to achieve lower dimensional text embeddings from existing\ntransformers and pave the way for data-specific and application-specific\nembedding compressions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 19:30:06 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Jha", "Rishi", ""], ["Mihata", "Kai", ""]]}, {"id": "2104.11348", "submitter": "Miguel Del Rio Fernandez", "authors": "Miguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang,\n  Nishchal Bhandari, Joseph Palakapilly, Quinten McNamara, Joshua Dong, Piotr\n  Zelasko, Miguel Jette", "title": "Earnings-21: A Practical Benchmark for ASR in the Wild", "comments": "Accepted to INTERSPEECH 2021. June 15 2021: Addressing the comments\n  of reviewers and updating the results of our internal ESPNet model. The\n  results do not change our conclusions. April 28th, 2021: We found and\n  resolved an issue in our experimental evaluation that scored the LibriSpeech\n  model at ~20% worse relative WER than the actual WER. The updated results do\n  not affect our conclusions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Commonly used speech corpora inadequately challenge academic and commercial\nASR systems. In particular, speech corpora lack metadata needed for detailed\nanalysis and WER measurement. In response, we present Earnings-21, a 39-hour\ncorpus of earnings calls containing entity-dense speech from nine different\nfinancial sectors. This corpus is intended to benchmark ASR systems in the wild\nwith special attention towards named entity recognition. We benchmark four\ncommercial ASR models, two internal models built with open-source tools, and an\nopen-source LibriSpeech model and discuss their differences in performance on\nEarnings-21. Using our recently released fstalign tool, we provide a candid\nanalysis of each model's recognition capabilities under different partitions.\nOur analysis finds that ASR accuracy for certain NER categories is poor,\npresenting a significant impediment to transcript comprehension and usage.\nEarnings-21 bridges academic and commercial ASR system evaluation and enables\nfurther research on entity modeling and WER on real world audio.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 23:04:28 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 15:43:46 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 02:32:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Del Rio", "Miguel", ""], ["Delworth", "Natalie", ""], ["Westerman", "Ryan", ""], ["Huang", "Michelle", ""], ["Bhandari", "Nishchal", ""], ["Palakapilly", "Joseph", ""], ["McNamara", "Quinten", ""], ["Dong", "Joshua", ""], ["Zelasko", "Piotr", ""], ["Jette", "Miguel", ""]]}, {"id": "2104.11384", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Sayyed M. Zahiri, Simon Hughes, Khalifa Al Jadda, Surya\n  Kallumadi, and Eugene Agichtein", "title": "APRF-Net: Attentive Pseudo-Relevance Feedback Network for Query\n  Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query categorization is an essential part of query intent understanding in\ne-commerce search. A common query categorization task is to select the relevant\nfine-grained product categories in a product taxonomy. For frequent queries,\nrich customer behavior (e.g., click-through data) can be used to infer the\nrelevant product categories. However, for more rare queries, which cover a\nlarge volume of search traffic, relying solely on customer behavior may not\nsuffice due to the lack of this signal. To improve categorization of rare\nqueries, we adapt the Pseudo-Relevance Feedback (PRF) approach to utilize the\nlatent knowledge embedded in semantically or lexically similar product\ndocuments to enrich the representation of the more rare queries. To this end,\nwe propose a novel deep neural model named Attentive Pseudo Relevance Feedback\nNetwork (APRF-Net) to enhance the representation of rare queries for query\ncategorization. To demonstrate the effectiveness of our approach, we collect\nsearch queries from a large commercial search engine, and compare APRF-Net to\nstate-of-the-art deep learning models for text classification. Our results show\nthat the APRF-Net significantly improves query categorization by 5.9% on F1@1\nscore over the baselines, which increases to 8.2% improvement for the rare\n(tail) queries. The findings of this paper can be leveraged for further\nimprovements in search query representation and understanding.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:34:08 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 18:47:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Zahiri", "Sayyed M.", ""], ["Hughes", "Simon", ""], ["Jadda", "Khalifa Al", ""], ["Kallumadi", "Surya", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2104.11390", "submitter": "Zhang Han", "authors": "Han Zhang", "title": "Transfer training from smaller language model", "comments": "7 pages 7 figures. arXiv admin note: text overlap with\n  arXiv:2103.14636 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large language models have led to state-of-the-art accuracies across a range\nof tasks. However,training large language model needs massive computing\nresource, as more and more open source pre-training models are available, it is\nworthy to study how to take full advantage of available model. We find a method\nto save training time and resource cost by changing the small well-trained\nmodel to large model. We initialize a larger target model from a smaller source\nmodel by copy weight values from source model and padding with zeros or small\ninitialization values on it to make the source and target model have\napproximate outputs, which is valid due to block matrix multiplication and\nresidual connection in transformer structure. We test the target model on\nseveral data sets and find it is still comparable with the source model. When\nwe continue training the target model, the training loss can start from a\nsmaller value.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:56:02 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zhang", "Han", ""]]}, {"id": "2104.11394", "submitter": "Munazza Zaib", "authors": "Munazza Zaib and Dai Hoang Tran and Subhash Sagar and Adnan Mahmood\n  and Wei E. Zhang and Quan Z. Sheng", "title": "BERT-CoQAC: BERT-based Conversational Question Answering in Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one promising way to inquire about any particular information through a\ndialog with the bot, question answering dialog systems have gained increasing\nresearch interests recently. Designing interactive QA systems has always been a\nchallenging task in natural language processing and used as a benchmark to\nevaluate a machine's ability of natural language understanding. However, such\nsystems often struggle when the question answering is carried out in multiple\nturns by the users to seek more information based on what they have already\nlearned, thus, giving rise to another complicated form called Conversational\nQuestion Answering (CQA). CQA systems are often criticized for not\nunderstanding or utilizing the previous context of the conversation when\nanswering the questions. To address the research gap, in this paper, we explore\nhow to integrate conversational history into the neural machine comprehension\nsystem. On one hand, we introduce a framework based on a publically available\npre-trained language model called BERT for incorporating history turns into the\nsystem. On the other hand, we propose a history selection mechanism that\nselects the turns that are relevant and contributes the most to answer the\ncurrent question. Experimentation results revealed that our framework is\ncomparable in performance with the state-of-the-art models on the QuAC leader\nboard. We also conduct a number of experiments to show the side effects of\nusing entire context information which brings unnecessary information and noise\nsignals resulting in a decline in the model's performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 03:05:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zaib", "Munazza", ""], ["Tran", "Dai Hoang", ""], ["Sagar", "Subhash", ""], ["Mahmood", "Adnan", ""], ["Zhang", "Wei E.", ""], ["Sheng", "Quan Z.", ""]]}, {"id": "2104.11462", "submitter": "Laurent Besacier", "authors": "Solene Evain, Ha Nguyen, Hang Le, Marcely Zanon Boito, Salima\n  Mdhaffar, Sina Alisamir, Ziyi Tong, Natalia Tomashenko, Marco Dinarelli,\n  Titouan Parcollet, Alexandre Allauzen, Yannick Esteve, Benjamin Lecouteux,\n  Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab and Laurent\n  Besacier", "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised\n  Representation Learning from Speech", "comments": "Will be presented at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-Supervised Learning (SSL) using huge unlabeled data has been\nsuccessfully explored for image and natural language processing. Recent works\nalso investigated SSL from speech. They were notably successful to improve\nperformance on downstream tasks such as automatic speech recognition (ASR).\nWhile these works suggest it is possible to reduce dependence on labeled data\nfor building efficient speech systems, their evaluation was mostly made on ASR\nand using multiple and heterogeneous experimental settings (most of them for\nEnglish). This questions the objective comparison of SSL approaches and the\nevaluation of their impact on building speech systems. In this paper, we\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\nnot only includes ASR (high and low resource) tasks but also spoken language\nunderstanding, speech translation and emotion recognition. We also focus on\nspeech technologies in a language different than English: French. SSL models of\ndifferent sizes are trained from carefully sourced and documented datasets.\nExperiments show that SSL is beneficial for most but not all tasks which\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\nimpact. LeBenchmark is shared with the scientific community for reproducible\nresearch in SSL from speech.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:27:09 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:30:30 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Evain", "Solene", ""], ["Nguyen", "Ha", ""], ["Le", "Hang", ""], ["Boito", "Marcely Zanon", ""], ["Mdhaffar", "Salima", ""], ["Alisamir", "Sina", ""], ["Tong", "Ziyi", ""], ["Tomashenko", "Natalia", ""], ["Dinarelli", "Marco", ""], ["Parcollet", "Titouan", ""], ["Allauzen", "Alexandre", ""], ["Esteve", "Yannick", ""], ["Lecouteux", "Benjamin", ""], ["Portet", "Francois", ""], ["Rossato", "Solange", ""], ["Ringeval", "Fabien", ""], ["Schwab", "Didier", ""], ["Besacier", "Laurent", ""]]}, {"id": "2104.11476", "submitter": "Pham Quang Nhat Minh Mr", "authors": "Nguyen Manh Duc Tuan, Pham Quang Nhat Minh", "title": "Multimodal Fusion with BERT and Attention Mechanism for Fake News\n  Detection", "comments": "RIVF 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news detection is an important task for increasing the credibility of\ninformation on the media since fake news is constantly spreading on social\nmedia every day and it is a very serious concern in our society. Fake news is\nusually created by manipulating images, texts, and videos. In this paper, we\npresent a novel method for detecting fake news by fusing multimodal features\nderived from textual and visual data. Specifically, we used a pre-trained BERT\nmodel to learn text features and a VGG-19 model pre-trained on the ImageNet\ndataset to extract image features. We proposed a scale-dot product attention\nmechanism to capture the relationship between text features and visual\nfeatures. Experimental results showed that our approach performs better than\nthe current state-of-the-art method on a public Twitter dataset by 3.1%\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:47:54 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 05:16:15 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Tuan", "Nguyen Manh Duc", ""], ["Minh", "Pham Quang Nhat", ""]]}, {"id": "2104.11514", "submitter": "Pride Kavumba", "authors": "Pride Kavumba, Benjamin Heinzerling, Ana Brassard, Kentaro Inui", "title": "Learning to Learn to be Right for the Right Reasons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Improving model generalization on held-out data is one of the core objectives\nin commonsense reasoning. Recent work has shown that models trained on the\ndataset with superficial cues tend to perform well on the easy test set with\nsuperficial cues but perform poorly on the hard test set without superficial\ncues. Previous approaches have resorted to manual methods of encouraging models\nnot to overfit to superficial cues. While some of the methods have improved\nperformance on hard instances, they also lead to degraded performance on easy\ninstances. Here, we propose to explicitly learn a model that does well on both\nthe easy test set with superficial cues and hard test set without superficial\ncues. Using a meta-learning objective, we learn such a model that improves\nperformance on both the easy test set and the hard test set. By evaluating our\nmodels on Choice of Plausible Alternatives (COPA) and Commonsense Explanation,\nwe show that our proposed method leads to improved performance on both the easy\ntest set and the hard test set upon which we observe up to 16.5 percentage\npoints improvement over the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:03:00 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kavumba", "Pride", ""], ["Heinzerling", "Benjamin", ""], ["Brassard", "Ana", ""], ["Inui", "Kentaro", ""]]}, {"id": "2104.11532", "submitter": "Amin Honarmandi Shandiz", "authors": "L\\'aszl\\'o T\\'oth, Amin Honarmandi Shandiz", "title": "3D Convolutional Neural Networks for Ultrasound-Based Silent Speech\n  Interfaces", "comments": "10 pages, 2 tables , 3 figures", "journal-ref": null, "doi": "10.1007/978-3-030-61401-0_16", "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Silent speech interfaces (SSI) aim to reconstruct the speech signal from a\nrecording of the articulatory movement, such as an ultrasound video of the\ntongue. Currently, deep neural networks are the most successful technology for\nthis task. The efficient solution requires methods that do not simply process\nsingle images, but are able to extract the tongue movement information from a\nsequence of video frames. One option for this is to apply recurrent neural\nstructures such as the long short-term memory network (LSTM) in combination\nwith 2D convolutional neural networks (CNNs). Here, we experiment with another\napproach that extends the CNN to perform 3D convolution, where the extra\ndimension corresponds to time. In particular, we apply the spatial and temporal\nconvolutions in a decomposed form, which proved very successful recently in\nvideo action recognition. We find experimentally that our 3D network\noutperforms the CNN+LSTM model, indicating that 3D CNNs may be a feasible\nalternative to CNN+LSTM networks in SSI systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:56:34 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["T\u00f3th", "L\u00e1szl\u00f3", ""], ["Shandiz", "Amin Honarmandi", ""]]}, {"id": "2104.11556", "submitter": "Li-Hsin Chang", "authors": "Li-Hsin Chang, Iiro Rastas, Sampo Pyysalo, Filip Ginter", "title": "Deep learning for sentence clustering in essay grading support", "comments": "Accepted to EDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Essays as a form of assessment test student knowledge on a deeper level than\nshort answer and multiple-choice questions. However, the manual evaluation of\nessays is time- and labor-consuming. Automatic clustering of essays, or their\nfragments, prior to manual evaluation presents a possible solution to reducing\nthe effort required in the evaluation process. Such clustering presents\nnumerous challenges due to the variability and ambiguity of natural language.\nIn this paper, we introduce two datasets of undergraduate student essays in\nFinnish, manually annotated for salient arguments on the sentence level. Using\nthese datasets, we evaluate several deep-learning embedding methods for their\nsuitability to sentence clustering in support of essay grading. We find that\nthe choice of the most suitable method depends on the nature of the exam\nquestion and the answers, with deep-learning methods being capable of, but not\nguaranteeing better performance over simpler methods based on lexical overlap.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:32:51 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Chang", "Li-Hsin", ""], ["Rastas", "Iiro", ""], ["Pyysalo", "Sampo", ""], ["Ginter", "Filip", ""]]}, {"id": "2104.11557", "submitter": "Anastasiia Sedova", "authors": "Anastasiia Sedova, Andreas Stephan, Marina Speranskaya, Benjamin Roth", "title": "Knodle: Modular Weakly Supervised Learning with PyTorch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strategies for improving the training and prediction quality of weakly\nsupervised machine learning models vary in how much they are tailored to a\nspecific task or integrated with a specific model architecture. In this work,\nwe introduce Knodle, a software framework that treats weak data annotations,\ndeep learning models, and methods for improving weakly supervised training as\nseparate, modular components. This modularization gives the training process\naccess to fine-grained information such as data set characteristics, matches of\nheuristic rules, or elements of the deep learning model ultimately used for\nprediction. Hence, our framework can encompass a wide range of training methods\nfor improving weak supervision, ranging from methods that only look at\ncorrelations of rules and output classes (independently of the machine learning\nmodel trained with the resulting labels), to those that harness the interplay\nof neural networks and weakly labeled data. We illustrate the benchmarking\npotential of the framework with a performance comparison of several reference\nimplementations on a selection of datasets that are already available in\nKnodle.\n  The framework is published as an open-source Python package knodle and\navailable at https://github.com/knodle/knodle.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:33:25 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 21:06:30 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 13:31:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sedova", "Anastasiia", ""], ["Stephan", "Andreas", ""], ["Speranskaya", "Marina", ""], ["Roth", "Benjamin", ""]]}, {"id": "2104.11559", "submitter": "Jochen Z\\\"ollner", "authors": "Jochen Z\\\"ollner, Konrad Sperfeld, Christoph Wick, Roger Labahn", "title": "Optimizing small BERTs trained for German NER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Currently, the most widespread neural network architecture for training\nlanguage models is the so called BERT which led to improvements in various NLP\ntasks. In general, the larger the number of parameters in a BERT model, the\nbetter the results obtained in these NLP tasks. Unfortunately, the memory\nconsumption and the training duration drastically increases with the size of\nthese models, though. In this article, we investigate various training\ntechniques of smaller BERT models and evaluate them on five public German NER\ntasks of which two are introduced by this article. We combine different methods\nfrom other BERT variants like ALBERT, RoBERTa, and relative positional\nencoding. In addition, we propose two new fine-tuning techniques leading to\nbetter performance: CSE-tagging and a modified form of LCRF. Furthermore, we\nintroduce a new technique called WWA which reduces BERT memory usage and leads\nto a small increase in performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:36:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Z\u00f6llner", "Jochen", ""], ["Sperfeld", "Konrad", ""], ["Wick", "Christoph", ""], ["Labahn", "Roger", ""]]}, {"id": "2104.11560", "submitter": "Wenliang Dai", "authors": "Wenliang Dai, Samuel Cahyawijaya, Yejin Bang, Pascale Fung", "title": "Weakly-supervised Multi-task Learning for Multimodal Affect Recognition", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal affect recognition constitutes an important aspect for enhancing\ninterpersonal relationships in human-computer interaction. However, relevant\ndata is hard to come by and notably costly to annotate, which poses a\nchallenging barrier to build robust multimodal affect recognition systems.\nModels trained on these relatively small datasets tend to overfit and the\nimprovement gained by using complex state-of-the-art models is marginal\ncompared to simple baselines. Meanwhile, there are many different multimodal\naffect recognition datasets, though each may be small. In this paper, we\npropose to leverage these datasets using weakly-supervised multi-task learning\nto improve the generalization performance on each of them. Specifically, we\nexplore three multimodal affect recognition tasks: 1) emotion recognition; 2)\nsentiment analysis; and 3) sarcasm recognition. Our experimental results show\nthat multi-tasking can benefit all these tasks, achieving an improvement up to\n2.9% accuracy and 3.3% F1-score. Furthermore, our method also helps to improve\nthe stability of model performance. In addition, our analysis suggests that\nweak supervision can provide a comparable contribution to strong supervision if\nthe tasks are highly correlated.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:36:19 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dai", "Wenliang", ""], ["Cahyawijaya", "Samuel", ""], ["Bang", "Yejin", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.11572", "submitter": "Xia Zeng", "authors": "Xia Zeng, Arkaitz Zubiaga", "title": "QMUL-SDS at SCIVER: Step-by-Step Binary Classification for Scientific\n  Claim Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific claim verification is a unique challenge that is attracting\nincreasing interest. The SCIVER shared task offers a benchmark scenario to test\nand compare claim verification approaches by participating teams and consists\nin three steps: relevant abstract selection, rationale selection and label\nprediction. In this paper, we present team QMUL-SDS's participation in the\nshared task. We propose an approach that performs scientific claim verification\nby doing binary classifications step-by-step. We trained a BioBERT-large\nclassifier to select abstracts based on pairwise relevance assessments for each\n<claim, title of the abstract> and continued to train it to select rationales\nout of each retrieved abstract based on <claim, sentence>. We then propose a\ntwo-step setting for label prediction, i.e. first predicting \"NOT_ENOUGH_INFO\"\nor \"ENOUGH_INFO\", then label those marked as \"ENOUGH_INFO\" as either \"SUPPORT\"\nor \"CONTRADICT\". Compared to the baseline system, we achieve substantial\nimprovements on the dev set. As a result, our team is the No. 4 team on the\nleaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:12:57 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zeng", "Xia", ""], ["Zubiaga", "Arkaitz", ""]]}, {"id": "2104.11573", "submitter": "Michael Bennett", "authors": "Michael Timothy Bennett, Yoshihiro Maruyama", "title": "Intensional Artificial Intelligence: From Symbol Emergence to\n  Explainable and Empathetic AI", "comments": "7 pages, submitted to IEEE ICDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We argue that an explainable artificial intelligence must possess a rationale\nfor its decisions, be able to infer the purpose of observed behaviour, and be\nable to explain its decisions in the context of what its audience understands\nand intends. To address these issues we present four novel contributions.\nFirstly, we define an arbitrary task in terms of perceptual states, and discuss\ntwo extremes of a domain of possible solutions. Secondly, we define the\nintensional solution. Optimal by some definitions of intelligence, it describes\nthe purpose of a task. An agent possessed of it has a rationale for its\ndecisions in terms of that purpose, expressed in a perceptual symbol system\ngrounded in hardware. Thirdly, to communicate that rationale requires natural\nlanguage, a means of encoding and decoding perceptual states. We propose a\ntheory of meaning in which, to acquire language, an agent should model the\nworld a language describes rather than the language itself. If the utterances\nof humans are of predictive value to the agent's goals, then the agent will\nimbue those utterances with meaning in terms of its own goals and perceptual\nstates. In the context of Peircean semiotics, a community of agents must share\nrough approximations of signs, referents and interpretants in order to\ncommunicate. Meaning exists only in the context of intent, so to communicate\nwith humans an agent must have comparable experiences and goals. An agent that\nlearns intensional solutions, compelled by objective functions somewhat\nanalogous to human motivators such as hunger and pain, may be capable of\nexplaining its rationale not just in terms of its own intent, but in terms of\nwhat its audience understands and intends. It forms some approximation of the\nperceptual states of humans.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:13:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bennett", "Michael Timothy", ""], ["Maruyama", "Yoshihiro", ""]]}, {"id": "2104.11612", "submitter": "Glorianna Jagfeld", "authors": "Glorianna Jagfeld, Fiona Lobban, Paul Rayson, Steven H. Jones", "title": "Understanding who uses Reddit: Profiling individuals with a\n  self-reported bipolar disorder diagnosis", "comments": "The Seventh Workshop on Computational Linguistics and Clinical\n  Psychology: Improving Access @NAACL 2021; Visual abstract on p. 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, research on mental health conditions using public online data,\nincluding Reddit, has surged in NLP and health research but has not reported\nuser characteristics, which are important to judge generalisability of\nfindings. This paper shows how existing NLP methods can yield information on\nclinical, demographic, and identity characteristics of almost 20K Reddit users\nwho self-report a bipolar disorder diagnosis. This population consists of\nslightly more feminine- than masculine-gendered mainly young or middle-aged\nUS-based adults who often report additional mental health diagnoses, which is\ncompared with general Reddit statistics and epidemiological studies.\nAdditionally, this paper carefully evaluates all methods and discusses ethical\nissues.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:58:20 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Jagfeld", "Glorianna", ""], ["Lobban", "Fiona", ""], ["Rayson", "Paul", ""], ["Jones", "Steven H.", ""]]}, {"id": "2104.11639", "submitter": "Roman Klinger", "authors": "Amelie W\\\"uhrl and Roman Klinger", "title": "Claim Detection in Biomedical Twitter Posts", "comments": "Accepted at the BioNLP Workshop at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media contains unfiltered and unique information, which is potentially\nof great value, but, in the case of misinformation, can also do great harm.\nWith regards to biomedical topics, false information can be particularly\ndangerous. Methods of automatic fact-checking and fake news detection address\nthis problem, but have not been applied to the biomedical domain in social\nmedia yet. We aim to fill this research gap and annotate a corpus of 1200\ntweets for implicit and explicit biomedical claims (the latter also with span\nannotations for the claim phrase). With this corpus, which we sample to be\nrelated to COVID-19, measles, cystic fibrosis, and depression, we develop\nbaseline models which detect tweets that contain a claim automatically. Our\nanalyses reveal that biomedical tweets are densely populated with claims (45 %\nin a corpus sampled to contain 1200 tweets focused on the domains mentioned\nabove). Baseline classification experiments with embedding-based classifiers\nand BERT-based transfer learning demonstrate that the detection is challenging,\nhowever, shows acceptable performance for the identification of explicit\nexpressions of claims. Implicit claim tweets are more challenging to detect.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:45:31 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 18:22:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["W\u00fchrl", "Amelie", ""], ["Klinger", "Roman", ""]]}, {"id": "2104.11642", "submitter": "Deniz  Kavi", "authors": "Deniz Kavi", "title": "Turkish Text Classification: From Lexicon Analysis to Bidirectional\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Text classification has seen an increased use in both academic and industry\nsettings. Though rule based methods have been fairly successful, supervised\nmachine learning has been shown to be most successful for most languages, where\nmost research was done on English. In this article, the success of lexicon\nanalysis, support vector machines, and extreme gradient boosting for the task\nof text classification and sentiment analysis are evaluated in Turkish and a\npretrained transformer based classifier is proposed, outperforming previous\nmethods for Turkish text classification. In the context of text classification,\nall machine learning models proposed in the article are domain-independent and\ndo not require any task-specific modifications.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:30:44 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kavi", "Deniz", ""]]}, {"id": "2104.11673", "submitter": "Gabriel Mittag", "authors": "Gabriel Mittag, Sebastian M\\\"oller", "title": "Deep Learning Based Assessment of Synthetic Speech Naturalness", "comments": "Late upload, presented at Interspeech 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-2382", "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new objective prediction model for synthetic\nspeech naturalness. It can be used to evaluate Text-To-Speech or Voice\nConversion systems and works language independently. The model is trained\nend-to-end and based on a CNN-LSTM network that previously showed to give good\nresults for speech quality estimation. We trained and tested the model on 16\ndifferent datasets, such as from the Blizzard Challenge and the Voice\nConversion Challenge. Further, we show that the reliability of deep\nlearning-based naturalness prediction can be improved by transfer learning from\nspeech quality prediction models that are trained on objective POLQA scores.\nThe proposed model is made publicly available and can, for example, be used to\nevaluate different TTS system configurations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:05:20 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mittag", "Gabriel", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2104.11681", "submitter": "Zhen Bi", "authors": "Zhen Bi, Ningyu Zhang, Ganqiang Ye, Haiyang Yu, Xi Chen, Huajun Chen", "title": "Interventional Aspect-Based Sentiment Analysis", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural-based aspect-based sentiment analysis approaches, though\nachieving promising improvement on benchmark datasets, have reported suffering\nfrom poor robustness when encountering confounder such as non-target aspects.\nIn this paper, we take a causal view to addressing this issue. We propose a\nsimple yet effective method, namely, Sentiment Adjustment (SENTA), by applying\na backdoor adjustment to disentangle those confounding factors. Experimental\nresults on the Aspect Robustness Test Set (ARTS) dataset demonstrate that our\napproach improves the performance while maintaining accuracy in the original\ntest set.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:54:29 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bi", "Zhen", ""], ["Zhang", "Ningyu", ""], ["Ye", "Ganqiang", ""], ["Yu", "Haiyang", ""], ["Chen", "Xi", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.11710", "submitter": "Marco Gaido", "authors": "Marco Gaido, Matteo Negri, Mauro Cettolo, Marco Turchi", "title": "Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct\n  Speech Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The audio segmentation mismatch between training data and those seen at\nrun-time is a major problem in direct speech translation. Indeed, while systems\nare usually trained on manually segmented corpora, in real use cases they are\noften presented with continuous audio requiring automatic (and sub-optimal)\nsegmentation. After comparing existing techniques (VAD-based, fixed-length and\nhybrid segmentation methods), in this paper we propose enhanced hybrid\nsolutions to produce better results without sacrificing latency. Through\nexperiments on different domains and language pairs, we show that our methods\noutperform all the other techniques, reducing by at least 30% the gap between\nthe traditional VAD-based approach and optimal manual segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:54:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gaido", "Marco", ""], ["Negri", "Matteo", ""], ["Cettolo", "Mauro", ""], ["Turchi", "Marco", ""]]}, {"id": "2104.11729", "submitter": "Maria Glenski", "authors": "Maria Glenski, Ellyn Ayton, Robin Cosbey, Dustin Arendt, and Svitlana\n  Volkova", "title": "Evaluating Deception Detection Model Robustness To Linguistic Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the increasing use of machine-learning driven algorithmic judgements, it\nis critical to develop models that are robust to evolving or manipulated\ninputs. We propose an extensive analysis of model robustness against linguistic\nvariation in the setting of deceptive news detection, an important task in the\ncontext of misinformation spread online. We consider two prediction tasks and\ncompare three state-of-the-art embeddings to highlight consistent trends in\nmodel performance, high confidence misclassifications, and high impact\nfailures. By measuring the effectiveness of adversarial defense strategies and\nevaluating model susceptibility to adversarial attacks using character- and\nword-perturbed text, we find that character or mixed ensemble models are the\nmost effective defenses and that character perturbation-based attack tactics\nare more successful.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:25:38 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Glenski", "Maria", ""], ["Ayton", "Ellyn", ""], ["Cosbey", "Robin", ""], ["Arendt", "Dustin", ""], ["Volkova", "Svitlana", ""]]}, {"id": "2104.11760", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Surya Kallumadi, Faizan Javed, and Eugene Agichtein", "title": "DeepCAT: Deep Category Representation for Query Understanding in\n  E-commerce Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping a search query to a set of relevant categories in the product\ntaxonomy is a significant challenge in e-commerce search for two reasons: 1)\nTraining data exhibits severe class imbalance problem due to biased click\nbehavior, and 2) queries with little customer feedback (e.g., tail queries) are\nnot well-represented in the training set, and cause difficulties for query\nunderstanding. To address these problems, we propose a deep learning model,\nDeepCAT, which learns joint word-category representations to enhance the query\nunderstanding process. We believe learning category interactions helps to\nimprove the performance of category mapping on minority classes, tail and torso\nqueries. DeepCAT contains a novel word-category representation model that\ntrains the category representations based on word-category co-occurrences in\nthe training set. The category representation is then leveraged to introduce a\nnew loss function to estimate the category-category co-occurrences for refining\njoint word-category embeddings. To demonstrate our model's effectiveness on\nminority categories and tail queries, we conduct two sets of experiments. The\nresults show that DeepCAT reaches a 10% improvement on minority classes and a\n7.1% improvement on tail queries over a state-of-the-art label embedding model.\nOur findings suggest a promising direction for improving e-commerce search by\nsemantic modeling of taxonomy hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:04:44 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:12:02 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Kallumadi", "Surya", ""], ["Javed", "Faizan", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2104.11761", "submitter": "Maria Glenski", "authors": "Maria Glenski, Ellyn Ayton, Robin Cosbey, Dustin Arendt, and Svitlana\n  Volkova", "title": "Towards Trustworthy Deception Detection: Benchmarking Model Robustness\n  across Domains, Modalities, and Languages", "comments": null, "journal-ref": "Proceedings of the 3rd International Workshop on Rumours and\n  Deception in Social Media (RDSM). 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evaluating model robustness is critical when developing trustworthy models\nnot only to gain deeper understanding of model behavior, strengths, and\nweaknesses, but also to develop future models that are generalizable and robust\nacross expected environments a model may encounter in deployment. In this paper\nwe present a framework for measuring model robustness for an important but\ndifficult text classification task - deceptive news detection. We evaluate\nmodel robustness to out-of-domain data, modality-specific features, and\nlanguages other than English.\n  Our investigation focuses on three type of models: LSTM models trained on\nmultiple datasets(Cross-Domain), several fusion LSTM models trained with images\nand text and evaluated with three state-of-the-art embeddings, BERT ELMo, and\nGloVe (Cross-Modality), and character-level CNN models trained on multiple\nlanguages (Cross-Language). Our analyses reveal a significant drop in\nperformance when testing neural models on out-of-domain data and non-English\nlanguages that may be mitigated using diverse training data. We find that with\nadditional image content as input, ELMo embeddings yield significantly fewer\nerrors compared to BERT orGLoVe. Most importantly, this work not only carefully\nanalyzes deception model robustness but also provides a framework of these\nanalyses that can be applied to new models or extended datasets in the future.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:05:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Glenski", "Maria", ""], ["Ayton", "Ellyn", ""], ["Cosbey", "Robin", ""], ["Arendt", "Dustin", ""], ["Volkova", "Svitlana", ""]]}, {"id": "2104.11832", "submitter": "Zhe Gan", "authors": "Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang\n  Wang, Jingjing Liu", "title": "Playing Lottery Tickets with Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large-scale transformer-based pre-training has recently revolutionized\nvision-and-language (V+L) research. Models such as LXMERT, ViLBERT and UNITER\nhave significantly lifted the state of the art over a wide range of V+L tasks.\nHowever, the large number of parameters in such models hinders their\napplication in practice. In parallel, work on the lottery ticket hypothesis has\nshown that deep neural networks contain small matching subnetworks that can\nachieve on par or even better performance than the dense networks when trained\nin isolation. In this work, we perform the first empirical study to assess\nwhether such trainable subnetworks also exist in pre-trained V+L models. We use\nUNITER, one of the best-performing V+L models, as the testbed, and consolidate\n7 representative V+L tasks for experiments, including visual question\nanswering, visual commonsense reasoning, visual entailment, referring\nexpression comprehension, image-text retrieval, GQA, and NLVR$^2$. Through\ncomprehensive analysis, we summarize our main findings as follows. ($i$) It is\ndifficult to find subnetworks (i.e., the tickets) that strictly match the\nperformance of the full UNITER model. However, it is encouraging to confirm\nthat we can find \"relaxed\" winning tickets at 50%-70% sparsity that maintain\n99% of the full accuracy. ($ii$) Subnetworks found by task-specific pruning\ntransfer reasonably well to the other tasks, while those found on the\npre-training tasks at 60%/70% sparsity transfer universally, matching 98%/96%\nof the full accuracy on average over all the tasks. ($iii$) Adversarial\ntraining can be further used to enhance the performance of the found lottery\ntickets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 22:24:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gan", "Zhe", ""], ["Chen", "Yen-Chun", ""], ["Li", "Linjie", ""], ["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Wang", "Shuohang", ""], ["Liu", "Jingjing", ""]]}, {"id": "2104.11838", "submitter": "Zekun Xu", "authors": "Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, Nathanael Teissier", "title": "On a Utilitarian Approach to Privacy Preserving Text Generation", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentially-private mechanisms for text generation typically add carefully\ncalibrated noise to input words and use the nearest neighbor to the noised\ninput as the output word. When the noise is small in magnitude, these\nmechanisms are susceptible to reconstruction of the original sensitive text.\nThis is because the nearest neighbor to the noised input is likely to be the\noriginal input. To mitigate this empirical privacy risk, we propose a novel\nclass of differentially private mechanisms that parameterizes the nearest\nneighbor selection criterion in traditional mechanisms. Motivated by Vickrey\nauction, where only the second highest price is revealed and the highest price\nis kept private, we balance the choice between the first and the second nearest\nneighbors in the proposed class of mechanisms using a tuning parameter. This\nparameter is selected by empirically solving a constrained optimization problem\nfor maximizing utility, while maintaining the desired privacy guarantees. We\nargue that this empirical measurement framework can be used to align different\nmechanisms along a common benchmark for their privacy-utility tradeoff,\nparticularly when different distance metrics are used to calibrate the amount\nof noise added. Our experiments on real text classification datasets show up to\n50% improvement in utility compared to the existing state-of-the-art with the\nsame empirical privacy guarantee.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 23:13:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xu", "Zekun", ""], ["Aggarwal", "Abhinav", ""], ["Feyisetan", "Oluwaseyi", ""], ["Teissier", "Nathanael", ""]]}, {"id": "2104.11882", "submitter": "Congying Xia", "authors": "Congying Xia, Wenpeng Yin, Yihao Feng, Philip Yu", "title": "Incremental Few-shot Text Classification with Multi-round New Classes:\n  Formulation, Dataset and System", "comments": "10 pages, accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is usually studied by labeling natural language texts\nwith relevant categories from a predefined set. In the real world, new classes\nmight keep challenging the existing system with limited labeled data. The\nsystem should be intelligent enough to recognize upcoming new classes with a\nfew examples. In this work, we define a new task in the NLP domain, incremental\nfew-shot text classification, where the system incrementally handles multiple\nrounds of new classes. For each round, there is a batch of new classes with a\nfew labeled examples per class. Two major challenges exist in this new task:\n(i) For the learning process, the system should incrementally learn new classes\nround by round without re-training on the examples of preceding classes; (ii)\nFor the performance, the system should perform well on new classes without much\nloss on preceding classes. In addition to formulating the new task, we also\nrelease two benchmark datasets in the incremental few-shot setting: intent\nclassification and relation classification. Moreover, we propose two entailment\napproaches, ENTAILMENT and HYBRID, which show promise for solving this novel\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 04:41:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xia", "Congying", ""], ["Yin", "Wenpeng", ""], ["Feng", "Yihao", ""], ["Yu", "Philip", ""]]}, {"id": "2104.11897", "submitter": "Yong Shan", "authors": "Yong Shan, Yang Feng, Chenze Shao", "title": "Modeling Coverage for Non-Autoregressive Neural Machine Translation", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Autoregressive Neural Machine Translation (NAT) has achieved significant\ninference speedup by generating all tokens simultaneously. Despite its high\nefficiency, NAT usually suffers from two kinds of translation errors:\nover-translation (e.g. repeated tokens) and under-translation (e.g. missing\ntranslations), which eventually limits the translation quality. In this paper,\nwe argue that these issues of NAT can be addressed through coverage modeling,\nwhich has been proved to be useful in autoregressive decoding. We propose a\nnovel Coverage-NAT to model the coverage information directly by a token-level\ncoverage iterative refinement mechanism and a sentence-level coverage\nagreement, which can remind the model if a source token has been translated or\nnot and improve the semantics consistency between the translation and the\nsource, respectively. Experimental results on WMT14 En-De and WMT16 En-Ro\ntranslation tasks show that our method can alleviate those errors and achieve\nstrong improvements over the baseline system.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 07:33:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shan", "Yong", ""], ["Feng", "Yang", ""], ["Shao", "Chenze", ""]]}, {"id": "2104.11902", "submitter": "Jivat Neet Kaur", "authors": "Jivat Neet Kaur, Yiding Jiang, Paul Pu Liang", "title": "Ask & Explore: Grounded Question Answering for Curiosity-Driven\n  Exploration", "comments": "Accepted at ICLR 2021 Workshop on Embodied Multimodal Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many real-world scenarios where extrinsic rewards to the agent are\nextremely sparse, curiosity has emerged as a useful concept providing intrinsic\nrewards that enable the agent to explore its environment and acquire\ninformation to achieve its goals. Despite their strong performance on many\nsparse-reward tasks, existing curiosity approaches rely on an overly holistic\nview of state transitions, and do not allow for a structured understanding of\nspecific aspects of the environment. In this paper, we formulate curiosity\nbased on grounded question answering by encouraging the agent to ask questions\nabout the environment and be curious when the answers to these questions\nchange. We show that natural language questions encourage the agent to uncover\nspecific knowledge about their environment such as the physical properties of\nobjects as well as their spatial relationships with other objects, which serve\nas valuable curiosity rewards to solve sparse-reward tasks more efficiently.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 07:56:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kaur", "Jivat Neet", ""], ["Jiang", "Yiding", ""], ["Liang", "Paul Pu", ""]]}, {"id": "2104.11928", "submitter": "Cheng Chen", "authors": "Cheng Chen, Yichun Yin, Lifeng Shang, Zhi Wang, Xin Jiang, Xiao Chen,\n  Qun Liu", "title": "Extract then Distill: Efficient and Effective Task-Agnostic BERT\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-agnostic knowledge distillation, a teacher-student framework, has been\nproved effective for BERT compression. Although achieving promising results on\nNLP tasks, it requires enormous computational resources. In this paper, we\npropose Extract Then Distill (ETD), a generic and flexible strategy to reuse\nthe teacher's parameters for efficient and effective task-agnostic\ndistillation, which can be applied to students of any size. Specifically, we\nintroduce two variants of ETD, ETD-Rand and ETD-Impt, which extract the\nteacher's parameters in a random manner and by following an importance metric\nrespectively. In this way, the student has already acquired some knowledge at\nthe beginning of the distillation process, which makes the distillation process\nconverge faster. We demonstrate the effectiveness of ETD on the GLUE benchmark\nand SQuAD. The experimental results show that: (1) compared with the baseline\nwithout an ETD strategy, ETD can save 70\\% of computation cost. Moreover, it\nachieves better results than the baseline when using the same computing\nresource. (2) ETD is generic and has been proven effective for different\ndistillation methods (e.g., TinyBERT and MiniLM) and students of different\nsizes. The source code will be publicly available upon publication.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:23:39 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chen", "Cheng", ""], ["Yin", "Yichun", ""], ["Shang", "Lifeng", ""], ["Wang", "Zhi", ""], ["Jiang", "Xin", ""], ["Chen", "Xiao", ""], ["Liu", "Qun", ""]]}, {"id": "2104.11969", "submitter": "Phuong Ha-Dieu Phan", "authors": "Nhung Thi-Hong Nguyen, Phuong Phan-Dieu Ha, Luan Thanh Nguyen, Kiet\n  Van Nguyen, Ngan Luu-Thuy Nguyen", "title": "Vietnamese Complaint Detection on E-Commerce Websites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer product reviews play a role in improving the quality of products and\nservices for business organizations or their brands. Complaining is an attitude\nthat expresses dissatisfaction with an event or a product not meeting customer\nexpectations. In this paper, we build a Open-domain Complaint Detection dataset\n(UIT-ViOCD), including 5,485 human-annotated reviews on four categories about\nproduct reviews on e-commerce sites. After the data collection phase, we\nproceed to the annotation task and achieve the inter-annotator agreement Am of\n87%. Then, we present an extensive methodology for the research purposes and\nachieve 92.16% by F1-score for identifying complaints. With the results, in the\nfuture, we aim to build a system for open-domain complaint detection in\nE-commerce websites.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 15:19:06 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 10:09:41 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 07:24:10 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Nguyen", "Nhung Thi-Hong", ""], ["Ha", "Phuong Phan-Dieu", ""], ["Nguyen", "Luan Thanh", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2104.11984", "submitter": "Ilaria Manco", "authors": "Ilaria Manco, Emmanouil Benetos, Elio Quinton, Gyorgy Fazekas", "title": "MusCaps: Generating Captions for Music Audio", "comments": "Accepted to IJCNN 2021 for the Special Session on Representation\n  Learning for Audio, Speech, and Music Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based music information retrieval has seen rapid progress with the\nadoption of deep learning. Current approaches to high-level music description\ntypically make use of classification models, such as in auto-tagging or genre\nand mood classification. In this work, we propose to address music description\nvia audio captioning, defined as the task of generating a natural language\ndescription of music audio content in a human-like manner. To this end, we\npresent the first music audio captioning model, MusCaps, consisting of an\nencoder-decoder with temporal attention. Our method combines convolutional and\nrecurrent neural network architectures to jointly process audio-text inputs\nthrough a multimodal encoder and leverages pre-training on audio data to obtain\nrepresentations that effectively capture and summarise musical features in the\ninput. Evaluation of the generated captions through automatic metrics shows\nthat our method outperforms a baseline designed for non-music audio captioning.\nThrough an ablation study, we unveil that this performance boost can be mainly\nattributed to pre-training of the audio encoder, while other design choices -\nmodality fusion, decoding strategy and the use of attention - contribute only\nmarginally. Our model represents a shift away from classification-based music\ndescription and combines tasks requiring both auditory and linguistic\nunderstanding to bridge the semantic gap in music information retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 16:34:47 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Manco", "Ilaria", ""], ["Benetos", "Emmanouil", ""], ["Quinton", "Elio", ""], ["Fazekas", "Gyorgy", ""]]}, {"id": "2104.11985", "submitter": "Nikolay Mikhaylovskiy", "authors": "Roman Bedyakin, Nikolay Mikhaylovskiy", "title": "Language ID Prediction from Speech Using Self-Attentive Pooling and\n  1D-Convolutions", "comments": "Accepted to SYGTYP-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This memo describes NTR-TSU submission for SIGTYP 2021 Shared Task on\npredicting language IDs from speech.\n  Spoken Language Identification (LID) is an important step in a multilingual\nAutomated Speech Recognition (ASR) system pipeline. For many low-resource and\nendangered languages, only single-speaker recordings may be available,\ndemanding a need for domain and speaker-invariant language ID systems. In this\nmemo, we show that a convolutional neural network with a Self-Attentive Pooling\nlayer shows promising results for the language identification task.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 16:41:17 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bedyakin", "Roman", ""], ["Mikhaylovskiy", "Nikolay", ""]]}, {"id": "2104.12114", "submitter": "King Keung Wu", "authors": "Pengfei Liu, Youzhang Ning, King Keung Wu, Kun Li and Helen Meng", "title": "Open Intent Discovery through Unsupervised Semantic Clustering and\n  Dependency Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Intent understanding plays an important role in dialog systems, and is\ntypically formulated as a supervised classification problem. However, it is\nchallenging and time-consuming to design the intent labels manually to support\na new domain. This paper proposes an unsupervised two-stage approach to\ndiscover intents and generate meaningful intent labels automatically from a\ncollection of unlabeled utterances. In the first stage, we aim to generate a\nset of semantically coherent clusters where the utterances within each cluster\nconvey the same intent. We obtain the utterance representation from various\npre-trained sentence embeddings and present a metric of balanced score to\ndetermine the optimal number of clusters in K-means clustering. In the second\nstage, the objective is to generate an intent label automatically for each\ncluster. We extract the ACTION-OBJECT pair from each utterance using a\ndependency parser and take the most frequent pair within each cluster, e.g.,\nbook-restaurant, as the generated cluster label. We empirically show that the\nproposed unsupervised approach can generate meaningful intent labels\nautomatically and achieves high precision and recall in utterance clustering\nand intent discovery.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 09:36:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liu", "Pengfei", ""], ["Ning", "Youzhang", ""], ["Wu", "King Keung", ""], ["Li", "Kun", ""], ["Meng", "Helen", ""]]}, {"id": "2104.12128", "submitter": "Dai Quoc Nguyen", "authors": "Thanh Vu and Dai Quoc Nguyen", "title": "Automatic Post-Editing for Translating Chinese Novels to Vietnamese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic post-editing (APE) is an important remedy for reducing errors of\nraw translated texts that are produced by machine translation (MT) systems or\nsoftware-aided translation. In this paper, we present the first attempt to\ntackle the APE task for Vietnamese. Specifically, we construct the first\nlarge-scale dataset of 5M Vietnamese translated and corrected sentence pairs.\nWe then apply strong neural MT models to handle the APE task, using our\nconstructed dataset. Experimental results from both automatic and human\nevaluations show the effectiveness of the neural MT models in handling the\nVietnamese APE task.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 10:59:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Vu", "Thanh", ""], ["Nguyen", "Dai Quoc", ""]]}, {"id": "2104.12201", "submitter": "Tharindu Ranasinghe Mr", "authors": "Lasitha Uyangodage, Tharindu Ranasinghe, Hansi Hettiarachchi", "title": "Transformers to Fight the COVID-19 Infodemic", "comments": "Accepted to Workshop on NLP for Internet Freedom (NLP4IF) at NAACL\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The massive spread of false information on social media has become a global\nrisk especially in a global pandemic situation like COVID-19. False information\ndetection has thus become a surging research topic in recent months.\nNLP4IF-2021 shared task on fighting the COVID-19 infodemic has been organised\nto strengthen the research in false information detection where the\nparticipants are asked to predict seven different binary labels regarding false\ninformation in a tweet. The shared task has been organised in three languages;\nArabic, Bulgarian and English. In this paper, we present our approach to tackle\nthe task objective using transformers. Overall, our approach achieves a 0.707\nmean F1 score in Arabic, 0.578 mean F1 score in Bulgarian and 0.864 mean F1\nscore in English ranking 4th place in all the languages.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 16:49:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Uyangodage", "Lasitha", ""], ["Ranasinghe", "Tharindu", ""], ["Hettiarachchi", "Hansi", ""]]}, {"id": "2104.12227", "submitter": "Francielle Alves Vargas", "authors": "Francielle Alves Vargas, Isabelle Carvalho, Fabiana Rodrigues de\n  G\\'oes", "title": "Identifying Offensive Expressions of Opinion in Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 18:35:39 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 09:49:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Vargas", "Francielle Alves", ""], ["Carvalho", "Isabelle", ""], ["de G\u00f3es", "Fabiana Rodrigues", ""]]}, {"id": "2104.12250", "submitter": "Jose Camacho-Collados", "authors": "Francesco Barbieri and Luis Espinosa Anke and Jose Camacho-Collados", "title": "XLM-T: A Multilingual Language Model Toolkit for Twitter", "comments": "Submitted to ACL demo. Code and data available at\n  https://github.com/cardiffnlp/xlm-t", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models are ubiquitous in current NLP, and their multilingual\ncapacity has recently attracted considerable attention. However, current\nanalyses have almost exclusively focused on (multilingual variants of) standard\nbenchmarks, and have relied on clean pre-training and task-specific corpora as\nmultilingual signals. In this paper, we introduce XLM-T, a framework for using\nand evaluating multilingual language models in Twitter. This framework features\ntwo main assets: (1) a strong multilingual baseline consisting of an XLM-R\n(Conneau et al. 2020) model pre-trained on millions of tweets in over thirty\nlanguages, alongside starter code to subsequently fine-tune on a target task;\nand (2) a set of unified sentiment analysis Twitter datasets in eight different\nlanguages. This is a modular framework that can easily be extended to\nadditional tasks, as well as integrated with recent efforts also aimed at the\nhomogenization of Twitter-specific datasets (Barbieri et al. 2020).\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 20:28:53 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Barbieri", "Francesco", ""], ["Anke", "Luis Espinosa", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2104.12259", "submitter": "Yingtong Dou", "authors": "Yingtong Dou, Kai Shu, Congying Xia, Philip S. Yu, Lichao Sun", "title": "User Preference-aware Fake News Detection", "comments": "Accepted by SIGIR'21. Code is available at\n  https://github.com/safe-graph/GNN-FakeNews", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disinformation and fake news have posed detrimental effects on individuals\nand society in recent years, attracting broad attention to fake news detection.\nThe majority of existing fake news detection algorithms focus on mining news\ncontent and/or the surrounding exogenous context for discovering deceptive\nsignals; while the endogenous preference of a user when he/she decides to\nspread a piece of fake news or not is ignored. The confirmation bias theory has\nindicated that a user is more likely to spread a piece of fake news when it\nconfirms his/her existing beliefs/preferences. Users' historical, social\nengagements such as posts provide rich information about users' preferences\ntoward news and have great potential to advance fake news detection. However,\nthe work on exploring user preference for fake news detection is somewhat\nlimited. Therefore, in this paper, we study the novel problem of exploiting\nuser preference for fake news detection. We propose a new framework, UPFD,\nwhich simultaneously captures various signals from user preferences by joint\ncontent and graph modeling. Experimental results on real-world datasets\ndemonstrate the effectiveness of the proposed framework. We release our code\nand data as a benchmark for GNN-based fake news detection:\nhttps://github.com/safe-graph/GNN-FakeNews.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:19:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dou", "Yingtong", ""], ["Shu", "Kai", ""], ["Xia", "Congying", ""], ["Yu", "Philip S.", ""], ["Sun", "Lichao", ""]]}, {"id": "2104.12265", "submitter": "Francielle Alves Vargas", "authors": "Francielle Alves Vargas, Fabiana Rodrigues de G\\'oes, Isabelle\n  Carvalho, Fabr\\'icio Benevenuto, Thiago Alexandre Salgueiro Pardo", "title": "Contextual Lexicon-Based Approach for Hate Speech and Offensive Language\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a new approach for offensive language and hate speech\ndetection on social media. Our approach incorporates an offensive lexicon\ncomposed of implicit and explicit offensive and swearing expressions annotated\nwith binary classes: context-dependent and context-independent offensive. Due\nto the severity of the hate speech and offensive comments in Brazil, and the\nlack of research in Portuguese, Brazilian Portuguese is the language used to\nvalidate the proposed method. Nevertheless, our proposal may be applied to any\nother language or domain. Based on the obtained results, the proposed approach\nshowed high-performance overcoming the current baselines for European and\nBrazilian Portuguese.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:34:51 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 23:07:22 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 17:39:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vargas", "Francielle Alves", ""], ["de G\u00f3es", "Fabiana Rodrigues", ""], ["Carvalho", "Isabelle", ""], ["Benevenuto", "Fabr\u00edcio", ""], ["Pardo", "Thiago Alexandre Salgueiro", ""]]}, {"id": "2104.12269", "submitter": "Diwanshu Shekhar", "authors": "Diwanshu Shekhar, Pooran S. Negi, Mohammad Mahoor", "title": "A Bi-Encoder LSTM Model For Learning Unstructured Dialogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Creating a data-driven model that is trained on a large dataset of\nunstructured dialogs is a crucial step in developing Retrieval-based Chatbot\nsystems. This paper presents a Long Short Term Memory (LSTM) based architecture\nthat learns unstructured multi-turn dialogs and provides results on the task of\nselecting the best response from a collection of given responses. Ubuntu Dialog\nCorpus Version 2 was used as the corpus for training. We show that our model\nachieves 0.8%, 1.0% and 0.3% higher accuracy for Recall@1, Recall@2 and\nRecall@5 respectively than the benchmark model. We also show results on\nexperiments performed by using several similarity functions, model\nhyper-parameters and word embeddings on the proposed architecture\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:37:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shekhar", "Diwanshu", ""], ["Negi", "Pooran S.", ""], ["Mahoor", "Mohammad", ""]]}, {"id": "2104.12277", "submitter": "Andreas Stolcke", "authors": "Wen Wang and Andreas Stolcke and Jing Zheng", "title": "Reranking Machine Translation Hypotheses with Structured and Web-based\n  Language Models", "comments": "With a correction to the math in Figure 1 caption", "journal-ref": "Proc. 2007 IEEE ASRU Workshop, pp. 159-164", "doi": "10.1109/ASRU.2007.4430102", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of linguistically motivated and\ncomputationally efficient structured language models for reranking N-best\nhypotheses in a statistical machine translation system. These language models,\ndeveloped from Constraint Dependency Grammar parses, tightly integrate\nknowledge of words, morphological and lexical features, and syntactic\ndependency constraints. Two structured language models are applied for N-best\nrescoring, one is an almost-parsing language model, and the other utilizes more\nsyntactic features by explicitly modeling syntactic dependencies between words.\nWe also investigate effective and efficient language modeling methods to use\nN-grams extracted from up to 1 teraword of web documents. We apply all these\nlanguage models for N-best re-ranking on the NIST and DARPA GALE program 2006\nand 2007 machine translation evaluation tasks and find that the combination of\nthese language models increases the BLEU score up to 1.6% absolutely on blind\ntest sets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 22:09:03 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Wen", ""], ["Stolcke", "Andreas", ""], ["Zheng", "Jing", ""]]}, {"id": "2104.12324", "submitter": "Fei Liu", "authors": "Jia Jin Koay and Alexander Roustai and Xiaojin Dai and Fei Liu", "title": "A Sliding-Window Approach to Automatic Creation of Meeting Minutes", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meeting minutes record any subject matters discussed, decisions reached and\nactions taken at meetings. The importance of minuting cannot be overemphasized\nin a time when a significant number of meetings take place in the virtual\nspace. In this paper, we present a sliding window approach to automatic\ngeneration of meeting minutes. It aims to tackle issues associated with the\nnature of spoken text, including lengthy transcripts and lack of document\nstructure, which make it difficult to identify salient content to be included\nin the meeting minutes. Our approach combines a sliding window and a neural\nabstractive summarizer to navigate through the transcripts to find salient\ncontent. The approach is evaluated on transcripts of natural meeting\nconversations, where we compare results obtained for human transcripts and two\nversions of automatic transcripts and discuss how and to what extent the\nsummarizer succeeds at capturing salient content.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 02:44:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Koay", "Jia Jin", ""], ["Roustai", "Alexander", ""], ["Dai", "Xiaojin", ""], ["Liu", "Fei", ""]]}, {"id": "2104.12333", "submitter": "Tao Ni", "authors": "Tao Ni, Qing Wang, Gabriela Ferraro", "title": "Explore BiLSTM-CRF-Based Models for Open Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting multiple relations from text sentences is still a challenge for\ncurrent Open Relation Extraction (Open RE) tasks. In this paper, we develop\nseveral Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural\nnetwork and different contextualized word embedding methods. We also propose a\nnew tagging scheme to solve overlapping problems and enhance models'\nperformance. From the evaluation results and comparisons between models, we\nselect the best combination of tagging scheme, word embedder, and BiLSTM-CRF\nnetwork to achieve an Open RE model with a remarkable extracting ability on\nmultiple-relation sentences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 03:37:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ni", "Tao", ""], ["Wang", "Qing", ""], ["Ferraro", "Gabriela", ""]]}, {"id": "2104.12369", "submitter": "Yi Liao", "authors": "Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin\n  Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong,\n  Yifan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang,\n  Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han\n  Zhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang,\n  Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu,\n  Yonghong Tian", "title": "PanGu-$\\alpha$: Large-scale Autoregressive Pretrained Chinese Language\n  Models with Auto-parallel Computation", "comments": "The technique report for PanGu-$\\alpha$", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale Pretrained Language Models (PLMs) have become the new paradigm\nfor Natural Language Processing (NLP). PLMs with hundreds of billions\nparameters such as GPT-3 have demonstrated strong performances on natural\nlanguage understanding and generation with \\textit{few-shot in-context}\nlearning. In this work, we present our practice on training large-scale\nautoregressive language models named PanGu-$\\alpha$, with up to 200 billion\nparameters. PanGu-$\\alpha$ is developed under the MindSpore and trained on a\ncluster of 2048 Ascend 910 AI processors. The training parallelism strategy is\nimplemented based on MindSpore Auto-parallel, which composes five parallelism\ndimensions to scale the training task to 2048 processors efficiently, including\ndata parallelism, op-level model parallelism, pipeline model parallelism,\noptimizer model parallelism and rematerialization. To enhance the\ngeneralization ability of PanGu-$\\alpha$, we collect 1.1TB high-quality Chinese\ndata from a wide range of domains to pretrain the model. We empirically test\nthe generation ability of PanGu-$\\alpha$ in various scenarios including text\nsummarization, question answering, dialogue generation, etc. Moreover, we\ninvestigate the effect of model scales on the few-shot performances across a\nbroad range of Chinese NLP tasks. The experimental results demonstrate the\nsuperior capabilities of PanGu-$\\alpha$ in performing various tasks under\nfew-shot or zero-shot settings.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 06:59:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zeng", "Wei", ""], ["Ren", "Xiaozhe", ""], ["Su", "Teng", ""], ["Wang", "Hui", ""], ["Liao", "Yi", ""], ["Wang", "Zhiwei", ""], ["Jiang", "Xin", ""], ["Yang", "ZhenZhang", ""], ["Wang", "Kaisheng", ""], ["Zhang", "Xiaoda", ""], ["Li", "Chen", ""], ["Gong", "Ziyan", ""], ["Yao", "Yifan", ""], ["Huang", "Xinjing", ""], ["Wang", "Jun", ""], ["Yu", "Jianfeng", ""], ["Guo", "Qi", ""], ["Yu", "Yue", ""], ["Zhang", "Yan", ""], ["Wang", "Jin", ""], ["Tao", "Hengtao", ""], ["Yan", "Dasen", ""], ["Yi", "Zexuan", ""], ["Peng", "Fang", ""], ["Jiang", "Fangqing", ""], ["Zhang", "Han", ""], ["Deng", "Lingfeng", ""], ["Zhang", "Yehong", ""], ["Lin", "Zhe", ""], ["Zhang", "Chao", ""], ["Zhang", "Shaojie", ""], ["Guo", "Mingyue", ""], ["Gu", "Shanzhi", ""], ["Fan", "Gaojun", ""], ["Wang", "Yaowei", ""], ["Jin", "Xuefeng", ""], ["Liu", "Qun", ""], ["Tian", "Yonghong", ""]]}, {"id": "2104.12377", "submitter": "Jiaqi Li", "authors": "Jiaqi Li, Ming Liu, Zihao Zheng, Heng Zhang, Bing Qin, Min-Yen Kan and\n  Ting Liu", "title": "DADgraph: A Discourse-aware Dialogue Graph Neural Network for Multiparty\n  Dialogue Machine Reading Comprehension", "comments": "Accepted by IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiparty Dialogue Machine Reading Comprehension (MRC) differs from\ntraditional MRC as models must handle the complex dialogue discourse structure,\npreviously unconsidered in traditional MRC. To fully exploit such discourse\nstructure in multiparty dialogue, we present a discourse-aware dialogue graph\nneural network, DADgraph, which explicitly constructs the dialogue graph using\ndiscourse dependency links and discourse relations. To validate our model, we\nperform experiments on the Molweni corpus, a large-scale MRC dataset built over\nmultiparty dialogue annotated with discourse structure. Experiments on Molweni\nshow that our discourse-aware model achieves statistically significant\nimprovements compared against strong neural network MRC baselines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:20:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Jiaqi", ""], ["Liu", "Ming", ""], ["Zheng", "Zihao", ""], ["Zhang", "Heng", ""], ["Qin", "Bing", ""], ["Kan", "Min-Yen", ""], ["Liu", "Ting", ""]]}, {"id": "2104.12395", "submitter": "Ryuichi Yamamoto", "authors": "Kosuke Futamata, Byeongseon Park, Ryuichi Yamamoto, Kentaro Tachibana", "title": "Phrase break prediction with bidirectional encoder representations in\n  Japanese text-to-speech synthesis", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel phrase break prediction method that combines implicit\nfeatures extracted from a pre-trained large language model, a.k.a BERT, and\nexplicit features extracted from BiLSTM with linguistic features. In\nconventional BiLSTM based methods, word representations and/or sentence\nrepresentations are used as independent components. The proposed method takes\naccount of both representations to extract the latent semantics, which cannot\nbe captured by previous methods. The objective evaluation results show that the\nproposed method obtains an absolute improvement of 3.2 points for the F1 score\ncompared with BiLSTM-based conventional methods using linguistic features.\nMoreover, the perceptual listening test results verify that a TTS system that\napplied our proposed method achieved a mean opinion score of 4.39 in prosody\nnaturalness, which is highly competitive with the score of 4.37 for synthesized\nspeech with ground-truth phrase breaks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 08:29:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Futamata", "Kosuke", ""], ["Park", "Byeongseon", ""], ["Yamamoto", "Ryuichi", ""], ["Tachibana", "Kentaro", ""]]}, {"id": "2104.12405", "submitter": "Lucio Messina", "authors": "Lucio Messina, Lucia Busso, Claudia Roberta Combei, Ludovica Pannitto,\n  Alessio Miaschi, Gabriele Sarti and Malvina Nissim", "title": "A dissemination workshop for introducing young Italian students to NLP", "comments": "3 pages, 4 figures, accepted at Teaching NLP 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe and make available the game-based material developed for a\nlaboratory run at several Italian science festivals to popularize NLP among\nyoung students.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:00:56 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 07:59:50 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Messina", "Lucio", ""], ["Busso", "Lucia", ""], ["Combei", "Claudia Roberta", ""], ["Pannitto", "Ludovica", ""], ["Miaschi", "Alessio", ""], ["Sarti", "Gabriele", ""], ["Nissim", "Malvina", ""]]}, {"id": "2104.12422", "submitter": "Lucio Messina", "authors": "Ludovica Pannitto, Lucia Busso, Claudia Roberta Combei, Lucio Messina,\n  Alessio Miaschi, Gabriele Sarti and Malvina Nissim", "title": "Teaching NLP with Bracelets and Restaurant Menus: An Interactive\n  Workshop for Italian Students", "comments": "11 pages, 16 figures, accepted at Teaching NLP 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although Natural Language Processing (NLP) is at the core of many tools young\npeople use in their everyday life, high school curricula (in Italy) do not\ninclude any computational linguistics education. This lack of exposure makes\nthe use of such tools less responsible than it could be and makes choosing\ncomputational linguistics as a university degree unlikely. To raise awareness,\ncuriosity, and longer-term interest in young people, we have developed an\ninteractive workshop designed to illustrate the basic principles of NLP and\ncomputational linguistics to high school Italian students aged between 13 and\n18 years. The workshop takes the form of a game in which participants play the\nrole of machines needing to solve some of the most common problems a computer\nfaces in understanding language: from voice recognition to Markov chains to\nsyntactic parsing. Participants are guided through the workshop with the help\nof instructors, who present the activities and explain core concepts from\ncomputational linguistics. The workshop was presented at numerous outlets in\nItaly between 2019 and 2021, both face-to-face and online.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:23:52 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 07:49:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Pannitto", "Ludovica", ""], ["Busso", "Lucia", ""], ["Combei", "Claudia Roberta", ""], ["Messina", "Lucio", ""], ["Miaschi", "Alessio", ""], ["Sarti", "Gabriele", ""], ["Nissim", "Malvina", ""]]}, {"id": "2104.12424", "submitter": "Tom Kersten", "authors": "Tom Kersten, Hugh Mee Wong, Jaap Jumelet, Dieuwke Hupkes", "title": "Attention vs non-attention for a Shapley-based explanation method", "comments": "Accepted for publication at DeeLIO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of explainable AI has recently seen an explosion in the number of\nexplanation methods for highly non-linear deep neural networks. The extent to\nwhich such methods -- that are often proposed and tested in the domain of\ncomputer vision -- are appropriate to address the explainability challenges in\nNLP is yet relatively unexplored. In this work, we consider Contextual\nDecomposition (CD) -- a Shapley-based input feature attribution method that has\nbeen shown to work well for recurrent NLP models -- and we test the extent to\nwhich it is useful for models that contain attention operations. To this end,\nwe extend CD to cover the operations necessary for attention-based models. We\nthen compare how long distance subject-verb relationships are processed by\nmodels with and without attention, considering a number of different syntactic\nstructures in two different languages: English and Dutch. Our experiments\nconfirm that CD can successfully be applied for attention-based models as well,\nproviding an alternative Shapley-based attribution method for modern neural\nnetworks. In particular, using CD, we show that the English and Dutch models\ndemonstrate similar processing behaviour, but that under the hood there are\nconsistent differences between our attention and non-attention models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:33:18 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kersten", "Tom", ""], ["Wong", "Hugh Mee", ""], ["Jumelet", "Jaap", ""], ["Hupkes", "Dieuwke", ""]]}, {"id": "2104.12454", "submitter": "Sebastian Duerr", "authors": "Sebastian Duerr, Krystian Teodor Lange, Peter A. Gloor", "title": "What Makes a Message Persuasive? Identifying Adaptations Towards\n  Persuasiveness in Nine Exploratory Case Studies", "comments": "10 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to persuade others is critical to professional and personal\nsuccess. However, crafting persuasive messages is demanding and poses various\nchallenges. We conducted nine exploratory case studies to identify adaptations\nthat professional and non-professional writers make in written scenarios to\nincrease their subjective persuasiveness. Furthermore, we identified challenges\nthat those writers faced and identified strategies to resolve them with\npersuasive natural language generation, i.e., artificial intelligence. Our\nfindings show that humans can achieve high degrees of persuasiveness (more so\nfor professional-level writers), and artificial intelligence can complement\nthem to achieve increased celerity and alignment in the process.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:35:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Duerr", "Sebastian", ""], ["Lange", "Krystian Teodor", ""], ["Gloor", "Peter A.", ""]]}, {"id": "2104.12465", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring", "title": "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video\n  Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional video summarization methods generate fixed video representations\nregardless of user interest. Therefore such methods limit users' expectations\nin content search and exploration scenarios. Multi-modal video summarization is\none of the methods utilized to address this problem. When multi-modal video\nsummarization is used to help video exploration, a text-based query is\nconsidered as one of the main drivers of video summary generation, as it is\nuser-defined. Thus, encoding the text-based query and the video effectively are\nboth important for the task of multi-modal video summarization. In this work, a\nnew method is proposed that uses a specialized attention network and\ncontextualized word representations to tackle this task. The proposed model\nconsists of a contextualized video summary controller, multi-modal attention\nmechanisms, an interactive attention network, and a video summary generator.\nBased on the evaluation of the existing multi-modal video summarization\nbenchmark, experimental results show that the proposed model is effective with\nthe increase of +5.88% in accuracy and +4.06% increase of F1-score, compared\nwith the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:50:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Murn", "Luka", ""], ["Mrak", "Marta", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12470", "submitter": "GongZheng Li", "authors": "Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Bai Liu, Changjie\n  Fan, Xiaoxi Mao, Zeng Zhao", "title": "Easy and Efficient Transformer : Scalable Inference Solution For large\n  NLP model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, large-scale transformer-based models have been proven to be\neffective over a variety of tasks across many domains. Nevertheless, putting\nthem into production is very expensive, requiring comprehensive optimization\ntechniques to reduce inference costs. This paper introduces a series of\ntransformer inference optimization techniques that are both in algorithm level\nand hardware level. These techniques include a pre-padding decoding mechanism\nthat improves token parallelism for text generation, and highly optimized\nkernels designed for very long input length and large hidden size. On this\nbasis, we propose a transformer inference acceleration library -- Easy and\nEfficient Transformer (EET), which has a significant performance improvement\nover existing libraries. Compared to Faster Transformer v4.0's implementation\nfor GPT-2 layer on A100, EET achieves a 1.5-4.5x state-of-art speedup varying\nwith different context lengths. EET is available at\nhttps://github.com/NetEase-FuXi/EET. A demo video is available at\nhttps://youtu.be/22UPcNGcErg.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:00:56 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 10:28:35 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 09:54:05 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Li", "Gongzheng", ""], ["Xi", "Yadong", ""], ["Ding", "Jingzhen", ""], ["Wang", "Duan", ""], ["Liu", "Bai", ""], ["Fan", "Changjie", ""], ["Mao", "Xiaoxi", ""], ["Zhao", "Zeng", ""]]}, {"id": "2104.12471", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Marcel Worring", "title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image captioning automatically generates a medical description to\ndescribe the content of a given medical image. A traditional medical image\ncaptioning model creates a medical description only based on a single medical\nimage input. Hence, an abstract medical description or concept is hard to be\ngenerated based on the traditional approach. Such a method limits the\neffectiveness of medical image captioning. Multi-modal medical image captioning\nis one of the approaches utilized to address this problem. In multi-modal\nmedical image captioning, textual input, e.g., expert-defined keywords, is\nconsidered as one of the main drivers of medical description generation. Thus,\nencoding the textual input and the medical image effectively are both important\nfor the task of multi-modal medical image captioning. In this work, a new\nend-to-end deep multi-modal medical image captioning model is proposed.\nContextualized keyword representations, textual feature reinforcement, and\nmasked self-attention are used to develop the proposed approach. Based on the\nevaluation of the existing multi-modal medical image captioning dataset,\nexperimental results show that the proposed model is effective with the\nincrease of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:08:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12567", "submitter": "Md Rizwan Parvez", "authors": "Md Rizwan Parvez and Kai-Wei Chang", "title": "Evaluating the Values of Sources in Transfer Learning", "comments": "NAACL 2021 Camera Ready", "journal-ref": "@inproceedings{parvez2021evaluating, title = {Evaluating the\n  Values of Sources in Transfer Learning}, author = {Parvez, Md Rizwan and\n  Chang, Kai-Wei}, booktitle = {{NAACL}}, year = {2021} }", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning that adapts a model trained on data-rich sources to\nlow-resource targets has been widely applied in natural language processing\n(NLP). However, when training a transfer model over multiple sources, not every\nsource is equally useful for the target. To better transfer a model, it is\nessential to understand the values of the sources. In this paper, we develop\nSEAL-Shap, an efficient source valuation framework for quantifying the\nusefulness of the sources (e.g., domains/languages) in transfer learning based\non the Shapley value method. Experiments and comprehensive analyses on both\ncross-domain and cross-lingual transfers demonstrate that our framework is not\nonly effective in choosing useful transfer sources but also the source values\nmatch the intuitive source-target similarity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:35:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Parvez", "Md Rizwan", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2104.12643", "submitter": "Jialin Yu", "authors": "Jialin Yu, Laila Alrajhi, Anoushka Harit, Zhongtian Sun, Alexandra I.\n  Cristea, Lei Shi", "title": "Exploring Bayesian Deep Learning for Urgent Instructor Intervention Need\n  in MOOC Forums", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-80421-3_10", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive Open Online Courses (MOOCs) have become a popular choice for\ne-learning thanks to their great flexibility. However, due to large numbers of\nlearners and their diverse backgrounds, it is taxing to offer real-time\nsupport. Learners may post their feelings of confusion and struggle in the\nrespective MOOC forums, but with the large volume of posts and high workloads\nfor MOOC instructors, it is unlikely that the instructors can identify all\nlearners requiring intervention. This problem has been studied as a Natural\nLanguage Processing (NLP) problem recently, and is known to be challenging, due\nto the imbalance of the data and the complex nature of the task. In this paper,\nwe explore for the first time Bayesian deep learning on learner-based text\nposts with two methods: Monte Carlo Dropout and Variational Inference, as a new\nsolution to assessing the need of instructor interventions for a learner's\npost. We compare models based on our proposed methods with probabilistic\nmodelling to its baseline non-Bayesian models under similar circumstances, for\ndifferent cases of applying prediction. The results suggest that Bayesian deep\nlearning offers a critical uncertainty measure that is not supplied by\ntraditional neural networks. This adds more explainability, trust and\nrobustness to AI, which is crucial in education-based applications.\nAdditionally, it can achieve similar or better performance compared to\nnon-probabilistic neural networks, as well as grant lower variance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:12:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yu", "Jialin", ""], ["Alrajhi", "Laila", ""], ["Harit", "Anoushka", ""], ["Sun", "Zhongtian", ""], ["Cristea", "Alexandra I.", ""], ["Shi", "Lei", ""]]}, {"id": "2104.12677", "submitter": "Howard Chen", "authors": "Howard Chen, Mengzhou Xia, and Danqi Chen", "title": "Non-Parametric Few-Shot Learning for Word Sense Disambiguation", "comments": "NAACL 2021. Our code is publicly available at:\n  https://github.com/princeton-nlp/metric-wsd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word sense disambiguation (WSD) is a long-standing problem in natural\nlanguage processing. One significant challenge in supervised all-words WSD is\nto classify among senses for a majority of words that lie in the long-tail\ndistribution. For instance, 84% of the annotated words have less than 10\nexamples in the SemCor training data. This issue is more pronounced as the\nimbalance occurs in both word and sense distributions. In this work, we propose\nMetricWSD, a non-parametric few-shot learning approach to mitigate this data\nimbalance issue. By learning to compute distances among the senses of a given\nword through episodic training, MetricWSD transfers knowledge (a learned metric\nspace) from high-frequency words to infrequent ones. MetricWSD constructs the\ntraining episodes tailored to word frequencies and explicitly addresses the\nproblem of the skewed distribution, as opposed to mixing all the words trained\nwith parametric models in previous work. Without resorting to any lexical\nresources, MetricWSD obtains strong performance against parametric\nalternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark\n(Raganato et al., 2017b). Our analysis further validates that infrequent words\nand senses enjoy significant improvement.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:08:46 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 13:28:37 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chen", "Howard", ""], ["Xia", "Mengzhou", ""], ["Chen", "Danqi", ""]]}, {"id": "2104.12714", "submitter": "Shrimai Prabhumoye", "authors": "Shrimai Prabhumoye, Kazuma Hashimoto, Yingbo Zhou, Alan W Black,\n  Ruslan Salakhutdinov", "title": "Focused Attention Improves Document-Grounded Generation", "comments": "Accepted at North American Chapter of the Association for\n  Computational Linguistics (NAACL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document grounded generation is the task of using the information provided in\na document to improve text generation. This work focuses on two different\ndocument grounded generation tasks: Wikipedia Update Generation task and\nDialogue response generation. Our work introduces two novel adaptations of\nlarge scale pre-trained encoder-decoder models focusing on building context\ndriven representation of the document and enabling specific attention to the\ninformation in the document. Additionally, we provide a stronger BART baseline\nfor these tasks. Our proposed techniques outperform existing methods on both\nautomated (at least 48% increase in BLEU-4 points) and human evaluation for\ncloseness to reference and relevance to the document. Furthermore, we perform\ncomprehensive manual inspection of the generated output and categorize errors\nto provide insights into future directions in modeling these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:56:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Prabhumoye", "Shrimai", ""], ["Hashimoto", "Kazuma", ""], ["Zhou", "Yingbo", ""], ["Black", "Alan W", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2104.12741", "submitter": "Julian Risch", "authors": "Timo M\\\"oller and Julian Risch and Malte Pietsch", "title": "GermanQuAD and GermanDPR: Improving Non-English Question Answering and\n  Passage Retrieval", "comments": "See https://deepset.ai/germanquad for downloading the datasets and\n  models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major challenge of research on non-English machine reading for question\nanswering (QA) is the lack of annotated datasets. In this paper, we present\nGermanQuAD, a dataset of 13,722 extractive question/answer pairs. To improve\nthe reproducibility of the dataset creation approach and foster QA research on\nother languages, we summarize lessons learned and evaluate reformulation of\nquestion/answer pairs as a way to speed up the annotation process. An\nextractive QA model trained on GermanQuAD significantly outperforms\nmultilingual models and also shows that machine-translated training data cannot\nfully substitute hand-annotated training data in the target language. Finally,\nwe demonstrate the wide range of applications of GermanQuAD by adapting it to\nGermanDPR, a training dataset for dense passage retrieval (DPR), and train and\nevaluate the first non-English DPR model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:34:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["M\u00f6ller", "Timo", ""], ["Risch", "Julian", ""], ["Pietsch", "Malte", ""]]}, {"id": "2104.12755", "submitter": "Hadi Jahanshahi", "authors": "Hadi Jahanshahi, Syed Kazmi, Mucahit Cevik", "title": "Auto Response Generation in Online Medical Chat Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telehealth helps to facilitate access to medical professionals by enabling\nremote medical services for the patients. These services have become gradually\npopular over the years with the advent of necessary technological\ninfrastructure. The benefits of telehealth have been even more apparent since\nthe beginning of the COVID-19 crisis, as people have become less inclined to\nvisit doctors in person during the pandemic. In this paper, we focus on\nfacilitating the chat sessions between a doctor and a patient. We note that the\nquality and efficiency of the chat experience can be critical as the demand for\ntelehealth services increases. Accordingly, we develop a smart auto-response\ngeneration mechanism for medical conversations that helps doctors respond to\nconsultation requests efficiently, particularly during busy sessions. We\nexplore over 900,000 anonymous, historical online messages between doctors and\npatients collected over nine months. We implement clustering algorithms to\nidentify the most frequent responses by doctors and manually label the data\naccordingly. We then train machine learning algorithms using this preprocessed\ndata to generate the responses. The considered algorithm has two steps: a\nfiltering (i.e., triggering) model to filter out infeasible patient messages\nand a response generator to suggest the top-3 doctor responses for the ones\nthat successfully pass the triggering phase. The method provides an accuracy of\n83.28\\% for precision@3 and shows robustness to its parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:45:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jahanshahi", "Hadi", ""], ["Kazmi", "Syed", ""], ["Cevik", "Mucahit", ""]]}, {"id": "2104.12756", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Viraj Bagal, Rub\\`en P\\'erez Tito, Dimosthenis\n  Karatzas, Ernest Valveny, C.V Jawahar", "title": "InfographicVQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographics are documents designed to effectively communicate information\nusing a combination of textual, graphical and visual elements. In this work, we\nexplore the automatic understanding of infographic images by using Visual\nQuestion Answering technique.To this end, we present InfographicVQA, a new\ndataset that comprises a diverse collection of infographics along with natural\nlanguage questions and answers annotations. The collected questions require\nmethods to jointly reason over the document layout, textual content, graphical\nelements, and data visualizations. We curate the dataset with emphasis on\nquestions that require elementary reasoning and basic arithmetic skills.\nFinally, we evaluate two strong baselines based on state of the art multi-modal\nVQA models, and establish baseline performance for the new task. The dataset,\ncode and leaderboard will be made available at http://docvqa.org\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:45:54 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mathew", "Minesh", ""], ["Bagal", "Viraj", ""], ["Tito", "Rub\u00e8n P\u00e9rez", ""], ["Karatzas", "Dimosthenis", ""], ["Valveny", "Ernest", ""], ["Jawahar", "C. V", ""]]}, {"id": "2104.12763", "submitter": "Aishwarya Kamath", "authors": "Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel\n  Synnaeve, Nicolas Carion", "title": "MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:55:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kamath", "Aishwarya", ""], ["Singh", "Mannat", ""], ["LeCun", "Yann", ""], ["Misra", "Ishan", ""], ["Synnaeve", "Gabriel", ""], ["Carion", "Nicolas", ""]]}, {"id": "2104.12846", "submitter": "Ekaterina Artemova", "authors": "Ekaterina Artemova and Murat Apishev and Veronika Sarkisyan and Sergey\n  Aksenov and Denis Kirjanov and Oleg Serikov", "title": "Teaching a Massive Open Online Course on Natural Language Processing", "comments": "To appear in the Proceedings of the Fifth Workshop on Teaching NLP @\n  NAACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new Massive Open Online Course on Natural Language\nProcessing, targeted at non-English speaking students. The course lasts 12\nweeks; every week consists of lectures, practical sessions, and quiz\nassignments. Three weeks out of 12 are followed by Kaggle-style coding\nassignments.\n  Our course intends to serve multiple purposes: (i) familiarize students with\nthe core concepts and methods in NLP, such as language modeling or word or\nsentence representations, (ii) show that recent advances, including pre-trained\nTransformer-based models, are built upon these concepts; (iii) introduce\narchitectures for most demanded real-life applications, (iv) develop practical\nskills to process texts in multiple languages. The course was prepared and\nrecorded during 2020, launched by the end of the year, and in early 2021 has\nreceived positive feedback.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:52:00 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 19:49:08 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Artemova", "Ekaterina", ""], ["Apishev", "Murat", ""], ["Sarkisyan", "Veronika", ""], ["Aksenov", "Sergey", ""], ["Kirjanov", "Denis", ""], ["Serikov", "Oleg", ""]]}, {"id": "2104.12847", "submitter": "Ekaterina Artemova", "authors": "Vladislav Mikhailov and Oleg Serikov and Ekaterina Artemova", "title": "Morph Call: Probing Morphosyntactic Content of Multilingual Transformers", "comments": "To appear in the Proceedings of the 3rd Workshop on Research in\n  Computational Typology and Multilingual NLP (SIGTYP, NAACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The outstanding performance of transformer-based language models on a great\nvariety of NLP and NLU tasks has stimulated interest in exploring their inner\nworkings. Recent research has focused primarily on higher-level and complex\nlinguistic phenomena such as syntax, semantics, world knowledge, and common\nsense. The majority of the studies are anglocentric, and little remains known\nregarding other languages, precisely their morphosyntactic properties. To this\nend, our work presents Morph Call, a suite of 46 probing tasks for four\nIndo-European languages of different morphology: English, French, German and\nRussian. We propose a new type of probing task based on the detection of guided\nsentence perturbations. We use a combination of neuron-, layer- and\nrepresentation-level introspection techniques to analyze the morphosyntactic\ncontent of four multilingual transformers, including their less explored\ndistilled versions. Besides, we examine how fine-tuning for POS-tagging affects\nthe model knowledge. The results show that fine-tuning can improve and decrease\nthe probing performance and change how morphosyntactic knowledge is distributed\nacross the model. The code and data are publicly available, and we hope to fill\nthe gaps in the less studied aspect of transformers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:53:00 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 19:27:10 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mikhailov", "Vladislav", ""], ["Serikov", "Oleg", ""], ["Artemova", "Ekaterina", ""]]}, {"id": "2104.12869", "submitter": "Neslihan Suzen", "authors": "Neslihan Suzen, Alexander Gorban, Jeremy Levesley and Evgeny Mirkes", "title": "Semantic Analysis for Automated Evaluation of the Potential Impact of\n  Research Articles", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can the analysis of the semantics of words used in the text of a scientific\npaper predict its future impact measured by citations? This study details\nexamples of automated text classification that achieved 80% success rate in\ndistinguishing between highly-cited and little-cited articles. Automated\nintelligent systems allow the identification of promising works that could\nbecome influential in the scientific community.\n  The problems of quantifying the meaning of texts and representation of human\nlanguage have been clear since the inception of Natural Language Processing.\nThis paper presents a novel method for vector representation of text meaning\nbased on information theory and show how this informational semantics is used\nfor text classification on the basis of the Leicester Scientific Corpus.\n  We describe the experimental framework used to evaluate the impact of\nscientific articles through their informational semantics. Our interest is in\ncitation classification to discover how important semantics of texts are in\npredicting the citation count. We propose the semantics of texts as an\nimportant factor for citation prediction.\n  For each article, our system extracts the abstract of paper, represents the\nwords of the abstract as vectors in Meaning Space, automatically analyses the\ndistribution of scientific categories (Web of Science categories) within the\ntext of abstract, and then classifies papers according to citation counts\n(highly-cited, little-cited).\n  We show that an informational approach to representing the meaning of a text\nhas offered a way to effectively predict the scientific impact of research\npapers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 20:37:13 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Suzen", "Neslihan", ""], ["Gorban", "Alexander", ""], ["Levesley", "Jeremy", ""], ["Mirkes", "Evgeny", ""]]}, {"id": "2104.12870", "submitter": "David Qiu", "authors": "David Qiu, Yanzhang He, Qiujia Li, Yu Zhang, Liangliang Cao, Ian\n  McGraw", "title": "Multi-Task Learning for End-to-End ASR Word and Utterance Confidence\n  with Deletion Prediction", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence scores are very useful for downstream applications of automatic\nspeech recognition (ASR) systems. Recent works have proposed using neural\nnetworks to learn word or utterance confidence scores for end-to-end ASR. In\nthose studies, word confidence by itself does not model deletions, and\nutterance confidence does not take advantage of word-level training signals.\nThis paper proposes to jointly learn word confidence, word deletion, and\nutterance confidence. Empirical results show that multi-task learning with all\nthree objectives improves confidence metrics (NCE, AUC, RMSE) without the need\nfor increasing the model size of the confidence estimation module. Using the\nutterance-level confidence for rescoring also decreases the word error rates on\nGoogle's Voice Search and Long-tail Maps datasets by 3-5% relative, without\nneeding a dedicated neural rescorer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 20:38:42 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Qiu", "David", ""], ["He", "Yanzhang", ""], ["Li", "Qiujia", ""], ["Zhang", "Yu", ""], ["Cao", "Liangliang", ""], ["McGraw", "Ian", ""]]}, {"id": "2104.12874", "submitter": "Soo Hyun Ryu", "authors": "Soo Hyun Ryu and Richard L. Lewis", "title": "Accounting for Agreement Phenomena in Sentence Comprehension with\n  Transformer Language Models: Effects of Similarity-based Interference on\n  Surprisal and Attention", "comments": "CMCL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We advance a novel explanation of similarity-based interference effects in\nsubject-verb and reflexive pronoun agreement processing, grounded in surprisal\nvalues computed from a pretrained large-scale Transformer model, GPT-2.\nSpecifically, we show that surprisal of the verb or reflexive pronoun predicts\nfacilitatory interference effects in ungrammatical sentences, where a\ndistractor noun that matches in number with the verb or pronoun leads to faster\nreading times, despite the distractor not participating in the agreement\nrelation. We review the human empirical evidence for such effects, including\nrecent meta-analyses and large-scale studies. We also show that attention\npatterns (indexed by entropy and other measures) in the Transformer show\npatterns of diffuse attention in the presence of similar distractors,\nconsistent with cue-based retrieval models of parsing. But in contrast to these\nmodels, the attentional cues and memory representations are learned entirely\nfrom the simple self-supervised task of predicting the next word.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 20:46:54 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ryu", "Soo Hyun", ""], ["Lewis", "Richard L.", ""]]}, {"id": "2104.12918", "submitter": "Ashkan Kazemi", "authors": "Ashkan Kazemi, Zehua Li, Ver\\'onica P\\'erez-Rosas, Rada Mihalcea", "title": "Extractive and Abstractive Explanations for Fact-Checking and Evaluation\n  of News", "comments": "Accepted to NLP for Internet Freedom Workshop at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the construction of natural language explanations\nfor news claims, with the goal of assisting fact-checking and news evaluation\napplications. We experiment with two methods: (1) an extractive method based on\nBiased TextRank -- a resource-effective unsupervised graph-based algorithm for\ncontent extraction; and (2) an abstractive method based on the GPT-2 language\nmodel. We perform comparative evaluations on two misinformation datasets in the\npolitical and health news domains, and find that the extractive method shows\nthe most promise.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:21:55 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kazemi", "Ashkan", ""], ["Li", "Zehua", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2104.12950", "submitter": "Balaji Ganesan", "authors": "Abhay M Shalghar, Ayush Kumar, Balaji Ganesan, Aswin Kannan, Shobha G", "title": "Document Structure aware Relational Graph Convolutional Networks for\n  Ontology Population", "comments": "16 pages single column, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies comprising of concepts, their attributes, and relationships, form\nthe quintessential backbone of many knowledge based AI systems. These systems\nmanifest in the form of question-answering or dialogue in number of business\nanalytics and master data management applications. While there have been\nefforts towards populating domain specific ontologies, we examine the role of\ndocument structure in learning ontological relationships between concepts in\nany document corpus. Inspired by ideas from hypernym discovery and\nexplainability, our method performs about 15 points more accurate than a\nstand-alone R-GCN model for this task.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:50:39 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Shalghar", "Abhay M", ""], ["Kumar", "Ayush", ""], ["Ganesan", "Balaji", ""], ["Kannan", "Aswin", ""], ["G", "Shobha", ""]]}, {"id": "2104.12977", "submitter": "Yang Feng", "authors": "Jicheng Li, Yang Feng, Jiao Ou", "title": "SE-DAE: Style-Enhanced Denoising Auto-Encoder for Unsupervised Text\n  Style Transfer", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text style transfer aims to change the style of sentences while preserving\nthe semantic meanings. Due to the lack of parallel data, the Denoising\nAuto-Encoder (DAE) is widely used in this task to model distributions of\ndifferent sentence styles. However, because of the conflict between the target\nof the conventional denoising procedure and the target of style transfer task,\nthe vanilla DAE can not produce satisfying enough results. To improve the\ntransferability of the model, most of the existing works combine DAE with\nvarious complicated unsupervised networks, which makes the whole system become\nover-complex. In this work, we design a novel DAE model named Style-Enhanced\nDAE (SE-DAE), which is specifically designed for the text style transfer task.\nCompared with previous complicated style-transfer models, our model do not\nconsist of any complicated unsupervised networks, but only relies on the\nhigh-quality pseudo-parallel data generated by a novel data refinement\nmechanism. Moreover, to alleviate the conflict between the targets of the\nconventional denoising procedure and the style transfer task, we propose\nanother novel style denoising mechanism, which is more compatible with the\ntarget of the style transfer task. We validate the effectiveness of our model\non two style benchmark datasets. Both automatic evaluation and human evaluation\nshow that our proposed model is highly competitive compared with previous\nstrong the state of the art (SOTA) approaches and greatly outperforms the\nvanilla DAE.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 04:41:18 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Li", "Jicheng", ""], ["Feng", "Yang", ""], ["Ou", "Jiao", ""]]}, {"id": "2104.13043", "submitter": "Yves Bestgen", "authors": "Yves Bestgen", "title": "LAST at CMCL 2021 Shared Task: Predicting Gaze Data During Reading with\n  a Gradient Boosting Decision Tree Approach", "comments": "To be published in the Proceedings of the Workshop on Cognitive\n  Modeling and Computational Linguistics, co-located with NAACL 2021 in Mexico\n  City, Mexico (virtual), on the 10th of June 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A LightGBM model fed with target word lexical characteristics and features\nobtained from word frequency lists, psychometric data and bigram association\nmeasures has been optimized for the 2021 CMCL Shared Task on Eye-Tracking Data\nPrediction. It obtained the best performance of all teams on two of the five\neye-tracking measures to predict, allowing it to rank first on the official\nchallenge criterion and to outperform all deep-learning based systems\nparticipating in the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:39:52 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Bestgen", "Yves", ""]]}, {"id": "2104.13095", "submitter": "Guanglin Niu", "authors": "Guanglin Niu, Yang Li, Chengguang Tang, Ruiying Geng, Jian Dai, Qiao\n  Liu, Hao Wang, Jian Sun, Fei Huang, Luo Si", "title": "Relational Learning with Gated and Attentive Neighbor Aggregator for\n  Few-Shot Knowledge Graph Completion", "comments": "The full version of a paper accepted to SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),\nfew-shot knowledge graph completion (FKGC) has recently gained more research\ninterests. Some existing models employ a few-shot relation's multi-hop neighbor\ninformation to enhance its semantic representation. However, noise neighbor\ninformation might be amplified when the neighborhood is excessively sparse and\nno neighbor is available to represent the few-shot relation. Moreover, modeling\nand inferring complex relations of one-to-many (1-N), many-to-one (N-1), and\nmany-to-many (N-N) by previous knowledge graph completion approaches requires\nhigh model complexity and a large amount of training instances. Thus, inferring\ncomplex relations in the few-shot scenario is difficult for FKGC models due to\nlimited training instances. In this paper, we propose a few-shot relational\nlearning with global-local framework to address the above issues. At the global\nstage, a novel gated and attentive neighbor aggregator is built for accurately\nintegrating the semantics of a few-shot relation's neighborhood, which helps\nfiltering the noise neighbors even if a KG contains extremely sparse\nneighborhoods. For the local stage, a meta-learning based TransH (MTransH)\nmethod is designed to model complex relations and train our model in a few-shot\nlearning fashion. Extensive experiments show that our model outperforms the\nstate-of-the-art FKGC approaches on the frequently-used benchmark datasets\nNELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model\nachieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on\nWiki-One by the metric Hits@10.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:38:44 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 02:56:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Niu", "Guanglin", ""], ["Li", "Yang", ""], ["Tang", "Chengguang", ""], ["Geng", "Ruiying", ""], ["Dai", "Jian", ""], ["Liu", "Qiao", ""], ["Wang", "Hao", ""], ["Sun", "Jian", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""]]}, {"id": "2104.13100", "submitter": "Pietro Liguori", "authors": "Pietro Liguori, Erfan Al-Hossami, Domenico Cotroneo, Roberto Natella,\n  Bojan Cukic and Samira Shaikh", "title": "Shellcode_IA32: A Dataset for Automatic Shellcode Generation", "comments": "Paper accepted to NLP4Prog Workshop 2021 co-located with ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We take the first step to address the task of automatically generating\nshellcodes, i.e., small pieces of code used as a payload in the exploitation of\na software vulnerability, starting from natural language comments. We assemble\nand release a novel dataset (Shellcode_IA32), consisting of challenging but\ncommon assembly instructions with their natural language descriptions. We\nexperiment with standard methods in neural machine translation (NMT) to\nestablish baseline performance levels on this task.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:50:47 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:41:21 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 09:23:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liguori", "Pietro", ""], ["Al-Hossami", "Erfan", ""], ["Cotroneo", "Domenico", ""], ["Natella", "Roberto", ""], ["Cukic", "Bojan", ""], ["Shaikh", "Samira", ""]]}, {"id": "2104.13103", "submitter": "Yoshinari Fujinuma", "authors": "Yoshinari Fujinuma, Masato Hagiwara", "title": "Semi-Supervised Joint Estimation of Word and Document Readability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readability or difficulty estimation of words and documents has been\ninvestigated independently in the literature, often assuming the existence of\nextensive annotated resources for the other. Motivated by our analysis showing\nthat there is a recursive relationship between word and document difficulty, we\npropose to jointly estimate word and document difficulty through a graph\nconvolutional network (GCN) in a semi-supervised fashion. Our experimental\nresults reveal that the GCN-based method can achieve higher accuracy than\nstrong baselines, and stays robust even with a smaller amount of labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:56:47 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Fujinuma", "Yoshinari", ""], ["Hagiwara", "Masato", ""]]}, {"id": "2104.13164", "submitter": "Hamed Babaei Giglou", "authors": "Hamed Babaei Giglou, Taher Rahgooy, Mostafa Rahgouy and Jafar Razmara", "title": "UoT-UWF-PartAI at SemEval-2021 Task 5: Self Attention Based Bi-GRU with\n  Multi-Embedding Representation for Toxicity Highlighter", "comments": "Accepted at SemEval-2021 Task 5: Toxic Spans Detection, ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Toxic Spans Detection(TSD) task is defined as highlighting spans that make a\ntext toxic. Many works have been done to classify a given comment or document\nas toxic or non-toxic. However, none of those proposed models work at the token\nlevel. In this paper, we propose a self-attention-based bidirectional gated\nrecurrent unit(BiGRU) with a multi-embedding representation of the tokens. Our\nproposed model enriches the representation by a combination of GPT-2, GloVe,\nand RoBERTa embeddings, which led to promising results. Experimental results\nshow that our proposed approach is very effective in detecting span tokens.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:18:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Giglou", "Hamed Babaei", ""], ["Rahgooy", "Taher", ""], ["Rahgouy", "Mostafa", ""], ["Razmara", "Jafar", ""]]}, {"id": "2104.13173", "submitter": "Xinmeng Li", "authors": "Xinmeng Li, Mamoun Alazab, Qian Li, Keping Yu, Quanjun Yin", "title": "Question-Aware Memory Network for Multi-hop Question Answering in\n  Human-Robot Interaction", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph question answering is an important technology in intelligent\nhuman-robot interaction, which aims at automatically giving answer to human\nnatural language question with the given knowledge graph. For the\nmulti-relation question with higher variety and complexity, the tokens of the\nquestion have different priority for the triples selection in the reasoning\nsteps. Most existing models take the question as a whole and ignore the\npriority information in it. To solve this problem, we propose question-aware\nmemory network for multi-hop question answering, named QA2MN, to update the\nattention on question timely in the reasoning process. In addition, we\nincorporate graph context information into knowledge graph embedding model to\nincrease the ability to represent entities and relations. We use it to\ninitialize the QA2MN model and fine-tune it in the training process. We\nevaluate QA2MN on PathQuestion and WorldCup2014, two representative datasets\nfor complex multi-hop question answering. The result demonstrates that QA2MN\nachieves state-of-the-art Hits@1 accuracy on the two datasets, which validates\nthe effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:32:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Xinmeng", ""], ["Alazab", "Mamoun", ""], ["Li", "Qian", ""], ["Yu", "Keping", ""], ["Yin", "Quanjun", ""]]}, {"id": "2104.13225", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Grzegorz Chrupa{\\l}a", "title": "Visually grounded models of spoken language: A survey of datasets,\n  architectures and evaluation techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This survey provides an overview of the evolution of visually grounded models\nof spoken language over the last 20 years. Such models are inspired by the\nobservation that when children pick up a language, they rely on a wide range of\nindirect and noisy clues, crucially including signals from the visual modality\nco-occurring with spoken utterances. Several fields have made important\ncontributions to this approach to modeling or mimicking the process of learning\nlanguage: Machine Learning, Natural Language and Speech Processing, Computer\nVision and Cognitive Science. The current paper brings together these\ncontributions in order to provide a useful introduction and overview for\npractitioners in all these areas. We discuss the central research questions\naddressed, the timeline of developments, and the datasets which enabled much of\nthis work. We then summarize the main modeling architectures and offer an\nexhaustive overview of the evaluation metrics and analysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:32:22 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 14:59:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "2104.13346", "submitter": "Artidoro Pagnoni", "authors": "Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov", "title": "Understanding Factuality in Abstractive Summarization with FRANK: A\n  Benchmark for Factuality Metrics", "comments": "Accepted at NAACL 2021. Second version fixes bug with BERTScore\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern summarization models generate highly fluent but often factually\nunreliable outputs. This motivated a surge of metrics attempting to measure the\nfactuality of automatically generated summaries. Due to the lack of common\nbenchmarks, these metrics cannot be compared. Moreover, all these methods treat\nfactuality as a binary concept and fail to provide deeper insights into the\nkinds of inconsistencies made by different systems. To address these\nlimitations, we devise a typology of factual errors and use it to collect human\nannotations of generated summaries from state-of-the-art summarization systems\nfor the CNN/DM and XSum datasets. Through these annotations, we identify the\nproportion of different categories of factual errors in various summarization\nmodels and benchmark factuality metrics, showing their correlation with human\njudgment as well as their specific strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:28:07 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 22:56:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pagnoni", "Artidoro", ""], ["Balachandran", "Vidhisha", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2104.13406", "submitter": "Eda Okur", "authors": "Saurav Sahay, Eda Okur, Nagib Hakim, Lama Nachman", "title": "Semi-supervised Interactive Intent Labeling", "comments": "NAACL 2021 - Workshop on Data Science with Human-in-the-loop:\n  Language Advances (DaSH-LA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building the Natural Language Understanding (NLU) modules of task-oriented\nSpoken Dialogue Systems (SDS) involves a definition of intents and entities,\ncollection of task-relevant data, annotating the data with intents and\nentities, and then repeating the same process over and over again for adding\nany functionality/enhancement to the SDS. In this work, we showcase an Intent\nBulk Labeling system where SDS developers can interactively label and augment\ntraining data from unlabeled utterance corpora using advanced clustering and\nvisual labeling methods. We extend the Deep Aligned Clustering work with a\nbetter backbone BERT model, explore techniques to select the seed data for\nlabeling, and develop a data balancing method using an oversampling technique\nthat utilizes paraphrasing models. We also look at the effect of data\naugmentation on the clustering process. Our results show that we can achieve\nover 10% gain in clustering accuracy on some datasets using the combination of\nthe above techniques. Finally, we extract utterance embeddings from the\nclustering model and plot the data to interactively bulk label the samples,\nreducing the time and effort for data labeling of the whole dataset\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:06:55 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 02:01:51 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sahay", "Saurav", ""], ["Okur", "Eda", ""], ["Hakim", "Nagib", ""], ["Nachman", "Lama", ""]]}, {"id": "2104.13456", "submitter": "Adrian {\\L}a\\'ncucki", "authors": "Pawe{\\l} Rychlikowski, Bart{\\l}omiej Najdecki, Adrian {\\L}a\\'ncucki,\n  Adam Kaczmarek", "title": "Named Entity Recognition and Linking Augmented with Large-Scale\n  Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our submissions to the 2nd and 3rd SlavNER Shared\nTasks held at BSNLP 2019 and BSNLP 2021, respectively. The tasks focused on the\nanalysis of Named Entities in multilingual Web documents in Slavic languages\nwith rich inflection. Our solution takes advantage of large collections of both\nunstructured and structured documents. The former serve as data for\nunsupervised training of language models and embeddings of lexical units. The\nlatter refers to Wikipedia and its structured counterpart - Wikidata, our\nsource of lemmatization rules, and real-world entities. With the aid of those\nresources, our system could recognize, normalize and link entities, while being\ntrained with only small amounts of labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:10:18 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rychlikowski", "Pawe\u0142", ""], ["Najdecki", "Bart\u0142omiej", ""], ["\u0141a\u0144cucki", "Adrian", ""], ["Kaczmarek", "Adam", ""]]}, {"id": "2104.13484", "submitter": "Mahmoud Hossam", "authors": "Mahmoud Hossam, Trung Le, He Zhao, Viet Huynh, Dinh Phung", "title": "Improved and Efficient Text Adversarial Attacks using Target Information", "comments": "Accepted in the International Conference on Learning Representations\n  (ICLR) workshop on Robust and Reliable Machine Learning in the Real World\n  (RobustML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been recently a growing interest in studying adversarial examples\non natural language models in the black-box setting. These methods attack\nnatural language classifiers by perturbing certain important words until the\nclassifier label is changed. In order to find these important words, these\nmethods rank all words by importance by querying the target model word by word\nfor each input sentence, resulting in high query inefficiency. A new\ninteresting approach was introduced that addresses this problem through\ninterpretable learning to learn the word ranking instead of previous expensive\nsearch. The main advantage of using this approach is that it achieves\ncomparable attack rates to the state-of-the-art methods, yet faster and with\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\nattacking agent. Nonetheless, this approach sacrificed the useful information\nthat could be leveraged from the target classifier for that sake of query\nefficiency. In this paper we study the effect of leveraging the target model\noutputs and data on both attack rates and average number of queries, and we\nshow that both can be improved, with a limited overhead of additional queries.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:25:55 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 08:49:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hossam", "Mahmoud", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Huynh", "Viet", ""], ["Phung", "Dinh", ""]]}, {"id": "2104.13488", "submitter": "Mahmoud Hossam", "authors": "Mahmoud Hossam, Trung Le, Michael Papasimeon, Viet Huynh, Dinh Phung", "title": "Text Generation with Deep Variational GAN", "comments": "Accepted in the Third Workshop on Bayesian Deep Learning (NIPS /\n  NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic sequences is a central task in many machine learning\napplications. There has been considerable recent progress on building deep\ngenerative models for sequence generation tasks. However, the issue of\nmode-collapsing remains a main issue for the current models. In this paper we\npropose a GAN-based generic framework to address the problem of mode-collapse\nin a principled approach. We change the standard GAN objective to maximize a\nvariational lower-bound of the log-likelihood while minimizing the\nJensen-Shanon divergence between data and model distributions. We experiment\nour model with text generation task and show that it can generate realistic\ntext with high diversity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:42:13 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Hossam", "Mahmoud", ""], ["Le", "Trung", ""], ["Papasimeon", "Michael", ""], ["Huynh", "Viet", ""], ["Phung", "Dinh", ""]]}, {"id": "2104.13498", "submitter": "Han-Chin Shing", "authors": "Han-Chin Shing, Chaitanya Shivade, Nima Pourdamghani, Feng Nan, Philip\n  Resnik, Douglas Oard and Parminder Bhatia", "title": "Towards Clinical Encounter Summarization: Learning to Compose Discharge\n  Summaries from Prior Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The records of a clinical encounter can be extensive and complex, thus\nplacing a premium on tools that can extract and summarize relevant information.\nThis paper introduces the task of generating discharge summaries for a clinical\nencounter. Summaries in this setting need to be faithful, traceable, and scale\nto multiple long documents, motivating the use of extract-then-abstract\nsummarization cascades. We introduce two new measures, faithfulness and\nhallucination rate for evaluation in this task, which complement existing\nmeasures for fluency and informativeness. Results across seven medical sections\nand five models show that a summarization architecture that supports\ntraceability yields promising results, and that a sentence-rewriting approach\nperforms consistently on the measure used for faithfulness\n(faithfulness-adjusted $F_3$) over a diverse range of generated sections.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 22:45:54 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Shing", "Han-Chin", ""], ["Shivade", "Chaitanya", ""], ["Pourdamghani", "Nima", ""], ["Nan", "Feng", ""], ["Resnik", "Philip", ""], ["Oard", "Douglas", ""], ["Bhatia", "Parminder", ""]]}, {"id": "2104.13559", "submitter": "Tariq Alhindi", "authors": "Tariq Alhindi, Amal Alabdulkarim, Ali Alshehri, Muhammad Abdul-Mageed\n  and Preslav Nakov", "title": "AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance\n  Detection for Fact Checking", "comments": "Accepted to the 2021 Workshop on NLP4IF: Censorship, Disinformation,\n  and Propaganda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the continuing spread of misinformation and disinformation online, it is\nof increasing importance to develop combating mechanisms at scale in the form\nof automated systems that support multiple languages. One task of interest is\nclaim veracity prediction, which can be addressed using stance detection with\nrespect to relevant documents retrieved online. To this end, we present our new\nArabic Stance Detection dataset (AraStance) of 4,063 claim--article pairs from\na diverse set of sources comprising three fact-checking websites and one news\nwebsite. AraStance covers false and true claims from multiple domains (e.g.,\npolitics, sports, health) and several Arab countries, and it is well-balanced\nbetween related and unrelated documents with respect to the claims. We\nbenchmark AraStance, along with two other stance detection datasets, using a\nnumber of BERT-based models. Our best model achieves an accuracy of 85\\% and a\nmacro F1 score of 78\\%, which leaves room for improvement and reflects the\nchallenging nature of AraStance and the task of stance detection in general.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 03:38:24 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 05:41:05 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Alhindi", "Tariq", ""], ["Alabdulkarim", "Amal", ""], ["Alshehri", "Ali", ""], ["Abdul-Mageed", "Muhammad", ""], ["Nakov", "Preslav", ""]]}, {"id": "2104.13579", "submitter": "Wei Ye", "authors": "Bo Li, Wei Ye, Canming Huang, and Shikun Zhang", "title": "Multi-view Inference for Relation Extraction with Uncertain Knowledge", "comments": "Published at AAAI 2021, full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs (KGs) are widely used to facilitate relation extraction (RE)\ntasks. While most previous RE methods focus on leveraging deterministic KGs,\nuncertain KGs, which assign a confidence score for each relation instance, can\nprovide prior probability distributions of relational facts as valuable\nexternal knowledge for RE models. This paper proposes to exploit uncertain\nknowledge to improve relation extraction. Specifically, we introduce ProBase,\nan uncertain KG that indicates to what extent a target entity belongs to a\nconcept, into our RE architecture. We then design a novel multi-view inference\nframework to systematically integrate local context and global knowledge across\nthree views: mention-, entity- and concept-view. The experimental results show\nthat our model achieves competitive performances on both sentence- and\ndocument-level relation extraction, which verifies the effectiveness of\nintroducing uncertain knowledge and the multi-view inference framework that we\ndesign.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 05:56:33 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Li", "Bo", ""], ["Ye", "Wei", ""], ["Huang", "Canming", ""], ["Zhang", "Shikun", ""]]}, {"id": "2104.13615", "submitter": "Minjin Choi", "authors": "Minjin Choi, Sunkyung Lee, Eunseong Choi, Heesoo Park, Junhyuk Lee,\n  Dongwon Lee, and Jongwuk Lee", "title": "MelBERT: Metaphor Detection via Contextualized Late Interaction using\n  Metaphorical Identification Theories", "comments": "In Proceedings of 2021 Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated metaphor detection is a challenging task to identify metaphorical\nexpressions of words in a sentence. To tackle this problem, we adopt\npre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we\npropose a novel metaphor detection model, namely metaphor-aware late\ninteraction over BERT (MelBERT). Our model not only leverages contextualized\nword representation but also benefits from linguistic metaphor identification\ntheories to distinguish between the contextual and literal meaning of words.\nOur empirical results demonstrate that MelBERT outperforms several strong\nbaselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:52:01 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Choi", "Minjin", ""], ["Lee", "Sunkyung", ""], ["Choi", "Eunseong", ""], ["Park", "Heesoo", ""], ["Lee", "Junhyuk", ""], ["Lee", "Dongwon", ""], ["Lee", "Jongwuk", ""]]}, {"id": "2104.13691", "submitter": "Emily Ohman", "authors": "Emily \\\"Ohman", "title": "SELF & FEIL: Emotion and Intensity Lexicons for Finnish", "comments": "unpublished short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a Sentiment and Emotion Lexicon for Finnish (SELF) and\na Finnish Emotion Intensity Lexicon (FEIL). We describe the lexicon creation\nprocess and evaluate the lexicon using some commonly available tools. The\nlexicon uses annotations projected from the NRC Emotion Lexicon with carefully\nedited translations. To our knowledge, this is the first comprehensive\nsentiment and emotion lexicon for Finnish.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:28:35 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["\u00d6hman", "Emily", ""]]}, {"id": "2104.13727", "submitter": "Songlin Yang", "authors": "Songlin Yang, Yanpeng Zhao, Kewei Tu", "title": "PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with\n  Many Symbols", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Probabilistic context-free grammars (PCFGs) with neural parameterization have\nbeen shown to be effective in unsupervised phrase-structure grammar induction.\nHowever, due to the cubic computational complexity of PCFG representation and\nparsing, previous approaches cannot scale up to a relatively large number of\n(nonterminal and preterminal) symbols. In this work, we present a new\nparameterization form of PCFGs based on tensor decomposition, which has at most\nquadratic computational complexity in the symbol number and therefore allows us\nto use a much larger number of symbols. We further use neural parameterization\nfor the new form to improve unsupervised parsing performance. We evaluate our\nmodel across ten languages and empirically demonstrate the effectiveness of\nusing more symbols. Our code: https://github.com/sustcsonglin/TN-PCFG\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:25:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Yang", "Songlin", ""], ["Zhao", "Yanpeng", ""], ["Tu", "Kewei", ""]]}, {"id": "2104.13733", "submitter": "Chuan Guo", "authors": "Chuan Guo, Alexandre Sablayrolles, Herv\\'e J\\'egou, Douwe Kiela", "title": "Gradient-based Adversarial Attacks against Text Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the first general-purpose gradient-based attack against\ntransformer models. Instead of searching for a single adversarial example, we\nsearch for a distribution of adversarial examples parameterized by a\ncontinuous-valued matrix, hence enabling gradient-based optimization. We\nempirically demonstrate that our white-box attack attains state-of-the-art\nattack performance on a variety of natural language tasks. Furthermore, we show\nthat a powerful black-box transfer attack, enabled by sampling from the\nadversarial distribution, matches or exceeds existing methods, while only\nrequiring hard-label outputs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:43:43 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Guo", "Chuan", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Kiela", "Douwe", ""]]}, {"id": "2104.13816", "submitter": "Andrea Wang", "authors": "Andrea W Wang (1), Jo-Yu Lan (2), Chihhao Yu (1), Ming-Hung Wang (2)\n  ((1) Information Operations Research Group (IORG) (2) Department of\n  Information Engineering and Computer Science, Feng Chia University)", "title": "The Evolution of Rumors on a Closed Platform during COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we looked into a dataset of 114 thousands of suspicious messages\ncollected from the most popular closed messaging platform in Taiwan between\nJanuary and July, 2020. We proposed an hybrid algorithm that could efficiently\ncluster a large number of text messages according their topics and narratives.\nThat is, we obtained groups of messages that are within a limited content\nalterations within each other. By employing the algorithm to the dataset, we\nwere able to look at the content alterations and the temporal dynamics of each\nparticular rumor over time. With qualitative case studies of three COVID-19\nrelated rumors, we have found that key authoritative figures were often\nmisquoted in false information. It was an effective measure to increase the\npopularity of one false information. In addition, fact-check was not effective\nin stopping misinformation from getting attention. In fact, the popularity of\none false information was often more influenced by major societal events and\neffective content alterations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:04:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Andrea W", ""], ["Lan", "Jo-Yu", ""], ["Yu", "Chihhao", ""], ["Wang", "Ming-Hung", ""]]}, {"id": "2104.13841", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Elliott Ash, Terry Ruas, Bela Gipp, Julian\n  Moreno-Schneider, Georg Rehm", "title": "Evaluating Document Representations for Content-based Legal Literature\n  Recommendations", "comments": "Accepted for publication at ICAIL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems assist legal professionals in finding relevant literature\nfor supporting their case. Despite its importance for the profession, legal\napplications do not reflect the latest advances in recommender systems and\nrepresentation learning research. Simultaneously, legal recommender systems are\ntypically evaluated in small-scale user study without any public available\nbenchmark datasets. Thus, these studies have limited reproducibility. To\naddress the gap between research and practice, we explore a set of\nstate-of-the-art document representation methods for the task of retrieving\nsemantically related US case law. We evaluate text-based (e.g., fastText,\nTransformers), citation-based (e.g., DeepWalk, Poincar\\'e), and hybrid methods.\nWe compare in total 27 methods using two silver standards with annotations for\n2,964 documents. The silver standards are newly created from Open Case Book and\nWikisource and can be reused under an open license facilitating\nreproducibility. Our experiments show that document representations from\naveraged fastText word vectors (trained on legal corpora) yield the best\nresults, closely followed by Poincar\\'e citation embeddings. Combining fastText\nand Poincar\\'e in a hybrid manner further improves the overall result. Besides\nthe overall performance, we analyze the methods depending on document length,\ncitation count, and the coverage of their recommendations. We make our source\ncode, models, and datasets publicly available at\nhttps://github.com/malteos/legal-document-similarity/.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:48:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ostendorff", "Malte", ""], ["Ash", "Elliott", ""], ["Ruas", "Terry", ""], ["Gipp", "Bela", ""], ["Moreno-Schneider", "Julian", ""], ["Rehm", "Georg", ""]]}, {"id": "2104.13872", "submitter": "Ukyo Honda", "authors": "Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, Yuji\n  Matsumoto", "title": "Removing Word-Level Spurious Alignment between Images and\n  Pseudo-Captions in Unsupervised Image Captioning", "comments": "EACL 2021 (11 pages, 3 figures; added references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised image captioning is a challenging task that aims at generating\ncaptions without the supervision of image-sentence pairs, but only with images\nand sentences drawn from different sources and object labels detected from the\nimages. In previous work, pseudo-captions, i.e., sentences that contain the\ndetected object labels, were assigned to a given image. The focus of the\nprevious work was on the alignment of input images and pseudo-captions at the\nsentence level. However, pseudo-captions contain many words that are irrelevant\nto a given image. In this work, we investigate the effect of removing\nmismatched words from image-sentence alignment to determine how they make this\ntask difficult. We propose a simple gating mechanism that is trained to align\nimage features with only the most reliable words in pseudo-captions: the\ndetected object labels. The experimental results show that our proposed method\noutperforms the previous methods without introducing complex sentence-level\nlearning objectives. Combined with the sentence-level alignment method of\nprevious work, our method further improves its performance. These results\nconfirm the importance of careful alignment in word-level details.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:36:52 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 07:04:37 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Honda", "Ukyo", ""], ["Ushiku", "Yoshitaka", ""], ["Hashimoto", "Atsushi", ""], ["Watanabe", "Taro", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "2104.13913", "submitter": "Peng Su", "authors": "Peng Su, Yifan Peng, K. Vijay-Shanker", "title": "Improving BERT Model Using Contrastive Learning for Biomedical Relation\n  Extraction", "comments": "Accepted by BioNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has been used to learn a high-quality representation of\nthe image in computer vision. However, contrastive learning is not widely\nutilized in natural language processing due to the lack of a general method of\ndata augmentation for text data. In this work, we explore the method of\nemploying contrastive learning to improve the text representation from the BERT\nmodel for relation extraction. The key knob of our framework is a unique\ncontrastive pre-training step tailored for the relation extraction tasks by\nseamlessly integrating linguistic knowledge into the data augmentation.\nFurthermore, we investigate how large-scale data constructed from the external\nknowledge bases can enhance the generality of contrastive pre-training of BERT.\nThe experimental results on three relation extraction benchmark datasets\ndemonstrate that our method can improve the BERT model representation and\nachieve state-of-the-art performance. In addition, we explore the\ninterpretability of models by showing that BERT with contrastive pre-training\nrelies more on rationales for prediction. Our code and data are publicly\navailable at: https://github.com/udel-biotm-lab/BERT-CLRE.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:50:24 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Su", "Peng", ""], ["Peng", "Yifan", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "2104.13933", "submitter": "Tianze Shi", "authors": "Tianze Shi, Ozan \\.Irsoy, Igor Malioutov, Lillian Lee", "title": "Learning Syntax from Naturally-Occurring Bracketings", "comments": "NAACL 2021", "journal-ref": "In Proceedings of NAACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturally-occurring bracketings, such as answer fragments to natural language\nquestions and hyperlinks on webpages, can reflect human syntactic intuition\nregarding phrasal boundaries. Their availability and approximate correspondence\nto syntax make them appealing as distant information sources to incorporate\ninto unsupervised constituency parsing. But they are noisy and incomplete; to\naddress this challenge, we develop a partial-brackets-aware structured ramp\nloss in learning. Experiments demonstrate that our distantly-supervised models\ntrained on naturally-occurring bracketing data are more accurate in inducing\nsyntactic structures than competing unsupervised systems. On the English WSJ\ncorpus, our models achieve an unlabeled F1 score of 68.9 for constituency\nparsing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:00:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Shi", "Tianze", ""], ["\u0130rsoy", "Ozan", ""], ["Malioutov", "Igor", ""], ["Lee", "Lillian", ""]]}, {"id": "2104.13936", "submitter": "Tianze Shi", "authors": "Tianze Shi, Adrian Benton, Igor Malioutov, Ozan \\.Irsoy", "title": "Diversity-Aware Batch Active Learning for Dependency Parsing", "comments": "NAACL 2021", "journal-ref": "In Proceedings of NAACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the predictive performance of modern statistical dependency parsers\nrelies heavily on the availability of expensive expert-annotated treebank data,\nnot all annotations contribute equally to the training of the parsers. In this\npaper, we attempt to reduce the number of labeled examples needed to train a\nstrong dependency parser using batch active learning (AL). In particular, we\ninvestigate whether enforcing diversity in the sampled batches, using\ndeterminantal point processes (DPPs), can improve over their diversity-agnostic\ncounterparts. Simulation experiments on an English newswire corpus show that\nselecting diverse batches with DPPs is superior to strong selection strategies\nthat do not enforce batch diversity, especially during the initial stages of\nthe learning process. Additionally, our diversityaware strategy is robust under\na corpus duplication setting, where diversity-agnostic sampling strategies\nexhibit significant degradation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:00:05 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Shi", "Tianze", ""], ["Benton", "Adrian", ""], ["Malioutov", "Igor", ""], ["\u0130rsoy", "Ozan", ""]]}, {"id": "2104.13983", "submitter": "Prasanna Date", "authors": "Prasanna Date, Catherine Schuman, Bill Kay, Thomas Potok", "title": "Neuromorphic Computing is Turing-Complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CC cs.CL cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing is a non-von Neumann computing paradigm that performs\ncomputation by emulating the human brain. Neuromorphic systems are extremely\nenergy-efficient and known to consume thousands of times less power than CPUs\nand GPUs. They have the potential to drive critical use cases such as\nautonomous vehicles, edge computing and internet of things in the future. For\nthis reason, they are sought to be an indispensable part of the future\ncomputing landscape. Neuromorphic systems are mainly used for spike-based\nmachine learning applications, although there are some non-machine learning\napplications in graph theory, differential equations, and spike-based\nsimulations. These applications suggest that neuromorphic computing might be\ncapable of general-purpose computing. However, general-purpose computability of\nneuromorphic computing has not been established yet. In this work, we prove\nthat neuromorphic computing is Turing-complete and therefore capable of\ngeneral-purpose computing. Specifically, we present a model of neuromorphic\ncomputing, with just two neuron parameters (threshold and leak), and two\nsynaptic parameters (weight and delay). We devise neuromorphic circuits for\ncomputing all the {\\mu}-recursive functions (i.e., constant, successor and\nprojection functions) and all the {\\mu}-recursive operators (i.e., composition,\nprimitive recursion and minimization operators). Given that the {\\mu}-recursive\nfunctions and operators are precisely the ones that can be computed using a\nTuring machine, this work establishes the Turing-completeness of neuromorphic\ncomputing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 19:25:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Date", "Prasanna", ""], ["Schuman", "Catherine", ""], ["Kay", "Bill", ""], ["Potok", "Thomas", ""]]}, {"id": "2104.14133", "submitter": "Jia-Huei Ju", "authors": "Jia-Huei Ju, Jheng-Hong Yang, Chuan-Ju Wang", "title": "Text-to-Text Multi-view Learning for Passage Re-ranking", "comments": "Accepted as short paper in SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, much progress in natural language processing has been driven by\ndeep contextualized representations pretrained on large corpora. Typically, the\nfine-tuning on these pretrained models for a specific downstream task is based\non single-view learning, which is however inadequate as a sentence can be\ninterpreted differently from different perspectives. Therefore, in this work,\nwe propose a text-to-text multi-view learning framework by incorporating an\nadditional view -- the text generation view -- into a typical single-view\npassage ranking model. Empirically, the proposed approach is of help to the\nranking performance compared to its single-view counterpart. Ablation studies\nare also reported in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 06:12:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ju", "Jia-Huei", ""], ["Yang", "Jheng-Hong", ""], ["Wang", "Chuan-Ju", ""]]}, {"id": "2104.14150", "submitter": "Alessio Mongelluzzo", "authors": "Patrizia Agnello, Silvia M. Ansaldi, Emilia Lenzi, Alessio\n  Mongelluzzo, Manuel Roveri", "title": "RECKONition: a NLP-based system for Industrial Accidents at Work\n  Prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting patterns and useful information from Natural Language datasets is\na challenging task, especially when dealing with data written in a language\ndifferent from English, like Italian. Machine and Deep Learning, together with\nNatural Language Processing (NLP) techniques have widely spread and improved\nlately, providing a plethora of useful methods to address both Supervised and\nUnsupervised problems on textual information. We propose RECKONition, a\nNLP-based system for Industrial Accidents at Work Prevention. RECKONition,\nwhich is meant to provide Natural Language Understanding, Clustering and\nInference, is the result of a joint partnership with the Italian National\nInstitute for Insurance against Accidents at Work (INAIL). The obtained results\nshowed the ability to process textual data written in Italian describing\nindustrial accidents dynamics and consequences.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 07:13:07 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Agnello", "Patrizia", ""], ["Ansaldi", "Silvia M.", ""], ["Lenzi", "Emilia", ""], ["Mongelluzzo", "Alessio", ""], ["Roveri", "Manuel", ""]]}, {"id": "2104.14185", "submitter": "Jean Neraud", "authors": "Jean N\\'eraud (LITIS, UNIROUEN)", "title": "Variable-Length Codes Independent or Closed with respect to Edit\n  Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate inference of variable-length codes in other domains of\ncomputer science, such as noisy information transmission or information\nretrieval-storage: in such topics, traditionally mostly constant-length\ncodewords act. The study is relied upon the two concepts of independent and\nclosed sets. We focus to those word relations whose images are computed by\napplying some peculiar combinations of deletion, insertion, or substitution. In\nparticular, characterizations of variable-length codes that are maximal in the\nfamilies of $\\tau$-independent or $\\tau$-closed codes are provided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:03:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["N\u00e9raud", "Jean", "", "LITIS, UNIROUEN"]]}, {"id": "2104.14209", "submitter": "Yves Bestgen", "authors": "Yves Bestgen", "title": "Using Fisher's Exact Test to Evaluate Association Measures for N-grams", "comments": "Unpublished English version of Bestgen, Y. (2017). Evaluation de\n  mesures d'association pour les bigrammes et les trigrammes au moyen du test\n  exact de Fisher. Actes de TALN2017 (Vol. 2, p. 10-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To determine whether some often-used lexical association measures assign high\nscores to n-grams that chance could have produced as frequently as observed, we\nused an extension of Fisher's exact test to sequences longer than two words to\nanalyse a corpus of four million words. The results, based on the\nprecision-recall curve and a new index called chance-corrected average\nprecision, show that, as expected, simple-ll is extremely effective. They also\nshow, however, that MI3 is more efficient than the other hypothesis tests-based\nmeasures and even reaches a performance level almost equal to simple-ll for\n3-grams. It is additionally observed that some measures are more efficient for\n3-grams than for 2-grams, while others stagnate.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:59:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bestgen", "Yves", ""]]}, {"id": "2104.14279", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Irene Nikkarinen, Kyle Mahowald, Ryan Cotterell,\n  Dami\\'an Blasi", "title": "How (Non-)Optimal is the Lexicon?", "comments": "Tiago Pimentel and Irene Nikkarinen contributed equally to this work.\n  Accepted at NAACL 2021. This is the camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mapping of lexical meanings to wordforms is a major feature of natural\nlanguages. While usage pressures might assign short words to frequent meanings\n(Zipf's law of abbreviation), the need for a productive and open-ended\nvocabulary, local constraints on sequences of symbols, and various other\nfactors all shape the lexicons of the world's languages. Despite their\nimportance in shaping lexical structure, the relative contributions of these\nfactors have not been fully quantified. Taking a coding-theoretic view of the\nlexicon and making use of a novel generative statistical model, we define upper\nbounds for the compressibility of the lexicon under various constraints.\nExamining corpora from 7 typologically diverse languages, we use those upper\nbounds to quantify the lexicon's optimality and to explore the relative costs\nof major constraints on natural codes. We find that (compositional) morphology\nand graphotactics can sufficiently account for most of the complexity of\nnatural codes -- as measured by code length.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 11:55:47 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 19:46:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Pimentel", "Tiago", ""], ["Nikkarinen", "Irene", ""], ["Mahowald", "Kyle", ""], ["Cotterell", "Ryan", ""], ["Blasi", "Dami\u00e1n", ""]]}, {"id": "2104.14314", "submitter": "Ekaterina Artemova", "authors": "Valentin Malykh, Alexander Kukushkin, Ekaterina Artemova, Vladislav\n  Mikhailov, Maria Tikhonova, Tatiana Shavrina", "title": "MOROCCO: Model Resource Comparison Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The new generation of pre-trained NLP models push the SOTA to the new limits,\nbut at the cost of computational resources, to the point that their use in real\nproduction environments is often prohibitively expensive. We tackle this\nproblem by evaluating not only the standard quality metrics on downstream tasks\nbut also the memory footprint and inference time. We present MOROCCO, a\nframework to compare language models compatible with \\texttt{jiant} environment\nwhich supports over 50 NLU tasks, including SuperGLUE benchmark and multiple\nprobing suites. We demonstrate its applicability for two GLUE-like suites in\ndifferent languages.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 13:01:27 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Malykh", "Valentin", ""], ["Kukushkin", "Alexander", ""], ["Artemova", "Ekaterina", ""], ["Mikhailov", "Vladislav", ""], ["Tikhonova", "Maria", ""], ["Shavrina", "Tatiana", ""]]}, {"id": "2104.14337", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\n  Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia,\n  Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp,\n  Robin Jia, Mohit Bansal, Christopher Potts, Adina Williams", "title": "Dynabench: Rethinking Benchmarking in NLP", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:49:17 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Kiela", "Douwe", ""], ["Bartolo", "Max", ""], ["Nie", "Yixin", ""], ["Kaushik", "Divyansh", ""], ["Geiger", "Atticus", ""], ["Wu", "Zhengxuan", ""], ["Vidgen", "Bertie", ""], ["Prasad", "Grusha", ""], ["Singh", "Amanpreet", ""], ["Ringshia", "Pratik", ""], ["Ma", "Zhiyi", ""], ["Thrush", "Tristan", ""], ["Riedel", "Sebastian", ""], ["Waseem", "Zeerak", ""], ["Stenetorp", "Pontus", ""], ["Jia", "Robin", ""], ["Bansal", "Mohit", ""], ["Potts", "Christopher", ""], ["Williams", "Adina", ""]]}, {"id": "2104.14339", "submitter": "Wenhao Wu", "authors": "Wenhao Wu and Sujian Li", "title": "A Comprehensive Attempt to Research Statement Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a researcher, writing a good research statement is crucial but costs a\nlot of time and effort. To help researchers, in this paper, we propose the\nresearch statement generation (RSG) task which aims to summarize one's research\nachievements and help prepare a formal research statement. For this task, we\nconduct a comprehensive attempt including corpus construction, method design,\nand performance evaluation. First, we construct an RSG dataset with 62 research\nstatements and the corresponding 1,203 publications. Due to the limitation of\nour resources, we propose a practical RSG method which identifies a\nresearcher's research directions by topic modeling and clustering techniques\nand extracts salient sentences by a neural text summarizer. Finally,\nexperiments show that our method outperforms all the baselines with better\ncontent coverage and coherence.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 03:57:00 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wu", "Wenhao", ""], ["Li", "Sujian", ""]]}, {"id": "2104.14346", "submitter": "Thibault Doutre", "authors": "Thibault Doutre, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Olivier\n  Siohan, Liangliang Cao", "title": "Bridging the gap between streaming and non-streaming ASR systems\n  bydistilling ensembles of CTC and RNN-T models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming end-to-end automatic speech recognition (ASR) systems are widely\nused in everyday applications that require transcribing speech to text in\nreal-time. Their minimal latency makes them suitable for such tasks. Unlike\ntheir non-streaming counterparts, streaming models are constrained to be causal\nwith no future context and suffer from higher word error rates (WER). To\nimprove streaming models, a recent study [1] proposed to distill a\nnon-streaming teacher model on unsupervised utterances, and then train a\nstreaming student using the teachers' predictions. However, the performance gap\nbetween teacher and student WERs remains high. In this paper, we aim to close\nthis gap by using a diversified set of non-streaming teacher models and\ncombining them using Recognizer Output Voting Error Reduction (ROVER). In\nparticular, we show that, despite being weaker than RNN-T models, CTC models\nare remarkable teachers. Further, by fusing RNN-T and CTC models together, we\nbuild the strongest teachers. The resulting student models drastically improve\nupon streaming models of previous work [1]: the WER decreases by 41% on\nSpanish, 27% on Portuguese, and 13% on French.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:20:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Doutre", "Thibault", ""], ["Han", "Wei", ""], ["Chiu", "Chung-Cheng", ""], ["Pang", "Ruoming", ""], ["Siohan", "Olivier", ""], ["Cao", "Liangliang", ""]]}, {"id": "2104.14445", "submitter": "Dominique Larchey-Wendling", "authors": "Dominik Kirst and Dominique Larchey-Wendling", "title": "Trakhtenbrot's Theorem in Coq: Finite Model Theory through the\n  Constructive Lens", "comments": "26 pages, extended version of the IJCAR 2020 paper. arXiv admin note:\n  substantial text overlap with arXiv:2004.07390", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. To showcase an application of Trakthenbrot's theorem, we continue\nour reduction chain with a many-one reduction from FSAT to separation logic.\nAll our results are mechanised in the framework of a growing Coq library of\nsynthetic undecidability proofs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:05:31 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Kirst", "Dominik", ""], ["Larchey-Wendling", "Dominique", ""]]}, {"id": "2104.14470", "submitter": "Ha Nguyen", "authors": "Ha Nguyen, Yannick Est\\`eve, Laurent Besacier", "title": "Impact of Encoding and Segmentation Strategies on End-to-End\n  Simultaneous Speech Translation", "comments": "Accepted for presentation at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosted by the simultaneous translation shared task at IWSLT 2020, promising\nend-to-end online speech translation approaches were recently proposed. They\nconsist in incrementally encoding a speech input (in a source language) and\ndecoding the corresponding text (in a target language) with the best possible\ntrade-off between latency and translation quality. This paper investigates two\nkey aspects of end-to-end simultaneous speech translation: (a) how to encode\nefficiently the continuous speech flow, and (b) how to segment the speech flow\nin order to alternate optimally between reading (R: encoding input) and writing\n(W: decoding output) operations. We extend our previously proposed end-to-end\nonline decoding strategy and show that while replacing BLSTM by ULSTM encoding\ndegrades performance in offline mode, it actually improves both efficiency and\nperformance in online mode. We also measure the impact of different methods to\nsegment the speech signal (using fixed interval boundaries, oracle word\nboundaries or randomly set boundaries) and show that our best end-to-end online\ndecoding strategy is surprisingly the one that alternates R/W operations on\nfixed size blocks on our English-German speech translation setup.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:33:08 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 17:42:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nguyen", "Ha", ""], ["Est\u00e8ve", "Yannick", ""], ["Besacier", "Laurent", ""]]}, {"id": "2104.14478", "submitter": "Markus Freitag", "authors": "Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun\n  Tan, Wolfgang Macherey", "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation\n  for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human evaluation of modern high-quality machine translation systems is a\ndifficult problem, and there is increasing evidence that inadequate evaluation\nprocedures can lead to erroneous conclusions. While there has been considerable\nresearch on human evaluation, the field still lacks a commonly-accepted\nstandard procedure. As a step toward this goal, we propose an evaluation\nmethodology grounded in explicit error analysis, based on the Multidimensional\nQuality Metrics (MQM) framework. We carry out the largest MQM research study to\ndate, scoring the outputs of top systems from the WMT 2020 shared task in two\nlanguage pairs using annotations provided by professional translators with\naccess to full document context. We analyze the resulting data extensively,\nfinding among other results a substantially different ranking of evaluated\nsystems from the one established by the WMT crowd workers, exhibiting a clear\npreference for human over machine output. Surprisingly, we also find that\nautomatic metrics based on pre-trained embeddings can outperform human crowd\nworkers. We make our corpus publicly available for further research.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:42:09 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Freitag", "Markus", ""], ["Foster", "George", ""], ["Grangier", "David", ""], ["Ratnakar", "Viresh", ""], ["Tan", "Qijun", ""], ["Macherey", "Wolfgang", ""]]}, {"id": "2104.14674", "submitter": "Jiawei Zhou", "authors": "Jiawei Zhou, Tahira Naseem, Ram\\'on Fernandez Astudillo, Radu Florian", "title": "AMR Parsing with Action-Pointer Transformer", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Meaning Representation parsing is a sentence-to-graph prediction\ntask where target nodes are not explicitly aligned to sentence tokens. However,\nsince graph nodes are semantically based on one or more sentence tokens,\nimplicit alignments can be derived. Transition-based parsers operate over the\nsentence from left to right, capturing this inductive bias via alignments at\nthe cost of limited expressiveness. In this work, we propose a transition-based\nsystem that combines hard-attention over sentences with a target-side action\npointer mechanism to decouple source tokens from node representations and\naddress alignments. We model the transitions as well as the pointer mechanism\nthrough straightforward modifications within a single Transformer architecture.\nParser state and graph structure information are efficiently encoded using\nattention heads. We show that our action-pointer approach leads to increased\nexpressiveness and attains large gains (+1.6 points) against the best\ntransition-based AMR parser in very similar conditions. While using no graph\nre-categorization, our single model yields the second best Smatch score on AMR\n2.0 (81.8), which is further improved to 83.4 with silver data and ensemble\ndecoding.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:01:41 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:48:31 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 05:52:41 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhou", "Jiawei", ""], ["Naseem", "Tahira", ""], ["Astudillo", "Ram\u00f3n Fernandez", ""], ["Florian", "Radu", ""]]}, {"id": "2104.14690", "submitter": "Sinong Wang", "authors": "Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma", "title": "Entailment as Few-Shot Learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models (LMs) have demonstrated remarkable ability\nas few-shot learners. However, their success hinges largely on scaling model\nparameters to a degree that makes it challenging to train and serve. In this\npaper, we propose a new approach, named as EFL, that can turn small LMs into\nbetter few-shot learners. The key idea of this approach is to reformulate\npotential NLP task into an entailment one, and then fine-tune the model with as\nlittle as 8 examples. We further demonstrate our proposed method can be: (i)\nnaturally combined with an unsupervised contrastive learning-based data\naugmentation method; (ii) easily extended to multilingual few-shot learning. A\nsystematic evaluation on 18 standard NLP tasks demonstrates that this approach\nimproves the various existing SOTA few-shot learning methods by 12\\%, and\nyields competitive few-shot performance with 500 times larger models, such as\nGPT-3.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:52:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wang", "Sinong", ""], ["Fang", "Han", ""], ["Khabsa", "Madian", ""], ["Mao", "Hanzi", ""], ["Ma", "Hao", ""]]}, {"id": "2104.14694", "submitter": "Aina Gar\\'i Soler", "authors": "Aina Gar\\'i Soler and Marianna Apidianaki", "title": "Let's Play Mono-Poly: BERT Can Reveal Words' Polysemy Level and\n  Partitionability into Senses", "comments": "Accepted to TACL. Pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (LMs) encode rich information about linguistic\nstructure but their knowledge about lexical polysemy remains unclear. We\npropose a novel experimental setup for analysing this knowledge in LMs\nspecifically trained for different languages (English, French, Spanish and\nGreek) and in multilingual BERT. We perform our analysis on datasets carefully\ndesigned to reflect different sense distributions, and control for parameters\nthat are highly correlated with polysemy such as frequency and grammatical\ncategory. We demonstrate that BERT-derived representations reflect words'\npolysemy level and their partitionability into senses. Polysemy-related\ninformation is more clearly present in English BERT embeddings, but models in\nother languages also manage to establish relevant distinctions between words at\ndifferent polysemy levels. Our results contribute to a better understanding of\nthe knowledge encoded in contextualised representations and open up new avenues\nfor multilingual lexical semantics research.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:15:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Soler", "Aina Gar\u00ed", ""], ["Apidianaki", "Marianna", ""]]}, {"id": "2104.14700", "submitter": "Ewan Dunbar", "authors": "Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen,\n  Maureen de Seyssel, Patricia Roz\\'e, Morgane Rivi\\`ere, Eugene Kharitonov,\n  Emmanuel Dupoux", "title": "The Interspeech Zero Resource Speech Challenge 2021: Spoken language\n  modelling", "comments": "Submitted to Interspeech 2021. arXiv admin note: text overlap with\n  arXiv:2011.11588", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or labels. The\nchallenge is based on the Libri-light dataset, which provides up to 60k hours\nof audio from English audio books without any associated text. We provide a\npipeline baseline system consisting on an encoder based on contrastive\npredictive coding (CPC), a quantizer ($k$-means) and a standard language model\n(BERT or LSTM). The metrics evaluate the learned representations at the\nacoustic (ABX discrimination), lexical (spot-the-word), syntactic\n(acceptability judgment) and semantic levels (similarity judgment). We present\nan overview of the eight submitted systems from four groups and discuss the\nmain results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:53:37 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Dunbar", "Ewan", ""], ["Bernard", "Mathieu", ""], ["Hamilakis", "Nicolas", ""], ["Nguyen", "Tu Anh", ""], ["de Seyssel", "Maureen", ""], ["Roz\u00e9", "Patricia", ""], ["Rivi\u00e8re", "Morgane", ""], ["Kharitonov", "Eugene", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2104.14703", "submitter": "Ankith Uppunda", "authors": "Ankith Uppunda, Susan D. Cochran, Jacob G. Foster, Alina\n  Arseniev-Koehler, Vickie M. Mays, Kai-Wei Chang", "title": "Adapting Coreference Resolution for Processing Violent Death Narratives", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coreference resolution is an important component in analyzing narrative text\nfrom administrative data (e.g., clinical or police sources). However, existing\ncoreference models trained on general language corpora suffer from poor\ntransferability due to domain gaps, especially when they are applied to\ngender-inclusive data with lesbian, gay, bisexual, and transgender (LGBT)\nindividuals. In this paper, we analyzed the challenges of coreference\nresolution in an exemplary form of administrative text written in English:\nviolent death narratives from the USA's Centers for Disease Control's (CDC)\nNational Violent Death Reporting System. We developed a set of data\naugmentation rules to improve model performance using a probabilistic data\nprogramming framework. Experiments on narratives from an administrative\ndatabase, as well as existing gender-inclusive coreference datasets,\ndemonstrate the effectiveness of data augmentation in training coreference\nmodels that can better handle text data about LGBT individuals.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 00:16:42 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Uppunda", "Ankith", ""], ["Cochran", "Susan D.", ""], ["Foster", "Jacob G.", ""], ["Arseniev-Koehler", "Alina", ""], ["Mays", "Vickie M.", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2104.14728", "submitter": "Ayme Arango", "authors": "Aym\\'e Arango, Jorge P\\'erez and Barbara Poblete", "title": "Cross-lingual hate speech detection based on multilingual\n  domain-specific word embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic hate speech detection in online social networks is an important\nopen problem in Natural Language Processing (NLP). Hate speech is a\nmultidimensional issue, strongly dependant on language and cultural factors.\nDespite its relevance, research on this topic has been almost exclusively\ndevoted to English. Most supervised learning resources, such as labeled\ndatasets and NLP tools, have been created for this same language. Considering\nthat a large portion of users worldwide speak in languages other than English,\nthere is an important need for creating efficient approaches for multilingual\nhate speech detection. In this work we propose to address the problem of\nmultilingual hate speech detection from the perspective of transfer learning.\nOur goal is to determine if knowledge from one particular language can be used\nto classify other language, and to determine effective ways to achieve this. We\npropose a hate specific data representation and evaluate its effectiveness\nagainst general-purpose universal representations most of which, unlike our\nproposed model, have been trained on massive amounts of data. We focus on a\ncross-lingual setting, in which one needs to classify hate speech in one\nlanguage without having access to any labeled data for that language. We show\nthat the use of our simple yet specific multilingual hate representations\nimproves classification results. We explain this with a qualitative analysis\nshowing that our specific representation is able to capture some common\npatterns in how hate speech presents itself in different languages.\n  Our proposal constitutes, to the best of our knowledge, the first attempt for\nconstructing multilingual specific-task representations. Despite its\nsimplicity, our model outperformed the previous approaches for most of the\nexperimental setups. Our findings can orient future solutions toward the use of\ndomain-specific representations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:24:50 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Arango", "Aym\u00e9", ""], ["P\u00e9rez", "Jorge", ""], ["Poblete", "Barbara", ""]]}, {"id": "2104.14751", "submitter": "Zeinab Rajabi", "authors": "Zeinab Rajabi, MohammadReza Valavi", "title": "A Survey on sentiment analysis in Persian: A Comprehensive System\n  Perspective Covering Challenges and Advances in Resources, and Methods", "comments": "31 pages, 2 figures, tables 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Social media has been remarkably grown during the past few years. Nowadays,\nposting messages on social media websites has become one of the most popular\nInternet activities. The vast amount of user-generated content has made social\nmedia the most extensive data source of public opinion. Sentiment analysis is\none of the techniques used to analyze user-generated data. The Persian language\nhas specific features and thereby requires unique methods and models to be\nadopted for sentiment analysis, which are different from those in English\nlanguage. Sentiment analysis in each language has specified prerequisites;\nhence, the direct use of methods, tools, and resources developed for English\nlanguage in Persian has its limitations. The main target of this paper is to\nprovide a comprehensive literature survey for state-of-the-art advances in\nPersian sentiment analysis. In this regard, the present study aims to\ninvestigate and compare the previous sentiment analysis studies on Persian\ntexts and describe contributions presented in articles published in the last\ndecade. First, the levels, approaches, and tasks for sentiment analysis are\ndescribed. Then, a detailed survey of the sentiment analysis methods used for\nPersian texts is presented, and previous relevant works on Persian Language are\ndiscussed. Moreover, we present in this survey the authentic and published\nstandard sentiment analysis resources and advances that have been done for\nPersian sentiment analysis. Finally, according to the state-of-the-art\ndevelopment of English sentiment analysis, some issues and challenges not being\naddressed in Persian texts are listed, and some guidelines and trends are\nprovided for future research on Persian texts. The paper provides information\nto help new or established researchers in the field as well as industry\ndevelopers who aim to deploy an operational complete sentiment analysis system.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 04:31:21 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Rajabi", "Zeinab", ""], ["Valavi", "MohammadReza", ""]]}, {"id": "2104.14757", "submitter": "Huijuan Wang", "authors": "Huijuan Wang, Shuangyin Li, Rong Pan", "title": "An Adversarial Transfer Network for Knowledge Representation Learning", "comments": "Accepted by TheWebConf 2021", "journal-ref": null, "doi": "10.1145/3442381.3450064", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation learning has received a lot of attention in the past\nfew years. The success of existing methods heavily relies on the quality of\nknowledge graphs. The entities with few triplets tend to be learned with less\nexpressive power. Fortunately, there are many knowledge graphs constructed from\nvarious sources, the representations of which could contain much information.\nWe propose an adversarial embedding transfer network ATransN, which transfers\nknowledge from one or more teacher knowledge graphs to a target one through an\naligned entity set without explicit data leakage. Specifically, we add soft\nconstraints on aligned entity pairs and neighbours to the existing knowledge\nrepresentation learning methods. To handle the problem of possible distribution\ndifferences between teacher and target knowledge graphs, we introduce an\nadversarial adaption module. The discriminator of this module evaluates the\ndegree of consistency between the embeddings of an aligned entity pair. The\nconsistency score is then used as the weights of soft constraints. It is not\nnecessary to acquire the relations and triplets in teacher knowledge graphs\nbecause we only utilize the entity representations. Knowledge graph completion\nresults show that ATransN achieves better performance against baselines without\ntransfer on three datasets, CN3l, WK3l, and DWY100k. The ablation study\ndemonstrates that ATransN can bring steady and consistent improvement in\ndifferent settings. The extension of combining other knowledge graph embedding\nalgorithms and the extension with three teacher graphs display the promising\ngeneralization of the adversarial transfer network.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:07:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wang", "Huijuan", ""], ["Li", "Shuangyin", ""], ["Pan", "Rong", ""]]}, {"id": "2104.14777", "submitter": "Khayrul Bashar", "authors": "Md. Khayrul Bashar", "title": "Event-driven timeseries analysis and the comparison of public reactions\n  on COVID-19", "comments": "15 pages, 10 figures, 6 tables, International Conference on Big Data\n  and Application (BDAP 2021), for associated file, see\n  https://aircconline.com/csit/abstract/v11n5/csit110507.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid spread of COVID-19 has already affected human lives throughout the\nglobe. Governments of different countries have taken various measures, but how\nthey affected people lives is not clear. In this study, a rule-based and a\nmachine-learning based models are applied to answer the above question using\npublic tweets from Japan, USA, UK, and Australia. Two polarity timeseries\n(meanPol and pnRatio) and two events, namely \"lockdown or emergency (LED)\" and\n\"the economic support package (ESP)\", are considered in this study. Statistical\ntesting on the sub-series around LED and ESP events showed their positive\nimpacts to the people of (UK and Australia) and (USA and UK), respectively\nunlike Japanese people that showed opposite effects. Manual validation with the\nrelevant tweets showed an agreement with the statistical results. A case study\nwith Japanese tweets using supervised logistic regression classifies tweets\ninto heath-worry, economy-worry and other classes with 83.11% accuracy.\nPredicted tweets around events re-confirm the statistical outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:14:53 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Bashar", "Md. Khayrul", ""]]}, {"id": "2104.14781", "submitter": "Pengfei Liu", "authors": "Pengfei Liu, Kun Li and Helen Meng", "title": "Hierarchical Modeling for Out-of-Scope Domain and Intent Classification", "comments": "Submitted to Interspeech-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  User queries for a real-world dialog system may sometimes fall outside the\nscope of the system's capabilities, but appropriate system responses will\nenable smooth processing throughout the human-computer interaction. This paper\nis concerned with the user's intent, and focuses on out-of-scope intent\nclassification in dialog systems. Although user intents are highly correlated\nwith the application domain, few studies have exploited such correlations for\nintent classification. Rather than developing a two-stage approach that first\nclassifies the domain and then the intent, we propose a hierarchical multi-task\nlearning approach based on a joint model to classify domain and intent\nsimultaneously. Novelties in the proposed approach include: (1) sharing\nsupervised out-of-scope signals in joint modeling of domain and intent\nclassification to replace a two-stage pipeline; and (2) introducing a\nhierarchical model that learns the intent and domain representations in the\nhigher and lower layers respectively. Experiments show that the model\noutperforms existing methods in terms of accuracy, out-of-scope recall and F1.\nAdditionally, threshold-based post-processing further improves performance by\nbalancing precision and recall in intent classification.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:38:23 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Liu", "Pengfei", ""], ["Li", "Kun", ""], ["Meng", "Helen", ""]]}, {"id": "2104.14791", "submitter": "Keyu An", "authors": "Keyu An, Yi Zhang, Zhijian Ou", "title": "Deformable TDNN with adaptive receptive fields for speech recognition", "comments": "5 pages. submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time Delay Neural Networks (TDNNs) are widely used in both DNN-HMM based\nhybrid speech recognition systems and recent end-to-end systems. Nevertheless,\nthe receptive fields of TDNNs are limited and fixed, which is not desirable for\ntasks like speech recognition, where the temporal dynamics of speech are varied\nand affected by many factors. This paper proposes to use deformable TDNNs for\nadaptive temporal dynamics modeling in end-to-end speech recognition. Inspired\nby deformable ConvNets, deformable TDNNs augment the temporal sampling\nlocations with additional offsets and learn the offsets automatically based on\nthe ASR criterion, without additional supervision. Experiments show that\ndeformable TDNNs obtain state-of-the-art results on WSJ benchmarks\n(1.42\\%/3.45\\% WER on WSJ eval92/dev93 respectively), outperforming standard\nTDNNs significantly. Furthermore, we propose the latency control mechanism for\ndeformable TDNNs, which enables deformable TDNNs to do streaming ASR without\naccuracy degradation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:10:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["An", "Keyu", ""], ["Zhang", "Yi", ""], ["Ou", "Zhijian", ""]]}, {"id": "2104.14795", "submitter": "Soroush Vosoughi Dr", "authors": "Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, Soroush\n  Vosoughi", "title": "Mitigating Political Bias in Language Models Through Reinforced\n  Calibration", "comments": "In proceedings of the 35th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current large-scale language models can be politically biased as a result of\nthe data they are trained on, potentially causing serious problems when they\nare deployed in real-world settings. In this paper, we describe metrics for\nmeasuring political bias in GPT-2 generation and propose a reinforcement\nlearning (RL) framework for mitigating political biases in generated text. By\nusing rewards from word embeddings or a classifier, our RL framework guides\ndebiased generation without having access to the training data or requiring the\nmodel to be retrained. In empirical experiments on three attributes sensitive\nto political bias (gender, location, and topic), our methods reduced bias\naccording to both our metrics and human evaluation, while maintaining\nreadability and semantic coherence.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:21:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Liu", "Ruibo", ""], ["Jia", "Chenyan", ""], ["Wei", "Jason", ""], ["Xu", "Guangxuan", ""], ["Wang", "Lili", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2104.14830", "submitter": "Bo Li", "authors": "Bo Li, Ruoming Pang, Tara N. Sainath, Anmol Gulati, Yu Zhang, James\n  Qin, Parisa Haghani, W. Ronny Huang, Min Ma", "title": "Scaling End-to-End Models for Large-Scale Multilingual ASR", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building ASR models across many language families is a challenging multi-task\nlearning problem due to large language variations and heavily unbalanced data.\nExisting work has shown positive transfer from high resource to low resource\nlanguages. However, degradations on high resource languages are commonly\nobserved due to interference from the heterogeneous multilingual data and\nreduction in per-language capacity. We conduct a capacity study on a\n15-language task, with the amount of data per language varying from 7.7K to\n54.7K hours. We adopt GShard [1] to efficiently scale up to 10B parameters.\nEmpirically, we find that (1) scaling the number of model parameters is an\neffective way to solve the capacity bottleneck - our 500M-param model is\nalready better than monolingual baselines and scaling it to 1B and 10B brought\nfurther quality gains; (2) larger models are not only more data efficient, but\nalso more efficient in terms of training cost as measured in TPU days - the\n1B-param model reaches the same accuracy at 34% of training time as the\n500M-param model; (3) given a fixed capacity budget, adding depth usually works\nbetter than width and large encoders tend to do better than large decoders.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:24:11 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Li", "Bo", ""], ["Pang", "Ruoming", ""], ["Sainath", "Tara N.", ""], ["Gulati", "Anmol", ""], ["Zhang", "Yu", ""], ["Qin", "James", ""], ["Haghani", "Parisa", ""], ["Huang", "W. Ronny", ""], ["Ma", "Min", ""]]}, {"id": "2104.14839", "submitter": "Yichong Huang", "authors": "Yichong Huang, Xiachong Feng, Xiaocheng Feng and Bing Qin", "title": "The Factual Inconsistency Problem in Abstractive Text Summarization: A\n  Survey", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, various neural encoder-decoder models pioneered by Seq2Seq\nframework have been proposed to achieve the goal of generating more abstractive\nsummaries by learning to map input text to output text. At a high level, such\nneural models can freely generate summaries without any constraint on the words\nor phrases used. Moreover, their format is closer to human-edited summaries and\noutput is more readable and fluent. However, the neural model's abstraction\nability is a double-edged sword. A commonly observed problem with the generated\nsummaries is the distortion or fabrication of factual information in the\narticle. This inconsistency between the original text and the summary has\ncaused various concerns over its applicability, and the previous evaluation\nmethods of text summarization are not suitable for this issue. In response to\nthe above problems, the current research direction is predominantly divided\ninto two categories, one is to design fact-aware evaluation metrics to select\noutputs without factual inconsistency errors, and the other is to develop new\nsummarization systems towards factual consistency. In this survey, we focus on\npresenting a comprehensive review of these fact-specific evaluation methods and\ntext summarization models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:46:13 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 01:49:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Yichong", ""], ["Feng", "Xiachong", ""], ["Feng", "Xiaocheng", ""], ["Qin", "Bing", ""]]}, {"id": "2104.14860", "submitter": "Silvia Casola", "authors": "Silvia Casola and Alberto Lavelli", "title": "Summarization, Simplification, and Generation: The Case of Patents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey Natural Language Processing (NLP) approaches to summarizing,\nsimplifying, and generating patents' text. While solving these tasks has\nimportant practical applications - given patents' centrality in the R&D process\n- patents' idiosyncrasies open peculiar challenges to the current NLP state of\nthe art. This survey aims at a) describing patents' characteristics and the\nquestions they raise to the current NLP systems, b) critically presenting\nprevious work and its evolution, and c) drawing attention to directions of\nresearch in which further work is needed. To the best of our knowledge, this is\nthe first survey of generative approaches in the patent domain.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:28:29 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Casola", "Silvia", ""], ["Lavelli", "Alberto", ""]]}, {"id": "2104.14914", "submitter": "Garima Gaur", "authors": "Siddhant Arora, Vinayak Gupta, Garima Gaur, Srikanta Bedathur", "title": "BERT Meets Relational DB: Contextual Representations of Relational\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of learning low dimension\nrepresentation of entities on relational databases consisting of multiple\ntables. Embeddings help to capture semantics encoded in the database and can be\nused in a variety of settings like auto-completion of tables, fully-neural\nquery processing of relational joins queries, seamlessly handling missing\nvalues, and more. Current work is restricted to working with just single table,\nor using pretrained embeddings over an external corpus making them unsuitable\nfor use in real-world databases. In this work, we look into ways of using these\nattention-based model to learn embeddings for entities in the relational\ndatabase. We are inspired by BERT style pretraining methods and are interested\nin observing how they can be extended for representation learning on structured\ndatabases. We evaluate our approach of the autocompletion of relational\ndatabases and achieve improvement over standard baselines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:23:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Arora", "Siddhant", ""], ["Gupta", "Vinayak", ""], ["Gaur", "Garima", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "2104.14925", "submitter": "Mark-Christoph M\\\"uller", "authors": "Mark-Christoph M\\\"uller, Sucheta Ghosh, Ulrike Wittig, and Maja Rey", "title": "Word-Level Alignment of Paper Documents with their Electronic Full-Text\n  Counterparts", "comments": "to appear in Proceedings of BioNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a simple procedure for the automatic creation of word-level\nalignments between printed documents and their respective full-text versions.\nThe procedure is unsupervised, uses standard, off-the-shelf components only,\nand reaches an F-score of 85.01 in the basic setup and up to 86.63 when using\npre- and post-processing. Potential areas of application are manual database\ncuration (incl. document triage) and biomedical expression OCR.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:43:05 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["M\u00fcller", "Mark-Christoph", ""], ["Ghosh", "Sucheta", ""], ["Wittig", "Ulrike", ""], ["Rey", "Maja", ""]]}, {"id": "2104.15104", "submitter": "Tanay Kumar Saha", "authors": "Sanghamitra Dutta and Liang Ma and Tanay Kumar Saha and Di Lu and Joel\n  Tetreault and Alejandro Jaimes", "title": "GTN-ED: Event Detection Using Graph Transformer Networks", "comments": null, "journal-ref": "TextGraphs 2021 : 15th Workshop on Graph-Based Natural Language\n  Processing", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works show that the graph structure of sentences, generated from\ndependency parsers, has potential for improving event detection. However, they\noften only leverage the edges (dependencies) between words, and discard the\ndependency labels (e.g., nominal-subject), treating the underlying graph edges\nas homogeneous. In this work, we propose a novel framework for incorporating\nboth dependencies and their labels using a recently proposed technique called\nGraph Transformer Networks (GTN). We integrate GTNs to leverage dependency\nrelations on two existing homogeneous-graph-based models, and demonstrate an\nimprovement in the F1 score on the ACE dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:35:29 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 10:53:10 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Ma", "Liang", ""], ["Saha", "Tanay Kumar", ""], ["Lu", "Di", ""], ["Tetreault", "Joel", ""], ["Jaimes", "Alejandro", ""]]}, {"id": "2104.15114", "submitter": "John Wieting", "authors": "John Wieting, Kevin Gimpel, Graham Neubig, Taylor Berg-Kirkpatrick", "title": "Paraphrastic Representations at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that allows users to train their own state-of-the-art\nparaphrastic sentence representations in a variety of languages. We also\nrelease trained models for English, Arabic, German, French, Spanish, Russian,\nTurkish, and Chinese. We train these models on large amounts of data, achieving\nsignificantly improved performance from the original papers proposing the\nmethods on a suite of monolingual semantic similarity, cross-lingual semantic\nsimilarity, and bitext mining tasks. Moreover, the resulting models surpass all\nprior work on unsupervised semantic textual similarity, significantly\noutperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych,\n2019). Additionally, our models are orders of magnitude faster than prior work\nand can be used on CPU with little difference in inference speed (even improved\nspeed over GPU when using more CPU cores), making these models an attractive\nchoice for users without access to GPUs or for use on embedded devices.\nFinally, we add significantly increased functionality to the code bases for\ntraining paraphrastic sentence models, easing their use for both inference and\nfor training them for any desired language with parallel data. We also include\ncode to automatically download and preprocess training data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:55:28 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wieting", "John", ""], ["Gimpel", "Kevin", ""], ["Neubig", "Graham", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2104.15135", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Francesca Toni", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fix a bug in a program, we need to locate where the bug is, understand why\nit causes the problem, and patch the code accordingly. This process becomes\nharder when the program is a trained machine learning model and even harder for\nopaque deep learning models. In this survey, we review papers that exploit\nexplanations to enable humans to debug NLP models. We call this problem\nexplanation-based human debugging (EBHD). In particular, we categorize and\ndiscuss existing works along three main dimensions of EBHD (the bug context,\nthe workflow, and the experimental setting), compile findings on how EBHD\ncomponents affect human debuggers, and highlight open problems that could be\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:53:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Toni", "Francesca", ""]]}]