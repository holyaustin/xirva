[{"id": "1701.00066", "submitter": "Sree Harsha Ramesh", "authors": "Sree Harsha Ramesh and Raveena R Kumar", "title": "A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP\n  Tools Contest Entry from Surukam", "comments": "4 Pages, 13th International Conference on Natural Language\n  Processing, Varanasi, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a\nparticularly challenging problem in computational linguistics due to a dearth\nof accurately annotated training corpora. ICON, as part of its NLP tools\ncontest has organized this challenge as a shared task for the second\nconsecutive year to improve the state-of-the-art. This paper describes the POS\ntagger built at Surukam to predict the coarse-grained and fine-grained POS tags\nfor three language pairs - Bengali-English, Telugu-English and Hindi-English,\nwith the text spanning three popular social media platforms - Facebook,\nWhatsApp and Twitter. We employed Conditional Random Fields as the sequence\ntagging algorithm and used a library called sklearn-crfsuite - a thin wrapper\naround CRFsuite for training our model. Among the features we used include -\ncharacter n-grams, language information and patterns for emoji, number,\npunctuation and web-address. Our submissions in the constrained\nenvironment,i.e., without making any use of monolingual POS taggers or the\nlike, obtained an overall average F1-score of 76.45%, which is comparable to\nthe 2015 winning score of 76.79%.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 07:09:52 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Ramesh", "Sree Harsha", ""], ["Kumar", "Raveena R", ""]]}, {"id": "1701.00138", "submitter": "Jun Suzuki", "authors": "Jun Suzuki, Masaaki Nagata", "title": "Cutting-off Redundant Repeating Generations for Neural Abstractive\n  Summarization", "comments": "7 pages, a draft version of EACL-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the reduction of redundant repeating generation that is\noften observed in RNN-based encoder-decoder models. Our basic idea is to\njointly estimate the upper-bound frequency of each target vocabulary in the\nencoder and control the output words based on the estimation in the decoder.\nOur method shows significant improvement over a strong RNN-based\nencoder-decoder baseline and achieved its best results on an abstractive\nsummarization benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 16:41:43 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 23:40:09 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Suzuki", "Jun", ""], ["Nagata", "Masaaki", ""]]}, {"id": "1701.00145", "submitter": "Silvio Amir", "authors": "Silvio Amir, R\\'amon Astudillo, Wang Ling, Paula C. Carvalho, M\\'ario\n  J. Silva", "title": "Expanding Subjective Lexicons for Social Media Mining with Embedding\n  Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for sentiment lexicon induction have capitalized on\npre-trained word embeddings that capture latent semantic properties. However,\nembeddings obtained by optimizing performance of a given task (e.g. predicting\ncontextual words) are sub-optimal for other applications. In this paper, we\naddress this problem by exploiting task-specific representations, induced via\nembedding sub-space projection. This allows us to expand lexicons describing\nmultiple semantic properties. For each property, our model jointly learns\nsuitable representations and the concomitant predictor. Experiments conducted\nover multiple subjective lexicons, show that our model outperforms previous\nwork and other baselines; even in low training data regimes. Furthermore,\nlexicon-based sentiment classifiers built on top of our lexicons outperform\nsimilar resources and yield performances comparable to those of supervised\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 17:00:08 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 17:49:28 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Amir", "Silvio", ""], ["Astudillo", "R\u00e1mon", ""], ["Ling", "Wang", ""], ["Carvalho", "Paula C.", ""], ["Silva", "M\u00e1rio J.", ""]]}, {"id": "1701.00168", "submitter": "Jan Snajder", "authors": "Jan \\v{S}najder", "title": "Social Media Argumentation Mining: The Quest for Deliberateness in\n  Raucousness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation mining from social media content has attracted increasing\nattention. The task is both challenging and rewarding. The informal nature of\nuser-generated content makes the task dauntingly difficult. On the other hand,\nthe insights that could be gained by a large-scale analysis of social media\nargumentation make it a very worthwhile task. In this position paper I discuss\nthe motivation for social media argumentation mining, as well as the tasks and\nchallenges involved.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 21:37:10 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["\u0160najder", "Jan", ""]]}, {"id": "1701.00185", "submitter": "Jiaming Xu", "authors": "Jiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guanhua Tian, Jun Zhao,\n  Bo Xu", "title": "Self-Taught Convolutional Neural Networks for Short Text Clustering", "comments": "33 pages, accepted for publication in Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2016.12.008", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text clustering is a challenging problem due to its sparseness of text\nrepresentation. Here we propose a flexible Self-Taught Convolutional neural\nnetwork framework for Short Text Clustering (dubbed STC^2), which can flexibly\nand successfully incorporate more useful semantic features and learn non-biased\ndeep text representation in an unsupervised manner. In our framework, the\noriginal raw text features are firstly embedded into compact binary codes by\nusing one existing unsupervised dimensionality reduction methods. Then, word\nembeddings are explored and fed into convolutional neural networks to learn\ndeep feature representations, meanwhile the output units are used to fit the\npre-trained binary codes in the training process. Finally, we get the optimal\nclusters by employing K-means to cluster the learned representations. Extensive\nexperimental results demonstrate that the proposed framework is effective,\nflexible and outperform several popular clustering methods when tested on three\npublic short text datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 01:57:59 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Xu", "Jiaming", ""], ["Xu", "Bo", ""], ["Wang", "Peng", ""], ["Zheng", "Suncong", ""], ["Tian", "Guanhua", ""], ["Zhao", "Jun", ""], ["Xu", "Bo", ""]]}, {"id": "1701.00188", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Regina Barzilay, Tommi Jaakkola", "title": "Aspect-augmented Adversarial Networks for Domain Adaptation", "comments": "TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural method for transfer learning between two (source and\ntarget) classification tasks or aspects over the same domain. Rather than\ntraining on target labels, we use a few keywords pertaining to source and\ntarget aspects indicating sentence relevance instead of document class labels.\nDocuments are encoded by learning to embed and softly select relevant sentences\nin an aspect-dependent manner. A shared classifier is trained on the source\nencoded documents and labels, and applied to target encoded documents. We\nensure transfer through aspect-adversarial training so that encoded documents\nare, as sets, aspect-invariant. Experimental results demonstrate that our\napproach outperforms different baselines and model variants on two datasets,\nyielding an improvement of 27% on a pathology dataset and 5% on a review\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 03:04:33 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 03:36:05 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Zhang", "Yuan", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1701.00289", "submitter": "Mariano Beguerisse-D\\'iaz", "authors": "David J.P. O'Sullivan and Guillermo Gardu\\~no-Hern\\'andez and James P.\n  Gleeson and Mariano Beguerisse-D\\'iaz", "title": "Integrating sentiment and social structure to determine preference\n  alignments: The Irish Marriage Referendum", "comments": "16 pages, 12 figures", "journal-ref": "R. Soc. open sci., 4, 170154 (2017)", "doi": "10.1098/rsos.170154", "report-no": null, "categories": "cs.SI cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the relationship between social structure and sentiment through\nthe analysis of a large collection of tweets about the Irish Marriage\nReferendum of 2015. We obtain the sentiment of every tweet with the hashtags\n#marref and #marriageref that was posted in the days leading to the referendum,\nand construct networks to aggregate sentiment and use it to study the\ninteractions among users. Our results show that the sentiment of mention tweets\nposted by users is correlated with the sentiment of received mentions, and\nthere are significantly more connections between users with similar sentiment\nscores than among users with opposite scores in the mention and follower\nnetworks. We combine the community structure of the two networks with the\nactivity level of the users and sentiment scores to find groups of users who\nsupport voting `yes' or `no' in the referendum. There were numerous\nconversations between users on opposing sides of the debate in the absence of\nfollower connections, which suggests that there were efforts by some users to\nestablish dialogue and debate across ideological divisions. Our analysis shows\nthat social structure can be integrated successfully with sentiment to analyse\nand understand the disposition of social media users. These results have\npotential applications in the integration of data and meta-data to study\nopinion dynamics, public opinion modelling, and polling.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 20:54:52 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 16:14:00 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["O'Sullivan", "David J. P.", ""], ["Gardu\u00f1o-Hern\u00e1ndez", "Guillermo", ""], ["Gleeson", "James P.", ""], ["Beguerisse-D\u00edaz", "Mariano", ""]]}, {"id": "1701.00504", "submitter": "Peter Krejzl", "authors": "Peter Krejzl, Barbora Hourov\\'a, Josef Steinberger", "title": "Stance detection in online discussions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our system created to detect stance in online\ndiscussions. The goal is to identify whether the author of a comment is in\nfavor of the given target or against. Our approach is based on a maximum\nentropy classifier, which uses surface-level, sentiment and domain-specific\nfeatures. The system was originally developed to detect stance in English\ntweets. We adapted it to process Czech news commentaries.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 19:23:55 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Krejzl", "Peter", ""], ["Hourov\u00e1", "Barbora", ""], ["Steinberger", "Josef", ""]]}, {"id": "1701.00562", "submitter": "Shi-Xiong Zhang", "authors": "Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li and Yifan Gong", "title": "End-to-End Attention based Text-Dependent Speaker Verification", "comments": "@article{zhang2016End2End, title={End-to-End Attention based\n  Text-Dependent Speaker Verification}, author={Shi-Xiong Zhang, Zhuo\n  Chen$^{\\dag}$, Yong Zhao, Jinyu Li and Yifan Gong}, journal={IEEE Workshop on\n  Spoken Language Technology}, pages={171--178}, year={2016}, publisher={IEEE}\n  }", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of End-to-End system for text-dependent speaker verification is\npresented in this paper. Previously, using the phonetically\ndiscriminative/speaker discriminative DNNs as feature extractors for speaker\nverification has shown promising results. The extracted frame-level (DNN\nbottleneck, posterior or d-vector) features are equally weighted and aggregated\nto compute an utterance-level speaker representation (d-vector or i-vector). In\nthis work we use speaker discriminative CNNs to extract the noise-robust\nframe-level features. These features are smartly combined to form an\nutterance-level speaker vector through an attention mechanism. The proposed\nattention model takes the speaker discriminative information and the phonetic\ninformation to learn the weights. The whole system, including the CNN and\nattention model, is joint optimized using an end-to-end criterion. The training\nalgorithm imitates exactly the evaluation process --- directly mapping a test\nutterance and a few target speaker utterances into a single verification score.\nThe algorithm can automatically select the most similar impostor for each\ntarget speaker to train the network. We demonstrated the effectiveness of the\nproposed end-to-end system on Windows $10$ \"Hey Cortana\" speaker verification\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 01:15:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Zhang", "Shi-Xiong", ""], ["Chen", "Zhuo", ""], ["Zhao", "Yong", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "1701.00576", "submitter": "Huijia Wu", "authors": "Huijia Wu, Jiajun Zhang, Chengqing Zong", "title": "Shortcut Sequence Tagging", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1610.03167", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep stacked RNNs are usually hard to train. Adding shortcut connections\nacross different layers is a common way to ease the training of stacked\nnetworks. However, extra shortcuts make the recurrent step more complicated. To\nsimply the stacked architecture, we propose a framework called shortcut block,\nwhich is a marriage of the gating mechanism and shortcuts, while discarding the\nself-connected part in LSTM cell. We present extensive empirical experiments\nshowing that this design makes training easy and improves generalization. We\npropose various shortcut block topologies and compositions to explore its\neffectiveness. Based on this architecture, we obtain a 6% relatively\nimprovement over the state-of-the-art on CCGbank supertagging dataset. We also\nget comparable results on POS tagging task.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 04:15:51 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Wu", "Huijia", ""], ["Zhang", "Jiajun", ""], ["Zong", "Chengqing", ""]]}, {"id": "1701.00660", "submitter": "EPTCS", "authors": "Dan Marsden (University of Oxford)", "title": "Ambiguity and Incomplete Information in Categorical Models of Language", "comments": "In Proceedings QPL 2016, arXiv:1701.00242", "journal-ref": "EPTCS 236, 2017, pp. 95-107", "doi": "10.4204/EPTCS.236.7", "report-no": null, "categories": "cs.LO cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate notions of ambiguity and partial information in categorical\ndistributional models of natural language. Probabilistic ambiguity has\npreviously been studied using Selinger's CPM construction. This construction\nworks well for models built upon vector spaces, as has been shown in quantum\ncomputational applications. Unfortunately, it doesn't seem to provide a\nsatisfactory method for introducing mixing in other compact closed categories\nsuch as the category of sets and binary relations. We therefore lack a uniform\nstrategy for extending a category to model imprecise linguistic information.\n  In this work we adopt a different approach. We analyze different forms of\nambiguous and incomplete information, both with and without quantitative\nprobabilistic data. Each scheme then corresponds to a suitable enrichment of\nthe category in which we model language. We view different monads as\nencapsulating the informational behaviour of interest, by analogy with their\nuse in modelling side effects in computation. Previous results of Jacobs then\nallow us to systematically construct suitable bases for enrichment.\n  We show that we can freely enrich arbitrary dagger compact closed categories\nin order to capture all the phenomena of interest, whilst retaining the\nimportant dagger compact closed structure. This allows us to construct a model\nwith real convex combination of binary relations that makes non-trivial use of\nthe scalars. Finally we relate our various different enrichments, showing that\nfinite subconvex algebra enrichment covers all the effects under consideration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:16:07 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Marsden", "Dan", "", "University of Oxford"]]}, {"id": "1701.00728", "submitter": "Pashutan Modaresi", "authors": "Pashutan Modaresi, Philipp Gross, Siavash Sefidrodi, Mirja Eckhof,\n  Stefan Conrad", "title": "On (Commercial) Benefits of Automatic Text Summarization Systems in the\n  News Domain: A Case of Media Monitoring and Media Response Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the results of a systematic study to investigate the\n(commercial) benefits of automatic text summarization systems in a real world\nscenario. More specifically, we define a use case in the context of media\nmonitoring and media response analysis and claim that even using a simple\nquery-based extractive approach can dramatically save the processing time of\nthe employees without significantly reducing the quality of their work.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 15:49:35 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Modaresi", "Pashutan", ""], ["Gross", "Philipp", ""], ["Sefidrodi", "Siavash", ""], ["Eckhof", "Mirja", ""], ["Conrad", "Stefan", ""]]}, {"id": "1701.00749", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel and Evangelos Kanoulas and Maarten de Rijke", "title": "Pyndri: a Python Interface to the Indri Search Engine", "comments": "ECIR2017. Proceedings of the 39th European Conference on Information\n  Retrieval. 2017. The final publication will be available at Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce pyndri, a Python interface to the Indri search engine. Pyndri\nallows to access Indri indexes from Python at two levels: (1) dictionary and\ntokenized document collection, (2) evaluating queries on the index. We hope\nthat with the release of pyndri, we will stimulate reproducible, open and\nfast-paced IR research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 17:17:34 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1701.00798", "submitter": "Amir Yazdavar", "authors": "Amir Hossein Yazdavar, Monireh Ebrahimi, Naomie Salim", "title": "Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences", "comments": "Text mining, Natural language processing, Sentiment analysis, Fuzzy\n  set theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the rapid growth of social media on the web, emotional polarity\ncomputation has become a flourishing frontier in the text mining community.\nHowever, it is challenging to understand the latest trends and summarize the\nstate or general opinions about products due to the big diversity and size of\nsocial media data and this creates the need of automated and real time opinion\nextraction and mining. On the other hand, the bulk of current research has been\ndevoted to study the subjective sentences which contain opinion keywords and\nlimited work has been reported for objective statements that imply sentiment.\nIn this paper, fuzzy based knowledge engineering model has been developed for\nsentiment classification of special group of such sentences including the\nchange or deviation from desired range or value. Drug reviews are the rich\nsource of such statements. Therefore, in this research, some experiments were\ncarried out on patient's reviews on several different cholesterol lowering\ndrugs to determine their sentiment polarity. The main conclusion through this\nstudy is, in order to increase the accuracy level of existing drug opinion\nmining systems, objective sentences which imply opinion should be taken into\naccount. Our experimental results demonstrate that our proposed model obtains\nover 72 percent F1 value.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 19:41:24 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Yazdavar", "Amir Hossein", ""], ["Ebrahimi", "Monireh", ""], ["Salim", "Naomie", ""]]}, {"id": "1701.00851", "submitter": "Herman Kamper", "authors": "Herman Kamper", "title": "Unsupervised neural and Bayesian models for zero-resource speech\n  processing", "comments": "PhD thesis, University of Edinburgh, 107 pages, submitted and\n  accepted 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In settings where only unlabelled speech data is available, zero-resource\nspeech technology needs to be developed without transcriptions, pronunciation\ndictionaries, or language modelling text. There are two central problems in\nzero-resource speech processing: (i) finding frame-level feature\nrepresentations which make it easier to discriminate between linguistic units\n(phones or words), and (ii) segmenting and clustering unlabelled speech into\nmeaningful units. In this thesis, we argue that a combination of top-down and\nbottom-up modelling is advantageous in tackling these two problems.\n  To address the problem of frame-level representation learning, we present the\ncorrespondence autoencoder (cAE), a neural network trained with weak top-down\nsupervision from an unsupervised term discovery system. By combining this\ntop-down supervision with unsupervised bottom-up initialization, the cAE yields\nmuch more discriminative features than previous approaches. We then present our\nunsupervised segmental Bayesian model that segments and clusters unlabelled\nspeech into hypothesized words. By imposing a consistent top-down segmentation\nwhile also using bottom-up knowledge from detected syllable boundaries, our\nsystem outperforms several others on multi-speaker conversational English and\nXitsonga speech data. Finally, we show that the clusters discovered by the\nsegmental Bayesian model can be made less speaker- and gender-specific by using\nfeatures from the cAE instead of traditional acoustic features.\n  In summary, the different models and systems presented in this thesis show\nthat both top-down and bottom-up modelling can improve representation learning,\nsegmentation and clustering of unlabelled speech data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 22:26:10 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Kamper", "Herman", ""]]}, {"id": "1701.00874", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Eduard Hovy", "title": "Neural Probabilistic Model for Non-projective MST Parsing", "comments": "To appear in IJCNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 00:10:17 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 04:09:29 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 23:06:11 GMT"}, {"version": "v4", "created": "Sun, 3 Sep 2017 21:12:40 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""]]}, {"id": "1701.00946", "submitter": "Ryan Cotterell Ryan D Cotterell", "authors": "Ryan Cotterell and Hinrich Sch\\\"utze", "title": "Joint Semantic Synthesis and Morphological Analysis of the Derived Word", "comments": "TACL 2017 (presented at ACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much like sentences are composed of words, words themselves are composed of\nsmaller units. For example, the English word questionably can be analyzed as\nquestion+able+ly. However, this structural decomposition of the word does not\ndirectly give us a semantic representation of the word's meaning. Since\nmorphology obeys the principle of compositionality, the semantics of the word\ncan be systematically derived from the meaning of its parts. In this work, we\npropose a novel probabilistic model of word formation that captures both the\nanalysis of a word w into its constituents segments and the synthesis of the\nmeaning of w from the meanings of those segments. Our model jointly learns to\nsegment words into morphemes and compose distributional semantic vectors of\nthose morphemes. We experiment with the model on English CELEX data and German\nDerivBase (Zeller et al., 2013) data. We show that jointly modeling semantics\nincreases both segmentation accuracy and morpheme F1 by between 3% and 5%.\nAdditionally, we investigate different models of vector composition, showing\nthat recurrent neural networks yield an improvement over simple additive\nmodels. Finally, we study the degree to which the representations correspond to\na linguist's notion of morphological productivity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 10:13:02 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 07:14:55 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 21:02:10 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Cotterell", "Ryan", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1701.00991", "submitter": "Robert J\\\"aschke", "authors": "Christoph Hube, Frank Fischer, Robert J\\\"aschke, Gerhard Lauer, Mads\n  Rosendahl Thomsen", "title": "World Literature According to Wikipedia: Introduction to a DBpedia-Based\n  Framework", "comments": "33 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Among the manifold takes on world literature, it is our goal to contribute to\nthe discussion from a digital point of view by analyzing the representation of\nworld literature in Wikipedia with its millions of articles in hundreds of\nlanguages. As a preliminary, we introduce and compare three different\napproaches to identify writers on Wikipedia using data from DBpedia, a\ncommunity project with the goal of extracting and providing structured\ninformation from Wikipedia. Equipped with our basic set of writers, we analyze\nhow they are represented throughout the 15 biggest Wikipedia language versions.\nWe combine intrinsic measures (mostly examining the connectedness of articles)\nwith extrinsic ones (analyzing how often articles are frequented by readers)\nand develop methods to evaluate our results. The better part of our findings\nseems to convey a rather conservative, old-fashioned version of world\nliterature, but a version derived from reproducible facts revealing an implicit\nliterary canon based on the editing and reading behavior of millions of people.\nWhile still having to solve some known issues, the introduced methods will help\nus build an observatory of world literature to further investigate its\nrepresentativeness and biases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:06:18 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Hube", "Christoph", ""], ["Fischer", "Frank", ""], ["J\u00e4schke", "Robert", ""], ["Lauer", "Gerhard", ""], ["Thomsen", "Mads Rosendahl", ""]]}, {"id": "1701.01126", "submitter": "Kai Zhao", "authors": "Kai Zhao, Liang Huang, Mingbo Ma", "title": "Textual Entailment with Structured Attentions and Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques are increasingly popular in the textual entailment\ntask, overcoming the fragility of traditional discrete models with hard\nalignments and logics. In particular, the recently proposed attention models\n(Rockt\\\"aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art\naccuracy by computing soft word alignments between the premise and hypothesis\nsentences. However, there remains a major limitation: this line of work\ncompletely ignores syntax and recursion, which is helpful in many traditional\nefforts. We show that it is beneficial to extend the attention model to tree\nnodes between premise and hypothesis. More importantly, this subtree-level\nattention reveals information about entailment relation. We study the recursive\ncomposition of this subtree-level entailment relation, which can be viewed as a\nsoft version of the Natural Logic framework (MacCartney and Manning, 2009).\nExperiments show that our structured attention and entailment composition model\ncan correctly identify and infer entailment relations from the bottom up, and\nbring significant improvements in accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 19:14:37 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhao", "Kai", ""], ["Huang", "Liang", ""], ["Ma", "Mingbo", ""]]}, {"id": "1701.01417", "submitter": "Pranav A", "authors": "Pranav Agrawal", "title": "Exploration of Proximity Heuristics in Length Normalization", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking functions used in information retrieval are primarily used in the\nsearch engines and they are often adopted for various language processing\napplications. However, features used in the construction of ranking functions\nshould be analyzed before applying it on a data set. This paper gives\nguidelines on construction of generalized ranking functions with\napplication-dependent features. The paper prescribes a specific case of a\ngeneralized function for recommendation system using feature engineering\nguidelines on the given data set. The behavior of both generalized and specific\nfunctions are studied and implemented on the unstructured textual data. The\nproximity feature based ranking function has outperformed by 52% from regular\nBM25.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 18:36:26 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Agrawal", "Pranav", ""]]}, {"id": "1701.01505", "submitter": "P. Jeffrey Brantingham", "authors": "Da Kuang, P. Jeffrey Brantingham, Andrea L. Bertozzi", "title": "Crime Topic Modeling", "comments": "47 pages, 4 tables, 7 figures", "journal-ref": "Kuang, D., Brantingham, P. J., & Bertozzi, A. L. (2017). Crime\n  topic modeling. Crime Science, 6(1), 12", "doi": "10.1186/s40163-017-0074-0", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of crime into discrete categories entails a massive loss\nof information. Crimes emerge out of a complex mix of behaviors and situations,\nyet most of these details cannot be captured by singular crime type labels.\nThis information loss impacts our ability to not only understand the causes of\ncrime, but also how to develop optimal crime prevention strategies. We apply\nmachine learning methods to short narrative text descriptions accompanying\ncrime records with the goal of discovering ecologically more meaningful latent\ncrime classes. We term these latent classes \"crime topics\" in reference to\ntext-based topic modeling methods that produce them. We use topic distributions\nto measure clustering among formally recognized crime types. Crime topics\nreplicate broad distinctions between violent and property crime, but also\nreveal nuances linked to target characteristics, situational conditions and the\ntools and methods of attack. Formal crime types are not discrete in topic\nspace. Rather, crime types are distributed across a range of crime topics.\nSimilarly, individual crime topics are distributed across a range of formal\ncrime types. Key ecological groups include identity theft, shoplifting,\nburglary and theft, car crimes and vandalism, criminal threats and confidence\ncrimes, and violent crimes. Though not a replacement for formal legal crime\nclassifications, crime topics provide a unique window into the heterogeneous\ncausal processes underlying crime.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 23:35:12 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 18:01:10 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Kuang", "Da", ""], ["Brantingham", "P. Jeffrey", ""], ["Bertozzi", "Andrea L.", ""]]}, {"id": "1701.01565", "submitter": "Edison Marrese-Taylor", "authors": "Edison Marrese-Taylor, Yutaka Matsuo", "title": "Replication issues in syntax-based aspect extraction for opinion mining", "comments": "Accepted in the EACL 2017 SRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducing experiments is an important instrument to validate previous work\nand build upon existing approaches. It has been tackled numerous times in\ndifferent areas of science. In this paper, we introduce an empirical\nreplicability study of three well-known algorithms for syntactic centric\naspect-based opinion mining. We show that reproducing results continues to be a\ndifficult endeavor, mainly due to the lack of details regarding preprocessing\nand parameter setting, as well as due to the absence of available\nimplementations that clarify these details. We consider these are important\nthreats to validity of the research on the field, specifically when compared to\nother problems in NLP where public datasets and code availability are critical\nvalidity components. We conclude by encouraging code-based research, which we\nthink has a key role in helping researchers to understand the meaning of the\nstate-of-the-art better and to generate continuous advances.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 08:18:38 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Marrese-Taylor", "Edison", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1701.01574", "submitter": "Haoyue Shi", "authors": "Haoyue Shi, Caihua Li and Junfeng Hu", "title": "Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word\n  Representation", "comments": "11 pages in CL4LC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous researches have shown that learning multiple representations for\npolysemous words can improve the performance of word embeddings on many tasks.\nHowever, this leads to another problem. Several vectors of a word may actually\npoint to the same meaning, namely pseudo multi-sense. In this paper, we\nintroduce the concept of pseudo multi-sense, and then propose an algorithm to\ndetect such cases. With the consideration of the detected pseudo multi-sense\ncases, we try to refine the existing word embeddings to eliminate the influence\nof pseudo multi-sense. Moreover, we apply our algorithm on previous released\nmulti-sense word embeddings and tested it on artificial word similarity tasks\nand the analogy task. The result of the experiments shows that diminishing\npseudo multi-sense can improve the quality of word representations. Thus, our\nmethod is actually an efficient way to reduce linguistic complexity.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 08:52:41 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Shi", "Haoyue", ""], ["Li", "Caihua", ""], ["Hu", "Junfeng", ""]]}, {"id": "1701.01614", "submitter": "Tsutomu Hirao", "authors": "Tsutomu Hirao, Masaaki Nishino, Jun Suzuki, Masaaki Nagata", "title": "Enumeration of Extractive Oracle Summaries", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze the limitations and the future directions of the extractive\nsummarization paradigm, this paper proposes an Integer Linear Programming (ILP)\nformulation to obtain extractive oracle summaries in terms of ROUGE-N. We also\npropose an algorithm that enumerates all of the oracle summaries for a set of\nreference summaries to exploit F-measures that evaluate which system summaries\ncontain how many sentences that are extracted as an oracle summary. Our\nexperimental results obtained from Document Understanding Conference (DUC)\ncorpora demonstrated the following: (1) room still exists to improve the\nperformance of extractive summarization; (2) the F-measures derived from the\nenumerated oracle summaries have significantly stronger correlations with human\njudgment than those derived from single oracle summaries.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 12:28:15 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Hirao", "Tsutomu", ""], ["Nishino", "Masaaki", ""], ["Suzuki", "Jun", ""], ["Nagata", "Masaaki", ""]]}, {"id": "1701.01623", "submitter": "Michael Sejr Schlichtkrull", "authors": "Michael Sejr Schlichtkrull and Anders S{\\o}gaard", "title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly\n  Low-Resource Languages", "comments": "To be published at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cross-lingual dependency annotation projection, information is often lost\nduring transfer because of early decoding. We present an end-to-end graph-based\nneural network dependency parser that can be trained to reproduce matrices of\nedge scores, which can be directly projected across word alignments. We show\nthat our approach to cross-lingual dependency parsing is not only simpler, but\nalso achieves an absolute improvement of 2.25% averaged across 10 languages\ncompared to the previous state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 12:54:48 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Schlichtkrull", "Michael Sejr", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1701.01811", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Alexandros Potamianos", "title": "Structural Attention Neural Networks for improved sentiment analysis", "comments": "Submitted to EACL2017 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a tree-structured attention neural network for sentences and\nsmall phrases and apply it to the problem of sentiment classification. Our\nmodel expands the current recursive models by incorporating structural\ninformation around a node of a syntactic tree using both bottom-up and top-down\ninformation propagation. Also, the model utilizes structural attention to\nidentify the most salient representations during the construction of the\nsyntactic tree. To our knowledge, the proposed models achieve state of the art\nperformance on the Stanford Sentiment Treebank dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 09:58:49 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1701.01854", "submitter": "Mohaddeseh Bastan", "authors": "Mohaddeseh Bastan, Shahram Khadivi, Mohammad Mehdi Homayounpour", "title": "Neural Machine Translation on Scarce-Resource Condition: A case-study on\n  Persian-English", "comments": "6 pages, Submitted in ICEE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) is a new approach for Machine Translation\n(MT), and due to its success, it has absorbed the attention of many researchers\nin the field. In this paper, we study NMT model on Persian-English language\npairs, to analyze the model and investigate the appropriateness of the model\nfor scarce-resourced scenarios, the situation that exists for Persian-centered\ntranslation systems. We adjust the model for the Persian language and find the\nbest parameters and hyper parameters for two tasks: translation and\ntransliteration. We also apply some preprocessing task on the Persian dataset\nwhich yields to increase for about one point in terms of BLEU score. Also, we\nhave modified the loss function to enhance the word alignment of the model.\nThis new loss function yields a total of 1.87 point improvements in terms of\nBLEU score in the translation quality.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 16:27:44 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Bastan", "Mohaddeseh", ""], ["Khadivi", "Shahram", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1701.01908", "submitter": "Fan Xu", "authors": "Fan Xu, Mingwen Wang and Maoxi Li", "title": "Sentence-level dialects identification in the greater China region", "comments": "12", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  5, No.6, December 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the different varieties of the same language is more challenging\nthan unrelated languages identification. In this paper, we propose an approach\nto discriminate language varieties or dialects of Mandarin Chinese for the\nMainland China, Hong Kong, Taiwan, Macao, Malaysia and Singapore, a.k.a., the\nGreater China Region (GCR). When applied to the dialects identification of the\nGCR, we find that the commonly used character-level or word-level uni-gram\nfeature is not very efficient since there exist several specific problems such\nas the ambiguity and context-dependent characteristic of words in the dialects\nof the GCR. To overcome these challenges, we use not only the general features\nlike character-level n-gram, but also many new word-level features, including\nPMI-based and word alignment-based features. A series of evaluation results on\nboth the news and open-domain dataset from Wikipedia show the effectiveness of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 03:13:37 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Xu", "Fan", ""], ["Wang", "Mingwen", ""], ["Li", "Maoxi", ""]]}, {"id": "1701.02025", "submitter": "Yadollah Yaghoobzadeh", "authors": "Yadollah Yaghoobzadeh and Hinrich Sch\\\"utze", "title": "Multi-level Representations for Fine-Grained Typing of Knowledge Base\n  Entities", "comments": "13 pages, in EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entities are essential elements of natural language. In this paper, we\npresent methods for learning multi-level representations of entities on three\ncomplementary levels: character (character patterns in entity names extracted,\ne.g., by neural networks), word (embeddings of words in entity names) and\nentity (entity embeddings). We investigate state-of-the-art learning methods on\neach level and find large differences, e.g., for deep learning models,\ntraditional ngram features and the subword model of fasttext (Bojanowski et\nal., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the\nword level; and for the order-aware model wang2vec (Ling et al., 2015a) on the\nentity level. We confirm experimentally that each level of representation\ncontributes complementary information and a joint representation of all three\nlevels improves the existing embedding based baseline for fine-grained entity\ntyping by a large margin. Additionally, we show that adding information from\nentity descriptions further improves multi-level representations of entities.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 22:20:22 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 22:11:51 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Yaghoobzadeh", "Yadollah", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1701.02073", "submitter": "Wei-Nan Zhang", "authors": "Weinan Zhang, Ting Liu, Yifa Wang, Qingfu Zhu", "title": "Neural Personalized Response Generation as Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the personalized response generation for\nconversational systems. Based on the sequence to sequence learning, especially\nthe encoder-decoder framework, we propose a two-phase approach, namely\ninitialization then adaptation, to model the responding style of human and then\ngenerate personalized responses. For evaluation, we propose a novel human aided\nmethod to evaluate the performance of the personalized response generation\nmodels by online real-time conversation and offline human judgement. Moreover,\nthe lexical divergence of the responses generated by the 5 personalized models\nindicates that the proposed two-phase approach achieves good results on\nmodeling the responding style of human and generating personalized responses\nfor the conversational systems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 06:42:57 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 01:34:45 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Weinan", ""], ["Liu", "Ting", ""], ["Wang", "Yifa", ""], ["Zhu", "Qingfu", ""]]}, {"id": "1701.02149", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin and Hinrich Sch\\\"utze", "title": "Task-Specific Attentive Pooling of Phrase Alignments Contributes to\n  Sentence Matching", "comments": "EACL'2017 long paper. arXiv admin note: substantial text overlap with\n  arXiv:1604.06896", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies comparatively two typical sentence matching tasks: textual\nentailment (TE) and answer selection (AS), observing that weaker phrase\nalignments are more critical in TE, while stronger phrase alignments deserve\nmore attention in AS. The key to reach this observation lies in phrase\ndetection, phrase representation, phrase alignment, and more importantly how to\nconnect those aligned phrases of different matching degrees with the final\nclassifier. Prior work (i) has limitations in phrase generation and\nrepresentation, or (ii) conducts alignment at word and phrase levels by\nhandcrafted features or (iii) utilizes a single framework of alignment without\nconsidering the characteristics of specific tasks, which limits the framework's\neffectiveness across tasks. We propose an architecture based on Gated Recurrent\nUnit that supports (i) representation learning of phrases of arbitrary\ngranularity and (ii) task-specific attentive pooling of phrase alignments\nbetween two sentences. Experimental results on TE and AS match our observation\nand show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 12:03:11 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1701.02163", "submitter": "Valentina Franzoni", "authors": "Valentina Franzoni", "title": "Just an Update on PMING Distance for Web-based Semantic Similarity in\n  Artificial Intelligence and Data Mining", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.20531.22560", "report-no": null, "categories": "cs.AI cs.CL cs.IR math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main problems that emerges in the classic approach to semantics is\nthe difficulty in acquisition and maintenance of ontologies and semantic\nannotations. On the other hand, the Internet explosion and the massive\ndiffusion of mobile smart devices lead to the creation of a worldwide system,\nwhich information is daily checked and fueled by the contribution of millions\nof users who interacts in a collaborative way. Search engines, continually\nexploring the Web, are a natural source of information on which to base a\nmodern approach to semantic annotation. A promising idea is that it is possible\nto generalize the semantic similarity, under the assumption that semantically\nsimilar terms behave similarly, and define collaborative proximity measures\nbased on the indexing information returned by search engines. The PMING\nDistance is a proximity measure used in data mining and information retrieval,\nwhich collaborative information express the degree of relationship between two\nterms, using only the number of documents returned as result for a query on a\nsearch engine. In this work, the PMINIG Distance is updated, providing a novel\nformal algebraic definition, which corrects previous works. The novel point of\nview underlines the features of the PMING to be a locally normalized linear\ncombination of the Pointwise Mutual Information and Normalized Google Distance.\nThe analyzed measure dynamically reflects the collaborative change made on the\nweb resources.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 13:02:35 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Franzoni", "Valentina", ""]]}, {"id": "1701.02185", "submitter": "Anca Dumitrache", "authors": "Anca Dumitrache, Lora Aroyo, Chris Welty", "title": "Crowdsourcing Ground Truth for Medical Relation Extraction", "comments": "Accepted for publication in ACM Transactions on Interactive\n  Intelligent Systems (TiiS) Special Issue on Human-Centered Machine Learning", "journal-ref": "ACM Transactions on Interactive Intelligent Systems (TIIS) Volume\n  8 Issue 2, July 2018", "doi": "10.1145/3152889", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cognitive computing systems require human labeled data for evaluation, and\noften for training. The standard practice used in gathering this data minimizes\ndisagreement between annotators, and we have found this results in data that\nfails to account for the ambiguity inherent in language. We have proposed the\nCrowdTruth method for collecting ground truth through crowdsourcing, that\nreconsiders the role of people in machine learning based on the observation\nthat disagreement between annotators provides a useful signal for phenomena\nsuch as ambiguity in the text. We report on using this method to build an\nannotated data set for medical relation extraction for the $cause$ and $treat$\nrelations, and how this data performed in a supervised training experiment. We\ndemonstrate that by modeling ambiguity, labeled data gathered from crowd\nworkers can (1) reach the level of quality of domain experts for this task\nwhile reducing the cost, and (2) provide better training data at scale than\ndistant supervision. We further propose and validate new weighted measures for\nprecision, recall, and F-measure, that account for ambiguity in both human and\nmachine performance on this task.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 14:13:23 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 15:04:43 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Dumitrache", "Anca", ""], ["Aroyo", "Lora", ""], ["Welty", "Chris", ""]]}, {"id": "1701.02477", "submitter": "Abhinav Thanda", "authors": "Abhinav Thanda, Shankar M Venkatesan", "title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) involves the simultaneous training of two or more\nrelated tasks over shared representations. In this work, we apply MTL to\naudio-visual automatic speech recognition(AV-ASR). Our primary task is to learn\na mapping between audio-visual fused features and frame labels obtained from\nacoustic GMM/HMM model. This is combined with an auxiliary task which maps\nvisual features to frame labels obtained from a separate visual GMM/HMM model.\nThe MTL model is tested at various levels of babble noise and the results are\ncompared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate\nthat MTL is especially useful at higher level of noise. Compared to base-line,\nupto 7\\% relative improvement in WER is reported at -3 SNR dB\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:47:56 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Thanda", "Abhinav", ""], ["Venkatesan", "Shankar M", ""]]}, {"id": "1701.02481", "submitter": "Yang Xu", "authors": "Yang Xu and Jiawei Liu", "title": "Implicitly Incorporating Morphological Information into Word Embedding", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose three novel models to enhance word embedding by\nimplicitly using morphological information. Experiments on word similarity and\nsyntactic analogy show that the implicit models are superior to traditional\nexplicit ones. Our models outperform all state-of-the-art baselines and\nsignificantly improve the performance on both tasks. Moreover, our performance\non the smallest corpus is similar to the performance of CBOW on the corpus\nwhich is five times the size of ours. Parameter analysis indicates that the\nimplicit models can supplement semantic information during the word embedding\ntraining process.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 08:59:38 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 01:35:53 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 03:19:20 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Xu", "Yang", ""], ["Liu", "Jiawei", ""]]}, {"id": "1701.02593", "submitter": "Diego Marcheggiani", "authors": "Diego Marcheggiani, Anton Frolov, Ivan Titov", "title": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based\n  Semantic Role Labeling", "comments": "To appear in CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and accurate neural model for dependency-based semantic\nrole labeling. Our model predicts predicate-argument dependencies relying on\nstates of a bidirectional LSTM encoder. The semantic role labeler achieves\ncompetitive performance on English, even without any kind of syntactic\ninformation and only using local inference. However, when automatically\npredicted part-of-speech tags are provided as input, it substantially\noutperforms all previous local models and approaches the best reported results\non the English CoNLL-2009 dataset. We also consider Chinese, Czech and Spanish\nwhere our approach also achieves competitive results. Syntactic parsers are\nunreliable on out-of-domain data, so standard (i.e., syntactically-informed)\nSRL models are hindered when tested in this setting. Our syntax-agnostic model\nappears more robust, resulting in the best reported results on standard\nout-of-domain test sets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 14:01:47 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 16:47:47 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Marcheggiani", "Diego", ""], ["Frolov", "Anton", ""], ["Titov", "Ivan", ""]]}, {"id": "1701.02720", "submitter": "Mohammad Pezeshki", "authors": "Ying Zhang, Mohammad Pezeshki, Philemon Brakel, Saizheng Zhang, Cesar\n  Laurent Yoshua Bengio, Aaron Courville", "title": "Towards End-to-End Speech Recognition with Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are effective models for reducing\nspectral variations and modeling spectral correlations in acoustic features for\nautomatic speech recognition (ASR). Hybrid speech recognition systems\nincorporating CNNs with Hidden Markov Models/Gaussian Mixture Models\n(HMMs/GMMs) have achieved the state-of-the-art in various benchmarks.\nMeanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural\nNetworks (RNNs), which is proposed for labeling unsegmented sequences, makes it\nfeasible to train an end-to-end speech recognition system instead of hybrid\nsettings. However, RNNs are computationally expensive and sometimes difficult\nto train. In this paper, inspired by the advantages of both CNNs and the CTC\napproach, we propose an end-to-end speech framework for sequence labeling, by\ncombining hierarchical CNNs with CTC directly without recurrent connections. By\nevaluating the approach on the TIMIT phoneme recognition task, we show that the\nproposed model is not only computationally efficient, but also competitive with\nthe existing baseline systems. Moreover, we argue that CNNs have the capability\nto model temporal correlations with appropriate context information.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 18:30:11 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Zhang", "Ying", ""], ["Pezeshki", "Mohammad", ""], ["Brakel", "Philemon", ""], ["Zhang", "Saizheng", ""], ["Bengio", "Cesar Laurent Yoshua", ""], ["Courville", "Aaron", ""]]}, {"id": "1701.02795", "submitter": "Zeshan Hussain", "authors": "Hardie Cate and Zeshan Hussain", "title": "Bidirectional American Sign Language to English Translation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a bidirectional translation system that converts sentences from\nAmerican Sign Language (ASL) to English, and vice versa. To perform machine\ntranslation between ASL and English, we utilize a generative approach.\nSpecifically, we employ an adjustment to the IBM word-alignment model 1 (IBM\nWAM1), where we define language models for English and ASL, as well as a\ntranslation model, and attempt to generate a translation that maximizes the\nposterior distribution defined by these models. Then, using these models, we\nare able to quantify the concepts of fluency and faithfulness of a translation\nbetween languages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 21:45:56 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Cate", "Hardie", ""], ["Hussain", "Zeshan", ""]]}, {"id": "1701.02810", "submitter": "Alexander M. Rush", "authors": "Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M.\n  Rush", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "comments": "Report for http://opennmt.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an open-source toolkit for neural machine translation (NMT). The\ntoolkit prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 23:32:43 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 15:54:27 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Klein", "Guillaume", ""], ["Kim", "Yoon", ""], ["Deng", "Yuntian", ""], ["Senellart", "Jean", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1701.02854", "submitter": "Cong Duy Vu Hoang Mr", "authors": "Cong Duy Vu Hoang (University of Melbourne), Gholamreza Haffari\n  (Monash University), Trevor Cohn (University of Melbourne)", "title": "Towards Decoding as Continuous Optimization in Neural Machine\n  Translation", "comments": "EMNLP 2017 Camera Ready Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel decoding approach for neural machine translation (NMT)\nbased on continuous optimisation. We convert decoding - basically a discrete\noptimization problem - into a continuous optimization problem. The resulting\nconstrained continuous optimisation problem is then tackled using\ngradient-based methods. Our powerful decoding framework enables decoding\nintractable models such as the intersection of left-to-right and right-to-left\n(bidirectional) as well as source-to-target and target-to-source (bilingual)\nNMT models. Our empirical results show that our decoding framework is\neffective, and leads to substantial improvements in translations generated from\nthe intersected models where the typical greedy or beam search is not feasible.\nWe also compare our framework against reranking, and analyse its advantages and\ndisadvantages.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 06:02:44 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 00:15:08 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 09:26:30 GMT"}, {"version": "v4", "created": "Sat, 22 Jul 2017 16:35:43 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hoang", "Cong Duy Vu", "", "University of Melbourne"], ["Haffari", "Gholamreza", "", "Monash University"], ["Cohn", "Trevor", "", "University of Melbourne"]]}, {"id": "1701.02877", "submitter": "Leon Derczynski", "authors": "Isabelle Augenstein, Leon Derczynski, Kalina Bontcheva", "title": "Generalisation in Named Entity Recognition: A Quantitative Analysis", "comments": "Preprint, accepted to Computer Speech and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Named Entity Recognition (NER) is a key NLP task, which is all the more\nchallenging on Web and user-generated content with their diverse and\ncontinuously changing language. This paper aims to quantify how this diversity\nimpacts state-of-the-art NER methods, by measuring named entity (NE) and\ncontext variability, feature sparsity, and their effects on precision and\nrecall. In particular, our findings indicate that NER approaches struggle to\ngeneralise in diverse genres with limited training data. Unseen NEs, in\nparticular, play an important role, which have a higher incidence in diverse\ngenres such as social media than in more regular genres such as newswire.\nCoupled with a higher incidence of unseen features more generally and the lack\nof large training corpora, this leads to significantly lower F1 scores for\ndiverse genres as compared to more regular ones. We also find that leading\nsystems rely heavily on surface forms found in training data, having problems\ngeneralising beyond these, and offer explanations for this observation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 08:02:40 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 23:29:49 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Derczynski", "Leon", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1701.02901", "submitter": "Antonio Toral", "authors": "Antonio Toral and V\\'ictor M. S\\'anchez-Cartagena", "title": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine\n  Translation for 9 Language Directions", "comments": "Conference of the European Chapter of the Association for\n  Computational Linguistics (EACL). April 2017, Val\\`encia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We aim to shed light on the strengths and weaknesses of the newly introduced\nneural machine translation paradigm. To that end, we conduct a multifaceted\nevaluation in which we compare outputs produced by state-of-the-art neural\nmachine translation and phrase-based machine translation systems for 9 language\ndirections across a number of dimensions. Specifically, we measure the\nsimilarity of the outputs, their fluency and amount of reordering, the effect\nof sentence length and performance across different error categories. We find\nout that translations produced by neural machine translation systems are\nconsiderably different, more fluent and more accurate in terms of word order\ncompared to those produced by phrase-based systems. Neural machine translation\nsystems are also more accurate at producing inflected forms, but they perform\npoorly when translating very long sentences.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 09:32:47 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Toral", "Antonio", ""], ["S\u00e1nchez-Cartagena", "V\u00edctor M.", ""]]}, {"id": "1701.02925", "submitter": "Waheeb Ahmed", "authors": "Waheeb Ahmed, Dr. Anto P Babu", "title": "Question Analysis for Arabic Question Answering Systems", "comments": "10 pages, 3 figures, published article in IJNLC", "journal-ref": null, "doi": "10.5121/ijnlc.2016.5603", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first step of processing a question in Question Answering(QA) Systems is\nto carry out a detailed analysis of the question for the purpose of determining\nwhat it is asking for and how to perfectly approach answering it. Our Question\nanalysis uses several techniques to analyze any question given in natural\nlanguage: a Stanford POS Tagger & parser for Arabic language, a named entity\nrecognizer, tokenizer,Stop-word removal, Question expansion, Question\nclassification and Question focus extraction components. We employ numerous\ndetection rules and trained classifier using features from this analysis to\ndetect important elements of the question, including: 1) the portion of the\nquestion that is a referring to the answer (the focus); 2) different terms in\nthe question that identify what type of entity is being asked for (the lexical\nanswer types); 3) Question expansion ; 4) a process of classifying the question\ninto one or more of several and different types; and We describe how these\nelements are identified and evaluate the effect of accurate detection on our\nquestion-answering system using the Mean Reciprocal Rank(MRR) accuracy measure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 11:12:24 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Ahmed", "Waheeb", ""], ["Babu", "Dr. Anto P", ""]]}, {"id": "1701.02946", "submitter": "Chlo\\'e Braud", "authors": "Chlo\\'e Braud and Maximin Coavoux and Anders S{\\o}gaard", "title": "Cross-lingual RST Discourse Parsing", "comments": "To be published in EACL 2017, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse parsing is an integral part of understanding information flow and\nargumentative structure in documents. Most previous research has focused on\ninducing and evaluating models from the English RST Discourse Treebank.\nHowever, discourse treebanks for other languages exist, including Spanish,\nGerman, Basque, Dutch and Brazilian Portuguese. The treebanks share the same\nunderlying linguistic theory, but differ slightly in the way documents are\nannotated. In this paper, we present (a) a new discourse parser which is\nsimpler, yet competitive (significantly better on 2/3 metrics) to state of the\nart for English, (b) a harmonization of discourse treebanks across languages,\nenabling us to present (c) what to the best of our knowledge are the first\nexperiments on cross-lingual discourse parsing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:16:25 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Braud", "Chlo\u00e9", ""], ["Coavoux", "Maximin", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1701.02962", "submitter": "Kim Anh Nguyen", "authors": "Kim Anh Nguyen and Sabine Schulte im Walde and Ngoc Thang Vu", "title": "Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network", "comments": "EACL 2017, 10 pages", "journal-ref": "EACL2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinguishing between antonyms and synonyms is a key task to achieve high\nperformance in NLP systems. While they are notoriously difficult to distinguish\nby distributional co-occurrence models, pattern-based methods have proven\neffective to differentiate between the relations. In this paper, we present a\nnovel neural network model AntSynNET that exploits lexico-syntactic patterns\nfrom syntactic parse trees. In addition to the lexical and syntactic\ninformation, we successfully integrate the distance between the related words\nalong the syntactic path as a new pattern feature. The results from\nclassification experiments show that AntSynNET improves the performance over\nprior pattern-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 13:11:48 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Nguyen", "Kim Anh", ""], ["Walde", "Sabine Schulte im", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "1701.03038", "submitter": "Arturo Argueta", "authors": "Arturo Argueta, David Chiang", "title": "Decoding with Finite-State Transducers on GPUs", "comments": "accepted at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted finite automata and transducers (including hidden Markov models and\nconditional random fields) are widely used in natural language processing (NLP)\nto perform tasks such as morphological analysis, part-of-speech tagging,\nchunking, named entity recognition, speech recognition, and others.\nParallelizing finite state algorithms on graphics processing units (GPUs) would\nbenefit many areas of NLP. Although researchers have implemented GPU versions\nof basic graph algorithms, limited previous work, to our knowledge, has been\ndone on GPU algorithms for weighted finite automata. We introduce a GPU\nimplementation of the Viterbi and forward-backward algorithm, achieving\ndecoding speedups of up to 5.2x over our serial implementation running on\ndifferent computer architectures and 6093x over OpenFST.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:07:27 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 14:48:24 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Argueta", "Arturo", ""], ["Chiang", "David", ""]]}, {"id": "1701.03051", "submitter": "Venkata Naveen Reddy Chedeti", "authors": "Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, Manish Singh", "title": "Efficient Twitter Sentiment Classification using Subjective Distant\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As microblogging services like Twitter are becoming more and more influential\nin today's globalised world, its facets like sentiment analysis are being\nextensively studied. We are no longer constrained by our own opinion. Others\nopinions and sentiments play a huge role in shaping our perspective. In this\npaper, we build on previous works on Twitter sentiment analysis using Distant\nSupervision. The existing approach requires huge computation resource for\nanalysing large number of tweets. In this paper, we propose techniques to speed\nup the computation process for sentiment analysis. We use tweet subjectivity to\nselect the right training samples. We also introduce the concept of EFWS\n(Effective Word Score) of a tweet that is derived from polarity scores of\nfrequently used words, which is an additional heuristic that can be used to\nspeed up the sentiment classification with standard machine learning\nalgorithms. We performed our experiments using 1.6 million tweets. Experimental\nevaluations show that our proposed technique is more efficient and has higher\naccuracy compared to previously proposed methods. We achieve overall accuracies\nof around 80% (EFWS heuristic gives an accuracy around 85%) on a training\ndataset of 100K tweets, which is half the size of the dataset used for the\nbaseline model. The accuracy of our proposed model is 2-3% higher than the\nbaseline model, and the model effectively trains at twice the speed of the\nbaseline model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:39:04 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Sahni", "Tapan", ""], ["Chandak", "Chinmay", ""], ["Chedeti", "Naveen Reddy", ""], ["Singh", "Manish", ""]]}, {"id": "1701.03079", "submitter": "Lili Mou", "authors": "Chongyang Tao, Lili Mou, Dongyan Zhao, Rui Yan", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain\n  Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain human-computer conversation has been attracting increasing\nattention over the past few years. However, there does not exist a standard\nautomatic evaluation metric for open-domain dialog systems; researchers usually\nresort to human annotation for model evaluation, which is time- and\nlabor-intensive. In this paper, we propose RUBER, a Referenced metric and\nUnreferenced metric Blended Evaluation Routine, which evaluates a reply by\ntaking into consideration both a groundtruth reply and a query (previous\nuser-issued utterance). Our metric is learnable, but its training does not\nrequire labels of human satisfaction. Hence, RUBER is flexible and extensible\nto different datasets and languages. Experiments on both retrieval and\ngenerative dialog systems show that RUBER has a high correlation with human\nannotation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:43:57 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 16:43:08 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tao", "Chongyang", ""], ["Mou", "Lili", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1701.03092", "submitter": "Besat Kassaie", "authors": "Besat Kassaie", "title": "Job Detection in Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we propose a new application for twitter data called\n\\textit{job detection}. We identify people's job category based on their\ntweets. As a preliminary work, we limited our task to identify only IT workers\nfrom other job holders. We have used and compared both simple bag of words\nmodel and a document representation based on Skip-gram model. Our results show\nthat the model based on Skip-gram, achieves a 76\\% precision and 82\\% recall.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 18:42:09 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Kassaie", "Besat", ""]]}, {"id": "1701.03126", "submitter": "Chiori Hori Dr.", "authors": "Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R.\n  Hershey, Tim K. Marks", "title": "Attention-Based Multimodal Fusion for Video Description", "comments": "Resubmitted to the rebuttal for CVPR 2017 for review, 8 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently successful methods for video description are based on\nencoder-decoder sentence generation using recur-rent neural networks (RNNs).\nRecent work has shown the advantage of integrating temporal and/or spatial\nattention mechanisms into these models, in which the decoder net-work predicts\neach word in the description by selectively giving more weight to encoded\nfeatures from specific time frames (temporal attention) or to features from\nspecific spatial regions (spatial attention). In this paper, we propose to\nexpand the attention model to selectively attend not just to specific times or\nspatial regions, but to specific modalities of input such as image features,\nmotion features, and audio features. Our new modality-dependent attention\nmechanism, which we call multimodal attention, provides a natural way to fuse\nmultimodal information for video description. We evaluate our method on the\nYoutube2Text dataset, achieving results that are competitive with current state\nof the art. More importantly, we demonstrate that our model incorporating\nmultimodal attention as well as temporal attention significantly outperforms\nthe model that uses temporal attention alone.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:16:42 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 22:57:10 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Hori", "Chiori", ""], ["Hori", "Takaaki", ""], ["Lee", "Teng-Yok", ""], ["Sumi", "Kazuhiro", ""], ["Hershey", "John R.", ""], ["Marks", "Tim K.", ""]]}, {"id": "1701.03129", "submitter": "Besat Kassaie", "authors": "Besat Kassaie", "title": "De-identification In practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report our effort to identify the sensitive information, subset of data\nitems listed by HIPAA (Health Insurance Portability and Accountability), from\nmedical text using the recent advances in natural language processing and\nmachine learning techniques. We represent the words with high dimensional\ncontinuous vectors learned by a variant of Word2Vec called Continous Bag Of\nWords (CBOW). We feed the word vectors into a simple neural network with a Long\nShort-Term Memory (LSTM) architecture. Without any attempts to extract manually\ncrafted features and considering that our medical dataset is too small to be\nfed into neural network, we obtained promising results. The results thrilled us\nto think about the larger scale of the project with precise parameter tuning\nand other possible improvements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:22:56 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Kassaie", "Besat", ""]]}, {"id": "1701.03163", "submitter": "H\\'ector Mart\\'inez Alonso", "authors": "H\\'ector Mart\\'inez Alonso and \\v{Z}eljko Agi\\'c and Barbara Plank and\n  Anders S{\\o}gaard", "title": "Parsing Universal Dependencies without training", "comments": "EACL 2017, 8+2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose UDP, the first training-free parser for Universal Dependencies\n(UD). Our algorithm is based on PageRank and a small set of head attachment\nrules. It features two-step decoding to guarantee that function words are\nattached as leaf nodes. The parser requires no training, and it is competitive\nwith a delexicalized transfer system. UDP offers a linguistically sound\nunsupervised alternative to cross-lingual parsing for UD, which can be used as\na baseline for such systems. The parser has very few parameters and is\ndistinctly robust to domain change across languages.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 20:56:29 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Alonso", "H\u00e9ctor Mart\u00ednez", ""], ["Agi\u0107", "\u017deljko", ""], ["Plank", "Barbara", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1701.03185", "submitter": "Louis Shao", "authors": "Louis Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, Ray\n  Kurzweil", "title": "Generating High-Quality and Informative Conversation Responses with\n  Sequence-to-Sequence Models", "comments": "To appear in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have been applied to the conversation response\ngeneration problem where the source sequence is the conversation history and\nthe target sequence is the response. Unlike translation, conversation\nresponding is inherently creative. The generation of long, informative,\ncoherent, and diverse responses remains a hard task. In this work, we focus on\nthe single turn setting. We add self-attention to the decoder to maintain\ncoherence in longer responses, and we propose a practical approach, called the\nglimpse-model, for scaling to large datasets. We introduce a stochastic\nbeam-search algorithm with segment-by-segment reranking which lets us inject\ndiversity earlier in the generation process. We trained on a combined data set\nof over 2.3B conversation messages mined from the web. In human evaluation\nstudies, our method produces longer responses overall, with a higher proportion\nrated as acceptable and excellent as length increases, compared to baseline\nsequence-to-sequence models with explicit length-promotion. A back-off strategy\nproduces better responses overall, in the full spectrum of lengths.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 22:55:04 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 16:53:33 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Shao", "Louis", ""], ["Gouws", "Stephan", ""], ["Britz", "Denny", ""], ["Goldie", "Anna", ""], ["Strope", "Brian", ""], ["Kurzweil", "Ray", ""]]}, {"id": "1701.03214", "submitter": "Chenhui Chu", "authors": "Chenhui Chu, Raj Dabre, and Sadao Kurohashi", "title": "An Empirical Comparison of Simple Domain Adaptation Methods for Neural\n  Machine Translation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel domain adaptation method named \"mixed fine\ntuning\" for neural machine translation (NMT). We combine two existing\napproaches namely fine tuning and multi domain NMT. We first train an NMT model\non an out-of-domain parallel corpus, and then fine tune it on a parallel corpus\nwhich is a mix of the in-domain and out-of-domain corpora. All corpora are\naugmented with artificial tags to indicate specific domains. We empirically\ncompare our proposed method against fine tuning and multi domain methods and\ndiscuss its benefits and shortcomings.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 02:37:09 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 21:46:12 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Chu", "Chenhui", ""], ["Dabre", "Raj", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "1701.03227", "submitter": "Angela Fan", "authors": "Angela Fan, Finale Doshi-Velez, Luke Miratrix", "title": "Prior matters: simple and general methods for evaluating and improving\n  topic quality in topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) models trained without stopword removal\noften produce topics with high posterior probabilities on uninformative words,\nobscuring the underlying corpus content. Even when canonical stopwords are\nmanually removed, uninformative words common in that corpus will still dominate\nthe most probable words in a topic. In this work, we first show how the\nstandard topic quality measures of coherence and pointwise mutual information\nact counter-intuitively in the presence of common but irrelevant words, making\nit difficult to even quantitatively identify situations in which topics may be\ndominated by stopwords. We propose an additional topic quality metric that\ntargets the stopword problem, and show that it, unlike the standard measures,\ncorrectly correlates with human judgements of quality. We also propose a\nsimple-to-implement strategy for generating topics that are evaluated to be of\nmuch higher quality by both human assessment and our new metric. This approach,\na collection of informative priors easily introduced into most LDA-style\ninference methods, automatically promotes terms with domain relevance and\ndemotes domain-specific stop words. We demonstrate this approach's\neffectiveness in three very different domains: Department of Labor accident\nreports, online health forum posts, and NIPS abstracts. Overall we find that\ncurrent practices thought to solve this problem do not do so adequately, and\nthat our proposal offers a substantial improvement for those interested in\ninterpreting their topics as objects in their own right.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 04:26:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 23:17:12 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 18:25:03 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Fan", "Angela", ""], ["Doshi-Velez", "Finale", ""], ["Miratrix", "Luke", ""]]}, {"id": "1701.03231", "submitter": "Manuel Amunategui", "authors": "Manuel Amunategui", "title": "Single-Pass, Adaptive Natural Language Filtering: Measuring Value in\n  User Generated Comments on Large-Scale, Social Media News Forums", "comments": "52 pages and python code included in index", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are large amounts of insight and social discovery potential in mining\ncrowd-sourced comments left on popular news forums like Reddit.com, Tumblr.com,\nFacebook.com and Hacker News. Unfortunately, due the overwhelming amount of\nparticipation with its varying quality of commentary, extracting value out of\nsuch data isn't always obvious nor timely. By designing efficient, single-pass\nand adaptive natural language filters to quickly prune spam, noise, copy-cats,\nmarketing diversions, and out-of-context posts, we can remove over a third of\nentries and return the comments with a higher probability of relatedness to the\noriginal article in question. The approach presented here uses an adaptive,\ntwo-step filtering process. It first leverages the original article posted in\nthe thread as a starting corpus to parse comments by matching intersecting\nwords and term-ratio balance per sentence then grows the corpus by adding new\nwords harvested from high-matching comments to increase filtering accuracy over\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 04:55:36 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Amunategui", "Manuel", ""]]}, {"id": "1701.03329", "submitter": "Andreas van Cranenburgh", "authors": "Andreas van Cranenburgh, Rens Bod", "title": "A Data-Oriented Model of Literary Language", "comments": "To be published in EACL 2017, 11 pages", "journal-ref": "Proceedings of EACL 2017, pp. 1228-1238", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of predicting how literary a text is, with a gold\nstandard from human ratings. Aside from a standard bigram baseline, we apply\nrich syntactic tree fragments, mined from the training set, and a series of\nhand-picked features. Our model is the first to distinguish degrees of highly\nand less literary novels using a variety of lexical and syntactic features, and\nexplains 76.0 % of the variation in literary ratings.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 13:03:49 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 23:39:25 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["van Cranenburgh", "Andreas", ""], ["Bod", "Rens", ""]]}, {"id": "1701.03338", "submitter": "Tom Kocmi", "authors": "Tom Kocmi, Ond\\v{r}ej Bojar", "title": "LanideNN: Multilingual Language Identification on Character Window", "comments": "Accepted to EACL 2017", "journal-ref": "Proceedings of the 15th Conference of the European Chapter of the\n  Association for Computational Linguistics: Volume 1, Long Papers, 927-936\n  (2017)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In language identification, a common first step in natural language\nprocessing, we want to automatically determine the language of some input text.\nMonolingual language identification assumes that the given document is written\nin one language. In multilingual language identification, the document is\nusually in two or three languages and we just want their names. We aim one step\nfurther and propose a method for textual language identification where\nlanguages can change arbitrarily and the goal is to identify the spans of each\nof the languages. Our method is based on Bidirectional Recurrent Neural\nNetworks and it performs well in monolingual and multilingual language\nidentification tasks on six datasets covering 131 languages. The method keeps\nthe accuracy also for short documents and across domains, so it is ideal for\noff-the-shelf use without preparation of training data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 13:41:08 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 15:52:00 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kocmi", "Tom", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1701.03434", "submitter": "Noura Farra Noura Farra", "authors": "Noura Farra, Kathleen McKeown", "title": "SMARTies: Sentiment Models for Arabic Target Entities", "comments": "To be published in Proceedings of the European Chapter of the\n  Association for Computational Linguistics (EACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider entity-level sentiment analysis in Arabic, a morphologically rich\nlanguage with increasing resources. We present a system that is applied to\ncomplex posts written in response to Arabic newspaper articles. Our goal is to\nidentify important entity \"targets\" within the post along with the polarity\nexpressed about each target. We achieve significant improvements over multiple\nbaselines, demonstrating that the use of specific morphological representations\nimproves the performance of identifying both important targets and their\nsentiment, and that the use of distributional semantic clusters further boosts\nperformances for these representations, especially when richer linguistic\nresources are not available.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 17:51:12 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Farra", "Noura", ""], ["McKeown", "Kathleen", ""]]}, {"id": "1701.03492", "submitter": "Emrah Budur", "authors": "Emrah Budur", "title": "Scalable, Trie-based Approximate Entity Extraction for Real-Time\n  Financial Transaction Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial institutions have to screen their transactions to ensure that they\nare not affiliated with terrorism entities. Developing appropriate solutions to\ndetect such affiliations precisely while avoiding any kind of interruption to\nlarge amount of legitimate transactions is essential. In this paper, we present\nbuilding blocks of a scalable solution that may help financial institutions to\nbuild their own software to extract terrorism entities out of both structured\nand unstructured financial messages in real time and with approximate\nsimilarity matching approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 20:14:52 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Budur", "Emrah", ""]]}, {"id": "1701.03577", "submitter": "Avner May", "authors": "Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu,\n  Aur\\'elien Bellet, Linxi Fan, Michael Collins, Daniel Hsu, Brian Kingsbury,\n  Michael Picheny, Fei Sha", "title": "Kernel Approximation Methods for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study large-scale kernel methods for acoustic modeling in speech\nrecognition and compare their performance to deep neural networks (DNNs). We\nperform experiments on four speech recognition datasets, including the TIMIT\nand Broadcast News benchmark tasks, and compare these two types of models on\nframe-level performance metrics (accuracy, cross-entropy), as well as on\nrecognition metrics (word/character error rate). In order to scale kernel\nmethods to these large datasets, we use the random Fourier feature method of\nRahimi and Recht (2007). We propose two novel techniques for improving the\nperformance of kernel acoustic models. First, in order to reduce the number of\nrandom features required by kernel models, we propose a simple but effective\nmethod for feature selection. The method is able to explore a large number of\nnon-linear features while maintaining a compact model more efficiently than\nexisting approaches. Second, we present a number of frame-level metrics which\ncorrelate very strongly with recognition performance when computed on the\nheldout set; we take advantage of these correlations by monitoring these\nmetrics during training in order to decide when to stop learning. This\ntechnique can noticeably improve the recognition performance of both DNN and\nkernel models, while narrowing the gap between them. Additionally, we show that\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\nof our kernel models significantly, in addition to speeding up training and\nmaking the models more compact. Together, these three methods dramatically\nimprove the performance of kernel acoustic models, making their performance\ncomparable to DNNs on the tasks we explored.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 07:24:18 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["May", "Avner", ""], ["Garakani", "Alireza Bagheri", ""], ["Lu", "Zhiyun", ""], ["Guo", "Dong", ""], ["Liu", "Kuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Hsu", "Daniel", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1701.03578", "submitter": "Seunghyun Yoon", "authors": "Seunghyun Yoon, Hyeongu Yun, Yuna Kim, Gyu-tae Park, Kyomin Jung", "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling\n  using Recurrent Neural Network", "comments": "AAAI workshop on Crowdsourcing, Deep Learning and Artificial\n  Intelligence Agents, Feb 2017, San Francisco CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient transfer leaning methods for training\na personalized language model using a recurrent neural network with long\nshort-term memory architecture. With our proposed fast transfer learning\nschemes, a general language model is updated to a personalized language model\nwith a small amount of user data and a limited computing resource. These\nmethods are especially useful for a mobile device environment while the data is\nprevented from transferring out of the device for privacy purposes. Through\nexperiments on dialogue data in a drama, it is verified that our transfer\nlearning methods have successfully generated the personalized language model,\nwhose output is more similar to the personal language style in both qualitative\nand quantitative aspects.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 07:26:00 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yoon", "Seunghyun", ""], ["Yun", "Hyeongu", ""], ["Kim", "Yuna", ""], ["Park", "Gyu-tae", ""], ["Jung", "Kyomin", ""]]}, {"id": "1701.03682", "submitter": "Emrah Budur", "authors": "Priyank Mathur, Arkajyoti Misra, Emrah Budur", "title": "LIDE: Language Identification from Text Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in the use of microblogging came along with the rapid growth on\nshort linguistic data. On the other hand deep learning is considered to be the\nnew frontier to extract meaningful information out of large amount of raw data\nin an automated manner. In this study, we engaged these two emerging fields to\ncome up with a robust language identifier on demand, namely Language\nIdentification Engine (LIDE). As a result, we achieved 95.12% accuracy in\nDiscriminating between Similar Languages (DSL) Shared Task 2015 dataset, which\nis comparable to the maximum reported accuracy of 95.54% achieved so far.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 14:20:06 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Mathur", "Priyank", ""], ["Misra", "Arkajyoti", ""], ["Budur", "Emrah", ""]]}, {"id": "1701.03849", "submitter": "Pavel Kral", "authors": "Ladislav Lenc, Pavel Kr\\'al", "title": "Deep Neural Networks for Czech Multi-label Document Classification", "comments": "Presented at 17th International Conference on Intelligent Text\n  Processing and Computational Linguistics (CICLing 2016), Konya, Turkey, 3-9\n  April 2016, pp. 460-471, Springer, ISBN: 978-3-319-75487-1", "journal-ref": null, "doi": "10.1007/978-3-319-75487-1_36", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on automatic multi-label document classification of\nCzech text documents. The current approaches usually use some pre-processing\nwhich can have negative impact (loss of information, additional implementation\nwork, etc). Therefore, we would like to omit it and use deep neural networks\nthat learn from simple features. This choice was motivated by their successful\nusage in many other machine learning fields. Two different networks are\ncompared: the first one is a standard multi-layer perceptron, while the second\none is a popular convolutional network. The experiments on a Czech newspaper\ncorpus show that both networks significantly outperform baseline method which\nuses a rich set of features with maximum entropy classifier. We have also shown\nthat convolutional network gives the best results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 23:23:12 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 23:17:30 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 20:07:14 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lenc", "Ladislav", ""], ["Kr\u00e1l", "Pavel", ""]]}, {"id": "1701.03924", "submitter": "Nadir Durrani Dr", "authors": "Nadir Durrani and Fahim Dalvi and Hassan Sajjad, Stephan Vogel", "title": "QCRI Machine Translation Systems for IWSLT 16", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes QCRI's machine translation systems for the IWSLT 2016\nevaluation campaign. We participated in the Arabic->English and English->Arabic\ntracks. We built both Phrase-based and Neural machine translation models, in an\neffort to probe whether the newly emerged NMT framework surpasses the\ntraditional phrase-based systems in Arabic-English language pairs. We trained a\nvery strong phrase-based system including, a big language model, the Operation\nSequence Model, Neural Network Joint Model and Class-based models along with\ndifferent domain adaptation techniques such as MML filtering, mixture modeling\nand using fine tuning over NNJM model. However, a Neural MT system, trained by\nstacking data from different genres through fine-tuning, and applying ensemble\nover 8 models, beat our very strong phrase-based system by a significant 2 BLEU\npoints margin in Arabic->English direction. We did not obtain similar gains in\nthe other direction but were still able to outperform the phrase-based system.\nWe also applied system combination on phrase-based and NMT outputs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 14:18:54 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Durrani", "Nadir", ""], ["Dalvi", "Fahim", ""], ["Sajjad", "Hassan", ""], ["Vogel", "Stephan", ""]]}, {"id": "1701.03947", "submitter": "Tuan Tran", "authors": "Tuan Tran, Claudia Nieder\\'ee, Nattiya Kanhabua, Ujwal Gadiraju,\n  Avishek Anand", "title": "Balancing Novelty and Salience: Adaptive Learning to Rank Entities for\n  Timeline Summarization of High-impact Events", "comments": "Published via ACM to CIKM 2015", "journal-ref": null, "doi": "10.1145/2806416.2806486", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-running, high-impact events such as the Boston Marathon bombing often\ndevelop through many stages and involve a large number of entities in their\nunfolding. Timeline summarization of an event by key sentences eases story\ndigestion, but does not distinguish between what a user remembers and what she\nmight want to re-check. In this work, we present a novel approach for timeline\nsummarization of high-impact events, which uses entities instead of sentences\nfor summarizing the event at each individual point in time. Such entity\nsummaries can serve as both (1) important memory cues in a retrospective event\nconsideration and (2) pointers for personalized event exploration. In order to\nautomatically create such summaries, it is crucial to identify the \"right\"\nentities for inclusion. We propose to learn a ranking function for entities,\nwith a dynamically adapted trade-off between the in-document salience of\nentities and the informativeness of entities across documents, i.e., the level\nof new information associated with an entity for a time point under\nconsideration. Furthermore, for capturing collective attention for an entity we\nuse an innovative soft labeling approach based on Wikipedia. Our experiments on\na real large news datasets confirm the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 16:47:51 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Tran", "Tuan", ""], ["Nieder\u00e9e", "Claudia", ""], ["Kanhabua", "Nattiya", ""], ["Gadiraju", "Ujwal", ""], ["Anand", "Avishek", ""]]}, {"id": "1701.03980", "submitter": "Graham Neubig", "authors": "Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed\n  Ammar, Antonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel\n  Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette,\n  Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya\n  Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha\n  Swayamdipta, Pengcheng Yin", "title": "DyNet: The Dynamic Neural Network Toolkit", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe DyNet, a toolkit for implementing neural network models based on\ndynamic declaration of network structure. In the static declaration strategy\nthat is used in toolkits like Theano, CNTK, and TensorFlow, the user first\ndefines a computation graph (a symbolic representation of the computation), and\nthen examples are fed into an engine that executes this computation and\ncomputes its derivatives. In DyNet's dynamic declaration strategy, computation\ngraph construction is mostly transparent, being implicitly constructed by\nexecuting procedural code that computes the network outputs, and the user is\nfree to use different network structures for each input. Dynamic declaration\nthus facilitates the implementation of more complicated network architectures,\nand DyNet is specifically designed to allow users to implement their models in\na way that is idiomatic in their preferred programming language (C++ or\nPython). One challenge with dynamic declaration is that because the symbolic\ncomputation graph is defined anew for every training example, its construction\nmust have low overhead. To achieve this, DyNet has an optimized C++ backend and\nlightweight graph representation. Experiments show that DyNet's speeds are\nfaster than or comparable with static declaration toolkits, and significantly\nfaster than Chainer, another dynamic declaration toolkit. DyNet is released\nopen-source under the Apache 2.0 license and available at\nhttp://github.com/clab/dynet.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 01:53:23 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Neubig", "Graham", ""], ["Dyer", "Chris", ""], ["Goldberg", "Yoav", ""], ["Matthews", "Austin", ""], ["Ammar", "Waleed", ""], ["Anastasopoulos", "Antonios", ""], ["Ballesteros", "Miguel", ""], ["Chiang", "David", ""], ["Clothiaux", "Daniel", ""], ["Cohn", "Trevor", ""], ["Duh", "Kevin", ""], ["Faruqui", "Manaal", ""], ["Gan", "Cynthia", ""], ["Garrette", "Dan", ""], ["Ji", "Yangfeng", ""], ["Kong", "Lingpeng", ""], ["Kuncoro", "Adhiguna", ""], ["Kumar", "Gaurav", ""], ["Malaviya", "Chaitanya", ""], ["Michel", "Paul", ""], ["Oda", "Yusuke", ""], ["Richardson", "Matthew", ""], ["Saphra", "Naomi", ""], ["Swayamdipta", "Swabha", ""], ["Yin", "Pengcheng", ""]]}, {"id": "1701.04024", "submitter": "Mihail Eric", "authors": "Mihail Eric and Christopher D. Manning", "title": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good\n  Performance on Task-Oriented Dialogue", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-oriented dialogue focuses on conversational agents that participate in\nuser-initiated dialogues on domain-specific topics. In contrast to chatbots,\nwhich simply seek to sustain open-ended meaningful discourse, existing\ntask-oriented agents usually explicitly model user intent and belief states.\nThis paper examines bypassing such an explicit representation by depending on a\nlatent neural embedding of state and learning selective attention to dialogue\nhistory together with copying to incorporate relevant prior context. We\ncomplement recent work by showing the effectiveness of simple\nsequence-to-sequence neural architectures with a copy mechanism. Our model\noutperforms more complex memory-augmented models by 7% in per-response\ngeneration and is on par with the current state-of-the-art on DSTC2.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 10:38:17 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 09:31:18 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 22:18:38 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Eric", "Mihail", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1701.04027", "submitter": "Feifei Zhai", "authors": "Feifei Zhai, Saloni Potdar, Bing Xiang, Bowen Zhou", "title": "Neural Models for Sequence Chunking", "comments": "Accepted by AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language understanding (NLU) tasks, such as shallow parsing\n(i.e., text chunking) and semantic slot filling, require the assignment of\nrepresentative labels to the meaningful chunks in a sentence. Most of the\ncurrent deep neural network (DNN) based methods consider these tasks as a\nsequence labeling problem, in which a word, rather than a chunk, is treated as\nthe basic unit for labeling. These chunks are then inferred by the standard IOB\n(Inside-Outside-Beginning) labels. In this paper, we propose an alternative\napproach by investigating the use of DNN for sequence chunking, and propose\nthree neural models so that each chunk can be treated as a complete unit for\nlabeling. Experimental results show that the proposed neural sequence chunking\nmodels can achieve start-of-the-art performance on both the text chunking and\nslot filling tasks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 11:08:28 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Zhai", "Feifei", ""], ["Potdar", "Saloni", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1701.04039", "submitter": "David Graus", "authors": "David Graus, Daan Odijk, Maarten de Rijke", "title": "The Birth of Collective Memories: Analyzing Emerging Entities in Text\n  Streams", "comments": "To appear in JASIST", "journal-ref": null, "doi": "10.1002/asi.24004", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how collective memories are formed online. We do so by tracking\nentities that emerge in public discourse, that is, in online text streams such\nas social media and news streams, before they are incorporated into Wikipedia,\nwhich, we argue, can be viewed as an online place for collective memory. By\ntracking how entities emerge in public discourse, i.e., the temporal patterns\nbetween their first mention in online text streams and subsequent incorporation\ninto collective memory, we gain insights into how the collective remembrance\nprocess happens online. Specifically, we analyze nearly 80,000 entities as they\nemerge in online text streams before they are incorporated into Wikipedia. The\nonline text streams we use for our analysis comprise of social media and news\nstreams, and span over 579 million documents in a timespan of 18 months. We\ndiscover two main emergence patterns: entities that emerge in a \"bursty\"\nfashion, i.e., that appear in public discourse without a precedent, blast into\nactivity and transition into collective memory. Other entities display a\n\"delayed\" pattern, where they appear in public discourse, experience a period\nof inactivity, and then resurface before transitioning into our cultural\ncollective memory.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 13:34:43 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 16:19:18 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Graus", "David", ""], ["Odijk", "Daan", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1701.04056", "submitter": "Bing Liu", "authors": "Bing Liu, Ian Lane", "title": "Dialog Context Language Modeling with Recurrent Neural Networks", "comments": "Accepted for publication at ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose contextual language models that incorporate dialog\nlevel discourse information into language modeling. Previous works on\ncontextual language model treat preceding utterances as a sequence of inputs,\nwithout considering dialog interactions. We design recurrent neural network\n(RNN) based contextual language models that specially track the interactions\nbetween speakers in a dialog. Experiment results on Switchboard Dialog Act\nCorpus show that the proposed model outperforms conventional single turn based\nRNN language model by 3.3% on perplexity. The proposed models also demonstrate\nadvantageous performance over other competitive contextual language models.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 15:10:29 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Liu", "Bing", ""], ["Lane", "Ian", ""]]}, {"id": "1701.04189", "submitter": "Cheng Li", "authors": "Cheng Li, Xiaoxiao Guo and Qiaozhu Mei", "title": "Deep Memory Networks for Attitude Identification", "comments": "Accepted to WSDM'17", "journal-ref": null, "doi": "10.1145/3018661.3018714", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of identifying attitudes towards a given set of entities\nfrom text. Conventionally, this task is decomposed into two separate subtasks:\ntarget detection that identifies whether each entity is mentioned in the text,\neither explicitly or implicitly, and polarity classification that classifies\nthe exact sentiment towards an identified entity (the target) into positive,\nnegative, or neutral.\n  Instead, we show that attitude identification can be solved with an\nend-to-end machine learning architecture, in which the two subtasks are\ninterleaved by a deep memory network. In this way, signals produced in target\ndetection provide clues for polarity classification, and reversely, the\npredicted polarity provides feedback to the identification of targets.\nMoreover, the treatments for the set of targets also influence each other --\nthe learned representations may share the same semantics for some targets but\nvary for others. The proposed deep memory network, the AttNet, outperforms\nmethods that do not consider the interactions between the subtasks or those\namong the targets, including conventional machine learning methods and the\nstate-of-the-art deep learning models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 06:49:01 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Li", "Cheng", ""], ["Guo", "Xiaoxiao", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1701.04290", "submitter": "Nadir Durrani Dr", "authors": "Nadeem Jadoon Khan, Waqas Anwar, Nadir Durrani", "title": "Machine Translation Approaches and Survey for Indian Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present an analysis regarding the performance of the\nstate-of-art Phrase-based Statistical Machine Translation (SMT) on multiple\nIndian languages. We report baseline systems on several language pairs. The\nmotivation of this study is to promote the development of SMT and linguistic\nresources for these language pairs, as the current state-of-the-art is quite\nbleak due to sparse data resources. The success of an SMT system is contingent\non the availability of a large parallel corpus. Such data is necessary to\nreliably estimate translation probabilities. We report the performance of\nbaseline systems translating from Indian languages (Bengali, Guajarati, Hindi,\nMalayalam, Punjabi, Tamil, Telugu and Urdu) into English with average 10%\naccurate results for all the language pairs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 13:55:01 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Khan", "Nadeem Jadoon", ""], ["Anwar", "Waqas", ""], ["Durrani", "Nadir", ""]]}, {"id": "1701.04292", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Piotr Borkowski and Krzysztof Ciesielski and Mieczys{\\l}aw A.\n  K{\\l}opotek", "title": "Semantic classifier approach to document classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new document classification method, bridging\ndiscrepancies (so-called semantic gap) between the training set and the\napplication sets of textual data. We demonstrate its superiority over classical\ntext classification approaches, including traditional classifier ensembles. The\nmethod consists in combining a document categorization technique with a single\nclassifier or a classifier ensemble (SEMCOM algorithm - Committee with Semantic\nCategorizer).\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 14:02:19 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Borkowski", "Piotr", ""], ["Ciesielski", "Krzysztof", ""], ["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1701.04313", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Andrew Rosenberg, Abhinav Sethy, Bhuvana\n  Ramabhadran, Brian Kingsbury", "title": "End-to-End ASR-free Keyword Search from Speech", "comments": "Published in the IEEE 2017 International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2017), scheduled for 5-9 March 2017 in\n  New Orleans, Louisiana, USA", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2759726", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 15:05:39 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Rosenberg", "Andrew", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Kingsbury", "Brian", ""]]}, {"id": "1701.04653", "submitter": "Marzieh Saeidi Marzieh Saeidi", "authors": "Marzieh Saeidi, Alessandro Venerandi, Licia Capra and Sebastian Riedel", "title": "Community Question Answering Platforms vs. Twitter for Predicting\n  Characteristics of Urban Neighbourhoods", "comments": "Submitted to ICWSM2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate whether text from a Community Question\nAnswering (QA) platform can be used to predict and describe real-world\nattributes. We experiment with predicting a wide range of 62 demographic\nattributes for neighbourhoods of London. We use the text from QA platform of\nYahoo! Answers and compare our results to the ones obtained from Twitter\nmicroblogs. Outcomes show that the correlation between the predicted\ndemographic attributes using text from Yahoo! Answers discussions and the\nobserved demographic attributes can reach an average Pearson correlation\ncoefficient of \\r{ho} = 0.54, slightly higher than the predictions obtained\nusing Twitter data. Our qualitative analysis indicates that there is semantic\nrelatedness between the highest correlated terms extracted from both datasets\nand their relative demographic attributes. Furthermore, the correlations\nhighlight the different natures of the information contained in Yahoo! Answers\nand Twitter. While the former seems to offer a more encyclopedic content, the\nlatter provides information related to the current sociocultural aspects or\nphenomena.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 12:53:19 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Saeidi", "Marzieh", ""], ["Venerandi", "Alessandro", ""], ["Capra", "Licia", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1701.05011", "submitter": "Eug\\'enio Ribeiro", "authors": "Eug\\'enio Ribeiro, Fernando Batista, Isabel Trancoso, Jos\\'e Lopes,\n  Ricardo Ribeiro, and David Martins de Matos", "title": "Assessing User Expertise in Spoken Dialog System Interactions", "comments": "10 pages", "journal-ref": "Advances in Speech and Language Technologies for Iberian\n  Languages: Third International Conference, IberSPEECH 2016, Lisbon, Portugal,\n  November 23-25, pp. 245-254", "doi": "10.1007/978-3-319-49169-1_24", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the level of expertise of its users is important for a system\nsince it can lead to a better interaction through adaptation techniques.\nFurthermore, this information can be used in offline processes of root cause\nanalysis. However, not much effort has been put into automatically identifying\nthe level of expertise of an user, especially in dialog-based interactions. In\nthis paper we present an approach based on a specific set of task related\nfeatures. Based on the distribution of the features among the two classes -\nNovice and Expert - we used Random Forests as a classification approach.\nFurthermore, we used a Support Vector Machine classifier, in order to perform a\nresult comparison. By applying these approaches on data from a real system,\nLet's Go, we obtained preliminary results that we consider positive, given the\ndifficulty of the task and the lack of competing approaches for comparison.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 11:10:59 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Ribeiro", "Eug\u00e9nio", ""], ["Batista", "Fernando", ""], ["Trancoso", "Isabel", ""], ["Lopes", "Jos\u00e9", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1701.05311", "submitter": "Valentina Franzoni", "authors": "Valentina Franzoni, Yuanxi Li, Clement H.C.Leung and Alfredo Milani", "title": "Semantic Evolutionary Concept Distances for Effective Information\n  Retrieval in Query Expansion", "comments": "author's copy of publication in NLCS ICCSA 2013 proceedings:\n  Collective Evolutionary Concept Distance Based Query Expansion for Effective\n  Web Document Retrieval", "journal-ref": "Chapter Computational Science and Its Applications, ICCSA 2013,\n  Volume 7974 of the series Lecture Notes in Computer Science, pp 657-672", "doi": "10.1007/978-3-642-39649-6_47", "report-no": null, "categories": "cs.IR cs.AI cs.CL math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work several semantic approaches to concept-based query expansion and\nreranking schemes are studied and compared with different ontology-based\nexpansion methods in web document search and retrieval. In particular, we focus\non concept-based query expansion schemes, where, in order to effectively\nincrease the precision of web document retrieval and to decrease the users\nbrowsing time, the main goal is to quickly provide users with the most suitable\nquery expansion. Two key tasks for query expansion in web document retrieval\nare to find the expansion candidates, as the closest concepts in web document\ndomain, and to rank the expanded queries properly. The approach we propose aims\nat improving the expansion phase for better web document retrieval and\nprecision. The basic idea is to measure the distance between candidate concepts\nusing the PMING distance, a collaborative semantic proximity measure, i.e. a\nmeasure which can be computed by using statistical results from web search\nengine. Experiments show that the proposed technique can provide users with\nmore satisfying expansion results and improve the quality of web document\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 06:38:33 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Franzoni", "Valentina", ""], ["Li", "Yuanxi", ""], ["Leung", "Clement H. C.", ""], ["Milani", "Alfredo", ""]]}, {"id": "1701.05334", "submitter": "S.M. Riazul Islam PhD", "authors": "Farman Ali, D. Kwak, Pervez Khan, S.M. Riazul Islam, K.H. Kim, and\n  K.S. Kwak", "title": "Fuzzy Ontology-Based Sentiment Analysis of Transportation and City\n  Feature Reviews for Safe Traveling", "comments": "24 pages, 7 figures, Transportation Research Part C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic congestion is rapidly increasing in urban areas, particularly in mega\ncities. To date, there exist a few sensor network based systems to address this\nproblem. However, these techniques are not suitable enough in terms of\nmonitoring an entire transportation system and delivering emergency services\nwhen needed. These techniques require real-time data and intelligent ways to\nquickly determine traffic activity from useful information. In addition, these\nexisting systems and websites on city transportation and travel rely on rating\nscores for different factors (e.g., safety, low crime rate, cleanliness, etc.).\nThese rating scores are not efficient enough to deliver precise information,\nwhereas reviews or tweets are significant, because they help travelers and\ntransportation administrators to know about each aspect of the city. However,\nit is difficult for travelers to read, and for transportation systems to\nprocess, all reviews and tweets to obtain expressive sentiments regarding the\nneeds of the city. The optimum solution for this kind of problem is analyzing\nthe information available on social network platforms and performing sentiment\nanalysis. On the other hand, crisp ontology-based frameworks cannot extract\nblurred information from tweets and reviews; therefore, they produce inadequate\nresults. In this regard, this paper proposes fuzzy ontology-based sentiment\nanalysis and SWRL rule-based decision-making to monitor transportation\nactivities and to make a city- feature polarity map for travelers. This system\nretrieves reviews and tweets related to city features and transportation\nactivities. The feature opinions are extracted from these retrieved data, and\nthen fuzzy ontology is used to determine the transportation and city-feature\npolarity. A fuzzy ontology and an intelligent system prototype are developed by\nusing Prot\\'eg\\'e OWL and Java, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 08:50:37 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Ali", "Farman", ""], ["Kwak", "D.", ""], ["Khan", "Pervez", ""], ["Islam", "S. M. Riazul", ""], ["Kim", "K. H.", ""], ["Kwak", "K. S.", ""]]}, {"id": "1701.05343", "submitter": "Zhongyu Wei", "authors": "Zhongyu Wei, Chen Li and Yang Liu", "title": "A Joint Framework for Argumentative Text Analysis Incorporating Domain\n  Knowledge", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For argumentation mining, there are several sub-tasks such as argumentation\ncomponent type classification, relation classification. Existing research tends\nto solve such sub-tasks separately, but ignore the close relation between them.\nIn this paper, we present a joint framework incorporating logical relation\nbetween sub-tasks to improve the performance of argumentation structure\ngeneration. We design an objective function to combine the predictions from\nindividual models for each sub-task and solve the problem with some constraints\nconstructed from background knowledge. We evaluate our proposed model on two\npublic corpora and the experiment results show that our model can outperform\nthe baseline that uses a separate model significantly for each sub-task. Our\nmodel also shows advantages on component-related sub-tasks compared to a\nstate-of-the-art joint model based on the evidence graph.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 09:32:08 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Wei", "Zhongyu", ""], ["Li", "Chen", ""], ["Liu", "Yang", ""]]}, {"id": "1701.05574", "submitter": "Diptesh Kanojia", "authors": "Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey and Pushpak\n  Bhattacharyya", "title": "Harnessing Cognitive Features for Sarcasm Detection", "comments": "The 54th Annual Meeting of The Association for Computational\n  Linguistics (ACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel mechanism for enriching the feature vector,\nfor the task of sarcasm detection, with cognitive features extracted from\neye-movement patterns of human readers. Sarcasm detection has been a\nchallenging research problem, and its importance for NLP applications such as\nreview summarization, dialog systems and sentiment analysis is well recognized.\nSarcasm can often be traced to incongruity that becomes apparent as the full\nsentence unfolds. This presence of incongruity- implicit or explicit- affects\nthe way readers eyes move through the text. We observe the difference in the\nbehaviour of the eye, while reading sarcastic and non sarcastic sentences.\nMotivated by his observation, we augment traditional linguistic and stylistic\nfeatures for sarcasm detection with the cognitive features obtained from\nreaders eye movement data. We perform statistical classification using the\nenhanced feature set so obtained. The augmented cognitive features improve\nsarcasm detection by 3.7% (in terms of F-score), over the performance of the\nbest reported system.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 19:32:06 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Mishra", "Abhijit", ""], ["Kanojia", "Diptesh", ""], ["Nagar", "Seema", ""], ["Dey", "Kuntal", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1701.05581", "submitter": "Diptesh Kanojia", "authors": "Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey and Pushpak\n  Bhattacharyya", "title": "Leveraging Cognitive Features for Sentiment Analysis", "comments": "The SIGNLL Conference on Computational Natural Language Learning\n  (CoNLL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiments expressed in user-generated short text and sentences are nuanced\nby subtleties at lexical, syntactic, semantic and pragmatic levels. To address\nthis, we propose to augment traditional features used for sentiment analysis\nand sarcasm detection, with cognitive features derived from the eye-movement\npatterns of readers. Statistical classification using our enhanced feature set\nimproves the performance (F-score) of polarity detection by a maximum of 3.7%\nand 9.3% on two datasets, over the systems that use only traditional features.\nWe perform feature significance analysis, and experiment on a held-out dataset,\nshowing that cognitive features indeed empower sentiment analyzers to handle\ncomplex constructs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 19:58:26 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Mishra", "Abhijit", ""], ["Kanojia", "Diptesh", ""], ["Nagar", "Seema", ""], ["Dey", "Kuntal", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1701.05625", "submitter": "Saeedeh Shekarpour", "authors": "Saeedeh Shekarpour, Faisal Alshargi, Valerie Shalin, Krishnaprasad\n  Thirunarayan, Amit P. Sheth", "title": "CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the general analysis of named entities has received substantial\nresearch attention on unstructured as well as structured data, the analysis of\nrelations among named entities has received limited focus. In fact, a review of\nthe literature revealed a deficiency in research on the abstract\nconceptualization required to organize relations. We believe that such an\nabstract conceptualization can benefit various communities and applications\nsuch as natural language processing, information extraction, machine learning,\nand ontology engineering. In this paper, we present Comprehensive EVent\nOntology (CEVO), built on Levin's conceptual hierarchy of English verbs that\ncategorizes verbs with shared meaning, and syntactic behavior. We present the\nfundamental concepts and requirements for this ontology. Furthermore, we\npresent three use cases employing the CEVO ontology on annotation tasks: (i)\nannotating relations in plain text, (ii) annotating ontological properties, and\n(iii) linking textual relations to ontological properties. These use-cases\ndemonstrate the benefits of using CEVO for annotation: (i) annotating English\nverbs from an abstract conceptualization, (ii) playing the role of an upper\nontology for organizing ontological properties, and (iii) facilitating the\nannotation of text relations using any underlying vocabulary. This resource is\navailable at https://shekarpour.github.io/cevo.io/ using https://w3id.org/cevo\nnamespace.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 22:14:25 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 21:07:07 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Shekarpour", "Saeedeh", ""], ["Alshargi", "Faisal", ""], ["Shalin", "Valerie", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Sheth", "Amit P.", ""]]}, {"id": "1701.05847", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Zuwei Li, Maja Pantic", "title": "End-To-End Visual Speech Recognition With LSTMs", "comments": "Accepted for publication, ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional visual speech recognition systems consist of two stages, feature\nextraction and classification. Recently, several deep learning approaches have\nbeen presented which automatically extract features from the mouth images and\naim to replace the feature extraction stage. However, research on joint\nlearning of features and classification is very limited. In this work, we\npresent an end-to-end visual speech recognition system based on Long-Short\nMemory (LSTM) networks. To the best of our knowledge, this is the first model\nwhich simultaneously learns to extract features directly from the pixels and\nperform classification and also achieves state-of-the-art performance in visual\nspeech classification. The model consists of two streams which extract features\ndirectly from the mouth and difference images, respectively. The temporal\ndynamics in each stream are modelled by an LSTM and the fusion of the two\nstreams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement\nof 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the\nCUAVE database when compared with other methods which use a similar visual\nfront-end.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 16:36:09 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Petridis", "Stavros", ""], ["Li", "Zuwei", ""], ["Pantic", "Maja", ""]]}, {"id": "1701.06233", "submitter": "Tianran Hu", "authors": "Tianran Hu, Haoyuan Xiao, Thuy-vy Thi Nguyen, Jiebo Luo", "title": "What the Language You Tweet Says About Your Occupation", "comments": "Published at the 10th International AAAI Conference on Web and Social\n  Media (ICWSM-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many aspects of people's lives are proven to be deeply connected to their\njobs. In this paper, we first investigate the distinct characteristics of major\noccupation categories based on tweets. From multiple social media platforms, we\ngather several types of user information. From users' LinkedIn webpages, we\nlearn their proficiencies. To overcome the ambiguity of self-reported\ninformation, a soft clustering approach is applied to extract occupations from\ncrowd-sourced data. Eight job categories are extracted, including Marketing,\nAdministrator, Start-up, Editor, Software Engineer, Public Relation, Office\nClerk, and Designer. Meanwhile, users' posts on Twitter provide cues for\nunderstanding their linguistic styles, interests, and personalities. Our\nresults suggest that people of different jobs have unique tendencies in certain\nlanguage styles and interests. Our results also clearly reveal distinctive\nlevels in terms of Big Five Traits for different jobs. Finally, a classifier is\nbuilt to predict job types based on the features extracted from tweets. A high\naccuracy indicates a strong discrimination power of language features for job\nprediction task.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 23:03:11 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Hu", "Tianran", ""], ["Xiao", "Haoyuan", ""], ["Nguyen", "Thuy-vy Thi", ""], ["Luo", "Jiebo", ""]]}, {"id": "1701.06247", "submitter": "Hongjie Shi", "authors": "Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi Yamagami, Noriaki\n  Horii", "title": "A Multichannel Convolutional Neural Network For Cross-language Dialog\n  State Tracking", "comments": "Copyright 2016 IEEE. Published in the 2016 IEEE Workshop on Spoken\n  Language Technology (SLT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fifth Dialog State Tracking Challenge (DSTC5) introduces a new\ncross-language dialog state tracking scenario, where the participants are asked\nto build their trackers based on the English training corpus, while evaluating\nthem with the unlabeled Chinese corpus. Although the computer-generated\ntranslations for both English and Chinese corpus are provided in the dataset,\nthese translations contain errors and careless use of them can easily hurt the\nperformance of the built trackers. To address this problem, we propose a\nmultichannel Convolutional Neural Networks (CNN) architecture, in which we\ntreat English and Chinese language as different input channels of one single\nCNN model. In the evaluation of DSTC5, we found that such multichannel\narchitecture can effectively improve the robustness against translation errors.\nAdditionally, our method for DSTC5 is purely machine learning based and\nrequires no prior knowledge about the target language. We consider this a\ndesirable property for building a tracker in the cross-language context, as not\nevery developer will be familiar with both languages.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 01:36:10 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Shi", "Hongjie", ""], ["Ushio", "Takashi", ""], ["Endo", "Mitsuru", ""], ["Yamagami", "Katsuyoshi", ""], ["Horii", "Noriaki", ""]]}, {"id": "1701.06279", "submitter": "Patrick Ng", "authors": "Patrick Ng", "title": "dna2vec: Consistent vector representations of variable-length k-mers", "comments": "10 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ubiquitous representation of long DNA sequence is dividing it into\nshorter k-mer components. Unfortunately, the straightforward vector encoding of\nk-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse\nyet, the distance between any pair of one-hot vectors is equidistant. This is\nparticularly problematic when applying the latest machine learning algorithms\nto solve problems in biological sequence analysis. In this paper, we propose a\nnovel method to train distributed representations of variable-length k-mers.\nOur method is based on the popular word embedding model word2vec, which is\ntrained on a shallow two-layer neural network. Our experiments provide evidence\nthat the summing of dna2vec vectors is akin to nucleotides concatenation. We\nalso demonstrate that there is correlation between Needleman-Wunsch similarity\nscore and cosine similarity of dna2vec vectors.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 07:21:43 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ng", "Patrick", ""]]}, {"id": "1701.06521", "submitter": "Iacer Calixto", "authors": "Iacer Calixto and Qun Liu and Nick Campbell", "title": "Incorporating Global Visual Features into Attention-Based Neural Machine\n  Translation", "comments": "8 pages (11 including references), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce multi-modal, attention-based neural machine translation (NMT)\nmodels which incorporate visual features into different parts of both the\nencoder and the decoder. We utilise global image features extracted using a\npre-trained convolutional neural network and incorporate them (i) as words in\nthe source sentence, (ii) to initialise the encoder hidden state, and (iii) as\nadditional data to initialise the decoder hidden state. In our experiments, we\nevaluate how these different strategies to incorporate global image features\ncompare and which ones perform best. We also study the impact that adding\nsynthetic multi-modal, multilingual data brings and find that the additional\ndata have a positive impact on multi-modal models. We report new\nstate-of-the-art results and our best models also significantly improve on a\ncomparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k\ndata set according to all metrics evaluated. To the best of our knowledge, it\nis the first time a purely neural model significantly improves over a PBSMT\nmodel on all metrics evaluated on this data set.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 17:43:23 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Calixto", "Iacer", ""], ["Liu", "Qun", ""], ["Campbell", "Nick", ""]]}, {"id": "1701.06538", "submitter": "Noam Shazeer", "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\n  Le, Geoffrey Hinton, Jeff Dean", "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:10:00 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Shazeer", "Noam", ""], ["Mirhoseini", "Azalia", ""], ["Maziarz", "Krzysztof", ""], ["Davis", "Andy", ""], ["Le", "Quoc", ""], ["Hinton", "Geoffrey", ""], ["Dean", "Jeff", ""]]}, {"id": "1701.06547", "submitter": "Jiwei Li", "authors": "Jiwei Li, Will Monroe, Tianlin Shi, S\\'ebastien Jean, Alan Ritter and\n  Dan Jurafsky", "title": "Adversarial Learning for Neural Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, drawing intuition from the Turing test, we propose using\nadversarial training for open-domain dialogue generation: the system is trained\nto produce sequences that are indistinguishable from human-generated dialogue\nutterances. We cast the task as a reinforcement learning (RL) problem where we\njointly train two systems, a generative model to produce response sequences,\nand a discriminator---analagous to the human evaluator in the Turing test--- to\ndistinguish between the human-generated dialogues and the machine-generated\nones. The outputs from the discriminator are then used as rewards for the\ngenerative model, pushing the system to generate dialogues that mostly resemble\nhuman dialogues.\n  In addition to adversarial training we describe a model for adversarial {\\em\nevaluation} that uses success in fooling an adversary as a dialogue evaluation\nmetric, while avoiding a number of potential pitfalls. Experimental results on\nseveral metrics, including adversarial evaluation, demonstrate that the\nadversarially-trained system generates higher-quality responses than previous\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:32:27 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 01:42:13 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 14:02:56 GMT"}, {"version": "v4", "created": "Wed, 22 Feb 2017 08:36:59 GMT"}, {"version": "v5", "created": "Sun, 24 Sep 2017 01:44:39 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Jiwei", ""], ["Monroe", "Will", ""], ["Shi", "Tianlin", ""], ["Jean", "S\u00e9bastien", ""], ["Ritter", "Alan", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1701.06549", "submitter": "Jiwei Li", "authors": "Jiwei Li, Will Monroe and Dan Jurafsky", "title": "Learning to Decode for Future Success", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple, general strategy to manipulate the behavior of a\nneural decoder that enables it to generate outputs that have specific\nproperties of interest (e.g., sequences of a pre-specified length). The model\ncan be thought of as a simple version of the actor-critic model that uses an\ninterpolation of the actor (the MLE-based token generation policy) and the\ncritic (a value function that estimates the future values of the desired\nproperty) for decision making. We demonstrate that the approach is able to\nincorporate a variety of properties that cannot be handled by standard neural\nsequence decoders, such as sequence length and backward probability\n(probability of sources given targets), in addition to yielding consistent\nimprovements in abstractive summarization and machine translation when the\nproperty to be optimized is BLEU or ROUGE scores.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 18:36:37 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 21:11:31 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Li", "Jiwei", ""], ["Monroe", "Will", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1701.07149", "submitter": "Chen Xing", "authors": "Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma", "title": "Hierarchical Recurrent Attention Network for Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-turn response generation in chatbots where a response is\ngenerated according to a conversation context. Existing work has modeled the\nhierarchy of the context, but does not pay enough attention to the fact that\nwords and utterances in the context are differentially important. As a result,\nthey may lose important information in context and generate irrelevant\nresponses. We propose a hierarchical recurrent attention network (HRAN) to\nmodel both aspects in a unified framework. In HRAN, a hierarchical attention\nmechanism attends to important parts within and among utterances with word\nlevel attention and utterance level attention respectively. With the word level\nattention, hidden vectors of a word level encoder are synthesized as utterance\nvectors and fed to an utterance level encoder to construct hidden\nrepresentations of the context. The hidden vectors of the context are then\nprocessed by the utterance level attention and formed as context vectors for\ndecoding the response. Empirical studies on both automatic evaluation and human\njudgment show that HRAN can significantly outperform state-of-the-art models\nfor multi-turn response generation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 03:04:31 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Xing", "Chen", ""], ["Wu", "Wei", ""], ["Wu", "Yu", ""], ["Zhou", "Ming", ""], ["Huang", "Yalou", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1701.07481", "submitter": "David Harwath", "authors": "David Harwath and James R. Glass", "title": "Learning Word-Like Units from Joint Audio-Visual Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of images and spoken audio captions, we present a method\nfor discovering word-like acoustic units in the continuous speech signal and\ngrounding them to semantically relevant image regions. For example, our model\nis able to detect spoken instances of the word 'lighthouse' within an utterance\nand associate them with image regions containing lighthouses. We do not use any\nform of conventional automatic speech recognition, nor do we use any text\ntranscriptions or conventional linguistic annotations. Our model effectively\nimplements a form of spoken language acquisition, in which the computer learns\nnot only to recognize word categories by sound, but also to enrich the words it\nlearns with semantics by grounding them in images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:40:56 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 15:15:41 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 22:10:25 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Harwath", "David", ""], ["Glass", "James R.", ""]]}, {"id": "1701.07795", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Hetunandan Kamisetty and Eric Ringger and Charlie\n  Clarke", "title": "Match-Tensor: a Deep Relevance Model for Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Deep Neural Networks for ranking in search engines may\nobviate the need for the extensive feature engineering common to current\nlearning-to-rank methods. However, we show that combining simple relevance\nmatching features like BM25 with existing Deep Neural Net models often\nsubstantially improves the accuracy of these models, indicating that they do\nnot capture essential local relevance matching signals. We describe a novel\ndeep Recurrent Neural Net-based model that we call Match-Tensor. The\narchitecture of the Match-Tensor model simultaneously accounts for both local\nrelevance matching and global topicality signals allowing for a rich interplay\nbetween them when computing the relevance of a document to a query. On a large\nheld-out test set consisting of social media documents, we demonstrate not only\nthat Match-Tensor outperforms BM25 and other classes of DNNs but also that it\nlargely subsumes signals present in these models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 17:59:38 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jaech", "Aaron", ""], ["Kamisetty", "Hetunandan", ""], ["Ringger", "Eric", ""], ["Clarke", "Charlie", ""]]}, {"id": "1701.07880", "submitter": "D\\'aivd M\\'ark Nemeskey", "authors": "D\\'avid M\\'ark Nemeskey", "title": "emLam -- a Hungarian Language Modeling baseline", "comments": "Additional resources: - the emLam repository:\n  https://github.com/DavidNemeskey/emLam - the emLam corpus:\n  http://hlt.bme.hu/en/resources/emLam", "journal-ref": "In Proceedings of the 13th Conference on Hungarian Computational\n  Linguistics (MSZNY), pp. 91-102. Szeged, 2017", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper aims to make up for the lack of documented baselines for Hungarian\nlanguage modeling. Various approaches are evaluated on three publicly available\nHungarian corpora. Perplexity values comparable to models of similar-sized\nEnglish corpora are reported. A new, freely downloadable Hungar- ian benchmark\ncorpus is introduced.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 21:18:32 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Nemeskey", "D\u00e1vid M\u00e1rk", ""]]}, {"id": "1701.07955", "submitter": "Saiful Islam Md", "authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem and Md. Saiful Islam", "title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 06:30:21 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Nirob", "Syed Mehedi Hasan", ""], ["Nayeem", "Md. Kazi", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1701.08071", "submitter": "Vladimir Chernykh", "authors": "Vladimir Chernykh and Pavel Prikhodko", "title": "Emotion Recognition From Speech With Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the task of emotion recognition from speech is considered.\nProposed approach uses deep recurrent neural network trained on a sequence of\nacoustic features calculated over small speech intervals. At the same time\nspecial probabilistic-nature CTC loss function allows to consider long\nutterances containing both emotional and neutral parts. The effectiveness of\nsuch an approach is shown in two ways. Firstly, the comparison with recent\nadvances in this field is carried out. Secondly, human performance on the same\ntask is measured. Both criteria show the high quality of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 14:50:36 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 16:12:22 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Chernykh", "Vladimir", ""], ["Prikhodko", "Pavel", ""]]}, {"id": "1701.08118", "submitter": "Bj\\\"orn Ross", "authors": "Bj\\\"orn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera,\n  Nils Kurowsky, Michael Wojatzki", "title": "Measuring the Reliability of Hate Speech Annotations: The Case of the\n  European Refugee Crisis", "comments": null, "journal-ref": "Proceedings of NLP4CMC III: 3rd Workshop on Natural Language\n  Processing for Computer-Mediated Communication (Bochum), Bochumer\n  Linguistische Arbeitsberichte, vol. 17, sep 2016, pp. 6-9", "doi": "10.17185/duepublico/42132", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some users of social media are spreading racist, sexist, and otherwise\nhateful content. For the purpose of training a hate speech detection system,\nthe reliability of the annotations is crucial, but there is no universally\nagreed-upon definition. We collected potentially hateful messages and asked two\ngroups of internet users to determine whether they were hate speech or not,\nwhether they should be banned or not and to rate their degree of offensiveness.\nOne of the groups was shown a definition prior to completing the survey. We\naimed to assess whether hate speech can be annotated reliably, and the extent\nto which existing definitions are in accordance with subjective ratings. Our\nresults indicate that showing users a definition caused them to partially align\ntheir own opinion with the definition but did not improve reliability, which\nwas very low overall. We conclude that the presence of hate speech should\nperhaps not be considered a binary yes-or-no decision, and raters need more\ndetailed instructions for the annotation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 17:09:07 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Ross", "Bj\u00f6rn", ""], ["Rist", "Michael", ""], ["Carbonell", "Guillermo", ""], ["Cabrera", "Benjamin", ""], ["Kurowsky", "Nils", ""], ["Wojatzki", "Michael", ""]]}, {"id": "1701.08156", "submitter": "Md Saiful Islam", "authors": "Sadia Tasnim Swarna, Shamim Ehsan, Md. Saiful Islam and Marium E\n  Jannat", "title": "A Comprehensive Survey on Bengali Phoneme Recognition", "comments": "7 pages, reference added in phoneme recognition methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:38:47 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 18:17:00 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Swarna", "Sadia Tasnim", ""], ["Ehsan", "Shamim", ""], ["Islam", "Md. Saiful", ""], ["Jannat", "Marium E", ""]]}, {"id": "1701.08198", "submitter": "Anjuli Kannan", "authors": "Anjuli Kannan and Oriol Vinyals", "title": "Adversarial Evaluation of Dialogue Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent application of RNN encoder-decoder models has resulted in\nsubstantial progress in fully data-driven dialogue systems, but evaluation\nremains a challenge. An adversarial loss could be a way to directly evaluate\nthe extent to which generated dialogue responses sound like they came from a\nhuman. This could reduce the need for human evaluation, while more directly\nevaluating on a generative task. In this work, we investigate this idea by\ntraining an RNN to discriminate a dialogue model's samples from human-generated\nsamples. Although we find some evidence this setup could be viable, we also\nnote that many issues remain in its practical application. We discuss both\naspects and conclude that future work is warranted.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 21:28:57 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Kannan", "Anjuli", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1701.08229", "submitter": "Danielle Mowery PhD", "authors": "Danielle Mowery and Craig Bryan and Mike Conway", "title": "Feature Studies to Inform the Classification of Depressive Symptoms from\n  Twitter Data for Population Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utility of Twitter data as a medium to support population-level mental\nhealth monitoring is not well understood. In an effort to better understand the\npredictive power of supervised machine learning classifiers and the influence\nof feature sets for efficiently classifying depression-related tweets on a\nlarge-scale, we conducted two feature study experiments. In the first\nexperiment, we assessed the contribution of feature groups such as lexical\ninformation (e.g., unigrams) and emotions (e.g., strongly negative) using a\nfeature ablation study. In the second experiment, we determined the percentile\nof top ranked features that produced the optimal classification performance by\napplying a three-step feature elimination approach. In the first experiment, we\nobserved that lexical features are critical for identifying depressive\nsymptoms, specifically for depressed mood (-35 points) and for disturbed sleep\n(-43 points). In the second experiment, we observed that the optimal F1-score\nperformance of top ranked features in percentiles variably ranged across\nclasses e.g., fatigue or loss of energy (5th percentile, 288 features) to\ndepressed mood (55th percentile, 3,168 features) suggesting there is no\nconsistent count of features for predicting depressive-related tweets. We\nconclude that simple lexical features and reduced feature sets can produce\ncomparable results to larger feature sets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 00:32:40 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mowery", "Danielle", ""], ["Bryan", "Craig", ""], ["Conway", "Mike", ""]]}, {"id": "1701.08251", "submitter": "Nasrin Mostafazadeh", "authors": "Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley,\n  Jianfeng Gao, Georgios P. Spithourakis, Lucy Vanderwende", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question\n  and Response Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of image sharing on social media and the engagement it creates\nbetween users reflects the important role that visual context plays in everyday\nconversations. We present a novel task, Image-Grounded Conversations (IGC), in\nwhich natural-sounding conversations are generated about a shared image. To\nbenchmark progress, we introduce a new multiple-reference dataset of\ncrowd-sourced, event-centric conversations on images. IGC falls on the\ncontinuum between chit-chat and goal-directed conversation models, where visual\ngrounding constrains the topic of conversation to event-driven utterances.\nExperiments with models trained on social media data show that the combination\nof visual and textual context enhances the quality of generated conversational\nturns. In human evaluation, the gap between human performance and that of both\nneural and retrieval architectures suggests that multi-modal IGC presents an\ninteresting challenge for dialogue research.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 05:06:11 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 00:36:35 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Mostafazadeh", "Nasrin", ""], ["Brockett", "Chris", ""], ["Dolan", "Bill", ""], ["Galley", "Michel", ""], ["Gao", "Jianfeng", ""], ["Spithourakis", "Georgios P.", ""], ["Vanderwende", "Lucy", ""]]}, {"id": "1701.08269", "submitter": "Rui Liu", "authors": "Rui Liu, Xiaoli Zhang", "title": "Systems of natural-language-facilitated human-robot cooperation: A\n  review", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural-language-facilitated human-robot cooperation (NLC), in which natural\nlanguage (NL) is used to share knowledge between a human and a robot for\nconducting intuitive human-robot cooperation (HRC), is continuously developing\nin the recent decade. Currently, NLC is used in several robotic domains such as\nmanufacturing, daily assistance and health caregiving. It is necessary to\nsummarize current NLC-based robotic systems and discuss the future developing\ntrends, providing helpful information for future NLC research. In this review,\nwe first analyzed the driving forces behind the NLC research. Regarding to a\nrobot s cognition level during the cooperation, the NLC implementations then\nwere categorized into four types {NL-based control, NL-based robot training,\nNL-based task execution, NL-based social companion} for comparison and\ndiscussion. Last based on our perspective and comprehensive paper review, the\nfuture research trends were discussed.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 08:32:35 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 20:27:17 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Liu", "Rui", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "1701.08303", "submitter": "Sunil Sahu", "authors": "Sunil Kumar Sahu, Ashish Anand", "title": "Drug-Drug Interaction Extraction from Biomedical Text Using Long Short\n  Term Memory Network", "comments": "Under review to the Journal of Biomedical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous administration of multiple drugs can have synergistic or\nantagonistic effects as one drug can affect activities of other drugs.\nSynergistic effects lead to improved therapeutic outcomes, whereas,\nantagonistic effects can be life-threatening, may lead to increased healthcare\ncost, or may even cause death. Thus identification of unknown drug-drug\ninteraction (DDI) is an important concern for efficient and effective\nhealthcare. Although multiple resources for DDI exist, they are often unable to\nkeep pace with rich amount of information available in fast growing biomedical\ntexts. Most existing methods model DDI extraction from text as a classification\nproblem and mainly rely on handcrafted features. Some of these features further\ndepend on domain specific tools. Recently neural network models using latent\nfeatures have been shown to give similar or better performance than the other\nexisting models dependent on handcrafted features. In this paper, we present\nthree models namely, {\\it B-LSTM}, {\\it AB-LSTM} and {\\it Joint AB-LSTM} based\non long short-term memory (LSTM) network. All three models utilize word and\nposition embedding as latent features and thus do not rely on explicit feature\nengineering. Further use of bidirectional long short-term memory (Bi-LSTM)\nnetworks allow implicit feature extraction from the whole sentence. The two\nmodels, {\\it AB-LSTM} and {\\it Joint AB-LSTM} also use attentive pooling in the\noutput of Bi-LSTM layer to assign weights to features. Our experimental results\non the SemEval-2013 DDI extraction dataset show that the {\\it Joint AB-LSTM}\nmodel outperforms all the existing methods, including those relying on\nhandcrafted features. The other two proposed LSTM models also perform\ncompetitively with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 17:04:21 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 12:56:03 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Sahu", "Sunil Kumar", ""], ["Anand", "Ashish", ""]]}, {"id": "1701.08339", "submitter": "Ebrahim Ansari", "authors": "Ebrahim Ansari, M.H. Sadreddini, Mostafa Sheikhalishahi, Richard\n  Wallace, Fatemeh Alimardani", "title": "Using English as Pivot to Extract Persian-Italian Parallel Sentences\n  from Non-Parallel Corpora", "comments": "30 pages, Accepted to be published in \"Applications of Comparable\n  Corpora\", Berlin: Language Science Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of a statistical machine translation system (SMT) is very\ndependent upon the amount of parallel corpus used in the training phase. For\nlow-resource language pairs there are not enough parallel corpora to build an\naccurate SMT. In this paper, a novel approach is presented to extract bilingual\nPersian-Italian parallel sentences from a non-parallel (comparable) corpus. In\nthis study, English is used as the pivot language to compute the matching\nscores between source and target sentences and candidate selection phase.\nAdditionally, a new monolingual sentence similarity metric, Normalized Google\nDistance (NGD) is proposed to improve the matching process. Moreover, some\nextensions of the baseline system are applied to improve the quality of\nextracted sentences measured with BLEU. Experimental results show that using\nthe new pivot based extraction can increase the quality of bilingual corpus\nsignificantly and consequently improves the performance of the Persian-Italian\nSMT system.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 00:22:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ansari", "Ebrahim", ""], ["Sadreddini", "M. H.", ""], ["Sheikhalishahi", "Mostafa", ""], ["Wallace", "Richard", ""], ["Alimardani", "Fatemeh", ""]]}, {"id": "1701.08340", "submitter": "Ebrahim Ansari", "authors": "Ebrahim Ansari, M.H. Sadreddini, Lucio Grandinetti, Mahsa Radinmehr,\n  Ziba Khosravan, and Mehdi Sheikhalishahi", "title": "Extracting Bilingual Persian Italian Lexicon from Comparable Corpora\n  Using Different Types of Seed Dictionaries", "comments": "16 pages, accepted to be published in \"Applications of Comparable\n  Corpora\", Berlin: Language Science Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilingual dictionaries are very important in various fields of natural\nlanguage processing. In recent years, research on extracting new bilingual\nlexicons from non-parallel (comparable) corpora have been proposed. Almost all\nuse a small existing dictionary or other resources to make an initial list\ncalled the \"seed dictionary\". In this paper, we discuss the use of different\ntypes of dictionaries as the initial starting list for creating a bilingual\nPersian-Italian lexicon from a comparable corpus. Our experiments apply\nstate-of-the-art techniques on three different seed dictionaries; an existing\ndictionary, a dictionary created with pivot-based schema, and a dictionary\nextracted from a small Persian-Italian parallel text. The interesting challenge\nof our approach is to find a way to combine different dictionaries together in\norder to produce a better and more accurate lexicon. In order to combine seed\ndictionaries, we propose two different combination models and examine the\neffect of our novel combination models on various comparable corpora that have\ndiffering degrees of comparability. We conclude with a proposal for a new\nweighting system to improve the extracted lexicon. The experimental results\nproduced by our implementation show the efficiency of our proposed models.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 00:28:20 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 10:14:22 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Ansari", "Ebrahim", ""], ["Sadreddini", "M. H.", ""], ["Grandinetti", "Lucio", ""], ["Radinmehr", "Mahsa", ""], ["Khosravan", "Ziba", ""], ["Sheikhalishahi", "Mehdi", ""]]}, {"id": "1701.08533", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Masoud Kiaeeha, Shahram Khadivi, Saeed Shiry\n  Ghidary", "title": "Graph-Based Semi-Supervised Conditional Random Fields For Spoken\n  Language Understanding Using Unaligned Data", "comments": "Workshop of The Australasian Language Technology Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We experiment graph-based Semi-Supervised Learning (SSL) of Conditional\nRandom Fields (CRF) for the application of Spoken Language Understanding (SLU)\non unaligned data. The aligned labels for examples are obtained using IBM\nModel. We adapt a baseline semi-supervised CRF by defining new feature set and\naltering the label propagation algorithm. Our results demonstrate that our\nproposed approach significantly improves the performance of the supervised\nmodel by utilizing the knowledge gained from the graph.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 10:28:49 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Kiaeeha", "Masoud", ""], ["Khadivi", "Shahram", ""], ["Ghidary", "Saeed Shiry", ""]]}, {"id": "1701.08655", "submitter": "Shrikant Malviya", "authors": "Shrikant Malviya, Rohit Mishra and Uma Shanker Tiwary", "title": "Structural Analysis of Hindi Phonetics and A Method for Extraction of\n  Phonetically Rich Sentences from a Very Large Hindi Text Corpus", "comments": "19th Coordination and Standardization of Speech Databases and\n  Assessment Technique (O-COCOSDA) at Bali, Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) and Text to speech (TTS) are two prominent\narea of research in human computer interaction nowadays. A set of phonetically\nrich sentences is in a matter of importance in order to develop these two\ninteractive modules of HCI. Essentially, the set of phonetically rich sentences\nhas to cover all possible phone units distributed uniformly. Selecting such a\nset from a big corpus with maintaining phonetic characteristic based similarity\nis still a challenging problem. The major objective of this paper is to devise\na criteria in order to select a set of sentences encompassing all phonetic\naspects of a corpus with size as minimum as possible. First, this paper\npresents a statistical analysis of Hindi phonetics by observing the structural\ncharacteristics. Further a two stage algorithm is proposed to extract\nphonetically rich sentences with a high variety of triphones from the EMILLE\nHindi corpus. The algorithm consists of a distance measuring criteria to select\na sentence in order to improve the triphone distribution. Moreover, a special\npreprocessing method is proposed to score each triphone in terms of inverse\nprobability in order to fasten the algorithm. The results show that the\napproach efficiently build uniformly distributed phonetically-rich corpus with\noptimum number of sentences.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 15:37:25 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 03:03:21 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Malviya", "Shrikant", ""], ["Mishra", "Rohit", ""], ["Tiwary", "Uma Shanker", ""]]}, {"id": "1701.08694", "submitter": "Saiful Islam Md", "authors": "Md. Saiful Islam, Fazla Elahi Md Jubayer and Syed Ikhtiar Ahmed", "title": "A Comparative Study on Different Types of Approaches to Bengali document\n  Categorization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document categorization is a technique where the category of a document is\ndetermined. In this paper three well-known supervised learning techniques which\nare Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient\nDescent(SGD) compared for Bengali document categorization. Besides classifier,\nclassification also depends on how feature is selected from dataset. For\nanalyzing those classifier performances on predicting a document against twelve\ncategories several feature selection techniques are also applied in this\narticle namely Chi square distribution, normalized TFIDF (term\nfrequency-inverse document frequency) with word analyzer. So, we attempt to\nexplore the efficiency of those three-classification algorithms by using two\ndifferent feature selection techniques in this article.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 13:08:08 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Jubayer", "Fazla Elahi Md", ""], ["Ahmed", "Syed Ikhtiar", ""]]}, {"id": "1701.08702", "submitter": "Md Saiful Islam", "authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam and Sabir Ismail", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:43:31 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Saha", "Dipaloke", ""], ["Hossain", "Md Saddam", ""], ["Islam", "MD. Saiful", ""], ["Ismail", "Sabir", ""]]}, {"id": "1701.08706", "submitter": "Saiful Islam Md", "authors": "Md. Fahad Hasan, Tasmin Afroz, Sabir Ismail and Md. Saiful Islam", "title": "Document Decomposition of Bangla Printed Text", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today all kind of information is getting digitized and along with all this\ndigitization, the huge archive of various kinds of documents is being digitized\ntoo. We know that, Optical Character Recognition is the method through which,\nnewspapers and other paper documents convert into digital resources. But, it is\na fact that this method works on texts only. As a result, if we try to process\nany document which contains non-textual zones, then we will get garbage texts\nas output. That is why; in order to digitize documents properly they should be\nprepossessed carefully. And while preprocessing, segmenting document in\ndifferent regions according to the category properly is most important. But,\nthe Optical Character Recognition processes available for Bangla language have\nno such algorithm that can categorize a newspaper/book page fully. So we worked\nto decompose a document into its several parts like headlines, sub headlines,\ncolumns, images etc. And if the input is skewed and rotated, then the input was\nalso deskewed and de-rotated. To decompose any Bangla document we found out the\nedges of the input image. Then we find out the horizontal and vertical area of\nevery pixel where it lies in. Later on the input image was cut according to\nthese areas. Then we pick each and every sub image and found out their\nheight-width ratio, line height. Then according to these values the sub images\nwere categorized. To deskew the image we found out the skew angle and de skewed\nthe image according to this angle. To de-rotate the image we used the line\nheight, matra line, pixel ratio of matra line.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:54:52 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Hasan", "Md. Fahad", ""], ["Afroz", "Tasmin", ""], ["Ismail", "Sabir", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1701.08711", "submitter": "Yan Chi Vinci Chow", "authors": "Vinci Chow", "title": "Predicting Auction Price of Vehicle License Plate with Deep Recurrent\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2019.113008", "report-no": null, "categories": "cs.CL cs.LG q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Chinese societies, superstition is of paramount importance, and vehicle\nlicense plates with desirable numbers can fetch very high prices in auctions.\nUnlike other valuable items, license plates are not allocated an estimated\nprice before auction. I propose that the task of predicting plate prices can be\nviewed as a natural language processing (NLP) task, as the value depends on the\nmeaning of each individual character on the plate and its semantics. I\nconstruct a deep recurrent neural network (RNN) to predict the prices of\nvehicle license plates in Hong Kong, based on the characters on a plate. I\ndemonstrate the importance of having a deep network and of retraining.\nEvaluated on 13 years of historical auction prices, the deep RNN's predictions\ncan explain over 80 percent of price variations, outperforming previous models\nby a significant margin. I also demonstrate how the model can be extended to\nbecome a search engine for plates and to provide estimates of the expected\nprice distribution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 17:14:25 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 17:41:35 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2017 16:36:38 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 08:34:32 GMT"}, {"version": "v5", "created": "Tue, 8 Oct 2019 16:25:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chow", "Vinci", ""]]}, {"id": "1701.08756", "submitter": "Rui Liu", "authors": "Rui Liu, Xiaoli Zhang", "title": "A Review of Methodologies for Natural-Language-Facilitated Human-Robot\n  Cooperation", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural-language-facilitated human-robot cooperation (NLC) refers to using\nnatural language (NL) to facilitate interactive information sharing and task\nexecutions with a common goal constraint between robots and humans. Recently,\nNLC research has received increasing attention. Typical NLC scenarios include\nrobotic daily assistance, robotic health caregiving, intelligent manufacturing,\nautonomous navigation, and robot social accompany. However, a thorough review,\nthat can reveal latest methodologies to use NL to facilitate human-robot\ncooperation, is missing. In this review, a comprehensive summary about\nmethodologies for NLC is presented. NLC research includes three main research\nfocuses: NL instruction understanding, NL-based execution plan generation, and\nknowledge-world mapping. In-depth analyses on theoretical methods,\napplications, and model advantages and disadvantages are made. Based on our\npaper review and perspective, potential research directions of NLC are\nsummarized.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 18:59:04 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 19:20:07 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 19:52:20 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Liu", "Rui", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "1701.08888", "submitter": "Guangneng Hu", "authors": "Guang-Neng Hu, Xin-Yu Dai", "title": "Integrating Reviews into Personalized Ranking for Cold Start\n  Recommendation", "comments": "TextBPR", "journal-ref": "PAKDD 2017", "doi": "10.1007/978-3-319-57529-2_55", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item recommendation task predicts a personalized ranking over a set of items\nfor each individual user. One paradigm is the rating-based methods that\nconcentrate on explicit feedbacks and hence face the difficulties in collecting\nthem. Meanwhile, the ranking-based methods are presented with rated items and\nthen rank the rated above the unrated. This paradigm takes advantage of widely\navailable implicit feedback. It, however, usually ignores a kind of important\ninformation: item reviews. Item reviews not only justify the preferences of\nusers, but also help alleviate the cold-start problem that fails the\ncollaborative filtering. In this paper, we propose two novel and simple models\nto integrate item reviews into Bayesian personalized ranking. In each model, we\nmake use of text features extracted from item reviews using word embeddings. On\ntop of text features we uncover the review dimensions that explain the\nvariation in users' feedback and these review factors represent a prior\npreference of users. Experiments on six real-world data sets show the benefits\nof leveraging item reviews on ranking prediction. We also conduct analyses to\nunderstand the proposed models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 02:13:57 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 08:50:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Hu", "Guang-Neng", ""], ["Dai", "Xin-Yu", ""]]}, {"id": "1701.08954", "submitter": "Marco Baroni", "authors": "Marco Baroni, Armand Joulin, Allan Jabri, Germ\\`an Kruszewski,\n  Angeliki Lazaridou, Klemen Simonic, Tomas Mikolov", "title": "CommAI: Evaluating the first steps towards a useful general AI", "comments": "Published in ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With machine learning successfully applied to new daunting problems almost\nevery day, general AI starts looking like an attainable goal. However, most\ncurrent research focuses instead on important but narrow applications, such as\nimage classification or machine translation. We believe this to be largely due\nto the lack of objective ways to measure progress towards broad machine\nintelligence. In order to fill this gap, we propose here a set of concrete\ndesiderata for general AI, together with a platform to test machines on how\nwell they satisfy such desiderata, while keeping all further complexities to a\nminimum.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 09:20:17 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 18:47:01 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Baroni", "Marco", ""], ["Joulin", "Armand", ""], ["Jabri", "Allan", ""], ["Kruszewski", "Germ\u00e0n", ""], ["Lazaridou", "Angeliki", ""], ["Simonic", "Klemen", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1701.09123", "submitter": "Rodrigo Agerri", "authors": "Rodrigo Agerri and German Rigau", "title": "Robust Multilingual Named Entity Recognition with Shallow\n  Semi-Supervised Features", "comments": "26 pages, 19 tables (submitted for publication on September 2015),\n  Artificial Intelligence (2016)", "journal-ref": "Artificial Intelligence, 238, 63-82 (2016)", "doi": "10.1016/j.artint.2016.05.003", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a multilingual Named Entity Recognition approach based on a robust\nand general set of features across languages and datasets. Our system combines\nshallow local information with clustering semi-supervised features induced on\nlarge amounts of unlabeled text. Understanding via empirical experimentation\nhow to effectively combine various types of clustering features allows us to\nseamlessly export our system to other datasets and languages. The result is a\nsimple but highly competitive system which obtains state of the art results\nacross five languages and twelve datasets. The results are reported on standard\nshared task evaluation data such as CoNLL for English, Spanish and Dutch.\nFurthermore, and despite the lack of linguistically motivated features, we also\nreport best results for languages such as Basque and German. In addition, we\ndemonstrate that our method also obtains very competitive results even when the\namount of supervised data is cut by half, alleviating the dependency on\nmanually annotated data. Finally, the results show that our emphasis on\nclustering features is crucial to develop robust out-of-domain models. The\nsystem and models are freely available to facilitate its use and guarantee the\nreproducibility of results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 16:36:06 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Agerri", "Rodrigo", ""], ["Rigau", "German", ""]]}]