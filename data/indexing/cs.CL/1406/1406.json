[{"id": "1406.0032", "submitter": "Pollyanna Gon\\c{c}alves", "authors": "Pollyanna Gon\\c{c}alves and Matheus Ara\\'ujo and Fabr\\'icio Benevenuto\n  and Meeyoung Cha", "title": "Comparing and Combining Sentiment Analysis Methods", "comments": "Proceedings of the first ACM conference on Online social networks\n  (2013) 27-38", "journal-ref": null, "doi": "10.1145/2512938.2512951", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Several messages express opinions about events, products, and services,\npolitical views or even their author's emotional state and mood. Sentiment\nanalysis has been used in several applications including analysis of the\nrepercussions of events in social networks, analysis of opinions about products\nand services, and simply to better understand aspects of social communication\nin Online Social Networks (OSNs). There are multiple methods for measuring\nsentiments, including lexical-based approaches and supervised machine learning\nmethods. Despite the wide use and popularity of some methods, it is unclear\nwhich method is better for identifying the polarity (i.e., positive or\nnegative) of a message as the current literature does not provide a method of\ncomparison among existing methods. Such a comparison is crucial for\nunderstanding the potential limitations, advantages, and disadvantages of\npopular methods in analyzing the content of OSNs messages. Our study aims at\nfilling this gap by presenting comparisons of eight popular sentiment analysis\nmethods in terms of coverage (i.e., the fraction of messages whose sentiment is\nidentified) and agreement (i.e., the fraction of identified sentiments that are\nin tune with ground truth). We develop a new method that combines existing\napproaches, providing the best coverage results and competitive agreement. We\nalso present a free Web service called iFeel, which provides an open API for\naccessing and comparing results across different sentiment methods for a given\ntext.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 22:47:49 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Gon\u00e7alves", "Pollyanna", ""], ["Ara\u00fajo", "Matheus", ""], ["Benevenuto", "Fabr\u00edcio", ""], ["Cha", "Meeyoung", ""]]}, {"id": "1406.0079", "submitter": "Shashishekar Ramakrishna", "authors": "Shashishekar Ramakrishna and Adrian Paschke", "title": "Bridging the gap between Legal Practitioners and Knowledge Engineers\n  using semi-formal KR", "comments": "published in proceedings of the 8th International Workshop on Value\n  Modeling and Business Ontology, VMBO, Berlin, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Structured English as a computation independent knowledge\nrepresentation format for non-technical users in business rules representation\nhas been proposed in OMGs Semantics and Business Vocabulary Representation\n(SBVR). In the legal domain we face a similar problem. Formal representation\nlanguages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2\nontologies etc.) support the technical knowledge engineer and the automated\nreasoning. But, they can be hardly used directly by the legal domain experts\nwho do not have a computer science background. In this paper we adapt the SBVR\nStructured English approach for the legal domain and implement a\nproof-of-concept, called KR4IPLaw, which enables legal domain experts to\nrepresent their knowledge in Structured English in a computational independent\nand hence, for them, more usable way. The benefit of this approach is that the\nunderlying pre-defined semantics of the Structured English approach makes\ntransformations into formal languages such as OASIS LegalRuleML and OWL2\nontologies possible. We exemplify our approach in the domain of patent law.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 14:16:30 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Ramakrishna", "Shashishekar", ""], ["Paschke", "Adrian", ""]]}, {"id": "1406.1078", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry\n  Bahdanau, Fethi Bougares, Holger Schwenk and Yoshua Bengio", "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation", "comments": "EMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel neural network model called RNN\nEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNN\nencodes a sequence of symbols into a fixed-length vector representation, and\nthe other decodes the representation into another sequence of symbols. The\nencoder and decoder of the proposed model are jointly trained to maximize the\nconditional probability of a target sequence given a source sequence. The\nperformance of a statistical machine translation system is empirically found to\nimprove by using the conditional probabilities of phrase pairs computed by the\nRNN Encoder-Decoder as an additional feature in the existing log-linear model.\nQualitatively, we show that the proposed model learns a semantically and\nsyntactically meaningful representation of linguistic phrases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 17:47:08 GMT"}, {"version": "v2", "created": "Thu, 24 Jul 2014 20:07:13 GMT"}, {"version": "v3", "created": "Wed, 3 Sep 2014 00:25:02 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Cho", "Kyunghyun", ""], ["van Merrienboer", "Bart", ""], ["Gulcehre", "Caglar", ""], ["Bahdanau", "Dzmitry", ""], ["Bougares", "Fethi", ""], ["Schwenk", "Holger", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1406.1143", "submitter": "Jimmy Lin", "authors": "Sarah Weissman, Samet Ayhan, Joshua Bradley, and Jimmy Lin", "title": "Identifying Duplicate and Contradictory Information in Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study identifies sentences in Wikipedia articles that are either\nidentical or highly similar by applying techniques for near-duplicate detection\nof web pages. This is accomplished with a MapReduce implementation of minhash\nto identify clusters of sentences with high Jaccard similarity. We show that\nthese clusters can be categorized into six different types, two of which are\nparticularly interesting: identical sentences quantify the extent to which\ncontent in Wikipedia is copied and pasted, and near-duplicate sentences that\nstate contradictory facts point to quality issues in Wikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 18:59:00 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Weissman", "Sarah", ""], ["Ayhan", "Samet", ""], ["Bradley", "Joshua", ""], ["Lin", "Jimmy", ""]]}, {"id": "1406.1203", "submitter": "Ashudeep Singh", "authors": "Divyanshu Bhartiya, Ashudeep Singh", "title": "A Semantic Approach to Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence extraction based summarization methods has some limitations as it\ndoesn't go into the semantics of the document. Also, it lacks the capability of\nsentence generation which is intuitive to humans. Here we present a novel\nmethod to summarize text documents taking the process to semantic levels with\nthe use of WordNet and other resources, and using a technique for sentence\ngeneration. We involve semantic role labeling to get the semantic\nrepresentation of text and use of segmentation to form clusters of the related\npieces of text. Picking out the centroids and sentence generation completes the\ntask. We evaluate our system against human composed summaries and also present\nan evaluation done by humans to measure the quality attributes of our\nsummaries.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 20:22:30 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Bhartiya", "Divyanshu", ""], ["Singh", "Ashudeep", ""]]}, {"id": "1406.1234", "submitter": "Lijiang Chen", "authors": "Chen Lijiang", "title": "A Geometric Method to Obtain the Generation Probability of a Sentence", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"How to generate a sentence\" is the most critical and difficult problem in\nall the natural language processing technologies. In this paper, we present a\nnew approach to explain the generation process of a sentence from the\nperspective of mathematics. Our method is based on the premise that in our\nbrain a sentence is a part of a word network which is formed by many word\nnodes. Experiments show that the probability of the entire sentence can be\nobtained by the probabilities of single words and the probabilities of the\nco-occurrence of word pairs, which indicate that human use the synthesis method\nto generate a sentence.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:28:51 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Lijiang", "Chen", ""]]}, {"id": "1406.1241", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "T. El-Shishtawy and A. El-Sammak", "title": "The Best Templates Match Technique For Example Based Machine Translation", "comments": "Eleventh International Conference on Artificial Intelligence\n  Applications, 2003", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  It has been proved that large scale realistic Knowledge Based Machine\nTranslation applications require acquisition of huge knowledge about language\nand about the world. This knowledge is encoded in computational grammars,\nlexicons and domain models. Another approach which avoids the need for\ncollecting and analyzing massive knowledge, is the Example Based approach,\nwhich is the topic of this paper. We show through the paper that using Example\nBased in its native form is not suitable for translating into Arabic. Therefore\na modification to the basic approach is presented to improve the accuracy of\nthe translation process. The basic idea of the new approach is to improve the\ntechnique by which template-based approaches select the appropriate templates.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:56:08 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["El-Shishtawy", "T.", ""], ["El-Sammak", "A.", ""]]}, {"id": "1406.1280", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Sunil Kumar Kopparapu and M Laxminarayana", "title": "Basis Identification for Automatic Creation of Pronunciation Lexicon for\n  Proper Names", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of a proper names pronunciation lexicon is usually a manual\neffort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,\nin literature, are usually rule based and work best for non-proper names in a\nparticular language. Proper names are foreign to a G2P module. We follow an\noptimization approach to enable automatic construction of proper names\npronunciation lexicon. The idea is to construct a small orthogonal set of words\n(basis) which can span the set of names in a given database. We propose two\nalgorithms for the construction of this basis. The transcription lexicon of all\nthe proper names in a database can be produced by the manual transcription of\nonly the small set of basis words. We first construct a cost function and show\nthat the minimization of the cost function results in a basis. We derive\nconditions for convergence of this cost function and validate them\nexperimentally on a very large proper name database. Experiments show the\ntranscription can be achieved by transcribing a set of small number of basis\nwords. The algorithms proposed are generic and independent of language; however\nperformance is better if the proper names have same origin, namely, same\nlanguage or geographical region.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 07:07:44 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Kopparapu", "Sunil Kumar", ""], ["Laxminarayana", "M", ""]]}, {"id": "1406.1765", "submitter": "Maxime Warnier", "authors": "Anne Condamines and Maxime Warnier", "title": "Linguistic Analysis of Requirements of a Space Project and their\n  Conformity with the Recommendations Proposed by a Controlled Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long term aim of the project carried out by the French National Space\nAgency (CNES) is to design a writing guide based on the real and regular\nwriting of requirements. As a first step in the project, this paper proposes a\nlin-guistic analysis of requirements written in French by CNES engineers. The\naim is to determine to what extent they conform to two rules laid down in\nINCOSE, a recent guide for writing requirements. Although CNES engineers are\nnot obliged to follow any Controlled Natural Language in their writing of\nrequirements, we believe that language regularities are likely to emerge from\nthis task, mainly due to the writers' experience. The issue is approached using\nnatural language processing tools to identify sentences that do not comply with\nINCOSE rules. We further review these sentences to understand why the\nrecommendations cannot (or should not) always be applied when specifying\nlarge-scale projects.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 18:09:59 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Condamines", "Anne", ""], ["Warnier", "Maxime", ""]]}, {"id": "1406.1827", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Christopher Potts, Christopher D. Manning", "title": "Recursive Neural Networks Can Learn Logical Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured recursive neural networks (TreeRNNs) for sentence meaning\nhave been successful for many applications, but it remains an open question\nwhether the fixed-length representations that they learn can support tasks as\ndemanding as logical deduction. We pursue this question by evaluating whether\ntwo such models---plain TreeRNNs and tree-structured neural tensor networks\n(TreeRNTNs)---can correctly learn to identify logical relationships such as\nentailment and contradiction using these representations. In our first set of\nexperiments, we generate artificial data from a logical grammar and use it to\nevaluate the models' ability to learn to handle basic relational reasoning,\nrecursive structures, and quantification. We then evaluate the models on the\nmore natural SICK challenge data. Both models perform competitively on the SICK\ndata and generalize well in all three experiments on simulated data, suggesting\nthat they can learn suitable representations for logical inference in natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 22:09:27 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 20:37:33 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 19:48:45 GMT"}, {"version": "v4", "created": "Thu, 14 May 2015 19:37:38 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Potts", "Christopher", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1406.1870", "submitter": "C. Maria Keet", "authors": "C. Maria Keet and Langa Khumalo", "title": "Toward verbalizing ontologies in isiZulu", "comments": "12 pages, 1 figure; CNL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IsiZulu is one of the eleven official languages of South Africa and roughly\nhalf the population can speak it. It is the first (home) language for over 10\nmillion people in South Africa. Only a few computational resources exist for\nisiZulu and its related Nguni languages, yet the imperative for tool\ndevelopment exists. We focus on natural language generation, and the grammar\noptions and preferences in particular, which will inform verbalization of\nknowledge representation languages and could contribute to machine translation.\nThe verbalization pattern specification shows that the grammar rules are\nelaborate and there are several options of which one may have preference. We\ndevised verbalization patterns for subsumption, basic disjointness, existential\nand universal quantification, and conjunction. This was evaluated in a survey\namong linguists and non-linguists. Some differences between linguists and\nnon-linguists can be observed, with the former much more in agreement, and\npreferences depend on the overall structure of the sentence, such as singular\nfor subsumption and plural in other cases.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 07:41:54 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Keet", "C. Maria", ""], ["Khumalo", "Langa", ""]]}, {"id": "1406.1953", "submitter": "Peilei Liu", "authors": "Peilei Liu, Ting Wang", "title": "Automatic Extraction of Protein Interaction in Literature", "comments": "This paper has been withdrawn by the author due to its lack of\n  academic value", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein-protein interaction extraction is the key precondition of the\nconstruction of protein knowledge network, and it is very important for the\nresearch in the biomedicine. This paper extracted directional protein-protein\ninteraction from the biological text, using the SVM-based method. Experiments\nwere evaluated on the LLL05 corpus with good results. The results show that\ndependency features are import for the protein-protein interaction extraction\nand features related to the interaction word are effective for the interaction\ndirection judgment. At last, we analyzed the effects of different features and\nplaned for the next step.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 07:41:07 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 12:25:47 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Liu", "Peilei", ""], ["Wang", "Ting", ""]]}, {"id": "1406.2022", "submitter": "Rahul Tejwani", "authors": "Rahul Tejwani (University at Buffalo)", "title": "Two-dimensional Sentiment Analysis of text", "comments": "sentiment analysis, two-dimensional", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis aims to get the underlying viewpoint of the text, which\ncould be anything that holds a subjective opinion, such as an online review,\nMovie rating, Comments on Blog posts etc. This paper presents a novel approach\nthat classify text in two-dimensional Emotional space, based on the sentiments\nof the author. The approach uses existing lexical resources to extract feature\nset, which is trained using Supervised Learning techniques.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 20:05:36 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Tejwani", "Rahul", "", "University at Buffalo"]]}, {"id": "1406.2035", "submitter": "Dani Yogatama", "authors": "Dani Yogatama and Manaal Faruqui and Chris Dyer and Noah A. Smith", "title": "Learning Word Representations with Hierarchical Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning word representations using hierarchical\nregularization in sparse coding inspired by the linguistic study of word\nmeanings. We show an efficient learning algorithm based on stochastic proximal\nmethods that is significantly faster than previous approaches, making it\npossible to perform hierarchical sparse coding on a corpus of billions of word\ntokens. Experiments on various benchmark tasks---word similarity ranking,\nanalogies, sentence completion, and sentiment analysis---demonstrate that the\nmethod outperforms or is competitive with state-of-the-art methods. Our word\nrepresentations are available at\n\\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 22:35:09 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 14:26:21 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Yogatama", "Dani", ""], ["Faruqui", "Manaal", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1406.2096", "submitter": "Paul Brillant Feuto Njonko", "authors": "Paul Brillant Feuto Njonko, Sylviane Cardey, Peter Greenfield, and\n  Walid El Abed", "title": "RuleCNL: A Controlled Natural Language for Business Rule Specifications", "comments": "12 pages, 7 figures, Fourth Workshop on Controlled Natural Language\n  (CNL 2014) Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business rules represent the primary means by which companies define their\nbusiness, perform their actions in order to reach their objectives. Thus, they\nneed to be expressed unambiguously to avoid inconsistencies between business\nstakeholders and formally in order to be machine-processed. A promising\nsolution is the use of a controlled natural language (CNL) which is a good\nmediator between natural and formal languages. This paper presents RuleCNL,\nwhich is a CNL for defining business rules. Its core feature is the alignment\nof the business rule definition with the business vocabulary which ensures\ntraceability and consistency with the business domain. The RuleCNL tool\nprovides editors that assist end-users in the writing process and automatic\nmappings into the Semantics of Business Vocabulary and Business Rules (SBVR)\nstandard. SBVR is grounded in first order logic and includes constructs called\nsemantic formulations that structure the meaning of rules.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 07:19:53 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Njonko", "Paul Brillant Feuto", ""], ["Cardey", "Sylviane", ""], ["Greenfield", "Peter", ""], ["Abed", "Walid El", ""]]}, {"id": "1406.2204", "submitter": "Sandra Williams", "authors": "Sandra Williams, Richard Power and Allan Third", "title": "How Easy is it to Learn a Controlled Natural Language for Building a\n  Knowledge Base?", "comments": "CNL 2014 : Fourth Workshop on Controlled Natural Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in controlled natural language editors for knowledge\nengineering (KE) have given rise to expectations that they will make KE tasks\nmore accessible and perhaps even enable non-engineers to build knowledge bases.\nThis exploratory research focussed on novices and experts in knowledge\nengineering during their attempts to learn a controlled natural language (CNL)\nknown as OWL Simplified English and use it to build a small knowledge base.\nParticipants' behaviours during the task were observed through eye-tracking and\nscreen recordings. This was an attempt at a more ambitious user study than in\nprevious research because we used a naturally occurring text as the source of\ndomain knowledge, and left them without guidance on which information to\nselect, or how to encode it. We have identified a number of skills\n(competencies) required for this difficult task and key problems that authors\nface.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 14:54:22 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Williams", "Sandra", ""], ["Power", "Richard", ""], ["Third", "Allan", ""]]}, {"id": "1406.2298", "submitter": "Gordon J. Pace", "authors": "Gordon J. Pace and Michael Rosner", "title": "Explaining Violation Traces with Finite State Natural Language\n  Generation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential element of any verification technique is that of identifying and\ncommunicating to the user, system behaviour which leads to a deviation from the\nexpected behaviour. Such behaviours are typically made available as long traces\nof system actions which would benefit from a natural language explanation of\nthe trace and especially in the context of business logic level specifications.\nIn this paper we present a natural language generation model which can be used\nto explain such traces. A key idea is that the explanation language is a CNL\nthat is, formally speaking, regular language susceptible transformations that\ncan be expressed with finite state machinery. At the same time it admits\nvarious forms of abstraction and simplification which contribute to the\nnaturalness of explanations that are communicated to the user.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 19:51:10 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Pace", "Gordon J.", ""], ["Rosner", "Michael", ""]]}, {"id": "1406.2400", "submitter": "Normunds Gr\\=uz\\=itis", "authors": "Dana Dann\\'ells and Normunds Gr\\=uz\\=itis", "title": "Controlled Natural Language Generation from a Multilingual\n  FrameNet-based Grammar", "comments": null, "journal-ref": "Controlled Natural Language, Lecture Notes in Computer Science,\n  Vol. 8625, Springer, 2014, pp. 155-166", "doi": "10.1007/978-3-319-10223-8_15", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a currently bilingual but potentially multilingual\nFrameNet-based grammar library implemented in Grammatical Framework. The\ncontribution of this paper is two-fold. First, it offers a methodological\napproach to automatically generate the grammar based on semantico-syntactic\nvalence patterns extracted from FrameNet-annotated corpora. Second, it provides\na proof of concept for two use cases illustrating how the acquired multilingual\ngrammar can be exploited in different CNL applications in the domains of arts\nand tourism.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 01:01:48 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Dann\u00e9lls", "Dana", ""], ["Gr\u016bz\u012btis", "Normunds", ""]]}, {"id": "1406.2538", "submitter": "Guntis Barzdins", "authors": "Guntis Barzdins", "title": "FrameNet CNL: a Knowledge Representation and Information Extraction\n  Language", "comments": "CNL-2014 camera-ready version. The final publication is available at\n  link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a FrameNet-based information extraction and knowledge\nrepresentation framework, called FrameNet-CNL. The framework is used on natural\nlanguage documents and represents the extracted knowledge in a tailor-made\nFrame-ontology from which unambiguous FrameNet-CNL paraphrase text can be\ngenerated automatically in multiple languages. This approach brings together\nthe fields of information extraction and CNL, because a source text can be\nconsidered belonging to FrameNet-CNL, if information extraction parser produces\nthe correct knowledge representation as a result. We describe a\nstate-of-the-art information extraction parser used by a national news agency\nand speculate that FrameNet-CNL eventually could shape the natural language\nsubset used for writing the newswire articles.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:16:36 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Barzdins", "Guntis", ""]]}, {"id": "1406.2710", "submitter": "Ryan Kiros", "authors": "Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov", "title": "A Multiplicative Model for Learning Distributed Text-Based Attribute\n  Representations", "comments": "11 pages. An earlier version was accepted to the ICML-2014 Workshop\n  on Knowledge-Powered Deep Learning for Text Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a general framework for learning distributed\nrepresentations of attributes: characteristics of text whose representations\ncan be jointly learned with word embeddings. Attributes can correspond to\ndocument indicators (to learn sentence vectors), language indicators (to learn\ndistributed language representations), meta-data and side information (such as\nthe age, gender and industry of a blogger) or representations of authors. We\ndescribe a third-order model where word context and attribute vectors interact\nmultiplicatively to predict the next word in a sequence. This leads to the\nnotion of conditional word similarity: how meanings of words change when\nconditioned on different attributes. We perform several experimental tasks\nincluding sentiment classification, cross-lingual document classification, and\nblog authorship attribution. We also qualitatively evaluate conditional word\nneighbours and attribute-conditioned text generation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 20:29:10 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Kiros", "Ryan", ""], ["Zemel", "Richard S.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1406.2880", "submitter": "Ulf Sch\\\"oneberg", "authors": "Ulf Sch\\\"oneberg and Wolfram Sperber", "title": "POS Tagging and its Applications for Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content analysis of scientific publications is a nontrivial task, but a\nuseful and important one for scientific information services. In the Gutenberg\nera it was a domain of human experts; in the digital age many machine-based\nmethods, e.g., graph analysis tools and machine-learning techniques, have been\ndeveloped for it. Natural Language Processing (NLP) is a powerful\nmachine-learning approach to semiautomatic speech and language processing,\nwhich is also applicable to mathematics. The well established methods of NLP\nhave to be adjusted for the special needs of mathematics, in particular for\nhandling mathematical formulae. We demonstrate a mathematics-aware part of\nspeech tagger and give a short overview about our adaptation of NLP methods for\nmathematical publications. We show the use of the tools developed for key\nphrase extraction and classification in the database zbMATH.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 12:25:26 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Sch\u00f6neberg", "Ulf", ""], ["Sperber", "Wolfram", ""]]}, {"id": "1406.2903", "submitter": "Hazem Abdelaal", "authors": "Hazem Safwat and Brian Davis", "title": "A Brief State of the Art for Ontology Authoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges for building the Semantic web is Ontology\nAuthoring. Controlled Natural Languages CNLs offer a user friendly means for\nnon-experts to author ontologies. This paper provides a snapshot of the\nstate-of-the-art for the core CNLs for ontology authoring and reviews their\nrespective evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 13:47:22 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 01:39:49 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Safwat", "Hazem", ""], ["Davis", "Brian", ""]]}, {"id": "1406.2963", "submitter": "Ce Zhang", "authors": "Shanan E. Peters, Ce Zhang, Miron Livny, Christopher R\\'e", "title": "A machine-compiled macroevolutionary history of Phanerozoic life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.LG q-bio.PE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many aspects of macroevolutionary theory and our understanding of biotic\nresponses to global environmental change derive from literature-based\ncompilations of palaeontological data. Existing manually assembled databases\nare, however, incomplete and difficult to assess and enhance. Here, we develop\nand validate the quality of a machine reading system, PaleoDeepDive, that\nautomatically locates and extracts data from heterogeneous text, tables, and\nfigures in publications. PaleoDeepDive performs comparably to humans in complex\ndata extraction and inference tasks and generates congruent synthetic\nmacroevolutionary results. Unlike traditional databases, PaleoDeepDive produces\na probabilistic database that systematically improves as information is added.\nWe also show that the system can readily accommodate sophisticated data types,\nsuch as morphological data in biological illustrations and associated textual\ndescriptions. Our machine reading approach to scientific data integration and\nsynthesis brings within reach many questions that are currently underdetermined\nand does so in ways that may stimulate entirely new modes of inquiry.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 17:02:14 GMT"}, {"version": "v2", "created": "Sat, 19 Jul 2014 16:40:47 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Peters", "Shanan E.", ""], ["Zhang", "Ce", ""], ["Livny", "Miron", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1406.3287", "submitter": "Matthew Mayo", "authors": "Matthew Mayo", "title": "A Clustering Analysis of Tweet Length and its Relation to Sentiment", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis of Twitter data is performed. The researcher has made the\nfollowing contributions via this paper: (1) an innovative method for deriving\nsentiment score dictionaries using an existing sentiment dictionary as seed\nwords is explored, and (2) an analysis of clustered tweet sentiment scores\nbased on tweet length is performed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 17:01:10 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 14:07:35 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 19:44:24 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Mayo", "Matthew", ""]]}, {"id": "1406.3460", "submitter": "Karolina Suchowolec", "authors": "Karolina Suchowolec", "title": "Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG", "comments": "Fourth Workshop on Controlled Natural Language (CNL 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled natural languages for industrial application are often regarded as\na response to the challenges of translation and multilingual communication.\nThis paper presents a quite different approach taken by Koenig & Bauer AG,\nwhere the main goal was the improvement of the authoring process for technical\ndocumentation. Most importantly, this paper explores the notion of a controlled\nlanguage and demonstrates how style guides can emerge from non-linguistic\nconsiderations. Moreover, it shows the transition from loose language\nrecommendations into precise and prescriptive rules and investigates whether\nsuch rules can be regarded as a full-fledged controlled language.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 09:23:53 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Suchowolec", "Karolina", ""]]}, {"id": "1406.3676", "submitter": "Jason  Weston", "authors": "Antoine Bordes, Sumit Chopra, Jason Weston", "title": "Question Answering with Subgraph Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a system which learns to answer questions on a broad\nrange of topics from a knowledge base using few hand-crafted features. Our\nmodel learns low-dimensional embeddings of words and knowledge base\nconstituents; these representations are used to score natural language\nquestions against candidate answers. Training our system using pairs of\nquestions and structured representations of their answers, and pairs of\nquestion paraphrases, yields competitive results on a competitive benchmark of\nthe literature.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 03:00:23 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 01:02:11 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 00:25:35 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Bordes", "Antoine", ""], ["Chopra", "Sumit", ""], ["Weston", "Jason", ""]]}, {"id": "1406.3714", "submitter": "Richa Sharma", "authors": "Richa Sharma, Shweta Nigam and Rekha Jain", "title": "Mining of product reviews at aspect level", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol.4, No.3, May 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Todays world is a world of Internet, almost all work can be done with the\nhelp of it, from simple mobile phone recharge to biggest business deals can be\ndone with the help of this technology. People spent their most of the times on\nsurfing on the Web it becomes a new source of entertainment, education,\ncommunication, shopping etc. Users not only use these websites but also give\ntheir feedback and suggestions that will be useful for other users. In this way\na large amount of reviews of users are collected on the Web that needs to be\nexplored, analyse and organized for better decision making. Opinion Mining or\nSentiment Analysis is a Natural Language Processing and Information Extraction\ntask that identifies the users views or opinions explained in the form of\npositive, negative or neutral comments and quotes underlying the text. Aspect\nbased opinion mining is one of the level of Opinion mining that determines the\naspect of the given reviews and classify the review for each feature. In this\npaper an aspect based opinion mining system is proposed to classify the reviews\nas positive, negative and neutral for each feature. Negation is also handled in\nthe proposed system. Experimental results using reviews of products show the\neffectiveness of the system.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 10:39:57 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Sharma", "Richa", ""], ["Nigam", "Shweta", ""], ["Jain", "Rekha", ""]]}, {"id": "1406.3830", "submitter": "Misha Denil", "authors": "Misha Denil and Alban Demiraj and Nal Kalchbrenner and Phil Blunsom\n  and Nando de Freitas", "title": "Modelling, Visualising and Summarising Documents with a Single\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the compositional process which maps the meaning of words to that\nof documents is a central challenge for researchers in Natural Language\nProcessing and Information Retrieval. We introduce a model that is able to\nrepresent the meaning of documents by embedding them in a low dimensional\nvector space, while preserving distinctions of word and sentence order crucial\nfor capturing nuanced semantics. Our model is based on an extended Dynamic\nConvolution Neural Network, which learns convolution filters at both the\nsentence and document level, hierarchically learning to capture and compose low\nlevel lexical features into high level semantic concepts. We demonstrate the\neffectiveness of this model on a range of document modelling tasks, achieving\nstrong results with no feature engineering and with a more compact model.\nInspired by recent advances in visualising deep convolution networks for\ncomputer vision, we present a novel visualisation technique for our document\nnetworks which not only provides insight into their learning process, but also\ncan be interpreted to produce a compelling automatic summarisation system for\ntexts.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 17:15:32 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Denil", "Misha", ""], ["Demiraj", "Alban", ""], ["Kalchbrenner", "Nal", ""], ["Blunsom", "Phil", ""], ["de Freitas", "Nando", ""]]}, {"id": "1406.3855", "submitter": "Peter Sheridan Dodds", "authors": "Peter Sheridan Dodds, Eric M. Clark, Suma Desu, Morgan R. Frank,\n  Andrew J. Reagan, Jake Ryland Williams, Lewis Mitchell, Kameron Decker\n  Harris, Isabel M. Kloumann, James P. Bagrow, Karine Megerdoomian, Matthew T.\n  McMahon, Brian F. Tivnan, and Christopher M. Danforth", "title": "Human language reveals a universal positivity bias", "comments": "Manuscript: 7 pages, 4 figures; Supplementary Material: 49 pages, 43\n  figures, 6 tables. Online appendices available at\n  http://www.uvm.edu/storylab/share/papers/dodds2014a/", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using human evaluation of 100,000 words spread across 24 corpora in 10\nlanguages diverse in origin and culture, we present evidence of a deep imprint\nof human sociality in language, observing that (1) the words of natural human\nlanguage possess a universal positivity bias; (2) the estimated emotional\ncontent of words is consistent between languages under translation; and (3)\nthis positivity bias is strongly independent of frequency of word usage.\nAlongside these general regularities, we describe inter-language variations in\nthe emotional spectrum of languages which allow us to rank corpora. We also\nshow how our word evaluations can be used to construct physical-like\ninstruments for both real-time and offline measurement of the emotional content\nof large-scale texts.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 19:38:25 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Dodds", "Peter Sheridan", ""], ["Clark", "Eric M.", ""], ["Desu", "Suma", ""], ["Frank", "Morgan R.", ""], ["Reagan", "Andrew J.", ""], ["Williams", "Jake Ryland", ""], ["Mitchell", "Lewis", ""], ["Harris", "Kameron Decker", ""], ["Kloumann", "Isabel M.", ""], ["Bagrow", "James P.", ""], ["Megerdoomian", "Karine", ""], ["McMahon", "Matthew T.", ""], ["Tivnan", "Brian F.", ""], ["Danforth", "Christopher M.", ""]]}, {"id": "1406.3915", "submitter": "Sankar Mukherjee", "authors": "Sankar Mukherjee, Shyamal Kumar Das Mandal", "title": "A Bengali HMM Based Speech Synthesis System", "comments": null, "journal-ref": "Oriental COCOSDA 2012, pp.225 259", "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper presents the capability of an HMM-based TTS system to produce\nBengali speech. In this synthesis method, trajectories of speech parameters are\ngenerated from the trained Hidden Markov Models. A final speech waveform is\nsynthesized from those speech parameters. In our experiments, spectral\nproperties were represented by Mel Cepstrum Coefficients. Both the training and\nsynthesis issues are investigated in this paper using annotated Bengali speech\ndatabase. Experimental evaluation depicts that the developed text-to-speech\nsystem is capable of producing adequately natural speech in terms of\nintelligibility and intonation for Bengali.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 06:41:54 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Mukherjee", "Sankar", ""], ["Mandal", "Shyamal Kumar Das", ""]]}, {"id": "1406.3969", "submitter": "Siddhartha Ghosh", "authors": "Siddhartha Ghosh, Sujata Thamke and Kalyani U.R.S", "title": "Translation Of Telugu-Marathi and Vice-Versa using Rule Based Machine\n  Translation", "comments": "13 pages, Fourth International Conference on Advances in Computing\n  and Information Technology (ACITY 2014) Delhi, India - May 2014", "journal-ref": null, "doi": "10.5121/csit.2014.4501", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays digital world automated Machine Translation of one language to\nanother has covered a long way to achieve different kinds of success stories.\nWhereas Babel Fish supports a good number of foreign languages and only Hindi\nfrom Indian languages, the Google Translator takes care of about 10 Indian\nlanguages. Though most of the Automated Machine Translation Systems are doing\nwell but handling Indian languages needs a major care while handling the local\nproverbs/ idioms. Most of the Machine Translation system follows the direct\ntranslation approach while translating one Indian language to other. Our\nresearch at KMIT R&D Lab found that handling the local proverbs/idioms is not\ngiven enough attention by the earlier research work. This paper focuses on two\nof the majorly spoken Indian languages Marathi and Telugu, and translation\nbetween them. Handling proverbs and idioms of both the languages have been\ngiven a special care, and the research outcome shows a significant achievement\nin this direction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 10:59:03 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Ghosh", "Siddhartha", ""], ["Thamke", "Sujata", ""], ["S", "Kalyani U. R.", ""]]}, {"id": "1406.3976", "submitter": "Inari Listenmaa", "authors": "Ramona Enache, Inari Listenmaa, Prasanth Kolachina", "title": "Handling non-compositionality in multilingual CNLs", "comments": "CNL workshop in COLING 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe methods for handling multilingual\nnon-compositional constructions in the framework of GF. We specifically look at\nmethods to detect and extract non-compositional phrases from parallel texts and\npropose methods to handle such constructions in GF grammars. We expect that the\nmethods to handle non-compositional constructions will enrich CNLs by providing\nmore flexibility in the design of controlled languages. We look at two specific\nuse cases of non-compositional constructions: a general-purpose method to\ndetect and extract multilingual multiword expressions and a procedure to\nidentify nominal compounds in German. We evaluate our procedure for multiword\nexpressions by performing a qualitative analysis of the results. For the\nexperiments on nominal compounds, we incorporate the detected compounds in a\nfull SMT pipeline and evaluate the impact of our method in machine translation\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 11:30:51 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Enache", "Ramona", ""], ["Listenmaa", "Inari", ""], ["Kolachina", "Prasanth", ""]]}, {"id": "1406.3987", "submitter": "Patrick Saint Dizier", "authors": "Juyeon Kang, Patrick Saint Dizier", "title": "Towards an Error Correction Memory to Enhance Technical Texts Authoring\n  in LELIE", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we investigate and experiment the notion of error correction\nmemory applied to error correction in technical texts. The main purpose is to\ninduce relatively generic correction patterns associated with more contextual\ncorrection recommendations, based on previously memorized and analyzed\ncorrections. The notion of error correction memory is developed within the\nframework of the LELIE project and illustrated on the case of fuzzy lexical\nitems, which is a major problem in technical texts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 12:03:49 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Kang", "Juyeon", ""], ["Dizier", "Patrick Saint", ""]]}, {"id": "1406.4057", "submitter": "Aarne Ranta", "authors": "Aarne Ranta", "title": "Embedded Controlled Languages", "comments": "7 pages, extended abstract, preprint for CNL 2014 in Galway", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by embedded programming languages, an embedded CNL (controlled\nnatural language) is a proper fragment of an entire natural language (its host\nlanguage), but it has a parser that recognizes the entire host language. This\nmakes it possible to process out-of-CNL input and give useful feedback to\nusers, instead of just reporting syntax errors. This extended abstract explains\nthe main concepts of embedded CNL implementation in GF (Grammatical Framework),\nwith examples from machine translation and some other ongoing work.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 16:11:32 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Ranta", "Aarne", ""]]}, {"id": "1406.4211", "submitter": "Thierry Poibeau", "authors": "Pierre Bourreau and Thierry Poibeau", "title": "Mapping the Economic Crisis: Some Preliminary Investigations", "comments": "Technical paper describing the Lattice submission to the 2014\n  PoliInformatics Unshared task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our contribution to the PoliInformatics 2014\nChallenge on the 2007-2008 financial crisis. We propose a state of the art\ntechnique to extract information from texts and provide different\nrepresentations, giving first a static overview of the domain and then a\ndynamic representation of its main evolutions. We show that this strategy\nprovides a practical solution to some recent theories in social sciences that\nare facing a lack of methods and tools to automatically extract information\nfrom natural language texts.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 01:34:22 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Bourreau", "Pierre", ""], ["Poibeau", "Thierry", ""]]}, {"id": "1406.4441", "submitter": "Martin Gerlach", "authors": "Martin Gerlach and Eduardo G. Altmann", "title": "Scaling laws and fluctuations in the statistics of word frequencies", "comments": "19 pages, 4 figures", "journal-ref": "New Journal of Physics 16 (2014), 113010", "doi": "10.1088/1367-2630/16/11/113010", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine statistical analysis of large text databases and\nsimple stochastic models to explain the appearance of scaling laws in the\nstatistics of word frequencies. Besides the sublinear scaling of the vocabulary\nsize with database size (Heaps' law), here we report a new scaling of the\nfluctuations around this average (fluctuation scaling analysis). We explain\nboth scaling laws by modeling the usage of words by simple stochastic processes\nin which the overall distribution of word-frequencies is fat tailed (Zipf's\nlaw) and the frequency of a single word is subject to fluctuations across\ndocuments (as in topic models). In this framework, the mean and the variance of\nthe vocabulary size can be expressed as quenched averages, implying that: i)\nthe inhomogeneous dissemination of words cause a reduction of the average\nvocabulary size in comparison to the homogeneous case, and ii) correlations in\nthe co-occurrence of words lead to an increase in the variance and the\nvocabulary size becomes a non-self-averaging quantity. We address the\nimplications of these observations to the measurement of lexical richness. We\ntest our results in three large text databases (Google-ngram, Enlgish\nWikipedia, and a collection of scientific articles).\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 17:29:39 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 13:36:44 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Gerlach", "Martin", ""], ["Altmann", "Eduardo G.", ""]]}, {"id": "1406.4469", "submitter": "Santiago Segarra", "authors": "Santiago Segarra, Mark Eisen, Alejandro Ribeiro", "title": "Authorship Attribution through Function Word Adjacency Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2451111", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for authorship attribution based on function word adjacency networks\n(WANs) is introduced. Function words are parts of speech that express\ngrammatical relationships between other words but do not carry lexical meaning\non their own. In the WANs in this paper, nodes are function words and directed\nedges stand in for the likelihood of finding the sink word in the ordered\nvicinity of the source word. WANs of different authors can be interpreted as\ntransition probabilities of a Markov chain and are therefore compared in terms\nof their relative entropies. Optimal selection of WAN parameters is studied and\nattribution accuracy is benchmarked across a diverse pool of authors and\nvarying text lengths. This analysis shows that, since function words are\nindependent of content, their use tends to be specific to an author and that\nthe relational data captured by function WANs is a good summary of stylometric\nfingerprints. Attribution accuracy is observed to exceed the one achieved by\nmethods that rely on word frequencies alone. Further combining WANs with\nmethods that rely on word frequencies alone, results in larger attribution\naccuracy, indicating that both sources of information encode different aspects\nof authorial styles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 18:32:18 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Segarra", "Santiago", ""], ["Eisen", "Mark", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1406.4498", "submitter": "Fakhteh Ghanbarnejad", "authors": "Fakhteh Ghanbarnejad, Martin Gerlach, Jose M. Miotto, Eduardo G.\n  Altmann", "title": "Extracting information from S-curves of language change", "comments": "9 pages, 5 figures, Supplementary Material is available at\n  http://dx.doi.org/10.6084/m9.figshare.1221782", "journal-ref": "J. R. Soc. Interface 6 December 2014 vol. 11 no. 101 20141044", "doi": "10.1098/rsif.2014.1044", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well accepted that adoption of innovations are described by S-curves\n(slow start, accelerating period, and slow end). In this paper, we analyze how\nmuch information on the dynamics of innovation spreading can be obtained from a\nquantitative description of S-curves. We focus on the adoption of linguistic\ninnovations for which detailed databases of written texts from the last 200\nyears allow for an unprecedented statistical precision. Combining data analysis\nwith simulations of simple models (e.g., the Bass dynamics on complex networks)\nwe identify signatures of endogenous and exogenous factors in the S-curves of\nadoption. We propose a measure to quantify the strength of these factors and\nthree different methods to estimate it from S-curves. We obtain cases in which\nthe exogenous factors are dominant (in the adoption of German orthographic\nreforms and of one irregular verb) and cases in which endogenous factors are\ndominant (in the adoption of conventions for romanization of Russian names and\nin the regularization of most studied verbs). These results show that the shape\nof S-curve is not universal and contains information on the adoption mechanism.\n(published at \"J. R. Soc. Interface, vol. 11, no. 101, (2014) 1044\"; DOI:\nhttp://dx.doi.org/10.1098/rsif.2014.1044)\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 19:49:03 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 16:50:53 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Ghanbarnejad", "Fakhteh", ""], ["Gerlach", "Martin", ""], ["Miotto", "Jose M.", ""], ["Altmann", "Eduardo G.", ""]]}, {"id": "1406.4690", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Mehrnoosh Sadrzadeh and Stephen Clark and Bob Coecke", "title": "The Frobenius anatomy of word meanings II: possessive relative pronouns", "comments": "40 pages, Journal of Logic and Computation, Essays dedicated to Roy\n  Dyckhoff on the occasion of his retirement, S. Graham-Lengrand and D.\n  Galmiche (eds.), 2014", "journal-ref": null, "doi": "10.1093/logcom/exu027", "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the categorical compositional distributional model of meaning, we\nprovide semantic interpretations for the subject and object roles of the\npossessive relative pronoun `whose'. This is done in terms of Frobenius\nalgebras over compact closed categories. These algebras and their diagrammatic\nlanguage expose how meanings of words in relative clauses interact with each\nother. We show how our interpretation is related to Montague-style semantics\nand provide a truth-theoretic interpretation. We also show how vector spaces\nprovide a concrete interpretation and provide preliminary corpus-based\nexperimental evidence. In a prequel to this paper, we used similar methods and\ndealt with the case of subject and object relative pronouns.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 11:54:13 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Sadrzadeh", "Mehrnoosh", ""], ["Clark", "Stephen", ""], ["Coecke", "Bob", ""]]}, {"id": "1406.4710", "submitter": "Christian Retor\\'e", "authors": "Christian Retor\\'e", "title": "Typed Hilbert Epsilon Operators and the Semantics of Determiner Phrases\n  (Invited Lecture)", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-44121-3_2", "report-no": null, "categories": "cs.CL cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantics of determiner phrases, be they definite de- scriptions,\nindefinite descriptions or quantified noun phrases, is often as- sumed to be a\nfully solved question: common nouns are properties, and determiners are\ngeneralised quantifiers that apply to two predicates: the property\ncorresponding to the common noun and the one corresponding to the verb phrase.\nWe first present a criticism of this standard view. Firstly, the semantics of\ndeterminers does not follow the syntactical structure of the sentence. Secondly\nthe standard interpretation of the indefinite article cannot ac- count for\nnominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry\nbetween the two properties of a generalised quantifier. In the sequel, we\npropose a treatment of determiners and quantifiers as Hilbert terms in a richly\ntyped system that we initially developed for lexical semantics, using a many\nsorted logic for semantical representations. We present this semantical\nframework called the Montagovian generative lexicon and show how these terms\nbetter match the syntactical structure and avoid the aforementioned problems of\nthe standard approach. Hilbert terms rather differ from choice functions in\nthat there is one polymorphic operator and not one operator per formula. They\nalso open an intriguing connection between the logic for meaning assembly, the\ntyped lambda calculus handling compositionality and the many-sorted logic for\nsemantical representations. Furthermore epsilon terms naturally introduce\ntype-judgements and confirm the claim that type judgment are a form of\npresupposition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 13:32:55 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Retor\u00e9", "Christian", ""]]}, {"id": "1406.4824", "submitter": "Nitu Kumari", "authors": "Rana D. Parshad, Vineeta Chand, Neha Sinha, Nitu Kumari", "title": "What is India speaking: The \"Hinglish\" invasion", "comments": "This paper has been withdrawan as the model has now been modified and\n  the existing model has some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While language competition models of diachronic language shift are\nincreasingly sophisticated, drawing on sociolinguistic components like variable\nlanguage prestige, distance from language centers and intermediate bilingual\ntransitionary populations, in one significant way they fall short. They fail to\nconsider contact-based outcomes resulting in mixed language practices, e.g.\noutcome scenarios such as creoles or unmarked code switching as an emergent\ncommunicative norm. On these lines something very interesting is uncovered in\nIndia, where traditionally there have been monolingual Hindi speakers and\nHindi/English bilinguals, but virtually no monolingual English speakers. While\nthe Indian census data reports a sharp increase in the proportion of\nHindi/English bilinguals, we argue that the number of Hindi/English bilinguals\nin India is inaccurate, given a new class of urban individuals speaking a mixed\nlect of Hindi and English, popularly known as \"Hinglish\". Based on\npredator-prey, sociolinguistic theories, salient local ecological factors and\nthe rural-urban divide in India, we propose a new mathematical model of\ninteracting monolingual Hindi speakers, Hindi/English bilinguals and Hinglish\nspeakers. The model yields globally asymptotic stable states of coexistence, as\nwell as bilingual extinction. To validate our model, sociolinguistic data from\ndifferent Indian classes are contrasted with census reports: We see that\npurported urban Hindi/English bilinguals are unable to maintain fluent Hindi\nspeech and instead produce Hinglish, whereas rural speakers evidence\nmonolingual Hindi. Thus we present evidence for the first time where an\nunrecognized mixed lect involving English but not \"English\", has possibly taken\nover a sizeable faction of a large global population.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 21:20:46 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 20:57:25 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Parshad", "Rana D.", ""], ["Chand", "Vineeta", ""], ["Sinha", "Neha", ""], ["Kumari", "Nitu", ""]]}, {"id": "1406.5181", "submitter": "Jake Williams", "authors": "Jake Ryland Williams, Paul R. Lessard, Suma Desu, Eric Clark, James P.\n  Bagrow, Christopher M. Danforth, and Peter Sheridan Dodds", "title": "Zipf's law holds for phrases, not words", "comments": "Manuscript: 6 pages, 3 figures; Supplementary Information: 8 pages,\n  18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With Zipf's law being originally and most famously observed for word\nfrequency, it is surprisingly limited in its applicability to human language,\nholding over no more than three to four orders of magnitude before hitting a\nclear break in scaling. Here, building on the simple observation that phrases\nof one or more words comprise the most coherent units of meaning in language,\nwe show empirically that Zipf's law for phrases extends over as many as nine\norders of rank magnitude. In doing so, we develop a principled and scalable\nstatistical mechanical method of random text partitioning, which opens up a\nrich frontier of rigorous text analysis via a rank ordering of mixed length\nphrases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 20:00:05 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 14:20:45 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Williams", "Jake Ryland", ""], ["Lessard", "Paul R.", ""], ["Desu", "Suma", ""], ["Clark", "Eric", ""], ["Bagrow", "James P.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1406.5598", "submitter": "Reshma Prasad", "authors": "Reshma Prasad and Mary Priya Sebastian", "title": "A survey on phrase structure learning methods for text classification", "comments": "14 pages, 2 figures, 2 tables, International Journal on Natural\n  Language Computing (IJNLC) Vol. 3, No.2, April 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is a task of automatic classification of text into one of\nthe predefined categories. The problem of text classification has been widely\nstudied in different communities like natural language processing, data mining\nand information retrieval. Text classification is an important constituent in\nmany information management tasks like topic identification, spam filtering,\nemail routing, language identification, genre classification, readability\nassessment etc. The performance of text classification improves notably when\nphrase patterns are used. The use of phrase patterns helps in capturing\nnon-local behaviours and thus helps in the improvement of text classification\ntask. Phrase structure extraction is the first step to continue with the phrase\npattern identification. In this survey, detailed study of phrase structure\nlearning methods have been carried out. This will enable future work in several\nNLP tasks, which uses syntactic information from phrase structure like grammar\ncheckers, question answering, information extraction, machine translation, text\nclassification. The paper also provides different levels of classification and\ndetailed comparison of the phrase structure learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 11:30:05 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Prasad", "Reshma", ""], ["Sebastian", "Mary Priya", ""]]}, {"id": "1406.5679", "submitter": "Andrej Karpathy", "authors": "Andrej Karpathy, Armand Joulin and Li Fei-Fei", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for bidirectional retrieval of images and sentences\nthrough a multi-modal embedding of visual and natural language data. Unlike\nprevious models that directly map images or sentences into a common embedding\nspace, our model works on a finer level and embeds fragments of images\n(objects) and fragments of sentences (typed dependency tree relations) into a\ncommon space. In addition to a ranking objective seen in previous work, this\nallows us to add a new fragment alignment objective that learns to directly\nassociate these fragments across modalities. Extensive experimental evaluation\nshows that reasoning on both the global level of images and sentences and the\nfiner level of their respective fragments significantly improves performance on\nimage-sentence retrieval tasks. Additionally, our model provides interpretable\npredictions since the inferred inter-modal fragment alignment is explicit.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 06:22:50 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Karpathy", "Andrej", ""], ["Joulin", "Armand", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1406.5691", "submitter": "John J. Camilleri", "authors": "John J. Camilleri, Gabriele Paganelli, Gerardo Schneider", "title": "A CNL for Contract-Oriented Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first step towards a framework for defining and manipulating\nnormative documents or contracts described as Contract-Oriented (C-O) Diagrams.\nThese diagrams provide a visual representation for such texts, giving the\npossibility to express a signatory's obligations, permissions and prohibitions,\nwith or without timing constraints, as well as the penalties resulting from the\nnon-fulfilment of a contract. This work presents a CNL for verbalising C-O\nDiagrams, a web-based tool allowing editing in this CNL, and another for\nvisualising and manipulating the diagrams interactively. We then show how these\nproof-of-concept tools can be used by applying them to a small example.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 09:41:24 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Camilleri", "John J.", ""], ["Paganelli", "Gabriele", ""], ["Schneider", "Gerardo", ""]]}, {"id": "1406.5824", "submitter": "Serena Yeung", "authors": "Serena Yeung, Alireza Fathi, and Li Fei-Fei", "title": "VideoSET: Video Summary Evaluation through Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present VideoSET, a method for Video Summary Evaluation\nthrough Text that can evaluate how well a video summary is able to retain the\nsemantic information contained in its original video. We observe that semantics\nis most easily expressed in words, and develop a text-based approach for the\nevaluation. Given a video summary, a text representation of the video summary\nis first generated, and an NLP-based metric is then used to measure its\nsemantic distance to ground-truth text summaries written by humans. We show\nthat our technique has higher agreement with human judgment than pixel-based\ndistance metrics. We also release text annotations and ground-truth text\nsummaries for a number of publicly available video datasets, for use by the\ncomputer vision community.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 07:56:23 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Yeung", "Serena", ""], ["Fathi", "Alireza", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1406.6101", "submitter": "Imen Trabelsi", "authors": "Imen Trabelsi, Dorra Ben Ayed, Noureddine Ellouze", "title": "Improved Frame Level Features and SVM Supervectors Approach for the\n  Recogniton of Emotional States from Speech: Application to categorical and\n  dimensional states", "comments": null, "journal-ref": "I.J. Image, Graphics and Signal Processing, 2013, 9, 8-13", "doi": "10.5815/ijigsp.2013.09.02", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The purpose of speech emotion recognition system is to classify speakers\nutterances into different emotional states such as disgust, boredom, sadness,\nneutral and happiness. Speech features that are commonly used in speech emotion\nrecognition rely on global utterance level prosodic features. In our work, we\nevaluate the impact of frame level feature extraction. The speech samples are\nfrom Berlin emotional database and the features extracted from these utterances\nare energy, different variant of mel frequency cepstrum coefficients, velocity\nand acceleration features.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 22:21:17 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Trabelsi", "Imen", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1406.6312", "submitter": "Ahmed El-Kishky", "authors": "Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han", "title": "Scalable Topical Phrase Mining from Text Corpora", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment, Vol. 8(3), pp. 305 - 316, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most topic modeling algorithms model text corpora with unigrams, human\ninterpretation often relies on inherent grouping of terms into phrases. As\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\nExisting work either performs post processing to the inference results of\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\nThese methods generally produce low-quality topical phrases or suffer from poor\nscalability on even moderately-sized datasets. We propose a different approach\nthat is both computationally efficient and effective. Our solution combines a\nnovel phrase mining framework to segment a document into single and multi-word\nphrases, and a new topic model that operates on the induced document partition.\nOur approach discovers high quality topical phrases with negligible extra cost\nto the bag-of-words topic model in a variety of datasets including research\npublication titles, abstracts, reviews, and news articles.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 17:10:29 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 00:18:06 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["El-Kishky", "Ahmed", ""], ["Song", "Yanglei", ""], ["Wang", "Chi", ""], ["Voss", "Clare", ""], ["Han", "Jiawei", ""]]}, {"id": "1406.6844", "submitter": "Normunds Gr\\=uz\\=itis", "authors": "Normunds Gruzitis, Peteris Paikens, Guntis Barzdins", "title": "FrameNet Resource Grammar Library for GF", "comments": null, "journal-ref": "Controlled Natural Language, Lecture Notes in Computer Science,\n  Vol. 7427, Springer, 2012, pp. 121-137", "doi": "10.1007/978-3-642-32612-7_9", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an ongoing research investigating the possibility\nand potential of integrating frame semantics, particularly FrameNet, in the\nGrammatical Framework (GF) application grammar development. An important\ncomponent of GF is its Resource Grammar Library (RGL) that encapsulates the\nlow-level linguistic knowledge about morphology and syntax of currently more\nthan 20 languages facilitating rapid development of multilingual applications.\nIn the ideal case, porting a GF application grammar to a new language would\nonly require introducing the domain lexicon - translation equivalents that are\ninterlinked via common abstract terms. While it is possible for a highly\nrestricted CNL, developing and porting a less restricted CNL requires above\naverage linguistic knowledge about the particular language, and above average\nGF experience. Specifying a lexicon is mostly straightforward in the case of\nnouns (incl. multi-word units), however, verbs are the most complex category\n(in terms of both inflectional paradigms and argument structure), and adding\nthem to a GF application grammar is not a straightforward task. In this paper\nwe are focusing on verbs, investigating the possibility of creating a\nmultilingual FrameNet-based GF library. We propose an extension to the current\nRGL, allowing GF application developers to define clauses on the semantic\nlevel, thus leaving the language-specific syntactic mapping to this extension.\nWe demonstrate our approach by reengineering the MOLTO Phrasebook application\ngrammar.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 11:14:44 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Gruzitis", "Normunds", ""], ["Paikens", "Peteris", ""], ["Barzdins", "Guntis", ""]]}, {"id": "1406.7314", "submitter": "Imen Trabelsi", "authors": "Imen Trabelsi and Dorra Ben Ayed", "title": "On the Use of Different Feature Extraction Methods for Linear and Non\n  Linear kernels", "comments": "8 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The speech feature extraction has been a key focus in robust speech\nrecognition research; it significantly affects the recognition performance. In\nthis paper, we first study a set of different features extraction methods such\nas linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC)\nand perceptual linear prediction (PLP) with several features normalization\ntechniques like rasta filtering and cepstral mean subtraction (CMS). Based on\nthis, a comparative evaluation of these features is performed on the task of\ntext independent speaker identification using a combination between gaussian\nmixture models (GMM) and linear and non-linear kernels based on support vector\nmachine (SVM).\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 20:56:00 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Trabelsi", "Imen", ""], ["Ayed", "Dorra Ben", ""]]}, {"id": "1406.7483", "submitter": "Alicia Gonz\\'alez Mart\\'inez", "authors": "Alicia Gonzalez Martinez, Susana Lopez Hervas, Doaa Samy, Carlos G.\n  Arques, Antonio Moreno Sandoval", "title": "Jabalin: a Comprehensive Computational Model of Modern Standard Arabic\n  Verbal Morphology Based on Traditional Arabic Prosody", "comments": "Jabalin implementation is available at\n  http://sourceforge.net/projects/jabalin/", "journal-ref": "Systems and Frameworks for Computational Morphology, Springer\n  Berlin Heidelberg, (2013) pp. 35-52", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The computational handling of Modern Standard Arabic is a challenge in the\nfield of natural language processing due to its highly rich morphology.\nHowever, several authors have pointed out that the Arabic morphological system\nis in fact extremely regular. The existing Arabic morphological analyzers have\nexploited this regularity to variable extent, yet we believe there is still\nsome scope for improvement. Taking inspiration in traditional Arabic prosody,\nwe have designed and implemented a compact and simple morphological system\nwhich in our opinion takes further advantage of the regularities encountered in\nthe Arabic morphological system. The output of the system is a large-scale\nlexicon of inflected forms that has subsequently been used to create an Online\nInterface for a morphological analyzer of Arabic verbs. The Jabalin Online\nInterface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the\nLLI-UAM lab. The generation system is also available under a GNU GPL 3 license.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 10:08:54 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Martinez", "Alicia Gonzalez", ""], ["Hervas", "Susana Lopez", ""], ["Samy", "Doaa", ""], ["Arques", "Carlos G.", ""], ["Sandoval", "Antonio Moreno", ""]]}, {"id": "1406.7558", "submitter": "Walter S. Lasecki", "authors": "Nicolas Fay, Monica Tamariz, T Mark Ellison, Dale Barr", "title": "Human Communication Systems Evolve by Cultural Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": "ci-2014/29", "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication systems, such as language, evolve culturally; their\ncomponents undergo reproduction and variation. However, a role for selection in\ncultural evolutionary dynamics is less clear. Often neutral evolution (also\nknown as 'drift') models, are used to explain the evolution of human\ncommunication systems, and cultural evolution more generally. Under this\naccount, cultural change is unbiased: for instance, vocabulary, baby names and\npottery designs have been found to spread through random copying.\n  While drift is the null hypothesis for models of cultural evolution it does\nnot always adequately explain empirical results. Alternative models include\ncultural selection, which assumes variant adoption is biased. Theoretical\nmodels of human communication argue that during conversation interlocutors are\nbiased to adopt the same labels and other aspects of linguistic representation\n(including prosody and syntax). This basic alignment mechanism has been\nextended by computer simulation to account for the emergence of linguistic\nconventions. When agents are biased to match the linguistic behavior of their\ninterlocutor, a single variant can propagate across an entire population of\ninteracting computer agents. This behavior-matching account operates at the\nlevel of the individual. We call it the Conformity-biased model. Under a\ndifferent selection account, called content-biased selection, functional\nselection or replicator selection, variant adoption depends upon the intrinsic\nvalue of the particular variant (e.g., ease of learning or use). This second\nalternative account operates at the level of the cultural variant. Following\nBoyd and Richerson we call it the Content-biased model. The present paper tests\nthe drift model and the two biased selection models' ability to explain the\nspread of communicative signal variants in an experimental micro-society.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jun 2014 20:16:55 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Fay", "Nicolas", ""], ["Tamariz", "Monica", ""], ["Ellison", "T Mark", ""], ["Barr", "Dale", ""]]}, {"id": "1406.7806", "submitter": "Andrew Maas", "authors": "Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T.\n  Lengerich, Daniel Jurafsky and Andrew Y. Ng", "title": "Building DNN Acoustic Models for Large Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are now a central component of nearly all\nstate-of-the-art speech recognition systems. Building neural network acoustic\nmodels requires several design decisions including network architecture, size,\nand training loss function. This paper offers an empirical investigation on\nwhich aspects of DNN acoustic model design are most important for speech\nrecognition system performance. We report DNN classifier performance and final\nspeech recognizer word error rates, and compare DNNs using several metrics to\nquantify factors influencing differences in task performance. Our first set of\nexperiments use the standard Switchboard benchmark corpus, which contains\napproximately 300 hours of conversational telephone speech. We compare standard\nDNNs to convolutional networks, and present the first experiments using\nlocally-connected, untied neural networks for acoustic modeling. We\nadditionally build systems on a corpus of 2,100 hours of training data by\ncombining the Switchboard and Fisher corpora. This larger corpus allows us to\nmore thoroughly examine performance of large DNN models -- with up to ten times\nmore parameters than those typically used in speech recognition systems. Our\nresults suggest that a relatively simple DNN architecture and optimization\ntechnique produces strong results. These findings, along with previous work,\nhelp establish a set of best practices for building DNN hybrid speech\nrecognition systems with maximum likelihood training. Our experiments in DNN\noptimization additionally serve as a case study for training DNNs with\ndiscriminative loss functions for speech tasks, as well as DNN classifiers more\ngenerally.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 16:42:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 07:44:15 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Maas", "Andrew L.", ""], ["Qi", "Peng", ""], ["Xie", "Ziang", ""], ["Hannun", "Awni Y.", ""], ["Lengerich", "Christopher T.", ""], ["Jurafsky", "Daniel", ""], ["Ng", "Andrew Y.", ""]]}]