[{"id": "1211.0074", "submitter": "Alex Rudnick", "authors": "Alex Rudnick", "title": "Transition-Based Dependency Parsing With Pluggable Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In principle, the design of transition-based dependency parsers makes it\npossible to experiment with any general-purpose classifier without other\nchanges to the parsing algorithm. In practice, however, it often takes\nsubstantial software engineering to bridge between the different\nrepresentations used by two software packages. Here we present extensions to\nMaltParser that allow the drop-in use of any classifier conforming to the\ninterface of the Weka machine learning package, a wrapper for the TiMBL\nmemory-based learner to this interface, and experiments on multilingual\ndependency parsing with a variety of classifiers. While earlier work had\nsuggested that memory-based learners might be a good choice for low-resource\nparsing scenarios, we cannot support that hypothesis in this work. We observed\nthat support-vector machines give better parsing performance than the\nmemory-based learner, regardless of the size of the training set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 02:10:06 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Rudnick", "Alex", ""]]}, {"id": "1211.0418", "submitter": "Normunds Gr\\=uz\\=itis", "authors": "Normunds Gr\\=uz\\=itis, Gunta Ne\\v{s}pore, Baiba Saul\\=ite", "title": "Verbalizing Ontologies in Controlled Baltic Languages", "comments": null, "journal-ref": "Human Language Technologies - The Baltic Perspective, Frontiers in\n  Artificial Intelligence and Applications, Vol. 219, IOS Press, 2010, pp.\n  187-194", "doi": "10.3233/978-1-60750-641-6-187", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled natural languages (mostly English-based) recently have emerged as\nseemingly informal supplementary means for OWL ontology authoring, if compared\nto the formal notations that are used by professional knowledge engineers. In\nthis paper we present by examples controlled Latvian language that has been\ndesigned to be compliant with the state of the art Attempto Controlled English.\nWe also discuss relation with controlled Lithuanian language that is being\ndesigned in parallel.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 11:09:44 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Gr\u016bz\u012btis", "Normunds", ""], ["Ne\u0161pore", "Gunta", ""], ["Saul\u012bte", "Baiba", ""]]}, {"id": "1211.0498", "submitter": "Rami Al-Rfou'", "authors": "Rami Al-Rfou'", "title": "Detecting English Writing Styles For Non-native Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing writing styles of non-native speakers is a challenging task. In\nthis paper, we analyze the comments written in the discussion pages of the\nEnglish Wikipedia. Using learning algorithms, we are able to detect native\nspeakers' writing style with an accuracy of 74%. Given the diversity of the\nEnglish Wikipedia users and the large number of languages they speak, we\nmeasure the similarities among their native languages by comparing the\ninfluence they have on their English writing style. Our results show that\nlanguages known to have the same origin and development path have similar\nfootprint on their speakers' English writing style. To enable further studies,\nthe dataset we extracted from Wikipedia will be made available publicly.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 17:37:06 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Al-Rfou'", "Rami", ""]]}, {"id": "1211.2290", "submitter": "Abhimanu Kumar", "authors": "Abhimanu Kumar, Jason Baldridge, Matthew Lease, Joydeep Ghosh", "title": "Dating Texts without Explicit Temporal Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles temporal resolution of documents, such as determining when\na document is about or when it was written, based only on its text. We apply\ntechniques from information retrieval that predict dates via language models\nover a discretized timeline. Unlike most previous works, we rely {\\it solely}\non temporal cues implicit in the text. We consider both document-likelihood and\ndivergence based techniques and several smoothing methods for both of them. Our\nbest model predicts the mid-point of individuals' lives with a median of 22 and\nmean error of 36 years for Wikipedia biographies from 3800 B.C. to the present\nday. We also show that this approach works well when training on such\nbiographies and predicting dates both for non-biographical Wikipedia pages\nabout specific years (500 B.C. to 2010 A.D.) and for publication dates of short\nstories (1798 to 2008). Together, our work shows that, even in absence of\ntemporal extraction resources, it is possible to achieve remarkable temporal\nlocality across a diverse set of texts.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 05:12:31 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Kumar", "Abhimanu", ""], ["Baldridge", "Jason", ""], ["Lease", "Matthew", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1211.2741", "submitter": "Venkateshwara Prasad Tangirala", "authors": "Kamlesh Sharma, S. V. A. V. Prasad and T. V. Prasad", "title": "A Hindi Speech Actuated Computer Interface for Web Search", "comments": "7 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications 3(10), 2012, 147-152", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at increasing system simplicity and flexibility, an audio evoked based\nsystem was developed by integrating simplified headphone and user-friendly\nsoftware design. This paper describes a Hindi Speech Actuated Computer\nInterface for Web search (HSACIWS), which accepts spoken queries in Hindi\nlanguage and provides the search result on the screen. This system recognizes\nspoken queries by large vocabulary continuous speech recognition (LVCSR),\nretrieves relevant document by text retrieval, and provides the search result\non the Web by the integration of the Web and the voice systems. The LVCSR in\nthis system showed enough performance levels for speech with acoustic and\nlanguage models derived from a query corpus with target contents.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 19:17:34 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Sharma", "Kamlesh", ""], ["Prasad", "S. V. A. V.", ""], ["Prasad", "T. V.", ""]]}, {"id": "1211.3402", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Genetic Optimization of Keywords Subset in the Classification Analysis\n  of Texts Authorship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genetic selection of keywords set, the text frequencies of which are\nconsidered as attributes in text classification analysis, has been analyzed.\nThe genetic optimization was performed on a set of words, which is the fraction\nof the frequency dictionary with given frequency limits. The frequency\ndictionary was formed on the basis of analyzed text array of texts of English\nfiction. As the fitness function which is minimized by the genetic algorithm,\nthe error of nearest k neighbors classifier was used. The obtained results show\nhigh precision and recall of texts classification by authorship categories on\nthe basis of attributes of keywords set which were selected by the genetic\nalgorithm from the frequency dictionary.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 20:04:51 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1211.3643", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn", "title": "A Principled Approach to Grammars for Controlled Natural Languages and\n  Predictive Editors", "comments": null, "journal-ref": "Journal of Logic, Language and Information, 22(1), 2013", "doi": "10.1007/s10849-012-9167-z", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled natural languages (CNL) with a direct mapping to formal logic have\nbeen proposed to improve the usability of knowledge representation systems,\nquery interfaces, and formal specifications. Predictive editors are a popular\napproach to solve the problem that CNLs are easy to read but hard to write.\nSuch predictive editors need to be able to \"look ahead\" in order to show all\npossible continuations of a given unfinished sentence. Such lookahead features,\nhowever, are difficult to implement in a satisfying way with existing grammar\nframeworks, especially if the CNL supports complex nonlocal structures such as\nanaphoric references. Here, methods and algorithms are presented for a new\ngrammar notation called Codeco, which is specifically designed for controlled\nnatural languages and predictive editors. A parsing approach for Codeco based\non an extended chart parsing algorithm is presented. A large subset of Attempto\nControlled English (ACE) has been represented in Codeco. Evaluation of this\ngrammar and the parser implementation shows that the approach is practical,\nadequate and efficient.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 16:24:25 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Kuhn", "Tobias", ""]]}, {"id": "1211.4161", "submitter": "Lingu LIGM", "authors": "Ae-Lim Ahn (DICORA), \\'Eric Laporte (LIGM), Jee-Sun Nam (DICORA, LIGM)", "title": "Semantic Polarity of Adjectival Predicates in Online Reviews", "comments": "electronic version (10 pp.)", "journal-ref": "Seoul International Conference on Linguistics (SICOL'10), Seoul :\n  Korea, Republic Of (2010)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web users produce more and more documents expressing opinions. Because these\nhave become important resources for customers and manufacturers, many have\nfocused on them. Opinions are often expressed through adjectives with positive\nor negative semantic values. In extracting information from users' opinion in\nonline reviews, exact recognition of the semantic polarity of adjectives is one\nof the most important requirements. Since adjectives have different semantic\norientations according to contexts, it is not satisfying to extract opinion\ninformation without considering the semantic and lexical relations between the\nadjectives and the feature nouns appropriate to a given domain. In this paper,\nwe present a classification of adjectives by polarity, and we analyze\nadjectives that are undetermined in the absence of contexts. Our research\nshould be useful for accurately predicting semantic orientations of opinion\nsentences, and should be taken into account before relying on an automatic\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 20:27:06 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Ahn", "Ae-Lim", "", "DICORA"], ["Laporte", "\u00c9ric", "", "LIGM"], ["Nam", "Jee-Sun", "", "DICORA, LIGM"]]}, {"id": "1211.4488", "submitter": "Jessica Ram\\'irez", "authors": "Jessica C. Ram\\'irez and Yuji Matsumoto", "title": "A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A\n  Comparable Corpora", "comments": "International Journal on Natural Language Computing (IJNLC) Vol.1,\n  No.3, October 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The performance of a Statistical Machine Translation System (SMT) system is\nproportionally directed to the quality and length of the parallel corpus it\nuses. However for some pair of languages there is a considerable lack of them.\nThe long term goal is to construct a Japanese-Spanish parallel corpus to be\nused for SMT, whereas, there are a lack of useful Japanese-Spanish parallel\nCorpus. To address this problem, In this study we proposed a method for\nextracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging\nand Rule-Based approach. The main focus of this approach is the syntactic\nfeatures of both languages. Human evaluation was performed over a sample and\nshows promising results, in comparison with the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 16:38:32 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Ram\u00edrez", "Jessica C.", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1211.4929", "submitter": "Trung Nguyen", "authors": "Trung V. Nguyen and Alice H. Oh", "title": "Summarizing Reviews with Variable-length Syntactic Patterns and Topic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel summarization framework for reviews of products and\nservices by selecting informative and concise text segments from the reviews.\nOur method consists of two major steps. First, we identify five frequently\noccurring variable-length syntactic patterns and use them to extract candidate\nsegments. Then we use the output of a joint generative sentiment topic model to\nfilter out the non-informative segments. We verify the proposed method with\nquantitative and qualitative experiments. In a quantitative study, our approach\noutperforms previous methods in producing informative segments and summaries\nthat capture aspects of products and services as expressed in the\nuser-generated pros and cons lists. Our user study with ninety users resonates\nwith this result: individual segments extracted and filtered by our method are\nrated as more useful by users compared to previous approaches by users.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 03:59:06 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Nguyen", "Trung V.", ""], ["Oh", "Alice H.", ""]]}, {"id": "1211.6847", "submitter": "Bernard Ycart", "authors": "Bernard Ycart (LJK)", "title": "Letter counting: a stem cell for Cryptology, Quantitative Linguistics,\n  and Statistics", "comments": null, "journal-ref": "Historiographia Linguistica 40, 3 (2013) 303-329", "doi": null, "report-no": null, "categories": "math.HO cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting letters in written texts is a very ancient practice. It has\naccompanied the development of Cryptology, Quantitative Linguistics, and\nStatistics. In Cryptology, counting frequencies of the different characters in\nan encrypted message is the basis of the so called frequency analysis method.\nIn Quantitative Linguistics, the proportion of vowels to consonants in\ndifferent languages was studied long before authorship attribution. In\nStatistics, the alternation vowel-consonants was the only example that Markov\never gave of his theory of chained events. A short history of letter counting\nis presented. The three domains, Cryptology, Quantitative Linguistics, and\nStatistics, are then examined, focusing on the interactions with the other two\nfields through letter counting. As a conclusion, the eclectism of past\ncenturies scholars, their background in humanities, and their familiarity with\ncryptograms, are identified as contributing factors to the mutual enrichment\nprocess which is described here.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:00:59 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Ycart", "Bernard", "", "LJK"]]}, {"id": "1211.6887", "submitter": "Marcin Mi{\\l}kowski", "authors": "Marcin Mi{\\l}kowski", "title": "Automating rule generation for grammar checkers", "comments": "Draft of the chapter published In: Explorations Across Languages and\n  Corpora. PALC 2009, ed. by S. Go\\'zd\\'z-Roszkowski, Peter Lang, 2011, p.\n  123-133", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I describe several approaches to automatic or semi-automatic\ndevelopment of symbolic rules for grammar checkers from the information\ncontained in corpora. The rules obtained this way are an important addition to\nmanually-created rules that seem to dominate in rule-based checkers. However,\nthe manual process of creation of rules is costly, time-consuming and\nerror-prone. It seems therefore advisable to use machine-learning algorithms to\ncreate the rules automatically or semi-automatically. The results obtained seem\nto corroborate my initial hypothesis that symbolic machine learning algorithms\ncan be useful for acquiring new rules for grammar checking. It turns out,\nhowever, that for practical uses, error corpora cannot be the sole source of\ninformation used in grammar checking. I suggest therefore that only by using\ndifferent approaches, grammar-checkers, or more generally, computer-aided\nproofreading tools, will be able to cover most frequent and severe mistakes and\navoid false alarms that seem to distract users.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 11:35:25 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Mi\u0142kowski", "Marcin", ""]]}]