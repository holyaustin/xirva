[{"id": "1302.0393", "submitter": "Edward Grefenstette", "authors": "Bob Coecke, Edward Grefenstette, and Mehrnoosh Sadrzadeh", "title": "Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams\n  for Lambek Calculus", "comments": "29 pages, pending publication in Annals of Pure and Applied Logic", "journal-ref": null, "doi": "10.1016/j.apal.2013.05.009", "report-no": null, "categories": "math.LO cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Distributional Compositional Categorical (DisCoCat) model is a\nmathematical framework that provides compositional semantics for meanings of\nnatural language sentences. It consists of a computational procedure for\nconstructing meanings of sentences, given their grammatical structure in terms\nof compositional type-logic, and given the empirically derived meanings of\ntheir words. For the particular case that the meaning of words is modelled\nwithin a distributional vector space model, its experimental predictions,\nderived from real large scale data, have outperformed other empirically\nvalidated methods that could build vectors for a full sentence. This success\ncan be attributed to a conceptually motivated mathematical underpinning, by\nintegrating qualitative compositional type-logic and quantitative modelling of\nmeaning within a category-theoretic mathematical framework.\n  The type-logic used in the DisCoCat model is Lambek's pregroup grammar.\nPregroup types form a posetal compact closed category, which can be passed, in\na functorial manner, on to the compact closed structure of vector spaces,\nlinear maps and tensor product. The diagrammatic versions of the equational\nreasoning in compact closed categories can be interpreted as the flow of word\nmeanings within sentences. Pregroups simplify Lambek's previous type-logic, the\nLambek calculus, which has been extensively used to formalise and reason about\nvarious linguistic phenomena. The apparent reliance of the DisCoCat on\npregroups has been seen as a shortcoming. This paper addresses this concern, by\npointing out that one may as well realise a functorial passage from the\noriginal type-logic of Lambek, a monoidal bi-closed category, to vector spaces,\nor to any other model of meaning organised within a monoidal bi-closed\ncategory. The corresponding string diagram calculus, due to Baez and Stay, now\ndepicts the flow of word meanings.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 15:57:26 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Coecke", "Bob", ""], ["Grefenstette", "Edward", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1302.1123", "submitter": "Ciprian Chelba", "authors": "Ciprian Chelba, Peng Xu, Fernando Pereira, Thomas Richardson", "title": "Large Scale Distributed Acoustic Modeling With Back-off N-grams", "comments": null, "journal-ref": null, "doi": "10.1109/TASL.2013.2245649", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper revives an older approach to acoustic modeling that borrows from\nn-gram language modeling in an attempt to scale up both the amount of training\ndata and model size (as measured by the number of parameters in the model), to\napproximately 100 times larger than current sizes used in automatic speech\nrecognition. In such a data-rich setting, we can expand the phonetic context\nsignificantly beyond triphones, as well as increase the number of Gaussian\nmixture components for the context-dependent states that allow it. We have\nexperimented with contexts that span seven or more context-independent phones,\nand up to 620 mixture components per state. Dealing with unseen phonetic\ncontexts is accomplished using the familiar back-off technique used in language\nmodeling due to implementation simplicity. The back-off acoustic model is\nestimated, stored and served using MapReduce distributed computing\ninfrastructure.\n  Speech recognition experiments are carried out in an N-best list rescoring\nframework for Google Voice Search. Training big models on large amounts of data\nproves to be an effective way to increase the accuracy of a state-of-the-art\nautomatic speech recognition system. We use 87,000 hours of training data\n(speech along with transcription) obtained by filtering utterances in Voice\nSearch logs on automatic speech recognition confidence. Models ranging in size\nbetween 20--40 million Gaussians are estimated using maximum likelihood\ntraining. They achieve relative reductions in word-error-rate of 11% and 6%\nwhen combined with first-pass models trained using maximum likelihood, and\nboosted maximum mutual information, respectively. Increasing the context size\nbeyond five phones (quinphones) does not help.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 17:09:49 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Chelba", "Ciprian", ""], ["Xu", "Peng", ""], ["Pereira", "Fernando", ""], ["Richardson", "Thomas", ""]]}, {"id": "1302.1380", "submitter": "Catarina Moreira", "authors": "Catarina Moreira and Ana Cristina Mendes and Lu\\'isa Coheur and Bruno\n  Martins", "title": "Towards the Rapid Development of a Natural Language Understanding Module", "comments": null, "journal-ref": "In Proceedings of the 11th International Conference on Intelligent\n  Virtual Agents, 2011", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When developing a conversational agent, there is often an urgent need to have\na prototype available in order to test the application with real users. A\nWizard of Oz is a possibility, but sometimes the agent should be simply\ndeployed in the environment where it will be used. Here, the agent should be\nable to capture as many interactions as possible and to understand how people\nreact to failure. In this paper, we focus on the rapid development of a natural\nlanguage understanding module by non experts. Our approach follows the learning\nparadigm and sees the process of understanding natural language as a\nclassification problem. We test our module with a conversational agent that\nanswers questions in the art domain. Moreover, we show how our approach can be\nused by a natural language interface to a cinema database.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 14:17:55 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Moreira", "Catarina", ""], ["Mendes", "Ana Cristina", ""], ["Coheur", "Lu\u00edsa", ""], ["Martins", "Bruno", ""]]}, {"id": "1302.1422", "submitter": "Christian Retore", "authors": "Christian Retor\\'e (LaBRI, IRIT)", "title": "S\\'emantique des d\\'eterminants dans un cadre richement typ\\'e", "comments": null, "journal-ref": "Traitement Automatique du Langage Naturel 2013, Les Sables\n  d'Olonnes : France (2013)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variation of word meaning according to the context leads us to enrich the\ntype system of our syntactical and semantic analyser of French based on\ncategorial grammars and Montague semantics (or lambda-DRT). The main advantage\nof a deep semantic analyse is too represent meaning by logical formulae that\ncan be easily used e.g. for inferences. Determiners and quantifiers play a\nfundamental role in the construction of those formulae. But in our rich type\nsystem the usual semantic terms do not work. We propose a solution ins- pired\nby the tau and epsilon operators of Hilbert, kinds of generic elements and\nchoice functions. This approach unifies the treatment of the different determi-\nners and quantifiers as well as the dynamic binding of pronouns. Above all,\nthis fully computational view fits in well within the wide coverage parser\nGrail, both from a theoretical and a practical viewpoint.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 16:26:49 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 14:17:49 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Retor\u00e9", "Christian", "", "LaBRI, IRIT"]]}, {"id": "1302.1572", "submitter": "Ian Thomas", "authors": "Ian Thomas, Ingrid Zukerman, Jonathan Oliver, David Albrecht, Bhavani\n  Raskutti", "title": "Lexical Access for Speech Understanding using Minimum Message Length\n  Encoding", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-464-471", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lexical Access Problem consists of determining the intended sequence of\nwords corresponding to an input sequence of phonemes (basic speech sounds) that\ncome from a low-level phoneme recognizer. In this paper we present an\ninformation-theoretic approach based on the Minimum Message Length Criterion\nfor solving the Lexical Access Problem. We model sentences using phoneme\nrealizations seen in training, and word and part-of-speech information obtained\nfrom text corpora. We show results on multiple-speaker, continuous, read speech\nand discuss a heuristic using equivalence classes of similar sounding words\nwhich speeds up the recognition process without significant deterioration in\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:59:24 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Thomas", "Ian", ""], ["Zukerman", "Ingrid", ""], ["Oliver", "Jonathan", ""], ["Albrecht", "David", ""], ["Raskutti", "Bhavani", ""]]}, {"id": "1302.1612", "submitter": "Hanane Froud", "authors": "Hanane Froud, Abdelmonaime Lachkar, Said Alaoui Ouatik", "title": "Arabic text summarization based on latent semantic analysis to enhance\n  arabic documents clustering", "comments": null, "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP)- 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic Documents Clustering is an important task for obtaining good results\nwith the traditional Information Retrieval (IR) systems especially with the\nrapid growth of the number of online documents present in Arabic language.\nDocuments clustering aim to automatically group similar documents in one\ncluster using different similarity/distance measures. This task is often\naffected by the documents length, useful information on the documents is often\naccompanied by a large amount of noise, and therefore it is necessary to\neliminate this noise while keeping useful information to boost the performance\nof Documents clustering. In this paper, we propose to evaluate the impact of\ntext summarization using the Latent Semantic Analysis Model on Arabic Documents\nClustering in order to solve problems cited above, using five\nsimilarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard\nCoefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler\nDivergence, for two times: without and with stemming. Our experimental results\nindicate that our proposed approach effectively solves the problems of noisy\ninformation and documents length, and thus significantly improve the clustering\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 23:24:37 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Froud", "Hanane", ""], ["Lachkar", "Abdelmonaime", ""], ["Ouatik", "Said Alaoui", ""]]}, {"id": "1302.2131", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Data Mining of the Concept \"End of the World\" in Twitter Microblogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the analysis of quantitative characteristics of frequent\nsets and association rules in the posts of Twitter microblogs, related to the\ndiscussion of \"end of the world\", which was allegedly predicted on December 21,\n2012 due to the Mayan calendar. Discovered frequent sets and association rules\ncharacterize semantic relations between the concepts of analyzed subjects.The\nsupport for some fequent sets reaches the global maximum before the expected\nevent with some time delay. Such frequent sets may be considered as predictive\nmarkers that characterize the significance of expected events for blogosphere\nusers. It was shown that time dynamics of confidence of some revealed\nassociation rules can also have predictive characteristics. Exceeding a certain\nthreshold, it may be a signal for the corresponding reaction in the society\nduring the time interval between the maximum and probable coming of an event.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 19:56:43 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1302.2569", "submitter": "Olivier Catoni", "authors": "Olivier Catoni and Thomas Mainguy", "title": "Toric grammars: a new statistical approach to natural language modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical model for computational linguistics. Rather than\ntrying to estimate directly the probability distribution of a random sentence\nof the language, we define a Markov chain on finite sets of sentences with many\nfinite recurrent communicating classes and define our language model as the\ninvariant probability measures of the chain on each recurrent communicating\nclass. This Markov chain, that we call a communication model, recombines at\neach step randomly the set of sentences forming its current state, using some\ngrammar rules. When the grammar rules are fixed and known in advance instead of\nbeing estimated on the fly, we can prove supplementary mathematical properties.\nIn particular, we can prove in this case that all states are recurrent states,\nso that the chain defines a partition of its state space into finite recurrent\ncommunicating classes. We show that our approach is a decisive departure from\nMarkov models at the sentence level and discuss its relationships with Context\nFree Grammars. Although the toric grammars we use are closely related to\nContext Free Grammars, the way we generate the language from the grammar is\nqualitatively different. Our communication model has two purposes. On the one\nhand, it is used to define indirectly the probability distribution of a random\nsentence of the language. On the other hand it can serve as a (crude) model of\nlanguage transmission from one speaker to another speaker through the\ncommunication of a (large) set of sentences.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 18:51:03 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Catoni", "Olivier", ""], ["Mainguy", "Thomas", ""]]}, {"id": "1302.3057", "submitter": "Jacob Dlougach", "authors": "Jacob Dlougach, Irina Galinskaya", "title": "Building a reordering system using tree-to-string hierarchical model", "comments": "8 pages + references, published in Proceedings of COLING 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to the First Workshop on Reordering for\nStatistical Machine Translation. We have decided to build a reordering system\nbased on tree-to-string model, using only publicly available tools to\naccomplish this task. With the provided training data we have built a\ntranslation model using Moses toolkit, and then we applied a chart decoder,\nimplemented in Moses, to reorder the sentences. Even though our submission only\ncovered English-Farsi language pair, we believe that the approach itself should\nwork regardless of the choice of the languages, so we have also carried out the\nexperiments for English-Italian and English-Urdu. For these language pairs we\nhave noticed a significant improvement over the baseline in BLEU, Kendall-Tau\nand Hamming metrics. A detailed description is given, so that everyone can\nreproduce our results. Also, some possible directions for further improvements\nare discussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 11:54:56 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Dlougach", "Jacob", ""], ["Galinskaya", "Irina", ""]]}, {"id": "1302.3831", "submitter": "Diederik Aerts", "authors": "Diederik Aerts and Sandro Sozzo", "title": "Quantum Entanglement in Concept Combinations", "comments": "16 pages, no figures", "journal-ref": "International Journal of Theoretical Physics, 53, pp. 3587-3603,\n  2014", "doi": "10.1007/s10773-013-1946-z", "report-no": null, "categories": "cs.AI cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the application of quantum structures to cognitive science\nconfirms that these structures quite systematically appear in the dynamics of\nconcepts and their combinations and quantum-based models faithfully represent\nexperimental data of situations where classical approaches are problematical.\nIn this paper, we analyze the data we collected in an experiment on a specific\nconceptual combination, showing that Bell's inequalities are violated in the\nexperiment. We present a new refined entanglement scheme to model these data\nwithin standard quantum theory rules, where 'entangled measurements and\nentangled evolutions' occur, in addition to the expected 'entangled states',\nand present a full quantum representation in complex Hilbert space of the data.\nThis stronger form of entanglement in measurements and evolutions might have\nrelevant applications in the foundations of quantum theory, as well as in the\ninterpretation of nonlocality tests. It could indeed explain some\nnon-negligible 'anomalies' identified in EPR-Bell experiments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 18:20:25 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 17:51:50 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1302.3892", "submitter": "Adilson Enio Motter", "authors": "Eduardo G. Altmann, Zakary L. Whichard, Adilson E. Motter", "title": "Identifying trends in word frequency dynamics", "comments": null, "journal-ref": "J. Stat. Phys. 151, p. 277 (2013)", "doi": "10.1007/s10955-013-0699-7", "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.CL q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word-stock of a language is a complex dynamical system in which words can\nbe created, evolve, and become extinct. Even more dynamic are the short-term\nfluctuations in word usage by individuals in a population. Building on the\nrecent demonstration that word niche is a strong determinant of future rise or\nfall in word frequency, here we introduce a model that allows us to distinguish\npersistent from temporary increases in frequency. Our model is illustrated\nusing a 10^8-word database from an online discussion group and a 10^11-word\ncollection of digitized books. The model reveals a strong relation between\nchanges in word dissemination and changes in frequency. Aside from their\nimplications for short-term word frequency dynamics, these observations are\npotentially important for language evolution as new words must survive in the\nshort term in order to survive in the long term.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 21:22:10 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Altmann", "Eduardo G.", ""], ["Whichard", "Zakary L.", ""], ["Motter", "Adilson E.", ""]]}, {"id": "1302.4383", "submitter": "Armen Allahverdyan", "authors": "Armen E. Allahverdyan, Weibing Deng, and Q. A. Wang", "title": "Explaining Zipf's Law via Mental Lexicon", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.88.062804", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zipf's law is the major regularity of statistical linguistics that served\nas a prototype for rank-frequency relations and scaling laws in natural\nsciences. Here we show that the Zipf's law -- together with its applicability\nfor a single text and its generalizations to high and low frequencies including\nhapax legomena -- can be derived from assuming that the words are drawn into\nthe text with random probabilities. Their apriori density relates, via the\nBayesian statistics, to general features of the mental lexicon of the author\nwho produced the text.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:38:05 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Allahverdyan", "Armen E.", ""], ["Deng", "Weibing", ""], ["Wang", "Q. A.", ""]]}, {"id": "1302.4465", "submitter": "Diego  Amancio Raphael", "authors": "Diego R. Amancio, Osvaldo N. Oliveira Jr. and Luciano da F. Costa", "title": "Unveiling the relationship between complex networks metrics and word\n  senses", "comments": "The Supplementary Information (SI) is available from\n  http://dl.dropbox.com/u/2740286/epl_SI.pdf", "journal-ref": "Europhysics Letters (2012) 98 18002", "doi": "10.1209/0295-5075/98/18002", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic disambiguation of word senses (i.e., the identification of\nwhich of the meanings is used in a given context for a word that has multiple\nmeanings) is essential for such applications as machine translation and\ninformation retrieval, and represents a key step for developing the so-called\nSemantic Web. Humans disambiguate words in a straightforward fashion, but this\ndoes not apply to computers. In this paper we address the problem of Word Sense\nDisambiguation (WSD) by treating texts as complex networks, and show that word\nsenses can be distinguished upon characterizing the local structure around\nambiguous words. Our goal was not to obtain the best possible disambiguation\nsystem, but we nevertheless found that in half of the cases our approach\noutperforms traditional shallow methods. We show that the hierarchical\nconnectivity and clustering of words are usually the most relevant features for\nWSD. The results reported here shine light on the relationship between semantic\nand structural parameters of complex networks. They also indicate that when\ncombined with traditional techniques the complex network approach may be useful\nto enhance the discrimination of senses in large texts\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 21:34:59 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Amancio", "Diego R.", ""], ["Oliveira", "Osvaldo N.", "Jr."], ["Costa", "Luciano da F.", ""]]}, {"id": "1302.4471", "submitter": "Diego  Amancio Raphael", "authors": "Thiago C. Silva and Diego R. Amancio", "title": "Word sense disambiguation via high order of learning in complex networks", "comments": "The Supplementary Information (SI) is hosted at\n  http://dl.dropbox.com/u/2740286/epl_SI_9apr.pdf", "journal-ref": "Europhysics Letters (2012) 98 58001", "doi": "10.1209/0295-5075/98/58001", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks have been employed to model many real systems and as a\nmodeling tool in a myriad of applications. In this paper, we use the framework\nof complex networks to the problem of supervised classification in the word\ndisambiguation task, which consists in deriving a function from the supervised\n(or labeled) training data of ambiguous words. Traditional supervised data\nclassification takes into account only topological or physical features of the\ninput data. On the other hand, the human (animal) brain performs both low- and\nhigh-level orders of learning and it has facility to identify patterns\naccording to the semantic meaning of the input data. In this paper, we apply a\nhybrid technique which encompasses both types of learning in the field of word\nsense disambiguation and show that the high-level order of learning can really\nimprove the accuracy rate of the model. This evidence serves to demonstrate\nthat the internal structures formed by the words do present patterns that,\ngenerally, cannot be correctly unveiled by only traditional techniques.\nFinally, we exhibit the behavior of the model for different weights of the low-\nand high-level classifiers by plotting decision boundaries. This study helps\none to better understand the effectiveness of the model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 22:06:52 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Silva", "Thiago C.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1302.4489", "submitter": "Chengzhi Zhang", "authors": "Sa Liu and Chengzhi Zhang", "title": "Termhood-based Comparability Metrics of Comparable Corpus in Special\n  Domain", "comments": null, "journal-ref": "Lecture Notes in Computer Science Volume 7717, 2013, pp 134-144", "doi": "10.1007/978-3-642-36337-5_15", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-Language Information Retrieval (CLIR) and machine translation (MT)\nresources, such as dictionaries and parallel corpora, are scarce and hard to\ncome by for special domains. Besides, these resources are just limited to a few\nlanguages, such as English, French, and Spanish and so on. So, obtaining\ncomparable corpora automatically for such domains could be an answer to this\nproblem effectively. Comparable corpora, that the subcorpora are not\ntranslations of each other, can be easily obtained from web. Therefore,\nbuilding and using comparable corpora is often a more feasible option in\nmultilingual information processing. Comparability metrics is one of key issues\nin the field of building and using comparable corpus. Currently, there is no\nwidely accepted definition or metrics method of corpus comparability. In fact,\nDifferent definitions or metrics methods of comparability might be given to\nsuit various tasks about natural language processing. A new comparability,\nnamely, termhood-based metrics, oriented to the task of bilingual terminology\nextraction, is proposed in this paper. In this method, words are ranked by\ntermhood not frequency, and then the cosine similarities, calculated based on\nthe ranking lists of word termhood, is used as comparability. Experiments\nresults show that termhood-based metrics performs better than traditional\nfrequency-based metrics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 00:30:57 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Liu", "Sa", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "1302.4490", "submitter": "Diego  Amancio Raphael", "authors": "Diego R. Amancio, Sandra M. Aluisio, Osvaldo N. Oliveira Jr. and\n  Luciano da F. Costa", "title": "Complex networks analysis of language complexity", "comments": "The Supplementary Information (SI) is available from\n  https://dl.dropbox.com/u/2740286/supplementary.pdf", "journal-ref": "Europhysics Letters (2012) 100 58002", "doi": "10.1209/0295-5075/100/58002", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from statistical physics, such as those involving complex networks,\nhave been increasingly used in quantitative analysis of linguistic phenomena.\nIn this paper, we represented pieces of text with different levels of\nsimplification in co-occurrence networks and found that topological regularity\ncorrelated negatively with textual complexity. Furthermore, in less complex\ntexts the distance between concepts, represented as nodes, tended to decrease.\nThe complex networks metrics were treated with multivariate pattern recognition\ntechniques, which allowed us to distinguish between original texts and their\nsimplified versions. For each original text, two simplified versions were\ngenerated manually with increasing number of simplification operations. As\nexpected, distinction was easier for the strongly simplified versions, where\nthe most relevant metrics were node strength, shortest paths and diversity.\nAlso, the discrimination of complex texts was improved with higher hierarchical\nnetwork metrics, thus pointing to the usefulness of considering wider contexts\naround the concepts. Though the accuracy rate in the distinction was not as\nhigh as in methods using deep linguistic knowledge, the complex network\napproach is still useful for a rapid screening of texts whenever assessing\ncomplexity is essential to guarantee accessibility to readers with limited\nreading ability\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 00:32:22 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Amancio", "Diego R.", ""], ["Aluisio", "Sandra M.", ""], ["Oliveira", "Osvaldo N.", "Jr."], ["Costa", "Luciano da F.", ""]]}, {"id": "1302.4492", "submitter": "Chengzhi Zhang", "authors": "Chengzhi Zhang and Dan Wu", "title": "Bilingual Terminology Extraction Using Multi-level Termhood", "comments": null, "journal-ref": "Electronic Library, The, Vol. 30 Iss: 2, 2012, pp.295 - 309", "doi": "10.1108/02640471211221395", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Terminology is the set of technical words or expressions used in\nspecific contexts, which denotes the core concept in a formal discipline and is\nusually applied in the fields of machine translation, information retrieval,\ninformation extraction and text categorization, etc. Bilingual terminology\nextraction plays an important role in the application of bilingual dictionary\ncompilation, bilingual Ontology construction, machine translation and\ncross-language information retrieval etc. This paper addresses the issues of\nmonolingual terminology extraction and bilingual term alignment based on\nmulti-level termhood.\n  Design/methodology/approach: A method based on multi-level termhood is\nproposed. The new method computes the termhood of the terminology candidate as\nwell as the sentence that includes the terminology by the comparison of the\ncorpus. Since terminologies and general words usually have differently\ndistribution in the corpus, termhood can also be used to constrain and enhance\nthe performance of term alignment when aligning bilingual terms on the parallel\ncorpus. In this paper, bilingual term alignment based on termhood constraints\nis presented.\n  Findings: Experiment results show multi-level termhood can get better\nperformance than existing method for terminology extraction. If termhood is\nused as constrain factor, the performance of bilingual term alignment can be\nimproved.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 00:37:21 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Zhang", "Chengzhi", ""], ["Wu", "Dan", ""]]}, {"id": "1302.4619", "submitter": "Dmitry Lande", "authors": "D.V. Lande, A.A.Snarskii", "title": "Compactified Horizontal Visibility Graph for the Language Network", "comments": "9 pages, 3 figures, 2 appendix tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compactified horizontal visibility graph for the language network is\nproposed. It was found that the networks constructed in such way are scale\nfree, and have a property that among the nodes with largest degrees there are\nwords that determine not only a text structure communication, but also its\ninformational structure.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 14:32:17 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Lande", "D. V.", ""], ["Snarskii", "A. A.", ""]]}, {"id": "1302.4726", "submitter": "Khalil Riad Bouzidi", "authors": "Khalil Riad Bouzidi (INRIA Sophia Antipolis / Laboratoire I3S), Bruno\n  Fies (CSTB Sophia Antipolis), Marc Bourdeau (CSTB Sophia Antipolis),\n  Catherine Faron-Zucker (INRIA Sophia Antipolis / Laboratoire I3S), Nhan\n  Le-Thanh (I3S)", "title": "An Ontology for Modelling and Supporting the Process of Authoring\n  Technical Assessments", "comments": "In the International Council for Building Conference, CIB 2011 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a semantic web approach for modelling the process\nof creating new technical and regulatory documents related to the Building\nsector. This industry, among other industries, is currently experiencing a\nphenomenal growth in its technical and regulatory texts. Therefore, it is\nurgent and crucial to improve the process of creating regulations by automating\nit as much as possible. We focus on the creation of particular technical\ndocuments issued by the French Scientific and Technical Centre for Building\n(CSTB), called Technical Assessments, and we propose services based on Semantic\nWeb models and techniques for modelling the process of their creation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 20:09:08 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Bouzidi", "Khalil Riad", "", "INRIA Sophia Antipolis / Laboratoire I3S"], ["Fies", "Bruno", "", "CSTB Sophia Antipolis"], ["Bourdeau", "Marc", "", "CSTB Sophia Antipolis"], ["Faron-Zucker", "Catherine", "", "INRIA Sophia Antipolis / Laboratoire I3S"], ["Le-Thanh", "Nhan", "", "I3S"]]}, {"id": "1302.4811", "submitter": "Khalil Riad Bouzidi", "authors": "Khalil Riad Bouzidi (INRIA Sophia Antipolis / Laboratoire I3S),\n  Catherine Faron-Zucker (INRIA Sophia Antipolis / Laboratoire I3S), Bruno Fies\n  (CSTB Sophia Antipolis), Olivier Corby (INRIA Sophia Antipolis / Laboratoire\n  I3S), Le-Thanh Nhan (I3S)", "title": "Towards a Semantic-based Approach for Modeling Regulatory Documents in\n  Building Industry", "comments": null, "journal-ref": "9th European Conference on Product \\& Process Modelling, ECPPM\n  2012 (2012)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulations in the Building Industry are becoming increasingly complex and\ninvolve more than one technical area. They cover products, components and\nproject implementation. They also play an important role to ensure the quality\nof a building, and to minimize its environmental impact. In this paper, we are\nparticularly interested in the modeling of the regulatory constraints derived\nfrom the Technical Guides issued by CSTB and used to validate Technical\nAssessments. We first describe our approach for modeling regulatory constraints\nin the SBVR language, and formalizing them in the SPARQL language. Second, we\ndescribe how we model the processes of compliance checking described in the\nCSTB Technical Guides. Third, we show how we implement these processes to\nassist industrials in drafting Technical Documents in order to acquire a\nTechnical Assessment; a compliance report is automatically generated to explain\nthe compliance or noncompliance of this Technical Documents.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 05:46:53 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Bouzidi", "Khalil Riad", "", "INRIA Sophia Antipolis / Laboratoire I3S"], ["Faron-Zucker", "Catherine", "", "INRIA Sophia Antipolis / Laboratoire I3S"], ["Fies", "Bruno", "", "CSTB Sophia Antipolis"], ["Corby", "Olivier", "", "INRIA Sophia Antipolis / Laboratoire\n  I3S"], ["Nhan", "Le-Thanh", "", "I3S"]]}, {"id": "1302.4813", "submitter": "Jackie Chi Kit Cheung", "authors": "Jackie Chi Kit Cheung, Hoifung Poon, Lucy Vanderwende", "title": "Probabilistic Frame Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural-language discourse, related events tend to appear near each other\nto describe a larger scenario. Such structures can be formalized by the notion\nof a frame (a.k.a. template), which comprises a set of related events and\nprototypical participants and event transitions. Identifying frames is a\nprerequisite for information extraction and natural language generation, and is\nusually done manually. Methods for inducing frames have been proposed recently,\nbut they typically use ad hoc procedures and are difficult to diagnose or\nextend. In this paper, we propose the first probabilistic approach to frame\ninduction, which incorporates frames, events, participants as latent topics and\nlearns those frame and event transitions that best explain the text. The number\nof frames is inferred by a novel application of a split-merge method from\nsyntactic parsing. In end-to-end evaluations from text to induced frames and\nextracted facts, our method produced state-of-the-art results while\nsubstantially reducing engineering effort.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 05:47:32 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Cheung", "Jackie Chi Kit", ""], ["Poon", "Hoifung", ""], ["Vanderwende", "Lucy", ""]]}, {"id": "1302.4814", "submitter": "Olivier Kraif", "authors": "Georges Antoniadis (LIDILEM), Sylviane Granger, Olivier Kraif\n  (LIDILEM), Claude Ponton (LIDILEM), Virginie Zampa (LIDILEM)", "title": "NLP and CALL: integration is working", "comments": null, "journal-ref": "TaLC7, France (2006)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part of this article, we explore the background of\ncomputer-assisted learning from its beginnings in the early XIXth century and\nthe first teaching machines, founded on theories of learning, at the start of\nthe XXth century. With the arrival of the computer, it became possible to offer\nlanguage learners different types of language activities such as comprehension\ntasks, simulations, etc. However, these have limits that cannot be overcome\nwithout some contribution from the field of natural language processing (NLP).\nIn what follows, we examine the challenges faced and the issues raised by\nintegrating NLP into CALL. We hope to demonstrate that the key to success in\nintegrating NLP into CALL is to be found in multidisciplinary work between\ncomputer experts, linguists, language teachers, didacticians and NLP\nspecialists.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 05:47:44 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Antoniadis", "Georges", "", "LIDILEM"], ["Granger", "Sylviane", "", "LIDILEM"], ["Kraif", "Olivier", "", "LIDILEM"], ["Ponton", "Claude", "", "LIDILEM"], ["Zampa", "Virginie", "", "LIDILEM"]]}, {"id": "1302.4874", "submitter": "Gon\\c{c}alo Sim\\~oes", "authors": "Gon\\c{c}alo Sim\\~oes, Helena Galhardas, David Matos", "title": "A Labeled Graph Kernel for Relationship Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach for Relationship Extraction (RE) based\non labeled graph kernels. The kernel we propose is a particularization of a\nrandom walk kernel that exploits two properties previously studied in the RE\nliterature: (i) the words between the candidate entities or connecting them in\na syntactic representation are particularly likely to carry information\nregarding the relationship; and (ii) combining information from distinct\nsources in a kernel may help the RE system make better decisions. We performed\nexperiments on a dataset of protein-protein interactions and the results show\nthat our approach obtains effectiveness values that are comparable with the\nstate-of-the art kernel methods. Moreover, our approach is able to outperform\nthe state-of-the-art kernels when combined with other kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 11:06:25 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Sim\u00f5es", "Gon\u00e7alo", ""], ["Galhardas", "Helena", ""], ["Matos", "David", ""]]}, {"id": "1302.5181", "submitter": "Mark Burgin", "authors": "Mark Burgin", "title": "Basic Classes of Grammars with Prohibition", "comments": "2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical tool for natural language modeling and development of\nhuman-machine interaction is developed in the context of formal grammars and\nlanguages. A new type of formal grammars, called grammars with prohibition, is\nintroduced. Grammars with prohibition provide more powerful tools for natural\nlanguage generation and better describe processes of language learning than the\nconventional formal grammars. Here we study relations between languages\ngenerated by different grammars with prohibition based on conventional types of\nformal grammars such as context-free or context sensitive grammars. Besides, we\ncompare languages generated by different grammars with prohibition and\nlanguages generated by conventional formal grammars. In particular, it is\ndemonstrated that they have essentially higher computational power and\nexpressive possibilities in comparison with the conventional formal grammars.\nThus, while conventional formal grammars are recursive and subrecursive\nalgorithms, many classes of grammars with prohibition are superrecursive\nalgorithms. Results presented in this work are aimed at the development of\nhuman-machine interaction, modeling natural languages, empowerment of\nprogramming languages, computer simulation, better software systems, and theory\nof recursion.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 04:58:48 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Burgin", "Mark", ""]]}, {"id": "1302.5526", "submitter": "Richard A. Blythe", "authors": "Rainer Reisenauer, Kenny Smith and Richard A. Blythe", "title": "Stochastic dynamics of lexicon learning in an uncertain and nonuniform\n  world", "comments": "7 pages, 3 figures. Version 2 contains additional discussion and will\n  appear in Phys. Rev. Lett", "journal-ref": "Phys Rev Lett (2013) 110 258701", "doi": "10.1103/PhysRevLett.110.258701", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the time taken by a language learner to correctly identify the\nmeaning of all words in a lexicon under conditions where many plausible\nmeanings can be inferred whenever a word is uttered. We show that the most\nbasic form of cross-situational learning - whereby information from multiple\nepisodes is combined to eliminate incorrect meanings - can perform badly when\nwords are learned independently and meanings are drawn from a nonuniform\ndistribution. If learners further assume that no two words share a common\nmeaning, we find a phase transition between a maximally-efficient learning\nregime, where the learning time is reduced to the shortest it can possibly be,\nand a partially-efficient regime where incorrect candidate meanings for words\npersist at late times. We obtain exact results for the word-learning process\nthrough an equivalence to a statistical mechanical problem of enumerating loops\nin the space of word-meaning mappings.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 09:23:50 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 08:20:34 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Reisenauer", "Rainer", ""], ["Smith", "Kenny", ""], ["Blythe", "Richard A.", ""]]}, {"id": "1302.5645", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Role of temporal inference in the recognition of textual inference", "comments": "2008 thesis, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project is a part of nature language processing and its aims to develop\na system of recognition inference text-appointed TIMINF. This type of system\ncan detect, given two portions of text, if a text is semantically deducted from\nthe other. We focused on making the inference time in this type of system. For\nthat we have built and analyzed a body built from questions collected through\nthe web. This study has enabled us to classify different types of times\ninferences and for designing the architecture of TIMINF which seeks to\nintegrate a module inference time in a detection system inference text. We also\nassess the performance of sorties TIMINF system on a test corpus with the same\nstrategy adopted in the challenge RTE.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 15:28:51 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1302.5675", "submitter": "Natheer Gharaibeh Dr", "authors": "Wafa N. Bdour, Natheer K. Gharaibeh", "title": "Development of Yes/No Arabic Question Answering System", "comments": "13 pages, 4 figures, 5 tables", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol.4, No.1, January 2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Developing Question Answering systems has been one of the important research\nissues because it requires insights from a variety of\ndisciplines,including,Artificial Intelligence,Information Retrieval,\nInformation Extraction,Natural Language Processing, and Psychology.In this\npaper we realize a formal model for a lightweight semantic based open domain\nyes/no Arabic question answering system based on paragraph retrieval with\nvariable length. We propose a constrained semantic representation. Using an\nexplicit unification framework based on semantic similarities and query\nexpansion synonyms and antonyms.This frequently improves the precision of the\nsystem. Employing the passage retrieval system achieves a better precision by\nretrieving more paragraphs that contain relevant answers to the question; It\nsignificantly reduces the amount of text to be processed by the system.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 18:59:06 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Bdour", "Wafa N.", ""], ["Gharaibeh", "Natheer K.", ""]]}, {"id": "1302.6334", "submitter": "EPTCS", "authors": "Guillaume Bonfante (LORIA Universit\\'e de Lorraine), Bruno Guillaume\n  (LORIA Inria Nancy Grand-Est)", "title": "Non-simplifying Graph Rewriting Termination", "comments": "In Proceedings TERMGRAPH 2013, arXiv:1302.5997", "journal-ref": "EPTCS 110, 2013, pp. 4-16", "doi": "10.4204/EPTCS.110.3", "report-no": null, "categories": "cs.CL cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, a very large amount of work in Natural Language Processing (NLP) rely\non trees as the core mathematical structure to represent linguistic\ninformations (e.g. in Chomsky's work). However, some linguistic phenomena do\nnot cope properly with trees. In a former paper, we showed the benefit of\nencoding linguistic structures by graphs and of using graph rewriting rules to\ncompute on those structures. Justified by some linguistic considerations, graph\nrewriting is characterized by two features: first, there is no node creation\nalong computations and second, there are non-local edge modifications. Under\nthese hypotheses, we show that uniform termination is undecidable and that\nnon-uniform termination is decidable. We describe two termination techniques\nbased on weights and we give complexity bound on the derivation length for\nthese rewriting system.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 06:50:08 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Bonfante", "Guillaume", "", "LORIA Universit\u00e9 de Lorraine"], ["Guillaume", "Bruno", "", "LORIA Inria Nancy Grand-Est"]]}, {"id": "1302.6777", "submitter": "Greg Adams", "authors": "Greg Adams, Beth Millar, Eric Neufeld, Tim Philip", "title": "Ending-based Strategies for Part-of-speech Tagging", "comments": "Appears in Proceedings of the Tenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1994)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1994-PG-1-7", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic approaches to part-of-speech tagging rely primarily on\nwhole-word statistics about word/tag combinations as well as contextual\ninformation. But experience shows about 4 per cent of tokens encountered in\ntest sets are unknown even when the training set is as large as a million\nwords. Unseen words are tagged using secondary strategies that exploit word\nfeatures such as endings, capitalizations and punctuation marks. In this work,\nword-ending statistics are primary and whole-word statistics are secondary.\nFirst, a tagger was trained and tested on word endings only. Subsequent\nexperiments added back whole-word statistics for the words occurring most\nfrequently in the training set. As grew larger, performance was expected to\nimprove, in the limit performing the same as word-based taggers. Surprisingly,\nthe ending-based tagger initially performed nearly as well as the word-based\ntagger; in the best case, its performance significantly exceeded that of the\nword-based tagger. Lastly, and unexpectedly, an effect of negative returns was\nobserved - as grew larger, performance generally improved and then declined. By\nvarying factors such as ending length and tag-list strategy, we achieved a\nsuccess rate of 97.5 percent.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 14:13:10 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Adams", "Greg", ""], ["Millar", "Beth", ""], ["Neufeld", "Eric", ""], ["Philip", "Tim", ""]]}, {"id": "1302.7056", "submitter": "Wesam Elshamy", "authors": "Wesam Elshamy, Doina Caragea, William Hsu", "title": "KSU KDD: Word Sense Induction by Clustering in Topic Space", "comments": null, "journal-ref": "Proceedings of the 5th International Workshop on Semantic\n  Evaluation, pages 367-370, Uppsala, Sweden, July 2010. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our language-independent unsupervised word sense induction\nsystem. This system only uses topic features to cluster different word senses\nin their global context topic space. Using unlabeled data, this system trains a\nlatent Dirichlet allocation (LDA) topic model then uses it to infer the topics\ndistribution of the test instances. By clustering these topics distributions in\ntheir topic space we cluster them into different senses. Our hypothesis is that\ncloseness in topic space reflects similarity between different word senses.\nThis system participated in SemEval-2 word sense induction and disambiguation\ntask and achieved the second highest V-measure score among all other systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 02:10:38 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Elshamy", "Wesam", ""], ["Caragea", "Doina", ""], ["Hsu", "William", ""]]}]