[{"id": "0907.0784", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Cross-Task Knowledge-Constrained Self Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithmic framework for learning multiple related tasks. Our\nframework exploits a form of prior knowledge that relates the output spaces of\nthese tasks. We present PAC learning results that analyze the conditions under\nwhich such learning is possible. We present results on learning a shallow\nparser and named-entity recognition system that exploits our framework, showing\nconsistent improvements over baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:42:01 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.0785", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Lyle Campbell", "title": "A Bayesian Model for Discovering Typological Implications", "comments": null, "journal-ref": "ACL 2007", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard form of analysis for linguistic typology is the universal\nimplication. These implications state facts about the range of extant\nlanguages, such as ``if objects come after verbs, then adjectives come after\nnouns.'' Such implications are typically discovered by painstaking hand\nanalysis over a small sample of languages. We propose a computational model for\nassisting at this process. Our model is able to discover both well-known\nimplications as well as some novel implications that deserve further study.\nMoreover, through a careful application of hierarchical analysis, we are able\nto cope with the well-known sampling problem: languages are not independent.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:43:16 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Campbell", "Lyle", ""]]}, {"id": "0907.0786", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and John Langford and Daniel Marcu", "title": "Search-based Structured Prediction", "comments": null, "journal-ref": "Machine Learning Journal 2009", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Searn, an algorithm for integrating search and learning to solve\ncomplex structured prediction problems such as those that occur in natural\nlanguage, speech, computational biology, and vision. Searn is a meta-algorithm\nthat transforms these complex problems into simple classification problems to\nwhich any binary classifier may be applied. Unlike current algorithms for\nstructured learning that require decomposition of both the loss function and\nthe feature functions over the predicted structure, Searn is able to learn\nprediction functions for any loss function and any class of features. Moreover,\nSearn comes with a strong, natural theoretical guarantee: good performance on\nthe derived classification problems implies good performance on the structured\nprediction problem.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:48:34 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""], ["Marcu", "Daniel", ""]]}, {"id": "0907.0804", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Daniel Marcu", "title": "Induction of Word and Phrase Alignments for Automatic Document\n  Summarization", "comments": null, "journal-ref": "Computational Linguistics, 2005", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research in automatic single document summarization is dominated by\ntwo effective, yet naive approaches: summarization by sentence extraction, and\nheadline generation via bag-of-words models. While successful in some tasks,\nneither of these models is able to adequately capture the large set of\nlinguistic devices utilized by humans when they produce summaries. One possible\nexplanation for the widespread use of these models is that good techniques have\nbeen developed to extract appropriate training data for them from existing\ndocument/abstract and document/headline corpora. We believe that future\nprogress in automatic summarization will be driven both by the development of\nmore sophisticated, linguistically informed models, as well as a more effective\nleveraging of document/abstract corpora. In order to open the doors to\nsimultaneously achieving both of these goals, we have developed techniques for\nautomatically producing word-to-word and phrase-to-phrase alignments between\ndocuments and their human-written abstracts. These alignments make explicit the\ncorrespondences that exist in such document/abstract pairs, and create a\npotentially rich data source from which complex summarization algorithms may\nlearn. This paper describes experiments we have carried out to analyze the\nability of humans to perform such alignments, and based on these analyses, we\ndescribe experiments for creating them automatically. Our model for the\nalignment task is based on an extension of the standard hidden Markov model,\nand learns to create alignments in a completely unsupervised fashion. We\ndescribe our model in detail and present experimental results that show that\nour model is able to learn to reliably identify word- and phrase-level\nalignments in a corpus of <document,abstract> pairs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 22:57:26 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Marcu", "Daniel", ""]]}, {"id": "0907.0806", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Daniel Marcu", "title": "A Noisy-Channel Model for Document Compression", "comments": null, "journal-ref": "ACL 2002", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a document compression system that uses a hierarchical\nnoisy-channel model of text production. Our compression system first\nautomatically derives the syntactic structure of each sentence and the overall\ndiscourse structure of the text given as input. The system then uses a\nstatistical hierarchical model of text production in order to drop\nnon-important syntactic and discourse constituents so as to generate coherent,\ngrammatical document compressions of arbitrary length. The system outperforms\nboth a baseline and a sentence-based compression system that operates by\nsimplifying sequentially all sentences in a text. Our results support the claim\nthat discourse knowledge plays an important role in document summarization.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 22:26:47 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Marcu", "Daniel", ""]]}, {"id": "0907.0807", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Daniel Marcu", "title": "A Large-Scale Exploration of Effective Global Features for a Joint\n  Entity Detection and Tracking Model", "comments": null, "journal-ref": "HLT/EMNLP 2005", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity detection and tracking (EDT) is the task of identifying textual\nmentions of real-world entities in documents, extending the named entity\ndetection and coreference resolution task by considering mentions other than\nnames (pronouns, definite descriptions, etc.). Like NE tagging and coreference\nresolution, most solutions to the EDT task separate out the mention detection\naspect from the coreference aspect. By doing so, these solutions are limited to\nusing only local features for learning. In contrast, by modeling both aspects\nof the EDT task simultaneously, we are able to learn using highly complex,\nnon-local features. We develop a new joint EDT model and explore the utility of\nmany features, demonstrating their effectiveness on this task.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 22:28:15 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Marcu", "Daniel", ""]]}, {"id": "0907.0809", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III and Daniel Marcu", "title": "Learning as Search Optimization: Approximate Large Margin Methods for\n  Structured Prediction", "comments": null, "journal-ref": "ICML 2005", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mappings to structured output spaces (strings, trees, partitions, etc.) are\ntypically learned using extensions of classification algorithms to simple\ngraphical structures (eg., linear chains) in which search and parameter\nestimation can be performed exactly. Unfortunately, in many complex problems,\nit is rare that exact search or parameter estimation is tractable. Instead of\nlearning exact models and searching via heuristic means, we embrace this\ndifficulty and treat the structured output problem in terms of approximate\nsearch. We present a framework for learning as search optimization, and two\nparameter updates with convergence theorems and bounds. Empirical evidence\nshows that our integrated approach to learning and decoding can outperform\nexact models at smaller computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 22:34:25 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Daum\u00e9", "Hal", "III"], ["Marcu", "Daniel", ""]]}, {"id": "0907.1558", "submitter": "Marcelo A. Montemurro", "authors": "Marcelo A. Montemurro and Damian Zanette", "title": "Towards the quantification of the semantic information encoded in\n  written language", "comments": "19 pages, 4 figures", "journal-ref": "Advances in Complex Systems, Volume 13, Issue 2 (2010), pp.\n  135-153", "doi": "10.1142/S0219525910002530", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Written language is a complex communication signal capable of conveying\ninformation encoded in the form of ordered sequences of words. Beyond the local\norder ruled by grammar, semantic and thematic structures affect long-range\npatterns in word usage. Here, we show that a direct application of information\ntheory quantifies the relationship between the statistical distribution of\nwords and the semantic content of the text. We show that there is a\ncharacteristic scale, roughly around a few thousand words, which establishes\nthe typical size of the most informative segments in written language.\nMoreover, we find that the words whose contributions to the overall information\nis larger, are the ones more closely associated with the main subjects and\ntopics of the text. This scenario can be explained by a model of word usage\nthat assumes that words are distributed along the text in domains of a\ncharacteristic size where their frequency is higher than elsewhere. Our\nconclusions are based on the analysis of a large database of written language,\ndiverse in subjects and styles, and thus are likely to be applicable to general\nlanguage sequences encoding complex information.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2009 14:36:47 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2009 11:35:08 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2009 06:40:04 GMT"}], "update_date": "2010-05-17", "authors_parsed": [["Montemurro", "Marcelo A.", ""], ["Zanette", "Damian", ""]]}, {"id": "0907.1814", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Bayesian Query-Focused Summarization", "comments": null, "journal-ref": "ACL 2006", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BayeSum (for ``Bayesian summarization''), a model for sentence\nextraction in query-focused summarization. BayeSum leverages the common case in\nwhich multiple documents are relevant to a single query. Using these documents\nas reinforcement for query terms, BayeSum is not afflicted by the paucity of\ninformation in short queries. We show that approximate inference in BayeSum is\npossible on large data sets and results in a state-of-the-art summarization\nsystem. Furthermore, we show how BayeSum can be understood as a justified query\nexpansion technique in the language modeling for IR framework.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 13:24:55 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.1815", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Frustratingly Easy Domain Adaptation", "comments": null, "journal-ref": "ACL 2007", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to domain adaptation that is appropriate exactly in\nthe case when one has enough ``target'' data to do slightly better than just\nusing only ``source'' data. Our approach is incredibly simple, easy to\nimplement as a preprocessing step (10 lines of Perl!) and outperforms\nstate-of-the-art approaches on a range of datasets. Moreover, it is trivially\nextended to a multi-domain adaptation problem, where one has data from a\nvariety of different domains.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 13:25:48 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.2452", "submitter": "Laurent Romary", "authors": "Koichi Takeuchi (NII), Kyo Kageura (NII), Teruo Koyama (NII),\n  B\\'eatrice Daille (LINA), Laurent Romary (INRIA Lorraine - LORIA)", "title": "Pattern Based Term Extraction Using ACABIT System", "comments": null, "journal-ref": "IEIC Technical Report 103, 280 (2003) 31-36", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pattern-based term extraction approach for\nJapanese, applying ACABIT system originally developed for French. The proposed\napproach evaluates termhood using morphological patterns of basic terms and\nterm variants. After extracting term candidates, ACABIT system filters out\nnon-terms from the candidates based on log-likelihood. This approach is\nsuitable for Japanese term extraction because most of Japanese terms are\ncompound nouns or simple phrasal patterns.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2009 21:02:14 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Takeuchi", "Koichi", "", "NII"], ["Kageura", "Kyo", "", "NII"], ["Koyama", "Teruo", "", "NII"], ["Daille", "B\u00e9atrice", "", "LINA"], ["Romary", "Laurent", "", "INRIA Lorraine - LORIA"]]}, {"id": "0907.3781", "submitter": "Stephanie Leon", "authors": "St\\'ephanie L\\'eon (LIRMM)", "title": "Un syst\\`eme modulaire d'acquisition automatique de traductions \\`a\n  partir du Web", "comments": null, "journal-ref": "TALN'09 (Traitement Automatique des Langues Naturelles), France\n  (2009)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of automatic translation (French/English) of Complex\nLexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular\nsystem is based on linguistic properties (compositionality, polysemy, etc.).\nDifferent aspects of the multilingual Web are used to validate candidate\ntranslations and collect new terms. We first build a French corpus of Web pages\nto collect CLU. Three adapted processing stages are applied for each linguistic\nproperty : compositional and non polysemous translations, compositional\npolysemous translations and non compositional translations. Our evaluation on a\nsample of CLU shows that our technique based on the Web can reach a very high\nprecision.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2009 06:25:59 GMT"}], "update_date": "2009-07-23", "authors_parsed": [["L\u00e9on", "St\u00e9phanie", "", "LIRMM"]]}, {"id": "0907.4960", "submitter": "Muthiah Annamalai", "authors": "Muthiah Annamalai", "title": "Ezhil: A Tamil Programming Language", "comments": "6 pages, Tamil UTF-8 characters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Ezhil is a Tamil language based interpreted procedural programming language.\nTamil keywords and grammar are chosen to make the native Tamil speaker write\nprograms in the Ezhil system. Ezhil allows easy representation of computer\nprogram closer to the Tamil language logical constructs equivalent to the\nconditional, branch and loop statements in modern English based programming\nlanguages. Ezhil is a compact programming language aimed towards Tamil speaking\nnovice computer users. Grammar for Ezhil and a few example programs are\nreported here, from the initial proof-of-concept implementation using the\nPython programming language1. To the best of our knowledge, Ezhil language is\nthe first freely available Tamil programming language.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2009 19:24:51 GMT"}], "update_date": "2009-07-29", "authors_parsed": [["Annamalai", "Muthiah", ""]]}, {"id": "0907.5083", "submitter": "EPTCS", "authors": "M. Sakthi Balan (Infosys)", "title": "Serializing the Parallelism in Parallel Communicating Pushdown Automata\n  Systems", "comments": null, "journal-ref": "EPTCS 3, 2009, pp. 59-68", "doi": "10.4204/EPTCS.3.5", "report-no": null, "categories": "cs.FL cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parallel communicating pushdown automata systems (PCPA) and\ndefine a property called known communication for it. We use this property to\nprove that the power of a variant of PCPA, called returning centralized\nparallel communicating pushdown automata (RCPCPA), is equivalent to that of\nmulti-head pushdown automata. The above result presents a new sub-class of\nreturning parallel communicating pushdown automata systems (RPCPA) called\nsimple-RPCPA and we show that it can be written as a finite intersection of\nmulti-head pushdown automata systems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2009 09:45:32 GMT"}], "update_date": "2009-07-30", "authors_parsed": [["Balan", "M. Sakthi", "", "Infosys"]]}]