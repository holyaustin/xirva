[{"id": "1303.0347", "submitter": "Diego  Amancio Raphael", "authors": "Diego R. Amancio, Eduardo G. Altmann, Diego Rybski, Osvaldo N.\n  Oliveira Jr. and Luciano da F. Costa", "title": "Probing the statistical properties of unknown texts: application to the\n  Voynich Manuscript", "comments": null, "journal-ref": "PLoS ONE 8(7): e67310 (2013)", "doi": "10.1371/journal.pone.0067310", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the use of statistical physics methods to analyze large corpora has\nbeen useful to unveil many patterns in texts, no comprehensive investigation\nhas been performed investigating the properties of statistical measurements\nacross different languages and texts. In this study we propose a framework that\naims at determining if a text is compatible with a natural language and which\nlanguages are closest to it, without any knowledge of the meaning of the words.\nThe approach is based on three types of statistical measurements, i.e. obtained\nfrom first-order statistics of word properties in a text, from the topology of\ncomplex networks representing text, and from intermittency concepts where text\nis treated as a time series. Comparative experiments were performed with the\nNew Testament in 15 different languages and with distinct books in English and\nPortuguese in order to quantify the dependency of the different measurements on\nthe language and on the story being told in the book. The metrics found to be\ninformative in distinguishing real texts from their shuffled versions include\nassortativity, degree and selectivity of words. As an illustration, we analyze\nan undeciphered medieval manuscript known as the Voynich Manuscript. We show\nthat it is mostly compatible with natural languages and incompatible with\nrandom texts. We also obtain candidates for key-words of the Voynich Manuscript\nwhich could be helpful in the effort of deciphering it. Because we were able to\nidentify statistical measurements that are more dependent on the syntax than on\nthe semantics, the framework may also serve for text analysis in\nlanguage-dependent applications.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 03:57:15 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Amancio", "Diego R.", ""], ["Altmann", "Eduardo G.", ""], ["Rybski", "Diego", ""], ["Oliveira", "Osvaldo N.", "Jr."], ["Costa", "Luciano da F.", ""]]}, {"id": "1303.0350", "submitter": "Diego  Amancio Raphael", "authors": "Diego R. Amancio and Osvaldo N. Oliveira Jr. and Luciano da F. Costa", "title": "Structure-semantics interplay in complex networks and its effects on the\n  predictability of similarity in texts", "comments": null, "journal-ref": "Physica A 391 18 4406-4419, (2012)", "doi": "10.1016/j.physa.2012.04.011", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are different ways to define similarity for grouping similar texts into\nclusters, as the concept of similarity may depend on the purpose of the task.\nFor instance, in topic extraction similar texts mean those within the same\nsemantic field, whereas in author recognition stylistic features should be\nconsidered. In this study, we introduce ways to classify texts employing\nconcepts of complex networks, which may be able to capture syntactic, semantic\nand even pragmatic features. The interplay between the various metrics of the\ncomplex networks is analyzed with three applications, namely identification of\nmachine translation (MT) systems, evaluation of quality of machine translated\ntexts and authorship recognition. We shall show that topological features of\nthe networks representing texts can enhance the ability to identify MT systems\nin particular cases. For evaluating the quality of MT texts, on the other hand,\nhigh correlation was obtained with methods capable of capturing the semantics.\nThis was expected because the golden standards used are themselves based on\nword co-occurrence. Notwithstanding, the Katz similarity, which involves\nsemantic and structure in the comparison of texts, achieved the highest\ncorrelation with the NIST measurement, indicating that in some cases the\ncombination of both approaches can improve the ability to quantify quality in\nMT. In authorship recognition, again the topological features were relevant in\nsome contexts, though for the books and authors analyzed good results were\nobtained with semantic features as well. Because hybrid approaches encompassing\nsemantic and topological features have not been extensively used, we believe\nthat the methodology proposed here may be useful to enhance text classification\nconsiderably, as it combines well-established strategies.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 04:26:02 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Amancio", "Diego R.", ""], ["Oliveira", "Osvaldo N.", "Jr."], ["Costa", "Luciano da F.", ""]]}, {"id": "1303.0445", "submitter": "Kanagavalli V R", "authors": "Kanagavalli V R and Raja. K", "title": "Detecting and resolving spatial ambiguity in text using named entity\n  extraction and self learning fuzzy logic techniques", "comments": "National Conference on Recent Trends in Data Mining and Distributed\n  Systems September 2011", "journal-ref": null, "doi": null, "report-no": "ISBN 978-81-909042-5-4 P.no.71-76", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction identifies useful and relevant text in a document and\nconverts unstructured text into a form that can be loaded into a database\ntable. Named entity extraction is a main task in the process of information\nextraction and is a classification problem in which words are assigned to one\nor more semantic classes or to a default non-entity class. A word which can\nbelong to one or more classes and which has a level of uncertainty in it can be\nbest handled by a self learning Fuzzy Logic Technique. This paper proposes a\nmethod for detecting the presence of spatial uncertainty in the text and\ndealing with spatial ambiguity using named entity extraction techniques coupled\nwith self learning fuzzy logic techniques\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 01:21:58 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["R", "Kanagavalli V", ""], ["K", "Raja.", ""]]}, {"id": "1303.0446", "submitter": "Boyan Bonev", "authors": "Boyan Bonev, Gema Ram\\'irez-S\\'anchez, Sergio Ortiz Rojas", "title": "Statistical sentiment analysis performance in Opinum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The classification of opinion texts in positive and negative is becoming a\nsubject of great interest in sentiment analysis. The existence of many labeled\nopinions motivates the use of statistical and machine-learning methods.\nFirst-order statistics have proven to be very limited in this field. The Opinum\napproach is based on the order of the words without using any syntactic and\nsemantic information. It consists of building one probabilistic model for the\npositive and another one for the negative opinions. Then the test opinions are\ncompared to both models and a decision and confidence measure are calculated.\nIn order to reduce the complexity of the training corpus we first lemmatize the\ntexts and we replace most named-entities with wildcards. Opinum presents an\naccuracy above 81% for Spanish opinions in the financial products domain. In\nthis work we discuss which are the most important factors that have impact on\nthe classification performance.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 01:38:03 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Bonev", "Boyan", ""], ["Ram\u00edrez-S\u00e1nchez", "Gema", ""], ["Rojas", "Sergio Ortiz", ""]]}, {"id": "1303.0489", "submitter": "Leena Patil Leena Homraj Patil", "authors": "Leena H. Patil, Mohammed Atique", "title": "A Semantic approach for effective document clustering using WordNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now a days, the text document is spontaneously increasing over the internet,\ne-mail and web pages and they are stored in the electronic database format. To\narrange and browse the document it becomes difficult. To overcome such problem\nthe document preprocessing, term selection, attribute reduction and maintaining\nthe relationship between the important terms using background knowledge,\nWordNet, becomes an important parameters in data mining. In these paper the\ndifferent stages are formed, firstly the document preprocessing is done by\nremoving stop words, stemming is performed using porter stemmer algorithm, word\nnet thesaurus is applied for maintaining relationship between the important\nterms, global unique words, and frequent word sets get generated, Secondly,\ndata matrix is formed, and thirdly terms are extracted from the documents by\nusing term selection approaches tf-idf, tf-df, and tf2 based on their minimum\nthreshold value. Further each and every document terms gets preprocessed, where\nthe frequency of each term within the document is counted for representation.\nThe purpose of this approach is to reduce the attributes and find the effective\nterm selection method using WordNet for better clustering accuracy. Experiments\nare evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and\nship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group\n(Hardware), 20 News group (Computer Graphics) etc.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 12:19:18 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Patil", "Leena H.", ""], ["Atique", "Mohammed", ""]]}, {"id": "1303.1232", "submitter": "Jessica C. Ram\\'irez", "authors": "Jessica Ram\\'irez, Masayuki Asahara, Yuji Matsumoto", "title": "Japanese-Spanish Thesaurus Construction Using English as a Pivot", "comments": null, "journal-ref": "In Proceeding of The Third International Joint Conference on\n  Natural Language Processing (IJCNLP-08), Hyderabad, India. pages 473-480,\n  2008", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of research with the goal of automatically creating a\nmultilingual thesaurus based on the freely available resources of Wikipedia and\nWordNet. Our goal is to increase resources for natural language processing\ntasks such as machine translation targeting the Japanese-Spanish language pair.\nGiven the scarcity of resources, we use existing English resources as a pivot\nfor creating a trilingual Japanese-Spanish-English thesaurus. Our approach\nconsists of extracting the translation tuples from Wikipedia, disambiguating\nthem by mapping them to WordNet word senses. We present results comparing two\nmethods of disambiguation, the first using VSM on Wikipedia article texts and\nWordNet definitions, and the second using categorical information extracted\nfrom Wikipedia, We find that mixing the two methods produces favorable results.\nUsing the proposed method, we have constructed a multilingual\nSpanish-Japanese-English thesaurus consisting of 25,375 entries. The same\nmethod can be applied to any pair of languages that are linked to English in\nWikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 01:30:58 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Ram\u00edrez", "Jessica", ""], ["Asahara", "Masayuki", ""], ["Matsumoto", "Yuji", ""]]}, {"id": "1303.1441", "submitter": "Kamal Sarkar", "authors": "Kamal Sarkar", "title": "A Hybrid Approach to Extract Keyphrases from Medical Documents", "comments": null, "journal-ref": "International Journal of Computer Applications 63(18):14-19,\n  February 2013. Published by Foundation of Computer Science, New York, USA", "doi": "10.5120/10565-5528", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrases are the phrases, consisting of one or more words, representing the\nimportant concepts in the articles. Keyphrases are useful for a variety of\ntasks such as text summarization, automatic indexing,\nclustering/classification, text mining etc. This paper presents a hybrid\napproach to keyphrase extraction from medical documents. The keyphrase\nextraction approach presented in this paper is an amalgamation of two methods:\nthe first one assigns weights to candidate keyphrases based on an effective\ncombination of features such as position, term frequency, inverse document\nfrequency and the second one assign weights to candidate keyphrases using some\nknowledge about their similarities to the structure and characteristics of\nkeyphrases available in the memory (stored list of keyphrases). An efficient\ncandidate keyphrase identification method as the first component of the\nproposed keyphrase extraction system has also been introduced in this paper.\nThe experimental results show that the proposed hybrid approach performs better\nthan some state-of-the art keyphrase extraction approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 20:09:05 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2014 16:34:35 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Sarkar", "Kamal", ""]]}, {"id": "1303.1599", "submitter": "Xiao-Yong Yan", "authors": "Xiao-Yong Yan, Ying Fan, Zengru Di, Shlomo Havlin, Jinshan Wu", "title": "Efficient learning strategy of Chinese characters based on network\n  approach", "comments": "8 pages, 6 figures", "journal-ref": "PLoS ONE 8(8): e69745 (2013)", "doi": "10.1371/journal.pone.0069745", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on network analysis of hierarchical structural relations among Chinese\ncharacters, we develop an efficient learning strategy of Chinese characters. We\nregard a more efficient learning method if one learns the same number of useful\nChinese characters in less effort or time. We construct a node-weighted network\nof Chinese characters, where character usage frequencies are used as node\nweights. Using this hierarchical node-weighted network, we propose a new\nlearning method, the distributed node weight (DNW) strategy, which is based on\na new measure of nodes' importance that takes into account both the weight of\nthe nodes and the hierarchical structure of the network. Chinese character\nlearning strategies, particularly their learning order, are analyzed as\ndynamical processes over the network. We compare the efficiency of three\ntheoretical learning methods and two commonly used methods from mainstream\nChinese textbooks, one for Chinese elementary school students and the other for\nstudents learning Chinese as a second language. We find that the DNW method\nsignificantly outperforms the others, implying that the efficiency of current\nlearning methods of major textbooks can be greatly improved.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 03:19:34 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Yan", "Xiao-Yong", ""], ["Fan", "Ying", ""], ["Di", "Zengru", ""], ["Havlin", "Shlomo", ""], ["Wu", "Jinshan", ""]]}, {"id": "1303.1703", "submitter": "Fatiha Boubekeur", "authors": "Fatiha Boubekeur and Wassila Azzoug", "title": "Concept-based indexing in text information retrieval", "comments": "18 pages, 5 tables, 3 figures", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 5, No 1, February 2013", "doi": "10.5121/ijcsit", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional information retrieval systems rely on keywords to index documents\nand queries. In such systems, documents are retrieved based on the number of\nshared keywords with the query. This lexical-focused retrieval leads to\ninaccurate and incomplete results when different keywords are used to describe\nthe documents and queries. Semantic-focused retrieval approaches attempt to\novercome this problem by relying on concepts rather than on keywords to\nindexing and retrieval. The goal is to retrieve documents that are semantically\nrelevant to a given user query. This paper addresses this issue by proposing a\nsolution at the indexing level. More precisely, we propose a novel approach for\nsemantic indexing based on concepts identified from a linguistic resource. In\nparticular, our approach relies on the joint use of WordNet and WordNetDomains\nlexical databases for concept identification. Furthermore, we propose a\nsemantic-based concept weighting scheme that relies on a novel definition of\nconcept centrality. The resulting system is evaluated on the TIME test\ncollection. Experimental results show the effectiveness of our proposition over\ntraditional IR approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 14:46:17 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Boubekeur", "Fatiha", ""], ["Azzoug", "Wassila", ""]]}, {"id": "1303.1929", "submitter": "Nuria Bel", "authors": "Muntsa Padr\\'o, N\\'uria Bel, Silvia Necsulescu", "title": "Towards the Fully Automatic Merging of Lexical Resources: A Step Forward", "comments": "7 pages, 1 figure, 5 tables. Also available in UPF institutional\n  repository (http://hdl.handle.net/10230/20417)", "journal-ref": "LREC 2012 Workshop on Language Resource Merging; 2012 May 22;\n  Istanbul, Turkey. Paris: European Language Resources Association; 2012. p.\n  8-14", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This article reports on the results of the research done towards the fully\nautomatically merging of lexical resources. Our main goal is to show the\ngenerality of the proposed approach, which have been previously applied to\nmerge Spanish Subcategorization Frames lexica. In this work we extend and apply\nthe same technique to perform the merging of morphosyntactic lexica encoded in\nLMF. The experiments showed that the technique is general enough to obtain good\nresults in these two different tasks which is an important step towards\nperforming the merging of lexical resources fully automatically.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 10:13:56 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Padr\u00f3", "Muntsa", ""], ["Bel", "N\u00faria", ""], ["Necsulescu", "Silvia", ""]]}, {"id": "1303.1930", "submitter": "Nuria Bel", "authors": "N\\'uria Bel, Lauren Romeo, Muntsa Padr\\'o", "title": "Automatic lexical semantic classification of nouns", "comments": "8 pages, 8 tables. Also available in UPF institutional repository\n  (http://hdl.handle.net/10230/20420)", "journal-ref": "Proceedings of the Eight International Conference on Language\n  Resources and Evaluation (LREC'12); 2012 May 23-25; Istanbul, Turkey. Paris:\n  European Language Resources Association; 2012. p. 1448-1455", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The work we present here addresses cue-based noun classification in English\nand Spanish. Its main objective is to automatically acquire lexical semantic\ninformation by classifying nouns into previously known noun lexical classes.\nThis is achieved by using particular aspects of linguistic contexts as cues\nthat identify a specific lexical class. Here we concentrate on the task of\nidentifying such cues and the theoretical background that allows for an\nassessment of the complexity of the task. The results show that, despite of the\na-priori complexity of the task, cue-based classification is a useful tool in\nthe automatic acquisition of lexical semantic classes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 10:14:04 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Bel", "N\u00faria", ""], ["Romeo", "Lauren", ""], ["Padr\u00f3", "Muntsa", ""]]}, {"id": "1303.1931", "submitter": "Nuria Bel", "authors": "Silvia V\\'azquez, N\\'uria Bel", "title": "A Classification of Adjectives for Polarity Lexicons Enhancement", "comments": "5 pages, 7 tables. Also available in UPF institutional repository\n  (http://hdl.handle.net/10230/20419)", "journal-ref": "Proceedings of the Eight International Conference on Language\n  Resources and Evaluation (LREC'12); 2012 May 23-25; Istanbul, Turkey. Paris:\n  European Language Resources Association; 2012. p. 3557-3561", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Subjective language detection is one of the most important challenges in\nSentiment Analysis. Because of the weight and frequency in opinionated texts,\nadjectives are considered a key piece in the opinion extraction process. These\nsubjective units are more and more frequently collected in polarity lexicons in\nwhich they appear annotated with their prior polarity. However, at the moment,\nany polarity lexicon takes into account prior polarity variations across\ndomains. This paper proves that a majority of adjectives change their prior\npolarity value depending on the domain. We propose a distinction between domain\ndependent and domain independent adjectives. Moreover, our analysis led us to\npropose a further classification related to subjectivity degree: constant,\nmixed and highly subjective adjectives. Following this classification, polarity\nvalues will be a better support for Sentiment Analysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 10:14:31 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["V\u00e1zquez", "Silvia", ""], ["Bel", "N\u00faria", ""]]}, {"id": "1303.1932", "submitter": "Nuria Bel", "authors": "N\\'uria Bel, Vassilis Papavasiliou, Prokopis Prokopidis, Antonio\n  Toral, Victoria Arranz", "title": "Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform", "comments": "3 pages. Also available in UPF institutional repository\n  (http://hdl.handle.net/10230/20416)", "journal-ref": "Proceedings of the 5th Workshop on Building and Using Comparable\n  Corpora at the Eighth International Conference on Language Resources and\n  Evaluation (LREC-2012); 2012 May 23-25; Istanbul, Turkey. Paris: ELRA; 2012.\n  p. 24-26", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform\nthat automates the stages involved in the acquisition, production, updating and\nmaintenance of the large language resources required by, among others, MT\nsystems. The development of a Corpus Acquisition Component (CAC) for extracting\nmonolingual and bilingual data from the web is one of the most innovative\nbuilding blocks of PANACEA. The CAC, which is the first stage in the PANACEA\npipeline for building Language Resources, adopts an efficient and distributed\nmethodology to crawl for web documents with rich textual content in specific\nlanguages and predefined domains. The CAC includes modules that can acquire\nparallel data from sites with in-domain content available in more than one\nlanguage. In order to extrinsically evaluate the CAC methodology, we have\nconducted several experiments that used crawled parallel corpora for the\nidentification and extraction of parallel sentences using sentence alignment.\nThe corpora were then successfully used for domain adaptation of Machine\nTranslation Systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 10:15:57 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Bel", "N\u00faria", ""], ["Papavasiliou", "Vassilis", ""], ["Prokopidis", "Prokopis", ""], ["Toral", "Antonio", ""], ["Arranz", "Victoria", ""]]}, {"id": "1303.2430", "submitter": "Diederik Aerts", "authors": "Diederik Aerts", "title": "Quantum and Concept Combination, Entangled Measurements and Prototype\n  Theory", "comments": "5 pages, 1 figure", "journal-ref": "Topics in Cognitive Science, 6, pp. 129-137, 2014", "doi": "10.1111/tops.12073", "report-no": null, "categories": "cs.AI cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the meaning of the violation of the marginal probability law for\nsituations of correlation measurements where entanglement is identified. We\nshow that for quantum theory applied to the cognitive realm such a violation\ndoes not lead to the type of problems commonly believed to occur in situations\nof quantum theory applied to the physical realm. We briefly situate our quantum\napproach for modeling concepts and their combinations with respect to the\nnotions of 'extension' and 'intension' in theories of meaning, and in existing\nconcept theories.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 05:36:34 GMT"}, {"version": "v2", "created": "Sun, 19 May 2013 12:50:05 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Aerts", "Diederik", ""]]}, {"id": "1303.2448", "submitter": "Nuria Bel", "authors": "N\\'uria Bel, Maria Coll, Gabriela Resnik", "title": "Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon\n  Production", "comments": "7 pages, 2 figures. Also available in UPF institutional repository\n  (http://hdl.handle.net/10230/20325)", "journal-ref": "Proceedings of the 23rd International Conference on Computational\n  Linguistics (Coling 2010); 2010 Aug 23-27; Beijing, CN. Stroudsburg: ACL;\n  2010. p. 46-52", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this work we present the results of our experimental work on the\ndevelop-ment of lexical class-based lexica by automatic means. The objective is\nto as-sess the use of linguistic lexical-class based information as a feature\nselection methodology for the use of classifiers in quick lexical development.\nThe results show that the approach can help in re-ducing the human effort\nrequired in the development of language resources sig-nificantly.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 08:21:17 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Bel", "N\u00faria", ""], ["Coll", "Maria", ""], ["Resnik", "Gabriela", ""]]}, {"id": "1303.2449", "submitter": "Nuria Bel", "authors": "Lauren Romeo, Sara Mendes, N\\'uria Bel", "title": "Using qualia information to identify lexical semantic classes in an\n  unsupervised clustering task", "comments": "10 pages, 5 tables. Also available in UPF institutional repository\n  (http://hdl.handle.net/10230/20383)", "journal-ref": "Proceedings of COLING 2012: Posters: 24th International Conference\n  on Computational Linguistics COLING 2012; 2012 December 8-15; Mumbai, India.\n  Mumbai: The COLING 2012 Organizing Committee; 2012. p. 1029-1038", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Acquiring lexical information is a complex problem, typically approached by\nrelying on a number of contexts to contribute information for classification.\nOne of the first issues to address in this domain is the determination of such\ncontexts. The work presented here proposes the use of automatically obtained\nFORMAL role descriptors as features used to draw nouns from the same lexical\nsemantic class together in an unsupervised clustering task. We have dealt with\nthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The\nresults obtained show that it is possible to discriminate between elements from\ndifferent lexical semantic classes using only FORMAL role information, hence\nvalidating our initial hypothesis. Also, iterating our method accurately\naccounts for fine-grained distinctions within lexical classes, namely\ndistinctions involving ambiguous expressions. Moreover, a filtering and\nbootstrapping strategy employed in extracting FORMAL role descriptors proved to\nminimize effects of sparse data and noise in our task.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 08:21:48 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Romeo", "Lauren", ""], ["Mendes", "Sara", ""], ["Bel", "N\u00faria", ""]]}, {"id": "1303.2826", "submitter": "William Darling", "authors": "William M. Darling and Fei Song", "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA", "comments": "Currently under review for the journal Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a probabilistic generative model for text based on\nsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).\nPOSLDA simultaneously uncovers short-range syntactic patterns (syntax) and\nlong-range semantic patterns (topics) that exist in document collections. This\nresults in word distributions that are specific to both topics (sports,\neducation, ...) and parts-of-speech (nouns, verbs, ...). For example,\nmultinomial distributions over words are uncovered that can be understood as\n\"nouns about weather\" or \"verbs about law\". We describe the model and an\napproximate inference algorithm and then demonstrate the quality of the learned\ntopics both qualitatively and quantitatively. Then, we discuss an NLP\napplication where the output of POSLDA can lead to strong improvements in\nquality: unsupervised part-of-speech tagging. We describe algorithms for this\ntask that make use of POSLDA-learned distributions that result in improved\nperformance beyond the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 10:20:50 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Darling", "William M.", ""], ["Song", "Fei", ""]]}, {"id": "1303.3036", "submitter": "Christian Retore", "authors": "Christian Retor\\'e (LaBRI, IRIT)", "title": "Type-theoretical natural language semantics: on the system F for meaning\n  assembly", "comments": null, "journal-ref": "TYPES 2013, Toulouse : France (2013)", "doi": null, "report-no": null, "categories": "cs.LO cs.CL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents and extends our type theoretical framework for a\ncompositional treatment of natural language semantics with some lexical\nfeatures like coercions (e.g. of a town into a football club) and copredication\n(e.g. on a town as a set of people and as a location). The second order typed\nlambda calculus was shown to be a good framework, and here we discuss how to\nintroduced predefined types and coercive subtyping which are much more natural\nthan internally coded similar constructs. Linguistic applications of these new\nfeatures are also exemplified.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 21:06:22 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Retor\u00e9", "Christian", "", "LaBRI, IRIT"]]}, {"id": "1303.3170", "submitter": "Peter Hines", "authors": "Peter Hines", "title": "Types and forgetfulness in categorical linguistics and quantum mechanics", "comments": "37 pages, 4 figures", "journal-ref": "in C. Heunen, M. Sadrzadeh, E. Grefenstette (ed.s), Quantum\n  Physics and Linguistics: a compositional diagrammatic discourse, Oxford\n  University Press (2013)", "doi": null, "report-no": null, "categories": "cs.CL math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of types in categorical models of meaning is investigated. A general\nscheme for how typed models of meaning may be used to compare sentences,\nregardless of their grammatical structure is described, and a toy example is\nused as an illustration. Taking as a starting point the question of whether the\nevaluation of such a type system 'loses information', we consider the\nparametrized typing associated with connectives from this viewpoint.\n  The answer to this question implies that, within full categorical models of\nmeaning, the objects associated with types must exhibit a simple but subtle\ncategorical property known as self-similarity. We investigate the category\ntheory behind this, with explicit reference to typed systems, and their\nmonoidal closed structure. We then demonstrate close connections between such\nself-similar structures and dagger Frobenius algebras. In particular, we\ndemonstrate that the categorical structures implied by the polymorphically\ntyped connectives give rise to a (lax unitless) form of the special forms of\nFrobenius algebras known as classical structures, used heavily in abstract\ncategorical approaches to quantum mechanics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 14:32:03 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Hines", "Peter", ""]]}, {"id": "1303.3592", "submitter": "Maxim Makatchev", "authors": "Maxim Makatchev, Reid Simmons, Majd Sakr and Micheline Ziadee", "title": "Expressing Ethnicity through Behaviors of a Robot Character", "comments": "10 pages, 4 figures", "journal-ref": "Proceedings of the 8th ACM/IEEE international conference on\n  Human-robot interaction (HRI), Tokyo, Japan, 2013, pages 357-364", "doi": "10.1109/HRI.2013.6483610", "report-no": null, "categories": "cs.CL cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Achieving homophily, or association based on similarity, between a human user\nand a robot holds a promise of improved perception and task performance.\nHowever, no previous studies that address homophily via ethnic similarity with\nrobots exist. In this paper, we discuss the difficulties of evoking ethnic cues\nin a robot, as opposed to a virtual agent, and an approach to overcome those\ndifficulties based on using ethnically salient behaviors. We outline our\nmethodology for selecting and evaluating such behaviors, and culminate with a\nstudy that evaluates our hypotheses of the possibility of ethnic attribution of\na robot character through verbal and nonverbal behaviors and of achieving the\nhomophily effect.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 20:07:48 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Makatchev", "Maxim", ""], ["Simmons", "Reid", ""], ["Sakr", "Majd", ""], ["Ziadee", "Micheline", ""]]}, {"id": "1303.3948", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, Vilas Thakare", "title": "An Adaptive Methodology for Ubiquitous ASR System", "comments": "10 Pages, 05 Tables, 03 Figures", "journal-ref": "Computer and Information Science;Vol.6,No.1;2013 ISSN 1913-8989\n  E-ISSN 1913-8997 Computer and Information Science; Vol. 6, No. 1; 2013, ISSN\n  1913-8989 E-ISSN 1913-8997, Published by Canadian Center of Science and\n  Education", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Achieving and maintaining the performance of ubiquitous (Automatic Speech\nRecognition) ASR system is a real challenge. The main objective of this work is\nto develop a method that will improve and show the consistency in performance\nof ubiquitous ASR system for real world noisy environment. An adaptive\nmethodology has been developed to achieve an objective with the help of\nimplementing followings, -Cleaning speech signal as much as possible while\npreserving originality / intangibility using various modified filters and\nenhancement techniques. -Extracting features from speech signals using various\nsizes of parameter. -Train the system for ubiquitous environment using\nmulti-environmental adaptation training methods. -Optimize the word recognition\nrate with appropriate variable size of parameters using fuzzy technique. The\nconsistency in performance is tested using standard noise databases as well as\nin real world environment. A good improvement is noticed. This work will be\nhelpful to give discriminative training of ubiquitous ASR system for better\nHuman Computer Interaction (HCI) using Speech User Interface (SUI).\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 05:35:39 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Thakare", "Vilas", ""]]}, {"id": "1303.4293", "submitter": "Tobias Kuhn", "authors": "Kaarel Kaljurand and Tobias Kuhn", "title": "A Multilingual Semantic Wiki Based on Attempto Controlled English and\n  Grammatical Framework", "comments": "To appear in the Proceedings of the 10th Extended Semantic Web\n  Conference (ESWC 2013)", "journal-ref": null, "doi": "10.1007/978-3-642-38288-8_29", "report-no": "LNCS 7882", "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a semantic wiki system with an underlying controlled natural\nlanguage grammar implemented in Grammatical Framework (GF). The grammar\nrestricts the wiki content to a well-defined subset of Attempto Controlled\nEnglish (ACE), and facilitates a precise bidirectional automatic translation\nbetween ACE and language fragments of a number of other natural languages,\nmaking the wiki content accessible multilingually. Additionally, our approach\nallows for automatic translation into the Web Ontology Language (OWL), which\nenables automatic reasoning over the wiki content. The developed wiki\nenvironment thus allows users to build, query and view OWL knowledge bases via\na user-friendly multilingual natural language interface. As a further feature,\nthe underlying multilingual grammar is integrated into the wiki and can be\ncollaboratively edited to extend the vocabulary of the wiki or even customize\nits sentence structures. This work demonstrates the combination of the existing\ntechnologies of Attempto Controlled English and Grammatical Framework, and is\nimplemented as an extension of the existing semantic wiki engine AceWiki.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 08:10:39 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Kaljurand", "Kaarel", ""], ["Kuhn", "Tobias", ""]]}, {"id": "1303.4959", "submitter": "Lu\\'is F.  Seoane MsC", "authors": "Victoria Otero-Espinar, Lu\\'is F. Seoane, Juan J. Nieto, Jorge Mira", "title": "Analytic solution of a model of language competition with bilingualism\n  and interlinguistic similarity", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.physd.2013.08.011", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An in-depth analytic study of a model of language dynamics is presented: a\nmodel which tackles the problem of the coexistence of two languages within a\nclosed community of speakers taking into account bilingualism and incorporating\na parameter to measure the distance between languages. After previous numerical\nsimulations, the model yielded that coexistence might lead to survival of both\nlanguages within monolingual speakers along with a bilingual community or to\nextinction of the weakest tongue depending on different parameters. In this\npaper, such study is closed with thorough analytical calculations to settle the\nresults in a robust way and previous results are refined with some\nmodifications. From the present analysis it is possible to almost completely\nassay the number and nature of the equilibrium points of the model, which\ndepend on its parameters, as well as to build a phase space based on them.\nAlso, we obtain conclusions on the way the languages evolve with time. Our\nrigorous considerations also suggest ways to further improve the model and\nfacilitate the comparison of its consequences with those from other approaches\nor with real data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 15:12:01 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Otero-Espinar", "Victoria", ""], ["Seoane", "Lu\u00eds F.", ""], ["Nieto", "Juan J.", ""], ["Mira", "Jorge", ""]]}, {"id": "1303.5148", "submitter": "Mark Dredze", "authors": "Damianos Karakos and Mark Dredze and Sanjeev Khudanpur", "title": "Estimating Confusions in the ASR Channel for Improved Topic-based\n  Language Model Adaptation", "comments": "Technical Report 8, Human Language Technology Center of Excellence,\n  Johns Hopkins University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language is a combination of elemental languages/domains/styles that\nchange across and sometimes within discourses. Language models, which play a\ncrucial role in speech recognizers and machine translation systems, are\nparticularly sensitive to such changes, unless some form of adaptation takes\nplace. One approach to speech language model adaptation is self-training, in\nwhich a language model's parameters are tuned based on automatically\ntranscribed audio. However, transcription errors can misguide self-training,\nparticularly in challenging settings such as conversational speech. In this\nwork, we propose a model that considers the confusions (errors) of the ASR\nchannel. By modeling the likely confusions in the ASR output instead of using\njust the 1-best, we improve self-training efficacy by obtaining a more reliable\nreference transcription estimate. We demonstrate improved topic-based language\nmodeling adaptation results over both 1-best and lattice self-training using\nour ASR channel confusion estimates on telephone conversations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 02:56:43 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Karakos", "Damianos", ""], ["Dredze", "Mark", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1303.5513", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, Vilas Thakare", "title": "Parameters Optimization for Improving ASR Performance in Adverse Real\n  World Noisy Environmental Conditions", "comments": "13 pages, 3 figures, 5 tables", "journal-ref": "International Journal of Human Computer Interaction (IJHCI) 3(3),\n  58-70, 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the existing research it has been observed that many techniques and\nmethodologies are available for performing every step of Automatic Speech\nRecognition (ASR) system, but the performance (Minimization of Word Error\nRecognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodology\nis not dependent on the only technique applied in that method. The research\nwork indicates that, performance mainly depends on the category of the noise,\nthe level of the noise and the variable size of the window, frame, frame\noverlap etc is considered in the existing methods. The main aim of the work\npresented in this paper is to use variable size of parameters like window size,\nframe size and frame overlap percentage to observe the performance of\nalgorithms for various categories of noise with different levels and also train\nthe system for all size of parameters and category of real world noisy\nenvironment to improve the performance of the speech recognition system. This\npaper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test by\napplying variable size of parameters. It is observed that, it is really very\nhard to evaluate test results and decide parameter size for ASR performance\nimprovement for its resultant optimization. Hence, this study further suggests\nthe feasible and optimum parameter size using Fuzzy Inference System (FIS) for\nenhancing resultant accuracy in adverse real world noisy environmental\nconditions. This work will be helpful to give discriminative training of\nubiquitous ASR system for better Human Computer Interaction (HCI).\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 04:20:16 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Thakare", "Vilas", ""]]}, {"id": "1303.5515", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, VM Thakare", "title": "Adverse Conditions and ASR Techniques for Robust Speech User Interface", "comments": "10 pages 2 Tables", "journal-ref": "International Journal of Computer Science Issues (IJCSI), 8(5),\n  440-449, 2011", "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main motivation for Automatic Speech Recognition (ASR) is efficient\ninterfaces to computers, and for the interfaces to be natural and truly useful,\nit should provide coverage for a large group of users. The purpose of these\ntasks is to further improve man-machine communication. ASR systems exhibit\nunacceptable degradations in performance when the acoustical environments used\nfor training and testing the system are not the same. The goal of this research\nis to increase the robustness of the speech recognition systems with respect to\nchanges in the environment. A system can be labeled as environment-independent\nif the recognition accuracy for a new environment is the same or higher than\nthat obtained when the system is retrained for that environment. Attaining such\nperformance is the dream of the researchers. This paper elaborates some of the\ndifficulties with Automatic Speech Recognition (ASR). These difficulties are\nclassified into Speakers characteristics and environmental conditions, and\ntried to suggest some techniques to compensate variations in speech signal.\nThis paper focuses on the robustness with respect to speakers variations and\nchanges in the acoustical environment. We discussed several different external\nfactors that change the environment and physiological differences that affect\nthe performance of a speech recognition system followed by techniques that are\nhelpful to design a robust ASR system.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 04:44:37 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Thakare", "VM", ""]]}, {"id": "1303.5778", "submitter": "Alex Graves", "authors": "Alex Graves, Abdel-rahman Mohamed and Geoffrey Hinton", "title": "Speech Recognition with Deep Recurrent Neural Networks", "comments": "To appear in ICASSP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are a powerful model for sequential data.\nEnd-to-end training methods such as Connectionist Temporal Classification make\nit possible to train RNNs for sequence labelling problems where the\ninput-output alignment is unknown. The combination of these methods with the\nLong Short-term Memory RNN architecture has proved particularly fruitful,\ndelivering state-of-the-art results in cursive handwriting recognition. However\nRNN performance in speech recognition has so far been disappointing, with\nbetter results returned by deep feedforward networks. This paper investigates\n\\emph{deep recurrent neural networks}, which combine the multiple levels of\nrepresentation that have proved so effective in deep networks with the flexible\nuse of long range context that empowers RNNs. When trained end-to-end with\nsuitable regularisation, we find that deep Long Short-term Memory RNNs achieve\na test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to\nour knowledge is the best recorded score.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 20:55:48 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Graves", "Alex", ""], ["Mohamed", "Abdel-rahman", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1303.5960", "submitter": "Daniel Christen Mr.", "authors": "Daniel Christen", "title": "SYNTAGMA. A Linguistic Approach to Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SYNTAGMA is a rule-based parsing system, structured on two levels: a general\nparsing engine and a language specific grammar. The parsing engine is a\nlanguage independent program, while grammar and language specific rules and\nresources are given as text files, consisting in a list of constituent\nstructuresand a lexical database with word sense related features and\nconstraints. Since its theoretical background is principally Tesniere's\nElements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument\nstructure (valency) in constraint satisfaction, and allows also horizontal\nbounds, for instance treating coordination. Notions such as Pro, traces, empty\ncategories are derived from Generative Grammar and some solutions are close to\nGovernment&Binding Theory, although they are the result of an autonomous\nresearch. These properties allow SYNTAGMA to manage complex syntactic\nconfigurations and well known weak points in parsing engineering. An important\nresource is the semantic network, which is used in disambiguation tasks.\nParsing process follows a bottom-up, rule driven strategy. Its behavior can be\ncontrolled and fine-tuned.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 15:27:51 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 02:26:11 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 20:23:31 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Christen", "Daniel", ""]]}, {"id": "1303.6175", "submitter": "Ramon Ferrer i Cancho", "authors": "R. Ferrer-i-Cancho, A. Hern\\'andez-Fern\\'andez, D. Lusseau, G.\n  Agoramoorthy, M. J. Hsu and S. Semple", "title": "Compression as a universal principle of animal behavior", "comments": "This is the pre-proofed version. The published version will be\n  available at\n  http://onlinelibrary.wiley.com/journal/10.1111/%28ISSN%291551-6709", "journal-ref": null, "doi": "10.1111/cogs.12061", "report-no": null, "categories": "q-bio.NC cs.CL cs.IT math.IT physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aim in biology and psychology is to identify fundamental principles\nunderpinning the behavior of animals, including humans. Analyses of human\nlanguage and the behavior of a range of non-human animal species have provided\nevidence for a common pattern underlying diverse behavioral phenomena: words\nfollow Zipf's law of brevity (the tendency of more frequently used words to be\nshorter), and conformity to this general pattern has been seen in the behavior\nof a number of other animals. It has been argued that the presence of this law\nis a sign of efficient coding in the information theoretic sense. However, no\nstrong direct connection has been demonstrated between the law and compression,\nthe information theoretic principle of minimizing the expected length of a\ncode. Here we show that minimizing the expected code length implies that the\nlength of a word cannot increase as its frequency increases. Furthermore, we\nshow that the mean code length or duration is significantly small in human\nlanguage, and also in the behavior of other species in all cases where\nagreement with the law of brevity has been found. We argue that compression is\na general principle of animal behavior, that reflects selection for efficiency\nof coding.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 15:43:48 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Ferrer-i-Cancho", "R.", ""], ["Hern\u00e1ndez-Fern\u00e1ndez", "A.", ""], ["Lusseau", "D.", ""], ["Agoramoorthy", "G.", ""], ["Hsu", "M. J.", ""], ["Semple", "S.", ""]]}, {"id": "1303.7310", "submitter": "Niraj Kumar", "authors": "Niraj Kumar, Rashmi Gangadharaiah, Kannan Srinathan and Vasudeva Varma", "title": "Exploring the Role of Logically Related Non-Question Phrases for\n  Answering Why-Questions", "comments": "Got accepted in NLDB-2013; as Paper ID: 23; Title: \"Exploring the\n  Role of Logically Related Non-Question Phrases for Answering Why-Questions\",\n  Withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we show that certain phrases although not present in a given\nquestion/query, play a very important role in answering the question. Exploring\nthe role of such phrases in answering questions not only reduces the dependency\non matching question phrases for extracting answers, but also improves the\nquality of the extracted answers. Here matching question phrases means phrases\nwhich co-occur in given question and candidate answers. To achieve the above\ndiscussed goal, we introduce a bigram-based word graph model populated with\nsemantic and topical relatedness of terms in the given document. Next, we apply\nan improved version of ranking with a prior-based approach, which ranks all\nwords in the candidate document with respect to a set of root words (i.e.\nnon-stopwords present in the question and in the candidate document). As a\nresult, terms logically related to the root words are scored higher than terms\nthat are not related to the root words. Experimental results show that our\ndevised system performs better than state-of-the-art for the task of answering\nWhy-questions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 06:31:40 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Kumar", "Niraj", ""], ["Gangadharaiah", "Rashmi", ""], ["Srinathan", "Kannan", ""], ["Varma", "Vasudeva", ""]]}]