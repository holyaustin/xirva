[{"id": "1603.00106", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Emily Cohn, John S. Brownstein,\n  and Naren Ramakrishnan", "title": "Characterizing Diseases from Unstructured Text: A Vocabulary Driven\n  Word2vec Approach", "comments": "this paper has been submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional disease surveillance can be augmented with a wide variety of\nreal-time sources such as, news and social media. However, these sources are in\ngeneral unstructured and, construction of surveillance tools such as\ntaxonomical correlations and trace mapping involves considerable human\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\nfrom the HealthMap news corpus. We use these word embeddings to automatically\ncreate disease taxonomies and evaluate our model against corresponding human\nannotated taxonomies. We compare our model accuracies against several\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\noutperforms traditional distributed vector representations in its ability to\nfaithfully capture taxonomical attributes across different class of diseases\nsuch as endemic, emerging and rare.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 00:45:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 20:45:50 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Cohn", "Emily", ""], ["Brownstein", "John S.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1603.00223", "submitter": "Liang Lu", "authors": "Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith and Steve Renals", "title": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", "comments": "5 pages, 2 figures, accepted by Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the segmental recurrent neural network for end-to-end acoustic\nmodelling. This model connects the segmental conditional random field (CRF)\nwith a recurrent neural network (RNN) used for feature extraction. Compared to\nmost previous CRF-based acoustic models, it does not rely on an external system\nto provide features or segmentation boundaries. Instead, this model\nmarginalises out all the possible segmentations, and features are extracted\nfrom the RNN trained together with the segmental CRF. In essence, this model is\nself-contained and can be trained end-to-end. In this paper, we discuss\npractical training and decoding issues as well as the method to speed up the\ntraining in the context of speech recognition. We performed experiments on the\nTIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass\ndecoding --- the best reported result using CRFs, despite the fact that we only\nused a zeroth-order CRF and without using any language model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 10:43:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 10:29:23 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lu", "Liang", ""], ["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""], ["Renals", "Steve", ""]]}, {"id": "1603.00260", "submitter": "Dhruv Gupta", "authors": "Dhruv Gupta", "title": "Event Search and Analytics: Detecting Events in Semantically Annotated\n  Corpora for Search and Analytics", "comments": "Extended research report of an extended abstract published at WSDM\n  2016 Doctoral Consortium. in WSDM 2016 Proceedings of the Ninth ACM\n  International Conference on Web Search and Data Mining", "journal-ref": null, "doi": "10.1145/2835776.2855083", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, I present the questions that I seek to answer in my PhD\nresearch. I posit to analyze natural language text with the help of semantic\nannotations and mine important events for navigating large text corpora.\nSemantic annotations such as named entities, geographic locations, and temporal\nexpressions can help us mine events from the given corpora. These events thus\nprovide us with useful means to discover the locked knowledge in them. I pose\nthree problems that can help unlock this knowledge vault in semantically\nannotated text corpora: i. identifying important events; ii. semantic search;\nand iii. event analytics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 13:14:33 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Gupta", "Dhruv", ""]]}, {"id": "1603.00375", "submitter": "Eliyahu Kiperwasser", "authors": "Eliyahu Kiperwasser, Yoav Goldberg", "title": "Easy-First Dependency Parsing with Hierarchical Tree LSTMs", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics,\n  4:445-461 (2016)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a compositional vector representation of parse trees that relies\non a recursive combination of recurrent-neural network encoders. To demonstrate\nits effectiveness, we use the representation as the backbone of a greedy,\nbottom-up dependency parser, achieving state-of-the-art accuracies for English\nand Chinese, without relying on external word embeddings. The parser's\nimplementation is available for download at the first author's webpage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 17:43:37 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 10:11:44 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Kiperwasser", "Eliyahu", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1603.00423", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Quantifying the vanishing gradient and long distance dependency problem\n  in recursive neural networks and recursive LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural networks (RNN) and their recently proposed extension\nrecursive long short term memory networks (RLSTM) are models that compute\nrepresentations for sentences, by recursively combining word embeddings\naccording to an externally provided parse tree. Both models thus, unlike\nrecurrent networks, explicitly make use of the hierarchical structure of a\nsentence. In this paper, we demonstrate that RNNs nevertheless suffer from the\nvanishing gradient and long distance dependency problem, and that RLSTMs\ngreatly improve over RNN's on these problems. We present an artificial learning\ntask that allows us to quantify the severity of these problems for both models.\nWe further show that a ratio of gradients (at the root node and a focal leaf\nnode) is highly indicative of the success of backpropagation at optimizing the\nrelevant weights low in the tree. This paper thus provides an explanation for\nexisting, superior results of RLSTMs on tasks such as sentiment analysis, and\nsuggests that the benefits of including hierarchical structure and of including\nLSTM-style gating are complementary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 19:45:25 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1603.00786", "submitter": "Nanyun Peng", "authors": "Nanyun Peng and Mark Dredze", "title": "Improving Named Entity Recognition for Chinese Social Media with Word\n  Segmentation Representation Learning", "comments": "This is the camera ready version of our ACL'16 paper. We also added a\n  supplementary material containing the results of our systems on a cleaner\n  dataset (much higher F1 scores). More information please refer to the repo\n  https://github.com/hltcoe/golden-horse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition, and other information extraction tasks, frequently\nuse linguistic features such as part of speech tags or chunkings. For languages\nwhere word boundaries are not readily identified in text, word segmentation is\na key first step to generating features for an NER system. While using word\nboundary tags as features are helpful, the signals that aid in identifying\nthese boundaries may provide richer information for an NER system. New\nstate-of-the-art word segmentation systems use neural models to learn\nrepresentations for predicting word boundaries. We show that these same\nrepresentations, jointly trained with an NER system, yield significant\nimprovements in NER for Chinese social media. In our experiments, jointly\ntraining NER and word segmentation with an LSTM-CRF model yields nearly 5%\nabsolute improvement over previously published results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:41:40 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 19:14:40 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Peng", "Nanyun", ""], ["Dredze", "Mark", ""]]}, {"id": "1603.00810", "submitter": "Marta R. Costa-Juss\\`a", "authors": "Marta R. Costa-Juss\\`a and Jos\\'e A. R. Fonollosa", "title": "Character-based Neural Machine Translation", "comments": "Accepted for publication at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (MT) has reached state-of-the-art results.\nHowever, one of the main challenges that neural MT still faces is dealing with\nvery large vocabularies and morphologically rich languages. In this paper, we\npropose a neural MT system using character-based embeddings in combination with\nconvolutional and highway layers to replace the standard lookup-based word\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\nembeddings are tested in a state-of-the-art neural MT based on an\nattention-based bidirectional recurrent neural network. The proposed MT scheme\nprovides improved results even when the source language is not morphologically\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 18:01:57 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 14:02:48 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 10:28:36 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Costa-Juss\u00e0", "Marta R.", ""], ["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1603.00892", "submitter": "Nikola Mrk\\v{s}i\\'c", "authors": "Nikola Mrk\\v{s}i\\'c and Diarmuid \\'O S\\'eaghdha and Blaise Thomson and\n  Milica Ga\\v{s}i\\'c and Lina Rojas-Barahona and Pei-Hao Su and David Vandyke\n  and Tsung-Hsien Wen and Steve Young", "title": "Counter-fitting Word Vectors to Linguistic Constraints", "comments": "Paper accepted for the 15th Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics (NAACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel counter-fitting method which injects\nantonymy and synonymy constraints into vector space representations in order to\nimprove the vectors' capability for judging semantic similarity. Applying this\nmethod to publicly available pre-trained word vectors leads to a new state of\nthe art performance on the SimLex-999 dataset. We also show how the method can\nbe used to tailor the word vector space for the downstream task of dialogue\nstate tracking, resulting in robust improvements across different dialogue\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 21:19:36 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Thomson", "Blaise", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Rojas-Barahona", "Lina", ""], ["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1603.00957", "submitter": "Kun Xu", "authors": "Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, Dongyan Zhao", "title": "Question Answering on Freebase via Relation Extraction and Textual\n  Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing knowledge-based question answering systems often rely on small\nannotated training data. While shallow methods like relation extraction are\nrobust to data scarcity, they are less expressive than the deep meaning\nrepresentation methods like semantic parsing, thereby failing at answering\nquestions involving multiple constraints. Here we alleviate this problem by\nempowering a relation extraction method with additional evidence from\nWikipedia. We first present a neural network based relation extractor to\nretrieve the candidate answers from Freebase, and then infer over Wikipedia to\nvalidate these answers. Experiments on the WebQuestions question answering\ndataset show that our method achieves an F_1 of 53.3%, a substantial\nimprovement over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 03:22:01 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 11:05:53 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 15:12:19 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Xu", "Kun", ""], ["Reddy", "Siva", ""], ["Feng", "Yansong", ""], ["Huang", "Songfang", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1603.00968", "submitter": "Ye Zhang", "authors": "Ye Zhang, Stephen Roller, Byron Wallace", "title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for\n  Sentence Classification", "comments": "This paper got accepted by NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel, simple convolution neural network (CNN) architecture -\nmulti-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of\nword embeddings for sentence classification. MGNC-CNN extracts features from\ninput embedding sets independently and then joins these at the penultimate\nlayer in the network to form a final feature vector. We then adopt a group\nregularization strategy that differentially penalizes weights associated with\nthe subcomponents generated from the respective embedding sets. This model is\nmuch simpler than comparable alternative architectures and requires\nsubstantially less training time. Furthermore, it is flexible in that it does\nnot require input word embeddings to be of the same dimensionality. We show\nthat MGNC-CNN consistently outperforms baseline models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 04:12:02 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2016 03:28:31 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Zhang", "Ye", ""], ["Roller", "Stephen", ""], ["Wallace", "Byron", ""]]}, {"id": "1603.01032", "submitter": "Javier Arias Navarro", "authors": "Javier Arias Navarro", "title": "Right Ideals of a Ring and Sublanguages of Science", "comments": "Keywords: Zellig Sabbetai Harris, Information Structure of Language,\n  Sublanguages of Science, Ideal Numbers, Ernst Kummer, Ideals, Richard\n  Dedekind, Ring Theory, Right Ideals, Emmy Noether, Order Theory, Marshall\n  Harvey Stone", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among Zellig Harris's numerous contributions to linguistics his theory of the\nsublanguages of science probably ranks among the most underrated. However, not\nonly has this theory led to some exhaustive and meaningful applications in the\nstudy of the grammar of immunology language and its changes over time, but it\nalso illustrates the nature of mathematical relations between chunks or subsets\nof a grammar and the language as a whole. This becomes most clear when dealing\nwith the connection between metalanguage and language, as well as when\nreflecting on operators.\n  This paper tries to justify the claim that the sublanguages of science stand\nin a particular algebraic relation to the rest of the language they are\nembedded in, namely, that of right ideals in a ring.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 09:33:09 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 10:52:54 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Navarro", "Javier Arias", ""]]}, {"id": "1603.01232", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona,\n  Pei-Hao Su, David Vandyke, Steve Young", "title": "Multi-domain Neural Network Language Generation for Spoken Dialogue\n  Systems", "comments": "Accepted as a long paper in NAACL-HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving from limited-domain natural language generation (NLG) to open domain\nis difficult because the number of semantic input combinations grows\nexponentially with the number of domains. Therefore, it is important to\nleverage existing resources and exploit similarities between domains to\nfacilitate domain adaptation. In this paper, we propose a procedure to train\nmulti-domain, Recurrent Neural Network-based (RNN) language generators via\nmultiple adaptation steps. In this procedure, a model is first trained on\ncounterfeited data synthesised from an out-of-domain dataset, and then fine\ntuned on a small set of in-domain utterances with a discriminative objective\nfunction. Corpus-based evaluation results show that the proposed procedure can\nachieve competitive performance in terms of BLEU score and slot error rate\nwhile significantly reducing the data needed to train generators in new, unseen\ndomains. In subjective testing, human judges confirm that the procedure greatly\nimproves generator performance when only a small amount of data is available in\nthe domain.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 19:49:32 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Gasic", "Milica", ""], ["Mrksic", "Nikola", ""], ["Rojas-Barahona", "Lina M.", ""], ["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Young", "Steve", ""]]}, {"id": "1603.01333", "submitter": "Lei Sha", "authors": "Lei Sha, Sujian Li, Baobao Chang, Zhifang Sui", "title": "Joint Learning Templates and Slots for Event Schema Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic event schema induction (AESI) means to extract meta-event from raw\ntext, in other words, to find out what types (templates) of event may exist in\nthe raw text and what roles (slots) may exist in each event type. In this\npaper, we propose a joint entity-driven model to learn templates and slots\nsimultaneously based on the constraints of templates and slots in the same\nsentence. In addition, the entities' semantic information is also considered\nfor the inner connectivity of the entities. We borrow the normalized cut\ncriteria in image segmentation to divide the entities into more accurate\ntemplate clusters and slot clusters. The experiment shows that our model gains\na relatively higher result than previous work.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 02:15:11 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Sha", "Lei", ""], ["Li", "Sujian", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "1603.01354", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Eduard Hovy", "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF", "comments": "10 pages, 3 figures. To appear on ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art sequence labeling systems traditionally require large\namounts of task-specific knowledge in the form of hand-crafted features and\ndata pre-processing. In this paper, we introduce a novel neutral network\narchitecture that benefits from both word- and character-level representations\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\nsystem is truly end-to-end, requiring no feature engineering or data\npre-processing, thus making it applicable to a wide range of sequence labeling\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\ncorpus for named entity recognition (NER). We obtain state-of-the-art\nperformance on both the two data --- 97.55\\% accuracy for POS tagging and\n91.21\\% F1 for NER.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 05:55:02 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 06:19:37 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 05:16:17 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 21:46:13 GMT"}, {"version": "v5", "created": "Sun, 29 May 2016 00:42:15 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Ma", "Xuezhe", ""], ["Hovy", "Eduard", ""]]}, {"id": "1603.01360", "submitter": "Guillaume Lample", "authors": "Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya\n  Kawakami, Chris Dyer", "title": "Neural Architectures for Named Entity Recognition", "comments": "Proceedings of NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art named entity recognition systems rely heavily on\nhand-crafted features and domain-specific knowledge in order to learn\neffectively from the small, supervised training corpora that are available. In\nthis paper, we introduce two new neural architectures---one based on\nbidirectional LSTMs and conditional random fields, and the other that\nconstructs and labels segments using a transition-based approach inspired by\nshift-reduce parsers. Our models rely on two sources of information about\nwords: character-based word representations learned from the supervised corpus\nand unsupervised word representations learned from unannotated corpora. Our\nmodels obtain state-of-the-art performance in NER in four languages without\nresorting to any language-specific knowledge or resources such as gazetteers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 06:36:29 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 03:11:58 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 15:09:36 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Lample", "Guillaume", ""], ["Ballesteros", "Miguel", ""], ["Subramanian", "Sandeep", ""], ["Kawakami", "Kazuya", ""], ["Dyer", "Chris", ""]]}, {"id": "1603.01417", "submitter": "Richard Socher", "authors": "Caiming Xiong, Stephen Merity, Richard Socher", "title": "Dynamic Memory Networks for Visual and Textual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 10:40:28 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Xiong", "Caiming", ""], ["Merity", "Stephen", ""], ["Socher", "Richard", ""]]}, {"id": "1603.01514", "submitter": "Nikhil Garg", "authors": "Nikhil Garg, James Henderson", "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian model of unsupervised semantic role induction in\nmultiple languages, and use it to explore the usefulness of parallel corpora\nfor this task. Our joint Bayesian model consists of individual models for each\nlanguage plus additional latent variables that capture alignments between roles\nacross languages. Because it is a generative Bayesian model, we can do\nevaluations in a variety of scenarios just by varying the inference procedure,\nwithout changing the model, thereby comparing the scenarios directly. We\ncompare using only monolingual data, using a parallel corpus, using a parallel\ncorpus with annotations in the other language, and using small amounts of\nannotation in the target language. We find that the biggest impact of adding a\nparallel corpus to training is actually the increase in mono-lingual data, with\nthe alignments to another language resulting in small improvements, even with\nlabeled data for the other language.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 16:03:53 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Garg", "Nikhil", ""], ["Henderson", "James", ""]]}, {"id": "1603.01520", "submitter": "Daniel Rubio Bonilla", "authors": "Daniel Rubio Bonilla, Colin W. Glass, Jan Kuper", "title": "Optimized Polynomial Evaluation with Semantic Annotations", "comments": "Part of the Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\n  2016, 7 pages, LaTeX, 4 PNG figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss how semantic annotations can be used to introduce\nmathematical algorithmic information of the underlying imperative code to\nenable compilers to produce code transformations that will enable better\nperformance. By using this approaches not only good performance is achieved,\nbut also better programmability, maintainability and portability across\ndifferent hardware architectures. To exemplify this we will use polynomial\nequations of different degrees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 16:13:24 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 14:17:38 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 11:31:59 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Bonilla", "Daniel Rubio", ""], ["Glass", "Colin W.", ""], ["Kuper", "Jan", ""]]}, {"id": "1603.01541", "submitter": "Dirk Roorda", "authors": "Martijn Naaijer and Dirk Roorda", "title": "Parallel Texts in the Hebrew Bible, New Methods and Visualizations", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article we develop an algorithm to detect parallel texts in the\nMasoretic Text of the Hebrew Bible. The results are presented online and\nchapters in the Hebrew Bible containing parallel passages can be inspected\nsynoptically. Differences between parallel passages are highlighted. In a\nsimilar way the MT of Isaiah is presented synoptically with 1QIsaa. We also\ninvestigate how one can investigate the degree of similarity between parallel\npassages with the help of a case study of 2 Kings 19-25 and its parallels in\nIsaiah, Jeremiah and 2 Chronicles.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 17:11:51 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Naaijer", "Martijn", ""], ["Roorda", "Dirk", ""]]}, {"id": "1603.01547", "submitter": "Ondrej Bajgar", "authors": "Rudolf Kadlec, Martin Schmid, Ondrej Bajgar and Jan Kleindienst", "title": "Text Understanding with the Attention Sum Reader Network", "comments": "Presented at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several large cloze-style context-question-answer datasets have been\nintroduced recently: the CNN and Daily Mail news data and the Children's Book\nTest. Thanks to the size of these datasets, the associated text comprehension\ntask is well suited for deep-learning techniques that currently seem to\noutperform all alternative approaches. We present a new, simple model that uses\nattention to directly pick the answer from the context as opposed to computing\nthe answer using a blended representation of words in the document as is usual\nin similar models. This makes the model particularly suitable for\nquestion-answering problems where the answer is a single word from the\ndocument. Ensemble of our models sets new state of the art on all evaluated\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 17:32:42 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 13:04:47 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Kadlec", "Rudolf", ""], ["Schmid", "Martin", ""], ["Bajgar", "Ondrej", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1603.01595", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan, Patrice Bellot, Frederic Bechet", "title": "Sentiment Analysis in Scholarly Book Reviews", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  So far different studies have tackled the sentiment analysis in several\ndomains such as restaurant and movie reviews. But, this problem has not been\nstudied in scholarly book reviews which is different in terms of review style\nand size. In this paper, we propose to combine different features in order to\nbe presented to a supervised classifiers which extract the opinion target\nexpressions and detect their polarities in scholarly book reviews. We construct\na labeled corpus for training and evaluating our methods in French book\nreviews. We also evaluate them on English restaurant reviews in order to\nmeasure their robustness across the domains and languages. The evaluation shows\nthat our methods are enough robust for English restaurant reviews and French\nbook reviews.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 20:04:31 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Hamdan", "Hussam", ""], ["Bellot", "Patrice", ""], ["Bechet", "Frederic", ""]]}, {"id": "1603.01597", "submitter": "Jeroen De Gussem", "authors": "Mike Kestemont, Jeroen De Gussem", "title": "Integrated Sequence Tagging for Medieval Latin Using Deep Representation\n  Learning", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages, Towards a\n  Digital Ecosystem: NLP. Corpus infrastructure. Methods for Retrieving Texts\n  and Computing Text Similarities (August 6, 2017) jdmdh:3835", "doi": "10.46298/jdmdh.1398", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two sequence tagging tasks for medieval Latin:\npart-of-speech tagging and lemmatization. These are both basic, yet\nfoundational preprocessing steps in applications such as text re-use detection.\nNevertheless, they are generally complicated by the considerable orthographic\nvariation which is typical of medieval Latin. In Digital Classics, these tasks\nare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.\nFor example, a lexicon is used to generate all the potential lemma-tag pairs\nfor a token, and next, a context-aware PoS-tagger is used to select the most\nappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,\nerror percolation is a major downside of such approaches. In this paper we\nexplore the possibility to elegantly solve these tasks using a single,\nintegrated approach. For this, we make use of a layered neural network\narchitecture from the field of deep representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 20:13:56 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 08:18:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kestemont", "Mike", ""], ["De Gussem", "Jeroen", ""]]}, {"id": "1603.01648", "submitter": "Gabriel Stanovsky", "authors": "Gabriel Stanovsky, Jessica Ficler, Ido Dagan, Yoav Goldberg", "title": "Getting More Out Of Syntax with PropS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic NLP applications often rely on dependency trees to recognize major\nelements of the proposition structure of sentences. Yet, while much semantic\nstructure is indeed expressed by syntax, many phenomena are not easily read out\nof dependency trees, often leading to further ad-hoc heuristic post-processing\nor to information loss. To directly address the needs of semantic applications,\nwe present PropS -- an output representation designed to explicitly and\nuniformly express much of the proposition structure which is implied from\nsyntax, and an associated tool for extracting it from dependency trees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 22:47:46 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Stanovsky", "Gabriel", ""], ["Ficler", "Jessica", ""], ["Dagan", "Ido", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1603.01833", "submitter": "Giuliano Lancioni", "authors": "Giuliano Lancioni, Valeria Pettinari, Laura Garofalo, Marta\n  Campanelli, Ivana Pepe, Simona Olivieri, Ilaria Cicola", "title": "Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive\n  Disambiguation: the eXtended Revised AraMorph (XRAM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extended, revised form of Tim Buckwalter's Arabic lexical and\nmorphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM),\nis presented which addresses a number of weaknesses and inconsistencies of the\noriginal model by allowing a wider coverage of real-world Classical and\ncontemporary (both formal and informal) Arabic texts. Building upon previous\nresearch, XRAM enhancements include (i) flag-selectable usage markers, (ii)\nprobabilistic mildly context-sensitive POS tagging, filtering, disambiguation\nand ranking of alternative morphological analyses, (iii) semi-automatic\nincrement of lexical coverage through extraction of lexical and morphological\ninformation from existing lexical resources. Testing of XRAM through a\nfront-end Python module showed a remarkable success level.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 14:12:30 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Lancioni", "Giuliano", ""], ["Pettinari", "Valeria", ""], ["Garofalo", "Laura", ""], ["Campanelli", "Marta", ""], ["Pepe", "Ivana", ""], ["Olivieri", "Simona", ""], ["Cicola", "Ilaria", ""]]}, {"id": "1603.01913", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Gholamreza Haffari and Jacob Eisenstein", "title": "A Latent Variable Recurrent Neural Network for Discourse Relation\n  Language Models", "comments": "NAACL 2016 camera ready, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel latent variable recurrent neural network\narchitecture for jointly modeling sequences of words and (possibly latent)\ndiscourse relations between adjacent sentences. A recurrent neural network\ngenerates individual words, thus reaping the benefits of\ndiscriminatively-trained vector representations. The discourse relations are\nrepresented with a latent variable, which can be predicted or marginalized,\ndepending on the task. The resulting model can therefore employ a training\nobjective that includes not only discourse relation classification, but also\nword prediction. As a result, it outperforms state-of-the-art alternatives for\ntwo tasks: implicit discourse relation classification in the Penn Discourse\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\nby marginalizing over latent discourse relations at test time, we obtain a\ndiscourse informed language model, which improves over a strong LSTM baseline.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 01:54:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:58:10 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Ji", "Yangfeng", ""], ["Haffari", "Gholamreza", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1603.01987", "submitter": "Vittoria Cozza", "authors": "Vittoria Cozza and Marinella Petrocchi and Angelo Spognardi", "title": "A matter of words: NLP for quality evaluation of Wikipedia medical\n  articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Automatic quality evaluation of Web information is a task with many fields of\napplications and of great relevance, especially in critical domains like the\nmedical one. We move from the intuition that the quality of content of medical\nWeb documents is affected by features related with the specific domain. First,\nthe usage of a specific vocabulary (Domain Informativeness); then, the adoption\nof specific codes (like those used in the infoboxes of Wikipedia articles) and\nthe type of document (e.g., historical and technical ones). In this paper, we\npropose to leverage specific domain features to improve the results of the\nevaluation of Wikipedia medical articles. In particular, we evaluate the\narticles adopting an \"actionable\" model, whose features are related to the\ncontent of the articles, so that the model can also directly suggest strategies\nfor improving a given article quality. We rely on Natural Language Processing\n(NLP) and dictionaries-based techniques in order to extract the bio-medical\nconcepts in a text. We prove the effectiveness of our approach by classifying\nthe medical articles of the Wikipedia Medicine Portal, which have been\npreviously manually labeled by the Wiki Project team. The results of our\nexperiments confirm that, by considering domain-oriented features, it is\npossible to obtain sensible improvements with respect to existing solutions,\nmainly for those articles that other approaches have less correctly classified.\nOther than being interesting by their own, the results call for further\nresearch in the area of domain specific features suitable for Web data quality\nassessment.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 09:54:11 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Cozza", "Vittoria", ""], ["Petrocchi", "Marinella", ""], ["Spognardi", "Angelo", ""]]}, {"id": "1603.02488", "submitter": "Shimaa Abd El-Salam Mohamed", "authors": "Shimaa M. Abd El-salam, Enas M.F. El Houby, A.K. Al Sammak and T.A.\n  El-Shishtawy", "title": "Extracting Arabic Relations from the Web", "comments": "18 pages, 5 figure,7 tables in International Journal of Computer\n  Science & Information Technology (IJCSIT) Vol 8, No 1, February 2016", "journal-ref": null, "doi": "10.5121/ijcsit.2016.8107", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this research is to extract a large list or table from named\nentities and relations in a specific domain. A small set of a handful of\ninstance relations is required as input from the user. The system exploits\nsummaries from Google search engine as a source text. These instances are used\nto extract patterns. The output is a set of new entities and their relations.\nThe results from four experiments show that precision and recall varies\naccording to relation type. Precision ranges from 0.61 to 0.75 while recall\nranges from 0.71 to 0.83. The best result is obtained for (player, club)\nrelationship, 0.72 and 0.83 for precision and recall respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 11:47:35 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["El-salam", "Shimaa M. Abd", ""], ["Houby", "Enas M. F. El", ""], ["Sammak", "A. K. Al", ""], ["El-Shishtawy", "T. A.", ""]]}, {"id": "1603.02514", "submitter": "Weidi Xu", "authors": "Weidi Xu, Haoze Sun, Chao Deng, Ying Tan", "title": "Variational Autoencoders for Semi-supervised Text Classification", "comments": "8 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Although semi-supervised variational autoencoder (SemiVAE) works in image\nclassification task, it fails in text classification task if using vanilla LSTM\nas its decoder. From a perspective of reinforcement learning, it is verified\nthat the decoder's capability to distinguish between different categorical\nlabels is essential. Therefore, Semi-supervised Sequential Variational\nAutoencoder (SSVAE) is proposed, which increases the capability by feeding\nlabel into its decoder RNN at each time-step. Two specific decoder structures\nare investigated and both of them are verified to be effective. Besides, in\norder to reduce the computational complexity in training, a novel optimization\nmethod is proposed, which estimates the gradient of the unlabeled objective\nfunction by sampling, along with two variance reduction techniques.\nExperimental results on Large Movie Review Dataset (IMDB) and AG's News corpus\nshow that the proposed approach significantly improves the classification\naccuracy compared with pure-supervised classifiers, and achieves competitive\nperformance against previous advanced methods. State-of-the-art results can be\nobtained by integrating other pretraining-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 13:24:45 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 14:33:50 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 08:18:31 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Xu", "Weidi", ""], ["Sun", "Haoze", ""], ["Deng", "Chao", ""], ["Tan", "Ying", ""]]}, {"id": "1603.02604", "submitter": "Ralf Steinberger", "authors": "Ralf Steinberger, Aldo Podavini, Alexandra Balahur, Guillaume Jacquet,\n  Hristo Tanev, Jens Linge, Martin Atkinson, Michele Chinosi, Vanni Zavarella,\n  Yaniv Steiner, Erik van der Goot", "title": "Observing Trends in Automated Multilingual Media Analysis", "comments": "Proceedings of the Symposium on New Frontiers of Automated Content\n  Analysis in the Social Sciences (ACA'2015), Z\\\"urich, Switzerland, 1-3 July\n  2015 (20 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any large organisation, be it public or private, monitors the media for\ninformation to keep abreast of developments in their field of interest, and\nusually also to become aware of positive or negative opinions expressed towards\nthem. At least for the written media, computer programs have become very\nefficient at helping the human analysts significantly in their monitoring task\nby gathering media reports, analysing them, detecting trends and - in some\ncases - even to issue early warnings or to make predictions of likely future\ndevelopments. We present here trend recognition-related functionality of the\nEurope Media Monitor (EMM) system, which was developed by the European\nCommission's Joint Research Centre (JRC) for public administrations in the\nEuropean Union (EU) and beyond. EMM performs large-scale media analysis in up\nto seventy languages and recognises various types of trends, some of them\ncombining information from news articles written in different languages and\nfrom social media posts. EMM also lets users explore the huge amount of\nmultilingual media data through interactive maps and graphs, allowing them to\nexamine the data from various view points and according to multiple criteria. A\nlot of EMM's functionality is accessibly freely over the internet or via apps\nfor hand-held devices.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 17:43:48 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Steinberger", "Ralf", ""], ["Podavini", "Aldo", ""], ["Balahur", "Alexandra", ""], ["Jacquet", "Guillaume", ""], ["Tanev", "Hristo", ""], ["Linge", "Jens", ""], ["Atkinson", "Martin", ""], ["Chinosi", "Michele", ""], ["Zavarella", "Vanni", ""], ["Steiner", "Yaniv", ""], ["van der Goot", "Erik", ""]]}, {"id": "1603.02618", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham, Marco Baroni", "title": "The red one!: On learning to refer to things based on their\n  discriminative properties", "comments": "Accepted as an ACL-short sumbmission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a first step towards agents learning to communicate about their visual\nenvironment, we propose a system that, given visual representations of a\nreferent (cat) and a context (sofa), identifies their discriminative\nattributes, i.e., properties that distinguish them (has_tail). Moreover,\ndespite the lack of direct supervision at the attribute level, the model learns\nto assign plausible attributes to objects (sofa-has_cushion). Finally, we\npresent a preliminary experiment confirming the referential success of the\npredicted discriminative attributes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:39:46 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 17:04:15 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1603.02776", "submitter": "Yang Liu", "authors": "Yang Liu, Sujian Li, Xiaodong Zhang and Zhifang Sui", "title": "Implicit Discourse Relation Classification via Multi-Task Neural\n  Networks", "comments": "This is the pre-print version of a paper accepted by AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without discourse connectives, classifying implicit discourse relations is a\nchallenging task and a bottleneck for building a practical discourse parser.\nPrevious research usually makes use of one kind of discourse framework such as\nPDTB or RST to improve the classification performance on discourse relations.\nActually, under different discourse annotation frameworks, there exist multiple\ncorpora which have internal connections. To exploit the combination of\ndifferent discourse corpora, we design related discourse classification tasks\nspecific to a corpus, and propose a novel Convolutional Neural Network embedded\nmulti-task learning system to synthesize these tasks by learning both unique\nand shared representations for each task. The experimental results on the PDTB\nimplicit discourse relation classification task demonstrate that our model\nachieves significant gains over baseline systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 03:13:37 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Liu", "Yang", ""], ["Li", "Sujian", ""], ["Zhang", "Xiaodong", ""], ["Sui", "Zhifang", ""]]}, {"id": "1603.02845", "submitter": "Herman Kamper", "authors": "Herman Kamper, Aren Jansen, Sharon Goldwater", "title": "Unsupervised word segmentation and lexicon discovery using acoustic word\n  embeddings", "comments": "11 pages, 8 figures; Accepted to the IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing", "journal-ref": "IEEE/ACM Trans. Audio, Speech, Language Process. 24 (2016) 669-679", "doi": "10.1109/TASLP.2016.2517567", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In settings where only unlabelled speech data is available, speech technology\nneeds to be developed without transcriptions, pronunciation dictionaries, or\nlanguage modelling text. A similar problem is faced when modelling infant\nlanguage acquisition. In these cases, categorical linguistic structure needs to\nbe discovered directly from speech audio. We present a novel unsupervised\nBayesian model that segments unlabelled speech and clusters the segments into\nhypothesized word groupings. The result is a complete unsupervised tokenization\nof the input speech in terms of discovered word types. In our approach, a\npotential word segment (of arbitrary length) is embedded in a fixed-dimensional\nacoustic vector space. The model, implemented as a Gibbs sampler, then builds a\nwhole-word acoustic model in this space while jointly performing segmentation.\nWe report word error rates in a small-vocabulary connected digit recognition\ntask by mapping the unsupervised decoded output to ground truth transcriptions.\nThe model achieves around 20% error rate, outperforming a previous HMM-based\nsystem by about 10% absolute. Moreover, in contrast to the baseline, our model\ndoes not require a pre-specified vocabulary size.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 11:14:23 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Kamper", "Herman", ""], ["Jansen", "Aren", ""], ["Goldwater", "Sharon", ""]]}, {"id": "1603.02905", "submitter": "Adel Rahimi", "authors": "Adel Rahimi", "title": "Lexical bundles in computational linguistics academic literature", "comments": "16 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we analyzed a corpus of 8 million words academic literature\nfrom Computational lingustics' academic literature. the lexical bundles from\nthis corpus are categorized based on structures and functions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 14:56:44 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 19:20:47 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Rahimi", "Adel", ""]]}, {"id": "1603.03112", "submitter": "Lifu Huang", "authors": "Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji", "title": "Building a Fine-Grained Entity Typing System Overnight for a New X (X =\n  Language, Domain, Genre)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent research has shown great progress on fine-grained entity typing. Most\nexisting methods require pre-defining a set of types and training a multi-class\nclassifier from a large labeled data set based on multi-level linguistic\nfeatures. They are thus limited to certain domains, genres and languages. In\nthis paper, we propose a novel unsupervised entity typing framework by\ncombining symbolic and distributional semantics. We start from learning general\nembeddings for each entity mention, compose the embeddings of specific contexts\nusing linguistic structures, link the mention to knowledge bases and learn its\nrelated knowledge representations. Then we develop a novel joint hierarchical\nclustering and linking algorithm to type all mentions using these\nrepresentations. This framework doesn't rely on any annotated data, predefined\ntyping schema, or hand-crafted features, therefore it can be quickly adapted to\na new domain, genre and language. Furthermore, it has great flexibility at\nincorporating linguistic structures (e.g., Abstract Meaning Representation\n(AMR), dependency relations) to improve specific context representation.\nExperiments on genres (news and discussion forum) show comparable performance\nwith state-of-the-art supervised typing systems trained from a large amount of\nlabeled data. Results on various languages (English, Chinese, Japanese, Hausa,\nand Yoruba) and domains (general and biomedical) demonstrate the portability of\nour framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 00:33:28 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Huang", "Lifu", ""], ["May", "Jonathan", ""], ["Pan", "Xiaoman", ""], ["Ji", "Heng", ""]]}, {"id": "1603.03144", "submitter": "Yi Yang", "authors": "Yi Yang and Jacob Eisenstein", "title": "Part-of-Speech Tagging for Historical English", "comments": "Accepted to NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more historical texts are digitized, there is interest in applying natural\nlanguage processing tools to these archives. However, the performance of these\ntools is often unsatisfactory, due to language change and genre differences.\nSpelling normalization heuristics are the dominant solution for dealing with\nhistorical texts, but this approach fails to account for changes in usage and\nvocabulary. In this empirical paper, we assess the capability of domain\nadaptation techniques to cope with historical texts, focusing on the classic\nbenchmark task of part-of-speech tagging. We evaluate several domain adaptation\nmethods on the task of tagging Early Modern English and Modern British English\ntexts in the Penn Corpora of Historical English. We demonstrate that the\nFeature Embedding method for unsupervised domain adaptation outperforms word\nembeddings and Brown clusters, showing the importance of embedding the entire\nfeature space, rather than just individual words. Feature Embeddings also give\nbetter performance than spelling normalization, but the combination of the two\nmethods is better still, yielding a 5% raw improvement in tagging accuracy on\nEarly Modern English texts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 04:27:15 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 16:59:38 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Yang", "Yi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1603.03153", "submitter": "Bohdan Khomtchouk", "authors": "Bohdan B. Khomtchouk, Claes Wahlestedt", "title": "Zipf's law emerges asymptotically during phase transitions in\n  communicative systems", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law predicts a power-law relationship between word rank and frequency\nin language communication systems, and is widely reported in texts yet remains\nenigmatic as to its origins. Computer simulations have shown that language\ncommunication systems emerge at an abrupt phase transition in the fidelity of\nmappings between symbols and objects. Since the phase transition approximates\nthe Heaviside or step function, we show that Zipfian scaling emerges\nasymptotically at high rank based on the Laplace transform. We thereby\ndemonstrate that Zipf's law gradually emerges from the moment of phase\ntransition in communicative systems. We show that this power-law scaling\nbehavior explains the emergence of natural languages at phase transitions. We\nfind that the emergence of Zipf's law during language communication suggests\nthat the use of rare words in a lexicon is critical for the construction of an\neffective communicative system at the phase transition.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 06:01:28 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 03:40:12 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Khomtchouk", "Bohdan B.", ""], ["Wahlestedt", "Claes", ""]]}, {"id": "1603.03170", "submitter": "Laurent Romary", "authors": "Laurent Romary (CMB, ALPAGE), Mike Mertens, Anne Baillot (CMB, ALPAGE)", "title": "Data fluidity in DARIAH -- pushing the agenda forward", "comments": null, "journal-ref": "BIBLIOTHEK Forschung und Praxis, De Gruyter, 2016, 39 (3),\n  pp.350-357", "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides both an update concerning the setting up of the European\nDARIAH infrastructure and a series of strong action lines related to the\ndevelopment of a data centred strategy for the humanities in the coming years.\nIn particular we tackle various aspect of data management: data hosting, the\nsetting up of a DARIAH seal of approval, the establishment of a charter between\ncultural heritage institutions and scholars and finally a specific view on\ncertification mechanisms for data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 07:43:15 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 07:54:58 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Romary", "Laurent", "", "CMB, ALPAGE"], ["Mertens", "Mike", "", "CMB, ALPAGE"], ["Baillot", "Anne", "", "CMB, ALPAGE"]]}, {"id": "1603.03185", "submitter": "Ouais Alsharif", "authors": "Ian McGraw, Rohit Prabhavalkar, Raziel Alvarez, Montse Gonzalez\n  Arenas, Kanishka Rao, David Rybach, Ouais Alsharif, Hasim Sak, Alexander\n  Gruenstein, Francoise Beaufays, Carolina Parada", "title": "Personalized Speech recognition on mobile devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a large vocabulary speech recognition system that is accurate,\nhas low latency, and yet has a small enough memory and computational footprint\nto run faster than real-time on a Nexus 5 Android smartphone. We employ a\nquantized Long Short-Term Memory (LSTM) acoustic model trained with\nconnectionist temporal classification (CTC) to directly predict phoneme\ntargets, and further reduce its memory footprint using an SVD-based compression\nscheme. Additionally, we minimize our memory footprint by using a single\nlanguage model for both dictation and voice command domains, constructed using\nBayesian interpolation. Finally, in order to properly handle device-specific\ninformation, such as proper names and other context-dependent information, we\ninject vocabulary items into the decoder graph and bias the language model\non-the-fly. Our system achieves 13.5% word error rate on an open-ended\ndictation task, running with a median speed that is seven times faster than\nreal-time.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 08:51:51 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 22:25:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["McGraw", "Ian", ""], ["Prabhavalkar", "Rohit", ""], ["Alvarez", "Raziel", ""], ["Arenas", "Montse Gonzalez", ""], ["Rao", "Kanishka", ""], ["Rybach", "David", ""], ["Alsharif", "Ouais", ""], ["Sak", "Hasim", ""], ["Gruenstein", "Alexander", ""], ["Beaufays", "Francoise", ""], ["Parada", "Carolina", ""]]}, {"id": "1603.03610", "submitter": "Mark-Jan Nederhof", "authors": "Mark-Jan Nederhof", "title": "A short proof that $O_2$ is an MCFL", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new proof that $O_2$ is a multiple context-free language. It\ncontrasts with a recent proof by Salvati (2015) in its avoidance of concepts\nthat seem specific to two-dimensional geometry, such as the complex exponential\nfunction. Our simple proof creates realistic prospects of widening the results\nto higher dimensions. This finding is of central importance to the relation\nbetween extreme free word order and classes of grammars used to describe the\nsyntax of natural language.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 12:32:29 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Nederhof", "Mark-Jan", ""]]}, {"id": "1603.03758", "submitter": "Dane Bell", "authors": "Dane Bell and Gus Hahn-Powell and Marco A. Valenzuela-Esc\\'arcega and\n  Mihai Surdeanu", "title": "Sieve-based Coreference Resolution in the Biomedical Domain", "comments": "This paper appears in LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe challenges and advantages unique to coreference resolution in the\nbiomedical domain, and a sieve-based architecture that leverages domain\nknowledge for both entity and event coreference resolution. Domain-general\ncoreference resolution algorithms perform poorly on biomedical documents,\nbecause the cues they rely on such as gender are largely absent in this domain,\nand because they do not encode domain-specific knowledge such as the number and\ntype of participants required in chemical reactions. Moreover, it is difficult\nto directly encode this knowledge into most coreference resolution algorithms\nbecause they are not rule-based. Our rule-based architecture uses sequentially\napplied hand-designed \"sieves\", with the output of each sieve informing and\nconstraining subsequent sieves. This architecture provides a 3.2% increase in\nthroughput to our Reach event extraction system with precision parallel to that\nof the stricter system that relies solely on syntactic patterns for extraction.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 20:48:49 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 18:32:35 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Bell", "Dane", ""], ["Hahn-Powell", "Gus", ""], ["Valenzuela-Esc\u00e1rcega", "Marco A.", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1603.03784", "submitter": "Dane Bell", "authors": "Dane Bell and Daniel Fried and Luwen Huangfu and Mihai Surdeanu and\n  Stephen Kobourov", "title": "Towards using social media to identify individuals at risk for\n  preventable chronic illness", "comments": "This paper will appear in LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a strategy for the acquisition of training data necessary to\nbuild a social-media-driven early detection system for individuals at risk for\n(preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-like\nquiz with data and questions acquired semi-automatically from Twitter. The\nquestions are designed to inspire participant engagement and collect relevant\ndata to train a public-health model applied to individuals. Prior systems\ndesigned to use social media such as Twitter to predict obesity (a risk factor\nfor T2DM) operate on entire communities such as states, counties, or cities,\nbased on statistics gathered by government agencies. Because there is\nconsiderable variation among individuals within these groups, training data on\nthe individual level would be more effective, but this data is difficult to\nacquire. The approach proposed here aims to address this issue. Our strategy\nhas two steps. First, we trained a random forest classifier on data gathered\nfrom (public) Twitter statuses and state-level statistics with state-of-the-art\naccuracy. We then converted this classifier into a 20-questions-style quiz and\nmade it available online. In doing so, we achieved high engagement with\nindividuals that took the quiz, while also building a training set of\nvoluntarily supplied individual-level data for future classification.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:09:19 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Bell", "Dane", ""], ["Fried", "Daniel", ""], ["Huangfu", "Luwen", ""], ["Surdeanu", "Mihai", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1603.03793", "submitter": "Miguel Ballesteros", "authors": "Miguel Ballesteros, Yoav Goldberg, Chris Dyer, Noah A. Smith", "title": "Training with Exploration Improves a Greedy Stack-LSTM Parser", "comments": "In proceedings of EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to\nsupport a training-with-exploration procedure using dynamic oracles(Goldberg\nand Nivre, 2013) instead of cross-entropy minimization. This form of training,\nwhich accounts for model predictions at training time rather than assuming an\nerror-free action history, improves parsing accuracies for both English and\nChinese, obtaining very strong results for both languages. We discuss some\nmodifications needed in order to get training with exploration to work well for\na probabilistic neural-network.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:34:20 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 14:54:51 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Ballesteros", "Miguel", ""], ["Goldberg", "Yoav", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1603.03827", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt", "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks", "comments": "Accepted as a conference paper at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 00:02:51 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""]]}, {"id": "1603.03873", "submitter": "Biao Zhang", "authors": "Biao Zhang, Deyi Xiong, Jinsong Su", "title": "Neural Discourse Relation Recognition with Semantic Memory", "comments": "7 pages", "journal-ref": null, "doi": "10.1016/j.neucom.2017.09.074", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans comprehend the meanings and relations of discourses heavily relying on\ntheir semantic memory that encodes general knowledge about concepts and facts.\nInspired by this, we propose a neural recognizer for implicit discourse\nrelation analysis, which builds upon a semantic memory that stores knowledge in\na distributed fashion. We refer to this recognizer as SeMDER. Starting from\nword embeddings of discourse arguments, SeMDER employs a shallow encoder to\ngenerate a distributed surface representation for a discourse. A semantic\nencoder with attention to the semantic memory matrix is further established\nover surface representations. It is able to retrieve a deep semantic meaning\nrepresentation for the discourse from the memory. Using the surface and\nsemantic representations as input, SeMDER finally predicts implicit discourse\nrelations via a neural recognizer. Experiments on the benchmark data set show\nthat SeMDER benefits from the semantic memory and achieves substantial\nimprovements of 2.56\\% on average over current state-of-the-art baselines in\nterms of F1-score.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 08:54:16 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Zhang", "Biao", ""], ["Xiong", "Deyi", ""], ["Su", "Jinsong", ""]]}, {"id": "1603.03876", "submitter": "Biao Zhang", "authors": "Biao Zhang, Deyi Xiong, Jinsong Su, Qun Liu, Rongrong Ji, Hong Duan,\n  Min Zhang", "title": "Variational Neural Discourse Relation Recognizer", "comments": "10 pages, accepted at emnlp 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit discourse relation recognition is a crucial component for automatic\ndiscourselevel analysis and nature language understanding. Previous studies\nexploit discriminative models that are built on either powerful manual features\nor deep discourse representations. In this paper, instead, we explore\ngenerative models and propose a variational neural discourse relation\nrecognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed\nprobabilistic model with a latent continuous variable that generates both a\ndiscourse and the relation between the two arguments of the discourse. In order\nto perform efficient inference and learning, we introduce neural discourse\nrelation models to approximate the prior and posterior distributions of the\nlatent variable, and employ these approximated distributions to optimize a\nreparameterized variational lower bound. This allows VarNDRR to be trained with\nstandard stochastic gradient methods. Experiments on the benchmark data set\nshow that VarNDRR can achieve comparable results against stateof- the-art\nbaselines without using any manual features.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 09:11:30 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 23:33:44 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Zhang", "Biao", ""], ["Xiong", "Deyi", ""], ["Su", "Jinsong", ""], ["Liu", "Qun", ""], ["Ji", "Rongrong", ""], ["Duan", "Hong", ""], ["Zhang", "Min", ""]]}, {"id": "1603.04236", "submitter": "Nicolai Winther-Nielsen", "authors": "Nicolai Winther-Nielsen (FIUC-Dk)", "title": "Interactive Tools and Tasks for the Hebrew Bible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution to a special issue on \"Computer-aided processing of\nintertextuality\" in ancient texts will illustrate how using digital tools to\ninteract with the Hebrew Bible offers new promising perspectives for\nvisualizing the texts and for performing tasks in education and research. This\ncontribution explores how the corpus of the Hebrew Bible created and maintained\nby the Eep Talstra Centre for Bible and Computer can support new methods for\nmodern knowledge workers within the field of digital humanities and theology be\napplied to ancient texts, and how this can be envisioned as a new field of\ndigital intertextuality. The article first describes how the corpus was used to\ndevelop the Bible Online Learner as a persuasive technology to enhance language\nlearning with, in, and around a database that acts as the engine driving\ninteractive tasks for learners. Intertextuality in this case is a matter of\nactive exploration and ongoing practice. Furthermore, interactive\ncorpus-technology has an important bearing on the task of textual criticism as\na specialized area of research that depends increasingly on the availability of\ndigital resources. Commercial solutions developed by software companies like\nLogos and Accordance offer a market-based intertextuality defined by the\nproduction of advanced digital resources for scholars and students as useful\nalternatives to often inaccessible and expensive printed versions. It is\nreasonable to expect that in the future interactive corpus technology will\nallow scholars to do innovative academic tasks in textual criticism and\ninterpretation. We have already seen the emergence of promising tools for text\ncategorization, analysis of translation shifts, and interpretation. Broadly\nspeaking, interactive tools and tasks within the three areas of language\nlearning, textual criticism, and Biblical studies illustrate a new kind of\nintertextuality emerging within digital humanities.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 12:33:00 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 13:28:05 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 14:54:10 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 14:49:16 GMT"}, {"version": "v5", "created": "Tue, 24 Oct 2017 06:18:02 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Winther-Nielsen", "Nicolai", "", "FIUC-Dk"]]}, {"id": "1603.04351", "submitter": "Eliyahu Kiperwasser", "authors": "Eliyahu Kiperwasser, Yoav Goldberg", "title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature\n  Representations", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics,\n  4:313--327 (2016)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective scheme for dependency parsing which is\nbased on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with\na BiLSTM vector representing the token in its sentential context, and feature\nvectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is\ntrained jointly with the parser objective, resulting in very effective feature\nextractors for parsing. We demonstrate the effectiveness of the approach by\napplying it to a greedy transition-based parser as well as to a globally\noptimized graph-based parser. The resulting parsers have very simple\narchitectures, and match or surpass the state-of-the-art accuracies on English\nand Chinese.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 17:18:27 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 09:45:18 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2016 15:17:29 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Kiperwasser", "Eliyahu", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1603.04513", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Hinrich Sch\\\"utze", "title": "Multichannel Variable-Size Convolution for Sentence Classification", "comments": "in Proceeding of CoNLL2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MVCNN, a convolution neural network (CNN) architecture for\nsentence classification. It (i) combines diverse versions of pretrained word\nembeddings and (ii) extracts features of multigranular phrases with\nvariable-size convolution filters. We also show that pretraining MVCNN is\ncritical for good performance. MVCNN achieves state-of-the-art performance on\nfour tasks: on small-scale binary, small-scale multi-class and largescale\nTwitter sentiment prediction and on subjectivity classification.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 00:25:02 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Yin", "Wenpeng", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1603.04553", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma and Zhengzhong Liu and Eduard Hovy", "title": "Unsupervised Ranking Model for Entity Coreference Resolution", "comments": "Accepted by NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coreference resolution is one of the first stages in deep language\nunderstanding and its importance has been well recognized in the natural\nlanguage processing community. In this paper, we propose a generative,\nunsupervised ranking model for entity coreference resolution by introducing\nresolution mode variables. Our unsupervised system achieves 58.44% F1 score of\nthe CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan\net al., 2012), outperforming the Stanford deterministic system (Lee et al.,\n2013) by 3.01%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:39:15 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Ma", "Xuezhe", ""], ["Liu", "Zhengzhong", ""], ["Hovy", "Eduard", ""]]}, {"id": "1603.04747", "submitter": "Ramandeep Randhawa", "authors": "Ramandeep S Randhawa and Parag Jain and Gagan Madan", "title": "Topic Modeling Using Distributed Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for topic modeling, Vec2Topic, that identifies the\nmain topics in a corpus using semantic information captured via\nhigh-dimensional distributed word embeddings. Our technique is unsupervised and\ngenerates a list of topics ranked with respect to importance. We find that it\nworks better than existing topic modeling techniques such as Latent Dirichlet\nAllocation for identifying key topics in user-generated content, such as\nemails, chats, etc., where topics are diffused across the corpus. We also find\nthat Vec2Topic works equally well for non-user generated content, such as\npapers, reports, etc., and for small corpora such as a single-document.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 16:21:58 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Randhawa", "Ramandeep S", ""], ["Jain", "Parag", ""], ["Madan", "Gagan", ""]]}, {"id": "1603.04767", "submitter": "Eneko Agirre", "authors": "Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning and\n  Eneko Agirre", "title": "Evaluating the word-expert approach for Named-Entity Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Disambiguation (NED) is the task of linking a named-entity\nmention to an instance in a knowledge-base, typically Wikipedia. This task is\nclosely related to word-sense disambiguation (WSD), where the supervised\nword-expert approach has prevailed. In this work we present the results of the\nword-expert approach to NED, where one classifier is built for each target\nentity mention string. The resources necessary to build the system, a\ndictionary and a set of training instances, have been automatically derived\nfrom Wikipedia. We provide empirical evidence of the value of this approach, as\nwell as a study of the differences between WSD and NED, including ambiguity and\nsynonymy statistics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 17:16:02 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Chang", "Angel X.", ""], ["Spitkovsky", "Valentin I.", ""], ["Manning", "Christopher D.", ""], ["Agirre", "Eneko", ""]]}, {"id": "1603.05118", "submitter": "Stanislau Semeniuta", "authors": "Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth", "title": "Recurrent Dropout without Memory Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to recurrent neural network (RNN)\nregularization. Differently from the widely adopted dropout method, which is\napplied to \\textit{forward} connections of feed-forward architectures or RNNs,\nwe propose to drop neurons directly in \\textit{recurrent} connections in a way\nthat does not cause loss of long-term memory. Our approach is as easy to\nimplement and apply as the regular feed-forward dropout and we demonstrate its\neffectiveness for Long Short-Term Memory network, the most popular type of RNN\ncells. Our experiments on NLP benchmarks show consistent improvements even when\ncombined with conventional feed-forward dropout.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 14:33:47 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 09:59:25 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Semeniuta", "Stanislau", ""], ["Severyn", "Aliaksei", ""], ["Barth", "Erhardt", ""]]}, {"id": "1603.05157", "submitter": "Heike Adel", "authors": "Heike Adel and Benjamin Roth and Hinrich Sch\\\"utze", "title": "Comparing Convolutional Neural Networks to Traditional Models for Slot\n  Filling", "comments": "NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address relation classification in the context of slot filling, the task\nof finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X\nfounded Apple\". We propose a convolutional neural network which splits the\ninput sentence into three parts according to the relation arguments and compare\nit to state-of-the-art and traditional approaches of relation classification.\nFinally, we combine different methods and show that the combination is better\nthan individual approaches. We also analyze the effect of genre differences on\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 16:02:03 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 14:54:28 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Adel", "Heike", ""], ["Roth", "Benjamin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1603.05350", "submitter": "Javier Vera Z\\'u\\~niga", "authors": "Javier Vera", "title": "Self-organization of vocabularies under different interaction orders", "comments": "5 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1510.02358", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the formation of vocabularies has been studied by agent-based\nmodels (specially, the Naming Game) in which random pairs of agents negotiate\nword-meaning associations at each discrete time step. This paper proposes a\nfirst approximation to a novel question: To what extent the negotiation of\nword-meaning associations is influenced by the order in which the individuals\ninteract? Automata Networks provide the adequate mathematical framework to\nexplore this question. Computer simulations suggest that on two-dimensional\nlattices the typical features of the formation of word-meaning associations are\nrecovered under random schemes that update small fractions of the population at\nthe same time.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 03:54:41 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:34:48 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Vera", "Javier", ""]]}, {"id": "1603.05354", "submitter": "Javier Vera Z\\'u\\~niga", "authors": "Javier Vera", "title": "Modeling self-organization of vocabularies under phonological similarity\n  effects", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a computational model (by Automata Networks) of\nphonological similarity effects involved in the formation of word-meaning\nassociations on artificial populations of speakers. Classical studies show that\nin recalling experiments memory performance was impaired for phonologically\nsimilar words versus dissimilar ones. Here, the individuals confound\nphonologically similar words according to a predefined parameter. The main\nhypothesis is that there is a critical range of the parameter, and with this,\nof working-memory mechanisms, which implies drastic changes in the final\nconsensus of the entire population. Theoretical results present proofs of\nconvergence for a particular case of the model within a worst-case complexity\nframework. Computer simulations describe the evolution of an energy function\nthat measures the amount of local agreement between individuals. The main\nfinding is the appearance of sudden changes in the energy function at critical\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 04:39:02 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 14:58:38 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Vera", "Javier", ""]]}, {"id": "1603.05570", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka", "title": "Predicate Gradual Logic and Linguistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several major proposals for treating donkey anaphora such as\ndiscourse representation theory and the likes, or E-Type theories and the\nlikes. Every one of them works well for a set of specific examples that they\nuse to demonstrate validity of their approaches. As I show in this paper,\nhowever, they are not very generalisable and do not account for essentially the\nsame problem that they remedy when it manifests in other examples. I propose\nanother logical approach. I develoop logic that extends a recent, propositional\ngradual logic, and show that it can treat donkey anaphora generally. I also\nidentify and address a problem around the modern convention on existential\nimport. Furthermore, I show that Aristotle's syllogisms and conversion are\nrealisable in this logic.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 16:43:37 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Arisaka", "Ryuta", ""]]}, {"id": "1603.05670", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Peter Sarlin", "title": "Bank distress in the news: Describing events through deep learning", "comments": "Forthcoming in Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:1507.07870 [in version 1]", "journal-ref": "Neurocomputing, 264, 2017", "doi": "10.1016/j.neucom.2016.12.11", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.NE q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many models are purposed for detecting the occurrence of significant\nevents in financial systems, the task of providing qualitative detail on the\ndevelopments is not usually as well automated. We present a deep learning\napproach for detecting relevant discussion in text and extracting natural\nlanguage descriptions of events. Supervised by only a small set of event\ninformation, comprising entity names and dates, the model is leveraged by\nunsupervised learning of semantic vector representations on extensive text\ndata. We demonstrate applicability to the study of financial risk based on news\n(6.6M articles), particularly bank distress and government interventions (243\nevents), where indices can signal the level of bank-stress-related reporting at\nthe entity level, or aggregated at national or European level, while being\ncoupled with explanations. Thus, we exemplify how text, as timely, widely\navailable and descriptive data, can serve as a useful complementary source of\ninformation for financial and systemic risk analytics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 20:06:27 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 23:24:49 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1603.05673", "submitter": "Hamidreza Chinaei", "authors": "Samantha Wong and Hamidreza Chinaei and Frank Rudzicz", "title": "Predicting health inspection results from online restaurant reviews", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informatics around public health are increasingly shifting from the\nprofessional to the public spheres. In this work, we apply linguistic analytics\nto restaurant reviews, from Yelp, in order to automatically predict official\nhealth inspection reports. We consider two types of feature sets, i.e., keyword\ndetection and topic model features, and use these in several classification\nmethods. Our empirical analysis shows that these extracted features can predict\npublic health inspection reports with over 90% accuracy using simple support\nvector machines.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 20:20:32 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Wong", "Samantha", ""], ["Chinaei", "Hamidreza", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1603.05739", "submitter": "Elliot Schumacher", "authors": "Elliot Schumacher, Maxine Eskenazi", "title": "A Readability Analysis of Campaign Speeches from the 2016 US\n  Presidential Campaign", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMU-LTI-16-001", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readability is defined as the reading level of the speech from grade 1 to\ngrade 12. It results from the use of the REAP readability analysis (vocabulary\n- Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), which\nuse the lexical contents and grammatical structure of the sentences in a\ndocument to predict the reading level. After analysis, results were grouped\ninto the average readability of each candidate, the evolution of the\ncandidate's speeches' readability over time and the standard deviation, or how\nmuch each candidate varied their speech from one venue to another. For\ncomparison, one speech from four past presidents and the Gettysburg Address\nwere also analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 00:55:52 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Schumacher", "Elliot", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1603.05962", "submitter": "Stanislas Lauly", "authors": "Stanislas Lauly, Yin Zheng, Alexandre Allauzen, Hugo Larochelle", "title": "Document Neural Autoregressive Distribution Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach based on feed-forward neural networks for learning the\ndistribution of textual documents. This approach is inspired by the Neural\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\nIn this paper, we present how NADE can successfully be adapted to the case of\ntextual data, retaining from NADE the property that sampling or computing the\nprobability of observations can be done exactly and efficiently. The approach\ncan also be used to learn deep representations of documents that are\ncompetitive to those learned by the alternative topic modeling approaches.\nFinally, we describe how the approach can be combined with a regular neural\nnetwork N-gram model and substantially improve its performance, by making its\nlearned representation sensitive to the larger, document-specific context.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 19:24:44 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Lauly", "Stanislas", ""], ["Zheng", "Yin", ""], ["Allauzen", "Alexandre", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1603.06009", "submitter": "Sowmya Vajjala", "authors": "Sowmya Vajjala and Detmar Meurers", "title": "Readability-based Sentence Ranking for Evaluating Text Simplification", "comments": "Unpublished technical report from 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for evaluating the readability of simplified\nsentences through pair-wise ranking. The validity of the method is established\nthrough in-corpus and cross-corpus evaluation experiments. The approach\ncorrectly identifies the ranking of simplified and unsimplified sentences in\nterms of their reading level with an accuracy of over 80%, significantly\noutperforming previous results. To gain qualitative insights into the nature of\nsimplification at the sentence level, we studied the impact of specific\nlinguistic features. We empirically confirm that both word-level and syntactic\nfeatures play a role in comparing the degree of simplification of authentic\ndata. To carry out this research, we created a new sentence-aligned corpus from\nprofessionally simplified news articles. The new corpus resource enriches the\nempirical basis of sentence-level simplification research, which so far relied\non a single resource. Most importantly, it facilitates cross-corpus evaluation\nfor simplification, a key step towards generalizable results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 22:24:54 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Vajjala", "Sowmya", ""], ["Meurers", "Detmar", ""]]}, {"id": "1603.06021", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta,\n  Christopher D. Manning, Christopher Potts", "title": "A Fast Unified Model for Parsing and Sentence Understanding", "comments": "To appear at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured neural networks exploit valuable syntactic parse information\nas they interpret the meanings of sentences. However, they suffer from two key\ntechnical problems that make them slow and unwieldy for large-scale NLP tasks:\nthey usually operate on parsed sentences and they do not directly support\nbatched computation. We address these issues by introducing the Stack-augmented\nParser-Interpreter Neural Network (SPINN), which combines parsing and\ninterpretation within a single tree-sequence hybrid model by integrating\ntree-structured sentence interpretation into the linear sequential structure of\na shift-reduce parser. Our model supports batched computation for a speedup of\nup to 25 times over other tree-structured models, and its integrated parser can\noperate on unparsed data with little loss in accuracy. We evaluate it on the\nStanford NLI entailment task and show that it significantly outperforms other\nsentence-encoding models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 00:22:20 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 23:19:08 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 18:36:15 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Gauthier", "Jon", ""], ["Rastogi", "Abhinav", ""], ["Gupta", "Raghav", ""], ["Manning", "Christopher D.", ""], ["Potts", "Christopher", ""]]}, {"id": "1603.06042", "submitter": "Daniel Andor", "authors": "Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro\n  Presta, Kuzman Ganchev, Slav Petrov and Michael Collins", "title": "Globally Normalized Transition-Based Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a globally normalized transition-based neural network model that\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\nsentence compression results. Our model is a simple feed-forward neural network\nthat operates on a task-specific transition system, yet achieves comparable or\nbetter accuracies than recurrent models. We discuss the importance of global as\nopposed to local normalization: a key insight is that the label bias problem\nimplies that globally normalized models can be strictly more expressive than\nlocally normalized models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:56:03 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 13:43:30 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Andor", "Daniel", ""], ["Alberti", "Chris", ""], ["Weiss", "David", ""], ["Severyn", "Aliaksei", ""], ["Presta", "Alessandro", ""], ["Ganchev", "Kuzman", ""], ["Petrov", "Slav", ""], ["Collins", "Michael", ""]]}, {"id": "1603.06059", "submitter": "Nasrin Mostafazadeh", "authors": "Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell,\n  Xiaodong He, Lucy Vanderwende", "title": "Generating Natural Questions About an Image", "comments": "Proceedings of the 54th Annual Meeting of the Association for\n  Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of work in the vision & language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision & language.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 07:27:15 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 06:54:58 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 01:20:49 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Mostafazadeh", "Nasrin", ""], ["Misra", "Ishan", ""], ["Devlin", "Jacob", ""], ["Mitchell", "Margaret", ""], ["He", "Xiaodong", ""], ["Vanderwende", "Lucy", ""]]}, {"id": "1603.06067", "submitter": "Kazuma Hashimoto", "authors": "Kazuma Hashimoto and Yoshimasa Tsuruoka", "title": "Adaptive Joint Learning of Compositional and Non-Compositional Phrase\n  Embeddings", "comments": "Accepted as a full paper at the 54th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for jointly learning compositional and\nnon-compositional phrase embeddings by adaptively weighting both types of\nembeddings using a compositionality scoring function. The scoring function is\nused to quantify the level of compositionality of each phrase, and the\nparameters of the function are jointly optimized with the objective for\nlearning phrase embeddings. In experiments, we apply the adaptive joint\nlearning method to the task of learning embeddings of transitive verb phrases,\nand show that the compositionality scores have strong correlation with human\nratings for verb-object compositionality, substantially outperforming the\nprevious state of the art. Moreover, our embeddings improve upon the previous\nbest model on a transitive verb disambiguation task. We also show that a simple\nensemble technique further improves the results for both tasks.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 08:53:29 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 07:24:51 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 07:46:27 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Hashimoto", "Kazuma", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1603.06075", "submitter": "Kazuma Hashimoto", "authors": "Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka", "title": "Tree-to-Sequence Attentional Neural Machine Translation", "comments": "Accepted as a full paper at the 54th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing Neural Machine Translation (NMT) models focus on the\nconversion of sequential data and do not directly use syntactic information. We\npropose a novel end-to-end syntactic NMT model, extending a\nsequence-to-sequence model with the source-side phrase structure. Our model has\nan attention mechanism that enables the decoder to generate a translated word\nwhile softly aligning it with phrases as well as words of the source sentence.\nExperimental results on the WAT'15 English-to-Japanese dataset demonstrate that\nour proposed model considerably outperforms sequence-to-sequence attentional\nNMT models and compares favorably with the state-of-the-art tree-to-string SMT\nsystem.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 10:08:40 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 09:55:39 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 08:39:11 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Eriguchi", "Akiko", ""], ["Hashimoto", "Kazuma", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1603.06076", "submitter": "Vered Shwartz", "authors": "Vered Shwartz, Yoav Goldberg and Ido Dagan", "title": "Improving Hypernymy Detection with an Integrated Path-based and\n  Distributional Method", "comments": "ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting hypernymy relations is a key task in NLP, which is addressed in the\nliterature using two complementary approaches. Distributional methods, whose\nsupervised variants are the current best performers, and path-based methods,\nwhich received less research attention. We suggest an improved path-based\nalgorithm, in which the dependency paths are encoded using a recurrent neural\nnetwork, that achieves results comparable to distributional methods. We then\nextend the approach to integrate both path-based and distributional signals,\nsignificantly improving upon the state-of-the-art on this task.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 10:09:53 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 16:07:40 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 10:09:43 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Shwartz", "Vered", ""], ["Goldberg", "Yoav", ""], ["Dagan", "Ido", ""]]}, {"id": "1603.06111", "submitter": "Lili Mou", "authors": "Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, Zhi Jin", "title": "How Transferable are Neural Networks in NLP Applications?", "comments": "Accepted by EMNLP-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is aimed to make use of valuable knowledge in a source\ndomain to help model performance in a target domain. It is particularly\nimportant to neural networks, which are very likely to be overfitting. In some\nfields like image processing, many studies have shown the effectiveness of\nneural network-based transfer learning. For neural NLP, however, existing\nstudies have only casually applied transfer learning, and conclusions are\ninconsistent. In this paper, we conduct systematic case studies and provide an\nilluminating picture on the transferability of neural networks in NLP.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 16:38:31 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 07:45:31 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Mou", "Lili", ""], ["Meng", "Zhao", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1603.06127", "submitter": "Petr Baudi\\v{s}", "authors": "Petr Baudi\\v{s}, Jan Pichl, Tom\\'a\\v{s} Vysko\\v{c}il, Jan \\v{S}ediv\\'y", "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "comments": "submitted as paper to CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the task of Sentence Pair Scoring, popular in the literature in\nvarious forms - viewed as Answer Sentence Selection, Semantic Text Scoring,\nNext Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a\ncomponent of Memory Networks.\n  We argue that all such tasks are similar from the model perspective and\npropose new baselines by comparing the performance of common IR metrics and\npopular convolutional, recurrent and attention-based neural models across many\nSentence Pair Scoring tasks and datasets. We discuss the problem of evaluating\nrandomized models, propose a statistically grounded methodology, and attempt to\nimprove comparisons by releasing new datasets that are much harder than some of\nthe currently used well explored benchmarks. We introduce a unified open source\nsoftware framework with easily pluggable models and tasks, which enables us to\nexperiment with multi-task reusability of trained sentence model. We set a new\nstate-of-art in performance on the Ubuntu Dialogue dataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:35:26 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 03:10:26 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 22:17:36 GMT"}, {"version": "v4", "created": "Tue, 17 May 2016 14:08:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Baudi\u0161", "Petr", ""], ["Pichl", "Jan", ""], ["Vysko\u010dil", "Tom\u00e1\u0161", ""], ["\u0160ediv\u00fd", "Jan", ""]]}, {"id": "1603.06147", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Kyunghyun Cho and Yoshua Bengio", "title": "A Character-Level Decoder without Explicit Segmentation for Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing machine translation systems, whether phrase-based or neural,\nhave relied almost exclusively on word-level modelling with explicit\nsegmentation. In this paper, we ask a fundamental question: can neural machine\ntranslation generate a character sequence without any explicit segmentation? To\nanswer this question, we evaluate an attention-based encoder-decoder with a\nsubword-level encoder and a character-level decoder on four language\npairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.\nOur experiments show that the models with a character-level decoder outperform\nthe ones with a subword-level decoder on all of the four language pairs.\nFurthermore, the ensembles of neural models with a character-level decoder\noutperform the state-of-the-art non-neural machine translation systems on\nEn-Cs, En-De and En-Fi and perform comparably on En-Ru.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 21:35:04 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 20:57:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 04:06:01 GMT"}, {"version": "v4", "created": "Tue, 21 Jun 2016 01:12:22 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Chung", "Junyoung", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.06155", "submitter": "Michel Galley", "authors": "Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis,\n  Jianfeng Gao and Bill Dolan", "title": "A Persona-Based Neural Conversation Model", "comments": "Accepted for publication at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present persona-based models for handling the issue of speaker consistency\nin neural response generation. A speaker model encodes personas in distributed\nembeddings that capture individual characteristics such as background\ninformation and speaking style. A dyadic speaker-addressee model captures\nproperties of interactions between two interlocutors. Our models yield\nqualitative performance improvements in both perplexity and BLEU scores over\nbaseline sequence-to-sequence models, with similar gains in speaker consistency\nas measured by human judges.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 23:15:18 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 17:19:58 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Li", "Jiwei", ""], ["Galley", "Michel", ""], ["Brockett", "Chris", ""], ["Spithourakis", "Georgios P.", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "1603.06270", "submitter": "Zhilin Yang", "authors": "Zhilin Yang and Ruslan Salakhutdinov and William Cohen", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep hierarchical recurrent neural network for sequence tagging.\nGiven a sequence of words, our model employs deep gated recurrent units on both\ncharacter and word levels to encode morphology and context information, and\napplies a conditional random field layer to predict the tags. Our model is task\nindependent, language independent, and feature engineering free. We further\nextend our model to multi-task and cross-lingual joint training by sharing the\narchitecture and parameters. Our model achieves state-of-the-art results in\nmultiple languages on several benchmark tasks including POS tagging, chunking,\nand NER. We also demonstrate that multi-task and cross-lingual joint training\ncan improve the performance in various cases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 21:15:56 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 15:07:39 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Yang", "Zhilin", ""], ["Salakhutdinov", "Ruslan", ""], ["Cohen", "William", ""]]}, {"id": "1603.06318", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing", "title": "Harnessing Deep Neural Networks with Logic Rules", "comments": "Fix typos in appendix. ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 03:33:20 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 05:28:21 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 23:30:48 GMT"}, {"version": "v4", "created": "Tue, 15 Nov 2016 21:41:21 GMT"}, {"version": "v5", "created": "Tue, 26 Mar 2019 05:16:10 GMT"}, {"version": "v6", "created": "Sat, 8 Aug 2020 07:38:00 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hu", "Zhiting", ""], ["Ma", "Xuezhe", ""], ["Liu", "Zhengzhong", ""], ["Hovy", "Eduard", ""], ["Xing", "Eric", ""]]}, {"id": "1603.06393", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Zhengdong Lu, Hang Li and Victor O.K. Li", "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "comments": "10 pages, 5 figures, accepted by ACL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 11:35:08 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 03:33:58 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 13:53:21 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1603.06485", "submitter": "Lisa Posch", "authors": "Lisa Posch and Philipp Schaer and Arnim Bleier and Markus Strohmaier", "title": "A System for Probabilistic Linking of Thesauri and Classification\n  Systems", "comments": null, "journal-ref": "KI - K\\\"unstliche Intelligenz, 2015", "doi": "10.1007/s13218-015-0413-9", "report-no": null, "categories": "cs.AI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a system which creates and visualizes probabilistic\nsemantic links between concepts in a thesaurus and classes in a classification\nsystem. For creating the links, we build on the Polylingual Labeled Topic Model\n(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in\nthe classification system by using information from the natural language text\nof documents, their assigned thesaurus descriptors and their designated\nclasses. The links are then presented to users of the system in an interactive\nvisualization, providing them with an automatically generated overview of the\nrelations between the thesaurus and the classification system.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:34:13 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Posch", "Lisa", ""], ["Schaer", "Philipp", ""], ["Bleier", "Arnim", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1603.06503", "submitter": "Miguel Ballesteros", "authors": "Bernd Bohnet and Miguel Ballesteros and Ryan McDonald and Joakim Nivre", "title": "Static and Dynamic Feature Selection in Morphosyntactic Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of greedy feature selection methods for morphosyntactic\ntagging under a number of different conditions. We compare a static ordering of\nfeatures to a dynamic ordering based on mutual information statistics, and we\napply the techniques to standalone taggers as well as joint systems for tagging\nand parsing. Experiments on five languages show that feature selection can\nresult in more compact models as well as higher accuracy under all conditions,\nbut also that a dynamic ordering works better than a static ordering and that\njoint systems benefit more than standalone taggers. We also show that the same\ntechniques can be used to select which morphosyntactic categories to predict in\norder to maximize syntactic accuracy in a joint system. Our final results\nrepresent a substantial improvement of the state of the art for several\nlanguages, while at the same time reducing both the number of features and the\nrunning time by up to 80% in some cases.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 17:20:34 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bohnet", "Bernd", ""], ["Ballesteros", "Miguel", ""], ["McDonald", "Ryan", ""], ["Nivre", "Joakim", ""]]}, {"id": "1603.06571", "submitter": "Oren Barkan", "authors": "Oren Barkan", "title": "Bayesian Neural Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several works in the domain of natural language processing\npresented successful methods for word embedding. Among them, the Skip-Gram with\nnegative sampling, known also as word2vec, advanced the state-of-the-art of\nvarious linguistics tasks. In this paper, we propose a scalable Bayesian neural\nword embedding algorithm. The algorithm relies on a Variational Bayes solution\nfor the Skip-Gram objective and a detailed step by step description is\nprovided. We present experimental results that demonstrate the performance of\nthe proposed algorithm for word analogy and similarity tasks on six different\ndatasets and show it is competitive with the original Skip-Gram method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:32:06 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 16:49:11 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:45:33 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Barkan", "Oren", ""]]}, {"id": "1603.06598", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, David Weiss", "title": "Stack-propagation: Improved Representation Learning for Syntax", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional syntax models typically leverage part-of-speech (POS) information\nby constructing features from hand-tuned templates. We demonstrate that a\nbetter approach is to utilize POS tags as a regularizer of learned\nrepresentations. We propose a simple method for learning a stacked pipeline of\nmodels which we call \"stack-propagation\". We apply this to dependency parsing\nand tagging, where we use the hidden layer of the tagger network as a\nrepresentation of the input tokens for the parser. At test time, our parser\ndoes not require predicted POS tags. On 19 languages from the Universal\nDependencies, our method is 1.3% (absolute) more accurate than a\nstate-of-the-art graph-based approach and 2.7% more accurate than the most\ncomparable greedy model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 20:12:44 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 01:39:25 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Zhang", "Yuan", ""], ["Weiss", "David", ""]]}, {"id": "1603.06677", "submitter": "Percy Liang", "authors": "Percy Liang", "title": "Learning Executable Semantic Parsers for Natural Language Understanding", "comments": "Accepted to the Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For building question answering systems and natural language interfaces,\nsemantic parsing has emerged as an important and powerful paradigm. Semantic\nparsers map natural language into logical forms, the classic representation for\nmany important linguistic phenomena. The modern twist is that we are interested\nin learning semantic parsers from data, which introduces a new layer of\nstatistical and computational issues. This article lays out the components of a\nstatistical semantic parser, highlighting the key challenges. We will see that\nsemantic parsing is a rich fusion of the logical and the statistical world, and\nthat this fusion will play an integral role in the future of natural language\nunderstanding systems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 05:07:16 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Liang", "Percy", ""]]}, {"id": "1603.06679", "submitter": "Wenya Wang", "authors": "Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier and Xiaokui Xiao", "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In aspect-based sentiment analysis, extracting aspect terms along with the\nopinions being expressed from user-generated content is one of the most\nimportant subtasks. Previous studies have shown that exploiting connections\nbetween aspect and opinion terms is promising for this task. In this paper, we\npropose a novel joint model that integrates recursive neural networks and\nconditional random fields into a unified framework for explicit aspect and\nopinion terms co-extraction. The proposed model learns high-level\ndiscriminative features and double propagate information between aspect and\nopinion terms, simultaneously. Moreover, it is flexible to incorporate\nhand-crafted features into the proposed model to further boost its information\nextraction performance. Experimental results on the SemEval Challenge 2014\ndataset show the superiority of our proposed model over several baseline\nmethods as well as the winning systems of the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 05:59:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 06:24:06 GMT"}, {"version": "v3", "created": "Mon, 19 Sep 2016 14:00:43 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wang", "Wenya", ""], ["Pan", "Sinno Jialin", ""], ["Dahlmeier", "Daniel", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "1603.06744", "submitter": "Wang Ling", "authors": "Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tom\\'a\\v{s}\n  Ko\\v{c}isk\\'y, Andrew Senior, Fumin Wang, Phil Blunsom", "title": "Latent Predictor Networks for Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many language generation tasks require the production of text conditioned on\nboth structured and unstructured inputs. We present a novel neural network\narchitecture which generates an output sequence conditioned on an arbitrary\nnumber of input functions. Crucially, our approach allows both the choice of\nconditioning context and the granularity of generation, for example characters\nor tokens, to be marginalised, thus permitting scalable and effective training.\nUsing this framework, we address the problem of generating programming code\nfrom a mixed natural language and structured specification. We create two new\ndata sets for this paradigm derived from the collectible trading card games\nMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,\nwe demonstrate that marginalising multiple predictors allows our model to\noutperform strong benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 11:41:51 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 14:46:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Ling", "Wang", ""], ["Grefenstette", "Edward", ""], ["Hermann", "Karl Moritz", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Senior", "Andrew", ""], ["Wang", "Fumin", ""], ["Blunsom", "Phil", ""]]}, {"id": "1603.06785", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Emilia Rejmund, Krzysztof Marasek", "title": "Multi-domain machine translation enhancements by parallel data\n  extraction from comparable corpora", "comments": "parallel corpus, Polish, English, machine learning, comparable\n  corpora, NLP. in Gruszczy\\'nska, Ewa; Le\\'nko-Szyma\\'nska, Agnieszka, red.\n  (2016). Polskoj\\k{e}zyczne korpusy r\\'ownoleg{\\l}e. Polish-language Parallel\n  Corpora. Warszawa: Instytut Lingwistyki Stosowanej. ISBN: 978-83-935320-4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel texts are a relatively rare language resource, however, they\nconstitute a very useful research material with a wide range of applications.\nThis study presents and analyses new methodologies we developed for obtaining\nsuch data from previously built comparable corpora. The methodologies are\nautomatic and unsupervised which makes them good for large scale research. The\ntask is highly practical as non-parallel multilingual data occur much more\nfrequently than parallel corpora and accessing them is easy, although parallel\nsentences are a considerably more useful resource. In this study, we propose a\nmethod of automatic web crawling in order to build topic-aligned comparable\ncorpora, e.g. based on the Wikipedia or Euronews.com. We also developed new\nmethods of obtaining parallel sentences from comparable data and proposed\nmethods of filtration of corpora capable of selecting inconsistent or only\npartially equivalent translations. Our methods are easily scalable to other\nlanguages. Evaluation of the quality of the created corpora was performed by\nanalysing the impact of their use on statistical machine translation systems.\nExperiments were presented on the basis of the Polish-English language pair for\ntexts from different domains, i.e. lectures, phrasebooks, film dialogues,\nEuropean Parliament proceedings and texts contained medicines leaflets. We also\ntested a second method of creating parallel corpora based on data from\ncomparable corpora which allows for automatically expanding the existing corpus\nof sentences about a given domain on the basis of analogies found between them.\nIt does not require, therefore, having past parallel resources in order to\ntrain a classifier.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 13:34:28 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Rejmund", "Emilia", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1603.06807", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Alberto Garc\\'ia-Dur\\'an, Caglar Gulcehre, Sungjin\n  Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M\n  Factoid Question-Answer Corpus", "comments": "13 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:25:16 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 20:00:20 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Chandar", "Sarath", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.07012", "submitter": "Dayu Yuan", "authors": "Dayu Yuan and Julian Richardson and Ryan Doherty and Colin Evans and\n  Eric Altendorf", "title": "Semi-supervised Word Sense Disambiguation with Neural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Determining the intended sense of words in text - word sense disambiguation\n(WSD) - is a long standing problem in natural language processing. Recently,\nresearchers have shown promising results using word vectors extracted from a\nneural network language model as features in WSD algorithms. However, a simple\naverage or concatenation of word vectors for each word in a text loses the\nsequential and syntactic information of the text. In this paper, we study WSD\nwith a sequence learning neural net, LSTM, to better capture the sequential and\nsyntactic patterns of the text. To alleviate the lack of training data in\nall-words WSD, we employ the same LSTM in a semi-supervised label propagation\nclassifier. We demonstrate state-of-the-art results, especially on verbs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 22:15:10 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 01:15:21 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Yuan", "Dayu", ""], ["Richardson", "Julian", ""], ["Doherty", "Ryan", ""], ["Evans", "Colin", ""], ["Altendorf", "Eric", ""]]}, {"id": "1603.07044", "submitter": "Wei-Ning Hsu", "authors": "Wei-Ning Hsu, Yu Zhang and James Glass", "title": "Recurrent Neural Network Encoder with Attention for Community Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a general recurrent neural network (RNN) encoder framework to\ncommunity question answering (cQA) tasks. Our approach does not rely on any\nlinguistic processing, and can be applied to different languages or domains.\nFurther improvements are observed when we extend the RNN encoders with a neural\nattention mechanism that encourages reasoning over entire sequences. To deal\nwith practical issues such as data sparsity and imbalanced labels, we apply\nvarious techniques such as transfer learning and multitask learning. Our\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\ncompared to an information retrieval-based approach, and achieve comparable\nperformance to a strong handcrafted feature-based method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 01:52:54 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Hsu", "Wei-Ning", ""], ["Zhang", "Yu", ""], ["Glass", "James", ""]]}, {"id": "1603.07150", "submitter": "Martyn Harris", "authors": "Martyn Harris, Mark Levene, Dell Zhang, Dan Levene", "title": "The Anatomy of a Search and Mining System for Digital Archives", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samtla (Search And Mining Tools with Linguistic Analysis) is a digital\nhumanities system designed in collaboration with historians and linguists to\nassist them with their research work in quantifying the content of any textual\ncorpora through approximate phrase search and document comparison. The\nretrieval engine uses a character-based n-gram language model rather than the\nconventional word-based one so as to achieve great flexibility in language\nagnostic query processing.\n  The index is implemented as a space-optimised character-based suffix tree\nwith an accompanying database of document content and metadata. A number of\ntext mining tools are integrated into the system to allow researchers to\ndiscover textual patterns, perform comparative analysis, and find out what is\ncurrently popular in the research community.\n  Herein we describe the system architecture, user interface, models and\nalgorithms, and data storage of the Samtla system. We also present several case\nstudies of its usage in practice together with an evaluation of the systems'\nranking performance through crowdsourcing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 12:02:12 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Harris", "Martyn", ""], ["Levene", "Mark", ""], ["Zhang", "Dell", ""], ["Levene", "Dan", ""]]}, {"id": "1603.07185", "submitter": "Rajesh Bordawekar", "authors": "Rajesh Bordawekar and Oded Shmueli", "title": "Enabling Cognitive Intelligence Queries in Relational Databases using\n  Low-dimensional Word Embeddings", "comments": "Submitted to VLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply distributed language embedding methods from Natural Language\nProcessing to assign a vector to each database entity associated token (for\nexample, a token may be a word occurring in a table row, or the name of a\ncolumn). These vectors, of typical dimension 200, capture the meaning of tokens\nbased on the contexts in which the tokens appear together. To form vectors, we\napply a learning method to a token sequence derived from the database. We\ndescribe various techniques for extracting token sequences from a database. The\ntechniques differ in complexity, in the token sequences they output and in the\ndatabase information used (e.g., foreign keys). The vectors can be used to\nalgebraically quantify semantic relationships between the tokens such as\nsimilarities and analogies. Vectors enable a dual view of the data: relational\nand (meaningful rather than purely syntactical) text. We introduce and explore\na new class of queries called cognitive intelligence (CI) queries that extract\ninformation from the database based, in part, on the relationships encoded by\nvectors. We have implemented a prototype system on top of Spark to exhibit the\npower of CI queries. Here, CI queries are realized via SQL UDFs. This power\ngoes far beyond text extensions to relational systems due to the information\nencoded in vectors. We also consider various extensions to the basic scheme,\nincluding using a collection of views derived from the database to focus on a\ndomain of interest, utilizing vectors and/or text from external sources,\nmaintaining vectors as the database evolves and exploring a database without\nutilizing its schema. For the latter, we consider minimal extensions to SQL to\nvastly improve query expressiveness.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 13:57:33 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Bordawekar", "Rajesh", ""], ["Shmueli", "Oded", ""]]}, {"id": "1603.07252", "submitter": "Jianpeng Cheng J", "authors": "Jianpeng Cheng, Mirella Lapata", "title": "Neural Summarization by Extracting Sentences and Words", "comments": "ACL2016 conference paper with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to extractive summarization rely heavily on\nhuman-engineered features. In this work we propose a data-driven approach based\non neural networks and continuous sentence features. We develop a general\nframework for single-document summarization composed of a hierarchical document\nencoder and an attention-based extractor. This architecture allows us to\ndevelop different classes of summarization models which can extract sentences\nor words. We train our models on large scale corpora containing hundreds of\nthousands of document-summary pairs. Experimental results on two summarization\ndatasets demonstrate that our models obtain results comparable to the state of\nthe art without any access to linguistic annotation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 16:05:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 13:41:50 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 03:16:03 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Lapata", "Mirella", ""]]}, {"id": "1603.07253", "submitter": "Kimberly Glasgow", "authors": "Kimberly Glasgow, Matthew Roos, Amy Haufler, Mark Chevillet, Michael\n  Wolmetz", "title": "Evaluating semantic models with word-sentence relatedness", "comments": "8 pages, 2 figures, ancillary files. Replaced original version to fix\n  typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic textual similarity (STS) systems are designed to encode and evaluate\nthe semantic similarity between words, phrases, sentences, and documents. One\nmethod for assessing the quality or authenticity of semantic information\nencoded in these systems is by comparison with human judgments. A data set for\nevaluating semantic models was developed consisting of 775 English\nword-sentence pairs, each annotated for semantic relatedness by human raters\nengaged in a Maximum Difference Scaling (MDS) task, as well as a faster\nalternative task. As a sample application of this relatedness data,\nbehavior-based relatedness was compared to the relatedness computed via four\noff-the-shelf STS models: n-gram, Latent Semantic Analysis (LSA), Word2Vec, and\nUMBC Ebiquity. Some STS models captured much of the variance in the human\njudgments collected, but they were not sensitive to the implicatures and\nentailments that were processed and considered by the participants. All text\nstimuli and judgment data have been made freely available.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 16:12:34 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 17:25:08 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Glasgow", "Kimberly", ""], ["Roos", "Matthew", ""], ["Haufler", "Amy", ""], ["Chevillet", "Mark", ""], ["Wolmetz", "Michael", ""]]}, {"id": "1603.07313", "submitter": "Jes\\'us Tramullas", "authors": "Piedad Garrido, Jesus Tramullas, Manuel Coll", "title": "CONDITOR1: Topic Maps and DITA labelling tool for textual documents with\n  historical information", "comments": null, "journal-ref": "Journal of Digital Information, 10, 4, 2009", "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conditor is a software tool which works with textual documents containing\nhistorical information. The purpose of this work two-fold: firstly to show the\nvalidity of the developed engine to correctly identify and label the entities\nof the universe of discourse with a labelled-combined XTM-DITA model. Secondly\nto explain the improvements achieved in the information retrieval process\nthanks to the use of a object-oriented database (JPOX) as well as its\nintegration into the Lucene-type database search process to not only accomplish\nmore accurate searches, but to also help the future development of a\nrecommender system. We finish with a brief demo in a 3D-graph of the results of\nthe aforementioned search.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 19:26:20 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Garrido", "Piedad", ""], ["Tramullas", "Jesus", ""], ["Coll", "Manuel", ""]]}, {"id": "1603.07603", "submitter": "Fei Sun", "authors": "Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng", "title": "Semantic Regularities in Document Representations", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work exhibited that distributed word representations are good at\ncapturing linguistic regularities in language. This allows vector-oriented\nreasoning based on simple linear algebra between words. Since many different\nmethods have been proposed for learning document representations, it is natural\nto ask whether there is also linear structure in these learned representations\nto allow similar reasoning at document level. To answer this question, we\ndesign a new document analogy task for testing the semantic regularities in\ndocument representations, and conduct empirical evaluations over several\nstate-of-the-art document representation models. The results reveal that neural\nembedding based document representations work better on this analogy task than\nconventional methods, and we provide some preliminary explanations over these\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 14:45:20 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Sun", "Fei", ""], ["Guo", "Jiafeng", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1603.07609", "submitter": "Yevgeni Berzak", "authors": "Yevgeni Berzak, Roi Reichart and Boris Katz", "title": "Contrastive Analysis with Predictive Power: Typology Driven Estimation\n  of Grammatical Error Distributions in ESL", "comments": "Published in CoNLL 2015", "journal-ref": "Proceedings of the 19th Conference on Computational Language\n  Learning, pages 94-102, Beijing, China, July 30-31, 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the impact of cross-linguistic transfer on grammatical\nerrors in English as Second Language (ESL) texts. Using a computational\nframework that formalizes the theory of Contrastive Analysis (CA), we\ndemonstrate that language specific error distributions in ESL writing can be\npredicted from the typological properties of the native language and their\nrelation to the typology of English. Our typology driven model enables to\nobtain accurate estimates of such distributions without access to any ESL data\nfor the target languages. Furthermore, we present a strategy for adjusting our\nmethod to low-resource languages that lack typological documentation using a\nbootstrapping approach which approximates native language typology from ESL\ntexts. Finally, we show that our framework is instrumental for linguistic\ninquiry seeking to identify first language factors that contribute to a wide\nrange of difficulties in second language acquisition.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 14:59:45 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Reichart", "Roi", ""], ["Katz", "Boris", ""]]}, {"id": "1603.07624", "submitter": "Diego Klabjan", "authors": "Eun Hee Ko, Diego Klabjan", "title": "Semantic Properties of Customer Sentiment in Tweets", "comments": "The 28th IEEE International Conference on Advanced Information\n  Networking and Applications. Victoria, Canada, 2014", "journal-ref": null, "doi": "10.1109/WAINA.2014.151", "report-no": null, "categories": "cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of people are using online social networking services\n(SNSs), and a significant amount of information related to experiences in\nconsumption is shared in this new media form. Text mining is an emerging\ntechnique for mining useful information from the web. We aim at discovering in\nparticular tweets semantic patterns in consumers' discussions on social media.\nSpecifically, the purposes of this study are twofold: 1) finding similarity and\ndissimilarity between two sets of textual documents that include consumers'\nsentiment polarities, two forms of positive vs. negative opinions and 2)\ndriving actual content from the textual data that has a semantic trend. The\nconsidered tweets include consumers opinions on US retail companies (e.g.,\nAmazon, Walmart). Cosine similarity and K-means clustering methods are used to\nachieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic\nmodeling algorithm, is used for the latter purpose. This is the first study\nwhich discover semantic properties of textual data in consumption context\nbeyond sentiment analysis. In addition to major findings, we apply LDA (Latent\nDirichlet Allocations) to the same data and drew latent topics that represent\nconsumers' positive opinions and negative opinions on social media.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 15:22:52 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ko", "Eun Hee", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07646", "submitter": "Saurabh Kataria", "authors": "Saurabh Kataria", "title": "Recursive Neural Language Architecture for Tag Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 16:39:37 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kataria", "Saurabh", ""]]}, {"id": "1603.07695", "submitter": "Quan Liu", "authors": "Quan Liu, Zhen-Hua Ling, Hui Jiang, Yu Hu", "title": "Part-of-Speech Relevance Weights for Learning Word Embeddings", "comments": "Word embeddings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a model to learn word embeddings with weighted contexts\nbased on part-of-speech (POS) relevance weights. POS is a fundamental element\nin natural language. However, state-of-the-art word embedding models fail to\nconsider it. This paper proposes to use position-dependent POS relevance\nweighting matrices to model the inherent syntactic relationship among words\nwithin a context window. We utilize the POS relevance weights to model each\nword-context pairs during the word embedding training process. The model\nproposed in this paper paper jointly optimizes word vectors and the POS\nrelevance matrices. Experiments conducted on popular word analogy and word\nsimilarity tasks all demonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 18:22:39 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Liu", "Quan", ""], ["Ling", "Zhen-Hua", ""], ["Jiang", "Hui", ""], ["Hu", "Yu", ""]]}, {"id": "1603.07771", "submitter": "David Grangier", "authors": "Remi Lebret, David Grangier, Michael Auli", "title": "Neural Text Generation from Structured Data with Application to the\n  Biography Domain", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a neural model for concept-to-text generation that\nscales to large, rich domains. We experiment with a new dataset of biographies\nfrom Wikipedia that is an order of magnitude larger than existing resources\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\nmodel builds upon recent work on conditional neural language model for text\ngeneration. To deal with the large vocabulary, we extend these models to mix a\nfixed vocabulary with copy actions that transfer sample-specific words from the\ninput database to the generated output sentence. Our neural model significantly\nout-performs a classical Kneser-Ney language model adapted to this task by\nnearly 15 BLEU.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 22:40:00 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 14:47:44 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 15:16:46 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Lebret", "Remi", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1603.07954", "submitter": "Karthik Narasimhan", "authors": "Karthik Narasimhan, Adam Yala and Regina Barzilay", "title": "Improving Information Extraction by Acquiring External Evidence with\n  Reinforcement Learning", "comments": "Appearing in EMNLP 2016 (12 pages incl. supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful information extraction systems operate with access to a large\ncollection of documents. In this work, we explore the task of acquiring and\nincorporating external evidence to improve extraction accuracy in domains where\nthe amount of training data is scarce. This process entails issuing search\nqueries, extraction from new sources and reconciliation of extracted values,\nwhich are repeated until sufficient evidence is collected. We approach the\nproblem using a reinforcement learning framework where our model learns to\nselect optimal actions based on contextual information. We employ a deep\nQ-network, trained to optimize a reward function that reflects extraction\naccuracy while penalizing extra effort. Our experiments on two databases -- of\nshooting incidents, and food adulteration cases -- demonstrate that our system\nsignificantly outperforms traditional extractors and a competitive\nmeta-classifier baseline.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 16:38:54 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 03:24:37 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 23:33:28 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Narasimhan", "Karthik", ""], ["Yala", "Adam", ""], ["Barzilay", "Regina", ""]]}, {"id": "1603.08016", "submitter": "Reed Coke", "authors": "Reed Coke, Ben King, Dragomir Radev", "title": "Classifying Syntactic Regularities for Hundreds of Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comparison of classification methods for linguistic\ntypology for the purpose of expanding an extensive, but sparse language\nresource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath,\n2013). We experimented with a variety of regression and nearest-neighbor\nmethods for use in classification over a set of 325 languages and six syntactic\nrules drawn from WALS. To classify each rule, we consider the typological\nfeatures of the other five rules; linguistic features extracted from a\nword-aligned Bible in each language; and genealogical features (genus and\nfamily) of each language. In general, we find that propagating the majority\nlabel among all languages of the same genus achieves the best accuracy in label\npre- diction. Following this, a logistic regression model that combines\ntypological and linguistic features offers the next best performance.\nInterestingly, this model actually outperforms the majority labels among all\nlanguages of the same family.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:09:29 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 18:40:55 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Coke", "Reed", ""], ["King", "Ben", ""], ["Radev", "Dragomir", ""]]}, {"id": "1603.08023", "submitter": "Ryan Lowe T.", "authors": "Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent\n  Charlin, Joelle Pineau", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of\n  Unsupervised Evaluation Metrics for Dialogue Response Generation", "comments": "First 4 authors had equal contribution. 13 pages, 5 tables, 6\n  figures. EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model's generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:32:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 18:28:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Liu", "Chia-Wei", ""], ["Lowe", "Ryan", ""], ["Serban", "Iulian V.", ""], ["Noseworthy", "Michael", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1603.08042", "submitter": "Ouais Alsharif", "authors": "Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw", "title": "On the Compression of Recurrent Neural Networks with an Application to\n  LVCSR acoustic modeling for Embedded Speech Recognition", "comments": "Accepted in ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of compressing recurrent neural networks (RNNs). In\nparticular, we focus on the compression of RNN acoustic models, which are\nmotivated by the goal of building compact and accurate speech recognition\nsystems which can be run efficiently on mobile devices. In this work, we\npresent a technique for general recurrent model compression that jointly\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\nfind that the proposed technique allows us to reduce the size of our Long\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\nnegligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 21:43:28 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 15:19:30 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Prabhavalkar", "Rohit", ""], ["Alsharif", "Ouais", ""], ["Bruguier", "Antoine", ""], ["McGraw", "Ian", ""]]}, {"id": "1603.08048", "submitter": "Michael Ruster", "authors": "Michael Ruster", "title": "\"Did I Say Something Wrong?\" A Word-Level Analysis of Wikipedia Articles\n  for Deletion Discussions", "comments": "Master's Thesis, Koblenz 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This thesis focuses on gaining linguistic insights into textual discussions\non a word level. It was of special interest to distinguish messages that\nconstructively contribute to a discussion from those that are detrimental to\nthem. Thereby, we wanted to determine whether \"I\"- and \"You\"-messages are\nindicators for either of the two discussion styles. These messages are nowadays\noften used in guidelines for successful communication. Although their effects\nhave been successfully evaluated multiple times, a large-scale analysis has\nnever been conducted.\n  Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions\ntogether with the records of blocked users and developed a fully automated\ncreation of an annotated data set. In this data set, messages were labelled\neither constructive or disruptive. We applied binary classifiers to the data to\ndetermine characteristic words for both discussion styles. Thereby, we also\ninvestigated whether function words like pronouns and conjunctions play an\nimportant role in distinguishing the two.\n  We found that \"You\"-messages were a strong indicator for disruptive messages\nwhich matches their attributed effects on communication. However, we found\n\"I\"-messages to be indicative for disruptive messages as well which is contrary\nto their attributed effects. The importance of function words could neither be\nconfirmed nor refuted. Other characteristic words for either communication\nstyle were not found. Yet, the results suggest that a different model might\nrepresent disruptive and constructive messages in textual discussions better.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 22:36:40 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ruster", "Michael", ""]]}, {"id": "1603.08079", "submitter": "Andrei Barbu", "authors": "Yevgeni Berzak and Andrei Barbu and Daniel Harari and Boris Katz and\n  Shimon Ullman", "title": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities", "comments": "EMNLP 2015", "journal-ref": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), 2015, pages 1477--1487", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding language goes hand in hand with the ability to integrate\ncomplex contextual information obtained via perception. In this work, we\npresent a novel task for grounded language understanding: disambiguating a\nsentence given a visual scene which depicts one of the possible interpretations\nof that sentence. To this end, we introduce a new multimodal corpus containing\nambiguous sentences, representing a wide range of syntactic, semantic and\ndiscourse ambiguities, coupled with videos that visualize the different\ninterpretations for each sentence. We address this task by extending a vision\nmodel which determines if a sentence is depicted by a video. We demonstrate how\nsuch a model can be adjusted to recognize different interpretations of the same\nunderlying sentence, allowing to disambiguate sentences in a unified fashion\nacross the different ambiguity types.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 06:49:33 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Berzak", "Yevgeni", ""], ["Barbu", "Andrei", ""], ["Harari", "Daniel", ""], ["Katz", "Boris", ""], ["Ullman", "Shimon", ""]]}, {"id": "1603.08089", "submitter": "Qingqing Zhou", "authors": "Qingqing Zhou, Rui Xia, Chengzhi Zhang", "title": "Online shopping behavior study based on multi-granularity opinion\n  mining: China vs. America", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of e-commerce, many products are now being sold\nworldwide, and manufacturers are eager to obtain a better understanding of\ncustomer behavior in various regions. To achieve this goal, most previous\nefforts have focused mainly on questionnaires, which are time-consuming and\ncostly. The tremendous volume of product reviews on e-commerce websites has\nseen a new trend emerge, whereby manufacturers attempt to understand user\npreferences by analyzing online reviews. Following this trend, this paper\naddresses the problem of studying customer behavior by exploiting recently\ndeveloped opinion mining techniques. This work is novel for three reasons.\nFirst, questionnaire-based investigation is automatically enabled by employing\nalgorithms for template-based question generation and opinion mining-based\nanswer extraction. Using this system, manufacturers are able to obtain reports\nof customer behavior featuring a much larger sample size, more direct\ninformation, a higher degree of automation, and a lower cost. Second,\ninternational customer behavior study is made easier by integrating tools for\nmultilingual opinion mining. Third, this is the first time an automatic\nquestionnaire investigation has been conducted to compare customer behavior in\nChina and America, where product reviews are written and read in Chinese and\nEnglish, respectively. Our study on digital cameras, smartphones, and tablet\ncomputers yields three findings. First, Chinese customers follow the Doctrine\nof the Mean, and often use euphemistic expressions, while American customers\nexpress their opinions more directly. Second, Chinese customers care more about\ngeneral feelings, while American customers pay more attention to product\ndetails. Third, Chinese customers focus on external features, while American\ncustomers care more about the internal features of products.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 08:57:22 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zhou", "Qingqing", ""], ["Xia", "Rui", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "1603.08091", "submitter": "Qingqing Zhou", "authors": "Qingqing Zhou, Chengzhi Zhang, Star X. Zhao, Bikun Chen", "title": "Measuring Book Impact Based on the Multi-granularity Online Review\n  Mining", "comments": "21pages,3 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with articles and journals, the customary methods for measuring books'\nacademic impact mainly involve citations, which is easy but limited to\ninterrogating traditional citation databases and scholarly book reviews,\nResearchers have attempted to use other metrics, such as Google Books,\nlibcitation, and publisher prestige. However, these approaches lack\ncontent-level information and cannot determine the citation intentions of\nusers. Meanwhile, the abundant online review resources concerning academic\nbooks can be used to mine deeper information and content utilizing altmetric\nperspectives. In this study, we measure the impacts of academic books by\nmulti-granularity mining online reviews, and we identify factors that affect a\nbook's impact. First, online reviews of a sample of academic books on Amazon.cn\nare crawled and processed. Then, multi-granularity review mining is conducted\nto identify review sentiment polarities and aspects' sentiment values. Lastly,\nthe numbers of positive reviews and negative reviews, aspect sentiment values,\nstar values, and information regarding helpfulness are integrated via the\nentropy method, and lead to the calculation of the final book impact scores.\nThe results of a correlation analysis of book impact scores obtained via our\nmethod versus traditional book citations show that, although there are\nsubstantial differences between subject areas, online book reviews tend to\nreflect the academic impact. Thus, we infer that online reviews represent a\npromising source for mining book impact within the altmetric perspective and at\nthe multi-granularity content level. Moreover, our proposed method might also\nbe a means by which to measure other books besides academic publications.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 09:25:16 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Zhou", "Qingqing", ""], ["Zhang", "Chengzhi", ""], ["Zhao", "Star X.", ""], ["Chen", "Bikun", ""]]}, {"id": "1603.08148", "submitter": "\\c{C}a\\u{g}lar G\\\"ul\\c{c}ehre", "authors": "Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou and Yoshua\n  Bengio", "title": "Pointing the Unknown Words", "comments": "ACL 2016 Oral Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of rare and unknown words is an important issue that can\npotentially influence the performance of many NLP systems, including both the\ntraditional count-based and the deep learning models. We propose a novel way to\ndeal with the rare and unseen words for the neural network models using\nattention. Our model uses two softmax layers in order to predict the next word\nin conditional language models: one predicts the location of a word in the\nsource sentence, and the other predicts a word in the shortlist vocabulary. At\neach time-step, the decision of which softmax layer to use choose adaptively\nmade by an MLP which is conditioned on the context.~We motivate our work from a\npsychological evidence that humans naturally have a tendency to point towards\nobjects in the context or the environment when the name of an object is not\nknown.~We observe improvements on two tasks, neural machine translation on the\nEuroparl English to French parallel corpora and text summarization on the\nGigaword dataset using our proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 22:31:57 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 21:12:57 GMT"}, {"version": "v3", "created": "Sun, 21 Aug 2016 20:03:39 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Ahn", "Sungjin", ""], ["Nallapati", "Ramesh", ""], ["Zhou", "Bowen", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1603.08321", "submitter": "Linlin Chao", "authors": "Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li and Zhengqi Wen", "title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on two key problems for audio-visual emotion recognition\nin the video. One is the audio and visual streams temporal alignment for\nfeature level fusion. The other one is locating and re-weighting the perception\nattentions in the whole audio-visual stream for better recognition. The Long\nShort Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main\nclassification architecture. Firstly, soft attention mechanism aligns the audio\nand visual streams. Secondly, seven emotion embedding vectors, which are\ncorresponding to each classification emotion type, are added to locate the\nperception attentions. The locating and re-weighting process is also based on\nthe soft attention mechanism. The experiment results on EmotiW2015 dataset and\nthe qualitative analysis show the efficiency of the proposed two techniques.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 06:06:10 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Chao", "Linlin", ""], ["Tao", "Jianhua", ""], ["Yang", "Minghao", ""], ["Li", "Ya", ""], ["Wen", "Zhengqi", ""]]}, {"id": "1603.08458", "submitter": "Shaodian Zhang", "authors": "Shaodian Zhang, Edouard Grave, Elizabeth Sklar, Noemie Elhadad", "title": "Longitudinal Analysis of Discussion Topics in an Online Breast Cancer\n  Community using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying topics of discussions in online health communities (OHC) is\ncritical to various applications, but can be difficult because topics of OHC\ncontent are usually heterogeneous and domain-dependent. In this paper, we\nprovide a multi-class schema, an annotated dataset, and supervised classifiers\nbased on convolutional neural network (CNN) and other models for the task of\nclassifying discussion topics. We apply the CNN classifier to the most popular\nbreast cancer online community, and carry out a longitudinal analysis to show\ntopic distributions and topic changes throughout members' participation. Our\nexperimental results suggest that CNN outperforms other classifiers in the task\nof topic classification, and that certain trajectories can be detected with\nrespect to topic changes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 17:47:42 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 22:46:39 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 15:09:05 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Zhang", "Shaodian", ""], ["Grave", "Edouard", ""], ["Sklar", "Elizabeth", ""], ["Elhadad", "Noemie", ""]]}, {"id": "1603.08474", "submitter": "Oswaldo Ludwig", "authors": "Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens", "title": "Deep Embedding for Spatial Role Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the visually informed embedding of word (VIEW), a\ncontinuous vector representation for a word extracted from a deep neural model\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\nbetween visual objects, given a textual description. The model is composed of a\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\napplied to transferring multimodal background knowledge to Spatial Role\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\nmentioned in the text. This work also contributes with a new method to select\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 18:38:46 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Ludwig", "Oswaldo", ""], ["Liu", "Xiao", ""], ["Kordjamshidi", "Parisa", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1603.08507", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue,\n  Bernt Schiele, Trevor Darrell", "title": "Generating Visual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 19:54:12 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Rohrbach", "Marcus", ""], ["Donahue", "Jeff", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""]]}, {"id": "1603.08594", "submitter": "Gholamreza Haffari", "authors": "Geetanjali Rakshit, Sagar Sontakke, Pushpak Bhattacharyya, Gholamreza\n  Haffari", "title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and\n  Alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to solve the problem of Prepositional Phrase (PP)\nattachments in English. The motivation for the work comes from NLP applications\nlike Machine Translation, for which, getting the correct attachment of\nprepositions is very crucial. The idea is to correct the PP-attachments for a\nsentence with the help of alignments from parallel data in another language.\nThe novelty of our work lies in the formulation of the problem into a dual\ndecomposition based algorithm that enforces agreement between the parse trees\nfrom two languages as a constraint. Experiments were performed on the\nEnglish-Hindi language pair and the performance improved by 10% over the\nbaseline, where the baseline is the attachment predicted by the MSTParser model\ntrained for English.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 00:06:11 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Rakshit", "Geetanjali", ""], ["Sontakke", "Sagar", ""], ["Bhattacharyya", "Pushpak", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "1603.08636", "submitter": "EPTCS", "authors": "Jiri Vinarek (Charles University in Prague, Faculty of Mathematics and\n  Physics, Department of Distributed and Dependable Systems), Petr Hnetynka\n  (Charles University in Prague, Faculty of Mathematics and Physics, Department\n  of Distributed and Dependable Systems)", "title": "Towards an Automated Requirements-driven Development of Smart\n  Cyber-Physical Systems", "comments": "In Proceedings FESCA 2016, arXiv:1603.08371", "journal-ref": "EPTCS 205, 2016, pp. 59-68", "doi": "10.4204/EPTCS.205.5", "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Invariant Refinement Method for Self Adaptation (IRM-SA) is a design\nmethod targeting development of smart Cyber-Physical Systems (sCPS). It allows\nfor a systematic translation of the system requirements into the system\narchitecture expressed as an ensemble-based component system (EBCS). However,\nsince the requirements are captured using natural language, there exists the\ndanger of their misinterpretation due to natural language requirements'\nambiguity, which could eventually lead to design errors. Thus, automation and\nvalidation of the design process is desirable. In this paper, we (i) analyze\nthe translation process of natural language requirements into the IRM-SA model,\n(ii) identify individual steps that can be automated and/or validated using\nnatural language processing techniques, and (iii) propose suitable methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 04:34:39 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Vinarek", "Jiri", "", "Charles University in Prague, Faculty of Mathematics and\n  Physics, Department of Distributed and Dependable Systems"], ["Hnetynka", "Petr", "", "Charles University in Prague, Faculty of Mathematics and Physics, Department\n  of Distributed and Dependable Systems"]]}, {"id": "1603.08701", "submitter": "Enrico Santus", "authors": "Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang", "title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL\n  Datasets", "comments": "in LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we claim that Vector Cosine, which is generally considered one\nof the most efficient unsupervised measures for identifying word similarity in\nVector Space Models, can be outperformed by a completely unsupervised measure\nthat evaluates the extent of the intersection among the most associated\ncontexts of two target words, weighting such intersection according to the rank\nof the shared contexts in the dependency ranked lists. This claim comes from\nthe hypothesis that similar words do not simply occur in similar contexts, but\nthey share a larger portion of their most relevant contexts compared to other\nrelated words. To prove it, we describe and evaluate APSyn, a variant of\nAverage Precision that, independently of the adopted parameters, outperforms\nthe Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the\nbest setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy\nin the TOEFL dataset, beating therefore the non-English US college applicants\n(whose average, as reported in the literature, is 64.50%) and several\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:00:27 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Santus", "Enrico", ""], ["Chiu", "Tin-Shing", ""], ["Lu", "Qin", ""], ["Lenci", "Alessandro", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1603.08702", "submitter": "Enrico Santus", "authors": "Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin Lu, Chu-Ren Huang", "title": "Nine Features in a Random Forest to Learn Taxonomical Semantic Relations", "comments": "in LREC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms\nand random words that is derived from the already introduced ROOT13 (Santus et\nal., 2016). It relies on a Random Forest algorithm and nine unsupervised\ncorpus-based features. We evaluate it with a 10-fold cross validation on 9,600\npairs, equally distributed among the three classes and involving several\nParts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are\npresent, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2%\n(vector cosine). When the classification is binary, ROOT9 achieves the\nfollowing results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%,\nhypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In\norder to compare the performance with the state-of-the-art, we have also\nevaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it\nis in fact competitive. Finally, we investigated whether the system learns the\nsemantic relation or it simply learns the prototypical hypernyms, as claimed by\nLevy et al. (2015). The second possibility seems to be the most likely, even\nthough ROOT9 can be trained on negative examples (i.e., switched hypernyms) to\ndrastically reduce this bias.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:00:40 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Santus", "Enrico", ""], ["Lenci", "Alessandro", ""], ["Chiu", "Tin-Shing", ""], ["Lu", "Qin", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1603.08705", "submitter": "Enrico Santus", "authors": "Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci and Chu-Ren\n  Huang", "title": "ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms", "comments": "in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe ROOT13, a supervised system for the classification\nof hypernyms, co-hyponyms and random words. The system relies on a Random\nForest algorithm and 13 unsupervised corpus-based features. We evaluate it with\na 10-fold cross validation on 9,600 pairs, equally distributed among the three\nclasses and involving several Parts-Of-Speech (i.e. adjectives, nouns and\nverbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%,\nagainst a baseline of 57.6% (vector cosine). When the classification is binary,\nROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%),\nhypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Our\nresults are competitive with stateof-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 10:05:05 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Santus", "Enrico", ""], ["Chiu", "Tin-Shing", ""], ["Lu", "Qin", ""], ["Lenci", "Alessandro", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1603.08832", "submitter": "Ethan Fast", "authors": "Ethan Fast, Tina Vachovsky, Michael S. Bernstein", "title": "Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias\n  in an Online Fiction Writing Community", "comments": "in ICWSM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a princess asleep in a castle, waiting for her prince to slay the\ndragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up\ngender roles. But what about more modern stories, borne of a generation\nincreasingly aware of social constructs like sexism and racism? Do these\nstories tend to reinforce gender stereotypes, or counter them? In this paper,\nwe present a technique that combines natural language processing with a\ncrowdsourced lexicon of stereotypes to capture gender biases in fiction. We\napply this technique across 1.8 billion words of fiction from the Wattpad\nonline writing community, investigating gender representation in stories, how\nmale and female characters behave and are described, and how authors' use of\ngender stereotypes is associated with the community's ratings. We find that\nmale over-representation and traditional gender stereotypes (e.g., dominant men\nand submissive women) are common throughout nearly every genre in our corpus.\nHowever, only some of these stereotypes, like sexual or violent men, are\nassociated with highly rated stories. Finally, despite women often being the\ntarget of negative stereotypes, female authors are equally likely to write such\nstereotypes as men.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 16:24:46 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Fast", "Ethan", ""], ["Vachovsky", "Tina", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1603.08865", "submitter": "Emil Axelsson", "authors": "Emil Axelsson", "title": "Compilation as a Typed EDSL-to-EDSL Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is about an implementation and compilation technique that is\nused in RAW-Feldspar which is a complete rewrite of the Feldspar embedded\ndomain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-level\nfunctional language that generates efficient C code to run on embedded targets.\nThe gist of the technique presented in this post is the following: rather\nwriting a back end that converts pure Feldspar expressions directly to C, we\ntranslate them to a low-level monadic EDSL. From the low-level EDSL, C code is\nthen generated. This approach has several advantages:\n  1. The translation is simpler to write than a complete C back end.\n  2. The translation is between two typed EDSLs, which rules out many potential\nerrors.\n  3. The low-level EDSL is reusable and can be shared between several\nhigh-level EDSLs.\n  Although the article contains a lot of code, most of it is in fact reusable.\nAs mentioned in Discussion, we can write the same implementation in less than\n50 lines of code using generic libraries that we have developed to support\nFeldspar.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 17:58:46 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 09:58:06 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 17:30:56 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Axelsson", "Emil", ""]]}, {"id": "1603.08868", "submitter": "Sowmya Vajjala", "authors": "Ildik\\'o Pil\\'an, Sowmya Vajjala and Elena Volodina", "title": "A Readable Read: Automatic Assessment of Language Learning Materials\n  based on Linguistic Complexity", "comments": "Presented at CICLING 2015 and won the best poster award (16th\n  International Conference on Intelligent Text Processing and Computational\n  Linguistics). To appear in International Journal of Computational Linguistics\n  and Applications (IJLCA), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corpora and web texts can become a rich language learning resource if we have\na means of assessing whether they are linguistically appropriate for learners\nat a given proficiency level. In this paper, we aim at addressing this issue by\npresenting the first approach for predicting linguistic complexity for Swedish\nsecond language learning material on a 5-point scale. After showing that the\ntraditional Swedish readability measure, L\\\"asbarhetsindex (LIX), is not\nsuitable for this task, we propose a supervised machine learning model, based\non a range of linguistic features, that can reliably classify texts according\nto their difficulty level. Our model obtained an accuracy of 81.3% and an\nF-score of 0.8, which is comparable to the state of the art in English and is\nconsiderably higher than previously reported results for other languages. We\nfurther studied the utility of our features with single sentences instead of\nfull texts since sentences are a common linguistic unit in language learning\nexercises. We trained a separate model on sentence-level data with five\nclasses, which yielded 63.4% accuracy. Although this is lower than the document\nlevel performance, we achieved an adjacent accuracy of 92%. Furthermore, we\nfound that using a combination of different features, compared to using lexical\nfeatures alone, resulted in 7% improvement in classification accuracy at the\nsentence level, whereas at the document level, lexical features were more\ndominant. Our models are intended for use in a freely accessible web-based\nlanguage learning platform for the automatic generation of exercises.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 18:12:28 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Pil\u00e1n", "Ildik\u00f3", ""], ["Vajjala", "Sowmya", ""], ["Volodina", "Elena", ""]]}, {"id": "1603.08884", "submitter": "Adam Trischler", "authors": "Adam Trischler and Zheng Ye and Xingdi Yuan and Jing He and Phillip\n  Bachman and Kaheer Suleman", "title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "comments": "9 pages, submitted to ACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding unstructured text is a major goal within natural language\nprocessing. Comprehension tests pose questions based on short text passages to\nevaluate such understanding. In this work, we investigate machine comprehension\non the challenging {\\it MCTest} benchmark. Partly because of its limited size,\nprior work on {\\it MCTest} has focused mainly on engineering better features.\nWe tackle the dataset with a neural approach, harnessing simple neural networks\narranged in a parallel hierarchy. The parallel hierarchy enables our model to\ncompare the passage, question, and answer from a variety of trainable\nperspectives, as opposed to using a manually designed, rigid feature set.\nPerspectives range from the word level to sentence fragments to sequences of\nsentences; the networks operate only on word-embedding representations of text.\nWhen trained with a methodology designed to help cope with limited training\ndata, our Parallel-Hierarchical model sets a new state of the art for {\\it\nMCTest}, outperforming previous feature-engineered approaches slightly and\nprevious neural approaches by a significant margin (over 15\\% absolute).\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 18:52:46 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Trischler", "Adam", ""], ["Ye", "Zheng", ""], ["Yuan", "Xingdi", ""], ["He", "Jing", ""], ["Bachman", "Phillip", ""], ["Suleman", "Kaheer", ""]]}, {"id": "1603.08887", "submitter": "Greg Durrett", "authors": "Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein", "title": "Learning-Based Single-Document Summarization with Compression and\n  Anaphoricity Constraints", "comments": "ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a discriminative model for single-document summarization that\nintegrally combines compression and anaphoricity constraints. Our model selects\ntextual units to include in the summary based on a rich set of sparse features\nwhose weights are learned on a large corpus. We allow for the deletion of\ncontent within a sentence when that deletion is licensed by compression rules;\nin our framework, these are implemented as dependencies between subsentential\nunits of text. Anaphoricity constraints then improve cross-sentence coherence\nby guaranteeing that, for each pronoun included in the summary, the pronoun's\nantecedent is included as well or the pronoun is rewritten as a full mention.\nWhen trained end-to-end, our final system outperforms prior work on both ROUGE\nas well as on human judgments of linguistic quality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 18:58:42 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 05:39:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Durrett", "Greg", ""], ["Berg-Kirkpatrick", "Taylor", ""], ["Klein", "Dan", ""]]}, {"id": "1603.09054", "submitter": "Enrico Santus", "authors": "Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci and Chu-Ren\n  Huang", "title": "Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence\n  and Vector Cosine in VSMs", "comments": "in AAAI 2016. arXiv admin note: substantial text overlap with\n  arXiv:1603.08701", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we claim that vector cosine, which is generally considered\namong the most efficient unsupervised measures for identifying word similarity\nin Vector Space Models, can be outperformed by an unsupervised measure that\ncalculates the extent of the intersection among the most mutually dependent\ncontexts of the target words. To prove it, we describe and evaluate APSyn, a\nvariant of the Average Precision that, without any optimization, outperforms\nthe vector cosine and the co-occurrence on the standard ESL test set, with an\nimprovement ranging between +9.00% and +17.98%, depending on the number of\nchosen top contexts.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 07:05:45 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Santus", "Enrico", ""], ["Chiu", "Tin-Shing", ""], ["Lu", "Qin", ""], ["Lenci", "Alessandro", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1603.09128", "submitter": "Simon \\v{S}uster", "authors": "Simon \\v{S}uster and Ivan Titov and Gertjan van Noord", "title": "Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders", "comments": "11 pages, to appear at NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learning multi-sense word embeddings relying both\non monolingual and bilingual information. Our model consists of an encoder,\nwhich uses monolingual and bilingual context (i.e. a parallel sentence) to\nchoose a sense for a given word, and a decoder which predicts context words\nbased on the chosen sense. The two components are estimated jointly. We observe\nthat the word representations induced from bilingual data outperform the\nmonolingual counterparts across a range of evaluation tasks, even though\ncrosslingual information is not available at test time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 11:09:01 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["\u0160uster", "Simon", ""], ["Titov", "Ivan", ""], ["van Noord", "Gertjan", ""]]}, {"id": "1603.09170", "submitter": "Bin Wang", "authors": "Bin Wang, Zhijian Ou, Yong He, Akinori Kawamura", "title": "Model Interpolation with Trans-dimensional Random Field Language Models\n  for Speech Recognition", "comments": "three pages, 2 experiment result tables, reporting the WERs on an\n  Englisth dateset and a Chinese dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant language models (LMs) such as n-gram and neural network (NN)\nmodels represent sentence probabilities in terms of conditionals. In contrast,\na new trans-dimensional random field (TRF) LM has been recently introduced to\nshow superior performances, where the whole sentence is modeled as a random\nfield. In this paper, we examine how the TRF models can be interpolated with\nthe NN models, and obtain 12.1\\% and 17.9\\% relative error rate reductions over\n6-gram LMs for English and Chinese speech recognition respectively through\nlog-linear combination.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:09:20 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 13:19:06 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 02:36:33 GMT"}, {"version": "v4", "created": "Wed, 17 Aug 2016 01:54:32 GMT"}, {"version": "v5", "created": "Sat, 20 Aug 2016 03:50:16 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wang", "Bin", ""], ["Ou", "Zhijian", ""], ["He", "Yong", ""], ["Kawamura", "Akinori", ""]]}, {"id": "1603.09188", "submitter": "Spandana Gella", "authors": "Spandana Gella, Mirella Lapata, Frank Keller", "title": "Unsupervised Visual Sense Disambiguation for Verbs using Multimodal\n  Embeddings", "comments": "11 pages, NAACL-HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, visual sense disambiguation for verbs: given an\nimage and a verb, assign the correct sense of the verb, i.e., the one that\ndescribes the action depicted in the image. Just as textual word sense\ndisambiguation is useful for a wide range of NLP tasks, visual sense\ndisambiguation can be useful for multimodal tasks such as image retrieval,\nimage description, and text illustration. We introduce VerSe, a new dataset\nthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.\nWe propose an unsupervised algorithm based on Lesk which performs visual sense\ndisambiguation using textual, visual, or multimodal embeddings. We find that\ntextual embeddings perform well when gold-standard textual annotations (object\nlabels and image descriptions) are available, while multimodal embeddings\nperform well on unannotated images. We also verify our findings by using the\ntextual and multimodal embeddings as features in a supervised setting and\nanalyse the performance of visual sense disambiguation task. VerSe is made\npublicly available and can be downloaded at:\nhttps://github.com/spandanagella/verse.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:43:38 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Gella", "Spandana", ""], ["Lapata", "Mirella", ""], ["Keller", "Frank", ""]]}, {"id": "1603.09381", "submitter": "Peng Li", "authors": "Peng Li and Heng Huang", "title": "Clinical Information Extraction via Convolutional Neural Network", "comments": "arXiv admin note: text overlap with arXiv:1408.5882 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report an implementation of a clinical information extraction tool that\nleverages deep neural network to annotate event spans and their attributes from\nraw clinical notes and pathology reports. Our approach uses context words and\ntheir part-of-speech tags and shape information as features. Then we hire\ntemporal (1D) convolutional neural network to learn hidden feature\nrepresentations. Finally, we use Multilayer Perceptron (MLP) to predict event\nspans. The empirical evaluation demonstrates that our approach significantly\noutperforms baselines.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 20:57:07 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Peng", ""], ["Huang", "Heng", ""]]}, {"id": "1603.09405", "submitter": "Peng Li", "authors": "Peng Li and Heng Huang", "title": "Enhancing Sentence Relation Modeling with Auxiliary Character-level\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based approaches for sentence relation modeling automatically\ngenerate hidden matching features from raw sentence pairs. However, the quality\nof matching feature representation may not be satisfied due to complex semantic\nrelations such as entailment or contradiction. To address this challenge, we\npropose a new deep neural network architecture that jointly leverage\npre-trained word embedding and auxiliary character embedding to learn sentence\nmeanings. The two kinds of word sequence representations as inputs into\nmulti-layer bidirectional LSTM to learn enhanced sentence representation. After\nthat, we construct matching features followed by another temporal CNN to learn\nhigh-level hidden matching feature representations. Experimental results\ndemonstrate that our approach consistently outperforms the existing methods on\nstandard evaluation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 22:39:59 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Li", "Peng", ""], ["Huang", "Heng", ""]]}, {"id": "1603.09457", "submitter": "Yi Luan", "authors": "Yi Luan, Yangfeng Ji, Mari Ostendorf", "title": "LSTM based Conversation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a conversational model that incorporates both\ncontext and participant role for two-party conversations. Different\narchitectures are explored for integrating participant role and context\ninformation into a Long Short-term Memory (LSTM) language model. The\nconversational model can function as a language model or a language generation\nmodel. Experiments on the Ubuntu Dialog Corpus show that our model can capture\nmultiple turn interaction between participants. The proposed method outperforms\na traditional LSTM model as measured by language model perplexity and response\nranking. Generated responses show characteristic differences between the two\nparticipant roles.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 05:14:10 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Luan", "Yi", ""], ["Ji", "Yangfeng", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1603.09460", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Dong Wang, Xiaodong Zhang, Thomas Fang Zheng, Panshi Jin", "title": "System Combination for Short Utterance Speaker Recognition", "comments": "APSIPA ASC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text-independent short-utterance speaker recognition (SUSR), the\nperformance often degrades dramatically. This paper presents a combination\napproach to the SUSR tasks with two phonetic-aware systems: one is the\nDNN-based i-vector system and the other is our recently proposed\nsubregion-based GMM-UBM system. The former employs phone posteriors to\nconstruct an i-vector model in which the shared statistics offers stronger\nrobustness against limited test data, while the latter establishes a\nphone-dependent GMM-UBM system which represents speaker characteristics with\nmore details. A score-level fusion is implemented to integrate the respective\nadvantages from the two systems. Experimental results show that for the\ntext-independent SUSR task, both the DNN-based i-vector system and the\nsubregion-based GMM-UBM system outperform their respective baselines, and the\nscore-level system combination delivers performance improvement.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 05:47:03 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 13:49:05 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Zhang", "Xiaodong", ""], ["Zheng", "Thomas Fang", ""], ["Jin", "Panshi", ""]]}, {"id": "1603.09509", "submitter": "Zhenyao Zhu", "authors": "Zhenyao Zhu, Jesse H. Engel, Awni Hannun", "title": "Learning Multiscale Features Directly From Waveforms", "comments": "\"fix typo in the title\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has dramatically improved the performance of speech recognition\nsystems through learning hierarchies of features optimized for the task at\nhand. However, true end-to-end learning, where features are learned directly\nfrom waveforms, has only recently reached the performance of hand-tailored\nrepresentations based on the Fourier transform. In this paper, we detail an\napproach to use convolutional filters to push past the inherent tradeoff of\ntemporal and frequency resolution that exists for spectral representations. At\nincreased computational cost, we show that increasing temporal resolution via\nreduced stride and increasing frequency resolution via additional filters\ndelivers significant performance improvements. Further, we find more efficient\nrepresentations by simultaneously learning at multiple scales, leading to an\noverall decrease in word error rate on a difficult internal speech test set by\n20.7% relative to networks with the same number of parameters trained on\nspectrograms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 09:54:44 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 14:17:09 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Zhu", "Zhenyao", ""], ["Engel", "Jesse H.", ""], ["Hannun", "Awni", ""]]}, {"id": "1603.09630", "submitter": "Pawel Swietojanski", "authors": "Pawel Swietojanski and Steve Renals", "title": "Differentiable Pooling for Unsupervised Acoustic Model Adaptation", "comments": "11 pages, 7 Tables, 7 Figures in IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing, vol. 24, num. 11, 2016", "journal-ref": null, "doi": "10.1109/TASLP.2016.2584700", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:10:40 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 18:12:49 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Swietojanski", "Pawel", ""], ["Renals", "Steve", ""]]}, {"id": "1603.09631", "submitter": "Miroslav Vodol\\'an", "authors": "Miroslav Vodol\\'an, Filip Jur\\v{c}\\'i\\v{c}ek", "title": "Data Collection for Interactive Learning through the Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a dataset collected from natural dialogs which enables to\ntest the ability of dialog systems to learn new facts from user utterances\nthroughout the dialog. This interactive learning will help with one of the most\nprevailing problems of open domain dialog system, which is the sparsity of\nfacts a dialog system can reason about. The proposed dataset, consisting of\n1900 collected dialogs, allows simulation of an interactive gaining of\ndenotations and questions explanations from users which can be used for the\ninteractive learning.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:13:51 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 13:03:26 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Vodol\u00e1n", "Miroslav", ""], ["Jur\u010d\u00ed\u010dek", "Filip", ""]]}, {"id": "1603.09643", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Lantian Li and Dong Wang", "title": "Multi-task Recurrent Model for Speech and Speaker Recognition", "comments": "APSIPA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although highly correlated, speech and speaker recognition have been regarded\nas two independent tasks and studied by two communities. This is certainly not\nthe way that people behave: we decipher both speech content and speaker traits\nat the same time. This paper presents a unified model to perform speech and\nspeaker recognition simultaneously and altogether. The model is based on a\nunified neural network where the output of one task is fed to the input of the\nother, leading to a multi-task recurrent network. Experiments show that the\njoint model outperforms the task-specific models on both the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:37:29 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 05:54:30 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 06:25:01 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 12:27:17 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Li", "Lantian", ""], ["Wang", "Dong", ""]]}, {"id": "1603.09727", "submitter": "Ziang Xie", "authors": "Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Jurafsky, Andrew Y. Ng", "title": "Neural Language Correction with Character-Based Attention", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language correction has the potential to help language learners\nimprove their writing skills. While approaches with separate classifiers for\ndifferent error types have high precision, they do not flexibly handle errors\nsuch as redundancy or non-idiomatic phrasing. On the other hand, word and\nphrase-based machine translation methods are not designed to cope with\northographic errors, and have recently been outpaced by neural models.\nMotivated by these issues, we present a neural network-based approach to\nlanguage correction. The core component of our method is an encoder-decoder\nrecurrent neural network with an attention mechanism. By operating at the\ncharacter level, the network avoids the problem of out-of-vocabulary words. We\nillustrate the flexibility of our approach on dataset of noisy, user-generated\ntext collected from an English learner forum. When combined with a language\nmodel, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014\nShared Task. We further demonstrate that training the network on additional\ndata with synthesized errors can improve performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:16:54 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Xie", "Ziang", ""], ["Avati", "Anand", ""], ["Arivazhagan", "Naveen", ""], ["Jurafsky", "Dan", ""], ["Ng", "Andrew Y.", ""]]}]