[{"id": "2005.00019", "submitter": "Michael Lepori Jr.", "authors": "Michael A. Lepori, Tal Linzen, and R. Thomas McCoy", "title": "Representations of Syntax [MASK] Useful: Effects of Constituency and\n  Dependency Structure in Recursive LSTMs", "comments": "To appear in Proceedings of the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-based neural networks show significant sensitivity to syntactic\nstructure, but they still perform less well on syntactic tasks than tree-based\nnetworks. Such tree-based networks can be provided with a constituency parse, a\ndependency parse, or both. We evaluate which of these two representational\nschemes more effectively introduces biases for syntactic structure that\nincrease performance on the subject-verb agreement prediction task. We find\nthat a constituency-based network generalizes more robustly than a\ndependency-based one, and that combining the two types of structure does not\nyield further improvement. Finally, we show that the syntactic robustness of\nsequential models can be substantially improved by fine-tuning on a small\namount of constructed data, suggesting that data augmentation is a viable\nalternative to explicit constituency structure for imparting the syntactic\nbiases that sequential models are lacking.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:00:06 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lepori", "Michael A.", ""], ["Linzen", "Tal", ""], ["McCoy", "R. Thomas", ""]]}, {"id": "2005.00033", "submitter": "Firoj Alam", "authors": "Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov,\n  Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem\n  Darwish, Preslav Nakov", "title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of\n  Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the\n  Society", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic is ranked\nsecond in the list of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Addressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nThus, here we design, annotate, and release to the research community a new\ndataset for fine-grained disinformation analysis that (i)focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society as a whole, and (iii) covers\nboth English and Arabic. Finally, we show strong evaluation results using\nstate-of-the-art Transformers, thus confirming the practical utility of the\nannotation schema and of the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:04:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 13:33:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Alam", "Firoj", ""], ["Shaar", "Shaden", ""], ["Dalvi", "Fahim", ""], ["Sajjad", "Hassan", ""], ["Nikolov", "Alex", ""], ["Mubarak", "Hamdy", ""], ["Martino", "Giovanni Da San", ""], ["Abdelali", "Ahmed", ""], ["Durrani", "Nadir", ""], ["Darwish", "Kareem", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.00036", "submitter": "Mohsen Mesgar", "authors": "Mohsen Mesgar, Edwin Simpson, Iryna Gurevych", "title": "Improving Factual Consistency Between a Response and Persona Facts", "comments": "Accepted in EACL'21 (https://www.aclweb.org/anthology/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models for response generation produce responses that are semantically\nplausible but not necessarily factually consistent with facts describing the\nspeaker's persona. These models are trained with fully supervised learning\nwhere the objective function barely captures factual consistency. We propose to\nfine-tune these models by reinforcement learning and an efficient reward\nfunction that explicitly captures the consistency between a response and\npersona facts as well as semantic plausibility. Our automatic and human\nevaluations on the PersonaChat corpus confirm that our approach increases the\nrate of responses that are factually consistent with persona facts over its\nsupervised counterpart while retaining the language quality of responses.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:08:22 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 04:21:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mesgar", "Mohsen", ""], ["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2005.00038", "submitter": "Wenhan Xiong", "authors": "Wenhan Xiong, Hong Wang, William Yang Wang", "title": "Progressively Pretrained Dense Corpus Index for Open-Domain Question\n  Answering", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extract answers from a large corpus, open-domain question answering (QA)\nsystems usually rely on information retrieval (IR) techniques to narrow the\nsearch space. Standard inverted index methods such as TF-IDF are commonly used\nas thanks to their efficiency. However, their retrieval performance is limited\nas they simply use shallow and sparse lexical features. To break the IR\nbottleneck, recent studies show that stronger retrieval performance can be\nachieved by pretraining a effective paragraph encoder that index paragraphs\ninto dense vectors. Once trained, the corpus can be pre-encoded into\nlow-dimensional vectors and stored within an index structure where the\nretrieval can be efficiently implemented as maximum inner product search.\n  Despite the promising results, pretraining such a dense index is expensive\nand often requires a very large batch size. In this work, we propose a simple\nand resource-efficient method to pretrain the paragraph encoder. First, instead\nof using heuristically created pseudo question-paragraph pairs for pretraining,\nwe utilize an existing pretrained sequence-to-sequence model to build a strong\nquestion generator that creates high-quality pretraining data. Second, we\npropose a progressive pretraining algorithm to ensure the existence of\neffective negative samples in each batch. Across three datasets, our method\noutperforms an existing dense retrieval method that uses 7 times more\ncomputational resources for pretraining.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:09:50 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 04:37:42 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Xiong", "Wenhan", ""], ["Wang", "Hong", ""], ["Wang", "William Yang", ""]]}, {"id": "2005.00042", "submitter": "Maharshi Pandya", "authors": "Maharshi R. Pandya, Jessica Reyes, Bob Vanderheyden", "title": "Method for Customizable Automated Tagging: Addressing the Problem of\n  Over-tagging and Under-tagging Text Documents", "comments": "Work done by Maharshi R. Pandya and Jessica Reyes as IBM interns\n  under leadership of Bob Vanderheyden. Article to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using author provided tags to predict tags for a new document often results\nin the overgeneration of tags. In the case where the author doesn't provide any\ntags, our documents face the severe under-tagging issue. In this paper, we\npresent a method to generate a universal set of tags that can be applied widely\nto a large document corpus. Using IBM Watson's NLU service, first, we collect\nkeywords/phrases that we call \"complex document tags\" from 8,854 popular\nreports in the corpus. We apply LDA model over these complex document tags to\ngenerate a set of 765 unique \"simple tags\". In applying the tags to a corpus of\ndocuments, we run each document through the IBM Watson NLU and apply\nappropriate simple tags. Using only 765 simple tags, our method allows us to\ntag 87,397 out of 88,583 total documents in the corpus with at least one tag.\nAbout 92.1% of the total 87,397 documents are also determined to be\nsufficiently-tagged. In the end, we discuss the performance of our method and\nits limitations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:28:42 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Pandya", "Maharshi R.", ""], ["Reyes", "Jessica", ""], ["Vanderheyden", "Bob", ""]]}, {"id": "2005.00048", "submitter": "Sivasurya Santhanam", "authors": "Sivasurya Santhanam", "title": "Context based Text-generation using LSTM networks", "comments": "10 pages, Abstract published in A2IC 2018\n  (https://www.premc.org/doc/A2IC2018/A2IC2018_Book_Of_Abstracts.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory(LSTM) units on sequence-based models are being used in\ntranslation, question-answering systems, classification tasks due to their\ncapability of learning long-term dependencies. In Natural language generation,\nLSTM networks are providing impressive results on text generation models by\nlearning language models with grammatically stable syntaxes. But the downside\nis that the network does not learn about the context. The network only learns\nthe input-output function and generates text given a set of input words\nirrespective of pragmatics. As the model is trained without any such context,\nthere is no semantic consistency among the generated sentences. The proposed\nmodel is trained to generate text for a given set of input words along with a\ncontext vector. A context vector is similar to a paragraph vector that grasps\nthe semantic meaning(context) of the sentence. Several methods of extracting\nthe context vectors are proposed in this work. While training a language model,\nin addition to the input-output sequences, context vectors are also trained\nalong with the inputs. Due to this structure, the model learns the relation\namong the input words, context vector and the target word. Given a set of\ncontext terms, a well trained model will generate text around the provided\ncontext. Based on the nature of computing context vectors, the model has been\ntried out with two variations (word importance and word clustering). In the\nword clustering method, the suitable embeddings among various domains are also\nexplored. The results are evaluated based on the semantic closeness of the\ngenerated text to the given context.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:39:25 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Santhanam", "Sivasurya", ""]]}, {"id": "2005.00050", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov and Mario Giulianelli", "title": "UiO-UvA at SemEval-2020 Task 1: Contextualised Embeddings for Lexical\n  Semantic Change Detection", "comments": "To appear in Proceedings of the 14th International Workshop on\n  Semantic Evaluation (SemEval-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We apply contextualised word embeddings to lexical semantic change detection\nin the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking\nwords by the degree of their semantic drift over time. We analyse the\nperformance of two contextualising architectures (BERT and ELMo) and three\nchange detection algorithms. We find that the most effective algorithms rely on\nthe cosine similarity between averaged token embeddings and the pairwise\ndistances between token embeddings. They outperform strong baselines by a large\nmargin (in the post-evaluation phase, we have the best Subtask 2 submission for\nSemEval-2020 Task 1), but interestingly, the choice of a particular algorithm\ndepends on the distribution of gold scores in the test set.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:43:57 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 17:46:21 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 01:44:32 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kutuzov", "Andrey", ""], ["Giulianelli", "Mario", ""]]}, {"id": "2005.00052", "submitter": "Jonas Pfeiffer", "authors": "Jonas Pfeiffer, Ivan Vuli\\'c, Iryna Gurevych, Sebastian Ruder", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal behind state-of-the-art pre-trained multilingual models such as\nmultilingual BERT and XLM-R is enabling and bootstrapping NLP applications in\nlow-resource languages through zero-shot or few-shot cross-lingual transfer.\nHowever, due to limited model capacity, their transfer performance is the\nweakest exactly on such low-resource languages and languages unseen during\npre-training. We propose MAD-X, an adapter-based framework that enables high\nportability and parameter-efficient transfer to arbitrary tasks and languages\nby learning modular language and task representations. In addition, we\nintroduce a novel invertible adapter architecture and a strong baseline method\nfor adapting a pre-trained multilingual model to a new language. MAD-X\noutperforms the state of the art in cross-lingual transfer across a\nrepresentative set of typologically diverse languages on named entity\nrecognition and causal commonsense reasoning, and achieves competitive results\non question answering. Our code and adapters are available at AdapterHub.ml\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:54:43 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:28:42 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 10:17:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pfeiffer", "Jonas", ""], ["Vuli\u0107", "Ivan", ""], ["Gurevych", "Iryna", ""], ["Ruder", "Sebastian", ""]]}, {"id": "2005.00062", "submitter": "Yiding Hao", "authors": "Yiding Hao", "title": "Attribution Analysis of Grammatical Dependencies in LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTM language models have been shown to capture syntax-sensitive grammatical\ndependencies such as subject-verb agreement with a high degree of accuracy\n(Linzen et al., 2016, inter alia). However, questions remain regarding whether\nthey do so using spurious correlations, or whether they are truly able to match\nverbs with their subjects. This paper argues for the latter hypothesis. Using\nlayer-wise relevance propagation (Bach et al., 2015), a technique that\nquantifies the contributions of input features to model behavior, we show that\nLSTM performance on number agreement is directly correlated with the model's\nability to distinguish subjects from other nouns. Our results suggest that LSTM\nlanguage models are able to infer robust representations of syntactic\ndependencies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:19:37 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Hao", "Yiding", ""]]}, {"id": "2005.00084", "submitter": "Benjamin Schiller", "authors": "Benjamin Schiller and Johannes Daxenberger and Iryna Gurevych", "title": "Aspect-Controlled Neural Argument Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We rely on arguments in our daily lives to deliver our opinions and base them\non evidence, making them more convincing in turn. However, finding and\nformulating arguments can be challenging. In this work, we train a language\nmodel for argument generation that can be controlled on a fine-grained level to\ngenerate sentence-level arguments for a given topic, stance, and aspect. We\ndefine argument aspect detection as a necessary method to allow this\nfine-granular control and crowdsource a dataset with 5,032 arguments annotated\nwith aspects. Our evaluation shows that our generation model is able to\ngenerate high-quality, aspect-specific arguments. Moreover, these arguments can\nbe used to improve the performance of stance detection models via data\naugmentation and to generate counter-arguments. We publish all datasets and\ncode to fine-tune the language model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:17:22 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Schiller", "Benjamin", ""], ["Daxenberger", "Johannes", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2005.00085", "submitter": "Anoop Kunchukuttan", "authors": "Anoop Kunchukuttan, Divyanshu Kakwani, Satish Golla, Gokul N.C., Avik\n  Bhattacharyya, Mitesh M. Khapra, Pratyush Kumar", "title": "AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for\n  Indic Languages", "comments": "7 pages, 8 tables,\n  https://github.com/ai4bharat-indicnlp/indicnlp_corpus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the IndicNLP corpus, a large-scale, general-domain corpus\ncontaining 2.7 billion words for 10 Indian languages from two language\nfamilies. We share pre-trained word embeddings trained on these corpora. We\ncreate news article category classification datasets for 9 languages to\nevaluate the embeddings. We show that the IndicNLP embeddings significantly\noutperform publicly available pre-trained embedding on multiple evaluation\ntasks. We hope that the availability of the corpus will accelerate Indic NLP\nresearch. The resources are available at\nhttps://github.com/ai4bharat-indicnlp/indicnlp_corpus.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:21:02 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Kunchukuttan", "Anoop", ""], ["Kakwani", "Divyanshu", ""], ["Golla", "Satish", ""], ["C.", "Gokul N.", ""], ["Bhattacharyya", "Avik", ""], ["Khapra", "Mitesh M.", ""], ["Kumar", "Pratyush", ""]]}, {"id": "2005.00087", "submitter": "Thy Tran", "authors": "Thy Thy Tran, Phong Le, Sophia Ananiadou", "title": "Revisiting Unsupervised Relation Extraction", "comments": "8 pages, 1 figure, 2 tables. Accepted in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised relation extraction (URE) extracts relations between named\nentities from raw text without manually-labelled data and existing knowledge\nbases (KBs). URE methods can be categorised into generative and discriminative\napproaches, which rely either on hand-crafted features or surface form.\nHowever, we demonstrate that by using only named entities to induce relation\ntypes, we can outperform existing methods on two popular datasets. We conduct a\ncomparison and evaluation of our findings with other URE techniques, to\nascertain the important features in URE. We conclude that entity types provide\na strong inductive bias for URE.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:22:45 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Tran", "Thy Thy", ""], ["Le", "Phong", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "2005.00096", "submitter": "Zixing Zhang", "authors": "Jing Han, Kun Qian, Meishu Song, Zijiang Yang, Zhao Ren, Shuo Liu,\n  Juan Liu, Huaiyuan Zheng, Wei Ji, Tomoya Koike, Xiao Li, Zixing Zhang,\n  Yoshiharu Yamamoto, Bj\\\"orn W. Schuller", "title": "An Early Study on Intelligent Analysis of Speech under COVID-19:\n  Severity, Sleep Quality, Fatigue, and Anxiety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 outbreak was announced as a global pandemic by the World Health\nOrganisation in March 2020 and has affected a growing number of people in the\npast few weeks. In this context, advanced artificial intelligence techniques\nare brought to the fore in responding to fight against and reduce the impact of\nthis global health crisis. In this study, we focus on developing some potential\nuse-cases of intelligent speech analysis for COVID-19 diagnosed patients. In\nparticular, by analysing speech recordings from these patients, we construct\naudio-only-based models to automatically categorise the health state of\npatients from four aspects, including the severity of illness, sleep quality,\nfatigue, and anxiety. For this purpose, two established acoustic feature sets\nand support vector machines are utilised. Our experiments show that an average\naccuracy of .69 obtained estimating the severity of illness, which is derived\nfrom the number of days in hospitalisation. We hope that this study can foster\nan extremely fast, low-cost, and convenient way to automatically detect the\nCOVID-19 disease.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:47:05 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 10:00:08 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Han", "Jing", ""], ["Qian", "Kun", ""], ["Song", "Meishu", ""], ["Yang", "Zijiang", ""], ["Ren", "Zhao", ""], ["Liu", "Shuo", ""], ["Liu", "Juan", ""], ["Zheng", "Huaiyuan", ""], ["Ji", "Wei", ""], ["Koike", "Tomoya", ""], ["Li", "Xiao", ""], ["Zhang", "Zixing", ""], ["Yamamoto", "Yoshiharu", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2005.00100", "submitter": "Alexander Gutkin", "authors": "Alexander Gutkin, Tatiana Merkulova and Martin Jansche", "title": "Linguistic Typology Features from Text: Inferring the Sparse Features of\n  World Atlas of Language Structures", "comments": "Originally prepared as a conference submission to EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The use of linguistic typological resources in natural language processing\nhas been steadily gaining more popularity. It has been observed that the use of\ntypological information, often combined with distributed language\nrepresentations, leads to significantly more powerful models. While linguistic\ntypology representations from various resources have mostly been used for\nconditioning the models, there has been relatively little attention on\npredicting features from these resources from the input data. In this paper we\ninvestigate whether the various linguistic features from World Atlas of\nLanguage Structures (WALS) can be reliably inferred from multi-lingual text.\nSuch a predictor can be used to infer structural features for a language never\nobserved in training data. We frame this task as a multi-label classification\ninvolving predicting the set of non-mutually exclusive and extremely sparse\nmulti-valued labels (WALS features). We construct a recurrent neural network\npredictor based on byte embeddings and convolutional layers and test its\nperformance on 556 languages, providing analysis for various linguistic types,\nmacro-areas, language families and individual features. We show that some\nfeatures from various linguistic types can be predicted reliably.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:00:53 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 20:53:11 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gutkin", "Alexander", ""], ["Merkulova", "Tatiana", ""], ["Jansche", "Martin", ""]]}, {"id": "2005.00110", "submitter": "Shane Steinert-Threlkeld", "authors": "Nur Geffen Lan, Emmanuel Chemla, Shane Steinert-Threlkeld", "title": "On the Spontaneous Emergence of Discrete and Compositional Signals", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework to study language emergence through signaling\ngames with neural agents. Using a continuous latent space, we are able to (i)\ntrain using backpropagation, (ii) show that discrete messages nonetheless\nnaturally emerge. We explore whether categorical perception effects follow and\nshow that the messages are not compositional.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:15:19 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lan", "Nur Geffen", ""], ["Chemla", "Emmanuel", ""], ["Steinert-Threlkeld", "Shane", ""]]}, {"id": "2005.00115", "submitter": "Sarthak Jain", "authors": "Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace", "title": "Learning to Faithfully Rationalize by Construction", "comments": "ACL2020 Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings it is important for one to be able to understand why a model\nmade a particular prediction. In NLP this often entails extracting snippets of\nan input text `responsible for' corresponding model output; when such a snippet\ncomprises tokens that indeed informed the model's prediction, it is a faithful\nexplanation. In some settings, faithfulness may be critical to ensure\ntransparency. Lei et al. (2016) proposed a model to produce faithful rationales\nfor neural text classification by defining independent snippet extraction and\nprediction modules. However, the discrete selection over input tokens performed\nby this method complicates training, leading to high variance and requiring\ncareful hyperparameter tuning. We propose a simpler variant of this approach\nthat provides faithful explanations by construction. In our scheme, named\nFRESH, arbitrary feature importance scores (e.g., gradients from a trained\nmodel) are used to induce binary labels over token inputs, which an extractor\ncan be trained to predict. An independent classifier module is then trained\nexclusively on snippets provided by the extractor; these snippets thus\nconstitute faithful explanations, even if the classifier is arbitrarily\ncomplex. In both automatic and manual evaluations we find that variants of this\nsimple framework yield predictive performance superior to `end-to-end'\napproaches, while being more general and easier to train. Code is available at\nhttps://github.com/successar/FRESH\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:45:40 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Jain", "Sarthak", ""], ["Wiegreffe", "Sarah", ""], ["Pinter", "Yuval", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.00119", "submitter": "Raviteja Anantha", "authors": "Raviteja Anantha, Srinivas Chappidi, and William Dawoodi", "title": "Learning to Rank Intents in Voice Assistants", "comments": "11 pages, 7 figures, 2 tables, accepted at IWSDS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice Assistants aim to fulfill user requests by choosing the best intent\nfrom multiple options generated by its Automated Speech Recognition and Natural\nLanguage Understanding sub-systems. However, voice assistants do not always\nproduce the expected results. This can happen because voice assistants choose\nfrom ambiguous intents - user-specific or domain-specific contextual\ninformation reduces the ambiguity of the user request. Additionally the user\ninformation-state can be leveraged to understand how relevant/executable a\nspecific intent is for a user request. In this work, we propose a novel\nEnergy-based model for the intent ranking task, where we learn an affinity\nmetric and model the trade-off between extracted meaning from speech utterances\nand relevance/executability aspects of the intent. Furthermore we present a\nMultisource Denoising Autoencoder based pretraining that is capable of learning\nfused representations of data from multiple sources. We empirically show our\napproach outperforms existing state of the art methods by reducing the\nerror-rate by 3.8%, which in turn reduces ambiguity and eliminates undesired\ndead-ends leading to better user experience. Finally, we evaluate the\nrobustness of our algorithm on the intent ranking task and show our algorithm\nimproves the robustness by 33.3%.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:51:26 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 03:19:07 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Anantha", "Raviteja", ""], ["Chappidi", "Srinivas", ""], ["Dawoodi", "William", ""]]}, {"id": "2005.00123", "submitter": "Dinesh Raghu", "authors": "Dinesh Raghu, Nikhil Gupta, Mausam", "title": "Unsupervised Learning of KB Queries in Task-Oriented Dialogs", "comments": "Presented at ACL 2021", "journal-ref": "Transactions of the Association for Computational Linguistics\n  (2021) 9: 374-390", "doi": "10.1162/tacl_a_00372", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialog (TOD) systems often need to formulate knowledge base\n(KB) queries corresponding to the user intent and use the query results to\ngenerate system responses. Existing approaches require dialog datasets to\nexplicitly annotate these KB queries -- these annotations can be time\nconsuming, and expensive. In response, we define the novel problems of\npredicting the KB query and training the dialog agent, without explicit KB\nquery annotation. For query prediction, we propose a reinforcement learning\n(RL) baseline, which rewards the generation of those queries whose KB results\ncover the entities mentioned in subsequent dialog. Further analysis reveals\nthat correlation among query attributes in KB can significantly confuse memory\naugmented policy optimization (MAPO), an existing state of the art RL agent. To\naddress this, we improve the MAPO baseline with simple but important\nmodifications suited to our task. To train the full TOD system for our setting,\nwe propose a pipelined approach: it independently predicts when to make a KB\nquery (query position predictor), then predicts a KB query at the predicted\nposition (query predictor), and uses the results of predicted query in\nsubsequent dialog (next response predictor). Overall, our work proposes first\nsolutions to our novel problem, and our analysis highlights the research\nchallenges in training TOD systems without query annotation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:10:00 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 04:27:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Raghu", "Dinesh", ""], ["Gupta", "Nikhil", ""], ["Mausam", "", ""]]}, {"id": "2005.00128", "submitter": "Patrick Xia", "authors": "Patrick Xia, Jo\\~ao Sedoc, Benjamin Van Durme", "title": "Incremental Neural Coreference Resolution in Constant Memory", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate modeling coreference resolution under a fixed memory\nconstraint by extending an incremental clustering algorithm to utilize\ncontextualized encoders and neural components. Given a new sentence, our\nend-to-end algorithm proposes and scores each mention span against explicit\nentity representations created from the earlier document context (if any).\nThese spans are then used to update the entity's representations before being\nforgotten; we only retain a fixed set of salient entities throughout the\ndocument. In this work, we successfully convert a high-performing model (Joshi\net al., 2020), asymptotically reducing its memory usage to constant space with\nonly a 0.3% relative loss in F1 on OntoNotes 5.0.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:31:20 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 19:49:31 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Xia", "Patrick", ""], ["Sedoc", "Jo\u00e3o", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2005.00129", "submitter": "Gideon Maillette de Buy Wenniger", "authors": "Gideon Maillette de Buy Wenniger, Thomas van Dongen, Eleri Aedmaa,\n  Herbert Teun Kruitbosch, Edwin A. Valentijn, and Lambert Schomaker", "title": "Structure-Tags Improve Text Classification for Scholarly Document\n  Quality Prediction", "comments": "This new version of the paper brings the paper up-to-date with the\n  improved paper, published at the First Workshop on Scholarly Document\n  Processing, at EMNLP 2020. .Additionally, minor corrections were made\n  including addition of color to Figures 1,2. The changes in comparison to the\n  first arXiv version are substantial, including various additional results,\n  and substantial improvements to the text", "journal-ref": "Proceedings of the First Workshop on Scholarly Document\n  Processing. Association for Computational Linguistics. (2020) 158-167.\n  EMNLP|SDP 2020 https://www.aclweb.org/anthology/2020.sdp-1.18", "doi": "10.18653/v1/2020.sdp-1.18", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training recurrent neural networks on long texts, in particular scholarly\ndocuments, causes problems for learning. While hierarchical attention networks\n(HANs) are effective in solving these problems, they still lose important\ninformation about the structure of the text. To tackle these problems, we\npropose the use of HANs combined with structure-tags which mark the role of\nsentences in the document. Adding tags to sentences, marking them as\ncorresponding to title, abstract or main body text, yields improvements over\nthe state-of-the-art for scholarly document quality prediction. The proposed\nsystem is applied to the task of accept/reject prediction on the PeerRead\ndataset and compared against a recent BiLSTM-based model and joint\ntextual+visual model as well as against plain HANs. Compared to plain HANs,\naccuracy increases on all three domains. On the computation and language domain\nour new model works best overall, and increases accuracy 4.7% over the best\nliterature result. We also obtain improvements when introducing the tags for\nprediction of the number of citations for 88k scientific publications that we\ncompiled from the Allen AI S2ORC dataset. For our HAN-system with\nstructure-tags we reach 28.5% explained variance, an improvement of 1.8% over\nour reimplementation of the BiLSTM-based model as well as 1.0% improvement over\nplain HANs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:34:34 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 20:35:14 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Wenniger", "Gideon Maillette de Buy", ""], ["van Dongen", "Thomas", ""], ["Aedmaa", "Eleri", ""], ["Kruitbosch", "Herbert Teun", ""], ["Valentijn", "Edwin A.", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2005.00136", "submitter": "Yu Cheng", "authors": "Yu Cheng, Zhe Gan, Yizhe Zhang, Oussama Elachqar, Dianqi Li, Jingjing\n  Liu", "title": "Contextual Text Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, Contextual Text Style Transfer - translating a\nsentence into a desired style with its surrounding context taken into account.\nThis brings two key challenges to existing style transfer approaches: ($i$) how\nto preserve the semantic meaning of target sentence and its consistency with\nsurrounding context during transfer; ($ii$) how to train a robust model with\nlimited labeled data accompanied with context. To realize high-quality style\ntransfer with natural context preservation, we propose a Context-Aware Style\nTransfer (CAST) model, which uses two separate encoders for each input sentence\nand its surrounding context. A classifier is further trained to ensure\ncontextual consistency of the generated sentence. To compensate for the lack of\nparallel data, additional self-reconstruction and back-translation losses are\nintroduced to leverage non-parallel data in a semi-supervised fashion. Two new\nbenchmarks, Enron-Context and Reddit-Context, are introduced for formality and\noffensiveness style transfer. Experimental results on these datasets\ndemonstrate the effectiveness of the proposed CAST model over state-of-the-art\nmethods across style accuracy, content preservation and contextual consistency\nmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 23:01:12 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Zhang", "Yizhe", ""], ["Elachqar", "Oussama", ""], ["Li", "Dianqi", ""], ["Liu", "Jingjing", ""]]}, {"id": "2005.00147", "submitter": "Yasumasa Onoe", "authors": "Yasumasa Onoe and Greg Durrett", "title": "Interpretable Entity Representations through Large-Scale Typing", "comments": "Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard methodology for natural language processing, entities in text are\ntypically embedded in dense vector spaces with pre-trained models. The\nembeddings produced this way are effective when fed into downstream models, but\nthey require end-task fine-tuning and are fundamentally difficult to interpret.\nIn this paper, we present an approach to creating entity representations that\nare human readable and achieve high performance on entity-related tasks out of\nthe box. Our representations are vectors whose values correspond to posterior\nprobabilities over fine-grained entity types, indicating the confidence of a\ntyping model's decision that the entity belongs to the corresponding type. We\nobtain these representations using a fine-grained entity typing model, trained\neither on supervised ultra-fine entity typing data (Choi et al. 2018) or\ndistantly-supervised examples from Wikipedia. On entity probing tasks involving\nrecognizing entity identity, our embeddings used in parameter-free downstream\nmodels achieve competitive performance with ELMo- and BERT-based embeddings in\ntrained models. We also show that it is possible to reduce the size of our type\nset in a learning-based way for particular domains. Finally, we show that these\nembeddings can be post-hoc modified through a small number of rules to\nincorporate domain knowledge and improve performance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 23:58:03 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 01:18:13 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Onoe", "Yasumasa", ""], ["Durrett", "Greg", ""]]}, {"id": "2005.00152", "submitter": "Gong Cheng", "authors": "Junyou Li, Gong Cheng, Qingxia Liu, Wen Zhang, Evgeny Kharlamov, Kalpa\n  Gunaratna, Huajun Chen", "title": "Neural Entity Summarization with Joint Encoding and Weak Supervision", "comments": "7 pages, accepted to IJCAI-PRICAI 2020 The paper is temporarily\n  withdrawn due to company policies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a large-scale knowledge graph (KG), an entity is often described by a\nlarge number of triple-structured facts. Many applications require abridged\nversions of entity descriptions, called entity summaries. Existing solutions to\nentity summarization are mainly unsupervised. In this paper, we present a\nsupervised approach NEST that is based on our novel neural model to jointly\nencode graph structure and text in KGs and generate high-quality diversified\nsummaries. Since it is costly to obtain manually labeled summaries for\ntraining, our supervision is weak as we train with programmatically labeled\ndata which may contain noise but is free of manual work. Evaluation results\nshow that our approach significantly outperforms the state of the art on two\npublic benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 00:14:08 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 08:30:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Junyou", ""], ["Cheng", "Gong", ""], ["Liu", "Qingxia", ""], ["Zhang", "Wen", ""], ["Kharlamov", "Evgeny", ""], ["Gunaratna", "Kalpa", ""], ["Chen", "Huajun", ""]]}, {"id": "2005.00158", "submitter": "Mohammed Belkhatir", "authors": "M. Maree, M. Belkhatir", "title": "On the Merging of Domain-Specific Heterogeneous Ontologies using Wordnet\n  and Web Pattern-based Queries", "comments": null, "journal-ref": null, "doi": "10.1142/S0219649211002808", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies form the basic interest in various computer science disciplines\nsuch as semantic web, information retrieval, database design, etc. They aim at\nproviding a formal, explicit and shared conceptualization and understanding of\ncommon domains between different communities. In addition, they allow for\nconcepts and their constraints of a specific domain to be explicitly defined.\nHowever, the distributed nature of ontology development and the differences in\nviewpoints of the ontology engineers have resulted in the so called \"semantic\nheterogeneity\" between ontologies. Semantic heterogeneity constitutes the major\nobstacle against achieving interoperability between ontologies. To overcome\nthis obstacle, we present a multi-purpose framework which exploits the WordNet\ngeneric knowledge base for: i) Discovering and correcting the incorrect\nsemantic relations between the concepts of the ontology in a specific domain.\nThis step is a primary step of ontology merging. ii) Merging domain-specific\nontologies through computing semantic relations between their concepts. iii)\nHandling the issue of missing concepts in WordNet through the acquisition of\nstatistical information on the Web. And iv) Enriching WordNet with these\nmissing concepts. An experimental instantiation of the framework and\ncomparisons with state-of-the-art syntactic and semantic-based systems validate\nour proposal.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:03:50 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Maree", "M.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.00159", "submitter": "Pratyush Maini", "authors": "Pratyush Maini, Keshav Kolluru, Danish Pruthi, Mausam", "title": "Why and when should you pool? Analyzing Pooling in Recurrent\n  Architectures", "comments": "Accepted to Findings of EMNLP 2020, to be presented at BlackBoxNLP.\n  Updated Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling-based recurrent neural architectures consistently outperform their\ncounterparts without pooling. However, the reasons for their enhanced\nperformance are largely unexamined. In this work, we examine three commonly\nused pooling techniques (mean-pooling, max-pooling, and attention), and propose\nmax-attention, a novel variant that effectively captures interactions among\npredictive tokens in a sentence. We find that pooling-based architectures\nsubstantially differ from their non-pooling equivalents in their learning\nability and positional biases--which elucidate their performance benefits. By\nanalyzing the gradient propagation, we discover that pooling facilitates better\ngradient flow compared to BiLSTMs. Further, we expose how BiLSTMs are\npositionally biased towards tokens in the beginning and the end of a sequence.\nPooling alleviates such biases. Consequently, we identify settings where\npooling offers large benefits: (i) in low resource scenarios, and (ii) when\nimportant words lie towards the middle of the sentence. Among the pooling\ntechniques studied, max-attention is the most effective, resulting in\nsignificant performance gains on several text classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 00:47:37 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 02:11:02 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Maini", "Pratyush", ""], ["Kolluru", "Keshav", ""], ["Pruthi", "Danish", ""], ["Mausam", "", ""]]}, {"id": "2005.00162", "submitter": "Samuel Mensah", "authors": "Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, Xudong Liu", "title": "Recurrent Interaction Network for Jointly Extracting Entities and\n  Classifying Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of using multi-task learning approaches to address the joint\nextraction of entity and relation is motivated by the relatedness between the\nentity recognition task and the relation classification task. Existing methods\nusing multi-task learning techniques to address the problem learn interactions\namong the two tasks through a shared network, where the shared information is\npassed into the task-specific networks for prediction. However, such an\napproach hinders the model from learning explicit interactions between the two\ntasks to improve the performance on the individual tasks. As a solution, we\ndesign a multi-task learning model which we refer to as recurrent interaction\nnetwork which allows the learning of interactions dynamically, to effectively\nmodel task-specific features for classification. Empirical studies on two\nreal-world datasets confirm the superiority of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:03:16 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 02:54:49 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sun", "Kai", ""], ["Zhang", "Richong", ""], ["Mensah", "Samuel", ""], ["Mao", "Yongyi", ""], ["Liu", "Xudong", ""]]}, {"id": "2005.00163", "submitter": "Sajad Sotudeh", "authors": "Sajad Sotudeh and Nazli Goharian and Ross W. Filice", "title": "Attend to Medical Ontologies: Content Selection for Clinical Abstractive\n  Summarization", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence (seq2seq) network is a well-established model for text\nsummarization task. It can learn to produce readable content; however, it falls\nshort in effectively identifying key regions of the source. In this paper, we\napproach the content selection problem for clinical abstractive summarization\nby augmenting salient ontological terms into the summarizer. Our experiments on\ntwo publicly available clinical data sets (107,372 reports of MIMIC-CXR, and\n3,366 reports of OpenI) show that our model statistically significantly boosts\nstate-of-the-art results in terms of Rouge metrics (with improvements: 2.9%\nRG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of\nimprovement impacts patients' welfare.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:12:49 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Sotudeh", "Sajad", ""], ["Goharian", "Nazli", ""], ["Filice", "Ross W.", ""]]}, {"id": "2005.00165", "submitter": "Forrest Davis", "authors": "Forrest Davis and Marten van Schijndel", "title": "Recurrent Neural Network Language Models Always Learn English-Like\n  Relative Clause Attachment", "comments": "Proceedings of 58th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2020); v3 updated references and added\n  additional corpus stats in discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach to evaluating language models analyzes how models assign\nprobabilities to valid versus invalid syntactic constructions (i.e. is a\ngrammatical sentence more probable than an ungrammatical sentence). Our work\nuses ambiguous relative clause attachment to extend such evaluations to cases\nof multiple simultaneous valid interpretations, where stark grammaticality\ndifferences are absent. We compare model performance in English and Spanish to\nshow that non-linguistic biases in RNN LMs advantageously overlap with\nsyntactic structure in English but not Spanish. Thus, English models may appear\nto acquire human-like syntactic preferences, while models trained on Spanish\nfail to acquire comparable human-like preferences. We conclude by relating\nthese results to broader concerns about the relationship between comprehension\n(i.e. typical language model use cases) and production (which generates the\ntraining data for language models), suggesting that necessary linguistic biases\nare not present in the training signal at all.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:21:47 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:13:23 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 15:21:58 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Davis", "Forrest", ""], ["van Schijndel", "Marten", ""]]}, {"id": "2005.00171", "submitter": "Muhao Chen", "authors": "Muhao Chen, Weijia Shi, Ben Zhou, Dan Roth", "title": "Cross-lingual Entity Alignment with Incidental Supervision", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research effort has been put to multilingual knowledge graph (KG)\nembedding methods to address the entity alignment task, which seeks to match\nentities in different languagespecific KGs that refer to the same real-world\nobject. Such methods are often hindered by the insufficiency of seed alignment\nprovided between KGs. Therefore, we propose an incidentally supervised model,\nJEANS , which jointly represents multilingual KGs and text corpora in a shared\nembedding scheme, and seeks to improve entity alignment with incidental\nsupervision signals from text. JEANS first deploys an entity grounding process\nto combine each KG with the monolingual text corpus. Then, two learning\nprocesses are conducted: (i) an embedding learning process to encode the KG and\ntext of each language in one embedding space, and (ii) a selflearning based\nalignment learning process to iteratively induce the matching of entities and\nthat of lexemes between embeddings. Experiments on benchmark datasets show that\nJEANS leads to promising improvement on entity alignment with incidental\nsupervision, and significantly outperforms state-of-the-art methods that solely\nrely on internal information of KGs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:53:56 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 05:15:45 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chen", "Muhao", ""], ["Shi", "Weijia", ""], ["Zhou", "Ben", ""], ["Roth", "Dan", ""]]}, {"id": "2005.00172", "submitter": "Pedro Rodriguez", "authors": "Pedro Rodriguez, Paul Crook, Seungwhan Moon, Zhiguang Wang", "title": "Information Seeking in the Spirit of Learning: a Dataset for\n  Conversational Curiosity", "comments": "EMNLP 2020: https://www.aclweb.org/anthology/2020.emnlp-main.655/", "journal-ref": null, "doi": "10.18653/v1/2020.emnlp-main.655", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-ended human learning and information-seeking are increasingly mediated\nby digital assistants. However, such systems often ignore the user's\npre-existing knowledge. Assuming a correlation between engagement and user\nresponses such as \"liking\" messages or asking followup questions, we design a\nWizard-of-Oz dialog task that tests the hypothesis that engagement increases\nwhen users are presented with facts related to what they know. Through\ncrowd-sourcing of this experiment, we collect and release 14K dialogs (181K\nutterances) where users and assistants converse about geographic topics like\ngeopolitical entities and locations. This dataset is annotated with\npre-existing user knowledge, message-level dialog acts, grounding to Wikipedia,\nand user reactions to messages. Responses using a user's prior knowledge\nincrease engagement. We incorporate this knowledge into a multi-task model that\nreproduces human assistant policies and improves over a BERT content model by\n13 mean reciprocal rank points.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:55:09 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:09:50 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Rodriguez", "Pedro", ""], ["Crook", "Paul", ""], ["Moon", "Seungwhan", ""], ["Wang", "Zhiguang", ""]]}, {"id": "2005.00174", "submitter": "Liwei Song", "authors": "Liwei Song, Xinwei Yu, Hsuan-Tung Peng, Karthik Narasimhan", "title": "Universal Adversarial Attacks with Natural Triggers for Text\n  Classification", "comments": "Accepted by NAACL 2021, code is available at\n  https://github.com/Hsuan-Tung/universal_attack_natural_trigger", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated the vulnerability of modern text classifiers to\nuniversal adversarial attacks, which are input-agnostic sequences of words\nadded to text processed by classifiers. Despite being successful, the word\nsequences produced in such attacks are often ungrammatical and can be easily\ndistinguished from natural text. We develop adversarial attacks that appear\ncloser to natural English phrases and yet confuse classification systems when\nadded to benign inputs. We leverage an adversarially regularized autoencoder\n(ARAE) to generate triggers and propose a gradient-based search that aims to\nmaximize the downstream classifier's prediction loss. Our attacks effectively\nreduce model accuracy on classification tasks while being less identifiable\nthan prior models as per automatic detection metrics and human-subject studies.\nOur aim is to demonstrate that adversarial attacks can be made harder to detect\nthan previously thought and to enable the development of appropriate defenses.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:58:24 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:03:23 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Song", "Liwei", ""], ["Yu", "Xinwei", ""], ["Peng", "Hsuan-Tung", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2005.00175", "submitter": "Richard Antonello", "authors": "Richard Antonello, Nicole Beckage, Javier Turek, and Alexander Huth", "title": "Selecting Informative Contexts Improves Language Model Finetuning", "comments": "Accepted submission at the Joint Conference of the 59th Annual\n  Meeting of the Association for Computational Linguistics and the 11th\n  International Joint Conference on Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language model fine-tuning is essential for modern natural language\nprocessing, but is computationally expensive and time-consuming. Further, the\neffectiveness of fine-tuning is limited by the inclusion of training examples\nthat negatively affect performance. Here we present a general fine-tuning\nmethod that we call information gain filtration for improving the overall\ntraining efficiency and final performance of language model fine-tuning. We\ndefine the information gain of an example as the improvement on a test metric\nafter training on that example. A secondary learner is then trained to\napproximate this quantity. During fine-tuning, this learner selects informative\nexamples and skips uninformative ones. We show that our method has consistent\nimprovement across datasets, fine-tuning tasks, and language model\narchitectures. For example, we achieve a median perplexity of 54.0 on a books\ndataset compared to 57.3 for standard fine-tuning. We present statistical\nevidence that offers insight into the improvements of our method over standard\nfine-tuning. The generality of our method leads us to propose a new paradigm\nfor language model fine-tuning -- we encourage researchers to release\npretrained secondary learners on common corpora to promote efficient and\neffective fine-tuning, thereby improving the performance and reducing the\noverall energy footprint of language model fine-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 02:01:18 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 20:22:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Antonello", "Richard", ""], ["Beckage", "Nicole", ""], ["Turek", "Javier", ""], ["Huth", "Alexander", ""]]}, {"id": "2005.00181", "submitter": "Yi Luan", "authors": "Yi Luan, Jacob Eisenstein, Kristina Toutanova, Michael Collins", "title": "Sparse, Dense, and Attentional Representations for Text Retrieval", "comments": "To appear in TACL 2020. The arXiv version is a pre-MIT Press\n  publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual encoders perform retrieval by encoding documents and queries into dense\nlowdimensional vectors, scoring each document by its inner product with the\nquery. We investigate the capacity of this architecture relative to sparse\nbag-of-words models and attentional neural networks. Using both theoretical and\nempirical analysis, we establish connections between the encoding dimension,\nthe margin between gold and lower-ranked documents, and the document length,\nsuggesting limitations in the capacity of fixed-length encodings to support\nprecise retrieval of long documents. Building on these insights, we propose a\nsimple neural model that combines the efficiency of dual encoders with some of\nthe expressiveness of more costly attentional architectures, and explore\nsparse-dense hybrids to capitalize on the precision of sparse retrieval. These\nmodels outperform strong alternatives in large-scale retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 02:21:17 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 19:12:42 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 23:18:25 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Luan", "Yi", ""], ["Eisenstein", "Jacob", ""], ["Toutanova", "Kristina", ""], ["Collins", "Michael", ""]]}, {"id": "2005.00187", "submitter": "Aaron Mueller", "authors": "Aaron Mueller, Garrett Nicolai, Panayiota Petrou-Zeniou, Natalia\n  Talmina, Tal Linzen", "title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models", "comments": "Accepted for presentation at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A range of studies have concluded that neural word prediction models can\ndistinguish grammatical from ungrammatical sentences with high accuracy.\nHowever, these studies are based primarily on monolingual evidence from\nEnglish. To investigate how these models' ability to learn syntax varies by\nlanguage, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax),\na syntactic evaluation suite for monolingual and multilingual models. CLAMS\nincludes subject-verb agreement challenge sets for English, French, German,\nHebrew and Russian, generated from grammars we develop. We use CLAMS to\nevaluate LSTM language models as well as monolingual and multilingual BERT.\nAcross languages, monolingual LSTMs achieved high accuracy on dependencies\nwithout attractors, and generally poor accuracy on agreement across object\nrelative clauses. On other constructions, agreement accuracy was generally\nhigher in languages with richer morphology. Multilingual models generally\nunderperformed monolingual models. Multilingual BERT showed high syntactic\naccuracy on English, but noticeable deficiencies in other languages.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 02:51:20 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 14:19:52 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Mueller", "Aaron", ""], ["Nicolai", "Garrett", ""], ["Petrou-Zeniou", "Panayiota", ""], ["Talmina", "Natalia", ""], ["Linzen", "Tal", ""]]}, {"id": "2005.00190", "submitter": "Winston Wu", "authors": "Winston Wu, Dustin Arendt, Svitlana Volkova", "title": "Evaluating Neural Machine Comprehension Model Robustness to Noisy Inputs\n  and Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate machine comprehension models' robustness to noise and adversarial\nattacks by performing novel perturbations at the character, word, and sentence\nlevel. We experiment with different amounts of perturbations to examine model\nconfidence and misclassification rate, and contrast model performance in\nadversarial training with different embedding types on two benchmark datasets.\nWe demonstrate improving model performance with ensembling. Finally, we analyze\nfactors that effect model behavior under adversarial training and develop a\nmodel to predict model errors during adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:05:43 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Wu", "Winston", ""], ["Arendt", "Dustin", ""], ["Volkova", "Svitlana", ""]]}, {"id": "2005.00192", "submitter": "Hwanhee Lee", "authors": "Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung\n  Bui, Joongbo Shin and Kyomin Jung", "title": "KPQA: A Metric for Generative Question Answering Using Keyphrase Weights", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the automatic evaluation of generative question answering (GenQA) systems,\nit is difficult to assess the correctness of generated answers due to the\nfree-form of the answer. Especially, widely used n-gram similarity metrics\noften fail to discriminate the incorrect answers since they equally consider\nall of the tokens. To alleviate this problem, we propose KPQA-metric, a new\nmetric for evaluating the correctness of GenQA. Specifically, our new metric\nassigns different weights to each token via keyphrase prediction, thereby\njudging whether a generated answer sentence captures the key meaning of the\nreference answer. To evaluate our metric, we create high-quality human\njudgments of correctness on two GenQA datasets. Using our human-evaluation\ndatasets, we show that our proposed metric has a significantly higher\ncorrelation with human judgments than existing metrics. The code is available\nat https://github.com/hwanheelee1993/KPQA.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:24:36 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 09:28:59 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 10:09:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lee", "Hwanhee", ""], ["Yoon", "Seunghyun", ""], ["Dernoncourt", "Franck", ""], ["Kim", "Doo Soon", ""], ["Bui", "Trung", ""], ["Shin", "Joongbo", ""], ["Jung", "Kyomin", ""]]}, {"id": "2005.00200", "submitter": "Zhe Gan", "authors": "Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HERO, a novel framework for large-scale video+language\nomni-representation learning. HERO encodes multimodal inputs in a hierarchical\nstructure, where local context of a video frame is captured by a Cross-modal\nTransformer via multimodal fusion, and global video context is captured by a\nTemporal Transformer. In addition to standard Masked Language Modeling (MLM)\nand Masked Frame Modeling (MFM) objectives, we design two new pre-training\ntasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global\nand local temporal alignment; and (ii) Frame Order Modeling (FOM), where the\nmodel predicts the right order of shuffled video frames. HERO is jointly\ntrained on HowTo100M and large-scale TV datasets to gain deep understanding of\ncomplex social dynamics with multi-character interactions. Comprehensive\nexperiments demonstrate that HERO achieves new state of the art on multiple\nbenchmarks over Text-based Video/Video-moment Retrieval, Video Question\nAnswering (QA), Video-and-language Inference and Video Captioning tasks across\ndifferent domains. We also introduce two new challenging benchmarks How2QA and\nHow2R for Video QA and Retrieval, collected from diverse video content over\nmultimodalities.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:49:26 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 20:37:17 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Li", "Linjie", ""], ["Chen", "Yen-Chun", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Yu", "Licheng", ""], ["Liu", "Jingjing", ""]]}, {"id": "2005.00205", "submitter": "Baiji Liu", "authors": "Baiji Liu and Songjun Cao and Sining Sun and Weibin Zhang and Long Ma", "title": "Multi-head Monotonic Chunkwise Attention For Online Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention mechanism of the Listen, Attend and Spell (LAS) model requires\nthe whole input sequence to calculate the attention context and thus is not\nsuitable for online speech recognition. To deal with this problem, we propose\nmulti-head monotonic chunk-wise attention (MTH-MoChA), an improved version of\nMoChA. MTH-MoChA splits the input sequence into small chunks and computes\nmulti-head attentions over the chunks. We also explore useful training\nstrategies such as LSTM pooling, minimum world error rate training and\nSpecAugment to further improve the performance of MTH-MoChA. Experiments on\nAISHELL-1 data show that the proposed model, along with the training\nstrategies, improve the character error rate (CER) of MoChA from 8.96% to 7.68%\non test set. On another 18000 hours in-car speech data set, MTH-MoChA obtains\n7.28% CER, which is significantly better than a state-of-the-art hybrid system.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 04:00:51 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Liu", "Baiji", ""], ["Cao", "Songjun", ""], ["Sun", "Sining", ""], ["Zhang", "Weibin", ""], ["Ma", "Long", ""]]}, {"id": "2005.00206", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Daniel Khashabi, Yangqiu Song, Dan Roth", "title": "TransOMCS: From Linguistic Graphs to Commonsense Knowledge", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge acquisition is a key problem for artificial\nintelligence. Conventional methods of acquiring commonsense knowledge generally\nrequire laborious and costly human annotations, which are not feasible on a\nlarge scale. In this paper, we explore a practical way of mining commonsense\nknowledge from linguistic graphs, with the goal of transferring cheap knowledge\nobtained with linguistic patterns into expensive commonsense knowledge. The\nresult is a conversion of ASER [Zhang et al., 2020], a large-scale selectional\npreference knowledge resource, into TransOMCS, of the same representation as\nConceptNet [Liu and Singh, 2004] but two orders of magnitude larger.\nExperimental results demonstrate the transferability of linguistic knowledge to\ncommonsense knowledge and the effectiveness of the proposed approach in terms\nof quantity, novelty, and quality. TransOMCS is publicly available at:\nhttps://github.com/HKUST-KnowComp/TransOMCS.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 04:03:58 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zhang", "Hongming", ""], ["Khashabi", "Daniel", ""], ["Song", "Yangqiu", ""], ["Roth", "Dan", ""]]}, {"id": "2005.00239", "submitter": "Mujeen Sung", "authors": "Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, Jaewoo Kang", "title": "Biomedical Entity Representations with Synonym Marginalization", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical named entities often play important roles in many biomedical text\nmining tools. However, due to the incompleteness of provided synonyms and\nnumerous variations in their surface forms, normalization of biomedical\nentities is very challenging. In this paper, we focus on learning\nrepresentations of biomedical entities solely based on the synonyms of\nentities. To learn from the incomplete synonyms, we use a model-based candidate\nselection and maximize the marginal likelihood of the synonyms present in top\ncandidates. Our model-based candidates are iteratively updated to contain more\ndifficult negative samples as our model evolves. In this way, we avoid the\nexplicit pre-selection of negative samples from more than 400K candidates. On\nfour biomedical entity normalization datasets having three different entity\ntypes (disease, chemical, adverse reaction), our model BioSyn consistently\noutperforms previous state-of-the-art models almost reaching the upper bound on\neach dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 06:20:36 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Sung", "Mujeen", ""], ["Jeon", "Hwisang", ""], ["Lee", "Jinhyuk", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2005.00242", "submitter": "Qiang Ning", "authors": "Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, Dan Roth", "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions", "comments": "15 pages (incl. 4 pages in the appendix); accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical part of reading is being able to understand the temporal\nrelationships between events described in a passage of text, even when those\nrelationships are not explicitly stated. However, current machine reading\ncomprehension benchmarks have practically no questions that test temporal\nphenomena, so systems trained on these benchmarks have no capacity to answer\nquestions such as \"what happened before/after [some event]?\" We introduce\nTORQUE, a new English reading comprehension benchmark built on 3.2k news\nsnippets with 21k human-generated questions querying temporal relationships.\nResults show that RoBERTa-large achieves an exact-match score of 51% on the\ntest set of TORQUE, about 30% behind human performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 06:29:56 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 03:57:19 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ning", "Qiang", ""], ["Wu", "Hao", ""], ["Han", "Rujun", ""], ["Peng", "Nanyun", ""], ["Gardner", "Matt", ""], ["Roth", "Dan", ""]]}, {"id": "2005.00246", "submitter": "Ashish V. Thapliyal", "authors": "Ashish V. Thapliyal and Radu Soricut", "title": "Cross-modal Language Generation using Pivot Stabilization for Web-scale\n  Language Coverage", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal language generation tasks such as image captioning are directly\nhurt in their ability to support non-English languages by the trend of\ndata-hungry models combined with the lack of non-English annotations. We\ninvestigate potential solutions for combining existing language-generation\nannotations in English with translation capabilities in order to create\nsolutions at web-scale in both domain and language coverage. We describe an\napproach called Pivot-Language Generation Stabilization (PLuGS), which\nleverages directly at training time both existing English annotations (gold\ndata) as well as their machine-translated versions (silver data); at run-time,\nit generates first an English caption and then a corresponding target-language\ncaption. We show that PLuGS models outperform other candidate solutions in\nevaluations performed over 5 different target languages, under a large-domain\ntestset using images from the Open Images dataset. Furthermore, we find an\ninteresting effect where the English captions generated by the PLuGS models are\nbetter than the captions generated by the original, monolingual English model.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 06:58:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Thapliyal", "Ashish V.", ""], ["Soricut", "Radu", ""]]}, {"id": "2005.00247", "submitter": "Jonas Pfeiffer", "authors": "Jonas Pfeiffer, Aishwarya Kamath, Andreas R\\\"uckl\\'e, Kyunghyun Cho,\n  Iryna Gurevych", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning", "comments": null, "journal-ref": "Proceedings of EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential fine-tuning and multi-task learning are methods aiming to\nincorporate knowledge from multiple tasks; however, they suffer from\ncatastrophic forgetting and difficulties in dataset balancing. To address these\nshortcomings, we propose AdapterFusion, a new two stage learning algorithm that\nleverages knowledge from multiple tasks. First, in the knowledge extraction\nstage we learn task specific parameters called adapters, that encapsulate the\ntask-specific information. We then combine the adapters in a separate knowledge\ncomposition step. We show that by separating the two stages, i.e., knowledge\nextraction and knowledge composition, the classifier can effectively exploit\nthe representations learned from multiple tasks in a non-destructive manner. We\nempirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it\neffectively combines various types of knowledge at different layers of the\nmodel. We show that our approach outperforms traditional strategies such as\nfull fine-tuning as well as multi-task learning. Our code and adapters are\navailable at AdapterHub.ml.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 07:03:42 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 14:34:32 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 12:54:33 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Pfeiffer", "Jonas", ""], ["Kamath", "Aishwarya", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Cho", "Kyunghyun", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2005.00250", "submitter": "Jonas Pfeiffer", "authors": "Jonas Pfeiffer, Edwin Simpson, Iryna Gurevych", "title": "Low Resource Multi-Task Sequence Tagging -- Revisiting Dynamic\n  Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare different models for low resource multi-task sequence tagging that\nleverage dependencies between label sequences for different tasks. Our analysis\nis aimed at datasets where each example has labels for multiple tasks. Current\napproaches use either a separate model for each task or standard multi-task\nlearning to learn shared feature representations. However, these approaches\nignore correlations between label sequences, which can provide important\ninformation in settings with small training datasets. To analyze which\nscenarios can profit from modeling dependencies between labels in different\ntasks, we revisit dynamic conditional random fields (CRFs) and combine them\nwith deep neural networks. We compare single-task, multi-task and dynamic CRF\nsetups for three diverse datasets at both sentence and document levels in\nEnglish and German low resource scenarios. We show that including silver labels\nfrom pretrained part-of-speech taggers as auxiliary tasks can improve\nperformance on downstream tasks. We find that especially in low-resource\nscenarios, the explicit modeling of inter-dependencies between task predictions\noutperforms single-task as well as standard multi-task models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 07:11:34 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Pfeiffer", "Jonas", ""], ["Simpson", "Edwin", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2005.00268", "submitter": "Emily Sheng", "authors": "Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng", "title": "Towards Controllable Biases in Language Generation", "comments": "16 pages, Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach towards controllable societal biases in natural\nlanguage generation (NLG). Building upon the idea of adversarial triggers, we\ndevelop a method to induce societal biases in generated text when input prompts\ncontain mentions of specific demographic groups. We then analyze two scenarios:\n1) inducing negative biases for one demographic and positive biases for another\ndemographic, and 2) equalizing biases between demographics. The former scenario\nenables us to detect the types of biases present in the model. Specifically, we\nshow the effectiveness of our approach at facilitating bias analysis by finding\ntopics that correspond to demographic inequalities in generated text and\ncomparing the relative effectiveness of inducing biases for different\ndemographics. The second scenario is useful for mitigating biases in downstream\napplications such as dialogue generation. In our experiments, the mitigation\ntechnique proves to be effective at equalizing the amount of biases across\ndemographics while simultaneously generating less negatively biased text\noverall.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 08:25:11 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 05:17:16 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Sheng", "Emily", ""], ["Chang", "Kai-Wei", ""], ["Natarajan", "Premkumar", ""], ["Peng", "Nanyun", ""]]}, {"id": "2005.00278", "submitter": "Yanpeng Zhao", "authors": "Yanpeng Zhao and Ivan Titov", "title": "Unsupervised Transfer of Semantic Role Models from Verbal to Nominal\n  Domain", "comments": "Our code is available at https://github.com/zhaoyanpeng/srltransfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic role labeling (SRL) is an NLP task involving the assignment of\npredicate arguments to types, called semantic roles. Though research on SRL has\nprimarily focused on verbal predicates and many resources available for SRL\nprovide annotations only for verbs, semantic relations are often triggered by\nother linguistic constructions, e.g., nominalizations. In this work, we\ninvestigate a transfer scenario where we assume role-annotated data for the\nsource verbal domain but only unlabeled data for the target nominal domain. Our\nkey assumption, enabling the transfer between the two domains, is that\nselectional preferences of a role (i.e., preferences or constraints on the\nadmissible arguments) do not strongly depend on whether the relation is\ntriggered by a verb or a noun. For example, the same set of arguments can fill\nthe Acquirer role for the verbal predicate `acquire' and its nominal form\n`acquisition'. We approach the transfer task from the variational autoencoding\nperspective. The labeler serves as an encoder (predicting role labels given a\nsentence), whereas selectional preferences are captured in the decoder\ncomponent (generating arguments for the predicting roles). Nominal roles are\nnot labeled in the training data, and the learning objective instead pushes the\nlabeler to assign roles predictive of the arguments. Sharing the decoder\nparameters across the domains encourages consistency between labels predicted\nfor both domains and facilitates the transfer. The method substantially\noutperforms baselines, such as unsupervised and `direct transfer' methods, on\nthe English CoNLL-2009 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:20:48 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 12:56:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhao", "Yanpeng", ""], ["Titov", "Ivan", ""]]}, {"id": "2005.00283", "submitter": "Alberto Poncelas", "authors": "Andy Way, Rejwanul Haque, Guodong Xie, Federico Gaspari, Maja Popovic,\n  Alberto Poncelas", "title": "Facilitating Access to Multilingual COVID-19 Information via Neural\n  Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every day, more people are becoming infected and dying from exposure to\nCOVID-19. Some countries in Europe like Spain, France, the UK and Italy have\nsuffered particularly badly from the virus. Others such as Germany appear to\nhave coped extremely well. Both health professionals and the general public are\nkeen to receive up-to-date information on the effects of the virus, as well as\ntreatments that have proven to be effective. In cases where language is a\nbarrier to access of pertinent information, machine translation (MT) may help\npeople assimilate information published in different languages. Our MT systems\ntrained on COVID-19 data are freely available for anyone to use to help\ntranslate information published in German, French, Italian, Spanish into\nEnglish, as well as the reverse direction.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:31:38 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Way", "Andy", ""], ["Haque", "Rejwanul", ""], ["Xie", "Guodong", ""], ["Gaspari", "Federico", ""], ["Popovic", "Maja", ""], ["Poncelas", "Alberto", ""]]}, {"id": "2005.00295", "submitter": "Manikandan Ravikiran", "authors": "Manikandan Ravikiran, Amin Ekant Muljibhai, Toshinori Miyoshi, Hiroaki\n  Ozaki, Yuta Koreeda and Sakata Masayuki", "title": "Hitachi at SemEval-2020 Task 12: Offensive Language Identification with\n  Noisy Labels using Statistical Sampling and Post-Processing", "comments": "preprint v1, Under submission for SemEval 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our participation in SemEval-2020 Task-12 Subtask-A\n(English Language) which focuses on offensive language identification from\nnoisy labels. To this end, we developed a hybrid system with the BERT\nclassifier trained with tweets selected using Statistical Sampling Algorithm\n(SA) and Post-Processed (PP) using an offensive wordlist. Our developed system\nachieved 34 th position with Macro-averaged F1-score (Macro-F1) of 0.90913 over\nboth offensive and non-offensive classes. We further show comprehensive results\nand error analysis to assist future research in offensive language\nidentification with noisy labels.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 10:16:40 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Ravikiran", "Manikandan", ""], ["Muljibhai", "Amin Ekant", ""], ["Miyoshi", "Toshinori", ""], ["Ozaki", "Hiroaki", ""], ["Koreeda", "Yuta", ""], ["Masayuki", "Sakata", ""]]}, {"id": "2005.00308", "submitter": "Alberto Poncelas", "authors": "Xabier Soto, Dimitar Shterionov, Alberto Poncelas, Andy Way", "title": "Selecting Backtranslated Data from Multiple Sources for Improved Neural\n  Machine Translation", "comments": null, "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics, ACL (2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) has benefited from using synthetic training data\noriginating from translating monolingual corpora, a technique known as\nbacktranslation. Combining backtranslated data from different sources has led\nto better results than when using such data in isolation. In this work we\nanalyse the impact that data translated with rule-based, phrase-based\nstatistical and neural MT systems has on new MT systems. We use a real-world\nlow-resource use-case (Basque-to-Spanish in the clinical domain) as well as a\nhigh-resource language pair (German-to-English) to test different scenarios\nwith backtranslation and employ data selection to optimise the synthetic\ncorpora. We exploit different data selection strategies in order to reduce the\namount of data used, while at the same time maintaining high-quality MT\nsystems. We further tune the data selection method by taking into account the\nquality of the MT systems used for backtranslation and lexical diversity of the\nresulting corpora. Our experiments show that incorporating backtranslated data\nfrom different sources can be beneficial, and that availing of data selection\ncan yield improved performance.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 10:50:53 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Soto", "Xabier", ""], ["Shterionov", "Dimitar", ""], ["Poncelas", "Alberto", ""], ["Way", "Andy", ""]]}, {"id": "2005.00311", "submitter": "Ronen Tamari", "authors": "Ronen Tamari, Chen Shani, Tom Hope, Miriam R. L. Petruck, Omri Abend,\n  Dafna Shahaf", "title": "Language (Re)modelling: Towards Embodied Language Understanding", "comments": "Accepted to ACL2020 Theme Track. Extended bibliography version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While natural language understanding (NLU) is advancing rapidly, today's\ntechnology differs from human-like language understanding in fundamental ways,\nnotably in its inferior efficiency, interpretability, and generalization. This\nwork proposes an approach to representation and learning based on the tenets of\nembodied cognitive linguistics (ECL). According to ECL, natural language is\ninherently executable (like programming languages), driven by mental simulation\nand metaphoric mappings over hierarchical compositions of structures and\nschemata learned through embodied interaction. This position paper argues that\nthe use of grounding by metaphoric inference and simulation will greatly\nbenefit NLU systems, and proposes a system architecture along with a roadmap\ntowards realizing this vision.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 10:57:02 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 12:53:34 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Tamari", "Ronen", ""], ["Shani", "Chen", ""], ["Hope", "Tom", ""], ["Petruck", "Miriam R. L.", ""], ["Abend", "Omri", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2005.00315", "submitter": "Prasetya Ajie Utama", "authors": "Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych", "title": "Mind the Trade-off: Debiasing NLU Models without Degrading the\n  In-distribution Performance", "comments": "to appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for natural language understanding (NLU) tasks often rely on the\nidiosyncratic biases of the dataset, which make them brittle against test cases\noutside the training distribution. Recently, several proposed debiasing methods\nare shown to be very effective in improving out-of-distribution performance.\nHowever, their improvements come at the expense of performance drop when models\nare evaluated on the in-distribution data, which contain examples with higher\ndiversity. This seemingly inevitable trade-off may not tell us much about the\nchanges in the reasoning and understanding capabilities of the resulting models\non broader types of examples beyond the small subset represented in the\nout-of-distribution data. In this paper, we address this trade-off by\nintroducing a novel debiasing method, called confidence regularization, which\ndiscourage models from exploiting biases while enabling them to receive enough\nincentive to learn from all the training examples. We evaluate our method on\nthree NLU tasks and show that, in contrast to its predecessors, it improves the\nperformance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset)\nwhile maintaining the original in-distribution accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 11:22:55 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Utama", "Prasetya Ajie", ""], ["Moosavi", "Nafise Sadat", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2005.00316", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee, Chitta Baral", "title": "Self-supervised Knowledge Triplet Learning for Zero-shot Question\n  Answering", "comments": "Accepted to EMNLP 2020 Long Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of all Question Answering (QA) systems is to be able to generalize to\nunseen questions. Current supervised methods are reliant on expensive data\nannotation. Moreover, such annotations can introduce unintended annotator bias\nwhich makes systems focus more on the bias than the actual task. In this work,\nwe propose Knowledge Triplet Learning (KTL), a self-supervised task over\nknowledge graphs. We propose heuristics to create synthetic graphs for\ncommonsense and scientific knowledge. We propose methods of how to use KTL to\nperform zero-shot QA and our experiments show considerable improvements over\nlarge pre-trained transformer models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 11:24:18 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 20:49:29 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Baral", "Chitta", ""]]}, {"id": "2005.00318", "submitter": "Djam\\'e Seddah", "authors": "Benjamin Muller and Benoit Sagot and Djam\\'e Seddah", "title": "Can Multilingual Language Models Transfer to an Unseen Dialect? A Case\n  Study on North African Arabizi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Building natural language processing systems for non standardized and low\nresource languages is a difficult challenge. The recent success of large-scale\nmultilingual pretrained language models provides new modeling tools to tackle\nthis. In this work, we study the ability of multilingual language models to\nprocess an unseen dialect. We take user generated North-African Arabic as our\ncase study, a resource-poor dialectal variety of Arabic with frequent\ncode-mixing with French and written in Arabizi, a non-standardized\ntransliteration of Arabic to Latin script. Focusing on two tasks,\npart-of-speech tagging and dependency parsing, we show in zero-shot and\nunsupervised adaptation scenarios that multilingual language models are able to\ntransfer to such an unseen dialect, specifically in two extreme cases: (i)\nacross scripts, using Modern Standard Arabic as a source language, and (ii)\nfrom a distantly related language, unseen during pretraining, namely Maltese.\nOur results constitute the first successful transfer experiments on this\ndialect, paving thus the way for the development of an NLP ecosystem for\nresource-scarce, non-standardized and highly variable vernacular languages.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 11:29:23 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Muller", "Benjamin", ""], ["Sagot", "Benoit", ""], ["Seddah", "Djam\u00e9", ""]]}, {"id": "2005.00329", "submitter": "Lei Shen", "authors": "Lei Shen, Yang Feng", "title": "CDL: Curriculum Dual Learning for Emotion-Controllable Response\n  Generation", "comments": "To appear at ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion-controllable response generation is an attractive and valuable task\nthat aims to make open-domain conversations more empathetic and engaging.\nExisting methods mainly enhance the emotion expression by adding regularization\nterms to standard cross-entropy loss and thus influence the training process.\nHowever, due to the lack of further consideration of content consistency, the\ncommon problem of response generation tasks, safe response, is intensified.\nBesides, query emotions that can help model the relationship between query and\nresponse are simply ignored in previous models, which would further hurt the\ncoherence. To alleviate these problems, we propose a novel framework named\nCurriculum Dual Learning (CDL) which extends the emotion-controllable response\ngeneration to a dual task to generate emotional responses and emotional queries\nalternatively. CDL utilizes two rewards focusing on emotion and content to\nimprove the duality. Additionally, it applies curriculum learning to gradually\ngenerate high-quality responses based on the difficulties of expressing various\nemotions. Experimental results show that CDL significantly outperforms the\nbaselines in terms of coherence, diversity, and relation to emotion factors.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:16:44 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 04:31:34 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 09:39:15 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 11:31:47 GMT"}, {"version": "v5", "created": "Sun, 7 Jun 2020 12:54:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Shen", "Lei", ""], ["Feng", "Yang", ""]]}, {"id": "2005.00330", "submitter": "Shailaja Keyur Sampat", "authors": "Shailaja Keyur Sampat, Yezhou Yang and Chitta Baral", "title": "Visuo-Linguistic Question Answering (VLQA) Challenge", "comments": "Findings of EMNLP 2020 (22 pages, 13 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding images and text together is an important aspect of cognition\nand building advanced Artificial Intelligence (AI) systems. As a community, we\nhave achieved good benchmarks over language and vision domains separately,\nhowever joint reasoning is still a challenge for state-of-the-art computer\nvision and natural language processing (NLP) systems. We propose a novel task\nto derive joint inference about a given image-text modality and compile the\nVisuo-Linguistic Question Answering (VLQA) challenge corpus in a question\nanswering setting. Each dataset item consists of an image and a reading\npassage, where questions are designed to combine both visual and textual\ninformation i.e., ignoring either modality would make the question\nunanswerable. We first explore the best existing vision-language architectures\nto solve VLQA subsets and show that they are unable to reason well. We then\ndevelop a modular method with slightly better baseline performance, but it is\nstill far behind human performance. We believe that VLQA will be a good\nbenchmark for reasoning over a visuo-linguistic context. The dataset, code and\nleaderboard is available at https://shailaja183.github.io/vlqa/.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:18:55 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 01:06:30 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 07:45:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sampat", "Shailaja Keyur", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "2005.00333", "submitter": "Edoardo Maria Ponti", "authors": "Edoardo Maria Ponti, Goran Glava\\v{s}, Olga Majewska, Qianchu Liu,\n  Ivan Vuli\\'c and Anna Korhonen", "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to simulate human language capacity, natural language processing\nsystems must be able to reason about the dynamics of everyday situations,\nincluding their possible causes and effects. Moreover, they should be able to\ngeneralise the acquired world knowledge to new languages, modulo cultural\ndifferences. Advances in machine reasoning and cross-lingual transfer depend on\nthe availability of challenging evaluation benchmarks. Motivated by both\ndemands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a\ntypologically diverse multilingual dataset for causal commonsense reasoning in\n11 languages, which includes resource-poor languages like Eastern Apur\\'imac\nQuechua and Haitian Creole. We evaluate a range of state-of-the-art models on\nthis novel dataset, revealing that the performance of current methods based on\nmultilingual pretraining and zero-shot fine-tuning falls short compared to\ntranslation-based transfer. Finally, we propose strategies to adapt\nmultilingual models to out-of-sample resource-lean languages where only a small\ncorpus or a bilingual dictionary is available, and report substantial\nimprovements over the random baseline. The XCOPA dataset is freely available at\ngithub.com/cambridgeltl/xcopa.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:22:33 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 23:23:58 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ponti", "Edoardo Maria", ""], ["Glava\u0161", "Goran", ""], ["Majewska", "Olga", ""], ["Liu", "Qianchu", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""]]}, {"id": "2005.00352", "submitter": "Louis Martin", "authors": "Louis Martin, Angela Fan, \\'Eric de la Clergerie, Antoine Bordes,\n  Beno\\^it Sagot", "title": "MUSS: Multilingual Unsupervised Sentence Simplification by Mining\n  Paraphrases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in sentence simplification has been hindered by a lack of labeled\nparallel simplification data, particularly in languages other than English. We\nintroduce MUSS, a Multilingual Unsupervised Sentence Simplification system that\ndoes not require labeled simplification data. MUSS uses a novel approach to\nsentence simplification that trains strong models using sentence-level\nparaphrase data instead of proper simplification data. These models leverage\nunsupervised pretraining and controllable generation mechanisms to flexibly\nadjust attributes such as length and lexical complexity at inference time. We\nfurther present a method to mine such paraphrase data in any language from\nCommon Crawl using semantic sentence embeddings, thus removing the need for\nlabeled data. We evaluate our approach on English, French, and Spanish\nsimplification benchmarks and closely match or outperform the previous best\nsupervised results, despite not using any labeled simplification data. We push\nthe state of the art further by incorporating labeled simplification data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:54:30 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 15:08:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Martin", "Louis", ""], ["Fan", "Angela", ""], ["de la Clergerie", "\u00c9ric", ""], ["Bordes", "Antoine", ""], ["Sagot", "Beno\u00eet", ""]]}, {"id": "2005.00357", "submitter": "Soujanya Poria", "authors": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Rada Mihalcea", "title": "Beneath the Tip of the Iceberg: Current Challenges and New Directions in\n  Sentiment Analysis Research", "comments": "Published in the IEEE Transactions on Affective Computing (TAFFC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sentiment analysis as a field has come a long way since it was first\nintroduced as a task nearly 20 years ago. It has widespread commercial\napplications in various domains like marketing, risk management, market\nresearch, and politics, to name a few. Given its saturation in specific\nsubtasks -- such as sentiment polarity classification -- and datasets, there is\nan underlying perception that this field has reached its maturity. In this\narticle, we discuss this perception by pointing out the shortcomings and\nunder-explored, yet key aspects of this field that are necessary to attain true\nsentiment understanding. We analyze the significant leaps responsible for its\ncurrent relevance. Further, we attempt to chart a possible course for this\nfield that covers many overlooked and unanswered questions.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 13:05:23 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 13:39:24 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 00:57:08 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 15:45:25 GMT"}, {"version": "v5", "created": "Mon, 16 Nov 2020 15:21:22 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Poria", "Soujanya", ""], ["Hazarika", "Devamanyu", ""], ["Majumder", "Navonil", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2005.00388", "submitter": "Costanza Conforti", "authors": "Costanza Conforti and Jakob Berndt and Mohammad Taher Pilehvar and\n  Chryssi Giannitsarou and Flavio Toxvaerd and Nigel Collier", "title": "Will-They-Won't-They: A Very Large Dataset for Stance Detection on\n  Twitter", "comments": "10 pages, accepted at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new challenging stance detection dataset, called\nWill-They-Won't-They (WT-WT), which contains 51,284 tweets in English, making\nit by far the largest available dataset of the type. All the annotations are\ncarried out by experts; therefore, the dataset constitutes a high-quality and\nreliable benchmark for future research in stance detection. Our experiments\nwith a wide range of recent state-of-the-art stance detection systems show that\nthe dataset poses a strong challenge to existing models in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:10:37 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Conforti", "Costanza", ""], ["Berndt", "Jakob", ""], ["Pilehvar", "Mohammad Taher", ""], ["Giannitsarou", "Chryssi", ""], ["Toxvaerd", "Flavio", ""], ["Collier", "Nigel", ""]]}, {"id": "2005.00396", "submitter": "Philipp Dufter", "authors": "Philipp Dufter, Hinrich Sch\\\"utze", "title": "Identifying Necessary Elements for BERT's Multilinguality", "comments": "EMNLP2020 CRV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that multilingual BERT (mBERT) yields high quality\nmultilingual representations and enables effective zero-shot transfer. This is\nsurprising given that mBERT does not use any crosslingual signal during\ntraining. While recent literature has studied this phenomenon, the reasons for\nthe multilinguality are still somewhat obscure. We aim to identify\narchitectural properties of BERT and linguistic properties of languages that\nare necessary for BERT to become multilingual. To allow for fast\nexperimentation we propose an efficient setup with small BERT models trained on\na mix of synthetic and natural data. Overall, we identify four architectural\nand two linguistic elements that influence multilinguality. Based on our\ninsights, we experiment with a multilingual pretraining setup that modifies the\nmasking strategy using VecMap, i.e., unsupervised embedding alignment.\nExperiments on XNLI with three languages indicate that our findings transfer\nfrom our small setup to larger scale settings.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:27:14 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 13:26:22 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 14:51:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Dufter", "Philipp", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2005.00432", "submitter": "Shrimai Prabhumoye", "authors": "Shrimai Prabhumoye, Ruslan Salakhutdinov, Alan W Black", "title": "Topological Sort for Sentence Ordering", "comments": "Will be published at the Proceedings of the 58th Annual Meeting of\n  the Association for Computational Linguistics (ACL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence ordering is the task of arranging the sentences of a given text in\nthe correct order. Recent work using deep neural networks for this task has\nframed it as a sequence prediction problem. In this paper, we propose a new\nframing of this task as a constraint solving problem and introduce a new\ntechnique to solve it. Additionally, we propose a human evaluation for this\ntask. The results on both automatic and human metrics across four different\ndatasets show that this new technique is better at capturing coherence in\ndocuments.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:07:59 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Prabhumoye", "Shrimai", ""], ["Salakhutdinov", "Ruslan", ""], ["Black", "Alan W", ""]]}, {"id": "2005.00436", "submitter": "Ying Luo", "authors": "Ying Luo and Hai Zhao", "title": "Bipartite Flat-Graph Network for Nested Named Entity Recognition", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for\nnested named entity recognition (NER), which contains two subgraph modules: a\nflat NER module for outermost entities and a graph module for all the entities\nlocated in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional\nnetwork (GCN) are adopted to jointly learn flat entities and their inner\ndependencies. Different from previous models, which only consider the\nunidirectional delivery of information from innermost layers to outer ones (or\noutside-to-inside), our model effectively captures the bidirectional\ninteraction between them. We first use the entities recognized by the flat NER\nmodule to construct an entity graph, which is fed to the next graph module. The\nricher representation learned from graph module carries the dependencies of\ninner entities and can be exploited to improve outermost entity predictions.\nExperimental results on three standard nested NER datasets demonstrate that our\nBiFlaG outperforms previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:14:22 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Luo", "Ying", ""], ["Zhao", "Hai", ""]]}, {"id": "2005.00446", "submitter": "Zhaoyang Wang", "authors": "Zhaoyang Wang and Hongtao Wang", "title": "Defense of Word-level Adversarial Attacks via Random Substitution\n  Encoding", "comments": "12 pages, 2 figures, 4 tables. Accepted as a FULL paper at KSEM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adversarial attacks against deep neural networks on computer vision tasks\nhave spawned many new technologies that help protect models from avoiding false\npredictions. Recently, word-level adversarial attacks on deep models of Natural\nLanguage Processing (NLP) tasks have also demonstrated strong power, e.g.,\nfooling a sentiment classification neural network to make wrong decisions.\nUnfortunately, few previous literatures have discussed the defense of such\nword-level synonym substitution based attacks since they are hard to be\nperceived and detected. In this paper, we shed light on this problem and\npropose a novel defense framework called Random Substitution Encoding (RSE),\nwhich introduces a random substitution encoder into the training process of\noriginal neural networks. Extensive experiments on text classification tasks\ndemonstrate the effectiveness of our framework on defense of word-level\nadversarial attacks, under various base and attack models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:28:43 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 05:55:34 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wang", "Zhaoyang", ""], ["Wang", "Hongtao", ""]]}, {"id": "2005.00456", "submitter": "Shikib Mehri", "authors": "Shikib Mehri and Maxine Eskenazi", "title": "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog\n  Generation", "comments": "Accepted to ACL 2020 as long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of meaningful automatic evaluation metrics for dialog has impeded\nopen-domain dialog research. Standard language generation metrics have been\nshown to be ineffective for evaluating dialog models. To this end, this paper\npresents USR, an UnSupervised and Reference-free evaluation metric for dialog.\nUSR is a reference-free metric that trains unsupervised models to measure\nseveral desirable qualities of dialog. USR is shown to strongly correlate with\nhuman judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and\nPersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces\ninterpretable measures for several desirable properties of dialog.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:50:50 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Mehri", "Shikib", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "2005.00458", "submitter": "Khyathi Raghavi Chandu", "authors": "Khyathi Raghavi Chandu, Alan W Black", "title": "Style Variation as a Vantage Point for Code-Switching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-Switching (CS) is a common phenomenon observed in several bilingual and\nmultilingual communities, thereby attaining prevalence in digital and social\nmedia platforms. This increasing prominence demands the need to model CS\nlanguages for critical downstream tasks. A major problem in this domain is the\ndearth of annotated data and a substantial corpora to train large scale neural\nmodels. Generating vast amounts of quality text assists several down stream\ntasks that heavily rely on language modeling such as speech recognition,\ntext-to-speech synthesis etc,. We present a novel vantage point of CS to be\nstyle variations between both the participating languages. Our approach does\nnot need any external annotations such as lexical language ids. It mainly\nrelies on easily obtainable monolingual corpora without any parallel alignment\nand a limited set of naturally CS sentences. We propose a two-stage generative\nadversarial training approach where the first stage generates competitive\nnegative examples for CS and the second stage generates more realistic CS\nsentences. We present our experiments on the following pairs of languages:\nSpanish-English, Mandarin-English, Hindi-English and Arabic-French. We show\nthat the trends in metrics for generated CS move closer to real CS data in each\nof the above language pairs through the dual stage training process. We believe\nthis viewpoint of CS as style variations opens new perspectives for modeling\nvarious tasks in CS text.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:53:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Chandu", "Khyathi Raghavi", ""], ["Black", "Alan W", ""]]}, {"id": "2005.00460", "submitter": "Shikhar Vashishth", "authors": "Shikhar Vashishth, Denis Newman-Griffis, Rishabh Joshi, Ritam Dutt,\n  Carolyn Rose", "title": "Improving Broad-Coverage Medical Entity Linking with Semantic Type\n  Prediction and Large-Scale Datasets", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical entity linking is the task of identifying and standardizing medical\nconcepts referred to in an unstructured text. Most of the existing methods\nadopt a three-step approach of (1) detecting mentions, (2) generating a list of\ncandidate concepts, and finally (3) picking the best concept among them. In\nthis paper, we probe into alleviating the problem of overgeneration of\ncandidate concepts in the candidate generation module, the most under-studied\ncomponent of medical entity linking. For this, we present MedType, a fully\nmodular system that prunes out irrelevant candidate concepts based on the\npredicted semantic type of an entity mention. We incorporate MedType into five\noff-the-shelf toolkits for medical entity linking and demonstrate that it\nconsistently improves entity linking performance across several benchmark\ndatasets. To address the dearth of annotated training data for medical entity\nlinking, we present WikiMed and PubMedDS, two large-scale medical entity\nlinking datasets, and demonstrate that pre-training MedType on these datasets\nfurther improves entity linking performance. We make our source code and\ndatasets publicly available for medical entity linking research.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:55:50 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 15:07:32 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 23:10:29 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Vashishth", "Shikhar", ""], ["Newman-Griffis", "Denis", ""], ["Joshi", "Rishabh", ""], ["Dutt", "Ritam", ""], ["Rose", "Carolyn", ""]]}, {"id": "2005.00463", "submitter": "George Awad", "authors": "Keith Curtis, George Awad, Shahzad Rajput, and Ian Soboroff", "title": "HLVU : A New Challenge to Test Deep Understanding of Movies the Way\n  Humans do", "comments": null, "journal-ref": null, "doi": "10.1145/3372278.3390742", "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new evaluation challenge and direction in the area\nof High-level Video Understanding. The challenge we are proposing is designed\nto test automatic video analysis and understanding, and how accurately systems\ncan comprehend a movie in terms of actors, entities, events and their\nrelationship to each other. A pilot High-Level Video Understanding (HLVU)\ndataset of open source movies were collected for human assessors to build a\nknowledge graph representing each of them. A set of queries will be derived\nfrom the knowledge graph to test systems on retrieving relationships among\nactors, as well as reasoning and retrieving non-visual concepts. The objective\nis to benchmark if a computer system can \"understand\" non-explicit but obvious\nrelationships the same way humans do when they watch the same movies. This is\nlong-standing problem that is being addressed in the text domain and this\nproject moves similar research to the video domain. Work of this nature is\nfoundational to future video analytics and video understanding technologies.\nThis work can be of interest to streaming services and broadcasters hoping to\nprovide more intuitive ways for their customers to interact with and consume\nvideo content.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:58:13 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Curtis", "Keith", ""], ["Awad", "George", ""], ["Rajput", "Shahzad", ""], ["Soboroff", "Ian", ""]]}, {"id": "2005.00468", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Iria da Cunha, Juan-Manuel Torres-Moreno", "title": "Automatic Discourse Segmentation: Review and Perspectives", "comments": "5 pages, 1 figure", "journal-ref": "International Workshop on African Human Language Technologies.\n  17-20 Jan 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual discourse parsing is a very prominent research topic. The first\nstage for discourse parsing is discourse segmentation. The study reported in\nthis article addresses a review of two on-line available discourse segmenters\n(for English and Portuguese). We evaluate the possibility of developing similar\ndiscourse segmenters for Spanish, French and African languages.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 16:03:37 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["da Cunha", "Iria", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "2005.00480", "submitter": "Vaibhav Adlakha", "authors": "Vaibhav Adlakha, Parth Shah, Srikanta Bedathur, Mausam", "title": "Knowledge Base Inference for Regular Expression Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two common types of tasks on Knowledge Bases have been studied -- single link\nprediction (Knowledge Base Completion) and path query answering. However, our\nanalysis of user queries on a real-world knowledge base reveals that a\nsignificant fraction of queries specify paths using regular expressions(regex).\nSuch regex queries cannot be handled by any of the existing link prediction or\npath query answering models. In response, we present Regex Query Answering, the\nnovel task of answering regex queries on incomplete KBs. We contribute two\ndatasets for the task, including one where test queries are harvested from\nactual user querylogs. We train baseline neural models for our new task and\npropose novel ways to handle disjunction and Kleene plus regex operators.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 16:43:06 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Adlakha", "Vaibhav", ""], ["Shah", "Parth", ""], ["Bedathur", "Srikanta", ""], ["Mausam", "", ""]]}, {"id": "2005.00481", "submitter": "Fernando Alva-Manchego", "authors": "Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina\n  Scarton, Beno\\^it Sagot, Lucia Specia", "title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification\n  Models with Multiple Rewriting Transformations", "comments": "Accepted to ACL 2020 (camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to simplify a sentence, human editors perform multiple rewriting\ntransformations: they split it into several shorter sentences, paraphrase words\n(i.e. replacing complex words or phrases by simpler synonyms), reorder\ncomponents, and/or delete information deemed unnecessary. Despite these varied\nrange of possible text alterations, current models for automatic sentence\nsimplification are evaluated using datasets that are focused on a single\ntransformation, such as lexical paraphrasing or splitting. This makes it\nimpossible to understand the ability of simplification models in more realistic\nsettings. To alleviate this limitation, this paper introduces ASSET, a new\ndataset for assessing sentence simplification in English. ASSET is a\ncrowdsourced multi-reference corpus where each simplification was produced by\nexecuting several rewriting transformations. Through quantitative and\nqualitative experiments, we show that simplifications in ASSET are better at\ncapturing characteristics of simplicity when compared to other standard\nevaluation datasets for the task. Furthermore, we motivate the need for\ndeveloping better methods for automatic evaluation using ASSET, since we show\nthat current popular metrics may not be suitable when multiple simplification\ntransformations are performed.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 16:44:54 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Alva-Manchego", "Fernando", ""], ["Martin", "Louis", ""], ["Bordes", "Antoine", ""], ["Scarton", "Carolina", ""], ["Sagot", "Beno\u00eet", ""], ["Specia", "Lucia", ""]]}, {"id": "2005.00496", "submitter": "Tao Li", "authors": "Tao Li, Parth Anand Jawale, Martha Palmer, Vivek Srikumar", "title": "Structured Tuning for Semantic Role Labeling", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural network-driven semantic role labeling (SRL) systems have shown\nimpressive improvements in F1 scores. These improvements are due to expressive\ninput representations, which, at least at the surface, are orthogonal to\nknowledge-rich constrained decoding mechanisms that helped linear SRL models.\nIntroducing the benefits of structure to inform neural models presents a\nmethodological challenge. In this paper, we present a structured tuning\nframework to improve models using softened constraints only at training time.\nOur framework leverages the expressiveness of neural networks and provides\nsupervision with structured loss components. We start with a strong baseline\n(RoBERTa) to validate the impact of our approach, and show that our framework\noutperforms the baseline by learning to comply with declarative constraints.\nAdditionally, our experiments with smaller training sizes show that we can\nachieve consistent improvements under low-resource scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:12:20 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 07:39:17 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Li", "Tao", ""], ["Jawale", "Parth Anand", ""], ["Palmer", "Martha", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2005.00502", "submitter": "Liyuan Liu", "authors": "Shi Zhi and Liyuan Liu and Yu Zhang and Shiyin Wang and Qi Li and Chao\n  Zhang and Jiawei Han", "title": "Partially-Typed NER Datasets Integration: Connecting Practice to Theory", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While typical named entity recognition (NER) models require the training set\nto be annotated with all target types, each available datasets may only cover a\npart of them. Instead of relying on fully-typed NER datasets, many efforts have\nbeen made to leverage multiple partially-typed ones for training and allow the\nresulting model to cover a full type set. However, there is neither guarantee\non the quality of integrated datasets, nor guidance on the design of training\nalgorithms. Here, we conduct a systematic analysis and comparison between\npartially-typed NER datasets and fully-typed ones, in both theoretical and\nempirical manner. Firstly, we derive a bound to establish that models trained\nwith partially-typed annotations can reach a similar performance with the ones\ntrained with fully-typed annotations, which also provides guidance on the\nalgorithm design. Moreover, we conduct controlled experiments, which shows\npartially-typed datasets leads to similar performance with the model trained\nwith the same amount of fully-typed annotations\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:16:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zhi", "Shi", ""], ["Liu", "Liyuan", ""], ["Zhang", "Yu", ""], ["Wang", "Shiyin", ""], ["Li", "Qi", ""], ["Zhang", "Chao", ""], ["Han", "Jiawei", ""]]}, {"id": "2005.00512", "submitter": "Sarthak Jain", "authors": "Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy", "title": "SciREX: A Challenge Dataset for Document-Level Information Extraction", "comments": "ACL2020 Camera Ready Submission, Work done by first authors while\n  interning at AI2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting information from full documents is an important problem in many\ndomains, but most previous work focus on identifying relationships within a\nsentence or a paragraph. It is challenging to create a large-scale information\nextraction (IE) dataset at the document level since it requires an\nunderstanding of the whole document to annotate entities and their\ndocument-level relationships that usually span beyond sentences or even\nsections. In this paper, we introduce SciREX, a document level IE dataset that\nencompasses multiple IE tasks, including salient entity identification and\ndocument level $N$-ary relation identification from scientific articles. We\nannotate our dataset by integrating automatic and human annotations, leveraging\nexisting scientific knowledge resources. We develop a neural model as a strong\nbaseline that extends previous state-of-the-art IE models to document-level IE.\nAnalyzing the model performance shows a significant gap between human\nperformance and current baselines, inviting the community to use our dataset as\na challenge to develop document-level IE models. Our data and code are publicly\navailable at https://github.com/allenai/SciREX\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:30:10 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Jain", "Sarthak", ""], ["van Zuylen", "Madeleine", ""], ["Hajishirzi", "Hannaneh", ""], ["Beltagy", "Iz", ""]]}, {"id": "2005.00513", "submitter": "Yue Dong", "authors": "Yue Dong, Andrei Mircea, Jackie C. K. Cheung", "title": "Discourse-Aware Unsupervised Summarization of Long Scientific Documents", "comments": "9 pages, 3 figures, EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised graph-based ranking model for extractive\nsummarization of long scientific documents. Our method assumes a two-level\nhierarchical graph representation of the source document, and exploits\nasymmetrical positional cues to determine sentence importance. Results on the\nPubMed and arXiv datasets show that our approach outperforms strong\nunsupervised baselines by wide margins in automatic metrics and human\nevaluation. In addition, it achieves performance comparable to many\nstate-of-the-art supervised approaches which are trained on hundreds of\nthousands of examples. These results suggest that patterns in the discourse\nstructure are a strong signal for determining importance in scientific\narticles.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:31:11 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 16:57:10 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Dong", "Yue", ""], ["Mircea", "Andrei", ""], ["Cheung", "Jackie C. K.", ""]]}, {"id": "2005.00524", "submitter": "Mozhi Zhang", "authors": "Mozhi Zhang, Yoshinari Fujinuma, Michael J. Paul, Jordan Boyd-Graber", "title": "Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word\n  Embeddings to Dictionaries", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon\ninduction (BLI). Recent CLWE methods use linear projections, which underfit the\ntraining dictionary, to generalize on BLI. However, underfitting can hinder\ngeneralization to other downstream tasks that rely on words from the training\ndictionary. We address this limitation by retrofitting CLWE to the training\ndictionary, which pulls training translation pairs closer in the embedding\nspace and overfits the training dictionary. This simple post-processing step\noften improves accuracy on two downstream tasks, despite lowering BLI test\naccuracy. We also retrofit to both the training dictionary and a synthetic\ndictionary induced from CLWE, which sometimes generalizes even better on\ndownstream tasks. Our results confirm the importance of fully exploiting\ntraining dictionary in downstream tasks and explains why BLI is a flawed CLWE\nevaluation.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:56:01 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zhang", "Mozhi", ""], ["Fujinuma", "Yoshinari", ""], ["Paul", "Michael J.", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2005.00545", "submitter": "Ines Chami", "authors": "Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi and\n  Christopher R\\'e", "title": "Low-Dimensional Hyperbolic Knowledge Graph Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph (KG) embeddings learn low-dimensional representations of\nentities and relations to predict missing facts. KGs often exhibit hierarchical\nand logical patterns which must be preserved in the embedding space. For\nhierarchical data, hyperbolic embedding methods have shown promise for\nhigh-fidelity and parsimonious representations. However, existing hyperbolic\nembedding methods do not account for the rich logical patterns in KGs. In this\nwork, we introduce a class of hyperbolic KG embedding models that\nsimultaneously capture hierarchical and logical patterns. Our approach combines\nhyperbolic reflections and rotations with attention to model complex relational\npatterns. Experimental results on standard KG benchmarks show that our method\nimproves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in\nmean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that\ndifferent geometric transformations capture different types of relations while\nattention-based transformations generalize to multiple relations. In high\ndimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR\nand 57.7% on YAGO3-10.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:00:02 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chami", "Ines", ""], ["Wolf", "Adva", ""], ["Juan", "Da-Cheng", ""], ["Sala", "Frederic", ""], ["Ravi", "Sujith", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2005.00547", "submitter": "Dorottya Demszky", "authors": "Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen,\n  Gaurav Nemade and Sujith Ravi", "title": "GoEmotions: A Dataset of Fine-Grained Emotions", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding emotion expressed in language has a wide range of applications,\nfrom building empathetic chatbots to detecting harmful online behavior.\nAdvancement in this area can be improved using large-scale datasets with a\nfine-grained typology, adaptable to multiple downstream tasks. We introduce\nGoEmotions, the largest manually annotated dataset of 58k English Reddit\ncomments, labeled for 27 emotion categories or Neutral. We demonstrate the high\nquality of the annotations via Principal Preserved Component Analysis. We\nconduct transfer learning experiments with existing emotion benchmarks to show\nthat our dataset generalizes well to other domains and different emotion\ntaxonomies. Our BERT-based model achieves an average F1-score of .46 across our\nproposed taxonomy, leaving much room for improvement.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:00:02 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 00:31:11 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Demszky", "Dorottya", ""], ["Movshovitz-Attias", "Dana", ""], ["Ko", "Jeongwoo", ""], ["Cowen", "Alan", ""], ["Nemade", "Gaurav", ""], ["Ravi", "Sujith", ""]]}, {"id": "2005.00558", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, Bill\n  Dolan", "title": "POINTER: Constrained Progressive Text Generation via Insertion-based\n  Generative Pre-training", "comments": "EMNLP 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained language models, such as BERT and GPT-2, have\nachieved excellent performance in language representation learning and\nfree-form text generation. However, these models cannot be directly employed to\ngenerate text under specified lexical constraints. To address this challenge,\nwe present POINTER (PrOgressive INsertion-based TransformER), a simple yet\nnovel insertion-based approach for hard-constrained text generation. The\nproposed method operates by progressively inserting new tokens between existing\ntokens in a parallel manner. This procedure is recursively applied until a\nsequence is completed. The resulting coarse-to-fine hierarchy makes the\ngeneration process intuitive and interpretable. We pre-train our model with the\nproposed progressive insertion-based objective on a 12GB Wikipedia dataset, and\nfine-tune it on downstream hard-constrained generation tasks.\nNon-autoregressive decoding yields an empirically logarithmic time complexity\nduring inference time. Experimental results on both News and Yelp datasets\ndemonstrate that POINTER achieves state-of-the-art performance on constrained\ntext generation. We released the pre-trained models and the source code to\nfacilitate future research (https://github.com/dreasysnail/POINTER).\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:11:54 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 00:07:39 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhang", "Yizhe", ""], ["Wang", "Guoyin", ""], ["Li", "Chunyuan", ""], ["Gan", "Zhe", ""], ["Brockett", "Chris", ""], ["Dolan", "Bill", ""]]}, {"id": "2005.00561", "submitter": "Anna Rogers", "authors": "Sai Prasanna, Anna Rogers, Anna Rumshisky", "title": "When BERT Plays the Lottery, All Tickets Are Winning", "comments": "EMNLP 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Transformer-based models were shown to be reducible to a smaller number\nof self-attention heads and layers. We consider this phenomenon from the\nperspective of the lottery ticket hypothesis, using both structured and\nmagnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find\nsubnetworks achieving performance that is comparable with that of the full\nmodel, and (b) similarly-sized subnetworks sampled from the rest of the model\nperform worse. Strikingly, with structured pruning even the worst possible\nsubnetworks remain highly trainable, indicating that most pre-trained BERT\nweights are potentially useful. We also study the \"good\" subnetworks to see if\ntheir success can be attributed to superior linguistic knowledge, but find them\nunstable, and not explained by meaningful self-attention patterns.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:24:42 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 10:15:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Prasanna", "Sai", ""], ["Rogers", "Anna", ""], ["Rumshisky", "Anna", ""]]}, {"id": "2005.00571", "submitter": "Deren Lei", "authors": "Deren Lei and Gangrong Jiang and Xiaotao Gu and Kexuan Sun and Yuning\n  Mao and Xiang Ren", "title": "Learning Collaborative Agents with Rule Guidance for Knowledge Graph\n  Reasoning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walk-based models have shown their advantages in knowledge graph (KG)\nreasoning by achieving decent performance while providing interpretable\ndecisions. However, the sparse reward signals offered by the KG during\ntraversal are often insufficient to guide a sophisticated walk-based\nreinforcement learning (RL) model. An alternate approach is to use traditional\nsymbolic methods (e.g., rule induction), which achieve good performance but can\nbe hard to generalize due to the limitation of symbolic representation. In this\npaper, we propose RuleGuider, which leverages high-quality rules generated by\nsymbolic-based methods to provide reward supervision for walk-based agents.\nExperiments on benchmark datasets show that RuleGuider improves the performance\nof walk-based models without losing interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:57:14 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 10:13:30 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Lei", "Deren", ""], ["Jiang", "Gangrong", ""], ["Gu", "Xiaotao", ""], ["Sun", "Kexuan", ""], ["Mao", "Yuning", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00572", "submitter": "Hu Hu", "authors": "Hu Hu, Rui Zhao, Jinyu Li, Liang Lu, Yifan Gong", "title": "Exploring Pre-training with Alignments for RNN Transducer based\n  End-to-End Speech Recognition", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the recurrent neural network transducer (RNN-T) architecture has\nbecome an emerging trend in end-to-end automatic speech recognition research\ndue to its advantages of being capable for online streaming speech recognition.\nHowever, RNN-T training is made difficult by the huge memory requirements, and\ncomplicated neural structure. A common solution to ease the RNN-T training is\nto employ connectionist temporal classification (CTC) model along with RNN\nlanguage model (RNNLM) to initialize the RNN-T parameters. In this work, we\nconversely leverage external alignments to seed the RNN-T model. Two different\npre-training solutions are explored, referred to as encoder pre-training, and\nwhole-network pre-training respectively. Evaluated on Microsoft 65,000 hours\nanonymized production data with personally identifiable information removed,\nour proposed methods can obtain significant improvement. In particular, the\nencoder pre-training solution achieved a 10% and a 8% relative word error rate\nreduction when compared with random initialization and the widely used\nCTC+RNNLM initialization strategy, respectively. Our solutions also\nsignificantly reduce the RNN-T model latency from the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 19:00:57 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hu", "Hu", ""], ["Zhao", "Rui", ""], ["Li", "Jinyu", ""], ["Lu", "Liang", ""], ["Gong", "Yifan", ""]]}, {"id": "2005.00574", "submitter": "Xiang Yue", "authors": "Xiang Yue, Bernal Jimenez Gutierrez and Huan Sun", "title": "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension has made great progress in recent years owing\nto large-scale annotated datasets. In the clinical domain, however, creating\nsuch datasets is quite difficult due to the domain expertise required for\nannotation. Recently, Pampari et al. (EMNLP'18) tackled this issue by using\nexpert-annotated question templates and existing i2b2 annotations to create\nemrQA, the first large-scale dataset for question answering (QA) based on\nclinical notes. In this paper, we provide an in-depth analysis of this dataset\nand the clinical reading comprehension (CliniRC) task. From our qualitative\nanalysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA\nquestions are often answerable without using domain knowledge. From our\nquantitative experiments, surprising results include that (iii) using a small\nsampled subset (5%-20%), we can obtain roughly equal performance compared to\nthe model trained on the entire dataset, (iv) this performance is close to\nhuman expert's performance, and (v) BERT models do not beat the best performing\nbase model. Following our analysis of the emrQA, we further explore two desired\naspects of CliniRC systems: the ability to utilize clinical domain knowledge\nand to generalize to unseen questions and contexts. We argue that both should\nbe considered when creating future datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 19:07:33 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yue", "Xiang", ""], ["Gutierrez", "Bernal Jimenez", ""], ["Sun", "Huan", ""]]}, {"id": "2005.00580", "submitter": "Xing Niu", "authors": "Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan", "title": "Evaluating Robustness to Input Perturbations for Neural Machine\n  Translation", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) models are sensitive to small perturbations\nin the input. Robustness to such perturbations is typically measured using\ntranslation quality metrics such as BLEU on the noisy input. This paper\nproposes additional metrics which measure the relative degradation and changes\nin translation when small perturbations are added to the input. We focus on a\nclass of models employing subword regularization to address robustness and\nperform extensive evaluations of these models using the robustness measures\nproposed. Results show that our proposed metrics reveal a clear trend of\nimproved robustness to perturbations when subword regularization methods are\nused.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 19:54:46 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Niu", "Xing", ""], ["Mathur", "Prashant", ""], ["Dinu", "Georgiana", ""], ["Al-Onaizan", "Yaser", ""]]}, {"id": "2005.00581", "submitter": "Sandeep Subramanian", "authors": "Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, Y-Lan\n  Boureau", "title": "Multi-scale Transformer Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate multi-scale transformer language models that learn\nrepresentations of text at multiple scales, and present three different\narchitectures that have an inductive bias to handle the hierarchical nature of\nlanguage. Experiments on large-scale language modeling benchmarks empirically\ndemonstrate favorable likelihood vs memory footprint trade-offs, e.g. we show\nthat it is possible to train a hierarchical variant with 30 layers that has 23%\nsmaller memory footprint and better perplexity, compared to a vanilla\ntransformer with less than half the number of layers, on the Toronto\nBookCorpus. We analyze the advantages of learned representations at multiple\nscales in terms of memory footprint, compute time, and perplexity, which are\nparticularly appealing given the quadratic scaling of transformers' run time\nand memory usage with respect to sequence length.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 19:58:56 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Subramanian", "Sandeep", ""], ["Collobert", "Ronan", ""], ["Ranzato", "Marc'Aurelio", ""], ["Boureau", "Y-Lan", ""]]}, {"id": "2005.00583", "submitter": "Koustuv Sinha", "authors": "Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe,\n  William L. Hamilton, Joelle Pineau", "title": "Learning an Unreferenced Metric for Online Dialogue Evaluation", "comments": "Accepted at ACL 2020, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the quality of a dialogue interaction between two agents is a\ndifficult task, especially in open-domain chit-chat style dialogue. There have\nbeen recent efforts to develop automatic dialogue evaluation metrics, but most\nof them do not generalize to unseen datasets and/or need a human-generated\nreference response during inference, making it infeasible for online\nevaluation. Here, we propose an unreferenced automated evaluation metric that\nuses large pre-trained language models to extract latent representations of\nutterances, and leverages the temporal transitions that exist between them. We\nshow that our model achieves higher correlation with human annotations in an\nonline setting, while not requiring true responses for comparison during\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:01:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Sinha", "Koustuv", ""], ["Parthasarathi", "Prasanna", ""], ["Wang", "Jasmine", ""], ["Lowe", "Ryan", ""], ["Hamilton", "William L.", ""], ["Pineau", "Joelle", ""]]}, {"id": "2005.00613", "submitter": "Zeqiu Wu", "authors": "Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang, Xiang Gao, Chris\n  Quirk, Rik Koncel-Kedziorski, Jianfeng Gao, Hannaneh Hajishirzi, Mari\n  Ostendorf and Bill Dolan", "title": "A Controllable Model of Grounded Response Generation", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current end-to-end neural conversation models inherently lack the flexibility\nto impose semantic control in the response generation process, often resulting\nin uninteresting responses. Attempts to boost informativeness alone come at the\nexpense of factual accuracy, as attested by pretrained language models'\npropensity to \"hallucinate\" facts. While this may be mitigated by access to\nbackground knowledge, there is scant guarantee of relevance and informativeness\nin generated responses. We propose a framework that we call controllable\ngrounded response generation (CGRG), in which lexical control phrases are\neither provided by a user or automatically extracted by a control phrase\npredictor from dialogue context and grounding knowledge. Quantitative and\nqualitative results show that, using this framework, a transformer based model\nwith a novel inductive attention mechanism, trained on a conversation-like\nReddit dataset, outperforms strong generation baselines.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:22:08 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 06:23:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Zeqiu", ""], ["Galley", "Michel", ""], ["Brockett", "Chris", ""], ["Zhang", "Yizhe", ""], ["Gao", "Xiang", ""], ["Quirk", "Chris", ""], ["Koncel-Kedziorski", "Rik", ""], ["Gao", "Jianfeng", ""], ["Hajishirzi", "Hannaneh", ""], ["Ostendorf", "Mari", ""], ["Dolan", "Bill", ""]]}, {"id": "2005.00614", "submitter": "Emily Dinan", "authors": "Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, Adina\n  Williams", "title": "Multi-Dimensional Gender Bias Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are trained to find patterns in data. NLP models can\ninadvertently learn socially undesirable patterns when training on gender\nbiased text. In this work, we propose a general framework that decomposes\ngender bias in text along several pragmatic and semantic dimensions: bias from\nthe gender of the person being spoken about, bias from the gender of the person\nbeing spoken to, and bias from the gender of the speaker. Using this\nfine-grained framework, we automatically annotate eight large scale datasets\nwith gender information. In addition, we collect a novel, crowdsourced\nevaluation benchmark of utterance-level gender rewrites. Distinguishing between\ngender bias along multiple dimensions is important, as it enables us to train\nfiner-grained gender bias classifiers. We show our classifiers prove valuable\nfor a variety of important applications, such as controlling for gender bias in\ngenerative models, detecting gender bias in arbitrary text, and shed light on\noffensive language in terms of genderedness.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:23:20 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Dinan", "Emily", ""], ["Fan", "Angela", ""], ["Wu", "Ledell", ""], ["Weston", "Jason", ""], ["Kiela", "Douwe", ""], ["Williams", "Adina", ""]]}, {"id": "2005.00619", "submitter": "Gabriel Ilharco", "authors": "Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi", "title": "Probing Contextual Language Models for Common Ground with Visual\n  Representations", "comments": "Proceedings of the 2021 North American Chapter of the Association for\n  Computational Linguistics (NAACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of large-scale contextual language models has attracted great\ninterest in probing what is encoded in their representations. In this work, we\nconsider a new question: to what extent contextual representations of concrete\nnouns are aligned with corresponding visual representations? We design a\nprobing model that evaluates how effective are text-only representations in\ndistinguishing between matching and non-matching visual representations. Our\nfindings show that language representations alone provide a strong signal for\nretrieving image patches from the correct object categories. Moreover, they are\neffective in retrieving specific instances of image patches; textual context\nplays an important role in this process. Visually grounded language models\nslightly outperform text-only language models in instance retrieval, but\ngreatly under-perform humans. We hope our analyses inspire future research in\nunderstanding and improving the visual capabilities of language models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:28:28 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 17:19:20 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 22:12:40 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 16:40:01 GMT"}, {"version": "v5", "created": "Tue, 13 Apr 2021 16:02:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ilharco", "Gabriel", ""], ["Zellers", "Rowan", ""], ["Farhadi", "Ali", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2005.00624", "submitter": "Yu Zhang", "authors": "Yu Zhang, Yu Meng, Jiaxin Huang, Frank F. Xu, Xuan Wang, Jiawei Han", "title": "Minimally Supervised Categorization of Text with Metadata", "comments": "10 pages; Accepted to SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document categorization, which aims to assign a topic label to each document,\nplays a fundamental role in a wide variety of applications. Despite the success\nof existing studies in conventional supervised document classification, they\nare less concerned with two real problems: (1) \\textit{the presence of\nmetadata}: in many domains, text is accompanied by various additional\ninformation such as authors and tags. Such metadata serve as compelling topic\nindicators and should be leveraged into the categorization framework; (2)\n\\textit{label scarcity}: labeled training samples are expensive to obtain in\nsome cases, where categorization needs to be performed using only a small set\nof annotated data. In recognition of these two challenges, we propose\n\\textsc{MetaCat}, a minimally supervised framework to categorize text with\nmetadata. Specifically, we develop a generative process describing the\nrelationships between words, documents, labels, and metadata. Guided by the\ngenerative model, we embed text and metadata into the same semantic space to\nencode heterogeneous signals. Then, based on the same generative process, we\nsynthesize training samples to address the bottleneck of label scarcity. We\nconduct a thorough evaluation on a wide range of datasets. Experimental results\nprove the effectiveness of \\textsc{MetaCat} over many competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:42:32 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 06:59:54 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Zhang", "Yu", ""], ["Meng", "Yu", ""], ["Huang", "Jiaxin", ""], ["Xu", "Frank F.", ""], ["Wang", "Xuan", ""], ["Han", "Jiawei", ""]]}, {"id": "2005.00626", "submitter": "Adina Williams", "authors": "Adina Williams, Tiago Pimentel, Arya D. McCarthy, Hagen Blix, Eleanor\n  Chodroff, Ryan Cotterell", "title": "Predicting Declension Class from Form and Meaning", "comments": "14 pages, 2 figures, the is the camera-ready version accepted at the\n  2020 Annual Conference of the Association for Computational Linguistics (ACL\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noun lexica of many natural languages are divided into several declension\nclasses with characteristic morphological properties. Class membership is far\nfrom deterministic, but the phonological form of a noun and/or its meaning can\noften provide imperfect clues. Here, we investigate the strength of those\nclues. More specifically, we operationalize this by measuring how much\ninformation, in bits, we can glean about declension class from knowing the form\nand/or meaning of nouns. We know that form and meaning are often also\nindicative of grammatical gender---which, as we quantitatively verify, can\nitself share information with declension class---so we also control for gender.\nWe find for two Indo-European languages (Czech and German) that form and\nmeaning respectively share significant amounts of information with class (and\ncontribute additional information above and beyond gender). The three-way\ninteraction between class, form, and meaning (given gender) is also\nsignificant. Our study is important for two reasons: First, we introduce a new\nmethod that provides additional quantitative support for a classic linguistic\nfinding that form and meaning are relevant for the classification of nouns into\ndeclensions. Secondly, we show not only that individual declensions classes\nvary in the strength of their clues within a language, but also that these\nvariations themselves vary across languages.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:48:48 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 21:15:29 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Williams", "Adina", ""], ["Pimentel", "Tiago", ""], ["McCarthy", "Arya D.", ""], ["Blix", "Hagen", ""], ["Chodroff", "Eleanor", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2005.00628", "submitter": "Richard Yuanzhe Pang", "authors": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi\n  Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, Samuel R. Bowman", "title": "Intermediate-Task Transfer Learning with Pretrained Models for Natural\n  Language Understanding: When and Why Does It Work?", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pretrained models such as BERT have shown large gains across natural\nlanguage understanding tasks, their performance can be improved by further\ntraining the model on a data-rich intermediate task, before fine-tuning it on a\ntarget task. However, it is still poorly understood when and why\nintermediate-task training is beneficial for a given target task. To\ninvestigate this, we perform a large-scale study on the pretrained RoBERTa\nmodel with 110 intermediate-target task combinations. We further evaluate all\ntrained models with 25 probing tasks meant to reveal the specific skills that\ndrive transfer. We observe that intermediate tasks requiring high-level\ninference and reasoning abilities tend to work best. We also observe that\ntarget task performance is strongly correlated with higher-level abilities such\nas coreference resolution. However, we fail to observe more granular\ncorrelations between probing and target task performance, highlighting the need\nfor further work on broad-coverage probing benchmarks. We also observe evidence\nthat the forgetting of knowledge learned during pretraining may limit our\nanalysis, highlighting the need for further work on transfer learning methods\nin these settings.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:49:34 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 05:23:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Pruksachatkun", "Yada", ""], ["Phang", "Jason", ""], ["Liu", "Haokun", ""], ["Htut", "Phu Mon", ""], ["Zhang", "Xiaoyi", ""], ["Pang", "Richard Yuanzhe", ""], ["Vania", "Clara", ""], ["Kann", "Katharina", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2005.00630", "submitter": "Piotr Rybak", "authors": "Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik", "title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a series of Transformer-based models unlocked major\nimprovements in general natural language understanding (NLU) tasks. Such a fast\npace of research would not be possible without general NLU benchmarks, which\nallow for a fair comparison of the proposed methods. However, such benchmarks\nare available only for a handful of languages. To alleviate this issue, we\nintroduce a comprehensive multi-task benchmark for the Polish language\nunderstanding, accompanied by an online leaderboard. It consists of a diverse\nset of tasks, adopted from existing datasets for named entity recognition,\nquestion-answering, textual entailment, and others. We also introduce a new\nsentiment analysis task for the e-commerce domain, named Allegro Reviews (AR).\nTo ensure a common evaluation scheme and promote models that generalize to\ndifferent NLU tasks, the benchmark includes datasets from varying domains and\napplications. Additionally, we release HerBERT, a Transformer-based model\ntrained specifically for the Polish language, which has the best average\nperformance and obtains the best results for three out of nine tasks. Finally,\nwe provide an extensive evaluation, including several standard baselines and\nrecently proposed, multilingual Transformer-based models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:55:40 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Rybak", "Piotr", ""], ["Mroczkowski", "Robert", ""], ["Tracz", "Janusz", ""], ["Gawlik", "Ireneusz", ""]]}, {"id": "2005.00633", "submitter": "Goran Glava\\v{s}", "authors": "Anne Lauscher and Vinit Ravishankar and Ivan Vuli\\'c and Goran\n  Glava\\v{s}", "title": "From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual\n  Transfer with Multilingual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Massively multilingual transformers pretrained with language modeling\nobjectives (e.g., mBERT, XLM-R) have become a de facto default transfer\nparadigm for zero-shot cross-lingual transfer in NLP, offering unmatched\ntransfer performance. Current downstream evaluations, however, verify their\nefficacy predominantly in transfer settings involving languages with sufficient\namounts of pretraining data, and with lexically and typologically close\nlanguages. In this work, we analyze their limitations and show that\ncross-lingual transfer via massively multilingual transformers, much like\ntransfer via cross-lingual word embeddings, is substantially less effective in\nresource-lean scenarios and for distant languages. Our experiments,\nencompassing three lower-level tasks (POS tagging, dependency parsing, NER), as\nwell as two high-level semantic tasks (NLI, QA), empirically correlate transfer\nperformance with linguistic similarity between the source and target languages,\nbut also with the size of pretraining corpora of target languages. We also\ndemonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e.,\nfine-tuning on a few target-language instances after fine-tuning in the source)\nacross the board. This suggests that additional research efforts should be\ninvested to reach beyond the limiting zero-shot conditions.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:04:58 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lauscher", "Anne", ""], ["Ravishankar", "Vinit", ""], ["Vuli\u0107", "Ivan", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2005.00635", "submitter": "Paiheng Xu", "authors": "Zach Wood-Doughty, Paiheng Xu, Xiao Liu, Mark Dredze", "title": "Using Noisy Self-Reports to Predict Twitter User Demographics", "comments": "The first two authors had an equal contribution. Accepted to\n  SocialNLP @ NAACL 2021", "journal-ref": null, "doi": "10.18653/v1/2021.socialnlp-1.11", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational social science studies often contextualize content analysis\nwithin standard demographics. Since demographics are unavailable on many social\nmedia platforms (e.g. Twitter) numerous studies have inferred demographics\nautomatically. Despite many studies presenting proof of concept inference of\nrace and ethnicity, training of practical systems remains elusive since there\nare few annotated datasets. Existing datasets are small, inaccurate, or fail to\ncover the four most common racial and ethnic groups in the United States. We\npresent a method to identify self-reports of race and ethnicity from Twitter\nprofile descriptions. Despite errors inherent in automated supervision, we\nproduce models with good performance when measured on gold standard self-report\nsurvey data. The result is a reproducible method for creating large-scale\ntraining resources for race and ethnicity.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:10:35 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 08:02:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wood-Doughty", "Zach", ""], ["Xu", "Paiheng", ""], ["Liu", "Xiao", ""], ["Dredze", "Mark", ""]]}, {"id": "2005.00636", "submitter": "Jasmijn Bastings", "authors": "Anders S{\\o}gaard and Sebastian Ebert and Jasmijn Bastings and Katja\n  Filippova", "title": "We Need to Talk About Random Splits", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gorman and Bedrick (2019) argued for using random splits rather than standard\nsplits in NLP experiments. We argue that random splits, like standard splits,\nlead to overly optimistic performance estimates. We can also split data in\nbiased or adversarial ways, e.g., training on short sentences and evaluating on\nlong ones. Biased sampling has been used in domain adaptation to simulate\nreal-world drift; this is known as the covariate shift assumption. In NLP,\nhowever, even worst-case splits, maximizing bias, often under-estimate the\nerror observed on new samples of in-domain data, i.e., the data that models\nshould minimally generalize to at test time. This invalidates the covariate\nshift assumption. Instead of using multiple random splits, future benchmarks\nshould ideally include multiple, independent test sets instead; if infeasible,\nwe argue that multiple biased splits leads to more realistic performance\nestimates than multiple random splits.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:14:16 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:54:55 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 12:05:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["S\u00f8gaard", "Anders", ""], ["Ebert", "Sebastian", ""], ["Bastings", "Jasmijn", ""], ["Filippova", "Katja", ""]]}, {"id": "2005.00637", "submitter": "Rajarshi Bhowmik", "authors": "Rajarshi Bhowmik and Gerard de Melo", "title": "Explainable Link Prediction for Emerging Entities in Knowledge Graphs", "comments": "To appear in the proceedings of International Semantic Web\n  Conference, 2020 (ISWC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite their large-scale coverage, cross-domain knowledge graphs invariably\nsuffer from inherent incompleteness and sparsity. Link prediction can alleviate\nthis by inferring a target entity, given a source entity and a query relation.\nRecent embedding-based approaches operate in an uninterpretable latent semantic\nvector space of entities and relations, while path-based approaches operate in\nthe symbolic space, making the inference process explainable. However, these\napproaches typically consider static snapshots of the knowledge graphs,\nseverely restricting their applicability for evolving knowledge graphs with\nnewly emerging entities. To overcome this issue, we propose an inductive\nrepresentation learning framework that is able to learn representations of\npreviously unseen entities. Our method finds reasoning paths between source and\ntarget entities, thereby making the link prediction for unseen entities\ninterpretable and providing support evidence for the inferred link.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:17:37 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 13:38:29 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Bhowmik", "Rajarshi", ""], ["de Melo", "Gerard", ""]]}, {"id": "2005.00642", "submitter": "Wonseok Hwang", "authors": "Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, Minjoon Seo", "title": "Spatial Dependency Parsing for Semi-Structured Document Information\n  Extraction", "comments": "Accepted at Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Extraction (IE) for semi-structured document images is often\napproached as a sequence tagging problem by classifying each recognized input\ntoken into one of the IOB (Inside, Outside, and Beginning) categories. However,\nsuch problem setup has two inherent limitations that (1) it cannot easily\nhandle complex spatial relationships and (2) it is not suitable for highly\nstructured information, which are nevertheless frequently observed in\nreal-world document images. To tackle these issues, we first formulate the IE\ntask as spatial dependency parsing problem that focuses on the relationship\namong text tokens in the documents. Under this setup, we then propose SPADE\n(SPAtial DEpendency parser) that models highly complex spatial relationships\nand an arbitrary number of information layers in the documents in an end-to-end\nmanner. We evaluate it on various kinds of documents such as receipts, name\ncards, forms, and invoices, and show that it achieves a similar or better\nperformance compared to strong baselines including BERT-based IOB taggger.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:59:56 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 22:00:46 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 08:32:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hwang", "Wonseok", ""], ["Yim", "Jinyeong", ""], ["Park", "Seunghyun", ""], ["Yang", "Sohee", ""], ["Seo", "Minjoon", ""]]}, {"id": "2005.00644", "submitter": "Wonseok Hwang", "authors": "Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Minjoon Seo", "title": "Syntactic Question Abstraction and Retrieval for Data-Scarce Semantic\n  Parsing", "comments": "Accepted to AKBC 2020 (conference paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to semantic parsing require a large amount of\nlabeled data, but annotating complex logical forms is costly. Here, we propose\nSyntactic Question Abstraction and Retrieval (SQAR), a method to build a neural\nsemantic parser that translates a natural language (NL) query to a SQL logical\nform (LF) with less than 1,000 annotated examples. SQAR first retrieves a\nlogical pattern from the train data by computing the similarity between NL\nqueries and then grounds a lexical information on the retrieved pattern in\norder to generate the final LF. We validate SQAR by training models using\nvarious small subsets of WikiSQL train data achieving up to 4.9% higher LF\naccuracy compared to the previous state-of-the-art models on WikiSQL test set.\nWe also show that by using query-similarity to retrieve logical pattern, SQAR\ncan leverage a paraphrasing dataset achieving up to 5.9% higher LF accuracy\ncompared to the case where SQAR is trained by using only WikiSQL data. In\ncontrast to a simple pattern classification approach, SQAR can generate unseen\nlogical patterns upon the addition of new examples without re-training the\nmodel. We also discuss an ideal way to create cost efficient and robust train\ndatasets when the data distribution can be approximated under a data-hungry\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:05:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hwang", "Wonseok", ""], ["Yim", "Jinyeong", ""], ["Park", "Seunghyun", ""], ["Seo", "Minjoon", ""]]}, {"id": "2005.00646", "submitter": "Bill Yuchen Lin", "authors": "Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan,\n  Xiang Ren", "title": "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question\n  Answering", "comments": "Accepted to EMNLP 2020. Project page:\n  https://github.com/INK-USC/MHGRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on augmenting question answering (QA) models with external\nknowledge (e.g., knowledge graphs) either struggle to model multi-hop relations\nefficiently, or lack transparency into the model's prediction rationale. In\nthis paper, we propose a novel knowledge-aware approach that equips pre-trained\nlanguage models (PTLMs) with a multi-hop relational reasoning module, named\nmulti-hop graph relation network (MHGRN). It performs multi-hop,\nmulti-relational reasoning over subgraphs extracted from external knowledge\ngraphs. The proposed reasoning module unifies path-based reasoning methods and\ngraph neural networks to achieve better interpretability and scalability. We\nalso empirically show its effectiveness and scalability on CommonsenseQA and\nOpenbookQA datasets, and interpret its behaviors with case studies.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:10:26 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 07:12:35 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Feng", "Yanlin", ""], ["Chen", "Xinyue", ""], ["Lin", "Bill Yuchen", ""], ["Wang", "Peifeng", ""], ["Yan", "Jun", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00649", "submitter": "Katherine Keith", "authors": "Katherine A. Keith, David Jensen, Brendan O'Connor", "title": "Text and Causal Inference: A Review of Using Text to Remove Confounding\n  from Causal Estimates", "comments": "Accepted to ACL 2020", "journal-ref": "ACL 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of computational social science aim to infer causal\nconclusions from non-experimental data. Such observational data often contains\nconfounders, variables that influence both potential causes and potential\neffects. Unmeasured or latent confounders can bias causal estimates, and this\nhas motivated interest in measuring potential confounders from observed text.\nFor example, an individual's entire history of social media posts or the\ncontent of a news article could provide a rich measurement of multiple\nconfounders. Yet, methods and applications for this problem are scattered\nacross different communities and evaluation practices are inconsistent. This\nreview is the first to gather and categorize these examples and provide a guide\nto data-processing and evaluation decisions. Despite increased attention on\nadjusting for confounding using text, there are still many open problems, which\nwe highlight in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:20:49 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Keith", "Katherine A.", ""], ["Jensen", "David", ""], ["O'Connor", "Brendan", ""]]}, {"id": "2005.00652", "submitter": "Bhargavi Paranjape", "authors": "Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi,\n  Luke Zettlemoyer", "title": "An Information Bottleneck Approach for Controlling Conciseness in\n  Rationale Extraction", "comments": "EMNLP 2020 main track accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisions of complex language understanding models can be rationalized by\nlimiting their inputs to a relevant subsequence of the original text. A\nrationale should be as concise as possible without significantly degrading task\nperformance, but this balance can be difficult to achieve in practice. In this\npaper, we show that it is possible to better manage this trade-off by\noptimizing a bound on the Information Bottleneck (IB) objective. Our fully\nunsupervised approach jointly learns an explainer that predicts sparse binary\nmasks over sentences, and an end-task predictor that considers only the\nextracted rationale. Using IB, we derive a learning objective that allows\ndirect control of mask sparsity levels through a tunable sparse prior.\nExperiments on ERASER benchmark tasks demonstrate significant gains over\nnorm-minimization techniques for both task performance and agreement with human\nrationales. Furthermore, we find that in the semi-supervised setting, a modest\namount of gold rationales (25% of training examples) closes the gap with a\nmodel that uses the full input.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:26:41 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:57:54 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 04:38:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Paranjape", "Bhargavi", ""], ["Joshi", "Mandar", ""], ["Thickstun", "John", ""], ["Hajishirzi", "Hannaneh", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2005.00660", "submitter": "Peter Clark", "authors": "Sumithra Bhakthavatsalam, Chloe Anastasiades, Peter Clark", "title": "GenericsKB: A Knowledge Base of Generic Statements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new resource for the NLP community, namely a large (3.5M+\nsentence) knowledge base of *generic statements*, e.g., \"Trees remove carbon\ndioxide from the atmosphere\", collected from multiple corpora. This is the\nfirst large resource to contain *naturally occurring* generic sentences, as\nopposed to extracted or crowdsourced triples, and thus is rich in high-quality,\ngeneral, semantically complete statements. All GenericsKB sentences are\nannotated with their topical term, surrounding context (sentences), and a\n(learned) confidence. We also release GenericsKB-Best (1M+ sentences),\ncontaining the best-quality generics in GenericsKB augmented with selected,\nsynthesized generics from WordNet and ConceptNet. In tests on two existing\ndatasets requiring multihop reasoning (OBQA and QASC), we find using GenericsKB\ncan result in higher scores and better explanations than using a much larger\ncorpus. This demonstrates that GenericsKB can be a useful resource for NLP\napplications, as well as providing data for linguistic studies of generics and\ntheir semantics. GenericsKB is available at\nhttps://allenai.org/data/genericskb.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:08:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bhakthavatsalam", "Sumithra", ""], ["Anastasiades", "Chloe", ""], ["Clark", "Peter", ""]]}, {"id": "2005.00661", "submitter": "Shashi Narayan", "authors": "Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan McDonald", "title": "On Faithfulness and Factuality in Abstractive Summarization", "comments": "ACL 2020, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the standard likelihood training and approximate\ndecoding objectives in neural text generation models lead to less human-like\nresponses for open-ended tasks such as language modeling and story generation.\nIn this paper we have analyzed limitations of these models for abstractive\ndocument summarization and found that these models are highly prone to\nhallucinate content that is unfaithful to the input document. We conducted a\nlarge scale human evaluation of several neural abstractive summarization\nsystems to better understand the types of hallucinations they produce. Our\nhuman annotators found substantial amounts of hallucinated content in all model\ngenerated summaries. However, our analysis does show that pretrained models are\nbetter summarizers not only in terms of raw metrics, i.e., ROUGE, but also in\ngenerating faithful and factual summaries as evaluated by humans. Furthermore,\nwe show that textual entailment measures better correlate with faithfulness\nthan standard metrics, potentially leading the way to automatic evaluation\nmetrics as well as training and decoding criteria.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:09:16 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Maynez", "Joshua", ""], ["Narayan", "Shashi", ""], ["Bohnet", "Bernd", ""], ["McDonald", "Ryan", ""]]}, {"id": "2005.00663", "submitter": "Xi Ye", "authors": "Xi Ye, Qiaochu Chen, Isil Dillig and Greg Durrett", "title": "Benchmarking Multimodal Regex Synthesis with Complex Structures", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing datasets for regular expression (regex) generation from natural\nlanguage are limited in complexity; compared to regex tasks that users post on\nStackOverflow, the regexes in these datasets are simple, and the language used\nto describe them is not diverse. We introduce StructuredRegex, a new regex\nsynthesis dataset differing from prior ones in three aspects. First, to obtain\nstructurally complex and realistic regexes, we generate the regexes using a\nprobabilistic grammar with pre-defined macros observed from real-world\nStackOverflow posts. Second, to obtain linguistically diverse natural language\ndescriptions, we show crowdworkers abstract depictions of the underlying regex\nand ask them to describe the pattern they see, rather than having them\nparaphrase synthetic language. Third, we augment each regex example with a\ncollection of strings that are and are not matched by the ground truth regex,\nsimilar to how real users give examples. Our quantitative and qualitative\nanalysis demonstrates the advantages of StructuredRegex over prior datasets.\nFurther experimental results using various multimodal synthesis techniques\nhighlight the challenge presented by our dataset, including non-local\nconstraints and multi-modal inputs.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:16:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ye", "Xi", ""], ["Chen", "Qiaochu", ""], ["Dillig", "Isil", ""], ["Durrett", "Greg", ""]]}, {"id": "2005.00669", "submitter": "Tassilo Klein", "authors": "Tassilo Klein and Moin Nabi", "title": "Contrastive Self-Supervised Learning for Commonsense Reasoning", "comments": "To appear at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised method to solve Pronoun Disambiguation and\nWinograd Schema Challenge problems. Our approach exploits the characteristic\nstructure of training corpora related to so-called \"trigger\" words, which are\nresponsible for flipping the answer in pronoun disambiguation. We achieve such\ncommonsense reasoning by constructing pair-wise contrastive auxiliary\npredictions. To this end, we leverage a mutual exclusive loss regularized by a\ncontrastive margin. Our architecture is based on the recently introduced\ntransformer networks, BERT, that exhibits strong performance on many NLP\nbenchmarks. Empirical results show that our method alleviates the limitation of\ncurrent supervised approaches for commonsense reasoning. This study opens up\navenues for exploiting inexpensive self-supervision to achieve performance gain\nin commonsense reasoning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:39:09 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Klein", "Tassilo", ""], ["Nabi", "Moin", ""]]}, {"id": "2005.00670", "submitter": "Morihiro Mizutani", "authors": "Morihiro Mizutani, Akifumi Okuno, Geewook Kim, Hidetoshi Shimodaira", "title": "Stochastic Neighbor Embedding of Multimodal Relational Data for\n  Image-Text Simultaneous Visualization", "comments": "20 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal relational data analysis has become of increasing importance in\nrecent years, for exploring across different domains of data, such as images\nand their text tags obtained from social networking services (e.g., Flickr). A\nvariety of data analysis methods have been developed for visualization; to give\nan example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional\nfeature vectors so that their similarities keep those of the observed data\nvectors. However, t-SNE is designed only for a single domain of data but not\nfor multimodal data; this paper aims at visualizing multimodal relational data\nconsisting of data vectors in multiple domains with relations across these\nvectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic\nNeighbor Embedding (MR-SNE), that (1) first computes augmented relations, where\nwe observe the relations across domains and compute those within each of\ndomains via the observed data vectors, and (2) jointly embeds the augmented\nrelations to a low-dimensional space. Through visualization of Flickr and\nAnimal with Attributes 2 datasets, proposed MR-SNE is compared with other graph\nembedding-based approaches; MR-SNE demonstrates the promising performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:39:29 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mizutani", "Morihiro", ""], ["Okuno", "Akifumi", ""], ["Kim", "Geewook", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "2005.00672", "submitter": "Valentin Hofmann", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\\\"utze", "title": "DagoBERT: Generating Derivational Morphology with a Pretrained Language\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can pretrained language models (PLMs) generate derivationally complex words?\nWe present the first study investigating this question, taking BERT as the\nexample PLM. We examine BERT's derivational capabilities in different settings,\nranging from using the unmodified pretrained model to full finetuning. Our best\nmodel, DagoBERT (Derivationally and generatively optimized BERT), clearly\noutperforms the previous state of the art in derivation generation (DG).\nFurthermore, our experiments show that the input segmentation crucially impacts\nBERT's derivational knowledge, suggesting that the performance of PLMs could be\nfurther improved if a morphologically informed vocabulary of units were used.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 01:26:46 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 16:57:31 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Hofmann", "Valentin", ""], ["Pierrehumbert", "Janet B.", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2005.00675", "submitter": "Renjie Zheng", "authors": "Renjie Zheng and Mingbo Ma and Baigong Zheng and Kaibo Liu and Liang\n  Huang", "title": "Opportunistic Decoding with Timely Correction for Simultaneous\n  Translation", "comments": "accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous translation has many important application scenarios and\nattracts much attention from both academia and industry recently. Most existing\nframeworks, however, have difficulties in balancing between the translation\nquality and latency, i.e., the decoding policy is usually either too aggressive\nor too conservative. We propose an opportunistic decoding technique with timely\ncorrection ability, which always (over-)generates a certain mount of extra\nwords at each step to keep the audience on track with the latest information.\nAt the same time, it also corrects, in a timely fashion, the mistakes in the\nformer overgenerated words when observing more source context to ensure high\ntranslation quality. Experiments show our technique achieves substantial\nreduction in latency and up to +3.1 increase in BLEU, with revision rate under\n8% in Chinese-to-English and English-to-Chinese translation.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 01:41:02 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zheng", "Renjie", ""], ["Ma", "Mingbo", ""], ["Zheng", "Baigong", ""], ["Liu", "Kaibo", ""], ["Huang", "Liang", ""]]}, {"id": "2005.00683", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, Xiang Ren", "title": "Birds have four legs?! NumerSense: Probing Numerical Commonsense\n  Knowledge of Pre-trained Language Models", "comments": "To appear in Proceedings of EMNLP 2020. Project page:\n  http://inklab.usc.edu/NumerSense/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works show that pre-trained language models (PTLMs), such as BERT,\npossess certain commonsense and factual knowledge. They suggest that it is\npromising to use PTLMs as \"neural knowledge bases\" via predicting masked words.\nSurprisingly, we find that this may not work for numerical commonsense\nknowledge (e.g., a bird usually has two legs). In this paper, we investigate\nwhether and to what extent we can induce numerical commonsense knowledge from\nPTLMs as well as the robustness of this process. To study this, we introduce a\nnovel probing task with a diagnostic dataset, NumerSense, containing 13.6k\nmasked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our\nanalysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly\non the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with\ndistant supervision brings some improvement; (3) the best supervised model\nstill performs poorly as compared to human performance (54.06% vs 96.3% in\naccuracy).\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 02:47:02 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 00:42:25 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Lee", "Seyeon", ""], ["Khanna", "Rahul", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00689", "submitter": "Ziyu Yao", "authors": "Ziyu Yao, Yiqi Tang, Wen-tau Yih, Huan Sun, Yu Su", "title": "An Imitation Game for Learning Semantic Parsers from User Interaction", "comments": "Accepted to EMNLP 2020. 21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widely successful applications, bootstrapping and fine-tuning\nsemantic parsers are still a tedious process with challenges such as costly\ndata annotation and privacy risks. In this paper, we suggest an alternative,\nhuman-in-the-loop methodology for learning semantic parsers directly from\nusers. A semantic parser should be introspective of its uncertainties and\nprompt for user demonstration when uncertain. In doing so it also gets to\nimitate the user behavior and continue improving itself autonomously with the\nhope that eventually it may become as good as the user in interpreting their\nquestions. To combat the sparsity of demonstration, we propose a novel\nannotation-efficient imitation learning algorithm, which iteratively collects\nnew datasets by mixing demonstrated states and confident predictions and\nre-trains the semantic parser in a Dataset Aggregation fashion (Ross et al.,\n2011). We provide a theoretical analysis of its cost bound and also empirically\ndemonstrate its promising performance on the text-to-SQL problem. Code will be\navailable at https://github.com/sunlab-osu/MISP.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 03:30:49 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 03:46:18 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 18:31:26 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yao", "Ziyu", ""], ["Tang", "Yiqi", ""], ["Yih", "Wen-tau", ""], ["Sun", "Huan", ""], ["Su", "Yu", ""]]}, {"id": "2005.00691", "submitter": "Peifeng Wang", "authors": "Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren", "title": "Connecting the Dots: A Knowledgeable Path Generator for Commonsense\n  Question Answering", "comments": "12 pages, 4 figures. Accepted to EMNLP'20 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense question answering (QA) requires background knowledge which is\nnot explicitly stated in a given context. Prior works use commonsense knowledge\ngraphs (KGs) to obtain this knowledge for reasoning. However, relying entirely\non these KGs may not suffice, considering their limited coverage and the\ncontextual dependence of their knowledge. In this paper, we augment a general\ncommonsense QA framework with a knowledgeable path generator. By extrapolating\nover existing paths in a KG with a state-of-the-art language model, our\ngenerator learns to connect a pair of entities in text with a dynamic, and\npotentially novel, multi-hop relational path. Such paths can provide structured\nevidence for solving commonsense questions without fine-tuning the path\ngenerator. Experiments on two datasets show the superiority of our method over\nprevious works which fully rely on knowledge from KGs (with up to 6%\nimprovement in accuracy), across various amounts of training data. Further\nevaluation suggests that the generated paths are typically interpretable,\nnovel, and relevant to the task.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 03:53:21 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 16:38:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wang", "Peifeng", ""], ["Peng", "Nanyun", ""], ["Ilievski", "Filip", ""], ["Szekely", "Pedro", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00692", "submitter": "Xingyu Fu", "authors": "Xingyu Fu, Weijia Shi, Xiaodong Yu, Zian Zhao, Dan Roth", "title": "Design Challenges in Low-resource Cross-lingual Entity Linking", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual Entity Linking (XEL), the problem of grounding mentions of\nentities in a foreign language text into an English knowledge base such as\nWikipedia, has seen a lot of research in recent years, with a range of\npromising techniques. However, current techniques do not rise to the challenges\nintroduced by text in low-resource languages (LRL) and, surprisingly, fail to\ngeneralize to text not taken from Wikipedia, on which they are usually trained.\n  This paper provides a thorough analysis of low-resource XEL techniques,\nfocusing on the key step of identifying candidate English Wikipedia titles that\ncorrespond to a given foreign language mention. Our analysis indicates that\ncurrent methods are limited by their reliance on Wikipedia's interlanguage\nlinks and thus suffer when the foreign language's Wikipedia is small. We\nconclude that the LRL setting requires the use of outside-Wikipedia\ncross-lingual resources and present a simple yet effective zero-shot XEL\nsystem, QuEL, that utilizes search engines query logs. With experiments on 25\nlanguages, QuEL~shows an average increase of 25\\% in gold candidate recall and\nof 13\\% in end-to-end linking accuracy over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:00:26 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 06:27:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Fu", "Xingyu", ""], ["Shi", "Weijia", ""], ["Yu", "Xiaodong", ""], ["Zhao", "Zian", ""], ["Roth", "Dan", ""]]}, {"id": "2005.00693", "submitter": "Gerard de Melo", "authors": "Abu Shoeb, Gerard de Melo", "title": "Are Emojis Emotional? A Study to Understand the Association between\n  Emojis and Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the growing ubiquity of emojis in language, there is a need for methods\nand resources that shed light on their meaning and communicative role. One\nconspicuous aspect of emojis is their use to convey affect in ways that may\notherwise be non-trivial to achieve. In this paper, we seek to explore the\nconnection between emojis and emotions by means of a new dataset consisting of\nhuman-solicited association ratings. We additionally conduct experiments to\nassess to what extent such associations can be inferred from existing data,\nsuch that similar associations can be predicted for a larger set of emojis. Our\nexperiments show that this succeeds when high-quality word-level information is\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:04:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shoeb", "Abu", ""], ["de Melo", "Gerard", ""]]}, {"id": "2005.00696", "submitter": "Tsung-Yen Yang", "authors": "Tsung-Yen Yang and Andrew S. Lan and Karthik Narasimhan", "title": "Robust and Interpretable Grounding of Spatial References with Relation\n  Networks", "comments": "Findings of Empirical Methods in Natural Language Processing (EMNLP)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations of spatial references in natural language is a key\nchallenge in tasks like autonomous navigation and robotic manipulation. Recent\nwork has investigated various neural architectures for learning multi-modal\nrepresentations for spatial concepts. However, the lack of explicit reasoning\nover entities makes such approaches vulnerable to noise in input text or state\nobservations. In this paper, we develop effective models for understanding\nspatial references in text that are robust and interpretable, without\nsacrificing performance. We design a text-conditioned \\textit{relation network}\nwhose parameters are dynamically computed with a cross-modal attention module\nto capture fine-grained spatial relations between entities. This design choice\nprovides interpretability of learned intermediate outputs. Experiments across\nthree tasks demonstrate that our model achieves superior performance, with a\n17\\% improvement in predicting goal locations and a 15\\% improvement in\nrobustness compared to state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:11:33 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 04:05:00 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yang", "Tsung-Yen", ""], ["Lan", "Andrew S.", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2005.00697", "submitter": "Qingqing Cao", "authors": "Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan\n  Balasubramanian", "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question\n  Answering", "comments": "ACL 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based QA models use input-wide self-attention -- i.e. across both\nthe question and the input passage -- at all layers, causing them to be slow\nand memory-intensive. It turns out that we can get by without input-wide\nself-attention at all layers, especially in the lower layers. We introduce\nDeFormer, a decomposed transformer, which substitutes the full self-attention\nwith question-wide and passage-wide self-attentions in the lower layers. This\nallows for question-independent processing of the input text representations,\nwhich in turn enables pre-computing passage representations reducing runtime\ncompute drastically. Furthermore, because DeFormer is largely similar to the\noriginal model, we can initialize DeFormer with the pre-training weights of a\nstandard transformer, and directly fine-tune on the target QA dataset. We show\nDeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and\nwith simple distillation-based losses they incur only a 1% drop in accuracy. We\nopen source the code at https://github.com/StonyBrookNLP/deformer.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:28:22 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Cao", "Qingqing", ""], ["Trivedi", "Harsh", ""], ["Balasubramanian", "Aruna", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2005.00699", "submitter": "Jieyu Zhao", "authors": "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang and\n  Ahmed Hassan Awadallah", "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual representations embed words from many languages into a single\nsemantic space such that words with similar meanings are close to each other\nregardless of the language. These embeddings have been widely used in various\nsettings, such as cross-lingual transfer, where a natural language processing\n(NLP) model trained on one language is deployed to another language. While the\ncross-lingual transfer techniques are powerful, they carry gender bias from the\nsource to target languages. In this paper, we study gender bias in multilingual\nembeddings and how it affects transfer learning for NLP applications. We create\na multilingual dataset for bias analysis and propose several ways for\nquantifying bias in multilingual representations from both the intrinsic and\nextrinsic perspectives. Experimental results show that the magnitude of bias in\nthe multilingual representations changes differently when we align the\nembeddings to different target spaces and that the alignment direction can also\nhave an influence on the bias in transfer learning. We further provide\nrecommendations for using the multilingual word representations for downstream\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:34:37 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhao", "Jieyu", ""], ["Mukherjee", "Subhabrata", ""], ["Hosseini", "Saghar", ""], ["Chang", "Kai-Wei", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2005.00700", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind\n  Tafjord, Peter Clark, Hannaneh Hajishirzi", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System", "comments": "EMNLP 2020 (Findings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) tasks have been posed using a variety of formats,\nsuch as extractive span selection, multiple choice, etc. This has led to\nformat-specialized models, and even to an implicit division in the QA\ncommunity. We argue that such boundaries are artificial and perhaps\nunnecessary, given the reasoning abilities we seek to teach are not governed by\nthe format. As evidence, we use the latest advances in language modeling to\nbuild a single pre-trained QA model, UnifiedQA, that performs surprisingly well\nacross 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par\nwith 9 different models that were trained on individual datasets themselves.\nEven when faced with 12 unseen datasets of observed formats, UnifiedQA performs\nsurprisingly well, showing strong generalization from its out-of-format\ntraining data. Finally, simply fine-tuning this pre-trained QA model into\nspecialized models results in a new state of the art on 6 datasets,\nestablishing UnifiedQA as a strong starting point for building QA systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:42:14 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:46:48 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 03:12:45 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Khashabi", "Daniel", ""], ["Min", "Sewon", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Tafjord", "Oyvind", ""], ["Clark", "Peter", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2005.00701", "submitter": "Yixin Cao", "authors": "Yixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu and\n  Tat-Seng Chua", "title": "Expertise Style Transfer: A New Task Towards Better Communication\n  between Experts and Laymen", "comments": "11 pages, 6 figures; To appear in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The curse of knowledge can impede communication between experts and laymen.\nWe propose a new task of expertise style transfer and contribute a manually\nannotated dataset with the goal of alleviating such cognitive biases. Solving\nthis task not only simplifies the professional language, but also improves the\naccuracy and expertise level of laymen descriptions using simple words. This is\na challenging task, unaddressed in previous work, as it requires the models to\nhave expert intelligence in order to modify text with a deep understanding of\ndomain knowledge and structures. We establish the benchmark performance of five\nstate-of-the-art models for style transfer and text simplification. The results\ndemonstrate a significant gap between machine and human performance. We also\ndiscuss the challenges of automatic evaluation, to provide insights into future\nresearch directions. The dataset is publicly available at\nhttps://srhthu.github.io/expertise-style-transfer.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:50:20 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Cao", "Yixin", ""], ["Shui", "Ruihao", ""], ["Pan", "Liangming", ""], ["Kan", "Min-Yen", ""], ["Liu", "Zhiyuan", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2005.00702", "submitter": "Asad Mahmood", "authors": "Asad Mahmood, Zubair Shafiq and Padmini Srinivasan", "title": "A Girl Has A Name: Detecting Authorship Obfuscation", "comments": "9 pages, 4 figures, 2 tables, ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution aims to identify the author of a text based on the\nstylometric analysis. Authorship obfuscation, on the other hand, aims to\nprotect against authorship attribution by modifying a text's style. In this\npaper, we evaluate the stealthiness of state-of-the-art authorship obfuscation\nmethods under an adversarial threat model. An obfuscator is stealthy to the\nextent an adversary finds it challenging to detect whether or not a text\nmodified by the obfuscator is obfuscated - a decision that is key to the\nadversary interested in authorship attribution. We show that the existing\nauthorship obfuscation methods are not stealthy as their obfuscated texts can\nbe identified with an average F1 score of 0.87. The reason for the lack of\nstealthiness is that these obfuscators degrade text smoothness, as ascertained\nby neural language models, in a detectable manner. Our results highlight the\nneed to develop stealthy authorship obfuscation methods that can better protect\nthe identity of an author seeking anonymity.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:52:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mahmood", "Asad", ""], ["Shafiq", "Zubair", ""], ["Srinivasan", "Padmini", ""]]}, {"id": "2005.00705", "submitter": "Thuy Vu", "authors": "Thuy Vu and Alessandro Moschitti", "title": "AVA: an Automatic eValuation Approach to Question Answering Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AVA, an automatic evaluation approach for Question Answering,\nwhich given a set of questions associated with Gold Standard answers, can\nestimate system Accuracy. AVA uses Transformer-based language models to encode\nquestion, answer, and reference text. This allows for effectively measuring the\nsimilarity between the reference and an automatic answer, biased towards the\nquestion semantics. To design, train and test AVA, we built multiple large\ntraining, development, and test sets on both public and industrial benchmarks.\nOur innovative solutions achieve up to 74.7% in F1 score in predicting human\njudgement for single answers. Additionally, AVA can be used to evaluate the\noverall system Accuracy with an RMSE, ranging from 0.02 to 0.09, depending on\nthe availability of multiple references.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 05:00:16 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2005.00706", "submitter": "Frank F. Xu", "authors": "Frank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neubig, Yonatan\n  Bisk, Nan Duan", "title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking\n  Videos", "comments": "Accepted by NLP Beyond Text - First International Workshop on Natural\n  Language Processing Beyond Text @ EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching instructional videos are often used to learn about procedures. Video\ncaptioning is one way of automatically collecting such knowledge. However, it\nprovides only an indirect, overall evaluation of multimodal models with no\nfiner-grained quantitative measure of what they have learned. We propose\ninstead, a benchmark of structured procedural knowledge extracted from cooking\nvideos. This work is complementary to existing tasks, but requires models to\nproduce interpretable structured knowledge in the form of verb-argument tuples.\nOur manually annotated open-vocabulary resource includes 356 instructional\ncooking videos and 15,523 video clip/sentence-level annotations. Our analysis\nshows that the proposed task is challenging and standard modeling approaches\nlike unsupervised segmentation, semantic role labeling, and visual action\ndetection perform poorly when forced to predict every action of a procedure in\na structured form.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 05:15:20 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:54:27 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Xu", "Frank F.", ""], ["Ji", "Lei", ""], ["Shi", "Botian", ""], ["Du", "Junyi", ""], ["Neubig", "Graham", ""], ["Bisk", "Yonatan", ""], ["Duan", "Nan", ""]]}, {"id": "2005.00719", "submitter": "Abhilasha Ravichander", "authors": "Abhilasha Ravichander, Yonatan Belinkov, Eduard Hovy", "title": "Probing the Probing Paradigm: Does Probing Accuracy Entail Task\n  Relevance?", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although neural models have achieved impressive results on several NLP\nbenchmarks, little is understood about the mechanisms they use to perform\nlanguage tasks. Thus, much recent attention has been devoted to analyzing the\nsentence representations learned by neural encoders, through the lens of\n`probing' tasks. However, to what extent was the information encoded in\nsentence representations, as discovered through a probe, actually used by the\nmodel to perform its task? In this work, we examine this probing paradigm\nthrough a case study in Natural Language Inference, showing that models can\nlearn to encode linguistic properties even if they are not needed for the task\non which the model was trained. We further identify that pretrained word\nembeddings play a considerable role in encoding these properties rather than\nthe training task itself, highlighting the importance of careful controls when\ndesigning probing experiments. Finally, through a set of controlled synthetic\ntasks, we demonstrate models can encode these properties considerably above\nchance-level even when distributed in the data as random noise, calling into\nquestion the interpretation of absolute claims on probing tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:19:20 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 18:50:31 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 18:38:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ravichander", "Abhilasha", ""], ["Belinkov", "Yonatan", ""], ["Hovy", "Eduard", ""]]}, {"id": "2005.00724", "submitter": "Sanjay Subramanian", "authors": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer\n  Singh, Jonathan Berant, Matt Gardner", "title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "comments": "ACL 2020; first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural module networks (NMNs) are a popular approach for modeling\ncompositionality: they achieve high accuracy when applied to problems in\nlanguage and vision, while reflecting the compositional structure of the\nproblem in the network architecture. However, prior work implicitly assumed\nthat the structure of the network modules, describing the abstract reasoning\nprocess, provides a faithful explanation of the model's reasoning; that is,\nthat all modules perform their intended behaviour. In this work, we propose and\nconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2\nand DROP, two datasets which require composing multiple reasoning steps. We\nfind that the intermediate outputs differ from the expected output,\nillustrating that the network structure does not provide a faithful explanation\nof model behaviour. To remedy that, we train the model with auxiliary\nsupervision and propose particular choices for module architecture that yield\nmuch better faithfulness, at a minimal cost to accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:50:35 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 15:52:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Subramanian", "Sanjay", ""], ["Bogin", "Ben", ""], ["Gupta", "Nitish", ""], ["Wolfson", "Tomer", ""], ["Singh", "Sameer", ""], ["Berant", "Jonathan", ""], ["Gardner", "Matt", ""]]}, {"id": "2005.00728", "submitter": "Jesse Thomason", "authors": "Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz,\n  Jianfeng Gao", "title": "RMM: A Recursive Mental Model for Dialog Navigation", "comments": "Findings of Empirical Methods in Natural Language Processing (EMNLP\n  Findings), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-guided robots must be able to both ask humans questions and\nunderstand answers. Much existing work focuses only on the latter. In this\npaper, we go beyond instruction following and introduce a two-agent task where\none agent navigates and asks questions that a second, guiding agent answers.\nInspired by theory of mind, we propose the Recursive Mental Model (RMM). The\nnavigating agent models the guiding agent to simulate answers given candidate\ngenerated questions. The guiding agent in turn models the navigating agent to\nsimulate navigation steps it would take to generate answers. We use the\nprogress agents make towards the goal as a reinforcement learning reward signal\nto directly inform not only navigation actions, but also both question and\nanswer generation. We demonstrate that RMM enables better generalization to\nnovel environments. Interlocutor modelling may be a way forward for human-agent\ndialogue where robots need to both ask and answer questions.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:57:14 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:16:27 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Roman", "Homero Roman", ""], ["Bisk", "Yonatan", ""], ["Thomason", "Jesse", ""], ["Celikyilmaz", "Asli", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2005.00730", "submitter": "Nazneen Fatema Rajani", "authors": "Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy\n  Weiss, Aadit Vyas, Abhijit Gupta, Caiming XIong, Richard Socher, Dragomir\n  Radev", "title": "ESPRIT: Explaining Solutions to Physical Reasoning Tasks", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks lack the ability to reason about qualitative physics and so\ncannot generalize to scenarios and tasks unseen during training. We propose\nESPRIT, a framework for commonsense reasoning about qualitative physics in\nnatural language that generates interpretable descriptions of physical events.\nWe use a two-step approach of first identifying the pivotal physical events in\nan environment and then generating natural language descriptions of those\nevents using a data-to-text approach. Our framework learns to generate\nexplanations of how the physical simulation will causally evolve so that an\nagent or a human can easily reason about a solution using those interpretable\ndescriptions. Human evaluations indicate that ESPRIT produces crucial\nfine-grained details and has high coverage of physical concepts compared to\neven human annotations. Dataset, code and documentation are available at\nhttps://github.com/salesforce/esprit.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 07:03:06 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 00:24:13 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rajani", "Nazneen Fatema", ""], ["Zhang", "Rui", ""], ["Tan", "Yi Chern", ""], ["Zheng", "Stephan", ""], ["Weiss", "Jeremy", ""], ["Vyas", "Aadit", ""], ["Gupta", "Abhijit", ""], ["XIong", "Caiming", ""], ["Socher", "Richard", ""], ["Radev", "Dragomir", ""]]}, {"id": "2005.00742", "submitter": "Weiqiu You", "authors": "Weiqiu You, Simeng Sun, Mohit Iyyer", "title": "Hard-Coded Gaussian Attention for Neural Machine Translation", "comments": "ACL 2020 Camera Ready (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has questioned the importance of the Transformer's multi-headed\nattention for achieving high translation quality. We push further in this\ndirection by developing a \"hard-coded\" attention variant without any learned\nparameters. Surprisingly, replacing all learned self-attention heads in the\nencoder and decoder with fixed, input-agnostic Gaussian distributions minimally\nimpacts BLEU scores across four different language pairs. However, additionally\nhard-coding cross attention (which connects the decoder to the encoder)\nsignificantly lowers BLEU, suggesting that it is more important than\nself-attention. Much of this BLEU drop can be recovered by adding just a single\nlearned cross attention head to an otherwise hard-coded Transformer. Taken as a\nwhole, our results offer insight into which components of the Transformer are\nactually important, which we hope will guide future work into the development\nof simpler and more efficient attention-based models.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 08:16:13 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["You", "Weiqiu", ""], ["Sun", "Simeng", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2005.00743", "submitter": "Yi Tay", "authors": "Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only $60\\%$ faster but also improves perplexity by a\nrelative $3.5\\%$. Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 08:16:19 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 12:16:06 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 12:19:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tay", "Yi", ""], ["Bahri", "Dara", ""], ["Metzler", "Donald", ""], ["Juan", "Da-Cheng", ""], ["Zhao", "Zhe", ""], ["Zheng", "Che", ""]]}, {"id": "2005.00766", "submitter": "Nora Kassner", "authors": "Nora Kassner and Hinrich Sch\\\"utze", "title": "BERT-kNN: Adding a kNN Search Component to Pretrained Language Models\n  for Better QA", "comments": "to appear in EMNLP Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve\nlanguage model performance. We show that this idea is beneficial for\nopen-domain question answering (QA). To improve the recall of facts encountered\nduring training, we combine BERT (Devlin et al., 2019) with a traditional\ninformation retrieval step (IR) and a kNN search over a large datastore of an\nembedded text collection. Our contributions are as follows: i) BERT-kNN\noutperforms BERT on cloze-style QA by large margins without any further\ntraining. ii) We show that BERT often identifies the correct response category\n(e.g., US city), but only kNN recovers the factually correct answer (e.g.,\n\"Miami\"). iii) Compared to BERT, BERT-kNN excels for rare facts. iv) BERT-kNN\ncan easily handle facts not covered by BERT's training set, e.g., recent\nevents.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 09:34:42 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 18:44:05 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kassner", "Nora", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2005.00770", "submitter": "Tu Vu", "authors": "Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam\n  Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer", "title": "Exploring and Predicting Transferability across NLP Tasks", "comments": "Accepted as a conference paper at EMNLP 2020, 45 pages, 3 figures, 34\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in NLP demonstrate the effectiveness of training large-scale\nlanguage models and transferring them to downstream tasks. Can fine-tuning\nthese models on tasks other than language modeling further improve performance?\nIn this paper, we conduct an extensive study of the transferability between 33\nNLP tasks across three broad classes of problems (text classification, question\nanswering, and sequence labeling). Our results show that transfer learning is\nmore beneficial than previously thought, especially when target task data is\nscarce, and can improve performance even when the source task is small or\ndiffers substantially from the target task (e.g., part-of-speech tagging\ntransfers well to the DROP QA dataset). We also develop task embeddings that\ncan be used to predict the most transferable source tasks for a given target\ntask, and we validate their effectiveness in experiments controlled for source\nand target data size. Overall, our experiments reveal that factors such as\nsource data size, task and domain similarity, and task complexity all play a\nrole in determining transferability.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 09:39:36 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 18:49:48 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Vu", "Tu", ""], ["Wang", "Tong", ""], ["Munkhdalai", "Tsendsuren", ""], ["Sordoni", "Alessandro", ""], ["Trischler", "Adam", ""], ["Mattarella-Micke", "Andrew", ""], ["Maji", "Subhransu", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2005.00771", "submitter": "Xiang Li", "authors": "Michael Boratko, Xiang Lorraine Li, Rajarshi Das, Tim O'Gorman, Dan\n  Le, Andrew McCallum", "title": "ProtoQA: A Question Answering Dataset for Prototypical Common-Sense\n  Reasoning", "comments": "First four authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given questions regarding some prototypical situation such as Name something\nthat people usually do before they leave the house for work? a human can easily\nanswer them via acquired experiences. There can be multiple right answers for\nsuch questions, with some more common for a situation than others. This paper\nintroduces a new question answering dataset for training and evaluating common\nsense reasoning capabilities of artificial intelligence systems in such\nprototypical situations. The training set is gathered from an existing set of\nquestions played in a long-running international game show FAMILY- FEUD. The\nhidden evaluation set is created by gathering answers for each question from\n100 crowd-workers. We also propose a generative evaluation task where a model\nhas to output a ranked list of answers, ideally covering all prototypical\nanswers for a question. After presenting multiple competitive baseline models,\nwe find that human performance still exceeds model scores on all evaluation\nmetrics with a meaningful gap, supporting the challenging nature of the task.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 09:40:05 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 05:35:05 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 21:23:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Boratko", "Michael", ""], ["Li", "Xiang Lorraine", ""], ["Das", "Rajarshi", ""], ["O'Gorman", "Tim", ""], ["Le", "Dan", ""], ["McCallum", "Andrew", ""]]}, {"id": "2005.00782", "submitter": "Pei Zhou", "authors": "Pei Zhou, Rahul Khanna, Seyeon Lee, Bill Yuchen Lin, Daniel Ho, Jay\n  Pujara, Xiang Ren", "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense\n  Axioms", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (PTLMs) have achieved impressive performance on\ncommonsense inference benchmarks, but their ability to employ commonsense to\nmake robust inferences, which is crucial for effective communications with\nhumans, is debated. In the pursuit of advancing fluid human-AI communication,\nwe propose a new challenge, RICA: Robust Inference capability based on\nCommonsense Axioms, that evaluates robust commonsense inference despite textual\nperturbations. To generate data for this challenge, we develop a systematic and\nscalable procedure using commonsense knowledge bases and probe PTLMs across two\ndifferent evaluation settings. Extensive experiments on our generated probe\nsets with more than 10k statements show that PTLMs perform no better than\nrandom guessing on the zero-shot setting, are heavily impacted by statistical\nbiases, and are not robust to perturbation attacks. We also find that\nfine-tuning on similar statements offer limited gains, as PTLMs still fail to\ngeneralize to unseen inferences. Our new large-scale benchmark exposes a\nsignificant gap between PTLMs and human-level language understanding and offers\na new challenge for PTLMs to demonstrate commonsense.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:36:55 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 04:11:53 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 23:40:23 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhou", "Pei", ""], ["Khanna", "Rahul", ""], ["Lee", "Seyeon", ""], ["Lin", "Bill Yuchen", ""], ["Ho", "Daniel", ""], ["Pujara", "Jay", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00785", "submitter": "Xisen Jin", "authors": "Xisen Jin, Junyi Du, Arka Sadhu, Ram Nevatia, Xiang Ren", "title": "Visually Grounded Continual Learning of Compositional Phrases", "comments": "EMNLP 2020; Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans acquire language continually with much more limited access to data\nsamples at a time, as compared to contemporary NLP systems. To study this\nhuman-like language acquisition ability, we present VisCOLL, a visually\ngrounded language learning task, which simulates the continual acquisition of\ncompositional phrases from streaming visual scenes. In the task, models are\ntrained on a paired image-caption stream which has shifting object\ndistribution; while being constantly evaluated by a visually-grounded masked\nlanguage prediction task on held-out test sets. VisCOLL compounds the\nchallenges of continual learning (i.e., learning from continuously shifting\ndata distribution) and compositional generalization (i.e., generalizing to\nnovel compositions). To facilitate research on VisCOLL, we construct two\ndatasets, COCO-shift and Flickr-shift, and benchmark them using different\ncontinual learning methods. Results reveal that SoTA continual learning\napproaches provide little to no improvements on VisCOLL, since storing examples\nof all possible compositions is infeasible. We conduct further ablations and\nanalysis to guide future work.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:45:30 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 21:19:50 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 10:25:18 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 18:31:37 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 03:12:11 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jin", "Xisen", ""], ["Du", "Junyi", ""], ["Sadhu", "Arka", ""], ["Nevatia", "Ram", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00789", "submitter": "Harsh Trivedi", "authors": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal", "title": "Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected\n  Reasoning", "comments": "Accepted at EMNLP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Has there been real progress in multi-hop question-answering? Models often\nexploit dataset artifacts to produce correct answers, without connecting\ninformation across multiple supporting facts. This limits our ability to\nmeasure true progress and defeats the purpose of building multi-hop QA\ndatasets. We make three contributions towards addressing this. First, we\nformalize such undesirable behavior as disconnected reasoning across subsets of\nsupporting facts. This allows developing a model-agnostic probe for measuring\nhow much any model can cheat via disconnected reasoning. Second, using a notion\nof \\emph{contrastive support sufficiency}, we introduce an automatic\ntransformation of existing datasets that reduces the amount of disconnected\nreasoning. Third, our experiments suggest that there hasn't been much progress\nin multi-hop QA in the reading comprehension setting. For a recent large-scale\nmodel (XLNet), we show that only 18 points out of its answer F1 score of 72 on\nHotpotQA are obtained through multifact reasoning, roughly the same as that of\na simpler RNN baseline. Our transformation substantially reduces disconnected\nreasoning (19 points in answer F1). It is complementary to adversarial\napproaches, yielding further reductions in conjunction.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:01:07 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 17:54:00 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 04:18:51 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Trivedi", "Harsh", ""], ["Balasubramanian", "Niranjan", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "2005.00791", "submitter": "Deepanway Ghosal", "authors": "Deepanway Ghosal, Devamanyu Hazarika, Abhinaba Roy, Navonil Majumder,\n  Rada Mihalcea and Soujanya Poria", "title": "KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cross-domain sentiment analysis has received significant attention in recent\nyears, prompted by the need to combat the domain gap between different\napplications that make use of sentiment analysis. In this paper, we take a\nnovel perspective on this task by exploring the role of external commonsense\nknowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet\nknowledge graph to enrich the semantics of a document by providing both\ndomain-specific and domain-general background concepts. These concepts are\nlearned by training a graph convolutional autoencoder that leverages\ninter-domain concepts in a domain-invariant manner. Conditioning a popular\ndomain-adversarial baseline method with these learned concepts helps improve\nits performance over state-of-the-art approaches, demonstrating the efficacy of\nour proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:03:25 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 08:44:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ghosal", "Deepanway", ""], ["Hazarika", "Devamanyu", ""], ["Roy", "Abhinaba", ""], ["Majumder", "Navonil", ""], ["Mihalcea", "Rada", ""], ["Poria", "Soujanya", ""]]}, {"id": "2005.00796", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, Richard\n  Socher", "title": "A Simple Language Model for Task-Oriented Dialogue", "comments": "22 Pages, 2 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialogue is often decomposed into three tasks: understanding\nuser input, deciding actions, and generating a response. While such\ndecomposition might suggest a dedicated model for each sub-task, we find a\nsimple, unified approach leads to state-of-the-art performance on the MultiWOZ\ndataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a\nsingle causal language model trained on all sub-tasks recast as a single\nsequence prediction problem. This allows SimpleTOD to fully leverage transfer\nlearning from pre-trained, open domain, causal language models such as GPT-2.\nSimpleTOD improves over the prior state-of-the-art by 0.49 points in joint goal\naccuracy for dialogue state tracking. More impressively, SimpleTOD also\nimproves the main metrics used to evaluate action decisions and response\ngeneration in an end-to-end setting for task-oriented dialog systems: inform\nrate by 8.1 points, success rate by 9.7 points, and combined score by 7.2\npoints.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:09:27 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 03:03:21 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 20:49:22 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["McCann", "Bryan", ""], ["Wu", "Chien-Sheng", ""], ["Yavuz", "Semih", ""], ["Socher", "Richard", ""]]}, {"id": "2005.00800", "submitter": "Joachim Wagner", "authors": "Joachim Wagner, James Barry and Jennifer Foster", "title": "Treebank Embedding Vectors for Out-of-domain Dependency Parsing", "comments": "Camera ready for ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent advance in monolingual dependency parsing is the idea of a treebank\nembedding vector, which allows all treebanks for a particular language to be\nused as training data while at the same time allowing the model to prefer\ntraining data from one treebank over others and to select the preferred\ntreebank at test time. We build on this idea by 1) introducing a method to\npredict a treebank vector for sentences that do not come from a treebank used\nin training, and 2) exploring what happens when we move away from predefined\ntreebank embedding vectors during test time and instead devise tailored\ninterpolations. We show that 1) there are interpolated vectors that are\nsuperior to the predefined ones, and 2) treebank vectors can be predicted with\nsufficient accuracy, for nine out of ten test languages, to match the\nperformance of an oracle approach that knows the most suitable predefined\ntreebank embedding for the test set.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:33:41 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wagner", "Joachim", ""], ["Barry", "James", ""], ["Foster", "Jennifer", ""]]}, {"id": "2005.00806", "submitter": "Qinyuan Ye", "authors": "Qinyuan Ye, Xiao Huang, Elizabeth Boschee, Xiang Ren", "title": "Teaching Machine Comprehension with Compositional Explanations", "comments": "Accepted to EMNLP 2020 Findings. Camera-ready version. Project page:\n  http://inklab.usc.edu/mrc-explanation-project/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in machine reading comprehension (MRC) rely heavily on the\ncollection of large scale human-annotated examples in the form of (question,\nparagraph, answer) triples. In contrast, humans are typically able to\ngeneralize with only a few examples, relying on deeper underlying world\nknowledge, linguistic sophistication, and/or simply superior deductive powers.\nIn this paper, we focus on \"teaching\" machines reading comprehension, using a\nsmall number of semi-structured explanations that explicitly inform machines\nwhy answer spans are correct. We extract structured variables and rules from\nexplanations and compose neural module teachers that annotate instances for\ntraining downstream MRC models. We use learnable neural modules and soft logic\nto handle linguistic variation and overcome sparse coverage; the modules are\njointly optimized with the MRC model to improve final performance. On the SQuAD\ndataset, our proposed method achieves 70.14% F1 score with supervision from 26\nexplanations, comparable to plain supervised learning using 1,100 labeled\ninstances, yielding a 12x speed up.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:54:34 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 20:13:10 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 19:28:50 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ye", "Qinyuan", ""], ["Huang", "Xiao", ""], ["Boschee", "Elizabeth", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.00811", "submitter": "Keerthiram Murugesan", "authors": "Keerthiram Murugesan, Mattia Atzeni, Pushkar Shukla, Mrinmaya Sachan,\n  Pavan Kapanipathi, Kartik Talamadupula", "title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the recent trend of evaluating progress on\nreinforcement learning technology by using text-based environments and games as\nevaluation environments. This reliance on text brings advances in natural\nlanguage processing into the ambit of these agents, with a recurring thread\nbeing the use of external knowledge to mimic and better human-level\nperformance. We present one such instantiation of agents that use commonsense\nknowledge from ConceptNet to show promising performance on two text-based\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 12:07:02 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Murugesan", "Keerthiram", ""], ["Atzeni", "Mattia", ""], ["Shukla", "Pushkar", ""], ["Sachan", "Mrinmaya", ""], ["Kapanipathi", "Pavan", ""], ["Talamadupula", "Kartik", ""]]}, {"id": "2005.00812", "submitter": "Zeljko Agic", "authors": "Jakob D. Havtorn, Jan Latko, Joakim Edin, Lasse Borgholt, Lars\n  Maal{\\o}e, Lorenzo Belgrano, Nicolai F. Jacobsen, Regitze Sdun, \\v{Z}eljko\n  Agi\\'c", "title": "MultiQT: Multimodal Learning for Real-Time Question Tracking in Speech", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a challenging and practical task of labeling questions in speech\nin real time during telephone calls to emergency medical services in English,\nwhich embeds within a broader decision support system for emergency\ncall-takers. We propose a novel multimodal approach to real-time sequence\nlabeling in speech. Our model treats speech and its own textual representation\nas two separate modalities or views, as it jointly learns from streamed audio\nand its noisy transcription into text via automatic speech recognition. Our\nresults show significant gains of jointly learning from the two modalities when\ncompared to text or audio only, under adverse noise and limited volume of\ntraining data. The results generalize to medical symptoms detection where we\nobserve a similar pattern of improvements with multimodal learning.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 12:16:14 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 17:42:42 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Havtorn", "Jakob D.", ""], ["Latko", "Jan", ""], ["Edin", "Joakim", ""], ["Borgholt", "Lasse", ""], ["Maal\u00f8e", "Lars", ""], ["Belgrano", "Lorenzo", ""], ["Jacobsen", "Nicolai F.", ""], ["Sdun", "Regitze", ""], ["Agi\u0107", "\u017deljko", ""]]}, {"id": "2005.00813", "submitter": "Vinodkumar Prabhakaran", "authors": "Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster,\n  Yu Zhong, Stephen Denuyl", "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "comments": "ACL 2020 short paper. 5 pages", "journal-ref": "ACL 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building equitable and inclusive NLP technologies demands consideration of\nwhether and how social attitudes are represented in ML models. In particular,\nrepresentations encoded in models often inadvertently perpetuate undesirable\nsocial biases from the data on which they are trained. In this paper, we\npresent evidence of such undesirable biases towards mentions of disability in\ntwo different English language models: toxicity prediction and sentiment\nanalysis. Next, we demonstrate that the neural embeddings that are the critical\nfirst step in most NLP pipelines similarly contain undesirable biases towards\nmentions of disability. We end by highlighting topical biases in the discourse\nabout disability which may contribute to the observed model biases; for\ninstance, gun violence, homelessness, and drug addiction are over-represented\nin texts discussing mental illness.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 12:16:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hutchinson", "Ben", ""], ["Prabhakaran", "Vinodkumar", ""], ["Denton", "Emily", ""], ["Webster", "Kellie", ""], ["Zhong", "Yu", ""], ["Denuyl", "Stephen", ""]]}, {"id": "2005.00816", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan,\n  Chitta Baral", "title": "DQI: Measuring Data Quality in NLP", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models have achieved human level performance across several\nNLP datasets. However, recent studies have shown that these models are not\ntruly learning the desired task; rather, their high performance is attributed\nto overfitting using spurious biases, which suggests that the capabilities of\nAI systems have been over-estimated. We introduce a generic formula for Data\nQuality Index (DQI) to help dataset creators create datasets free of such\nunwanted biases. We evaluate this formula using a recently proposed approach\nfor adversarial filtering, AFLite. We propose a new data creation paradigm\nusing DQI to create higher quality data. The data creation paradigm consists of\nseveral data visualizations to help data creators (i) understand the quality of\ndata and (ii) visualize the impact of the created data instance on the overall\nquality. It also has a couple of automation methods to (i) assist data creators\nand (ii) make the model more robust to adversarial attacks. We use DQI along\nwith these automation methods to renovate biased examples in SNLI. We show that\nmodels trained on the renovated SNLI dataset generalize better to out of\ndistribution tasks. Renovation results in reduced model performance, exposing a\nlarge gap with respect to human performance. DQI systematically helps in\ncreating harder benchmarks using active learning. Our work takes the process of\ndynamic dataset creation forward, wherein datasets evolve together with the\nevolving state of the art, therefore serving as a means of benchmarking the\ntrue progress of AI.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 12:34:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mishra", "Swaroop", ""], ["Arunkumar", "Anjana", ""], ["Sachdeva", "Bhavdeep", ""], ["Bryan", "Chris", ""], ["Baral", "Chitta", ""]]}, {"id": "2005.00820", "submitter": "Clara Meister", "authors": "Clara Meister, Elizabeth Salesky, Ryan Cotterell", "title": "Generalized Entropy Regularization or: There's Nothing Special about\n  Label Smoothing", "comments": "Published as long paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has explored directly regularizing the output distributions of\nprobabilistic models to alleviate peaky (i.e. over-confident) predictions, a\ncommon sign of overfitting. This class of techniques, of which label smoothing\nis one, has a connection to entropy regularization. Despite the consistent\nsuccess of label smoothing across architectures and data sets in language\ngeneration tasks, two problems remain open: (1) there is little understanding\nof the underlying effects entropy regularizers have on models, and (2) the full\nspace of entropy regularization techniques is largely unexplored. We introduce\na parametric family of entropy regularizers, which includes label smoothing as\na special case, and use it to gain a better understanding of the relationship\nbetween the entropy of a model and its performance on language generation\ntasks. We also find that variance in model performance can be explained largely\nby the resulting entropy of the model. Lastly, we find that label smoothing\nprovably does not allow for sparsity in an output distribution, an undesirable\nproperty for language generation models, and therefore advise the use of other\nentropy regularization methods in its place.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 12:46:28 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 06:22:06 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Meister", "Clara", ""], ["Salesky", "Elizabeth", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2005.00842", "submitter": "Tatsuki Kuribayashi", "authors": "Tatsuki Kuribayashi, Takumi Ito, Jun Suzuki, Kentaro Inui", "title": "Language Models as an Alternative Evaluator of Word Order Hypotheses: A\n  Case Study in Japanese", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a methodology using neural language models (LMs) for analyzing the\nword order of language. This LM-based method has the potential to overcome the\ndifficulties existing methods face, such as the propagation of preprocessor\nerrors in count-based methods. In this study, we explore whether the LM-based\nmethod is valid for analyzing the word order. As a case study, this study\nfocuses on Japanese due to its complex and flexible word order. To validate the\nLM-based method, we test (i) parallels between LMs and human word order\npreference, and (ii) consistency of the results obtained using the LM-based\nmethod with previous linguistic studies. Through our experiments, we\ntentatively conclude that LMs display sufficient word order knowledge for usage\nas an analysis tool. Finally, using the LM-based method, we demonstrate the\nrelationship between the canonical word order and topicalization, which had yet\nto be analyzed by large-scale experiments.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 14:32:40 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kuribayashi", "Tatsuki", ""], ["Ito", "Takumi", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2005.00847", "submitter": "David Mueller", "authors": "David Mueller and Nicholas Andrews and Mark Dredze", "title": "Sources of Transfer in Multilingual Named Entity Recognition", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named-entities are inherently multilingual, and annotations in any given\nlanguage may be limited. This motivates us to consider polyglot named-entity\nrecognition (NER), where one model is trained using annotated data drawn from\nmore than one language. However, a straightforward implementation of this\nsimple idea does not always work in practice: naive training of NER models\nusing annotated data drawn from multiple languages consistently underperforms\nmodels trained on monolingual data alone, despite having access to more\ntraining data. The starting point of this paper is a simple solution to this\nproblem, in which polyglot models are fine-tuned on monolingual data to\nconsistently and significantly outperform their monolingual counterparts. To\nexplain this phenomena, we explore the sources of multilingual transfer in\npolyglot NER models and examine the weight structure of polyglot models\ncompared to their monolingual counterparts. We find that polyglot models\nefficiently share many parameters across languages and that fine-tuning may\nutilize a large number of those parameters.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:00:02 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mueller", "David", ""], ["Andrews", "Nicholas", ""], ["Dredze", "Mark", ""]]}, {"id": "2005.00850", "submitter": "Lifu Tu", "authors": "Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel", "title": "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine\n  Translation", "comments": "ACL 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose to train a non-autoregressive machine translation model to\nminimize the energy defined by a pretrained autoregressive model. In\nparticular, we view our non-autoregressive translation system as an inference\nnetwork (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher\nenergy. This contrasts with the popular approach of training a\nnon-autoregressive model on a distilled corpus consisting of the beam-searched\noutputs of such a teacher model. Our approach, which we call ENGINE\n(ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive\nresults on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the\nperformance of autoregressive models.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:06:47 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 20:44:24 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Tu", "Lifu", ""], ["Pang", "Richard Yuanzhe", ""], ["Wiseman", "Sam", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2005.00851", "submitter": "Huy Nguyen Van", "authors": "Van Huy Nguyen, Thi Quynh Khanh Dinh, Truong Thinh Nguyen, Dang Khoa\n  Mac", "title": "A language score based output selection method for multilingual speech\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of a multilingual speech recognition system can be improved by\nadaptation methods if the input language is specified. For systems that can\naccept multilingual inputs, the popular approach is to apply a language\nidentifier to the input then switch or configure decoders in the next step, or\nuse one more subsequence model to select the output from a set of candidates.\nMotivated by the goal of reducing the latency for real-time applications, in\nthis paper, a language model rescoring method is firstly applied to produce all\npossible candidates for target languages, then a simple score is proposed to\nautomatically select the output without any identifier model or language\nspecification of the input language. The main point is that this score can be\nsimply and automatically estimated on-the-fly so that the whole decoding\npipeline is more simple and compact. Experimental results showed that this\nmethod can achieve the same quality as when the input language is specified. In\naddition, we present to design an English and Vietnamese End-to-End model to\ndeal with not only the problem of cross-lingual speakers but also as a solution\nto improve the accuracy of borrowed words of English in Vietnamese.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:07:14 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Nguyen", "Van Huy", ""], ["Dinh", "Thi Quynh Khanh", ""], ["Nguyen", "Truong Thinh", ""], ["Mac", "Dang Khoa", ""]]}, {"id": "2005.00856", "submitter": "Wentao Xu", "authors": "Wentao Xu, Shun Zheng, Liang He, Bin Shao, Jian Yin, Tie-Yan Liu", "title": "SEEK: Segmented Embedding of Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, knowledge graph embedding becomes a pretty hot research\ntopic of artificial intelligence and plays increasingly vital roles in various\ndownstream applications, such as recommendation and question answering.\nHowever, existing methods for knowledge graph embedding can not make a proper\ntrade-off between the model complexity and the model expressiveness, which\nmakes them still far from satisfactory. To mitigate this problem, we propose a\nlightweight modeling framework that can achieve highly competitive relational\nexpressiveness without increasing the model complexity. Our framework focuses\non the design of scoring functions and highlights two critical characteristics:\n1) facilitating sufficient feature interactions; 2) preserving both symmetry\nand antisymmetry properties of relations. It is noteworthy that owing to the\ngeneral and elegant design of scoring functions, our framework can incorporate\nmany famous existing methods as special cases. Moreover, extensive experiments\non public benchmarks demonstrate the efficiency and effectiveness of our\nframework. Source codes and data can be found at\n\\url{https://github.com/Wentao-Xu/SEEK}.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:15:50 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 10:51:55 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 03:27:31 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Xu", "Wentao", ""], ["Zheng", "Shun", ""], ["He", "Liang", ""], ["Shao", "Bin", ""], ["Yin", "Jian", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2005.00870", "submitter": "Antonios Anastasopoulos", "authors": "Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, Graham\n  Neubig", "title": "Predicting Performance for Natural Language Processing Tasks", "comments": "Accepted at ACL'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity of combinations of tasks, languages, and domains in\nnatural language processing (NLP) research, it is computationally prohibitive\nto exhaustively test newly proposed models on each possible experimental\nsetting. In this work, we attempt to explore the possibility of gaining\nplausible judgments of how well an NLP model can perform under an experimental\nsetting, without actually training or testing the model. To do so, we build\nregression models to predict the evaluation score of an NLP experiment given\nthe experimental settings as input. Experimenting on 9 different NLP tasks, we\nfind that our predictors can produce meaningful predictions over unseen\nlanguages and different modeling architectures, outperforming reasonable\nbaselines as well as human experts. Going further, we outline how our predictor\ncan be used to find a small subset of representative experiments that should be\nrun in order to obtain plausible predictions for all other experimental\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 16:02:18 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Xia", "Mengzhou", ""], ["Anastasopoulos", "Antonios", ""], ["Xu", "Ruochen", ""], ["Yang", "Yiming", ""], ["Neubig", "Graham", ""]]}, {"id": "2005.00879", "submitter": "Ryosuke Kuwabara", "authors": "Ryosuke Kuwabara, Jun Suzuki, Hideki Nakayama", "title": "Single Model Ensemble using Pseudo-Tags and Distinct Vectors", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model ensemble techniques often increase task performance in neural networks;\nhowever, they require increased time, memory, and management effort. In this\nstudy, we propose a novel method that replicates the effects of a model\nensemble with a single model. Our approach creates K-virtual models within a\nsingle parameter space using K-distinct pseudo-tags and K-distinct vectors.\nExperiments on text classification and sequence labeling tasks on several\ndatasets demonstrate that our method emulates or outperforms a traditional\nmodel ensemble with 1/K-times fewer parameters.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 16:23:47 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kuwabara", "Ryosuke", ""], ["Suzuki", "Jun", ""], ["Nakayama", "Hideki", ""]]}, {"id": "2005.00882", "submitter": "Kazuki Matsumaru", "authors": "Kazuki Matsumaru, Sho Takase, Naoaki Okazaki", "title": "Improving Truthfulness of Headline Generation", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies on abstractive summarization report ROUGE scores between system\nand reference summaries. However, we have a concern about the truthfulness of\ngenerated summaries: whether all facts of a generated summary are mentioned in\nthe source text. This paper explores improving the truthfulness in headline\ngeneration on two popular datasets. Analyzing headlines generated by the\nstate-of-the-art encoder-decoder model, we show that the model sometimes\ngenerates untruthful headlines. We conjecture that one of the reasons lies in\nuntruthful supervision data used for training the model. In order to quantify\nthe truthfulness of article-headline pairs, we consider the textual entailment\nof whether an article entails its headline. After confirming quite a few\nuntruthful instances in the datasets, this study hypothesizes that removing\nuntruthful instances from the supervision data may remedy the problem of the\nuntruthful behaviors of the model. Building a binary classifier that predicts\nan entailment relation between an article and its headline, we filter out\nuntruthful instances from the supervision data. Experimental results\ndemonstrate that the headline generation model trained on filtered supervision\ndata shows no clear difference in ROUGE scores but remarkable improvements in\nautomatic and manual evaluations of the generated headlines.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 16:33:37 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 02:02:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Matsumaru", "Kazuki", ""], ["Takase", "Sho", ""], ["Okazaki", "Naoaki", ""]]}, {"id": "2005.00889", "submitter": "Zhen Wang", "authors": "Zhen Wang, Jennifer Lee, Simon Lin, Huan Sun", "title": "Rationalizing Medical Relation Prediction from Corpus-level Statistics", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the interpretability of machine learning models is becoming\nincreasingly important, especially in the medical domain. Aiming to shed some\nlight on how to rationalize medical relation prediction, we present a new\ninterpretable framework inspired by existing theories on how human memory\nworks, e.g., theories of recall and recognition. Given the corpus-level\nstatistics, i.e., a global co-occurrence graph of a clinical text corpus, to\npredict the relations between two entities, we first recall rich contexts\nassociated with the target entities, and then recognize relational interactions\nbetween these contexts to form model rationales, which will contribute to the\nfinal prediction. We conduct experiments on a real-world public clinical\ndataset and show that our framework can not only achieve competitive predictive\nperformance against a comprehensive list of neural baseline models, but also\npresent rationales to justify its prediction. We further collaborate with\nmedical experts deeply to verify the usefulness of our model rationales for\nclinical decision making.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 17:39:40 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Zhen", ""], ["Lee", "Jennifer", ""], ["Lin", "Simon", ""], ["Sun", "Huan", ""]]}, {"id": "2005.00891", "submitter": "Giovanni Campagna", "authors": "Giovanni Campagna and Agata Foryciarz and Mehrad Moradshahi and Monica\n  S. Lam", "title": "Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain\n  Dialogue State Tracking", "comments": "9 pages. To appear in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot transfer learning for multi-domain dialogue state tracking can\nallow us to handle new domains without incurring the high cost of data\nacquisition. This paper proposes new zero-short transfer learning technique for\ndialogue state tracking where the in-domain training data are all synthesized\nfrom an abstract dialogue model and the ontology of the domain. We show that\ndata augmentation through synthesized data can improve the accuracy of\nzero-shot learning for both the TRADE model and the BERT-based SUMBT model on\nthe MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data\non the SUMBT model can reach about 2/3 of the accuracy obtained with the full\ntraining dataset. We improve the zero-shot learning state of the art on average\nacross domains by 21%.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 18:00:48 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Campagna", "Giovanni", ""], ["Foryciarz", "Agata", ""], ["Moradshahi", "Mehrad", ""], ["Lam", "Monica S.", ""]]}, {"id": "2005.00908", "submitter": "Shengjie Li", "authors": "Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut and Matthew\n  Stone", "title": "Clue: Cross-modal Coherence Modeling for Caption Generation", "comments": "Accepted as a long paper to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use coherence relations inspired by computational models of discourse to\nstudy the information needs and goals of image captioning. Using an annotation\nprotocol specifically devised for capturing image--caption coherence relations,\nwe annotate 10,000 instances from publicly-available image--caption pairs. We\nintroduce a new task for learning inferences in imagery and text, coherence\nrelation prediction, and show that these coherence annotations can be exploited\nto learn relation classifiers as an intermediary step, and also train\ncoherence-aware, controllable image captioning models. The results show a\ndramatic improvement in the consistency and quality of the generated captions\nwith respect to information needs specified via coherence relations.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 19:28:52 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Alikhani", "Malihe", ""], ["Sharma", "Piyush", ""], ["Li", "Shengjie", ""], ["Soricut", "Radu", ""], ["Stone", "Matthew", ""]]}, {"id": "2005.00912", "submitter": "Saif Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Examining Citations of Natural Language Processing Literature", "comments": null, "journal-ref": "Proceedings of the 58th Annual Meeting of the Association of\n  Computational Linguistics (ACL 2020). July 2020. Seattle, USA", "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extracted information from the ACL Anthology (AA) and Google Scholar (GS)\nto examine trends in citations of NLP papers. We explore questions such as: how\nwell cited are papers of different types (journal articles, conference papers,\ndemo papers, etc.)? how well cited are papers from different areas of within\nNLP? etc. Notably, we show that only about 56\\% of the papers in AA are cited\nten or more times. CL Journal has the most cited papers, but its citation\ndominance has lessened in recent years. On average, long papers get almost\nthree times as many citations as short papers; and papers on sentiment\nclassification, anaphora resolution, and entity recognition have the highest\nmedian citations. The analyses presented here, and the associated dataset of\nNLP papers mapped to citations, have a number of uses including: understanding\nhow the field is growing and quantifying the impact of different types of\npapers.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 20:01:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "2005.00928", "submitter": "Samira Abnar", "authors": "Samira Abnar and Willem Zuidema", "title": "Quantifying Attention Flow in Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Transformer model, \"self-attention\" combines information from attended\nembeddings into the representation of the focal embedding in the next layer.\nThus, across layers of the Transformer, information originating from different\ntokens gets increasingly mixed. This makes attention weights unreliable as\nexplanations probes. In this paper, we consider the problem of quantifying this\nflow of information through self-attention. We propose two methods for\napproximating the attention to input tokens given attention weights, attention\nrollout and attention flow, as post hoc methods when we use attention weights\nas the relative relevance of the input tokens. We show that these methods give\ncomplementary views on the flow of information, and compared to raw attention,\nboth yield higher correlations with importance scores of input tokens obtained\nusing an ablation method and input gradients.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:45:27 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 16:59:40 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Abnar", "Samira", ""], ["Zuidema", "Willem", ""]]}, {"id": "2005.00932", "submitter": "Jiawei Zhou", "authors": "Jiawei Zhou, Phillip Keung", "title": "Improving Non-autoregressive Neural Machine Translation with Monolingual\n  Data", "comments": "Published in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive (NAR) neural machine translation is usually done via\nknowledge distillation from an autoregressive (AR) model. Under this framework,\nwe leverage large monolingual corpora to improve the NAR model's performance,\nwith the goal of transferring the AR model's generalization ability while\npreventing overfitting. On top of a strong NAR baseline, our experimental\nresults on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that\nmonolingual data augmentation consistently improves the performance of the NAR\nmodel to approach the teacher AR model's performance, yields comparable or\nbetter results than the best non-iterative NAR methods in the literature and\nhelps reduce overfitting in the training process.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 22:24:52 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 03:50:58 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 21:48:51 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Zhou", "Jiawei", ""], ["Keung", "Phillip", ""]]}, {"id": "2005.00944", "submitter": "Hongyang Zhang", "authors": "Sen Wu, Hongyang R. Zhang, Christopher R\\'e", "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "comments": "Appeared in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate multi-task learning approaches that use a shared feature\nrepresentation for all tasks. To better understand the transfer of task\ninformation, we study an architecture with a shared module for all tasks and a\nseparate output module for each task. We study the theory of this setting on\nlinear and ReLU-activated models. Our key observation is that whether or not\ntasks' data are well-aligned can significantly affect the performance of\nmulti-task learning. We show that misalignment between task data can cause\nnegative transfer (or hurt performance) and provide sufficient conditions for\npositive transfer. Inspired by the theoretical insights, we show that aligning\ntasks' embedding layers leads to performance gains for multi-task training and\ntransfer learning on the GLUE benchmark and sentiment analysis tasks; for\nexample, we obtain a 2.35% GLUE score average improvement on 5 GLUE tasks over\nBERT-LARGE using our alignment method. We also design an SVD-based task\nreweighting scheme and show that it improves the robustness of multi-task\ntraining on a multi-label image dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:43:52 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wu", "Sen", ""], ["Zhang", "Hongyang R.", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2005.00950", "submitter": "Marija Stanojevic", "authors": "Quang Pham, Marija Stanojevic, Zoran Obradovic", "title": "Extracting Entities and Topics from News and Connecting Criminal Records", "comments": "This is a report submitted by an undergraduate student as preliminary\n  work on this problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to summarize methodologies used in extracting\nentities and topics from a database of criminal records and from a database of\nnewspapers. Statistical models had successfully been used in studying the\ntopics of roughly 300,000 New York Times articles. In addition, these models\nhad also been used to successfully analyze entities related to people,\norganizations, and places (D Newman, 2006). Additionally, analytical\napproaches, especially in hotspot mapping, were used in some researches with an\naim to predict crime locations and circumstances in the future, and those\napproaches had been tested quite successfully (S Chainey, 2008). Based on the\ntwo above notions, this research was performed with the intention to apply data\nscience techniques in analyzing a big amount of data, selecting valuable\nintelligence, clustering violations depending on their types of crime, and\ncreating a crime graph that changes through time. In this research, the task\nwas to download criminal datasets from Kaggle and a collection of news articles\nfrom Kaggle and EAGER project databases, and then to merge these datasets into\none general dataset. The most important goal of this project was performing\nstatistical and natural language processing methods to extract entities and\ntopics as well as to group similar data points into correct clusters, in order\nto understand public data about U.S related crimes better.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:06:01 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Pham", "Quang", ""], ["Stanojevic", "Marija", ""], ["Obradovic", "Zoran", ""]]}, {"id": "2005.00955", "submitter": "Tal Linzen", "authors": "Tal Linzen", "title": "How Can We Accelerate Progress Towards Human-like Linguistic\n  Generalization?", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper describes and critiques the Pretraining-Agnostic\nIdentically Distributed (PAID) evaluation paradigm, which has become a central\ntool for measuring progress in natural language understanding. This paradigm\nconsists of three stages: (1) pre-training of a word prediction model on a\ncorpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set\nrepresenting a classification task; (3) evaluation on a test set drawn from the\nsame distribution as that training set. This paradigm favors simple, low-bias\narchitectures, which, first, can be scaled to process vast amounts of data, and\nsecond, can capture the fine-grained statistical properties of a particular\ndata set, regardless of whether those properties are likely to generalize to\nexamples of the task outside the data set. This contrasts with humans, who\nlearn language from several orders of magnitude less data than the systems\nfavored by this evaluation paradigm, and generalize to new tasks in a\nconsistent way. We advocate for supplementing or replacing PAID with paradigms\nthat reward architectures that generalize as quickly and robustly as humans.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:31:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Linzen", "Tal", ""]]}, {"id": "2005.00956", "submitter": "William Lane", "authors": "William Lane and Steven Bird", "title": "Bootstrapping Techniques for Polysynthetic Morphological Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polysynthetic languages have exceptionally large and sparse vocabularies,\nthanks to the number of morpheme slots and combinations in a word. This\ncomplexity, together with a general scarcity of written data, poses a challenge\nto the development of natural language technologies. To address this challenge,\nwe offer linguistically-informed approaches for bootstrapping a neural\nmorphological analyzer, and demonstrate its application to Kunwinjku, a\npolysynthetic Australian language. We generate data from a finite state\ntransducer to train an encoder-decoder model. We improve the model by\n\"hallucinating\" missing linguistic structure into the training data, and by\nresampling from a Zipf distribution to simulate a more natural distribution of\nmorphemes. The best model accounts for all instances of reduplication in the\ntest set and achieves an accuracy of 94.7% overall, a 10 percentage point\nimprovement over the FST baseline. This process demonstrates the feasibility of\nbootstrapping a neural morph analyzer from minimal resources.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:35:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lane", "William", ""], ["Bird", "Steven", ""]]}, {"id": "2005.00962", "submitter": "Saif M. Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Gender Gap in Natural Language Processing Research: Disparities in\n  Authorship and Citations", "comments": null, "journal-ref": "Proceedings of the 58th Annual Meeting of the Association of\n  Computational Linguistics (ACL 2020). July 2020. Seattle, USA", "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparities in authorship and citations across gender can have substantial\nadverse consequences not just on the disadvantaged genders, but also on the\nfield of study as a whole. Measuring gender gaps is a crucial step towards\naddressing them. In this work, we examine female first author percentages and\nthe citations to their papers in Natural Language Processing (1965 to 2019). We\ndetermine aggregate-level statistics using an existing manually curated\nauthor--gender list as well as first names strongly associated with a gender.\nWe find that only about 29% of first authors are female and only about 25% of\nlast authors are female. Notably, this percentage has not improved since the\nmid 2000s. We also show that, on average, female first authors are cited less\nthan male first authors, even when controlling for experience and area of\nresearch. Finally, we discuss the ethical considerations involved in automatic\ndemographic analysis.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 01:31:12 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 20:00:08 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "2005.00963", "submitter": "Shuo Wang", "authors": "Shuo Wang, Zhaopeng Tu, Shuming Shi, Yang Liu", "title": "On the Inference Calibration of Neural Machine Translation", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence calibration, which aims to make model predictions equal to the\ntrue correctness measures, is important for neural machine translation (NMT)\nbecause it is able to offer useful indicators of translation errors in the\ngenerated output. While prior studies have shown that NMT models trained with\nlabel smoothing are well-calibrated on the ground-truth training data, we find\nthat miscalibration still remains a severe challenge for NMT during inference\ndue to the discrepancy between training and inference. By carefully designing\nexperiments on three language pairs, our work provides in-depth analyses of the\ncorrelation between calibration and translation performance as well as\nlinguistic properties of miscalibration and reports a number of interesting\nfindings that might help humans better analyze, understand and improve NMT\nmodels. Based on these observations, we further propose a new graduated label\nsmoothing method that can improve both inference calibration and translation\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:03:56 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Shuo", ""], ["Tu", "Zhaopeng", ""], ["Shi", "Shuming", ""], ["Liu", "Yang", ""]]}, {"id": "2005.00965", "submitter": "Tianlu Wang", "authors": "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann,\n  Vicente Ordonez, Caiming Xiong", "title": "Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings derived from human-generated corpora inherit strong gender\nbias which can be further amplified by downstream models. Some commonly adopted\ndebiasing approaches, including the seminal Hard Debias algorithm, apply\npost-processing procedures that project pre-trained word embeddings into a\nsubspace orthogonal to an inferred gender subspace. We discover that\nsemantic-agnostic corpus regularities such as word frequency captured by the\nword embeddings negatively impact the performance of these algorithms. We\npropose a simple but effective technique, Double Hard Debias, which purifies\nthe word embeddings against such corpus regularities prior to inferring and\nremoving the gender subspace. Experiments on three bias mitigation benchmarks\nshow that our approach preserves the distributional semantics of the\npre-trained word embeddings while reducing gender bias to a significantly\nlarger degree than prior approaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:33:20 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Tianlu", ""], ["Lin", "Xi Victoria", ""], ["Rajani", "Nazneen Fatema", ""], ["McCann", "Bryan", ""], ["Ordonez", "Vicente", ""], ["Xiong", "Caiming", ""]]}, {"id": "2005.00969", "submitter": "Zhenyi Wang", "authors": "Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen", "title": "Towards Faithful Neural Table-to-Text Generation with Content-Matching\n  Constraints", "comments": "Accepted at ACL2020", "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics, pp. 1072-1086, 2020", "doi": "10.18653/v1/2020.acl-main.101", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation from a knowledge base aims to translate knowledge triples to\nnatural language descriptions. Most existing methods ignore the faithfulness\nbetween a generated text description and the original table, leading to\ngenerated information that goes beyond the content of the table. In this paper,\nfor the first time, we propose a novel Transformer-based generation framework\nto achieve the goal. The core techniques in our method to enforce faithfulness\ninclude a new table-text optimal-transport matching loss and a table-text\nembedding similarity loss based on the Transformer model. Furthermore, to\nevaluate faithfulness, we propose a new automatic metric specialized to the\ntable-to-text generation problem. We also provide detailed analysis on each\ncomponent of our model in our experiments. Automatic and human evaluations show\nthat our framework can significantly outperform state-of-the-art by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:54:26 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wang", "Zhenyi", ""], ["Wang", "Xiaoyang", ""], ["An", "Bang", ""], ["Yu", "Dong", ""], ["Chen", "Changyou", ""]]}, {"id": "2005.00970", "submitter": "Huiming Jin", "authors": "Huiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya D. McCarthy,\n  Katharina Kann", "title": "Unsupervised Morphological Paradigm Completion", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the task of unsupervised morphological paradigm completion. Given\nonly raw text and a lemma list, the task consists of generating the\nmorphological paradigms, i.e., all inflected forms, of the lemmas. From a\nnatural language processing (NLP) perspective, this is a challenging\nunsupervised task, and high-performing systems have the potential to improve\ntools for low-resource languages or to assist linguistic annotators. From a\ncognitive science perspective, this can shed light on how children acquire\nmorphological knowledge. We further introduce a system for the task, which\ngenerates morphological paradigms via the following steps: (i) EDIT TREE\nretrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and\n(iv) inflection generation. We perform an evaluation on 14 typologically\ndiverse languages. Our system outperforms trivial baselines with ease and, for\nsome languages, even obtains a higher accuracy than minimally supervised\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:56:05 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 22:56:34 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Jin", "Huiming", ""], ["Cai", "Liwei", ""], ["Peng", "Yihui", ""], ["Xia", "Chen", ""], ["McCarthy", "Arya D.", ""], ["Kann", "Katharina", ""]]}, {"id": "2005.00975", "submitter": "Yu Zhang", "authors": "Yu Zhang, Zhenghua Li, Min Zhang", "title": "Efficient Second-Order TreeCRF for Neural Dependency Parsing", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 03:18:59 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 06:35:00 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zhang", "Yu", ""], ["Li", "Zhenghua", ""], ["Zhang", "Min", ""]]}, {"id": "2005.00979", "submitter": "Xinwei Geng", "authors": "Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu", "title": "How Does Selective Mechanism Improve Self-Attention Networks?", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention networks (SANs) with selective mechanism has produced\nsubstantial improvements in various NLP tasks by concentrating on a subset of\ninput words. However, the underlying reasons for their strong performance have\nnot been well explained. In this paper, we bridge the gap by assessing the\nstrengths of selective SANs (SSANs), which are implemented with a flexible and\nuniversal Gumbel-Softmax. Experimental results on several representative NLP\ntasks, including natural language inference, semantic role labelling, and\nmachine translation, show that SSANs consistently outperform the standard SANs.\nThrough well-designed probing experiments, we empirically validate that the\nimprovement of SSANs can be attributed in part to mitigating two commonly-cited\nweaknesses of SANs: word order encoding and structure modeling. Specifically,\nthe selective mechanism improves SANs by paying more attention to content words\nthat contribute to the meaning of the sentence. The code and data are released\nat https://github.com/xwgeng/SSAN.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 04:18:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Geng", "Xinwei", ""], ["Wang", "Longyue", ""], ["Wang", "Xing", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""], ["Tu", "Zhaopeng", ""]]}, {"id": "2005.00987", "submitter": "Masahiro Kaneko", "authors": "Masahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki, Kentaro Inui", "title": "Encoder-Decoder Models Can Benefit from Pre-trained Masked Language\n  Models in Grammatical Error Correction", "comments": "Accepted as a short paper to the 58th Annual Conference of the\n  Association for Computational Linguistics (ACL-2020)", "journal-ref": "Association for Computational Linguistics (ACL-2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to effectively incorporate a pre-trained masked\nlanguage model (MLM), such as BERT, into an encoder-decoder (EncDec) model for\ngrammatical error correction (GEC). The answer to this question is not as\nstraightforward as one might expect because the previous common methods for\nincorporating a MLM into an EncDec model have potential drawbacks when applied\nto GEC. For example, the distribution of the inputs to a GEC model can be\nconsiderably different (erroneous, clumsy, etc.) from that of the corpora used\nfor pre-training MLMs; however, this issue is not addressed in the previous\nmethods. Our experiments show that our proposed method, where we first\nfine-tune a MLM with a given GEC corpus and then use the output of the\nfine-tuned MLM as additional features in the GEC model, maximizes the benefit\nof the MLM. The best-performing model achieves state-of-the-art performances on\nthe BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at:\nhttps://github.com/kanekomasahiro/bert-gec.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 04:49:31 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 08:01:57 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Kaneko", "Masahiro", ""], ["Mita", "Masato", ""], ["Kiyono", "Shun", ""], ["Suzuki", "Jun", ""], ["Inui", "Kentaro", ""]]}, {"id": "2005.01006", "submitter": "Jiandong Zhang", "authors": "Wei Bao, Hongshu Che, Jiandong Zhang", "title": "An Accurate Model for Predicting the (Graded) Effect of Context in Word\n  Similarity Based on Bert", "comments": "ACL-SemEval 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) has been widely used in the semantic\nanalysis in recent years. Our paper mainly discusses a methodology to analyze\nthe effect that context has on human perception of similar words, which is the\nthird task of SemEval 2020. We apply several methods in calculating the\ndistance between two embedding vector generated by Bidirectional Encoder\nRepresentation from Transformer (BERT). Our team will_go won the 1st place in\nFinnish language track of subtask1, the second place in English track of\nsubtask1.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 06:48:35 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 16:02:46 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 03:03:51 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bao", "Wei", ""], ["Che", "Hongshu", ""], ["Zhang", "Jiandong", ""]]}, {"id": "2005.01027", "submitter": "Avinash Madasu", "authors": "Avinash Madasu and Vijjini Anvesh Rao", "title": "A Position Aware Decay Weighted Network for Aspect based Sentiment\n  Analysis", "comments": "Accepted Full Paper at 25th International Conference on Applications\n  of Natural Language to Information Systems, June 2020, DFKI Saarbr\\\"ucken,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect Based Sentiment Analysis (ABSA) is the task of identifying sentiment\npolarity of a text given another text segment or aspect. In ABSA, a text can\nhave multiple sentiments depending upon each aspect. Aspect Term Sentiment\nAnalysis (ATSA) is a subtask of ABSA, in which aspect terms are contained\nwithin the given sentence. Most of the existing approaches proposed for ATSA,\nincorporate aspect information through a different subnetwork thereby\noverlooking the advantage of aspect terms' presence within the sentence. In\nthis paper, we propose a model that leverages the positional information of the\naspect. The proposed model introduces a decay mechanism based on position. This\ndecay function mandates the contribution of input words for ABSA. The\ncontribution of a word declines as farther it is positioned from the aspect\nterms in the sentence. The performance is measured on two standard datasets\nfrom SemEval 2014 Task 4. In comparison with recent architectures, the\neffectiveness of the proposed model is demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 09:22:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Madasu", "Avinash", ""], ["Rao", "Vijjini Anvesh", ""]]}, {"id": "2005.01063", "submitter": "Guy Kushilevitz", "authors": "Guy Kushilevitz, Shaul Markovitch, Yoav Goldberg", "title": "A Two-Stage Masked LM Method for Term Set Expansion", "comments": "short paper accepted to acl 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the task of Term Set Expansion (TSE): given a small seed set of\nexample terms from a semantic class, finding more members of that class. The\ntask is of great practical utility, and also of theoretical utility as it\nrequires generalization from few examples. Previous approaches to the TSE task\ncan be characterized as either distributional or pattern-based. We harness the\npower of neural masked language models (MLM) and propose a novel TSE algorithm,\nwhich combines the pattern-based and distributional approaches. Due to the\nsmall size of the seed set, fine-tuning methods are not effective, calling for\nmore creative use of the MLM. The gist of the idea is to use the MLM to first\nmine for informative patterns with respect to the seed set, and then to obtain\nmore members of the seed class by generalizing these patterns. Our method\noutperforms state-of-the-art TSE algorithms. Implementation is available at:\nhttps://github.com/ guykush/TermSetExpansion-MPB/\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 12:06:06 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kushilevitz", "Guy", ""], ["Markovitch", "Shaul", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2005.01096", "submitter": "Ernie Chang", "authors": "Xiaoyu Shen, Ernie Chang, Hui Su, Jie Zhou, Dietrich Klakow", "title": "Neural Data-to-Text Generation via Jointly Learning the Segmentation and\n  Correspondence", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neural attention model has achieved great success in data-to-text\ngeneration tasks. Though usually excelling at producing fluent text, it suffers\nfrom the problem of information missing, repetition and \"hallucination\". Due to\nthe black-box nature of the neural attention architecture, avoiding these\nproblems in a systematic way is non-trivial. To address this concern, we\npropose to explicitly segment target text into fragment units and align them\nwith their data correspondences. The segmentation and correspondence are\njointly learned as latent variables without any human annotations. We further\nimpose a soft statistical constraint to regularize the segmental granularity.\nThe resulting architecture maintains the same expressive power as neural\nattention models, while being able to generate fully interpretable outputs with\nseveral times less computational cost. On both E2E and WebNLG benchmarks, we\nshow the proposed model consistently outperforms its neural attention\ncounterparts.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 14:28:28 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shen", "Xiaoyu", ""], ["Chang", "Ernie", ""], ["Su", "Hui", ""], ["Zhou", "Jie", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2005.01107", "submitter": "Jan Christian Blaise Cruz", "authors": "Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz,\n  Charibeth Cheng", "title": "Simplifying Paragraph-level Question Generation via Transformer Language\n  Models", "comments": "Formerly titled \"Transformer-based End-to-End Question Generation.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 14:57:24 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 06:43:17 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 06:36:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lopez", "Luis Enrico", ""], ["Cruz", "Diane Kathryn", ""], ["Cruz", "Jan Christian Blaise", ""], ["Cheng", "Charibeth", ""]]}, {"id": "2005.01119", "submitter": "Kata Gabor", "authors": "Rapha\\\"el Bailly and Kata G\\'abor", "title": "Emergence of Syntax Needs Minimal Supervision", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a theoretical contribution to the debate on the learnability of\nsyntax from a corpus without explicit syntax-specific guidance. Our approach\noriginates in the observable structure of a corpus, which we use to define and\nisolate grammaticality (syntactic information) and meaning/pragmatics\ninformation. We describe the formal characteristics of an autonomous syntax and\nshow that it becomes possible to search for syntax-based lexical categories\nwith a simple optimization process, without any prior hypothesis on the form of\nthe model.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 15:38:33 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bailly", "Rapha\u00ebl", ""], ["G\u00e1bor", "Kata", ""]]}, {"id": "2005.01151", "submitter": "Amirreza Shirani", "authors": "Amirreza Shirani, Franck Dernoncourt, Jose Echevarria, Paul Asente,\n  Nedim Lipka and Thamar Solorio", "title": "Let Me Choose: From Verbal Context to Font Selection", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we aim to learn associations between visual attributes of\nfonts and the verbal context of the texts they are typically applied to.\nCompared to related work leveraging the surrounding visual context, we choose\nto focus only on the input text as this can enable new applications for which\nthe text is the only visual element in the document. We introduce a new\ndataset, containing examples of different topics in social media posts and ads,\nlabeled through crowd-sourcing. Due to the subjective nature of the task,\nmultiple fonts might be perceived as acceptable for an input text, which makes\nthis problem challenging. To this end, we investigate different end-to-end\nmodels to learn label distributions on crowd-sourced data and capture\ninter-subjectivity across all annotations.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 17:36:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shirani", "Amirreza", ""], ["Dernoncourt", "Franck", ""], ["Echevarria", "Jose", ""], ["Asente", "Paul", ""], ["Lipka", "Nedim", ""], ["Solorio", "Thamar", ""]]}, {"id": "2005.01157", "submitter": "Matan Orbach", "authors": "Matan Orbach, Yonatan Bilu, Assaf Toledo, Dan Lahav, Michal Jacovi,\n  Ranit Aharonov and Noam Slonim", "title": "Out of the Echo Chamber: Detecting Countering Debate Speeches", "comments": "Accepted to ACL 2020 as Long Paper. For the associated debate\n  speeches corpus, see\n  https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Debate%20Speech%20Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An educated and informed consumption of media content has become a challenge\nin modern times. With the shift from traditional news outlets to social media\nand similar venues, a major concern is that readers are becoming encapsulated\nin \"echo chambers\" and may fall prey to fake news and disinformation, lacking\neasy access to dissenting views. We suggest a novel task aiming to alleviate\nsome of these concerns -- that of detecting articles that most effectively\ncounter the arguments -- and not just the stance -- made in a given text. We\nstudy this problem in the context of debate speeches. Given such a speech, we\naim to identify, from among a set of speeches on the same topic and with an\nopposing stance, the ones that directly counter it. We provide a large dataset\nof 3,685 such speeches (in English), annotated for this relation, which\nhopefully would be of general interest to the NLP community. We explore several\nalgorithms addressing this task, and while some are successful, all fall short\nof expert human performance, suggesting room for further research. All data\ncollected during this work is freely available for research.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:02:10 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Orbach", "Matan", ""], ["Bilu", "Yonatan", ""], ["Toledo", "Assaf", ""], ["Lahav", "Dan", ""], ["Jacovi", "Michal", ""], ["Aharonov", "Ranit", ""], ["Slonim", "Noam", ""]]}, {"id": "2005.01158", "submitter": "Gerard de Melo", "authors": "Kshitij Shah, Gerard de Melo", "title": "Correcting the Autocorrect: Context-Aware Typographical Error Correction\n  via Training Data Augmentation", "comments": "Accepted for publication at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the artificial generation of typographical errors\nbased on real-world statistics. We first draw on a small set of annotated data\nto compute spelling error statistics. These are then invoked to introduce\nerrors into substantially larger corpora. The generation methodology allows us\nto generate particularly challenging errors that require context-aware error\ndetection. We use it to create a set of English language error detection and\ncorrection datasets. Finally, we examine the effectiveness of machine learning\nmodels for detecting and correcting errors based on this data. The datasets are\navailable at http://typo.nlproc.org\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:08:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shah", "Kshitij", ""], ["de Melo", "Gerard", ""]]}, {"id": "2005.01159", "submitter": "Luyang Huang", "authors": "Luyang Huang, Lingfei Wu, Lu Wang", "title": "Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven\n  Cloze Reward", "comments": "Accepted as a long paper to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models for abstractive summarization have been studied\nextensively, yet the generated summaries commonly suffer from fabricated\ncontent, and are often found to be near-extractive. We argue that, to address\nthese issues, the summarizer should acquire semantic interpretation over input,\ne.g., via structured representation, to allow the generation of more\ninformative summaries. In this paper, we present ASGARD, a novel framework for\nAbstractive Summarization with Graph-Augmentation and semantic-driven RewarD.\nWe propose the use of dual encoders---a sequential document encoder and a\ngraph-structured encoder---to maintain the global context and local\ncharacteristics of entities, complementing each other. We further design a\nreward based on a multiple choice cloze test to drive the model to better\ncapture entity interactions. Results show that our models produce significantly\nhigher ROUGE scores than a variant without knowledge graph as input on both New\nYork Times and CNN/Daily Mail datasets. We also obtain better or comparable\nperformance compared to systems that are fine-tuned from large pretrained\nlanguage models. Human judges further rate our model outputs as more\ninformative and containing fewer unfaithful errors.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:23:06 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Huang", "Luyang", ""], ["Wu", "Lingfei", ""], ["Wang", "Lu", ""]]}, {"id": "2005.01172", "submitter": "John Wu", "authors": "John M. Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim\n  Dalvi, James Glass", "title": "Similarity Analysis of Contextual Word Representation Models", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates contextual word representation models from the lens\nof similarity analysis. Given a collection of trained models, we measure the\nsimilarity of their internal representations and attention. Critically, these\nmodels come from vastly different architectures. We use existing and novel\nsimilarity measures that aim to gauge the level of localization of information\nin the deep models, and facilitate the investigation of which design factors\naffect model similarity, without requiring any external linguistic annotation.\nThe analysis reveals that models within the same family are more similar to one\nanother, as may be expected. Surprisingly, different architectures have rather\nsimilar representations, but different individual neurons. We also observed\ndifferences in information localization in lower and higher layers and found\nthat higher layers are more affected by fine-tuning on downstream tasks.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:48:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wu", "John M.", ""], ["Belinkov", "Yonatan", ""], ["Sajjad", "Hassan", ""], ["Durrani", "Nadir", ""], ["Dalvi", "Fahim", ""], ["Glass", "James", ""]]}, {"id": "2005.01177", "submitter": "Cristina Espa\\~na-Bonet", "authors": "Cristina Espa\\~na-Bonet, Alberto Barr\\'on-Cede\\~no and Llu\\'is\n  M\\`arquez", "title": "Tailoring and Evaluating the Wikipedia for in-Domain Comparable Corpora\n  Extraction", "comments": "26 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic language-independent graph-based method to build\n\\`a-la-carte article collections on user-defined domains from the Wikipedia.\nThe core model is based on the exploration of the encyclopaedia's category\ngraph and can produce both monolingual and multilingual comparable collections.\nWe run thorough experiments to assess the quality of the obtained corpora in 10\nlanguages and 743 domains. According to an extensive manual evaluation, our\ngraph-based model outperforms a retrieval-based approach and reaches an average\nprecision of 84% on in-domain articles. As manual evaluations are costly, we\nintroduce the concept of \"domainness\" and design several automatic metrics to\naccount for the quality of the collections. Our best metric for domainness\nshows a strong correlation with the human-judged precision, representing a\nreasonable automatic alternative to assess the quality of domain-specific\ncorpora. We release the WikiTailor toolkit with the implementation of the\nextraction methods, the evaluation measures and several utilities. WikiTailor\nmakes obtaining multilingual in-domain data from the Wikipedia easy.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 20:08:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Espa\u00f1a-Bonet", "Cristina", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["M\u00e0rquez", "Llu\u00eds", ""]]}, {"id": "2005.01190", "submitter": "Kaiji Lu", "authors": "Kaiji Lu, Piotr Mardziel, Klas Leino, Matt Fedrikson, Anupam Datta", "title": "Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM\n  Language Models", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTM-based recurrent neural networks are the state-of-the-art for many\nnatural language processing (NLP) tasks. Despite their performance, it is\nunclear whether, or how, LSTMs learn structural features of natural languages\nsuch as subject-verb number agreement in English. Lacking this understanding,\nthe generality of LSTM performance on this task and their suitability for\nrelated tasks remains uncertain. Further, errors cannot be properly attributed\nto a lack of structural capability, training data omissions, or other\nexceptional faults. We introduce *influence paths*, a causal account of\nstructural properties as carried by paths across gates and neurons of a\nrecurrent neural network. The approach refines the notion of influence (the\nsubject's grammatical number has influence on the grammatical number of the\nsubsequent verb) into a set of gate or neuron-level paths. The set localizes\nand segments the concept (e.g., subject-verb agreement), its constituent\nelements (e.g., the subject), and related or interfering elements (e.g.,\nattractors). We exemplify the methodology on a widely-studied multi-layer LSTM\nlanguage model, demonstrating its accounting for subject-verb number agreement.\nThe results offer both a finer and a more complete view of an LSTM's handling\nof this structural aspect of the English language than prior results based on\ndiagnostic classifiers and ablation.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 21:10:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lu", "Kaiji", ""], ["Mardziel", "Piotr", ""], ["Leino", "Klas", ""], ["Fedrikson", "Matt", ""], ["Datta", "Anupam", ""]]}, {"id": "2005.01196", "submitter": "Wei Zhao", "authors": "Wei Zhao, Goran Glava\\v{s}, Maxime Peyrard, Yang Gao, Robert West,\n  Steffen Eger", "title": "On the Limitations of Cross-lingual Encoders as Exposed by\n  Reference-Free Machine Translation Evaluation", "comments": "ACL2020 Camera Ready (v3: several small fixes, e.g., Unicode errors)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of cross-lingual encoders is usually performed either via\nzero-shot cross-lingual transfer in supervised downstream tasks or via\nunsupervised cross-lingual textual similarity. In this paper, we concern\nourselves with reference-free machine translation (MT) evaluation where we\ndirectly compare source texts to (sometimes low-quality) system translations,\nwhich represents a natural adversarial setup for multilingual encoders.\nReference-free evaluation holds the promise of web-scale comparison of MT\nsystems. We systematically investigate a range of metrics based on\nstate-of-the-art cross-lingual semantic representations obtained with\npretrained M-BERT and LASER. We find that they perform poorly as semantic\nencoders for reference-free MT evaluation and identify their two key\nlimitations, namely, (a) a semantic mismatch between representations of mutual\ntranslations and, more prominently, (b) the inability to punish\n\"translationese\", i.e., low-quality literal translations. We propose two\npartial remedies: (1) post-hoc re-alignment of the vector spaces and (2)\ncoupling of semantic-similarity based metrics with target-side language\nmodeling. In segment-level MT evaluation, our best metric surpasses\nreference-based BLEU by 5.7 correlation points.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 22:10:23 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 11:45:09 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 11:27:25 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhao", "Wei", ""], ["Glava\u0161", "Goran", ""], ["Peyrard", "Maxime", ""], ["Gao", "Yang", ""], ["West", "Robert", ""], ["Eger", "Steffen", ""]]}, {"id": "2005.01204", "submitter": "Adina Williams", "authors": "Adina Williams, Ryan Cotterell, Lawrence Wolf-Sonkin, Dami\\'an Blasi,\n  and Hanna Wallach", "title": "On the Relationships Between the Grammatical Genders of Inanimate Nouns\n  and Their Co-Occurring Adjectives and Verbs", "comments": "17 pages, 6 figures, 4 tables, TACL(a) final submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use large-scale corpora in six different gendered languages, along with\ntools from NLP and information theory, to test whether there is a relationship\nbetween the grammatical genders of inanimate nouns and the adjectives used to\ndescribe those nouns. For all six languages, we find that there is a\nstatistically significant relationship. We also find that there are\nstatistically significant relationships between the grammatical genders of\ninanimate nouns and the verbs that take those nouns as direct objects, as\nindirect objects, and as subjects. We defer a deeper investigation of these\nrelationships for future work.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 22:49:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Williams", "Adina", ""], ["Cotterell", "Ryan", ""], ["Wolf-Sonkin", "Lawrence", ""], ["Blasi", "Dami\u00e1n", ""], ["Wallach", "Hanna", ""]]}, {"id": "2005.01218", "submitter": "Vikas Yadav", "authors": "Vikas Yadav, Steven Bethard and Mihai Surdeanu", "title": "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop\n  Question Answering", "comments": "Accepted at ACL 2020 as a long conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence retrieval is a critical stage of question answering (QA), necessary\nnot only to improve performance, but also to explain the decisions of the\ncorresponding QA method. We introduce a simple, fast, and unsupervised\niterative evidence retrieval method, which relies on three ideas: (a) an\nunsupervised alignment approach to soft-align questions and answers with\njustification sentences using only GloVe embeddings, (b) an iterative process\nthat reformulates queries focusing on terms that are not covered by existing\njustifications, which (c) a stopping criterion that terminates retrieval when\nthe terms in the given question and candidate answers are covered by the\nretrieved justifications. Despite its simplicity, our approach outperforms all\nthe previous methods (including supervised methods) on the evidence selection\ntask on two datasets: MultiRC and QASC. When these evidence sentences are fed\ninto a RoBERTa answer classification component, we achieve state-of-the-art QA\nperformance on these two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 00:19:48 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yadav", "Vikas", ""], ["Bethard", "Steven", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "2005.01229", "submitter": "Erik Jones", "authors": "Erik Jones, Robin Jia, Aditi Raghunathan, and Percy Liang", "title": "Robust Encodings: A Framework for Combating Adversarial Typos", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite excellent performance on many tasks, NLP systems are easily fooled by\nsmall adversarial perturbations of inputs. Existing procedures to defend\nagainst such perturbations are either (i) heuristic in nature and susceptible\nto stronger attacks or (ii) provide guaranteed robustness to worst-case\nattacks, but are incompatible with state-of-the-art models like BERT. In this\nwork, we introduce robust encodings (RobEn): a simple framework that confers\nguaranteed robustness, without making compromises on model architecture. The\ncore component of RobEn is an encoding function, which maps sentences to a\nsmaller, discrete space of encodings. Systems using these encodings as a\nbottleneck confer guaranteed robustness with standard training, and the same\nencodings can be used across multiple tasks. We identify two desiderata to\nconstruct robust encoding functions: perturbations of a sentence should map to\na small set of encodings (stability), and models using encodings should still\nperform well (fidelity). We instantiate RobEn to defend against a large family\nof adversarial typos. Across six tasks from GLUE, our instantiation of RobEn\npaired with BERT achieves an average robust accuracy of 71.3% against all\nadversarial typos in the family considered, while previous work using a\ntypo-corrector achieves only 35.3% accuracy against a simple greedy attack.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 01:28:18 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Jones", "Erik", ""], ["Jia", "Robin", ""], ["Raghunathan", "Aditi", ""], ["Liang", "Percy", ""]]}, {"id": "2005.01259", "submitter": "Liyan Xu", "authors": "Liyan Xu, Julien Hogan, Rachel E. Patzer and Jinho D. Choi", "title": "Noise Pollution in Hospital Readmission Prediction: Long Document\n  Classification with Reinforcement Learning", "comments": "Accepted to the ACL Workshop on Biomedical Natural Language\n  Processing, BioNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a reinforcement learning approach to extract noise in\nlong clinical documents for the task of readmission prediction after kidney\ntransplant. We face the challenges of developing robust models on a small\ndataset where each document may consist of over 10K tokens with full of noise\nincluding tabular text and task-irrelevant sentences. We first experiment four\ntypes of encoders to empirically decide the best document representation, and\nthen apply reinforcement learning to remove noisy text from the long documents,\nwhich models the noise extraction process as a sequential decision problem. Our\nresults show that the old bag-of-words encoder outperforms deep learning-based\nencoders on this task, and reinforcement learning is able to improve upon\nbaseline while pruning out 25% text segments. Our analysis depicts that\nreinforcement learning is able to identify both typical noisy tokens and\ntask-specific noisy text.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 04:06:53 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 04:36:36 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Xu", "Liyan", ""], ["Hogan", "Julien", ""], ["Patzer", "Rachel E.", ""], ["Choi", "Jinho D.", ""]]}, {"id": "2005.01278", "submitter": "Zhiqiang Zhan", "authors": "Zhiqiang Zhan, Zifeng Hou, Yang Zhang", "title": "A New Data Normalization Method to Improve Dialogue Generation by\n  Minimizing Long Tail Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural models have shown significant progress in dialogue generation.\nMost generation models are based on language models. However, due to the Long\nTail Phenomenon in linguistics, the trained models tend to generate words that\nappear frequently in training datasets, leading to a monotonous issue. To\naddress this issue, we analyze a large corpus from Wikipedia and propose three\nfrequency-based data normalization methods. We conduct extensive experiments\nbased on transformers and three datasets respectively collected from social\nmedia, subtitles, and the industrial application. Experimental results\ndemonstrate significant improvements in diversity and informativeness (defined\nas the numbers of nouns and verbs) of generated responses. More specifically,\nthe unigram and bigram diversity are increased by 2.6%-12.6% and 2.2%-18.9% on\nthe three datasets, respectively. Moreover, the informativeness, i.e. the\nnumbers of nouns and verbs, are increased by 4.0%-7.0% and 1.4%-12.1%,\nrespectively. Additionally, the simplicity and effectiveness enable our methods\nto be adapted to different generation models without much extra computational\ncost.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 05:20:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhan", "Zhiqiang", ""], ["Hou", "Zifeng", ""], ["Zhang", "Yang", ""]]}, {"id": "2005.01279", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin\n  Wang, Zheng Wen, Lawrence Carin", "title": "Improving Adversarial Text Generation by Modeling the Distant Future", "comments": "ACL 2020. arXiv admin note: substantial text overlap with\n  arXiv:1811.00696", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-regressive text generation models usually focus on local fluency, and\nmay cause inconsistent semantic meaning in long text generation. Further,\nautomatically generating words with similar semantics is challenging, and\nhand-crafted linguistic rules are difficult to apply. We consider a text\nplanning scheme and present a model-based imitation-learning approach to\nalleviate the aforementioned issues. Specifically, we propose a novel guider\nnetwork to focus on the generative process over a longer horizon, which can\nassist next-word prediction and provide intermediate rewards for generator\noptimization. Extensive experiments demonstrate that the proposed method leads\nto improved performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 05:45:13 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Chen", "Changyou", ""], ["Gan", "Zhe", ""], ["Wang", "Wenlin", ""], ["Shen", "Dinghan", ""], ["Wang", "Guoyin", ""], ["Wen", "Zheng", ""], ["Carin", "Lawrence", ""]]}, {"id": "2005.01281", "submitter": "Afshin Rahimi", "authors": "Afshin Rahimi and Timothy Baldwin and Karin Verspoor", "title": "WikiUMLS: Aligning UMLS to Wikipedia via Cross-lingual Neural Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our work on aligning the Unified Medical Language System (UMLS) to\nWikipedia, to facilitate manual alignment of the two resources. We propose a\ncross-lingual neural reranking model to match a UMLS concept with a Wikipedia\npage, which achieves a recall@1 of 72%, a substantial improvement of 20% over\nword- and char-level BM25, enabling manual alignment with minimal effort. We\nrelease our resources, including ranked Wikipedia pages for 700k UMLS concepts,\nand WikiUMLS, a dataset for training and evaluation of alignment models between\nUMLS and Wikipedia. This will provide easier access to Wikipedia for health\nprofessionals, patients, and NLP systems, including in multilingual settings.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 05:52:10 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 11:23:26 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 06:24:30 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Rahimi", "Afshin", ""], ["Baldwin", "Timothy", ""], ["Verspoor", "Karin", ""]]}, {"id": "2005.01282", "submitter": "Peng Jin", "authors": "Ping Cai, Xingyuan Chen, Peng Jin, Hongjun Wang, Tianrui Li", "title": "Distributional Discrepancy: A Metric for Unconditional Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of unconditional text generation is to train a model with real\nsentences, then generate novel sentences of the same quality and diversity as\nthe training data. However, when different metrics are used for comparing the\nmethods of unconditional text generation, contradictory conclusions are drawn.\nThe difficulty is that both the diversity and quality of the sample should be\nconsidered simultaneously when the models are evaluated. To solve this problem,\na novel metric of distributional discrepancy (DD) is designed to evaluate\ngenerators based on the discrepancy between the generated and real training\nsentences. However, it cannot compute the DD directly because the distribution\nof real sentences is unavailable. Thus, we propose a method for estimating the\nDD by training a neural-network-based text classifier. For comparison, three\nexisting metrics, bi-lingual evaluation understudy (BLEU) versus self-BLEU,\nlanguage model score versus reverse language model score, and Fr\\'{e}chet\nembedding distance, along with the proposed DD, are used to evaluate two\npopular generative models of long short-term memory and generative pretrained\ntransformer 2 on both syntactic and real data. Experimental results show that\nDD is significantly better than the three existing metrics for ranking these\ngenerative models.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 05:53:34 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 15:40:14 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Cai", "Ping", ""], ["Chen", "Xingyuan", ""], ["Jin", "Peng", ""], ["Wang", "Hongjun", ""], ["Li", "Tianrui", ""]]}, {"id": "2005.01306", "submitter": "Aryeh Tiktinsky", "authors": "Aryeh Tiktinsky, Yoav Goldberg, Reut Tsarfaty", "title": "pyBART: Evidence-based Syntactic Transformations for IE", "comments": "Accepted ACL2020 system demonstration paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic dependencies can be predicted with high accuracy, and are useful\nfor both machine-learned and pattern-based information extraction tasks.\nHowever, their utility can be improved. These syntactic dependencies are\ndesigned to accurately reflect syntactic relations, and they do not make\nsemantic relations explicit. Therefore, these representations lack many\nexplicit connections between content words, that would be useful for downstream\napplications. Proposals like English Enhanced UD improve the situation by\nextending universal dependency trees with additional explicit arcs. However,\nthey are not available to Python users, and are also limited in coverage. We\nintroduce a broad-coverage, data-driven and linguistically sound set of\ntransformations, that makes event-structure and many lexical relations\nexplicit. We present pyBART, an easy-to-use open-source Python library for\nconverting English UD trees either to Enhanced UD graphs or to our\nrepresentation. The library can work as a standalone package or be integrated\nwithin a spaCy NLP pipeline. When evaluated in a pattern-based relation\nextraction scenario, our representation results in higher extraction scores\nthan Enhanced UD, while requiring fewer patterns.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 07:38:34 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 10:25:10 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tiktinsky", "Aryeh", ""], ["Goldberg", "Yoav", ""], ["Tsarfaty", "Reut", ""]]}, {"id": "2005.01320", "submitter": "Chung-Chi Chen", "authors": "Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen", "title": "NLP in FinTech Applications: Past, Present and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial Technology (FinTech) is one of the worldwide rapidly-rising topics\nin the past five years according to the statistics of FinTech from Google\nTrends. In this position paper, we focus on the researches applying natural\nlanguage processing (NLP) technologies in the finance domain. Our goal is to\nindicate the position we are now and provide the blueprint for future\nresearches. We go through the application scenarios from three aspects\nincluding Know Your Customer (KYC), Know Your Product (KYP), and Satisfy Your\nCustomer (SYC). Both formal documents and informal textual data are analyzed to\nunderstand corporate customers and personal customers. Furthermore, we talk\nover how to dynamically update the features of products from the prospect and\nthe risk points of view. Finally, we discuss satisfying the customers in both\nB2C and C2C business models. After summarizing the past and the recent\nchallenges, we highlight several promising future research directions in the\ntrend of FinTech and the open finance tendency.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:37:27 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chen", "Chung-Chi", ""], ["Huang", "Hen-Hsen", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "2005.01328", "submitter": "Jon Ander Campos", "authors": "Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark\n  Cieliebak, Eneko Agirre", "title": "DoQA -- Accessing Domain-Specific FAQs via Conversational QA", "comments": "Accepted at ACL 2020. 13 pages 4 figures", "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics. 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to build conversational Question Answering (QA)\ninterfaces for the large body of domain-specific information available in FAQ\nsites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The\ndialogues are collected from three Stack Exchange sites using the Wizard of Oz\nmethod with crowdsourcing. Compared to previous work, DoQA comprises\nwell-defined information needs, leading to more coherent and natural\nconversations with less factoid questions and is multi-domain. In addition, we\nintroduce a more realistic information retrieval(IR) scenario where the system\nneeds to find the answer in any of the FAQ documents. The results of an\nexisting, strong, system show that, thanks to transfer learning from a\nWikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to\nbuild high quality conversational QA systems for FAQs without in-domain\ntraining data. The good results carry over into the more challenging IR\nscenario. In both cases, there is still ample room for improvement, as\nindicated by the higher human upperbound.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:58:54 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 07:54:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Campos", "Jon Ander", ""], ["Otegi", "Arantxa", ""], ["Soroa", "Aitor", ""], ["Deriu", "Jan", ""], ["Cieliebak", "Mark", ""], ["Agirre", "Eneko", ""]]}, {"id": "2005.01330", "submitter": "Stav Klein", "authors": "Reut Tsarfaty, Dan Bareket, Stav Klein, Amit Seker", "title": "From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of\n  Parsing Morphologically-Rich Languages (MRLs)?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been exactly a decade since the first establishment of SPMRL, a\nresearch initiative unifying multiple research efforts to address the peculiar\nchallenges of Statistical Parsing for Morphologically-Rich Languages\n(MRLs).Here we reflect on parsing MRLs in that decade, highlight the solutions\nand lessons learned for the architectural, modeling and lexical challenges in\nthe pre-neural era, and argue that similar challenges re-emerge in neural\narchitectures for MRLs. We then aim to offer a climax, suggesting that\nincorporating symbolic ideas proposed in SPMRL terms into nowadays neural\narchitectures has the potential to push NLP for MRLs to a new level. We sketch\nstrategies for designing Neural Models for MRLs (NMRL), and showcase\npreliminary support for these strategies via investigating the task of\nmulti-tagging in Hebrew, a morphologically-rich, high-fusion, language\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:05:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Tsarfaty", "Reut", ""], ["Bareket", "Dan", ""], ["Klein", "Stav", ""], ["Seker", "Amit", ""]]}, {"id": "2005.01348", "submitter": "Mostafa Abdou", "authors": "Mostafa Abdou, Vinit Ravishankar, Maria Barrett, Yonatan Belinkov,\n  Desmond Elliott, Anders S{\\o}gaard", "title": "The Sensitivity of Language Models and Humans to Winograd Schema\n  Perturbations", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pretrained language models are the major driving force behind\nrecent improvements in performance on the Winograd Schema Challenge, a widely\nemployed test of common sense reasoning ability. We show, however, with a new\ndiagnostic dataset, that these models are sensitive to linguistic perturbations\nof the Winograd examples that minimally affect human understanding. Our results\nhighlight interesting differences between humans and language models: language\nmodels are more sensitive to number or gender alternations and synonym\nreplacements than humans, and humans are more stable and consistent in their\npredictions, maintain a much higher absolute performance, and perform better on\nnon-associative instances than associative ones. Overall, humans are correct\nmore often than out-of-the-box models, and the models are sometimes right for\nthe wrong reasons. Finally, we show that fine-tuning on a large, task-specific\ndataset can offer a solution to these issues.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:44:54 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 06:48:57 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Abdou", "Mostafa", ""], ["Ravishankar", "Vinit", ""], ["Barrett", "Maria", ""], ["Belinkov", "Yonatan", ""], ["Elliott", "Desmond", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2005.01355", "submitter": "Alvaro Veizaga", "authors": "Alvaro Veizaga, Mauricio Alferez, Damiano Torre, Mehrdad Sabetzadeh,\n  Lionel Briand", "title": "On Systematically Building a Controlled Natural Language for Functional\n  Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Context] Natural language (NL) is pervasive in software requirements\nspecifications (SRSs). However, despite its popularity and widespread use, NL\nis highly prone to quality issues such as vagueness, ambiguity, and\nincompleteness. Controlled natural languages (CNLs) have been proposed as a way\nto prevent quality problems in requirements documents, while maintaining the\nflexibility to write and communicate requirements in an intuitive and\nuniversally understood manner. [Objective] In collaboration with an industrial\npartner from the financial domain, we systematically develop and evaluate a\nCNL, named Rimay, intended at helping analysts write functional requirements.\n[Method] We rely on Grounded Theory for building Rimay and follow well-known\nguidelines for conducting and reporting industrial case study research.\n[Results] Our main contributions are: (1) a qualitative methodology to\nsystematically define a CNL for functional requirements; this methodology is\ngeneral and applicable to information systems beyond the financial domain, (2)\na CNL grammar to represent functional requirements; this grammar is derived\nfrom our experience in the financial domain, but should be applicable, possibly\nwith adaptations, to other information-system domains, and (3) an empirical\nevaluation of our CNL (Rimay) through an industrial case study. Our\ncontributions draw on 15 representative SRSs, collectively containing 3215 NL\nrequirements statements from the financial domain. [Conclusion] Our evaluation\nshows that Rimay is expressive enough to capture, on average, 88% (405 out of\n460) of the NL requirements statements in four previously unseen SRSs from the\nfinancial domain.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:55:38 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Veizaga", "Alvaro", ""], ["Alferez", "Mauricio", ""], ["Torre", "Damiano", ""], ["Sabetzadeh", "Mehrdad", ""], ["Briand", "Lionel", ""]]}, {"id": "2005.01387", "submitter": "Natalia Tomashenko", "authors": "Natalia Tomashenko, Brij Mohan Lal Srivastava, Xin Wang, Emmanuel\n  Vincent, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Jose Patino,\n  Jean-Fran\\c{c}ois Bonastre, Paul-Gauthier No\\'e, Massimiliano Todisco", "title": "Introducing the VoicePrivacy Initiative", "comments": "Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The VoicePrivacy initiative aims to promote the development of privacy\npreservation tools for speech technology by gathering a new community to define\nthe tasks of interest and the evaluation methodology, and benchmarking\nsolutions through a series of challenges. In this paper, we formulate the voice\nanonymization task selected for the VoicePrivacy 2020 Challenge and describe\nthe datasets used for system development and evaluation. We also present the\nattack models and the associated objective and subjective evaluation metrics.\nWe introduce two anonymization baselines and report objective evaluation\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:07:52 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 08:48:45 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 22:02:45 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Tomashenko", "Natalia", ""], ["Srivastava", "Brij Mohan Lal", ""], ["Wang", "Xin", ""], ["Vincent", "Emmanuel", ""], ["Nautsch", "Andreas", ""], ["Yamagishi", "Junichi", ""], ["Evans", "Nicholas", ""], ["Patino", "Jose", ""], ["Bonastre", "Jean-Fran\u00e7ois", ""], ["No\u00e9", "Paul-Gauthier", ""], ["Todisco", "Massimiliano", ""]]}, {"id": "2005.01400", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Stavros Petridis, Maja Pantic", "title": "Does Visual Self-Supervision Improve Learning of Speech Representations\n  for Emotion Recognition?", "comments": "Accepted for publication in IEEE Transactions on Affective Computing;\n  v3: Publication-ready version including additional experiments and discussion", "journal-ref": null, "doi": "10.1109/TAFFC.2021.3062406", "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has attracted plenty of recent research interest.\nHowever, most works for self-supervision in speech are typically unimodal and\nthere has been limited work that studies the interaction between audio and\nvisual modalities for cross-modal self-supervision. This work (1) investigates\nvisual self-supervision via face reconstruction to guide the learning of audio\nrepresentations; (2) proposes an audio-only self-supervision approach for\nspeech representation learning; (3) shows that a multi-task combination of the\nproposed visual and audio self-supervision is beneficial for learning richer\nfeatures that are more robust in noisy conditions; (4) shows that\nself-supervised pretraining can outperform fully supervised training and is\nespecially useful to prevent overfitting on smaller sized datasets. We evaluate\nour learned audio representations for discrete emotion recognition, continuous\naffect recognition and automatic speech recognition. We outperform existing\nself-supervised methods for all tested downstream tasks. Our results\ndemonstrate the potential of visual self-supervision for audio feature learning\nand suggest that joint visual and audio self-supervision leads to more\ninformative audio representations for speech and emotion recognition.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:33:40 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 16:46:13 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 11:35:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Shukla", "Abhinav", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2005.01483", "submitter": "Danielle Saunders", "authors": "Danielle Saunders, Felix Stahlberg, Bill Byrne", "title": "Using Context in Neural Machine Translation Training Objectives", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Machine Translation (NMT) training using document-level\nmetrics with batch-level documents. Previous sequence-objective approaches to\nNMT training focus exclusively on sentence-level metrics like sentence BLEU\nwhich do not correspond to the desired evaluation metric, typically document\nBLEU. Meanwhile research into document-level NMT training focuses on data or\nmodel architecture rather than training procedure. We find that each of these\nlines of research has a clear space in it for the other, and propose merging\nthem with a scheme that allows a document-level evaluation metric to be used in\nthe NMT training objective.\n  We first sample pseudo-documents from sentence samples. We then approximate\nthe expected document BLEU gradient with Monte Carlo sampling for use as a cost\nfunction in Minimum Risk Training (MRT). This two-level sampling procedure\ngives NMT performance gains over sequence MRT and maximum-likelihood training.\nWe demonstrate that training is more robust for document-level metrics than\nwith sequence metrics. We further demonstrate improvements on NMT with TER and\nGrammatical Error Correction (GEC) using GLEU, both metrics used at the\ndocument level for evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:42:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Saunders", "Danielle", ""], ["Stahlberg", "Felix", ""], ["Byrne", "Bill", ""]]}, {"id": "2005.01497", "submitter": "Salma El Anigri", "authors": "Salma El Anigri, Mohammed Majid Himmi, Abdelhak Mahmoudi", "title": "Towards A Sign Language Gloss Representation Of Modern Standard Arabic", "comments": "4 pages,2 figures, AfricaNLP2020 workshop ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over 5% of the world's population (466 million people) has disabling hearing\nloss. 4 million are children. They can be hard of hearing or deaf. Deaf people\nmostly have profound hearing loss. Which implies very little or no hearing.\nOver the world, deaf people often communicate using a sign language with\ngestures of both hands and facial expressions. The sign language is a\nfull-fledged natural language with its own grammar and lexicon. Therefore,\nthere is a need for translation models from and to sign languages. In this\nwork, we are interested in the translation of Modern Standard Arabic(MSAr) into\nsign language. We generated a gloss representation from MSAr that extracts the\nfeatures mandatory for the generation of animation signs. Our approach locates\nthe most pertinent features that maintain the meaning of the input Arabic\nsentence.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:56:43 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Anigri", "Salma El", ""], ["Himmi", "Mohammed Majid", ""], ["Mahmoudi", "Abdelhak", ""]]}, {"id": "2005.01525", "submitter": "Jesse Dunietz", "authors": "Jesse Dunietz, Gregory Burnham, Akash Bharadwaj, Owen Rambow, Jennifer\n  Chu-Carroll, David Ferrucci", "title": "To Test Machine Comprehension, Start by Defining Comprehension", "comments": "Camera-ready ACL 2020 paper (Theme track). 9 pages; 3 figures; 1\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many tasks aim to measure machine reading comprehension (MRC), often focusing\non question types presumed to be difficult. Rarely, however, do task designers\nstart by considering what systems should in fact comprehend. In this paper we\nmake two key contributions. First, we argue that existing approaches do not\nadequately define comprehension; they are too unsystematic about what content\nis tested. Second, we present a detailed definition of comprehension -- a\n\"Template of Understanding\" -- for a widely useful class of texts, namely short\nnarratives. We then conduct an experiment that strongly suggests existing\nsystems are not up to the task of narrative understanding as we define it.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:36:07 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 14:57:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Dunietz", "Jesse", ""], ["Burnham", "Gregory", ""], ["Bharadwaj", "Akash", ""], ["Rambow", "Owen", ""], ["Chu-Carroll", "Jennifer", ""], ["Ferrucci", "David", ""]]}, {"id": "2005.01526", "submitter": "Dheeraj Rajagopal", "authors": "Dheeraj Rajagopal, Niket Tandon, Bhavana Dalvi, Peter Clark, Eduard\n  Hovy", "title": "What-if I ask you to explain: Explaining the effects of perturbations in\n  procedural text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of explaining the effects of perturbations in procedural\ntext, an important test of process comprehension. Consider a passage describing\na rabbit's life-cycle: humans can easily explain the effect on the rabbit\npopulation if a female rabbit becomes ill -- i.e., the female rabbit would not\nbecome pregnant, and as a result not have babies leading to a decrease in\nrabbit population. We present QUARTET, a system that constructs such\nexplanations from paragraphs, by modeling the explanation task as a multitask\nlearning problem. QUARTET provides better explanations (based on the sentences\nin the procedural text) compared to several strong baselines on a recent\nprocess comprehension benchmark. We also present a surprising secondary effect:\nour model also achieves a new SOTA with a 7% absolute F1 improvement on a\ndownstream QA task. This illustrates that good explanations do not have to come\nat the expense of end task performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:36:23 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 01:16:34 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Rajagopal", "Dheeraj", ""], ["Tandon", "Niket", ""], ["Dalvi", "Bhavana", ""], ["Clark", "Peter", ""], ["Hovy", "Eduard", ""]]}, {"id": "2005.01556", "submitter": "Lei Shen", "authors": "Lei Shen, Xiaoyu Guo, Meng Chen", "title": "Compose Like Humans: Jointly Improving the Coherence and Novelty for\n  Modern Chinese Poetry Generation", "comments": "To appear at IJCNN 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese poetry is an important part of worldwide culture, and classical and\nmodern sub-branches are quite different. The former is a unique genre and has\nstrict constraints, while the latter is very flexible in length, optional to\nhave rhymes, and similar to modern poetry in other languages. Thus, it requires\nmore to control the coherence and improve the novelty. In this paper, we\npropose a generate-retrieve-then-refine paradigm to jointly improve the\ncoherence and novelty. In the first stage, a draft is generated given keywords\n(i.e., topics) only. The second stage produces a \"refining vector\" from\nretrieval lines. At last, we take into consideration both the draft and the\n\"refining vector\" to generate a new poem. The draft provides future\nsentence-level information for a line to be generated. Meanwhile, the \"refining\nvector\" points out the direction of refinement based on impressive words\ndetection mechanism which can learn good patterns from references and then\ncreate new ones via insertion operation. Experimental results on a collected\nlarge-scale modern Chinese poetry dataset show that our proposed approach can\nnot only generate more coherent poems, but also improve the diversity and\nnovelty.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:16:10 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shen", "Lei", ""], ["Guo", "Xiaoyu", ""], ["Chen", "Meng", ""]]}, {"id": "2005.01618", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen, Lawrence\n  Carin", "title": "Reward Constrained Interactive Recommendation with Natural Language\n  Feedback", "comments": "Appeared in NeurIPS 2019; Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based interactive recommendation provides richer user feedback and has\ndemonstrated advantages over traditional interactive recommender systems.\nHowever, recommendations can easily violate preferences of users from their\npast natural-language feedback, since the recommender needs to explore new\nitems for further improvement. To alleviate this issue, we propose a novel\nconstraint-augmented reinforcement learning (RL) framework to efficiently\nincorporate user preferences over time. Specifically, we leverage a\ndiscriminator to detect recommendations violating user historical preference,\nwhich is incorporated into the standard RL objective of maximizing expected\ncumulative future rewards. Our proposed framework is general and is further\nextended to the task of constrained text generation. Empirical results show\nthat the proposed method yields consistent improvement relative to standard RL\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:23:34 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Yu", "Tong", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "2005.01619", "submitter": "Roy Bar-Haim", "authors": "Roy Bar-Haim, Lilach Eden, Roni Friedman, Yoav Kantor, Dan Lahav, Noam\n  Slonim", "title": "From Arguments to Key Points: Towards Automatic Argument Summarization", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a concise summary from a large collection of arguments on a given\ntopic is an intriguing yet understudied problem. We propose to represent such\nsummaries as a small set of talking points, termed \"key points\", each scored\naccording to its salience. We show, by analyzing a large dataset of\ncrowd-contributed arguments, that a small number of key points per topic is\ntypically sufficient for covering the vast majority of the arguments.\nFurthermore, we found that a domain expert can often predict these key points\nin advance. We study the task of argument-to-key point mapping, and introduce a\nnovel large-scale dataset for this task. We report empirical results for an\nextensive set of experiments with this dataset, showing promising performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:24:21 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 19:21:17 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bar-Haim", "Roy", ""], ["Eden", "Lilach", ""], ["Friedman", "Roni", ""], ["Kantor", "Yoav", ""], ["Lahav", "Dan", ""], ["Slonim", "Noam", ""]]}, {"id": "2005.01630", "submitter": "Alexander Erdmann", "authors": "Alexander Erdmann, Micha Elsner, Shijie Wu, Ryan Cotterell and Nizar\n  Habash", "title": "The Paradigm Discovery Problem", "comments": "Forthcoming at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work treats the paradigm discovery problem (PDP), the task of learning\nan inflectional morphological system from unannotated sentences. We formalize\nthe PDP and develop evaluation metrics for judging systems. Using currently\navailable resources, we construct datasets for the task. We also devise a\nheuristic benchmark for the PDP and report empirical results on five diverse\nlanguages. Our benchmark system first makes use of word embeddings and string\nsimilarity to cluster forms by cell and by paradigm. Then, we bootstrap a\nneural transducer on top of the clustered data to predict words to realize the\nempty paradigm slots. An error analysis of our system suggests clustering by\ncell across different inflection classes is the most pressing challenge for\nfuture work. Our code and data are available for public use.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:38:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Erdmann", "Alexander", ""], ["Elsner", "Micha", ""], ["Wu", "Shijie", ""], ["Cotterell", "Ryan", ""], ["Habash", "Nizar", ""]]}, {"id": "2005.01634", "submitter": "Jeniya Tabassum", "authors": "Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter", "title": "Code and Named Entity Recognition in StackOverflow", "comments": "updated with better results. (To appear in ACL 2020)", "journal-ref": null, "doi": null, "report-no": "Submission ID: 3161779", "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in studying natural language and computer\ncode together, as large corpora of programming texts become readily available\non the Internet. For example, StackOverflow currently has over 15 million\nprogramming related questions written by 8.5 million users. Meanwhile, there is\nstill a lack of fundamental NLP techniques for identifying code tokens or\nsoftware-related named entities that appear within natural language sentences.\nIn this paper, we introduce a new named entity recognition (NER) corpus for the\ncomputer programming domain, consisting of 15,372 sentences annotated with 20\nfine-grained entity types. We trained in-domain BERT representations\n(BERTOverflow) on 152 million sentences from StackOverflow, which lead to an\nabsolute increase of +10 F-1 score over off-the-shelf BERT. We also present the\nSoftNER model which achieves an overall 79.10 F$_1$ score for code and named\nentity recognition on StackOverflow data. Our SoftNER model incorporates a\ncontext-independent code token classifier with corpus-level features to improve\nthe BERT-based tagging model. Our code and data are available at:\nhttps://github.com/jeniyat/StackOverflowNER/\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:48:29 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 22:12:22 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 09:34:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Tabassum", "Jeniya", ""], ["Maddela", "Mounica", ""], ["Xu", "Wei", ""], ["Ritter", "Alan", ""]]}, {"id": "2005.01641", "submitter": "Rowan Hall Maudslay", "authors": "Rowan Hall Maudslay, Josef Valvoda, Tiago Pimentel, Adina Williams,\n  Ryan Cotterell", "title": "A Tale of a Probe and a Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring what linguistic information is encoded in neural models of language\nhas become popular in NLP. Researchers approach this enterprise by training\n\"probes\" - supervised models designed to extract linguistic structure from\nanother model's output. One such probe is the structural probe (Hewitt and\nManning, 2019), designed to quantify the extent to which syntactic information\nis encoded in contextualised word representations. The structural probe has a\nnovel design, unattested in the parsing literature, the precise benefit of\nwhich is not immediately obvious. To explore whether syntactic probes would do\nbetter to make use of existing techniques, we compare the structural probe to a\nmore traditional parser with an identical lightweight parameterisation. The\nparser outperforms structural probe on UUAS in seven of nine analysed\nlanguages, often by a substantial amount (e.g. by 11.1 points in English).\nUnder a second less common metric, however, there is the opposite trend - the\nstructural probe outperforms the parser. This begs the question: which metric\nshould we prefer?\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:57:31 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 10:38:43 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Maudslay", "Rowan Hall", ""], ["Valvoda", "Josef", ""], ["Pimentel", "Tiago", ""], ["Williams", "Adina", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2005.01646", "submitter": "Kartik Goyal", "authors": "Kartik Goyal, Chris Dyer, Christopher Warren, Max G'Sell, Taylor\n  Berg-Kirkpatrick", "title": "A Probabilistic Generative Model for Typographical Analysis of Early\n  Modern Printing", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep and interpretable probabilistic generative model to analyze\nglyph shapes in printed Early Modern documents. We focus on clustering\nextracted glyph images into underlying templates in the presence of multiple\nconfounding sources of variance. Our approach introduces a neural editor model\nthat first generates well-understood printing phenomena like spatial\nperturbations from template parameters via interpertable latent variables, and\nthen modifies the result by generating a non-interpretable latent vector\nresponsible for inking variations, jitter, noise from the archiving process,\nand other unforeseen phenomena associated with Early Modern printing.\nCritically, by introducing an inference network whose input is restricted to\nthe visual residual between the observation and the interpretably-modified\ntemplate, we are able to control and isolate what the vector-valued latent\nvariable captures. We show that our approach outperforms rigid interpretable\nclustering baselines (Ocular) and overly-flexible deep generative models (VAE)\nalike on the task of completely unsupervised discovery of typefaces in\nmixed-font documents.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:01:11 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Goyal", "Kartik", ""], ["Dyer", "Chris", ""], ["Warren", "Christopher", ""], ["G'Sell", "Max", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2005.01655", "submitter": "Arjun Akula", "authors": "Arjun R Akula, Spandana Gella, Yaser Al-Onaizan, Song-Chun Zhu, Siva\n  Reddy", "title": "Words aren't enough, their order matters: On the Robustness of Grounding\n  Visual Referring Expressions", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual referring expression recognition is a challenging task that requires\nnatural language understanding in the context of an image. We critically\nexamine RefCOCOg, a standard benchmark for this task, using a human study and\nshow that 83.7% of test instances do not require reasoning on linguistic\nstructure, i.e., words are enough to identify the target object, the word order\ndoesn't matter. To measure the true progress of existing models, we split the\ntest set into two sets, one which requires reasoning on linguistic structure\nand the other which doesn't. Additionally, we create an out-of-distribution\ndataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that\nthe target object changes. Using these datasets, we empirically show that\nexisting methods fail to exploit linguistic structure and are 12% to 23% lower\nin performance than the established progress for this task. We also propose two\nmethods, one based on contrastive learning and the other based on multi-task\nlearning, to increase the robustness of ViLBERT, the current state-of-the-art\nmodel for this task. Our datasets are publicly available at\nhttps://github.com/aws/aws-refcocog-adv\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:09:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Akula", "Arjun R", ""], ["Gella", "Spandana", ""], ["Al-Onaizan", "Yaser", ""], ["Zhu", "Song-Chun", ""], ["Reddy", "Siva", ""]]}, {"id": "2005.01672", "submitter": "Jierui Li", "authors": "Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming\n  Shi", "title": "Evaluating Explanation Methods for Neural Machine Translation", "comments": "Accepted to ACL 2020, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently many efforts have been devoted to interpreting the black-box NMT\nmodels, but little progress has been made on metrics to evaluate explanation\nmethods. Word Alignment Error Rate can be used as such a metric that matches\nhuman understanding, however, it can not measure explanation methods on those\ntarget words that are not aligned to any source word. This paper thereby makes\nan initial attempt to evaluate explanation methods from an alternative\nviewpoint. To this end, it proposes a principled metric based on fidelity in\nregard to the predictive behavior of the NMT model. As the exact computation\nfor this metric is intractable, we employ an efficient approach as its\napproximation. On six standard translation tasks, we quantitatively evaluate\nseveral explanation methods in terms of the proposed metric and we reveal some\nvaluable findings for these explanation methods in our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:26:25 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Li", "Jierui", ""], ["Liu", "Lemao", ""], ["Li", "Huayang", ""], ["Li", "Guanlin", ""], ["Huang", "Guoping", ""], ["Shi", "Shuming", ""]]}, {"id": "2005.01677", "submitter": "Young Mo Kang", "authors": "Young Mo Kang, Yingbo Zhou", "title": "Fast and Robust Unsupervised Contextual Biasing for Speech Recognition", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) system is becoming a ubiquitous\ntechnology. Although its accuracy is closing the gap with that of human level\nunder certain settings, one area that can further improve is to incorporate\nuser-specific information or context to bias its prediction. A common framework\nis to dynamically construct a small language model from the provided contextual\nmini corpus and interpolate its score with the main language model during the\ndecoding process.\n  Here we propose an alternative approach that does not entail explicit\ncontextual language model. Instead, we derive the bias score for every word in\nthe system vocabulary from the training corpus. The method is unique in that 1)\nit does not require meta-data or class-label annotation for the context or the\ntraining corpus. 2) The bias score is proportional to the word's\nlog-probability, thus not only would it bias the provided context, but also\nrobust against irrelevant context (e.g. user mis-specified or in case where it\nis hard to quantify a tight scope). 3) The bias score for the entire vocabulary\nis pre-determined during the training stage, thereby eliminating\ncomputationally expensive language model construction during inference.\n  We show significant improvement in recognition accuracy when the relevant\ncontext is available. Additionally, we also demonstrate that the proposed\nmethod exhibits high tolerance to false-triggering errors in the presence of\nirrelevant context.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:29:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kang", "Young Mo", ""], ["Zhou", "Yingbo", ""]]}, {"id": "2005.01678", "submitter": "Noriyuki Kojima", "authors": "Noriyuki Kojima, Hadar Averbuch-Elor, Alexander M. Rush, Yoav Artzi", "title": "What is Learned in Visually Grounded Neural Syntax Acquisition", "comments": "In ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual features are a promising signal for learning bootstrap textual models.\nHowever, blackbox learning models make it difficult to isolate the specific\ncontribution of visual components. In this analysis, we consider the case study\nof the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent\napproach for learning syntax from a visual training signal. By constructing\nsimplified versions of the model, we isolate the core factors that yield the\nmodel's strong performance. Contrary to what the model might be capable of\nlearning, we find significantly less expressive versions produce similar\npredictions and perform just as well, or even better. We also find that a\nsimple lexical signal of noun concreteness plays the main role in the model's\npredictions as opposed to more complex syntactic reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:32:20 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 19:19:57 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kojima", "Noriyuki", ""], ["Averbuch-Elor", "Hadar", ""], ["Rush", "Alexander M.", ""], ["Artzi", "Yoav", ""]]}, {"id": "2005.01777", "submitter": "Ngoc Thang Vu", "authors": "Chia-Yu Li, Daniel Ortega, Dirk V\\\"ath, Florian Lux, Lindsey\n  Vanderlyn, Maximilian Schmidt, Michael Neumann, Moritz V\\\"olkel, Pavel\n  Denisov, Sabrina Jenne, Zorica Kacarevic and Ngoc Thang Vu", "title": "ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and\n  Socially-engaged Conversational Agents", "comments": "All authors contributed equally. Accepted to be presented at ACL -\n  System demonstrations - 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ADVISER - an open-source, multi-domain dialog system toolkit that\nenables the development of multi-modal (incorporating speech, text and vision),\nsocially-engaged (e.g. emotion recognition, engagement level prediction and\nbackchanneling) conversational agents. The final Python-based implementation of\nour toolkit is flexible, easy to use, and easy to extend not only for\ntechnically experienced users, such as machine learning researchers, but also\nfor less technically experienced users, such as linguists or cognitive\nscientists, thereby providing a flexible platform for collaborative research.\nLink to open-source code: https://github.com/DigitalPhonetics/adviser\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 18:27:58 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Li", "Chia-Yu", ""], ["Ortega", "Daniel", ""], ["V\u00e4th", "Dirk", ""], ["Lux", "Florian", ""], ["Vanderlyn", "Lindsey", ""], ["Schmidt", "Maximilian", ""], ["Neumann", "Michael", ""], ["V\u00f6lkel", "Moritz", ""], ["Denisov", "Pavel", ""], ["Jenne", "Sabrina", ""], ["Kacarevic", "Zorica", ""], ["Vu", "Ngoc Thang", ""]]}, {"id": "2005.01791", "submitter": "Raphael Schumann", "authors": "Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, Katja Markert", "title": "Discrete Optimization for Unsupervised Sentence Summarization with\n  Word-Level Extraction", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic sentence summarization produces a shorter version of a sentence,\nwhile preserving its most important information. A good summary is\ncharacterized by language fluency and high information overlap with the source\nsentence. We model these two aspects in an unsupervised objective function,\nconsisting of language modeling and semantic similarity metrics. We search for\na high-scoring summary by discrete optimization. Our proposed method achieves a\nnew state-of-the art for unsupervised sentence summarization according to ROUGE\nscores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric\nis sensitive to summary length. Since this is unwillingly exploited in recent\nwork, we emphasize that future evaluation should explicitly group summarization\nsystems by output length brackets.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:01:55 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Schumann", "Raphael", ""], ["Mou", "Lili", ""], ["Lu", "Yao", ""], ["Vechtomova", "Olga", ""], ["Markert", "Katja", ""]]}, {"id": "2005.01795", "submitter": "Kundan Krishna", "authors": "Kundan Krishna, Sopan Khosla, Jeffrey P. Bigham, Zachary C. Lipton", "title": "Generating SOAP Notes from Doctor-Patient Conversations Using Modular\n  Summarization Techniques", "comments": "Published at ACL 2021 Main Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following each patient visit, physicians draft long semi-structured clinical\nsummaries called SOAP notes. While invaluable to clinicians and researchers,\ncreating digital SOAP notes is burdensome, contributing to physician burnout.\nIn this paper, we introduce the first complete pipelines to leverage deep\nsummarization models to generate these notes based on transcripts of\nconversations between physicians and patients. After exploring a spectrum of\nmethods across the extractive-abstractive spectrum, we propose Cluster2Sent, an\nalgorithm that (i) extracts important utterances relevant to each summary\nsection; (ii) clusters together related utterances; and then (iii) generates\none summary sentence per cluster. Cluster2Sent outperforms its purely\nabstractive counterpart by 8 ROUGE-1 points, and produces significantly more\nfactual and coherent sentences as assessed by expert human evaluators. For\nreproducibility, we demonstrate similar benefits on the publicly available AMI\ndataset. Our results speak to the benefits of structuring summaries into\nsections and annotating supporting evidence when constructing summarization\ncorpora.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:10:26 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 04:09:10 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 14:48:09 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Krishna", "Kundan", ""], ["Khosla", "Sopan", ""], ["Bigham", "Jeffrey P.", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "2005.01803", "submitter": "Haewoon Kwak", "authors": "Haewoon Kwak and Jisun An and Yong-Yeol Ahn", "title": "A Systematic Media Frame Analysis of 1.5 Million New York Times Articles\n  from 2000 to 2017", "comments": "10pages, WebSci'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Framing is an indispensable narrative device for news media because even the\nsame facts may lead to conflicting understandings if deliberate framing is\nemployed. Therefore, identifying media framing is a crucial step to\nunderstanding how news media influence the public. Framing is, however,\ndifficult to operationalize and detect, and thus traditional media framing\nstudies had to rely on manual annotation, which is challenging to scale up to\nmassive news datasets. Here, by developing a media frame classifier that\nachieves state-of-the-art performance, we systematically analyze the media\nframes of 1.5 million New York Times articles published from 2000 to 2017. By\nexamining the ebb and flow of media frames over almost two decades, we show\nthat short-term frame abundance fluctuation closely corresponds to major\nevents, while there also exist several long-term trends, such as the gradually\nincreasing prevalence of the ``Cultural identity'' frame. By examining specific\ntopics and sentiments, we identify characteristics and dynamics of each frame.\nFinally, as a case study, we delve into the framing of mass shootings,\nrevealing three major framing patterns. Our scalable, computational approach to\nmassive news datasets opens up new pathways for systematic media framing\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:25:39 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Kwak", "Haewoon", ""], ["An", "Jisun", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "2005.01810", "submitter": "Josef Klafka", "authors": "Josef Klafka and Allyson Ettinger", "title": "Spying on your neighbors: Fine-grained probing of contextual embeddings\n  for information about surrounding words", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although models using contextual word embeddings have achieved\nstate-of-the-art results on a host of NLP tasks, little is known about exactly\nwhat information these embeddings encode about the context words that they are\nunderstood to reflect. To address this question, we introduce a suite of\nprobing tasks that enable fine-grained testing of contextual embeddings for\nencoding of information about surrounding words. We apply these tasks to\nexamine the popular BERT, ELMo and GPT contextual encoders, and find that each\nof our tested information types is indeed encoded as contextual information\nacross tokens, often with near-perfect recoverability-but the encoders vary in\nwhich features they distribute to which tokens, how nuanced their distributions\nare, and how robust the encoding of each feature is to distance. We discuss\nimplications of these results for how different types of models breakdown and\nprioritize word-level context information when constructing token embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:34:46 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Klafka", "Josef", ""], ["Ettinger", "Allyson", ""]]}, {"id": "2005.01822", "submitter": "Shrimai Prabhumoye", "authors": "Shrimai Prabhumoye, Alan W Black, Ruslan Salakhutdinov", "title": "Exploring Controllable Text Generation Techniques", "comments": "Will be published at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural controllable text generation is an important area gaining attention\ndue to its plethora of applications. Although there is a large body of prior\nwork in controllable text generation, there is no unifying theme. In this work,\nwe provide a new schema of the pipeline of the generation process by\nclassifying it into five modules. The control of attributes in the generation\nprocess requires modification of these modules. We present an overview of\ndifferent techniques used to perform the modulation of these modules. We also\nprovide an analysis on the advantages and disadvantages of these techniques. We\nfurther pave ways to develop new architectures based on the combination of the\nmodules described in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:04:47 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 21:28:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Prabhumoye", "Shrimai", ""], ["Black", "Alan W", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2005.01828", "submitter": "Eric Melz", "authors": "Eric Melz", "title": "Understanding Scanned Receipts", "comments": "8 pages, 3 figures, no conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasking machines with understanding receipts can have important applications\nsuch as enabling detailed analytics on purchases, enforcing expense policies,\nand inferring patterns of purchase behavior on large collections of receipts.\nIn this paper, we focus on the task of Named Entity Linking (NEL) of scanned\nreceipt line items; specifically, the task entails associating shorthand text\nfrom OCR'd receipts with a knowledge base (KB) of grocery products. For\nexample, the scanned item \"STO BABY SPINACH\" should be linked to the catalog\nitem labeled \"Simple Truth Organic Baby Spinach\". Experiments that employ a\nvariety of Information Retrieval techniques in combination with statistical\nphrase detection shows promise for effective understanding of scanned receipt\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:20:33 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Melz", "Eric", ""]]}, {"id": "2005.01831", "submitter": "Peter Hase", "authors": "Peter Hase, Mohit Bansal", "title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users\n  Predict Model Behavior?", "comments": "ACL 2020 (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic approaches to interpreting machine learning models have\nproliferated in recent years. We carry out human subject tests that are the\nfirst of their kind to isolate the effect of algorithmic explanations on a key\naspect of model interpretability, simulatability, while avoiding important\nconfounding experimental factors. A model is simulatable when a person can\npredict its behavior on new inputs. Through two kinds of simulation tests\ninvolving text and tabular data, we evaluate five explanations methods: (1)\nLIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a\nComposite approach that combines explanations from each method. Clear evidence\nof method effectiveness is found in very few cases: LIME improves\nsimulatability in tabular classification, and our Prototype method is effective\nin counterfactual simulation tests. We also collect subjective ratings of\nexplanations, but we do not find that ratings are predictive of how helpful\nexplanations are. Our results provide the first reliable and comprehensive\nestimates of how explanations influence simulatability across a variety of\nexplanation methods and data domains. We show that (1) we need to be careful\nabout the metrics we use to evaluate explanation methods, and (2) there is\nsignificant room for improvement in current methods. All our supporting code,\ndata, and models are publicly available at:\nhttps://github.com/peterbhase/InterpretableNLP-ACL2020\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:35:17 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Hase", "Peter", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.01840", "submitter": "Bryan Li", "authors": "Faisal Ladhak and Bryan Li and Yaser Al-Onaizan and Kathleen McKeown", "title": "Exploring Content Selection in Summarization of Novel Chapters", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new summarization task, generating summaries of novel chapters\nusing summary/chapter pairs from online study guides. This is a harder task\nthan the news summarization task, given the chapter length as well as the\nextreme paraphrasing and generalization found in the summaries. We focus on\nextractive summarization, which requires the creation of a gold-standard set of\nextractive summaries. We present a new metric for aligning reference summary\nsentences with chapter sentences to create gold extracts and also experiment\nwith different alignment methods. Our experiments demonstrate significant\nimprovement over prior alignment approaches for our task as shown through\nautomatic metrics and a crowd-sourced pyramid analysis. We make our data\ncollection scripts available at\nhttps://github.com/manestay/novel-chapter-dataset .\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:45:39 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 22:42:49 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 01:05:09 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ladhak", "Faisal", ""], ["Li", "Bryan", ""], ["Al-Onaizan", "Yaser", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2005.01854", "submitter": "Thomas Kober", "authors": "Thomas Kober, Julie Weeds, Lorenzo Bertolini, David Weir", "title": "Data Augmentation for Hypernymy Detection", "comments": "to appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of hypernymy relationships represents a challenging\nproblem in NLP. The successful application of state-of-the-art supervised\napproaches using distributed representations has generally been impeded by the\nlimited availability of high quality training data. We have developed two novel\ndata augmentation techniques which generate new training examples from existing\nones. First, we combine the linguistic principles of hypernym transitivity and\nintersective modifier-noun composition to generate additional pairs of vectors,\nsuch as \"small dog - dog\" or \"small dog - animal\", for which a hypernymy\nrelationship can be assumed. Second, we use generative adversarial networks\n(GANs) to generate pairs of vectors for which the hypernymy relation can also\nbe assumed. We furthermore present two complementary strategies for extending\nan existing dataset by leveraging linguistic resources such as WordNet. Using\nan evaluation across 3 different datasets for hypernymy detection and 2\ndifferent vector spaces, we demonstrate that both of the proposed automatic\ndata augmentation and dataset extension strategies substantially improve\nclassifier performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:32:12 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 21:13:16 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kober", "Thomas", ""], ["Weeds", "Julie", ""], ["Bertolini", "Lorenzo", ""], ["Weir", "David", ""]]}, {"id": "2005.01866", "submitter": "Shruti Rijhwani", "authors": "Shruti Rijhwani, Shuyan Zhou, Graham Neubig, Jaime Carbonell", "title": "Soft Gazetteers for Low-Resource Named Entity Recognition", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional named entity recognition models use gazetteers (lists of\nentities) as features to improve performance. Although modern neural network\nmodels do not require such hand-crafted features for strong performance, recent\nwork has demonstrated their utility for named entity recognition on English\ndata. However, designing such features for low-resource languages is\nchallenging, because exhaustive entity gazetteers do not exist in these\nlanguages. To address this problem, we propose a method of \"soft gazetteers\"\nthat incorporates ubiquitously available information from English knowledge\nbases, such as Wikipedia, into neural named entity recognition models through\ncross-lingual entity linking. Our experiments on four low-resource languages\nshow an average improvement of 4 points in F1 score. Code and data are\navailable at https://github.com/neulab/soft-gazetteers.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:58:02 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Rijhwani", "Shruti", ""], ["Zhou", "Shuyan", ""], ["Neubig", "Graham", ""], ["Carbonell", "Jaime", ""]]}, {"id": "2005.01879", "submitter": "Majid Asgari-Bidhendi", "authors": "Majid Asgari-Bidhendi, Behrooz Janfada, Behrouz Minaei-Bidgoli", "title": "FarsBase-KBP: A Knowledge Base Population System for the Persian\n  Knowledge Graph", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most of the knowledge bases already support the English language, there\nis only one knowledge base for the Persian language, known as FarsBase, which\nis automatically created via semi-structured web information. Unlike English\nknowledge bases such as Wikidata, which have tremendous community support, the\npopulation of a knowledge base like FarsBase must rely on automatically\nextracted knowledge. Knowledge base population can let FarsBase keep growing in\nsize, as the system continues working. In this paper, we present a knowledge\nbase population system for the Persian language, which extracts knowledge from\nunlabeled raw text, crawled from the Web. The proposed system consists of a set\nof state-of-the-art modules such as an entity linking module as well as\ninformation and relation extraction modules designed for FarsBase. Moreover, a\ncanonicalization system is introduced to link extracted relations to FarsBase\nproperties. Then, the system uses knowledge fusion techniques with minimal\nintervention of human experts to integrate and filter the proper knowledge\ninstances, extracted by each module. To evaluate the performance of the\npresented knowledge base population system, we present the first gold dataset\nfor benchmarking knowledge base population in the Persian language, which\nconsisting of 22015 FarsBase triples and verified by human experts. The\nevaluation results demonstrate the efficiency of the proposed system.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 22:51:54 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Asgari-Bidhendi", "Majid", ""], ["Janfada", "Behrooz", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2005.01897", "submitter": "Chung-Chi Chen", "authors": "Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen", "title": "Fine-grained Financial Opinion Mining: A Survey and Research Agenda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining is a prevalent research issue in many domains. In the\nfinancial domain, however, it is still in the early stages. Most of the\nresearches on this topic only focus on the coarse-grained market sentiment\nanalysis, i.e., 2-way classification for bullish/bearish. Thanks to the recent\nfinancial technology (FinTech) development, some interdisciplinary researchers\nstart to involve in the in-depth analysis of investors' opinions. In this\nposition paper, we first define the financial opinions from both coarse-grained\nand fine-grained points of views, and then provide an overview on the issues\nalready tackled. In addition to listing research issues of the existing topics,\nwe further propose a road map of fine-grained financial opinion mining for\nfuture researches, and point out several challenges yet to explore. Moreover,\nwe provide possible directions to deal with the proposed research issues.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:07:07 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 00:43:39 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 11:30:48 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Chen", "Chung-Chi", ""], ["Huang", "Hen-Hsen", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "2005.01898", "submitter": "Hao Cheng", "authors": "Hao Cheng, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "title": "Probabilistic Assumptions Matter: Improved Models for\n  Distantly-Supervised Document-Level Question Answering", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of extractive question answering using document-level\ndistant super-vision, pairing questions and relevant documents with answer\nstrings. We compare previously used probability space and distant super-vision\nassumptions (assumptions on the correspondence between the weak answer string\nlabels and possible answer mention spans). We show that these assumptions\ninteract, and that different configurations provide complementary benefits. We\ndemonstrate that a multi-objective model can efficiently combine the advantages\nof multiple assumptions and out-perform the best individual formulation. Our\napproach outperforms previous state-of-the-art models by 4.3 points in F1 on\nTriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:08:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Cheng", "Hao", ""], ["Chang", "Ming-Wei", ""], ["Lee", "Kenton", ""], ["Toutanova", "Kristina", ""]]}, {"id": "2005.01901", "submitter": "Yoshihiko Suhara", "authors": "Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis, Wang-Chiew Tan", "title": "OpinionDigest: A Simple Framework for Opinion Summarization", "comments": "ACL 2020 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OpinionDigest, an abstractive opinion summarization framework,\nwhich does not rely on gold-standard summaries for training. The framework uses\nan Aspect-based Sentiment Analysis model to extract opinion phrases from\nreviews, and trains a Transformer model to reconstruct the original reviews\nfrom these extractions. At summarization time, we merge extractions from\nmultiple reviews and select the most popular ones. The selected opinions are\nused as input to the trained Transformer model, which verbalizes them into an\nopinion summary. OpinionDigest can also generate customized summaries, tailored\nto specific user needs, by filtering the selected opinions according to their\naspect and/or sentiment. Automatic evaluation on Yelp data shows that our\nframework outperforms competitive baselines. Human studies on two corpora\nverify that OpinionDigest produces informative summaries and shows promising\ncustomization capabilities.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:22:29 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Suhara", "Yoshihiko", ""], ["Wang", "Xiaolan", ""], ["Angelidis", "Stefanos", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2005.01932", "submitter": "Pang Wei Koh", "authors": "Shikhar Murty, Pang Wei Koh, and Percy Liang", "title": "ExpBERT: Representation Engineering with Natural Language Explanations", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we want to specify the inductive bias that married couples typically\ngo on honeymoons for the task of extracting pairs of spouses from text. In this\npaper, we allow model developers to specify these types of inductive biases as\nnatural language explanations. We use BERT fine-tuned on MultiNLI to\n``interpret'' these explanations with respect to the input sentence, producing\nexplanation-guided representations of the input. Across three relation\nextraction tasks, our method, ExpBERT, matches a BERT baseline but with 3--20x\nless labeled data and improves on the baseline by 3--10 F1 points with the same\namount of labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 03:40:23 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Murty", "Shikhar", ""], ["Koh", "Pang Wei", ""], ["Liang", "Percy", ""]]}, {"id": "2005.01972", "submitter": "Heng-Jui Chang", "authors": "Heng-Jui Chang, Alexander H. Liu, Hung-yi Lee, Lin-shan Lee", "title": "End-to-end Whispered Speech Recognition with Frequency-weighted\n  Approaches and Pseudo Whisper Pre-training", "comments": "Accepted to IEEE SLT 2021", "journal-ref": "2021 IEEE Spoken Language Technology Workshop (SLT)", "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whispering is an important mode of human speech, but no end-to-end\nrecognition results for it were reported yet, probably due to the scarcity of\navailable whispered speech data. In this paper, we present several approaches\nfor end-to-end (E2E) recognition of whispered speech considering the special\ncharacteristics of whispered speech and the scarcity of data. This includes a\nfrequency-weighted SpecAugment policy and a frequency-divided CNN feature\nextractor for better capturing the high-frequency structures of whispered\nspeech, and a layer-wise transfer learning approach to pre-train a model with\nnormal or normal-to-whispered converted speech then fine-tune it with whispered\nspeech to bridge the gap between whispered and normal speech. We achieve an\noverall relative reduction of 19.8% in PER and 44.4% in CER on a relatively\nsmall whispered TIMIT corpus. The results indicate as long as we have a good\nE2E model pre-trained on normal or pseudo-whispered speech, a relatively small\nset of whispered speech may suffice to obtain a reasonably good E2E whispered\nspeech recognizer.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 07:08:53 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 06:22:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chang", "Heng-Jui", ""], ["Liu", "Alexander H.", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "2005.02008", "submitter": "Hongfei Xu", "authors": "Hongfei Xu and Josef van Genabith and Deyi Xiong and Qiuhui Liu", "title": "Dynamically Adjusting Transformer Batch Size by Monitoring Gradient\n  Direction Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of hyper-parameters affects the performance of neural models.\nWhile much previous research (Sutskever et al., 2013; Duchi et al., 2011;\nKingma and Ba, 2015) focuses on accelerating convergence and reducing the\neffects of the learning rate, comparatively few papers concentrate on the\neffect of batch size. In this paper, we analyze how increasing batch size\naffects gradient direction, and propose to evaluate the stability of gradients\nwith their angle change. Based on our observations, the angle change of\ngradient direction first tends to stabilize (i.e. gradually decrease) while\naccumulating mini-batches, and then starts to fluctuate. We propose to\nautomatically and dynamically determine batch sizes by accumulating gradients\nof mini-batches and performing an optimization step at just the time when the\ndirection of gradients starts to fluctuate. To improve the efficiency of our\napproach for large models, we propose a sampling approach to select gradients\nof parameters sensitive to the batch size. Our approach dynamically determines\nproper and efficient batch sizes during training. In our experiments on the WMT\n14 English to German and English to French tasks, our approach improves the\nTransformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:47:34 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Xu", "Hongfei", ""], ["van Genabith", "Josef", ""], ["Xiong", "Deyi", ""], ["Liu", "Qiuhui", ""]]}, {"id": "2005.02013", "submitter": "Tanya Goyal", "authors": "Tanya Goyal and Greg Durrett", "title": "Neural Syntactic Preordering for Controlled Paraphrase Generation", "comments": "ACL 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paraphrasing natural language sentences is a multifaceted process: it might\ninvolve replacing individual words or short phrases, local rearrangement of\ncontent, or high-level restructuring like topicalization or passivization. Past\napproaches struggle to cover this space of paraphrase possibilities in an\ninterpretable manner. Our work, inspired by pre-ordering literature in machine\ntranslation, uses syntactic transformations to softly \"reorder'' the source\nsentence and guide our neural paraphrasing model. First, given an input\nsentence, we derive a set of feasible syntactic rearrangements using an\nencoder-decoder model. This model operates over a partially lexical, partially\nsyntactic view of the sentence and can reorder big chunks. Next, we use each\nproposed rearrangement to produce a sequence of position embeddings, which\nencourages our final encoder-decoder paraphrase model to attend to the source\nwords in a particular order. Our evaluation, both automatic and human, shows\nthat the proposed system retains the quality of the baseline approaches while\ngiving a substantial increase in the diversity of the generated paraphrases\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 09:02:25 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Goyal", "Tanya", ""], ["Durrett", "Greg", ""]]}, {"id": "2005.02049", "submitter": "Chulun Zhou", "authors": "Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng\n  Guo, Hua Wu", "title": "Exploring Contextual Word-level Style Relevance for Unsupervised Style\n  Transfer", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised style transfer aims to change the style of an input sentence\nwhile preserving its original content without using parallel training data. In\ncurrent dominant approaches, owing to the lack of fine-grained control on the\ninfluence from the target style,they are unable to yield desirable output\nsentences. In this paper, we propose a novel attentional sequence-to-sequence\n(Seq2seq) model that dynamically exploits the relevance of each output word to\nthe target style for unsupervised style transfer. Specifically, we first\npretrain a style classifier, where the relevance of each input word to the\noriginal style can be quantified via layer-wise relevance propagation. In a\ndenoising auto-encoding manner, we train an attentional Seq2seq model to\nreconstruct input sentences and repredict word-level previously-quantified\nstyle relevance simultaneously. In this way, this model is endowed with the\nability to automatically predict the style relevance of each output word. Then,\nwe equip the decoder of this model with a neural style component to exploit the\npredicted wordlevel style relevance for better style transfer. Particularly, we\nfine-tune this model using a carefully-designed objective function involving\nstyle transfer, style relevance consistency, content preservation and fluency\nmodeling loss terms. Experimental results show that our proposed model achieves\nstate-of-the-art performance in terms of both transfer accuracy and content\npreservation.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 10:24:28 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhou", "Chulun", ""], ["Chen", "Liangyu", ""], ["Liu", "Jiachen", ""], ["Xiao", "Xinyan", ""], ["Su", "Jinsong", ""], ["Guo", "Sheng", ""], ["Wu", "Hua", ""]]}, {"id": "2005.02068", "submitter": "Jan Christian Blaise Cruz", "authors": "Jan Christian Blaise Cruz and Charibeth Cheng", "title": "Establishing Baselines for Text Classification in Low-Resource Languages", "comments": "We release all our models, finetuning code, and data at\n  https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While transformer-based finetuning techniques have proven effective in tasks\nthat involve low-resource, low-data environments, a lack of properly\nestablished baselines and benchmark datasets make it hard to compare different\napproaches that are aimed at tackling the low-resource setting. In this work,\nwe provide three contributions. First, we introduce two previously unreleased\ndatasets as benchmark datasets for text classification and low-resource\nmultilabel text classification for the low-resource language Filipino. Second,\nwe pretrain better BERT and DistilBERT models for use within the Filipino\nsetting. Third, we introduce a simple degradation test that benchmarks a\nmodel's resistance to performance degradation as the number of training samples\nare reduced. We analyze our pretrained model's degradation speeds and look\ntowards the use of this method for comparing models aimed at operating within\nthe low-resource setting. We release all our models and datasets for the\nresearch community to use.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 11:17:07 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Cruz", "Jan Christian Blaise", ""], ["Cheng", "Charibeth", ""]]}, {"id": "2005.02087", "submitter": "Li-Min Wang", "authors": "Li-Min Wang, Sun-Ting Tsai, Shan-Jyun Wu, Meng-Xue Tsai, Daw-Wei Wang,\n  Yi-Ching Su, and Tzay-Ming Hong", "title": "Self-organizing Pattern in Multilayer Network for Words and Syllables", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ultimate goals for linguists is to find universal properties in\nhuman languages. Although words are generally considered as representing\narbitrary mapping between linguistic forms and meanings, we propose a new\nuniversal law that highlights the equally important role of syllables, which is\ncomplementary to Zipf's. By plotting rank-rank frequency distribution of word\nand syllable for English and Chinese corpora, visible lines appear and can be\nfit to a master curve. We discover the multi-layer network for words and\nsyllables based on this analysis exhibits the feature of self-organization\nwhich relies heavily on the inclusion of syllables and their connections.\nAnalytic form for the scaling structure is derived and used to quantify how\nInternet slang becomes fashionable, which demonstrates its usefulness as a new\ntool to evolutionary linguistics.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 12:01:47 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Wang", "Li-Min", ""], ["Tsai", "Sun-Ting", ""], ["Wu", "Shan-Jyun", ""], ["Tsai", "Meng-Xue", ""], ["Wang", "Daw-Wei", ""], ["Su", "Yi-Ching", ""], ["Hong", "Tzay-Ming", ""]]}, {"id": "2005.02146", "submitter": "Rahul Jha", "authors": "Rahul Jha, Keping Bi, Yang Li, Mahdi Pakdaman, Asli Celikyilmaz, Ivan\n  Zhiboedov, Kieran McDonald", "title": "Artemis: A Novel Annotation Methodology for Indicative Single Document\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Artemis (Annotation methodology for Rich, Tractable, Extractive,\nMulti-domain, Indicative Summarization), a novel hierarchical annotation\nprocess that produces indicative summaries for documents from multiple domains.\nCurrent summarization evaluation datasets are single-domain and focused on a\nfew domains for which naturally occurring summaries can be easily found, such\nas news and scientific articles. These are not sufficient for training and\nevaluation of summarization models for use in document management and\ninformation retrieval systems, which need to deal with documents from multiple\ndomains. Compared to other annotation methods such as Relative Utility and\nPyramid, Artemis is more tractable because judges don't need to look at all the\nsentences in a document when making an importance judgment for one of the\nsentences, while providing similarly rich sentence importance annotations. We\ndescribe the annotation process in detail and compare it with other similar\nevaluation systems. We also present analysis and experimental results over a\nsample set of 532 annotated documents.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:38:02 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 02:43:56 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Jha", "Rahul", ""], ["Bi", "Keping", ""], ["Li", "Yang", ""], ["Pakdaman", "Mahdi", ""], ["Celikyilmaz", "Asli", ""], ["Zhiboedov", "Ivan", ""], ["McDonald", "Kieran", ""]]}, {"id": "2005.02158", "submitter": "Jie Wang", "authors": "Hao Zhang, Jie Wang", "title": "An Unsupervised Semantic Sentence Ranking Scheme for Text Documents", "comments": "To appear in Integrated Computer-Aided Engineering (ICAE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Semantic SentenceRank (SSR), an unsupervised scheme for\nautomatically ranking sentences in a single document according to their\nrelative importance. In particular, SSR extracts essential words and phrases\nfrom a text document, and uses semantic measures to construct, respectively, a\nsemantic phrase graph over phrases and words, and a semantic sentence graph\nover sentences. It applies two variants of article-structure-biased PageRank to\nscore phrases and words on the first graph and sentences on the second graph.\nIt then combines these scores to generate the final score for each sentence.\nFinally, SSR solves a multi-objective optimization problem for ranking\nsentences based on their final scores and topic diversity through semantic\nsubtopic clustering. An implementation of SSR that runs in quadratic time is\npresented, and it outperforms, on the SummBank benchmarks, each individual\njudge's ranking and compares favorably with the combined ranking of all judges.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 20:17:51 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhang", "Hao", ""], ["Wang", "Jie", ""]]}, {"id": "2005.02171", "submitter": "Amjad Rehman Dr", "authors": "Amjad Rehman (PSU and UTM)", "title": "Neural Computing for Online Arabic Handwriting Character Recognition\n  using Hard Stroke Features Mining", "comments": "16 pages", "journal-ref": "IJICIC 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Arabic cursive character recognition is still a big challenge due to\nthe existing complexities including Arabic cursive script styles, writing\nspeed, writer mood and so forth. Due to these unavoidable constraints, the\naccuracy of online Arabic character's recognition is still low and retain space\nfor improvement. In this research, an enhanced method of detecting the desired\ncritical points from vertical and horizontal direction-length of handwriting\nstroke features of online Arabic script recognition is proposed. Each extracted\nstroke feature divides every isolated character into some meaningful pattern\nknown as tokens. A minimum feature set is extracted from these tokens for\nclassification of characters using a multilayer perceptron with a\nback-propagation learning algorithm and modified sigmoid function-based\nactivation function. In this work, two milestones are achieved; firstly, attain\na fixed number of tokens, secondly, minimize the number of the most repetitive\ntokens. For experiments, handwritten Arabic characters are selected from the\nOHASD benchmark dataset to test and evaluate the proposed method. The proposed\nmethod achieves an average accuracy of 98.6% comparable in state of art\ncharacter recognition techniques.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:17:08 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:40:50 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 10:58:36 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Rehman", "Amjad", "", "PSU and UTM"]]}, {"id": "2005.02178", "submitter": "Bill Yuchen Lin", "authors": "Wenxuan Zhou, Bill Yuchen Lin, Xiang Ren", "title": "IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-tuning pre-trained language models (PTLMs), such as BERT and its better\nvariant RoBERTa, has been a common practice for advancing performance in\nnatural language understanding (NLU) tasks. Recent advance in representation\nlearning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings\ncan significantly improve performance on downstream tasks with faster\nconvergence and better generalization. The isotropy of the pre-trained\nembeddings in PTLMs, however, is relatively under-explored. In this paper, we\nanalyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with\nstraightforward visualization, and point out two major issues: high variance in\ntheir standard deviation, and high correlation between different dimensions. We\nalso propose a new network regularization method, isotropic batch normalization\n(IsoBN) to address the issues, towards learning more isotropic representations\nin fine-tuning by dynamically penalizing dominating principal components. This\nsimple yet effective fine-tuning method yields about 1.0 absolute increment on\nthe average of seven NLU tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:49:09 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 01:40:26 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhou", "Wenxuan", ""], ["Lin", "Bill Yuchen", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.02230", "submitter": "Jheng-Hong Yang", "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai,\n  Chuan-Ju Wang and Jimmy Lin", "title": "Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term\n  Importance Estimation and Neural Query Rewriting", "comments": "28 pages. Accepted to ACM Transactions on Information Systems,\n  Special Issue on Conversational Search and Recommendation. The first two\n  authors contributed equally. Code: https://github.com/castorini/chatty-goose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search plays a vital role in conversational information\nseeking. As queries in information seeking dialogues are ambiguous for\ntraditional ad-hoc information retrieval (IR) systems due to the coreference\nand omission resolution problems inherent in natural language dialogue,\nresolving these ambiguities is crucial. In this paper, we tackle conversational\npassage retrieval (ConvPR), an important component of conversational search, by\naddressing query ambiguities with query reformulation integrated into a\nmulti-stage ad-hoc IR system. Specifically, we propose two conversational query\nreformulation (CQR) methods: (1) term importance estimation and (2) neural\nquery rewriting. For the former, we expand conversational queries using\nimportant terms extracted from the conversational context with frequency-based\nsignals. For the latter, we reformulate conversational queries into natural,\nstandalone, human-understandable queries with a pretrained sequence-tosequence\nmodel. Detailed analyses of the two CQR methods are provided quantitatively and\nqualitatively, explaining their advantages, disadvantages, and distinct\nbehaviors. Moreover, to leverage the strengths of both CQR methods, we propose\ncombining their output with reciprocal rank fusion, yielding state-of-the-art\nretrieval effectiveness, 30% improvement in terms of NDCG@3 compared to the\nbest submission of TREC CAsT 2019.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:30:20 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 14:33:53 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Nogueira", "Rodrigo", ""], ["Tsai", "Ming-Feng", ""], ["Wang", "Chuan-Ju", ""], ["Lin", "Jimmy", ""]]}, {"id": "2005.02233", "submitter": "Yinpei Dai", "authors": "Yinpei Dai, Huihua Yu, Yixuan Jiang, Chengguang Tang, Yongbin Li, Jian\n  Sun", "title": "A Survey on Dialog Management: Recent Advances and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog management (DM) is a crucial component in a task-oriented dialog\nsystem. Given the dialog history, DM predicts the dialog state and decides the\nnext action that the dialog agent should take. Recently, dialog policy learning\nhas been widely formulated as a Reinforcement Learning (RL) problem, and more\nworks focus on the applicability of DM. In this paper, we survey recent\nadvances and challenges within three critical topics for DM: (1) improving\nmodel scalability to facilitate dialog system modeling in new scenarios, (2)\ndealing with the data scarcity problem for dialog policy learning, and (3)\nenhancing the training efficiency to achieve better task-completion performance\n. We believe that this survey can shed a light on future research in dialog\nmanagement.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:31:24 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 06:21:49 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Dai", "Yinpei", ""], ["Yu", "Huihua", ""], ["Jiang", "Yixuan", ""], ["Tang", "Chengguang", ""], ["Li", "Yongbin", ""], ["Sun", "Jian", ""]]}, {"id": "2005.02235", "submitter": "Alessio Palmero Aprosio", "authors": "Alessio Palmero Aprosio, Stefano Menini, Sara Tonelli", "title": "Creating a Multimodal Dataset of Images and Text to Study Abusive\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to study online hate speech, the availability of datasets containing\nthe linguistic phenomena of interest are of crucial importance. However, when\nit comes to specific target groups, for example teenagers, collecting such data\nmay be problematic due to issues with consent and privacy restrictions.\nFurthermore, while text-only datasets of this kind have been widely used,\nlimitations set by image-based social media platforms like Instagram make it\ndifficult for researchers to experiment with multimodal hate speech data. We\ntherefore developed CREENDER, an annotation tool that has been used in school\nclasses to create a multimodal dataset of images and abusive comments, which we\nmake freely available under Apache 2.0 license. The corpus, with Italian\ncomments, has been analysed from different perspectives, to investigate whether\nthe subject of the images plays a role in triggering a comment. We find that\nusers judge the same images in different ways, although the presence of a\nperson in the picture increases the probability to get an offensive comment.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:31:47 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Aprosio", "Alessio Palmero", ""], ["Menini", "Stefano", ""], ["Tonelli", "Sara", ""]]}, {"id": "2005.02295", "submitter": "Vishal Garimella", "authors": "Srijan Bansal, Vishal Garimella, Ayush Suhane, Jasabanta Patro,\n  Animesh Mukherjee", "title": "Code-switching patterns can be an effective route to improve performance\n  of downstream NLP applications: A case study of humour, sarcasm and hate\n  speech detection", "comments": "This work is accepted as a short paper in the proceedings of ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate how code-switching patterns can be utilised to\nimprove various downstream NLP applications. In particular, we encode different\nswitching features to improve humour, sarcasm and hate speech detection tasks.\nWe believe that this simple linguistic observation can also be potentially\nhelpful in improving other similar NLP applications.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:48:34 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bansal", "Srijan", ""], ["Garimella", "Vishal", ""], ["Suhane", "Ayush", ""], ["Patro", "Jasabanta", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2005.02324", "submitter": "Chao Jiang", "authors": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu", "title": "Neural CRF Model for Sentence Alignment in Text Simplification", "comments": "The paper has been accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:47:51 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 12:25:06 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 21:05:14 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jiang", "Chao", ""], ["Maddela", "Mounica", ""], ["Lan", "Wuwei", ""], ["Zhong", "Yang", ""], ["Xu", "Wei", ""]]}, {"id": "2005.02325", "submitter": "Elhadji Mamadou Nguer Emnguer", "authors": "Elhadji Mamadou Nguer, Diop Sokhna Bao, Yacoub Ahmed Fall, Mouhamadou\n  Khoule", "title": "Digraph of Senegal s local languages: issues, challenges and prospects\n  of their transliteration", "comments": null, "journal-ref": "LTC 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local languages in Senegal, like those of West African countries in\ngeneral, are written based on two alphabets: supplemented Arabic alphabet\n(called Ajami) and Latin alphabet. Each writing has its own applications. Ajami\nwriting is generally used by people educated in Koranic schools for\ncommunication, business, literature (religious texts, poetry, etc.),\ntraditional religious medicine, etc. Writing with Latin characters is used for\nlocalization of ICT (Web, dictionaries, Windows and Google tools translated in\nWolof, etc.), the translation of legal texts (commercial code and constitution\ntranslated in Wolof) and religious ones (Quran and Bible in Wolof), book\nedition, etc. To facilitate both populations general access to knowledge, it is\nuseful to set up transliteration tools between these two scriptures. This work\nfalls within the framework of the implementation of project for a collaborative\nonline dictionary Wolof (Nguer E. M., Khoule M, Thiam M. N., Mbaye B. T.,\nThiare O., Cisse M. T., Mangeot M. 2014), which will involve people using Ajami\nwriting. Our goal will consist, on the one hand in raising the issues related\nto the transliteration and the challenges that this will raise, and on the\nother one, presenting the perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:48:37 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Nguer", "Elhadji Mamadou", ""], ["Bao", "Diop Sokhna", ""], ["Fall", "Yacoub Ahmed", ""], ["Khoule", "Mouhamadou", ""]]}, {"id": "2005.02354", "submitter": "Emanuele Bugliarello", "authors": "Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan\n  Cotterell, Naoaki Okazaki", "title": "It's Easier to Translate out of English than into it: Measuring Neural\n  Translation Difficulty by Cross-Mutual Information", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of neural machine translation systems is commonly evaluated\nin terms of BLEU. However, due to its reliance on target language properties\nand generation, the BLEU metric does not allow an assessment of which\ntranslation directions are more difficult to model. In this paper, we propose\ncross-mutual information (XMI): an asymmetric information-theoretic metric of\nmachine translation difficulty that exploits the probabilistic nature of most\nneural machine translation models. XMI allows us to better evaluate the\ndifficulty of translating text into the target language while controlling for\nthe difficulty of the target-side generation component independent of the\ntranslation task. We then present the first systematic and controlled study of\ncross-lingual translation difficulties using modern neural translation systems.\nCode for replicating our experiments is available online at\nhttps://github.com/e-bug/nmt-difficulty.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:38:48 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 06:59:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bugliarello", "Emanuele", ""], ["Mielke", "Sabrina J.", ""], ["Anastasopoulos", "Antonios", ""], ["Cotterell", "Ryan", ""], ["Okazaki", "Naoaki", ""]]}, {"id": "2005.02365", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Arman Cohan, Nazli Goharian", "title": "SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific\n  Knowledge Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With worldwide concerns surrounding the Severe Acute Respiratory Syndrome\nCoronavirus 2 (SARS-CoV-2), there is a rapidly growing body of literature on\nthe virus. Clinicians, researchers, and policy-makers need a way to effectively\nsearch these articles. In this work, we present a search system called SLEDGE,\nwhich utilizes SciBERT to effectively re-rank articles. We train the model on a\ngeneral-domain answer ranking dataset, and transfer the relevance signals to\nSARS-CoV-2 for evaluation. We observe SLEDGE's effectiveness as a strong\nbaseline on the TREC-COVID challenge (topping the learderboard with an nDCG@10\nof 0.6844). Insights provided by a detailed analysis provide some potential\nfuture directions to explore, including the importance of filtering by date and\nthe potential of neural methods that rely more heavily on count signals. We\nrelease the code to facilitate future work on this critical task at\nhttps://github.com/Georgetown-IR-Lab/covid-neural-ir\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:51:27 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 16:06:33 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 17:24:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["MacAvaney", "Sean", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "2005.02367", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang, Chieh-Yang Huang, Chien-Kuang Cornelia Ding,\n  Yen-Chia Hsu, and C. Lee Giles", "title": "CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on\n  10,000+ Abstracts in the COVID-19 Open Research Dataset", "comments": "Accepted by the NLP COVID-19 Workshop at ACL 2020. (The data, code,\n  and model are available at: https://github.com/windx0303/CODA-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces CODA-19, a human-annotated dataset that codes the\nBackground, Purpose, Method, Finding/Contribution, and Other sections of 10,966\nEnglish abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by\n248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved\nlabeling quality comparable to that of experts. Each abstract was annotated by\nnine different workers, and the final labels were acquired by majority vote.\nThe inter-annotator agreement (Cohen's kappa) between the crowd and the\nbiomedical expert (0.741) is comparable to inter-expert agreement (0.788).\nCODA-19's labels have an accuracy of 82.2% when compared to the biomedical\nexpert's labels, while the accuracy between experts was 85.0%. Reliable human\nannotations help scientists access and integrate the rapidly accelerating\ncoronavirus literature, and also serve as the battery of AI/NLP research, but\nobtaining expert annotations can be slow. We demonstrated that a non-expert\ncrowd can be rapidly employed at scale to join the fight against COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:51:42 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:28:51 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 01:53:51 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 06:22:02 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 22:00:29 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""], ["Huang", "Chieh-Yang", ""], ["Ding", "Chien-Kuang Cornelia", ""], ["Hsu", "Yen-Chia", ""], ["Giles", "C. Lee", ""]]}, {"id": "2005.02431", "submitter": "Ekaterina Kochmar", "authors": "Ekaterina Kochmar, Dung Do Vu, Robert Belfer, Varun Gupta, Iulian Vlad\n  Serban, and Joelle Pineau", "title": "Automated Personalized Feedback Improves Learning Gains in an\n  Intelligent Tutoring System", "comments": "To be published in Proceedings of the the 21st International\n  Conference on Artificial Intelligence in Education (AIED 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how automated, data-driven, personalized feedback in a\nlarge-scale intelligent tutoring system (ITS) improves student learning\noutcomes. We propose a machine learning approach to generate personalized\nfeedback, which takes individual needs of students into account. We utilize\nstate-of-the-art machine learning and natural language processing techniques to\nprovide the students with personalized hints, Wikipedia-based explanations, and\nmathematical hints. Our model is used in Korbit, a large-scale dialogue-based\nITS with thousands of students launched in 2019, and we demonstrate that the\npersonalized feedback leads to considerable improvement in student learning\noutcomes and in the subjective evaluation of the feedback.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:30:08 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 18:18:54 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kochmar", "Ekaterina", ""], ["Vu", "Dung Do", ""], ["Belfer", "Robert", ""], ["Gupta", "Varun", ""], ["Serban", "Iulian Vlad", ""], ["Pineau", "Joelle", ""]]}, {"id": "2005.02439", "submitter": "Xisen Jin", "authors": "Brendan Kennedy and Xisen Jin and Aida Mostafazadeh Davani and Morteza\n  Dehghani and Xiang Ren", "title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation", "comments": "To appear in Proceedings of the 2020 Annual Conference of the\n  Association for Computational Linguistics; Updated references and discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech classifiers trained on imbalanced datasets struggle to determine\nif group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced\nways. Such biases manifest in false positives when these identifiers are\npresent, due to models' inability to learn the contexts which constitute a\nhateful usage of identifiers. We extract SOC post-hoc explanations from\nfine-tuned BERT classifiers to efficiently detect bias towards identity terms.\nThen, we propose a novel regularization technique based on these explanations\nthat encourages models to learn from the context of group identifiers in\naddition to the identifiers themselves. Our approach improved over baselines in\nlimiting false positives on out-of-domain data while maintaining or improving\nin-domain performance. Project page:\nhttps://inklab.usc.edu/contextualize-hate-speech/.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:56:40 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 20:19:29 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 18:54:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kennedy", "Brendan", ""], ["Jin", "Xisen", ""], ["Davani", "Aida Mostafazadeh", ""], ["Dehghani", "Morteza", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.02470", "submitter": "Zein Shaheen", "authors": "Zein Shaheen, Gerhard Wohlgenannt, Bassel Zaity, Dmitry Mouromtsev,\n  Vadim Pak", "title": "Russian Natural Language Generation: Creation of a Language Modelling\n  Dataset and Evaluation with Modern Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating coherent, grammatically correct, and meaningful text is very\nchallenging, however, it is crucial to many modern NLP systems. So far,\nresearch has mostly focused on English language, for other languages both\nstandardized datasets, as well as experiments with state-of-the-art models, are\nrare. In this work, we i) provide a novel reference dataset for Russian\nlanguage modeling, ii) experiment with popular modern methods for text\ngeneration, namely variational autoencoders, and generative adversarial\nnetworks, which we trained on the new dataset. We evaluate the generated text\nregarding metrics such as perplexity, grammatical correctness and lexical\ndiversity.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:20:25 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Shaheen", "Zein", ""], ["Wohlgenannt", "Gerhard", ""], ["Zaity", "Bassel", ""], ["Mouromtsev", "Dmitry", ""], ["Pak", "Vadim", ""]]}, {"id": "2005.02472", "submitter": "Alireza Zareian", "authors": "Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng\n  Ji, Shih-Fu Chang", "title": "Cross-media Structured Common Space for Multimedia Event Extraction", "comments": "Accepted as an oral paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to\nextract events and their arguments from multimedia documents. We develop the\nfirst benchmark and collect a dataset of 245 multimedia news articles with\nextensively annotated events and arguments. We propose a novel method, Weakly\nAligned Structured Embedding (WASE), that encodes structured representations of\nsemantic information from textual and visual data into a common embedding\nspace. The structures are aligned across modalities by employing a weakly\nsupervised training strategy, which enables exploiting available resources\nwithout explicit cross-media annotation. Compared to uni-modal state-of-the-art\nmethods, our approach achieves 4.0% and 9.8% absolute F-score gains on text\nevent argument role labeling and visual event extraction. Compared to\nstate-of-the-art multimedia unstructured representations, we achieve 8.3% and\n5.0% absolute F-score gains on multimedia event extraction and argument role\nlabeling, respectively. By utilizing images, we extract 21.4% more event\nmentions than traditional text-only methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:21:53 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Manling", ""], ["Zareian", "Alireza", ""], ["Zeng", "Qi", ""], ["Whitehead", "Spencer", ""], ["Lu", "Di", ""], ["Ji", "Heng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2005.02473", "submitter": "Marco Antonio Sobrevilla Cabezudo", "authors": "Kervy Rivas Rojas, Gina Bustamante, Arturo Oncevay, Marco A.\n  Sobrevilla Cabezudo", "title": "Efficient strategies for hierarchical text classification: External\n  knowledge and auxiliary tasks", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hierarchical text classification, we perform a sequence of inference steps\nto predict the category of a document from top to bottom of a given class\ntaxonomy. Most of the studies have focused on developing novels neural network\narchitectures to deal with the hierarchical structure, but we prefer to look\nfor efficient ways to strengthen a baseline model. We first define the task as\na sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic\ntask of bottom-up-classification. Then, from external dictionaries, we retrieve\ntextual definitions for the classes of all the hierarchy's layers, and map them\ninto the word vector space. We use the class-definition embeddings as an\nadditional input to condition the prediction of the next layer and in an\nadapted beam search. Whereas the modified search did not provide large gains,\nthe combination of the auxiliary task and the additional input of\nclass-definitions significantly enhance the classification accuracy. With our\nefficient approaches, we outperform previous studies, using a drastically\nreduced number of parameters, in two well-known English datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:22:18 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 13:08:02 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Rojas", "Kervy Rivas", ""], ["Bustamante", "Gina", ""], ["Oncevay", "Arturo", ""], ["Cabezudo", "Marco A. Sobrevilla", ""]]}, {"id": "2005.02507", "submitter": "Mandy Guo", "authors": "Mandy Guo, Yinfei Yang, Daniel Cer, Qinlan Shen, Noah Constant", "title": "MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval question answering (ReQA) is the task of retrieving a\nsentence-level answer to a question from an open corpus (Ahmad et\nal.,2019).This paper presents MultiReQA, anew multi-domain ReQA evaluation\nsuite com-posed of eight retrieval QA tasks drawn from publicly available QA\ndatasets. We provide the first systematic retrieval based evaluation over these\ndatasets using two supervised neural models, based on fine-tuning BERT\nandUSE-QA models respectively, as well as a surprisingly strong information\nretrieval baseline,BM25. Five of these tasks contain both train-ing and test\ndata, while three contain test data only. Performance on the five tasks with\ntrain-ing data shows that while a general model covering all domains is\nachievable, the best performance is often obtained by training exclusively on\nin-domain data.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:30:16 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Guo", "Mandy", ""], ["Yang", "Yinfei", ""], ["Cer", "Daniel", ""], ["Shen", "Qinlan", ""], ["Constant", "Noah", ""]]}, {"id": "2005.02517", "submitter": "Maria Ryskina", "authors": "Maria Ryskina, Matthew R. Gormley, Taylor Berg-Kirkpatrick", "title": "Phonetic and Visual Priors for Decipherment of Informal Romanization", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informal romanization is an idiosyncratic process used by humans in informal\ndigital communication to encode non-Latin script languages into Latin character\nsets found on common keyboards. Character substitution choices differ between\nusers but have been shown to be governed by the same main principles observed\nacross a variety of languages---namely, character pairs are often associated\nthrough phonetic or visual similarity. We propose a noisy-channel WFST cascade\nmodel for deciphering the original non-Latin script from observed romanized\ntext in an unsupervised fashion. We train our model directly on romanized data\nfrom two languages: Egyptian Arabic and Russian. We demonstrate that adding\ninductive bias through phonetic and visual priors on character mappings\nsubstantially improves the model's performance on both languages, yielding\nresults much closer to the supervised skyline. Finally, we introduce a new\ndataset of romanized Russian, collected from a Russian social network website\nand partially annotated for our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:57:27 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ryskina", "Maria", ""], ["Gormley", "Matthew R.", ""], ["Berg-Kirkpatrick", "Taylor", ""]]}, {"id": "2005.02527", "submitter": "Tian Guo", "authors": "Tian Guo, Nicolas Jamet, Valentin Betrix, Louis-Alexandre Piquet,\n  Emmanuel Hauptmann", "title": "ESG2Risk: A Deep Learning Framework from ESG News to Stock Volatility\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating environmental, social, and governance (ESG) considerations into\nsystematic investments has drawn numerous attention recently. In this paper, we\nfocus on the ESG events in financial news flow and exploring the predictive\npower of ESG related financial news on stock volatility. In particular, we\ndevelop a pipeline of ESG news extraction, news representations, and Bayesian\ninference of deep learning models. Experimental evaluation on real data and\ndifferent markets demonstrates the superior predicting performance as well as\nthe relation of high volatility prediction to stocks with potential high risk\nand low return. It also shows the prospect of the proposed pipeline as a\nflexible predicting framework for various textual data and target variables.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:01:36 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Guo", "Tian", ""], ["Jamet", "Nicolas", ""], ["Betrix", "Valentin", ""], ["Piquet", "Louis-Alexandre", ""], ["Hauptmann", "Emmanuel", ""]]}, {"id": "2005.02534", "submitter": "Luca Soldaini", "authors": "Luca Soldaini and Alessandro Moschitti", "title": "The Cascade Transformer: an Application for Efficient Answer Sentence\n  Selection", "comments": "Accepted to ACL 2020 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large transformer-based language models have been shown to be very effective\nin many classification tasks. However, their computational complexity prevents\ntheir use in applications requiring the classification of a large set of\ncandidates. While previous works have investigated approaches to reduce model\nsize, relatively little attention has been paid to techniques to improve batch\nthroughput during inference. In this paper, we introduce the Cascade\nTransformer, a simple yet effective technique to adapt transformer-based models\ninto a cascade of rankers. Each ranker is used to prune a subset of candidates\nin a batch, thus dramatically increasing throughput at inference time. Partial\nencodings from the transformer model are shared among rerankers, providing\nfurther speed-up. When compared to a state-of-the-art transformer model, our\napproach reduces computation by 37% with almost no impact on accuracy, as\nmeasured on two English Question Answering datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:32:01 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 15:07:38 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Soldaini", "Luca", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2005.02539", "submitter": "Ahmed Elgohary", "authors": "Ahmed Elgohary, Saghar Hosseini, Ahmed Hassan Awadallah", "title": "Speak to your Parser: Interactive Text-to-SQL with Natural Language\n  Feedback", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of semantic parse correction with natural language\nfeedback. Given a natural language utterance, most semantic parsing systems\npose the problem as one-shot translation where the utterance is mapped to a\ncorresponding logical form. In this paper, we investigate a more interactive\nscenario where humans can further interact with the system by providing\nfree-form natural language feedback to correct the system when it generates an\ninaccurate interpretation of an initial utterance. We focus on natural language\nto SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL\ninterpretations and the corresponding natural language feedback. We compare\nvarious reference models for the correction task and show that incorporating\nsuch a rich form of feedback can significantly improve the overall semantic\nparsing accuracy while retaining the flexibility of natural language\ninteraction. While we estimated human correction accuracy is 81.5%, our best\nmodel achieves only 25.1%, which leaves a large gap for improvement in future\nresearch. SPLASH is publicly available at https://aka.ms/Splash_dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:58:09 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 22:01:15 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Elgohary", "Ahmed", ""], ["Hosseini", "Saghar", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2005.02557", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng, Meng Jiang", "title": "Crossing Variational Autoencoders for Answer Retrieval", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer retrieval is to find the most aligned answer from a large set of\ncandidates given a question. Learning vector representations of\nquestions/answers is the key factor. Question-answer alignment and\nquestion/answer semantics are two important signals for learning the\nrepresentations. Existing methods learned semantic representations with dual\nencoders or dual variational auto-encoders. The semantic information was\nlearned from language models or question-to-question (answer-to-answer)\ngenerative processes. However, the alignment and semantics were too separate to\ncapture the aligned semantics between question and answer. In this work, we\npropose to cross variational auto-encoders by generating questions with aligned\nanswers and generating answers with aligned questions. Experiments show that\nour method outperforms the state-of-the-art answer retrieval method on SQuAD.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:59:13 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 03:24:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yu", "Wenhao", ""], ["Wu", "Lingfei", ""], ["Zeng", "Qingkai", ""], ["Tao", "Shu", ""], ["Deng", "Yu", ""], ["Jiang", "Meng", ""]]}, {"id": "2005.02573", "submitter": "John Licato", "authors": "Zaid Marji, Animesh Nighojkar, John Licato", "title": "Probing the Natural Language Inference Task with Automated Reasoning\n  Tools", "comments": "Accepted to Proceedings of The 33rd International Florida Artificial\n  Intelligence Research Society Conference (FLAIRS-33, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Natural Language Inference (NLI) task is an important task in modern NLP,\nas it asks a broad question to which many other tasks may be reducible: Given a\npair of sentences, does the first entail the second? Although the\nstate-of-the-art on current benchmark datasets for NLI are deep learning-based,\nit is worthwhile to use other techniques to examine the logical structure of\nthe NLI task. We do so by testing how well a machine-oriented controlled\nnatural language (Attempto Controlled English) can be used to parse NLI\nsentences, and how well automated theorem provers can reason over the resulting\nformulae. To improve performance, we develop a set of syntactic and semantic\ntransformation rules. We report their performance, and discuss implications for\nNLI and logic-based NLP.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 03:18:11 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Marji", "Zaid", ""], ["Nighojkar", "Animesh", ""], ["Licato", "John", ""]]}, {"id": "2005.02590", "submitter": "Terra Blevins", "authors": "Terra Blevins and Luke Zettlemoyer", "title": "Moving Down the Long Tail of Word Sense Disambiguation with\n  Gloss-Informed Biencoders", "comments": "Accepted to ACL 2020; current version corrects typos and formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle in Word Sense Disambiguation (WSD) is that word senses are\nnot uniformly distributed, causing existing models to generally perform poorly\non senses that are either rare or unseen during training. We propose a\nbi-encoder model that independently embeds (1) the target word with its\nsurrounding context and (2) the dictionary definition, or gloss, of each sense.\nThe encoders are jointly optimized in the same representation space, so that\nsense disambiguation can be performed by finding the nearest sense embedding\nfor each target word embedding. Our system outperforms previous\nstate-of-the-art models on English all-words WSD; these gains predominantly\ncome from improved performance on rare senses, leading to a 31.1% error\nreduction on less frequent senses over prior work. This demonstrates that rare\nsenses can be more effectively disambiguated by modeling their definitions.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 04:21:45 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 04:01:26 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Blevins", "Terra", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "2005.02593", "submitter": "Yinqiao Li", "authors": "Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao,\n  Jingbo Zhu, Tongran Liu, Changliang Li", "title": "Learning Architectures from an Extended Search Space for Language\n  Modeling", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has advanced significantly in recent years\nbut most NAS systems restrict search to learning architectures of a recurrent\nor convolutional cell. In this paper, we extend the search space of NAS. In\nparticular, we present a general approach to learn both intra-cell and\ninter-cell architectures (call it ESS). For a better search result, we design a\njoint learning method to perform intra-cell and inter-cell NAS simultaneously.\nWe implement our model in a differentiable architecture search system. For\nrecurrent neural language modeling, it outperforms a strong baseline\nsignificantly on the PTB and WikiText data, with a new state-of-the-art on PTB.\nMoreover, the learned architectures show good transferability to other systems.\nE.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity\nrecognition (NER) tasks and CoNLL chunking task, indicating a promising line of\nresearch on large-scale pre-learned architectures.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 05:02:33 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 06:23:49 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Li", "Yinqiao", ""], ["Hu", "Chi", ""], ["Zhang", "Yuhao", ""], ["Xu", "Nuo", ""], ["Jiang", "Yufan", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""], ["Liu", "Tongran", ""], ["Li", "Changliang", ""]]}, {"id": "2005.02623", "submitter": "Hao Fang", "authors": "Hao Fang", "title": "Building A User-Centric and Content-Driven Socialbot", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build Sounding Board, we develop a system architecture that is capable of\naccommodating dialog strategies that we designed for socialbot conversations.\nThe architecture consists of a multi-dimensional language understanding module\nfor analyzing user utterances, a hierarchical dialog management framework for\ndialog context tracking and complex dialog control, and a language generation\nprocess that realizes the response plan and makes adjustments for speech\nsynthesis. Additionally, we construct a new knowledge base to power the\nsocialbot by collecting social chat content from a variety of sources. An\nimportant contribution of the system is the synergy between the knowledge base\nand the dialog management, i.e., the use of a graph structure to organize the\nknowledge base that makes dialog control very efficient in bringing related\ncontent to the discussion. Using the data collected from Sounding Board during\nthe competition, we carry out in-depth analyses of socialbot conversations and\nuser ratings which provide valuable insights in evaluation methods for\nsocialbots. We additionally investigate a new approach for system evaluation\nand diagnosis that allows scoring individual dialog segments in the\nconversation. Finally, observing that socialbots suffer from the issue of\nshallow conversations about topics associated with unstructured data, we study\nthe problem of enabling extended socialbot conversations grounded on a\ndocument. To bring together machine reading and dialog control techniques, a\ngraph-based document representation is proposed, together with methods for\nautomatically constructing the graph. Using the graph-based representation,\ndialog control can be carried out by retrieving nodes or moving along edges in\nthe graph. To illustrate the usage, a mixed-initiative dialog strategy is\ndesigned for socialbot conversations on news articles.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 07:11:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Fang", "Hao", ""]]}, {"id": "2005.02680", "submitter": "Longyin Zhang", "authors": "Longyin Zhang, Yuqing Xing, Fang Kong, Peifeng Li, Guodong Zhou", "title": "A Top-Down Neural Architecture towards Text-Level Parsing of Discourse\n  Rhetorical Structure", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its great importance in deep natural language understanding and\nvarious down-stream applications, text-level parsing of discourse rhetorical\nstructure (DRS) has been drawing more and more attention in recent years.\nHowever, all the previous studies on text-level discourse parsing adopt\nbottom-up approaches, which much limit the DRS determination on local\ninformation and fail to well benefit from global information of the overall\ndiscourse. In this paper, we justify from both computational and perceptive\npoints-of-view that the top-down architecture is more suitable for text-level\nDRS parsing. On the basis, we propose a top-down neural architecture toward\ntext-level DRS parsing. In particular, we cast discourse parsing as a recursive\nsplit point ranking task, where a split point is classified to different levels\naccording to its rank and the elementary discourse units (EDUs) associated with\nit are arranged accordingly. In this way, we can determine the complete DRS as\na hierarchical tree structure via an encoder-decoder with an internal stack.\nExperimentation on both the English RST-DT corpus and the Chinese CDTB corpus\nshows the great effectiveness of our proposed top-down approach towards\ntext-level DRS parsing.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:27:20 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 09:39:56 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 22:51:53 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 11:35:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhang", "Longyin", ""], ["Xing", "Yuqing", ""], ["Kong", "Fang", ""], ["Li", "Peifeng", ""], ["Zhou", "Guodong", ""]]}, {"id": "2005.02693", "submitter": "Henry Elder", "authors": "Henry Elder and Robert Burke and Alexander O'Connor and Jennifer\n  Foster", "title": "Shape of synth to come: Why we should use synthetic data for English\n  surface realization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language\nGeneration shared tasks with the goal of exploring approaches to surface\nrealization from Universal-Dependency-like trees to surface strings for several\nlanguages. In the 2018 shared task there was very little difference in the\nabsolute performance of systems trained with and without additional,\nsynthetically created data, and a new rule prohibiting the use of synthetic\ndata was introduced for the 2019 shared task. Contrary to the findings of the\n2018 shared task, we show, in experiments on the English 2018 dataset, that the\nuse of synthetic data can have a substantial positive effect - an improvement\nof almost 8 BLEU points for a previously state-of-the-art system. We analyse\nthe effects of synthetic data, and we argue that its use should be encouraged\nrather than prohibited so that future research efforts continue to explore\nsystems that can take advantage of such data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:00:55 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Elder", "Henry", ""], ["Burke", "Robert", ""], ["O'Connor", "Alexander", ""], ["Foster", "Jennifer", ""]]}, {"id": "2005.02721", "submitter": "Lieke Gelderloos", "authors": "Lieke Gelderloos, Grzegorz Chrupa{\\l}a, Afra Alishahi", "title": "Learning to Understand Child-directed and Adult-directed Speech", "comments": "Authors found an error in preprocessing of transcriptions before they\n  were fed to SBERT. After correction, the experiments were rerun. The updated\n  results can be found in this version. Importantly, - Most scores were\n  affected to a small degree (performance was slightly worse). - The effect was\n  consistent across conditions. Therefore, the general patterns remain the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech directed to children differs from adult-directed speech in linguistic\naspects such as repetition, word choice, and sentence length, as well as in\naspects of the speech signal itself, such as prosodic and phonemic variation.\nHuman language acquisition research indicates that child-directed speech helps\nlanguage learners. This study explores the effect of child-directed speech when\nlearning to extract semantic information from speech directly. We compare the\ntask performance of models trained on adult-directed speech (ADS) and\nchild-directed speech (CDS). We find indications that CDS helps in the initial\nstages of learning, but eventually, models trained on ADS reach comparable task\nperformance, and generalize better. The results suggest that this is at least\npartially due to linguistic rather than acoustic properties of the two\nregisters, as we see the same pattern when looking at models trained on\nacoustically comparable synthetic speech.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:47:02 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 15:52:11 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 15:40:18 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 15:25:26 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Gelderloos", "Lieke", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Alishahi", "Afra", ""]]}, {"id": "2005.02771", "submitter": "Timur Sokhin", "authors": "Timur Sokhin, Maria Khodorchenko, and Nikolay Butakov", "title": "Unsupervised Neural Aspect Search with Related Terms Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tasks of aspect identification and term extraction remain challenging in\nnatural language processing. While supervised methods achieve high scores, it\nis hard to use them in real-world applications due to the lack of labelled\ndatasets. Unsupervised approaches outperform these methods on several tasks,\nbut it is still a challenge to extract both an aspect and a corresponding term,\nparticularly in the multi-aspect setting. In this work, we present a novel\nunsupervised neural network with convolutional multi-attention mechanism, that\nallows extracting pairs (aspect, term) simultaneously, and demonstrate the\neffectiveness on the real-world dataset. We apply a special loss aimed to\nimprove the quality of multi-aspect extraction. The experimental study\ndemonstrates, what with this loss we increase the precision not only on this\njoint setting but also on aspect prediction only.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 12:39:45 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Sokhin", "Timur", ""], ["Khodorchenko", "Maria", ""], ["Butakov", "Nikolay", ""]]}, {"id": "2005.02780", "submitter": "Junhua Liu", "authors": "Junhua Liu, Yung Chuen Ng and Kwan Hui Lim", "title": "A Large-scale Industrial and Professional Occupation Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in utilizing occupational data mining and\nanalysis. In today's job market, occupational data mining and analysis is\ngrowing in importance as it enables companies to predict employee turnover,\nmodel career trajectories, screen through resumes and perform other human\nresource tasks. A key requirement to facilitate these tasks is the need for an\noccupation-related dataset. However, most research use proprietary datasets or\ndo not make their dataset publicly available, thus impeding development in this\narea. To solve this issue, we present the Industrial and Professional\nOccupation Dataset (IPOD), which comprises 192k job titles belonging to 56k\nLinkedIn users. In addition to making IPOD publicly available, we also: (i)\nmanually annotate each job title with its associated level of seniority, domain\nof work and location; and (ii) provide embedding for job titles and discuss\nvarious use cases. This dataset is publicly available at\nhttps://github.com/junhua/ipod.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 10:45:48 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Liu", "Junhua", ""], ["Ng", "Yung Chuen", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "2005.02799", "submitter": "Yifan Peng", "authors": "Yifan Peng, Qingyu Chen, Zhiyong Lu", "title": "An Empirical Study of Multi-Task Learning on BERT for Biomedical Text\n  Mining", "comments": "Accepted by BioNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has achieved remarkable success in natural language\nprocessing applications. In this work, we study a multi-task learning model\nwith multiple decoders on varieties of biomedical and clinical natural language\nprocessing tasks such as text similarity, relation extraction, named entity\nrecognition, and text inference. Our empirical results demonstrate that the MTL\nfine-tuned models outperform state-of-the-art transformer models (e.g., BERT\nand its variants) by 2.0% and 1.3% in biomedical and clinical domains,\nrespectively. Pairwise MTL further demonstrates more details about which tasks\ncan improve or decrease others. This is particularly helpful in the context\nthat researchers are in the hassle of choosing a suitable model for new\nproblems. The code and models are publicly available at\nhttps://github.com/ncbi-nlp/bluebert\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:25:21 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Peng", "Yifan", ""], ["Chen", "Qingyu", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2005.02827", "submitter": "Elhadji Mamadou Nguer Emnguer", "authors": "El hadji M. Fall, El hadji M. Nguer, Bao Diop Sokhna, Mouhamadou\n  Khoule, Mathieu Mangeot, Mame T. Cisse", "title": "Digraphie des langues ouest africaines : Latin2Ajami : un algorithme de\n  translitteration automatique", "comments": "in French. TAlaf TALN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The national languages of Senegal, like those of West Africa country in\ngeneral, are written with two alphabets : the Latin alphabet that draws its\nstrength from official decreesm and the completed Arabic script (Ajami),\nwidespread and well integrated, that has little institutional support. This\ndigraph created two worlds ignoring each other. Indeed, Ajami writing is\ngenerally used daily by populations from Koranic schools, while writing with\nthe Latin alphabet is used by people from the public school. To solve this\nproblem, it is useful to establish transliteration tools between these two\nscriptures. Preliminary work (Nguer, Bao-Diop, Fall, khoule, 2015) was\nperformed to locate the problems, challenges and prospects. This present work,\nmaking it subsequently fell into this. Its objective is the study and creation\nof a transliteration algorithm from latin towards Ajami.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:52:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Fall", "El hadji M.", ""], ["Nguer", "El hadji M.", ""], ["Sokhna", "Bao Diop", ""], ["Khoule", "Mouhamadou", ""], ["Mangeot", "Mathieu", ""], ["Cisse", "Mame T.", ""]]}, {"id": "2005.02835", "submitter": "Zhihao Liang", "authors": "Ruichu Cai, Zhihao Liang, Boyan Xu, Zijian Li, Yuexing Hao and Yao\n  Chen", "title": "TAG : Type Auxiliary Guiding for Code Comment Generation", "comments": "ACL 2020, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing leading code comment generation approaches with the\nstructure-to-sequence framework ignores the type information of the\ninterpretation of the code, e.g., operator, string, etc. However, introducing\nthe type information into the existing framework is non-trivial due to the\nhierarchical dependence among the type information. In order to address the\nissues above, we propose a Type Auxiliary Guiding encoder-decoder framework for\nthe code comment generation task which considers the source code as an N-ary\ntree with type information associated with each node. Specifically, our\nframework is featured with a Type-associated Encoder and a Type-restricted\nDecoder which enables adaptive summarization of the source code. We further\npropose a hierarchical reinforcement learning method to resolve the training\ndifficulties of our proposed framework. Extensive evaluations demonstrate the\nstate-of-the-art performance of our framework with both the auto-evaluated\nmetrics and case studies.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:04:13 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Cai", "Ruichu", ""], ["Liang", "Zhihao", ""], ["Xu", "Boyan", ""], ["Li", "Zijian", ""], ["Hao", "Yuexing", ""], ["Chen", "Yao", ""]]}, {"id": "2005.02843", "submitter": "Faegheh Hasibi", "authors": "Emma J. Gerritse, Faegheh Hasibi, and Arjen P. de Vries", "title": "Graph-Embedding Empowered Entity Retrieval", "comments": null, "journal-ref": "Advances in Information Retrieval. ECIR 2020. Lecture Notes in\n  Computer Science, vol 12035. Springer,", "doi": "10.1007/978-3-030-45439-5_7", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we improve upon the current state of the art in entity\nretrieval by re-ranking the result list using graph embeddings. The paper shows\nthat graph embeddings are useful for entity-oriented search tasks. We\ndemonstrate empirically that encoding information from the knowledge graph into\n(graph) embeddings contributes to a higher increase in effectiveness of entity\nretrieval results than using plain word embeddings. We analyze the impact of\nthe accuracy of the entity linker on the overall retrieval effectiveness. Our\nanalysis further deploys the cluster hypothesis to explain the observed\nadvantages of graph embeddings over the more widely used word embeddings, for\nuser tasks involving ranking entities.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:13:49 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gerritse", "Emma J.", ""], ["Hasibi", "Faegheh", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "2005.02877", "submitter": "Michael Heck", "authors": "Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser,\n  Hsien-Chin Lin, Marco Moresi, Milica Ga\\v{s}i\\'c", "title": "TripPy: A Triple Copy Strategy for Value Independent Neural Dialog State\n  Tracking", "comments": "10 pages, 6 figures, published in Proceedings of the 21st Annual\n  SIGdial Meeting on Discourse and Dialogue, Code at:\n  https://gitlab.cs.uni-duesseldorf.de/general/dsml/trippy-public", "journal-ref": "Proceedings of the 21th Annual Meeting of the Special Interest\n  Group on Discourse and Dialogue (July 2020), Pages 35-44; Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user's goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem's inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:52:48 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:10:27 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 15:14:56 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 13:46:29 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Heck", "Michael", ""], ["van Niekerk", "Carel", ""], ["Lubis", "Nurul", ""], ["Geishauser", "Christian", ""], ["Lin", "Hsien-Chin", ""], ["Moresi", "Marco", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "2005.02914", "submitter": "Xiangyang Li", "authors": "Xiangyang Li, Guo Pu, Keyu Ming, Pu Li, Jie Wang, Yuxuan Wang", "title": "Review of Text Style Transfer Based on Deep Learning", "comments": "There are some nonstandard problems in current papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text style transfer is a hot issue in recent natural language\nprocessing,which mainly studies the text to adapt to different specific\nsituations, audiences and purposes by making some changes. The style of the\ntext usually includes many aspects such as morphology, grammar, emotion,\ncomplexity, fluency, tense, tone and so on. In the traditional text style\ntransfer model, the text style is generally relied on by experts knowledge and\nhand-designed rules, but with the application of deep learning in the field of\nnatural language processing, the text style transfer method based on deep\nlearning Started to be heavily researched. In recent years, text style transfer\nis becoming a hot issue in natural language processing research. This article\nsummarizes the research on the text style transfer model based on deep learning\nin recent years, and summarizes, analyzes and compares the main research\ndirections and progress. In addition, the article also introduces public data\nsets and evaluation indicators commonly used for text style transfer. Finally,\nthe existing characteristics of the text style transfer model are summarized,\nand the future development trend of the text style transfer model based on deep\nlearning is analyzed and forecasted.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:35:53 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 02:59:13 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 03:04:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Li", "Xiangyang", ""], ["Pu", "Guo", ""], ["Ming", "Keyu", ""], ["Li", "Pu", ""], ["Wang", "Jie", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2005.02925", "submitter": "Li Dong", "authors": "Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, Ke Xu", "title": "Harvesting and Refining Question-Answer Pairs for Unsupervised QA", "comments": "Accepted by ACL-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) has shown great success thanks to the availability of\nlarge-scale datasets and the effectiveness of neural models. Recent research\nworks have attempted to extend these successes to the settings with few or no\nlabeled data available. In this work, we introduce two approaches to improve\nunsupervised QA. First, we harvest lexically and syntactically divergent\nquestions from Wikipedia to automatically construct a corpus of question-answer\npairs (named as RefQA). Second, we take advantage of the QA model to extract\nmore appropriate answers, which iteratively refines data over RefQA. We conduct\nexperiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to\nmanually annotated data. Our approach outperforms previous unsupervised\napproaches by a large margin and is competitive with early supervised models.\nWe also show the effectiveness of our approach in the few-shot learning\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:56:06 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Zhongli", ""], ["Wang", "Wenhui", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""], ["Xu", "Ke", ""]]}, {"id": "2005.02954", "submitter": "Benajmin Radford J", "authors": "Benjamin J. Radford", "title": "Multitask Models for Supervised Protests Detection in Texts", "comments": null, "journal-ref": "Working Notes of CLEF 2019 (2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CLEF 2019 ProtestNews Lab tasks participants to identify text relating to\npolitical protests within larger corpora of news data. Three tasks include\narticle classification, sentence detection, and event extraction. I apply\nmultitask neural networks capable of producing predictions for two and three of\nthese tasks simultaneously. The multitask framework allows the model to learn\nrelevant features from the training data of all three tasks. This paper\ndemonstrates performance near or above the reported state-of-the-art for\nautomated political event coding though noted differences in research design\nmake direct comparisons difficult.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:00:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Radford", "Benjamin J.", ""]]}, {"id": "2005.02966", "submitter": "Benajmin Radford J", "authors": "Benjamin J. Radford", "title": "Seeing the Forest and the Trees: Detection and Cross-Document\n  Coreference Resolution of Militarized Interstate Disputes", "comments": null, "journal-ref": "Workshop Proceedings of AESPEN at LREC 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous efforts to automate the detection of social and political events in\ntext have primarily focused on identifying events described within single\nsentences or documents. Within a corpus of documents, these automated systems\nare unable to link event references -- recognize singular events across\nmultiple sentences or documents. A separate literature in computational\nlinguistics on event coreference resolution attempts to link known events to\none another within (and across) documents. I provide a data set for evaluating\nmethods to identify certain political events in text and to link related texts\nto one another based on shared events. The data set, Headlines of War, is built\non the Militarized Interstate Disputes data set and offers headlines classified\nby dispute status and headline pairs labeled with coreference indicators.\nAdditionally, I introduce a model capable of accomplishing both tasks. The\nmulti-task convolutional neural network is shown to be capable of recognizing\nevents and event coreferences given the headlines' texts and publication dates.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:20:14 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Radford", "Benjamin J.", ""]]}, {"id": "2005.02982", "submitter": "Guy Edward Toh Emerson", "authors": "Guy Emerson", "title": "What are the Goals of Distributional Semantics?", "comments": "To be published in Proceedings of the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantic models have become a mainstay in NLP, providing\nuseful features for downstream tasks. However, assessing long-term progress\nrequires explicit long-term goals. In this paper, I take a broad linguistic\nperspective, looking at how well current models can deal with various semantic\nchallenges. Given stark differences between models proposed in different\nsubfields, a broad perspective is needed to see how we could integrate them. I\nconclude that, while linguistic insights can guide the design of model\narchitectures, future progress will require balancing the often conflicting\ndemands of linguistic expressiveness and computational tractability.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:36:16 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Emerson", "Guy", ""]]}, {"id": "2005.02990", "submitter": "Shubham Toshniwal", "authors": "Shubham Toshniwal, Allyson Ettinger, Kevin Gimpel, and Karen Livescu", "title": "PeTra: A Sparsely Supervised Memory Model for People Tracking", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PeTra, a memory-augmented neural network designed to track\nentities in its memory slots. PeTra is trained using sparse annotation from the\nGAP pronoun resolution dataset and outperforms a prior memory model on the task\nwhile using a simpler architecture. We empirically compare key modeling\nchoices, finding that we can simplify several aspects of the design of the\nmemory module while retaining strong performance. To measure the people\ntracking capability of memory models, we (a) propose a new diagnostic\nevaluation based on counting the number of unique entities in text, and (b)\nconduct a small scale human evaluation to compare evidence of people tracking\nin the memory logs of PeTra relative to a previous approach. PeTra is highly\neffective in both evaluations, demonstrating its ability to track people in its\nmemory despite being trained with limited annotation.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:45:35 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Toshniwal", "Shubham", ""], ["Ettinger", "Allyson", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "2005.02991", "submitter": "Guy Edward Toh Emerson", "authors": "Guy Emerson", "title": "Autoencoding Pixies: Amortised Variational Inference with Graph\n  Convolutions for Functional Distributional Semantics", "comments": "To be published in Proceedings of the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL); added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Distributional Semantics provides a linguistically interpretable\nframework for distributional semantics, by representing the meaning of a word\nas a function (a binary classifier), instead of a vector. However, the large\nnumber of latent variables means that inference is computationally expensive,\nand training a model is therefore slow to converge. In this paper, I introduce\nthe Pixie Autoencoder, which augments the generative model of Functional\nDistributional Semantics with a graph-convolutional neural network to perform\namortised variational inference. This allows the model to be trained more\neffectively, achieving better results on two tasks (semantic similarity in\ncontext and semantic composition), and outperforming BERT, a large pre-trained\nlanguage model.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:46:40 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 14:35:43 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Emerson", "Guy", ""]]}, {"id": "2005.03008", "submitter": "Artem Kramov", "authors": "Artem Kramov", "title": "Evaluating text coherence based on the graph of the consistency of\n  phrases to identify symptoms of schizophrenia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different state-of-the-art methods of the detection of schizophrenia symptoms\nbased on the estimation of text coherence have been analyzed. The analysis of a\ntext at the level of phrases has been suggested. The method based on the graph\nof the consistency of phrases has been proposed to evaluate the semantic\ncoherence and the cohesion of a text. The semantic coherence, cohesion, and\nother linguistic features (lexical diversity, lexical density) have been taken\ninto account to form feature vectors for the training of a model-classifier.\nThe training of the classifier has been performed on the set of\nEnglish-language interviews. According to the retrieved results, the impact of\neach feature on the output of the model has been analyzed. The results obtained\ncan indicate that the proposed method based on the graph of the consistency of\nphrases may be used in the different tasks of the detection of mental illness.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 08:38:20 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kramov", "Artem", ""]]}, {"id": "2005.03035", "submitter": "Tianze Shi", "authors": "Tianze Shi, Lillian Lee", "title": "Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging,\n  and Joint Modeling Approaches", "comments": "Proceedings of ACL, 2020", "journal-ref": "Proceedings of ACL, 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interesting and frequent type of multi-word expression (MWE) is the\nheadless MWE, for which there are no true internal syntactic dominance\nrelations; examples include many named entities (\"Wells Fargo\") and dates\n(\"July 5, 2020\") as well as certain productive constructions (\"blow for blow\",\n\"day after day\"). Despite their special status and prevalence, current\ndependency-annotation schemes require treating such flat structures as if they\nhad internal syntactic heads, and most current parsers handle them in the same\nfashion as headed constructions. Meanwhile, outside the context of parsing,\ntaggers are typically used for identifying MWEs, but taggers might benefit from\nstructural information. We empirically compare these two common\nstrategies--parsing and tagging--for predicting flat MWEs. Additionally, we\npropose an efficient joint decoding algorithm that combines scores from both\nstrategies. Experimental results on the MWE-Aware English Dependency Corpus and\non six non-English dependency treebanks with frequent flat structures show\nthat: (1) tagging is more accurate than parsing for identifying flat-structure\nMWEs, (2) our joint decoder reconciles the two different views and, for\nnon-BERT features, leads to higher accuracies, and (3) most of the gains result\nfrom feature sharing between the parsers and taggers.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 18:00:04 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Shi", "Tianze", ""], ["Lee", "Lillian", ""]]}, {"id": "2005.03066", "submitter": "Danushka Bollegala", "authors": "Asir Saeed, Khai Mai, Pham Minh, Nguyen Tuan Duc, Danushka Bollegala", "title": "Weakly-Supervised Neural Response Selection from an Ensemble of\n  Task-Specialised Dialogue Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue engines that incorporate different types of agents to converse with\nhumans are popular.\n  However, conversations are dynamic in the sense that a selected response will\nchange the conversation on-the-fly, influencing the subsequent utterances in\nthe conversation, which makes the response selection a challenging problem.\n  We model the problem of selecting the best response from a set of responses\ngenerated by a heterogeneous set of dialogue agents by taking into account the\nconversational history, and propose a \\emph{Neural Response Selection} method.\n  The proposed method is trained to predict a coherent set of responses within\na single conversation, considering its own predictions via a curriculum\ntraining mechanism.\n  Our experimental results show that the proposed method can accurately select\nthe most appropriate responses, thereby significantly improving the user\nexperience in dialogue systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 18:40:26 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Saeed", "Asir", ""], ["Mai", "Khai", ""], ["Minh", "Pham", ""], ["Duc", "Nguyen Tuan", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2005.03074", "submitter": "Lachlan McPheat", "authors": "Lachlan McPheat, Mehrnoosh Sadrzadeh, Hadi Wazni, Gijs Wijnholds", "title": "Categorical Vector Space Semantics for Lambek Calculus with a Relevant\n  Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a categorical compositional distributional semantics for Lambek\nCalculus with a Relevant Modality !L*, which has a limited edition of the\ncontraction and permutation rules. The categorical part of the semantics is a\nmonoidal biclosed category with a coalgebra modality, very similar to the\nstructure of a Differential Category. We instantiate this category to finite\ndimensional vector spaces and linear maps via \"quantisation\" functors and work\nwith three concrete interpretations of the coalgebra modality. We apply the\nmodel to construct categorical and concrete semantic interpretations for the\nmotivating example of !L*: the derivation of a phrase with a parasitic gap. The\neffectiveness of the concrete interpretations are evaluated via a\ndisambiguation task, on an extension of a sentence disambiguation dataset to\nparasitic gap phrases, using BERT, Word2Vec, and FastText vectors and\nRelational tensors.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 18:58:21 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 15:39:28 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 16:26:02 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["McPheat", "Lachlan", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Wazni", "Hadi", ""], ["Wijnholds", "Gijs", ""]]}, {"id": "2005.03086", "submitter": "Hao Tan", "authors": "Yubo Zhang, Hao Tan, Mohit Bansal", "title": "Diagnosing the Environment Bias in Vision-and-Language Navigation", "comments": "IJCAI 2020 (9 pages; first two authors contributed equally)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to follow\nnatural-language instructions, explore the given environments, and reach the\ndesired target locations. These step-by-step navigational instructions are\ncrucial when the agent is navigating new environments about which it has no\nprior knowledge. Most recent works that study VLN observe a significant\nperformance drop when tested on unseen environments (i.e., environments not\nused in training), indicating that the neural agent models are highly biased\ntowards training environments. Although this issue is considered as one of the\nmajor challenges in VLN research, it is still under-studied and needs a clearer\nexplanation. In this work, we design novel diagnosis experiments via\nenvironment re-splitting and feature replacement, looking into possible reasons\nfor this environment bias. We observe that neither the language nor the\nunderlying navigational graph, but the low-level visual appearance conveyed by\nResNet features directly affects the agent model and contributes to this\nenvironment bias in results. According to this observation, we explore several\nkinds of semantic representations that contain less low-level visual\ninformation, hence the agent learned with these features could be better\ngeneralized to unseen testing environments. Without modifying the baseline\nagent model and its training method, our explored semantic features\nsignificantly decrease the performance gaps between seen and unseen on multiple\ndatasets (i.e. R2R, R4R, and CVDN) and achieve competitive unseen results to\nprevious state-of-the-art models. Our code and features are available at:\nhttps://github.com/zhangybzbo/EnvBiasVLN\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 19:24:33 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhang", "Yubo", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.03119", "submitter": "Po-Yao Huang", "authors": "Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann", "title": "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual\n  Pivoting", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised machine translation (MT) has recently achieved impressive\nresults with monolingual corpora only. However, it is still challenging to\nassociate source-target sentences in the latent space. As people speak\ndifferent languages biologically share similar visual systems, the potential of\nachieving better alignment through visual content is promising yet\nunder-explored in unsupervised multimodal MT (MMT). In this paper, we\ninvestigate how to utilize visual content for disambiguation and promoting\nlatent space alignment in unsupervised MMT. Our model employs multimodal\nback-translation and features pseudo visual pivoting in which we learn a shared\nmultilingual visual-semantic embedding space and incorporate visually-pivoted\ncaptioning as additional weak supervision. The experimental results on the\nwidely used Multi30K dataset show that the proposed model significantly\nimproves over the state-of-the-art methods and generalizes well when the images\nare not available at the testing time.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 20:11:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Huang", "Po-Yao", ""], ["Hu", "Junjie", ""], ["Chang", "Xiaojun", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2005.03174", "submitter": "Ryota Tanaka", "authors": "Ryota Tanaka, Akinobu Lee", "title": "Fact-based Dialogue Generation with Convergent and Divergent Decoding", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact-based dialogue generation is a task of generating a human-like response\nbased on both dialogue context and factual texts. Various methods were proposed\nto focus on generating informative words that contain facts effectively.\nHowever, previous works implicitly assume a topic to be kept on a dialogue and\nusually converse passively, therefore the systems have a difficulty to generate\ndiverse responses that provide meaningful information proactively. This paper\nproposes an end-to-end fact-based dialogue system augmented with the ability of\nconvergent and divergent thinking over both context and facts, which can\nconverse about the current topic or introduce a new topic. Specifically, our\nmodel incorporates a novel convergent and divergent decoding that can generate\ninformative and diverse responses considering not only given inputs (context\nand facts) but also inputs-related topics. Both automatic and human evaluation\nresults on DSTC7 dataset show that our model significantly outperforms\nstate-of-the-art baselines, indicating that our model can generate more\nappropriate, informative, and diverse responses.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 23:49:35 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 00:43:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Tanaka", "Ryota", ""], ["Lee", "Akinobu", ""]]}, {"id": "2005.03191", "submitter": "Zhengdong Zhang", "authors": "Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James\n  Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu", "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech\n  Recognition with Global Context", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have shown promising results for\nend-to-end speech recognition, albeit still behind other state-of-the-art\nmethods in performance. In this paper, we study how to bridge this gap and go\nbeyond with a novel CNN-RNN-transducer architecture, which we call ContextNet.\nContextNet features a fully convolutional encoder that incorporates global\ncontext information into convolution layers by adding squeeze-and-excitation\nmodules. In addition, we propose a simple scaling method that scales the widths\nof ContextNet that achieves good trade-off between computation and accuracy. We\ndemonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves\na word error rate (WER) of 2.1%/4.6% without external language model (LM),\n1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy\nLibriSpeech test sets. This compares to the previous best published system of\n2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the\nproposed ContextNet model is also verified on a much larger internal dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 01:03:18 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 01:45:13 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 00:49:21 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Han", "Wei", ""], ["Zhang", "Zhengdong", ""], ["Zhang", "Yu", ""], ["Yu", "Jiahui", ""], ["Chiu", "Chung-Cheng", ""], ["Qin", "James", ""], ["Gulati", "Anmol", ""], ["Pang", "Ruoming", ""], ["Wu", "Yonghui", ""]]}, {"id": "2005.03257", "submitter": "Xiaodong Ge", "authors": "Siwei Fu, Kai Xiong, Xiaodong Ge, Siliang Tang, Wei Chen, Yingcai Wu", "title": "Quda: Natural Language Queries for Visual Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of analytic tasks from free text is critical for\nvisualization-oriented natural language interfaces (V-NLIs) to suggest\neffective visualizations. However, it is challenging due to the ambiguity and\ncomplexity nature of human language. To address this challenge, we present a\nnew dataset, called Quda, that aims to help V-NLIs recognize analytic tasks\nfrom free-form natural language by training and evaluating cutting-edge\nmulti-label classification models. Our dataset contains $14,035$ diverse user\nqueries, and each is annotated with one or multiple analytic tasks. We achieve\nthis goal by first gathering seed queries with data analysts and then employing\nextensive crowd force for paraphrase generation and validation. We demonstrate\nthe usefulness of Quda through three applications. This work is the first\nattempt to construct a large-scale corpus for recognizing analytic tasks. With\nthe release of Quda, we hope it will boost the research and development of\nV-NLIs in data analysis and visualization.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 05:35:16 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:00:51 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 12:45:39 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 07:34:50 GMT"}, {"version": "v5", "created": "Thu, 3 Dec 2020 06:58:56 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fu", "Siwei", ""], ["Xiong", "Kai", ""], ["Ge", "Xiaodong", ""], ["Tang", "Siliang", ""], ["Chen", "Wei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2005.03271", "submitter": "Chung-Cheng Chiu", "authors": "Chung-Cheng Chiu, Arun Narayanan, Wei Han, Rohit Prabhavalkar, Yu\n  Zhang, Navdeep Jaitly, Ruoming Pang, Tara N. Sainath, Patrick Nguyen,\n  Liangliang Cao, Yonghui Wu", "title": "RNN-T Models Fail to Generalize to Out-of-Domain Audio: Causes and\n  Solutions", "comments": "SLT camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, all-neural end-to-end approaches have obtained\nstate-of-the-art results on several challenging automatic speech recognition\n(ASR) tasks. However, most existing works focus on building ASR models where\ntrain and test data are drawn from the same domain. This results in poor\ngeneralization characteristics on mismatched-domains: e.g., end-to-end models\ntrained on short segments perform poorly when evaluated on longer utterances.\nIn this work, we analyze the generalization properties of streaming and\nnon-streaming recurrent neural network transducer (RNN-T) based end-to-end\nmodels in order to identify model components that negatively affect\ngeneralization performance. We propose two solutions: combining multiple\nregularization techniques during training, and using dynamic overlapping\ninference. On a long-form YouTube test set, when the nonstreaming RNN-T model\nis trained with shorter segments of data, the proposed combination improves\nword error rate (WER) from 22.3% to 14.8%; when the streaming RNN-T model\ntrained on short Search queries, the proposed techniques improve WER on the\nYouTube set from 67.0% to 25.3%. Finally, when trained on Librispeech, we find\nthat dynamic overlapping inference improves WER on YouTube from 99.8% to 33.0%.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 06:24:47 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 05:37:07 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 00:48:31 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Chiu", "Chung-Cheng", ""], ["Narayanan", "Arun", ""], ["Han", "Wei", ""], ["Prabhavalkar", "Rohit", ""], ["Zhang", "Yu", ""], ["Jaitly", "Navdeep", ""], ["Pang", "Ruoming", ""], ["Sainath", "Tara N.", ""], ["Nguyen", "Patrick", ""], ["Cao", "Liangliang", ""], ["Wu", "Yonghui", ""]]}, {"id": "2005.03292", "submitter": "Peter Hillmann", "authors": "Mario Golling, Robert Koch, Peter Hillmann, Rick Hofstede, Frank\n  Tietze", "title": "YANG2UML: Bijective Transformation and Simplification of YANG to UML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.DC cs.NI cs.PL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking is currently revolutionizing computer networking\nby decoupling the network control (control plane) from the forwarding functions\n(data plane) enabling the network control to become directly programmable and\nthe underlying infrastructure to be abstracted for applications and network\nservices. Next to the well-known OpenFlow protocol, the XML-based NETCONF\nprotocol is also an important means for exchanging configuration information\nfrom a management platform and is nowadays even part of OpenFlow. In\ncombination with NETCONF, YANG is the corresponding protocol that defines the\nassociated data structures supporting virtually all network configuration\nprotocols. YANG itself is a semantically rich language, which -- in order to\nfacilitate familiarization with the relevant subject -- is often visualized to\ninvolve other experts or developers and to support them by their daily work\n(writing applications which make use of YANG). In order to support this\nprocess, this paper presents an novel approach to optimize and simplify YANG\ndata models to assist further discussions with the management and\nimplementations (especially of interfaces) to reduce complexity. Therefore, we\nhave defined a bidirectional mapping of YANG to UML and developed a tool that\nrenders the created UML diagrams. This combines the benefits to use the formal\nlanguage YANG with automatically maintained UML diagrams to involve other\nexperts or developers, closing the gap between technically improved data models\nand their human readability.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:29:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Golling", "Mario", ""], ["Koch", "Robert", ""], ["Hillmann", "Peter", ""], ["Hofstede", "Rick", ""], ["Tietze", "Frank", ""]]}, {"id": "2005.03299", "submitter": "Yan Cao", "authors": "Yan Cao, Keting Lu, Xiaoping Chen, Shiqi Zhang", "title": "Adaptive Dialog Policy Learning with Hindsight and User Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning methods have been used to compute dialog policies from\nlanguage-based interaction experiences. Efficiency is of particular importance\nin dialog policy learning, because of the considerable cost of interacting with\npeople, and the very poor user experience from low-quality conversations.\nAiming at improving the efficiency of dialog policy learning, we develop\nalgorithm LHUA (Learning with Hindsight, User modeling, and Adaptation) that,\nfor the first time, enables dialog agents to adaptively learn with hindsight\nfrom both simulated and real users. Simulation and hindsight provide the dialog\nagent with more experience and more (positive) reinforcements respectively.\nExperimental results suggest that, in success rate and policy quality, LHUA\noutperforms competitive baselines from the literature, including its\nno-simulation, no-adaptation, and no-hindsight counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:43:43 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Cao", "Yan", ""], ["Lu", "Keting", ""], ["Chen", "Xiaoping", ""], ["Zhang", "Shiqi", ""]]}, {"id": "2005.03312", "submitter": "Avi Shmidman", "authors": "Avi Shmidman, Shaltiel Shmidman, Moshe Koppel, Yoav Goldberg", "title": "Nakdan: Professional Hebrew Diacritizer", "comments": "Accepted to ACL 2020, System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for automatic diacritization of Hebrew text. The system\ncombines modern neural models with carefully curated declarative linguistic\nknowledge and comprehensive manually constructed tables and dictionaries.\nBesides providing state of the art diacritization accuracy, the system also\nsupports an interface for manual editing and correction of the automatic\noutput, and has several features which make it particularly useful for\npreparation of scientific editions of Hebrew texts. The system supports Modern\nHebrew, Rabbinic Hebrew and Poetic Hebrew. The system is freely accessible for\nall use at http://nakdanpro.dicta.org.il.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:15:55 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Shmidman", "Avi", ""], ["Shmidman", "Shaltiel", ""], ["Koppel", "Moshe", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2005.03356", "submitter": "Seongho Choi", "authors": "Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang,\n  Minsu Lee, Byoung-Tak Zhang", "title": "DramaQA: Character-Centered Video Story Understanding with Hierarchical\n  QA", "comments": "15 pages, 11 figures, accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress on computer vision and natural language processing,\ndeveloping a machine that can understand video story is still hard to achieve\ndue to the intrinsic difficulty of video story. Moreover, researches on how to\nevaluate the degree of video understanding based on human cognitive process\nhave not progressed as yet. In this paper, we propose a novel video question\nanswering (Video QA) task, DramaQA, for a comprehensive understanding of the\nvideo story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an\nevaluation metric based on the cognitive developmental stages of human\nintelligence. 2) Character-centered video annotations to model local coherence\nof the story. Our dataset is built upon the TV drama \"Another Miss Oh\" and it\ncontains 17,983 QA pairs from 23,928 various length video clips, with each QA\npair belonging to one of four difficulty levels. We provide 217,308 annotated\nimages with rich character-centered annotations, including visual bounding\nboxes, behaviors and emotions of main characters, and coreference resolved\nscripts. Additionally, we suggest Multi-level Context Matching model which\nhierarchically understands character-centered representations of video to\nanswer questions. We release our dataset and model publicly for research\npurposes, and we expect our work to provide a new perspective on video story\nunderstanding research.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:44:58 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 02:59:37 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Choi", "Seongho", ""], ["On", "Kyoung-Woon", ""], ["Heo", "Yu-Jung", ""], ["Seo", "Ahjeong", ""], ["Jang", "Youwon", ""], ["Lee", "Minsu", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2005.03361", "submitter": "Zhuoyuan Mao", "authors": "Zhuoyuan Mao, Fabien Cromieres, Raj Dabre, Haiyue Song, Sadao\n  Kurohashi", "title": "JASS: Japanese-specific Sequence to Sequence Pre-training for Neural\n  Machine Translation", "comments": "LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) needs large parallel corpora for\nstate-of-the-art translation quality. Low-resource NMT is typically addressed\nby transfer learning which leverages large monolingual or parallel corpora for\npre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence\nto Sequence) are extremely effective in boosting NMT quality for languages with\nsmall parallel corpora. However, they do not account for linguistic information\nobtained using syntactic analyzers which is known to be invaluable for several\nNatural Language Processing (NLP) tasks. To this end, we propose JASS,\nJapanese-specific Sequence to Sequence, as a novel pre-training alternative to\nMASS for NMT involving Japanese as the source or target language. JASS is joint\nBMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence)\npre-training which focuses on Japanese linguistic units called bunsetsus. In\nour experiments on ASPEC Japanese--English and News Commentary\nJapanese--Russian translation we show that JASS can give results that are\ncompetitive with if not better than those given by MASS. Furthermore, we show\nfor the first time that joint MASS and JASS pre-training gives results that\nsignificantly surpass the individual methods indicating their complementary\nnature. We will release our code, pre-trained models and bunsetsu annotated\ndata as resources for researchers to use in their own NLP tasks.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:53:25 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Mao", "Zhuoyuan", ""], ["Cromieres", "Fabien", ""], ["Dabre", "Raj", ""], ["Song", "Haiyue", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "2005.03375", "submitter": "Pranav A", "authors": "Pranav A, Isabelle Augenstein", "title": "2kenize: Tying Subword Sequences for Chinese Script Conversion", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplified Chinese to Traditional Chinese character conversion is a common\npreprocessing step in Chinese NLP. Despite this, current approaches have poor\nperformance because they do not take into account that a simplified Chinese\ncharacter can correspond to multiple traditional characters. Here, we propose a\nmodel that can disambiguate between mappings and convert between the two\nscripts. The model is based on subword segmentation, two language models, as\nwell as a method for mapping between subword sequences. We further construct\nbenchmark datasets for topic classification and script conversion. Our proposed\nmethod outperforms previous Chinese Character conversion approaches by 6 points\nin accuracy. These results are further confirmed in a downstream application,\nwhere 2kenize is used to convert pretraining dataset for topic classification.\nAn error analysis reveals that our method's particular strengths are in dealing\nwith code-mixing and named entities.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 10:53:05 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["A", "Pranav", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2005.03393", "submitter": "Li Bei", "authors": "Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu,\n  Tongran Liu, Changliang Li", "title": "Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine\n  Translation", "comments": "5 pages, 2 figures, 5 tables, accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In encoder-decoder neural models, multiple encoders are in general used to\nrepresent the contextual information in addition to the individual sentence. In\nthis paper, we investigate multi-encoder approaches in documentlevel neural\nmachine translation (NMT). Surprisingly, we find that the context encoder does\nnot only encode the surrounding sentences but also behaves as a noise\ngenerator. This makes us rethink the real benefits of multi-encoder in\ncontext-aware translation - some of the improvements come from robust training.\nWe compare several methods that introduce noise and/or well-tuned dropout setup\ninto the training of these encoders. Experimental results show that noisy\ntraining plays an important role in multi-encoder-based NMT, especially when\nthe training data is small. Also, we establish a new state-of-the-art on IWSLT\nFr-En task by careful use of noise generation and dropout methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 11:39:11 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 03:09:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Bei", ""], ["Liu", "Hui", ""], ["Wang", "Ziyang", ""], ["Jiang", "Yufan", ""], ["Xiao", "Tong", ""], ["Zhu", "Jingbo", ""], ["Liu", "Tongran", ""], ["Li", "Changliang", ""]]}, {"id": "2005.03418", "submitter": "Juliette Millet", "authors": "Juliette Millet and Ewan Dunbar", "title": "The Perceptimatic English Benchmark for Speech Perception Models", "comments": "Accepted to CogSci Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Perceptimatic English Benchmark, an open experimental\nbenchmark for evaluating quantitative models of speech perception in English.\nThe benchmark consists of ABX stimuli along with the responses of 91 American\nEnglish-speaking listeners. The stimuli test discrimination of a large number\nof English and French phonemic contrasts. They are extracted directly from\ncorpora of read speech, making them appropriate for evaluating statistical\nacoustic models (such as those used in automatic speech recognition) trained on\ntypical speech data sets. We show that phone discrimination is correlated with\nseveral types of models, and give recommendations for researchers seeking\neasily calculated norms of acoustic distance on experimental stimuli. We show\nthat DeepSpeech, a standard English speech recognizer, is more specialized on\nEnglish phoneme discrimination than English listeners, and is poorly correlated\nwith their behaviour, even though it yields a low error on the decision task\ngiven to humans.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 12:35:44 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Millet", "Juliette", ""], ["Dunbar", "Ewan", ""]]}, {"id": "2005.03436", "submitter": "Dmitry Nikolaev", "authors": "Dmitry Nikolaev, Ofir Arviv, Taelin Karidi, Neta Kenneth, Veronika\n  Mitnik, Lilja Maria Saeboe, and Omri Abend", "title": "Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The patterns in which the syntax of different languages converges and\ndiverges are often used to inform work on cross-lingual transfer. Nevertheless,\nlittle empirical work has been done on quantifying the prevalence of different\nsyntactic divergences across language pairs. We propose a framework for\nextracting divergence patterns for any language pair from a parallel corpus,\nbuilding on Universal Dependencies. We show that our framework provides a\ndetailed picture of cross-language divergences, generalizes previous\napproaches, and lends itself to full automation. We further present a novel\ndataset, a manually word-aligned subset of the Parallel UD corpus in five\nlanguages, and use it to perform a detailed corpus study. We demonstrate the\nusefulness of the resulting analysis by showing that it can help account for\nperformance patterns of a cross-lingual parser.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:05:03 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 08:48:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nikolaev", "Dmitry", ""], ["Arviv", "Ofir", ""], ["Karidi", "Taelin", ""], ["Kenneth", "Neta", ""], ["Mitnik", "Veronika", ""], ["Saeboe", "Lilja Maria", ""], ["Abend", "Omri", ""]]}, {"id": "2005.03454", "submitter": "Christopher Brix", "authors": "Christopher Brix, Parnia Bahar, Hermann Ney", "title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the\n  Transformer Architecture", "comments": "Accepted at ACL2020; evaluation corrected: magnitude pruning may be\n  useful to find winning lottery tickets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse models require less memory for storage and enable a faster inference\nby reducing the necessary number of FLOPs. This is relevant both for\ntime-critical and on-device computations using neural networks. The stabilized\nlottery ticket hypothesis states that networks can be pruned after none or few\ntraining iterations, using a mask computed based on the unpruned converged\nmodel. On the transformer architecture and the WMT 2014 English-to-German and\nEnglish-to-French tasks, we show that stabilized lottery ticket pruning\nperforms similar to magnitude pruning for sparsity levels of up to 85%, and\npropose a new combination of pruning techniques that outperforms all other\ntechniques for even higher levels of sparsity. Furthermore, we confirm that the\nparameter's initial sign and not its specific value is the primary factor for\nsuccessful training, and show that magnitude pruning could be used to find\nwinning lottery tickets.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:17:28 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 15:22:02 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Brix", "Christopher", ""], ["Bahar", "Parnia", ""], ["Ney", "Hermann", ""]]}, {"id": "2005.03510", "submitter": "Dongyub Lee", "authors": "Dongyub Lee, Myeongcheol Shin, Taesun Whang, Seungwoo Cho, Byeongil\n  Ko, Daniel Lee, Eunggyun Kim, Jaechoon Jo", "title": "Reference and Document Aware Semantic Evaluation Methods for Korean\n  Language Summarization", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization refers to the process that generates a shorter form of\ntext from the source document preserving salient information. Many existing\nworks for text summarization are generally evaluated by using recall-oriented\nunderstudy for gisting evaluation (ROUGE) scores. However, as ROUGE scores are\ncomputed based on n-gram overlap, they do not reflect semantic meaning\ncorrespondences between generated and reference summaries. Because Korean is an\nagglutinative language that combines various morphemes into a word that express\nseveral meanings, ROUGE is not suitable for Korean summarization. In this\npaper, we propose evaluation metrics that reflect semantic meanings of a\nreference summary and the original document, Reference and Document Aware\nSemantic Score (RDASS). We then propose a method for improving the correlation\nof the metrics with human judgment. Evaluation results show that the\ncorrelation with human judgment is significantly higher for our evaluation\nmetrics than for ROUGE scores.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:26:30 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 02:40:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Lee", "Dongyub", ""], ["Shin", "Myeongcheol", ""], ["Whang", "Taesun", ""], ["Cho", "Seungwoo", ""], ["Ko", "Byeongil", ""], ["Lee", "Daniel", ""], ["Kim", "Eunggyun", ""], ["Jo", "Jaechoon", ""]]}, {"id": "2005.03519", "submitter": "Junpei Zhou", "authors": "Junpei Zhou, Ciprian Chelba, Yuezhang (Music) Li", "title": "Practical Perspectives on Quality Estimation for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence level quality estimation (QE) for machine translation (MT) attempts\nto predict the translation edit rate (TER) cost of post-editing work required\nto correct MT output. We describe our view on sentence-level QE as dictated by\nseveral practical setups encountered in the industry. We find consumers of MT\noutput---whether human or algorithmic ones---to be primarily interested in a\nbinary quality metric: is the translated sentence adequate as-is or does it\nneed post-editing? Motivated by this we propose a quality classification (QC)\nview on sentence-level QE whereby we focus on maximizing recall at precision\nabove a given threshold. We demonstrate that, while classical QE regression\nmodels fare poorly on this task, they can be re-purposed by replacing the\noutput regression layer with a binary classification one, achieving 50-60\\%\nrecall at 90\\% precision. For a high-quality MT system producing 75-80\\%\ncorrect translations, this promises a significant reduction in post-editing\nwork indeed.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 01:50:10 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhou", "Junpei", "", "Music"], ["Chelba", "Ciprian", "", "Music"], ["Yuezhang", "", "", "Music"], ["Li", "", ""]]}, {"id": "2005.03521", "submitter": "Manuel Ciosici", "authors": "Leon Str{\\o}mberg-Derczynski, Manuel R. Ciosici, Rebekah Baglini,\n  Morten H. Christiansen, Jacob Aarup Dalsgaard, Riccardo Fusaroli, Peter Juel\n  Henrichsen, Rasmus Hvingelby, Andreas Kirkedal, Alex Speed Kjeldsen, Claus\n  Ladefoged, Finn {\\AA}rup Nielsen, Malte Lau Petersen, Jonathan Hvithamar\n  Rystr{\\o}m, Daniel Varab", "title": "The Danish Gigaword Project", "comments": "Identical to the NoDaLiDa 2021 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Danish language technology has been hindered by a lack of broad-coverage\ncorpora at the scale modern NLP prefers. This paper describes the Danish\nGigaword Corpus, the result of a focused effort to provide a diverse and\nfreely-available one billion word corpus of Danish text. The Danish Gigaword\ncorpus covers a wide array of time periods, domains, speakers' socio-economic\nstatus, and Danish dialects.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:40:56 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 10:11:24 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 20:52:20 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Str\u00f8mberg-Derczynski", "Leon", ""], ["Ciosici", "Manuel R.", ""], ["Baglini", "Rebekah", ""], ["Christiansen", "Morten H.", ""], ["Dalsgaard", "Jacob Aarup", ""], ["Fusaroli", "Riccardo", ""], ["Henrichsen", "Peter Juel", ""], ["Hvingelby", "Rasmus", ""], ["Kirkedal", "Andreas", ""], ["Kjeldsen", "Alex Speed", ""], ["Ladefoged", "Claus", ""], ["Nielsen", "Finn \u00c5rup", ""], ["Petersen", "Malte Lau", ""], ["Rystr\u00f8m", "Jonathan Hvithamar", ""], ["Varab", "Daniel", ""]]}, {"id": "2005.03545", "submitter": "Devamanyu Hazarika", "authors": "Devamanyu Hazarika, Roger Zimmermann, Soujanya Poria", "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal\n  Sentiment Analysis", "comments": "Accepted at ACM MM 2020 (Oral Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multimodal Sentiment Analysis is an active area of research that leverages\nmultimodal signals for affective understanding of user-generated videos. The\npredominant approach, addressing this task, has been to develop sophisticated\nfusion techniques. However, the heterogeneous nature of the signals creates\ndistributional modality gaps that pose significant challenges. In this paper,\nwe aim to learn effective modality representations to aid the process of\nfusion. We propose a novel framework, MISA, which projects each modality to two\ndistinct subspaces. The first subspace is modality-invariant, where the\nrepresentations across modalities learn their commonalities and reduce the\nmodality gap. The second subspace is modality-specific, which is private to\neach modality and captures their characteristic features. These representations\nprovide a holistic view of the multimodal data, which is used for fusion that\nleads to task predictions. Our experiments on popular sentiment analysis\nbenchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art\nmodels. We also consider the task of Multimodal Humor Detection and experiment\non the recently proposed UR_FUNNY dataset. Here too, our model fares better\nthan strong baselines, establishing MISA as a useful multimodal framework.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:13:23 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 12:14:37 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 13:41:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hazarika", "Devamanyu", ""], ["Zimmermann", "Roger", ""], ["Poria", "Soujanya", ""]]}, {"id": "2005.03588", "submitter": "Inkit Padhi", "authors": "Inkit Padhi, Pierre Dognin, Ke Bai, Cicero Nogueira dos Santos, Vijil\n  Chenthamarakshan, Youssef Mroueh, Payel Das", "title": "Learning Implicit Text Generation via Feature Matching", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative feature matching network (GFMN) is an approach for training\nimplicit generative models for images by performing moment matching on features\nfrom pre-trained neural networks. In this paper, we present new GFMN\nformulations that are effective for sequential data. Our experimental results\nshow the effectiveness of the proposed method, SeqGFMN, for three distinct\ngeneration tasks in English: unconditional text generation, class-conditional\ntext generation, and unsupervised text style transfer. SeqGFMN is stable to\ntrain and outperforms various adversarial approaches for text generation and\ntext style transfer.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:16:24 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 00:17:49 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Padhi", "Inkit", ""], ["Dognin", "Pierre", ""], ["Bai", "Ke", ""], ["Santos", "Cicero Nogueira dos", ""], ["Chenthamarakshan", "Vijil", ""], ["Mroueh", "Youssef", ""], ["Das", "Payel", ""]]}, {"id": "2005.03593", "submitter": "Trevor Cohen", "authors": "Trevor Cohen and Serguei Pakhomov", "title": "A Tale of Two Perplexities: Sensitivity of Neural Language Models to\n  Lexical Retrieval Deficits in Dementia of the Alzheimer's Type", "comments": "To be published in the Proceedings of the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a burgeoning interest in the use of\ncomputational methods to distinguish between elicited speech samples produced\nby patients with dementia, and those from healthy controls. The difference\nbetween perplexity estimates from two neural language models (LMs) - one\ntrained on transcripts of speech produced by healthy participants and the other\ntrained on transcripts from patients with dementia - as a single feature for\ndiagnostic classification of unseen transcripts has been shown to produce\nstate-of-the-art performance. However, little is known about why this approach\nis effective, and on account of the lack of case/control matching in the most\nwidely-used evaluation set of transcripts (DementiaBank), it is unclear if\nthese approaches are truly diagnostic, or are sensitive to other variables. In\nthis paper, we interrogate neural LMs trained on participants with and without\ndementia using synthetic narratives previously developed to simulate\nprogressive semantic dementia by manipulating lexical frequency. We find that\nperplexity of neural LMs is strongly and differentially associated with lexical\nfrequency, and that a mixture model resulting from interpolating control and\ndementia LMs improves upon the current state-of-the-art for models trained on\ntranscript text exclusively.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:22:48 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 19:59:48 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Cohen", "Trevor", ""], ["Pakhomov", "Serguei", ""]]}, {"id": "2005.03624", "submitter": "Thanh Nguyen", "authors": "Thanh V. Nguyen, Nikhil Rao and Karthik Subbian", "title": "Learning Robust Models for e-Commerce Product Search", "comments": "This work has been accepted for publication at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Showing items that do not match search query intent degrades customer\nexperience in e-commerce. These mismatches result from counterfactual biases of\nthe ranking algorithms toward noisy behavioral signals such as clicks and\npurchases in the search logs. Mitigating the problem requires a large labeled\ndataset, which is expensive and time-consuming to obtain. In this paper, we\ndevelop a deep, end-to-end model that learns to effectively classify mismatches\nand to generate hard mismatched examples to improve the classifier. We train\nthe model end-to-end by introducing a latent variable into the cross-entropy\nloss that alternates between using the real and generated samples. This not\nonly makes the classifier more robust but also boosts the overall ranking\nperformance. Our model achieves a relative gain compared to baselines by over\n26% in F-score, and over 17% in Area Under PR curve. On live search traffic,\nour model gains significant improvement in multiple countries.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:22:21 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Nguyen", "Thanh V.", ""], ["Rao", "Nikhil", ""], ["Subbian", "Karthik", ""]]}, {"id": "2005.03640", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Edgard Marx, Andr\\'e Valdestilhas, Diego Moussallem,\n  Gustavo Publio, and Muhammad Saleem", "title": "Where is Linked Data in Question Answering over Linked Data?", "comments": "Position paper, THE Workshop @ ISWC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that \"Question Answering with Knowledge Base\" and \"Question\nAnswering over Linked Data\" are currently two instances of the same problem,\ndespite one explicitly declares to deal with Linked Data. We point out the lack\nof existing methods to evaluate question answering on datasets which exploit\nexternal links to the rest of the cloud or share common schema. To this end, we\npropose the creation of new evaluation settings to leverage the advantages of\nthe Semantic Web to achieve AI-complete question answering.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:42:13 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Soru", "Tommaso", ""], ["Marx", "Edgard", ""], ["Valdestilhas", "Andr\u00e9", ""], ["Moussallem", "Diego", ""], ["Publio", "Gustavo", ""], ["Saleem", "Muhammad", ""]]}, {"id": "2005.03642", "submitter": "Rico Sennrich", "authors": "Chaojun Wang and Rico Sennrich", "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine\n  Translation", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard training algorithm in neural machine translation (NMT) suffers\nfrom exposure bias, and alternative algorithms have been proposed to mitigate\nthis. However, the practical impact of exposure bias is under debate. In this\npaper, we link exposure bias to another well-known problem in NMT, namely the\ntendency to generate hallucinations under domain shift. In experiments on three\ndatasets with multiple test domains, we show that exposure bias is partially to\nblame for hallucinations, and that training with Minimum Risk Training, which\navoids exposure bias, can mitigate this. Our analysis explains why exposure\nbias is more problematic under domain shift, and also links exposure bias to\nthe beam search problem, i.e. performance deterioration with increasing beam\nsize. Our results provide a new justification for methods that reduce exposure\nbias: even if they do not increase performance on in-domain test sets, they can\nincrease model robustness to domain shift.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:46:02 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Wang", "Chaojun", ""], ["Sennrich", "Rico", ""]]}, {"id": "2005.03684", "submitter": "Daniel Fried", "authors": "Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen\n  Clark, Aida Nematzadeh", "title": "Learning to Segment Actions from Observation and Narration", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a generative segmental model of task structure, guided by narration,\nto action segmentation in video. We focus on unsupervised and weakly-supervised\nsettings where no action labels are known during training. Despite its\nsimplicity, our model performs competitively with previous work on a dataset of\nnaturalistic instructional videos. Our model allows us to vary the sources of\nsupervision used in training, and we find that both task structure and\nnarrative language provide large benefits in segmentation quality.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:03:57 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:21:27 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Fried", "Daniel", ""], ["Alayrac", "Jean-Baptiste", ""], ["Blunsom", "Phil", ""], ["Dyer", "Chris", ""], ["Clark", "Stephen", ""], ["Nematzadeh", "Aida", ""]]}, {"id": "2005.03692", "submitter": "Jennifer Hu", "authors": "Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, Roger P. Levy", "title": "A Systematic Assessment of Syntactic Generalization in Neural Language\n  Models", "comments": "To appear in the Proceedings of the Association for Computational\n  Linguistics (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While state-of-the-art neural network models continue to achieve lower\nperplexity scores on language modeling benchmarks, it remains unknown whether\noptimizing for broad-coverage predictive performance leads to human-like\nsyntactic knowledge. Furthermore, existing work has not provided a clear\npicture about the model properties required to produce proper syntactic\ngeneralizations. We present a systematic evaluation of the syntactic knowledge\nof neural language models, testing 20 combinations of model types and data\nsizes on a set of 34 English-language syntactic test suites. We find\nsubstantial differences in syntactic generalization performance by model\narchitecture, with sequential models underperforming other architectures.\nFactorially manipulating model architecture and training dataset size (1M--40M\nwords), we find that variability in syntactic generalization performance is\nsubstantially greater by architecture than by dataset size for the corpora\ntested in our experiments. Our results also reveal a dissociation between\nperplexity and syntactic generalization performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:35:25 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 02:23:41 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Hu", "Jennifer", ""], ["Gauthier", "Jon", ""], ["Qian", "Peng", ""], ["Wilcox", "Ethan", ""], ["Levy", "Roger P.", ""]]}, {"id": "2005.03695", "submitter": "Erfan Ghadery", "authors": "Erfan Ghadery, Marie-Francine Moens", "title": "LIIR at SemEval-2020 Task 12: A Cross-Lingual Augmentation Approach for\n  Multilingual Offensive Language Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our system entitled `LIIR' for SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval 2).\nWe have participated in sub-task A for English, Danish, Greek, Arabic, and\nTurkish languages. We adapt and fine-tune the BERT and Multilingual Bert models\nmade available by Google AI for English and non-English languages respectively.\nFor the English language, we use a combination of two fine-tuned BERT models.\nFor other languages we propose a cross-lingual augmentation approach in order\nto enrich training data and we use Multilingual BERT to obtain sentence\nrepresentations. LIIR achieved rank 14/38, 18/47, 24/86, 24/54, and 25/40 in\nGreek, Turkish, English, Arabic, and Danish languages, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:45:48 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 11:55:25 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ghadery", "Erfan", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2005.03724", "submitter": "Yang Gao", "authors": "Yang Gao, Wei Zhao, Steffen Eger", "title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for\n  Multi-Document Summarization", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised multi-document summarization evaluation metrics, which\nrequire neither human-written reference summaries nor human annotations (e.g.\npreferences, ratings, etc.). We propose SUPERT, which rates the quality of a\nsummary by measuring its semantic similarity with a pseudo reference summary,\ni.e. selected salient sentences from the source documents, using contextualized\nembeddings and soft token alignment techniques. Compared to the\nstate-of-the-art unsupervised evaluation metrics, SUPERT correlates better with\nhuman ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a\nneural-based reinforcement learning summarizer, yielding favorable performance\ncompared to the state-of-the-art unsupervised summarizers. All source code is\navailable at https://github.com/yg211/acl20-ref-free-eval.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 19:54:24 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gao", "Yang", ""], ["Zhao", "Wei", ""], ["Eger", "Steffen", ""]]}, {"id": "2005.03754", "submitter": "Esin Durmus", "authors": "Esin Durmus and He He and Mona Diab", "title": "FEQA: A Question Answering Evaluation Framework for Faithfulness\n  Assessment in Abstractive Summarization", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.454", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural abstractive summarization models are prone to generate content\ninconsistent with the source document, i.e. unfaithful. Existing automatic\nmetrics do not capture such mistakes effectively. We tackle the problem of\nevaluating faithfulness of a generated summary given its source document. We\nfirst collected human annotations of faithfulness for outputs from numerous\nmodels on two datasets. We find that current models exhibit a trade-off between\nabstractiveness and faithfulness: outputs with less word overlap with the\nsource document are more likely to be unfaithful. Next, we propose an automatic\nquestion answering (QA) based metric for faithfulness, FEQA, which leverages\nrecent advances in reading comprehension. Given question-answer pairs generated\nfrom the summary, a QA model extracts answers from the document; non-matched\nanswers indicate unfaithful information in the summary. Among metrics based on\nword overlap, embedding similarity, and learned language understanding models,\nour QA-based metric has significantly higher correlation with human\nfaithfulness scores, especially on highly abstractive summaries.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:00:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Durmus", "Esin", ""], ["He", "He", ""], ["Diab", "Mona", ""]]}, {"id": "2005.03774", "submitter": "Tiago Pimentel", "authors": "Tiago Pimentel, Brian Roark, Ryan Cotterell", "title": "Phonotactic Complexity and its Trade-offs", "comments": "Published in TACL: https://doi.org/10.1162/tacl_a_00296", "journal-ref": "Transactions of the Association for Computational Linguistics,\n  Vol. 8, 1-18", "doi": "10.1162/tacl_a_00296", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for calculating a measure of phonotactic complexity---bits\nper phoneme---that permits a straightforward cross-linguistic comparison. When\ngiven a word, represented as a sequence of phonemic segments such as symbols in\nthe international phonetic alphabet, and a statistical model trained on a\nsample of word types from the language, we can approximately measure bits per\nphoneme using the negative log-probability of that word under the model. This\nsimple measure allows us to compare the entropy across languages, giving\ninsight into how complex a language's phonotactics are. Using a collection of\n1016 basic concept words across 106 languages, we demonstrate a very strong\nnegative correlation of -0.74 between bits per phoneme and the average length\nof words.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:36:59 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Pimentel", "Tiago", ""], ["Roark", "Brian", ""], ["Cotterell", "Ryan", ""]]}, {"id": "2005.03776", "submitter": "Yang Li", "authors": "Yang Li and Jiacong He and Xin Zhou and Yuan Zhang and Jason Baldridge", "title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "comments": "Annual Conference of the Association for Computational Linguistics\n  (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new problem: grounding natural language instructions to mobile\nuser interface actions, and create three new datasets for it. For full task\nevaluation, we create PIXELHELP, a corpus that pairs English instructions with\nactions performed by people on a mobile UI emulator. To scale training, we\ndecouple the language and action data by (a) annotating action phrase spans in\nHowTo instructions and (b) synthesizing grounded descriptions of actions for\nmobile user interfaces. We use a Transformer to extract action phrase tuples\nfrom long-range natural language instructions. A grounding Transformer then\ncontextually represents UI objects using both their content and screen position\nand connects them to object descriptions. Given a starting screen and\ninstruction, our model achieves 70.59% accuracy on predicting complete\nground-truth action sequences in PIXELHELP.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:41:40 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 02:11:56 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Li", "Yang", ""], ["He", "Jiacong", ""], ["Zhou", "Xin", ""], ["Zhang", "Yuan", ""], ["Baldridge", "Jason", ""]]}, {"id": "2005.03812", "submitter": "Martina Toshevska", "authors": "Martina Toshevska, Frosina Stojanovska and Jovan Kalajdjieski", "title": "Comparative Analysis of Word Embeddings for Capturing Word Similarities", "comments": "Part of the 6th International Conference on Natural Language\n  Processing (NATP 2020)", "journal-ref": "6th International Conference on Natural Language Processing (NATP\n  2020)", "doi": "10.5121/csit.2020.100402", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed language representation has become the most widely used technique\nfor language representation in various natural language processing tasks. Most\nof the natural language processing models that are based on deep learning\ntechniques use already pre-trained distributed word representations, commonly\ncalled word embeddings. Determining the most qualitative word embeddings is of\ncrucial importance for such models. However, selecting the appropriate word\nembeddings is a perplexing task since the projected embedding space is not\nintuitive to humans. In this paper, we explore different approaches for\ncreating distributed word representations. We perform an intrinsic evaluation\nof several state-of-the-art word embedding methods. Their performance on\ncapturing word similarities is analysed with existing benchmark datasets for\nword pairs similarities. The research in this paper conducts a correlation\nanalysis between ground truth word similarities and similarities obtained by\ndifferent word embedding methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 01:16:03 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Toshevska", "Martina", ""], ["Stojanovska", "Frosina", ""], ["Kalajdjieski", "Jovan", ""]]}, {"id": "2005.03846", "submitter": "Mingshuang Luo", "authors": "Mingshuang Luo, Shuang Yang, Xilin Chen, Zitao Liu, Shiguang Shan", "title": "Synchronous Bidirectional Learning for Multilingual Lip Reading", "comments": "13 pages,2 figures,5 tables; Accepted in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading has received increasing attention in recent years. This paper\nfocuses on the synergy of multilingual lip reading. There are about as many as\n7000 languages in the world, which implies that it is impractical to train\nseparate lip reading models with large-scale data for each language. Although\neach language has its own linguistic and pronunciation rules, the lip movements\nof all languages share similar patterns due to the common structures of human\norgans. Based on this idea, we try to explore the synergized learning of\nmultilingual lip reading in this paper, and further propose a synchronous\nbidirectional learning (SBL) framework for effective synergy of multilingual\nlip reading. We firstly introduce phonemes as our modeling units for the\nmultilingual setting here. Phonemes are more closely related with the lip\nmovements than the alphabet letters. At the same time, similar phonemes always\nlead to similar visual patterns no matter which type the target language is.\nThen, a novel SBL block is proposed to learn the rules for each language in a\nfill-in-the-blank way. Specifically, the model has to learn to infer the target\nunit given its bidirectional context, which could represent the composition\nrules of phonemes for each language. To make the learning process more targeted\nat each particular language, an extra task of predicting the language identity\nis introduced in the learning process. Finally, a thorough comparison on LRW\n(English) and LRW-1000 (Mandarin) is performed, which shows the promising\nbenefits from the synergized learning of different languages and also reports a\nnew state-of-the-art result on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 04:19:57 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 02:45:23 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 01:04:28 GMT"}, {"version": "v4", "created": "Fri, 14 Aug 2020 15:34:49 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Luo", "Mingshuang", ""], ["Yang", "Shuang", ""], ["Chen", "Xilin", ""], ["Liu", "Zitao", ""], ["Shan", "Shiguang", ""]]}, {"id": "2005.03848", "submitter": "Wu Xing", "authors": "Xing Wu, Yibing Liu, Xiangyang Zhou and Dianhai Yu", "title": "Distilling Knowledge from Pre-trained Language Models via Text Smoothing", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies compressing pre-trained language models, like BERT (Devlin\net al.,2019), via teacher-student knowledge distillation. Previous works\nusually force the student model to strictly mimic the smoothed labels predicted\nby the teacher BERT. As an alternative, we propose a new method for BERT\ndistillation, i.e., asking the teacher to generate smoothed word ids, rather\nthan labels, for teaching the student model in knowledge distillation. We call\nthis kind of methodTextSmoothing. Practically, we use the softmax prediction of\nthe Masked Language Model(MLM) in BERT to generate word distributions for given\ntexts and smooth those input texts using that predicted soft word ids. We\nassume that both the smoothed labels and the smoothed texts can implicitly\naugment the input corpus, while text smoothing is intuitively more efficient\nsince it can generate more instances in one neural network forward\nstep.Experimental results on GLUE and SQuAD demonstrate that our solution can\nachieve competitive results compared with existing BERT distillation methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 04:34:00 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wu", "Xing", ""], ["Liu", "Yibing", ""], ["Zhou", "Xiangyang", ""], ["Yu", "Dianhai", ""]]}, {"id": "2005.03867", "submitter": "Myunghun Jung", "authors": "Myunghun Jung, Youngmoon Jung, Jahyun Goo, and Hoirin Kim", "title": "Multi-Task Network for Noise-Robust Keyword Spotting and Speaker\n  Verification using CTC-based Soft VAD and Global Query Attention", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword spotting (KWS) and speaker verification (SV) have been studied\nindependently although it is known that acoustic and speaker domains are\ncomplementary. In this paper, we propose a multi-task network that performs KWS\nand SV simultaneously to fully utilize the interrelated domain information. The\nmulti-task network tightly combines sub-networks aiming at performance\nimprovement in challenging conditions such as noisy environments,\nopen-vocabulary KWS, and short-duration SV, by introducing novel techniques of\nconnectionist temporal classification (CTC)-based soft voice activity detection\n(VAD) and global query attention. Frame-level acoustic and speaker information\nis integrated with phonetically originated weights so that forms a word-level\nglobal representation. Then it is used for the aggregation of feature vectors\nto generate discriminative embeddings. Our proposed approach shows 4.06% and\n26.71% relative improvements in equal error rate (EER) compared to the\nbaselines for both tasks. We also present a visualization example and results\nof ablation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 05:58:46 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 11:16:56 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 09:56:34 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 07:23:57 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Jung", "Myunghun", ""], ["Jung", "Youngmoon", ""], ["Goo", "Jahyun", ""], ["Kim", "Hoirin", ""]]}, {"id": "2005.03909", "submitter": "Bertie Vidgen Dr", "authors": "Bertie Vidgen, Austin Botelho, David Broniatowski, Ella Guest, Matthew\n  Hall, Helen Margetts, Rebekah Tromble, Zeerak Waseem, Scott Hale", "title": "Detecting East Asian Prejudice on Social Media", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 has transformed societies across the world as\ngovernments tackle the health, economic and social costs of the pandemic. It\nhas also raised concerns about the spread of hateful language and prejudice\nonline, especially hostility directed against East Asia. In this paper we\nreport on the creation of a classifier that detects and categorizes social\nmedia posts from Twitter into four classes: Hostility against East Asia,\nCriticism of East Asia, Meta-discussions of East Asian prejudice and a neutral\nclass. The classifier achieves an F1 score of 0.83 across all four classes. We\nprovide our final model (coded in Python), as well as a new 20,000 tweet\ntraining dataset used to make the classifier, two analyses of hashtags\nassociated with East Asian prejudice and the annotation codebook. The\nclassifier can be implemented by other researchers, assisting with both online\ncontent moderation processes and further research into the dynamics, prevalence\nand impact of East Asian prejudice online during this global pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 08:53:47 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Vidgen", "Bertie", ""], ["Botelho", "Austin", ""], ["Broniatowski", "David", ""], ["Guest", "Ella", ""], ["Hall", "Matthew", ""], ["Margetts", "Helen", ""], ["Tromble", "Rebekah", ""], ["Waseem", "Zeerak", ""], ["Hale", "Scott", ""]]}, {"id": "2005.03923", "submitter": "Puhai Yang", "authors": "Puhai Yang, Heyan Huang, and Xian-Ling Mao", "title": "Context-Sensitive Generation Network for Handing Unknown Slot Values in\n  Dialogue State Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key component in a dialogue system, dialogue state tracking plays an\nimportant role. It is very important for dialogue state tracking to deal with\nthe problem of unknown slot values. As far as we known, almost all existing\napproaches depend on pointer network to solve the unknown slot value problem.\nThese pointer network-based methods usually have a hidden assumption that there\nis at most one out-of-vocabulary word in an unknown slot value because of the\ncharacter of a pointer network. However, often, there are multiple\nout-of-vocabulary words in an unknown slot value, and it makes the existing\nmethods perform bad. To tackle the problem, in this paper, we propose a novel\nContext-Sensitive Generation network (CSG) which can facilitate the\nrepresentation of out-of-vocabulary words when generating the unknown slot\nvalue. Extensive experiments show that our proposed method performs better than\nthe state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:22:33 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 05:49:59 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 09:31:38 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yang", "Puhai", ""], ["Huang", "Heyan", ""], ["Mao", "Xian-Ling", ""]]}, {"id": "2005.03925", "submitter": "Meng Zhang", "authors": "Meng Zhang, Xin Jiang, Yang Liu, Qun Liu", "title": "Learning to Detect Unacceptable Machine Translations for Downstream\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of machine translation has progressed tremendously in recent years.\nEven though the translation quality has improved significantly, current systems\nare still unable to produce uniformly acceptable machine translations for the\nvariety of possible use cases. In this work, we put machine translation in a\ncross-lingual pipeline and introduce downstream tasks to define task-specific\nacceptability of machine translations. This allows us to leverage parallel data\nto automatically generate acceptability annotations on a large scale, which in\nturn help to learn acceptability detectors for the downstream tasks. We conduct\nexperiments to demonstrate the effectiveness of our framework for a range of\ndownstream tasks and translation models.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:37:19 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhang", "Meng", ""], ["Jiang", "Xin", ""], ["Liu", "Yang", ""], ["Liu", "Qun", ""]]}, {"id": "2005.03954", "submitter": "Zheng-Yu Niu", "authors": "Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu", "title": "Towards Conversational Recommendation over Multi-Type Dialogs", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task of conversational recommendation over multi-type\ndialogs, where the bots can proactively and naturally lead a conversation from\na non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into\naccount user's interests and feedback. To facilitate the study of this task, we\ncreate a human-to-human Chinese dialog dataset \\emph{DuRecDial} (about 10k\ndialogs, 156k utterances), which contains multiple sequential dialogs for every\npair of a recommendation seeker (user) and a recommender (bot). In each dialog,\nthe recommender proactively leads a multi-type dialog to approach\nrecommendation targets and then makes multiple recommendations with rich\ninteraction behavior. This dataset allows us to systematically investigate\ndifferent parts of the overall problem, e.g., how to naturally lead a dialog,\nhow to interact with users for recommendation. Finally we establish baseline\nresults on DuRecDial for future studies. Dataset and codes are publicly\navailable at\nhttps://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2020-DuRecDial.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 11:01:21 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 03:06:47 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 08:54:45 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Liu", "Zeming", ""], ["Wang", "Haifeng", ""], ["Niu", "Zheng-Yu", ""], ["Wu", "Hua", ""], ["Che", "Wanxiang", ""], ["Liu", "Ting", ""]]}, {"id": "2005.03959", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Jielei Zhang, Liang Zhang, Jiebo Luo, Cong Yao", "title": "On Vocabulary Reliance in Scene Text Recognition", "comments": "CVPR'20", "journal-ref": "EEE/CVF Conferences on Computer Vision and Pattern Recognition\n  (CVPR), Seattle, WA, June 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pursuit of high performance on public benchmarks has been the driving\nforce for research in scene text recognition, and notable progress has been\nachieved. However, a close investigation reveals a startling fact that the\nstate-of-the-art methods perform well on images with words within vocabulary\nbut generalize poorly to images with words outside vocabulary. We call this\nphenomenon \"vocabulary reliance\". In this paper, we establish an analytical\nframework to conduct an in-depth study on the problem of vocabulary reliance in\nscene text recognition. Key findings include: (1) Vocabulary reliance is\nubiquitous, i.e., all existing algorithms more or less exhibit such\ncharacteristic; (2) Attention-based decoders prove weak in generalizing to\nwords outside vocabulary and segmentation-based decoders perform well in\nutilizing visual features; (3) Context modeling is highly coupled with the\nprediction layers. These findings provide new insights and can benefit future\nresearch in scene text recognition. Furthermore, we propose a simple yet\neffective mutual learning strategy to allow models of two families\n(attention-based and segmentation-based) to learn collaboratively. This remedy\nalleviates the problem of vocabulary reliance and improves the overall scene\ntext recognition performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 11:16:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Zhang", "Jielei", ""], ["Zhang", "Liang", ""], ["Luo", "Jiebo", ""], ["Yao", "Cong", ""]]}, {"id": "2005.03975", "submitter": "Yan Xu", "authors": "Dan Su, Yan Xu, Tiezheng Yu, Farhad Bin Siddique, Elham J. Barezi,\n  Pascale Fung", "title": "CAiRE-COVID: A Question Answering and Query-focused Multi-Document\n  Summarization System for COVID-19 Scholarly Information Management", "comments": "Accepted EMNLP2020 NLP-COVID Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CAiRE-COVID, a real-time question answering (QA) and\nmulti-document summarization system, which won one of the 10 tasks in the\nKaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our\nsystem aims to tackle the recent challenge of mining the numerous scientific\narticles being published on COVID-19 by answering high priority questions from\nthe community and summarizing salient question-related information. It combines\ninformation extraction with state-of-the-art QA and query-focused\nmulti-document summarization techniques, selecting and highlighting evidence\nsnippets from existing literature given a query. We also propose query-focused\nabstractive and extractive multi-document summarization methods, to provide\nmore relevant information related to the question. We further conduct\nquantitative experiments that show consistent improvements on various metrics\nfor each module. We have launched our website CAiRE-COVID for broader use by\nthe medical community, and have open-sourced the code for our system, to\nbootstrap further study by other researches.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:07:27 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 02:47:06 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 11:30:49 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Su", "Dan", ""], ["Xu", "Yan", ""], ["Yu", "Tiezheng", ""], ["Siddique", "Farhad Bin", ""], ["Barezi", "Elham J.", ""], ["Fung", "Pascale", ""]]}, {"id": "2005.03993", "submitter": "Karthik Gopalakrishnan", "authors": "Karthik Gopalakrishnan, Fathi M.Salem", "title": "Sentiment Analysis Using Simplified Long Short-term Memory Recurrent\n  Neural Networks", "comments": "6 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTM or Long Short Term Memory Networks is a specific type of Recurrent\nNeural Network (RNN) that is very effective in dealing with long sequence data\nand learning long term dependencies. In this work, we perform sentiment\nanalysis on a GOP Debate Twitter dataset. To speed up training and reduce the\ncomputational cost and time, six different parameter reduced slim versions of\nthe LSTM model (slim LSTM) are proposed. We evaluate two of these models on the\ndataset. The performance of these two LSTM models along with the standard LSTM\nmodel is compared. The effect of Bidirectional LSTM Layers is also studied. The\nwork also consists of a study to choose the best architecture, apart from\nestablishing the best set of hyper parameters for different LSTM Models.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 12:50:10 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gopalakrishnan", "Karthik", ""], ["Salem", "Fathi M.", ""]]}, {"id": "2005.04044", "submitter": "Chenhui Lv", "authors": "Chenhui Lv and Qian Lu and Xiang Zhang", "title": "Literature Triage on Genomic Variation Publications by\n  Knowledge-enhanced Multi-channel CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: To investigate the correlation between genomic variation and\ncertain diseases or phenotypes, the fundamental task is to screen out the\nconcerning publications from massive literature, which is called literature\ntriage. Some knowledge bases, including UniProtKB/Swiss-Prot and NHGRI-EBI GWAS\nCatalog are created for collecting concerning publications. These publications\nare manually curated by experts, which is time-consuming. Moreover, the manual\ncuration of information from literature is not scalable due to the rapidly\nincreasing amount of publications. In order to cut down the cost of literature\ntriage, machine-learning models were adopted to automatically identify\nbiomedical publications. Methods: Comparing to previous studies utilizing\nmachine-learning models for literature triage, we adopt a multi-channel\nconvolutional network to utilize rich textual information and meanwhile bridge\nthe semantic gaps from different corpora. In addition, knowledge embeddings\nlearned from UMLS is also used to provide extra medical knowledge beyond\ntextual features in the process of triage. Results: We demonstrate that our\nmodel outperforms the state-of-the-art models over 5 datasets with the help of\nknowledge embedding and multiple channels. Our model improves the accuracy of\nbiomedical literature triage results. Conclusions: Multiple channels and\nknowledge embeddings enhance the performance of the CNN model in the task of\nbiomedical literature triage. Keywords: Literature Triage; Knowledge Embedding;\nMulti-channel Convolutional Network\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:47:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Lv", "Chenhui", ""], ["Lu", "Qian", ""], ["Zhang", "Xiang", ""]]}, {"id": "2005.04114", "submitter": "Da Yin", "authors": "Da Yin, Tao Meng, Kai-Wei Chang", "title": "SentiBERT: A Transferable Transformer-Based Architecture for\n  Compositional Sentiment Semantics", "comments": "ACL-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SentiBERT, a variant of BERT that effectively captures\ncompositional sentiment semantics. The model incorporates contextualized\nrepresentation with binary constituency parse tree to capture semantic\ncomposition. Comprehensive experiments demonstrate that SentiBERT achieves\ncompetitive performance on phrase-level sentiment classification. We further\ndemonstrate that the sentiment composition learned from the phrase-level\nannotations on SST can be transferred to other sentiment analysis tasks as well\nas related tasks, such as emotion classification tasks. Moreover, we conduct\nablation studies and design visualization methods to understand SentiBERT. We\nshow that SentiBERT is better than baseline approaches in capturing negation\nand the contrastive relation and model the compositional sentiment semantics.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:40:17 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 09:21:31 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 01:12:14 GMT"}, {"version": "v4", "created": "Thu, 21 May 2020 04:37:43 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Yin", "Da", ""], ["Meng", "Tao", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2005.04118", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh", "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList", "comments": null, "journal-ref": "Association for Computational Linguistics (ACL), 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:48:31 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Wu", "Tongshuang", ""], ["Guestrin", "Carlos", ""], ["Singh", "Sameer", ""]]}, {"id": "2005.04147", "submitter": "Konstantinos Meichanetzidis", "authors": "Konstantinos Meichanetzidis, Stefano Gogioso, Giovanni De Felice,\n  Nicol\\`o Chiappori, Alexis Toumi, Bob Coecke", "title": "Quantum Natural Language Processing on Near-Term Quantum Computers", "comments": "13 pages, submitted to Quantum Physics and Logic (QPL) 2020. This\n  work was originally commissioned by Cambridge Quantum Computing (CQC) and was\n  carried out independently by the CQC team and the Hashberg team", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we describe a full-stack pipeline for natural language\nprocessing on near-term quantum computers, aka QNLP. The language modelling\nframework we employ is that of compositional distributional semantics\n(DisCoCat), which extends and complements the compositional structure of\npregroup grammars. Within this model, the grammatical reduction of a sentence\nis interpreted as a diagram, encoding a specific interaction of words according\nto the grammar. It is this interaction which, together with a specific choice\nof word embedding, realises the meaning (or \"semantics\") of a sentence.\nBuilding on the formal quantum-like nature of such interactions, we present a\nmethod for mapping DisCoCat diagrams to quantum circuits. Our methodology is\ncompatible both with NISQ devices and with established Quantum Machine Learning\ntechniques, paving the way to near-term applications of quantum technology to\nnatural language processing.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 16:42:54 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Meichanetzidis", "Konstantinos", ""], ["Gogioso", "Stefano", ""], ["De Felice", "Giovanni", ""], ["Chiappori", "Nicol\u00f2", ""], ["Toumi", "Alexis", ""], ["Coecke", "Bob", ""]]}, {"id": "2005.04177", "submitter": "Jay DeYoung", "authors": "Jay DeYoung, Eric Lehman, Ben Nye, Iain J. Marshall, Byron C. Wallace", "title": "Evidence Inference 2.0: More Data, Better Models", "comments": "Accepted as workshop paper into BioNLP Updated results from SciBERT\n  to Biomed RoBERTa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do we most effectively treat a disease or condition? Ideally, we could\nconsult a database of evidence gleaned from clinical trials to answer such\nquestions. Unfortunately, no such database exists; clinical trial results are\ninstead disseminated primarily via lengthy natural language articles. Perusing\nall such articles would be prohibitively time-consuming for healthcare\npractitioners; they instead tend to depend on manually compiled systematic\nreviews of medical literature to inform care.\n  NLP may speed this process up, and eventually facilitate immediate consult of\npublished evidence. The Evidence Inference dataset was recently released to\nfacilitate research toward this end. This task entails inferring the\ncomparative performance of two treatments, with respect to a given outcome,\nfrom a particular article (describing a clinical trial) and identifying\nsupporting evidence. For instance: Does this article report that chemotherapy\nperformed better than surgery for five-year survival rates of operable cancers?\nIn this paper, we collect additional annotations to expand the Evidence\nInference dataset by 25\\%, provide stronger baseline models, systematically\ninspect the errors that these make, and probe dataset quality. We also release\nan abstract only (as opposed to full-texts) version of the task for rapid model\nprototyping. The updated corpus, documentation, and code for new baselines and\nevaluations are available at http://evidence-inference.ebm-nlp.com/.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:16:35 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:55:33 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["DeYoung", "Jay", ""], ["Lehman", "Eric", ""], ["Nye", "Ben", ""], ["Marshall", "Iain J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.04232", "submitter": "Keyon Vafa", "authors": "Keyon Vafa, Suresh Naidu, David M. Blei", "title": "Text-Based Ideal Points", "comments": "Appeared in Proceedings of the 2020 Conference of the Association for\n  Computational Linguistics (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideal point models analyze lawmakers' votes to quantify their political\npositions, or ideal points. But votes are not the only way to express a\npolitical position. Lawmakers also give speeches, release press statements, and\npost tweets. In this paper, we introduce the text-based ideal point model\n(TBIP), an unsupervised probabilistic topic model that analyzes texts to\nquantify the political positions of its authors. We demonstrate the TBIP with\ntwo types of politicized text data: U.S. Senate speeches and senator tweets.\nThough the model does not analyze their votes or political affiliations, the\nTBIP separates lawmakers by party, learns interpretable politicized topics, and\ninfers ideal points close to the classical vote-based ideal points. One benefit\nof analyzing texts, as opposed to votes, is that the TBIP can estimate ideal\npoints of anyone who authors political texts, including non-voting actors. To\nthis end, we use it to study tweets from the 2020 Democratic presidential\ncandidates. Using only the texts of their tweets, it identifies them along an\ninterpretable progressive-to-moderate spectrum.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 21:16:42 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 00:16:52 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Vafa", "Keyon", ""], ["Naidu", "Suresh", ""], ["Blei", "David M.", ""]]}, {"id": "2005.04245", "submitter": "Justine Zhang", "authors": "Justine Zhang, Cristian Danescu-Niculescu-Mizil", "title": "Balancing Objectives in Counseling Conversations: Advancing Forwards or\n  Looking Backwards", "comments": "14 pages, 6 figures, code available through the Cornell\n  Conversational Analysis Toolkit (https://convokit.cornell.edu/). in\n  Proceedings of ACL, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout a conversation, participants make choices that can orient the flow\nof the interaction. Such choices are particularly salient in the consequential\ndomain of crisis counseling, where a difficulty for counselors is balancing\nbetween two key objectives: advancing the conversation towards a resolution,\nand empathetically addressing the crisis situation.\n  In this work, we develop an unsupervised methodology to quantify how\ncounselors manage this balance. Our main intuition is that if an utterance can\nonly receive a narrow range of appropriate replies, then its likely aim is to\nadvance the conversation forwards, towards a target within that range.\nLikewise, an utterance that can only appropriately follow a narrow range of\npossible utterances is likely aimed backwards at addressing a specific\nsituation within that range. By applying this intuition, we can map each\nutterance to a continuous orientation axis that captures the degree to which it\nis intended to direct the flow of the conversation forwards or backwards.\n  This unsupervised method allows us to characterize counselor behaviors in a\nlarge dataset of crisis counseling conversations, where we show that known\ncounseling strategies intuitively align with this axis. We also illustrate how\nour measure can be indicative of a conversation's progress, as well as its\neffectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 18:00:27 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhang", "Justine", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "2005.04246", "submitter": "Jonathan P. Chang", "authors": "Jonathan P. Chang, Caleb Chiam, Liye Fu, Andrew Z. Wang, Justine\n  Zhang, Cristian Danescu-Niculescu-Mizil", "title": "ConvoKit: A Toolkit for the Analysis of Conversations", "comments": "Proceedings of SIGDIAL 2020 (System Demos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and functionality of ConvoKit, an open-source\ntoolkit for analyzing conversations and the social interactions embedded\nwithin. ConvoKit provides an unified framework for representing and\nmanipulating conversational data, as well as a large and diverse collection of\nconversational datasets. By providing an intuitive interface for exploring and\ninteracting with conversational data, this toolkit lowers the technical\nbarriers for the broad adoption of computational methods for conversational\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 18:00:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Chang", "Jonathan P.", ""], ["Chiam", "Caleb", ""], ["Fu", "Liye", ""], ["Wang", "Andrew Z.", ""], ["Zhang", "Justine", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "2005.04277", "submitter": "Peng Su", "authors": "Peng Su and K. Vijay-Shanker", "title": "Adversarial Learning for Supervised and Semi-supervised Relation\n  Extraction in Biomedical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is a technique of improving model performance by\ninvolving adversarial examples in the training process. In this paper, we\ninvestigate adversarial training with multiple adversarial examples to benefit\nthe relation extraction task. We also apply adversarial training technique in\nsemi-supervised scenarios to utilize unlabeled data. The evaluation results on\nprotein-protein interaction and protein subcellular localization task\nillustrate adversarial training provides improvement on the supervised model,\nand is also effective on involving unlabeled data in the semi-supervised\ntraining case. In addition, our method achieves state-of-the-art performance on\ntwo benchmarking datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 20:19:26 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 15:21:50 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Su", "Peng", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "2005.04288", "submitter": "Li Fu", "authors": "Li Fu, Xiaoxiao Li, Libo Zi", "title": "Incremental Learning for End-to-End Automatic Speech Recognition", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new incremental learning for end-to-end Automatic Speech\nRecognition (ASR) to extend the model's capacity on a new task while retaining\nthe performance on previous ones. The proposed method is effective without\naccessing to the old dataset to address the issues of high retraining cost and\nunavailable old dataset. To achieve this, both attention distillation and\nknowledge distillation are applied to preserve the ability of the old model\nduring the progressive learning. With an ASR model pre-trained on 12,000h\nMandarin speech, we test our proposed method on 300h new scenario task and 1h\nnew named entities task. Experiments show that our method yields 3.25% and\n0.88% absolute Character Error Rate (CER) reduction on the new scenario, when\ncompared with the pre-trained model and the full-data retraining baseline,\nrespectively. It even yields a surprising 0.37% absolute CER reduction on the\nnew scenario than the fine-tuning. For the new named entities task, our method\nsignificantly improves the accuracy compared with the pre-trained model, i.e.\n16.95% absolute CER reduction. For both of the new task adaptions, the new\nmodels still maintain a same accuracy with the retraining baseline on the old\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:18:08 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 03:39:18 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Fu", "Li", ""], ["Li", "Xiaoxiao", ""], ["Zi", "Libo", ""]]}, {"id": "2005.04304", "submitter": "Ben Zhou", "authors": "Ben Zhou and Qiang Ning and Daniel Khashabi and Dan Roth", "title": "Temporal Common Sense Acquisition with Minimal Supervision", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal common sense (e.g., duration and frequency of events) is crucial for\nunderstanding natural language. However, its acquisition is challenging, partly\nbecause such information is often not expressed explicitly in text, and human\nannotation on such concepts is costly. This work proposes a novel sequence\nmodeling approach that exploits explicit and implicit mentions of temporal\ncommon sense, extracted from a large corpus, to build TACOLM, a temporal common\nsense language model. Our method is shown to give quality predictions of\nvarious dimensions of temporal common sense (on UDST and a newly collected\ndataset from RealNews). It also produces representations of events for relevant\ntasks such as duration comparison, parent-child relations, event coreference\nand temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the\nstandard BERT. Thus, it will be an important component of temporal NLP.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 22:20:16 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhou", "Ben", ""], ["Ning", "Qiang", ""], ["Khashabi", "Daniel", ""], ["Roth", "Dan", ""]]}, {"id": "2005.04315", "submitter": "Emily Goodwin", "authors": "Emily Goodwin and Koustuv Sinha and Timothy J. O'Donnell", "title": "Probing Linguistic Systematicity", "comments": "To appear at ACL2020, 9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been much interest in the question of whether deep\nnatural language understanding models exhibit systematicity; generalizing such\nthat units like words make consistent contributions to the meaning of the\nsentences in which they appear. There is accumulating evidence that neural\nmodels often generalize non-systematically. We examined the notion of\nsystematicity from a linguistic perspective, defining a set of probes and a set\nof metrics to measure systematic behaviour. We also identified ways in which\nnetwork architectures can generalize non-systematically, and discuss why such\nforms of generalization may be unsatisfying. As a case study, we performed a\nseries of experiments in the setting of natural language inference (NLI),\ndemonstrating that some NLU systems achieve high overall performance despite\nbeing non-systematic.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 23:31:31 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 04:18:42 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Goodwin", "Emily", ""], ["Sinha", "Koustuv", ""], ["O'Donnell", "Timothy J.", ""]]}, {"id": "2005.04322", "submitter": "Gustavo Aguilar", "authors": "Gustavo Aguilar, Sudipta Kar, and Thamar Solorio", "title": "LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation", "comments": "Accepted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in NLP research have raised an interest in linguistic\ncode-switching (CS); modern approaches have been proposed to solve a wide range\nof NLP tasks on multiple language pairs. Unfortunately, these proposed methods\nare hardly generalizable to different code-switched languages. In addition, it\nis unclear whether a model architecture is applicable for a different task\nwhile still being compatible with the code-switching setting. This is mainly\nbecause of the lack of a centralized benchmark and the sparse corpora that\nresearchers employ based on their specific needs and interests. To facilitate\nresearch in this direction, we propose a centralized benchmark for Linguistic\nCode-switching Evaluation (LinCE) that combines ten corpora covering four\ndifferent code-switched language pairs (i.e., Spanish-English, Nepali-English,\nHindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks\n(i.e., language identification, named entity recognition, part-of-speech\ntagging, and sentiment analysis). As part of the benchmark centralization\neffort, we provide an online platform at ritual.uh.edu/lince, where researchers\ncan submit their results while comparing with others in real-time. In addition,\nwe provide the scores of different popular models, including LSTM, ELMo, and\nmultilingual BERT so that the NLP community can compare against\nstate-of-the-art systems. LinCE is a continuous effort, and we will expand it\nwith more low-resource languages and tasks.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 00:00:08 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Aguilar", "Gustavo", ""], ["Kar", "Sudipta", ""], ["Solorio", "Thamar", ""]]}, {"id": "2005.04330", "submitter": "Paul Tupper", "authors": "Simone Brugiapaglia, Matthew Liu, Paul Tupper", "title": "Generalizing Outside the Training Set: When Can Neural Networks Learn\n  Identity Effects?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in language and other areas of cognition, whether two components of an\nobject are identical or not determine whether it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na simple framework in which we can rigorously prove that algorithms satisfying\nsimple criteria cannot make the correct inference. We then show that a broad\nclass of algorithms including deep neural networks with standard architecture\nand training with backpropagation satisfy our criteria, dependent on the\nencoding of inputs. Finally, we demonstrate our theory with computational\nexperiments in which we explore the effect of different input encodings on the\nability of algorithms to generalize to novel inputs.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 01:08:07 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Brugiapaglia", "Simone", ""], ["Liu", "Matthew", ""], ["Tupper", "Paul", ""]]}, {"id": "2005.04346", "submitter": "Hui Su", "authors": "Hui Su, Xiaoyu Shen, Sanqiang Zhao, Xiao Zhou, Pengwei Hu, Randy\n  Zhong, Cheng Niu and Jie Zhou", "title": "Diversifying Dialogue Generation with Non-Conversational Text", "comments": "Accepted to ACL 2020 (long)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based sequence-to-sequence (seq2seq) models strongly suffer\nfrom the low-diversity problem when it comes to open-domain dialogue\ngeneration. As bland and generic utterances usually dominate the frequency\ndistribution in our daily chitchat, avoiding them to generate more interesting\nresponses requires complex data filtering, sampling techniques or modifying the\ntraining objective. In this paper, we propose a new perspective to diversify\ndialogue generation by leveraging non-conversational text. Compared with\nbilateral conversations, non-conversational text are easier to obtain, more\ndiverse and cover a much broader range of topics. We collect a large-scale\nnon-conversational corpus from multi sources including forum comments, idioms\nand book snippets. We further present a training paradigm to effectively\nincorporate these text via iterative back translation. The resulting model is\ntested on two conversational datasets and is shown to produce significantly\nmore diverse responses without sacrificing the relevance with context.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 02:16:05 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 08:11:35 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Su", "Hui", ""], ["Shen", "Xiaoyu", ""], ["Zhao", "Sanqiang", ""], ["Zhou", "Xiao", ""], ["Hu", "Pengwei", ""], ["Zhong", "Randy", ""], ["Niu", "Cheng", ""], ["Zhou", "Jie", ""]]}, {"id": "2005.04364", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty, Min-Yen Kan, Richard Socher", "title": "It's Morphin' Time! Combating Linguistic Discrimination with\n  Inflectional Perturbations", "comments": "To appear in the Proceedings of the 58th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2020)", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.263", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training on only perfect Standard English corpora predisposes pre-trained\nneural networks to discriminate against minorities from non-standard linguistic\nbackgrounds (e.g., African American Vernacular English, Colloquial Singapore\nEnglish, etc.). We perturb the inflectional morphology of words to craft\nplausible and semantically similar adversarial examples that expose these\nbiases in popular NLP models, e.g., BERT and Transformer, and show that\nadversarially fine-tuning them for a single epoch significantly improves\nrobustness without sacrificing performance on clean data.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 04:01:43 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""], ["Kan", "Min-Yen", ""], ["Socher", "Richard", ""]]}, {"id": "2005.04379", "submitter": "Xinting Huang", "authors": "Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang", "title": "Semi-Supervised Dialogue Policy Learning via Stochastic Reward\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue policy optimization often obtains feedback until task completion in\ntask-oriented dialogue systems. This is insufficient for training intermediate\ndialogue turns since supervision signals (or rewards) are only provided at the\nend of dialogues. To address this issue, reward learning has been introduced to\nlearn from state-action pairs of an optimal policy to provide turn-by-turn\nrewards. This approach requires complete state-action annotations of\nhuman-to-human dialogues (i.e., expert demonstrations), which is labor\nintensive. To overcome this limitation, we propose a novel reward learning\napproach for semi-supervised policy learning. The proposed approach learns a\ndynamics model as the reward function which models dialogue progress (i.e.,\nstate-action sequences) based on expert demonstrations, either with or without\nannotations. The dynamics model computes rewards by predicting whether the\ndialogue progress is consistent with expert demonstrations. We further propose\nto learn action embeddings for a better generalization of the reward function.\nThe proposed approach outperforms competitive policy learning baselines on\nMultiWOZ, a benchmark multi-domain dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 06:28:44 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Huang", "Xinting", ""], ["Qi", "Jianzhong", ""], ["Sun", "Yu", ""], ["Zhang", "Rui", ""]]}, {"id": "2005.04396", "submitter": "Junheng Huang", "authors": "Junheng Huang, Lu Pan, Kang Xu, Weihua Peng, Fayuan Li", "title": "Generating Pertinent and Diversified Comments with Topic-aware\n  Pointer-Generator Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment generation, a new and challenging task in Natural Language Generation\n(NLG), attracts a lot of attention in recent years. However, comments generated\nby previous work tend to lack pertinence and diversity. In this paper, we\npropose a novel generation model based on Topic-aware Pointer-Generator\nNetworks (TPGN), which can utilize the topic information hidden in the articles\nto guide the generation of pertinent and diversified comments. Firstly, we\ndesign a keyword-level and topic-level encoder attention mechanism to capture\ntopic information in the articles. Next, we integrate the topic information\ninto pointer-generator networks to guide comment generation. Experiments on a\nlarge scale of comment generation dataset show that our model produces the\nvaluable comments and outperforms competitive baseline models significantly.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:04:09 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Huang", "Junheng", ""], ["Pan", "Lu", ""], ["Xu", "Kang", ""], ["Peng", "Weihua", ""], ["Li", "Fayuan", ""]]}, {"id": "2005.04418", "submitter": "Dor Ringel Mr", "authors": "Dor Ringel, Rotem Dror, and Roi Reichart", "title": "The Structured Weighted Violations MIRA", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Structured Weighted Violation MIRA (SWVM), a new structured\nprediction algorithm that is based on an hybridization between MIRA (Crammer\nand Singer, 2003) and the structured weighted violations perceptron (SWVP)\n(Dror and Reichart, 2016). We demonstrate that the concepts developed in (Dror\nand Reichart, 2016) combined with a powerful structured prediction algorithm\ncan improve performance on sequence labeling tasks. In experiments with\nsyntactic chunking and named entity recognition (NER), the new algorithm\nsubstantially outperforms the original MIRA as well as the original structured\nperceptron and SWVP. Our code is available at\nhttps://github.com/dorringel/SWVM.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 11:02:46 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ringel", "Dor", ""], ["Dror", "Rotem", ""], ["Reichart", "Roi", ""]]}, {"id": "2005.04470", "submitter": "Ji-Ung Lee", "authors": "Ji-Ung Lee, Christian M. Meyer, Iryna Gurevych", "title": "Empowering Active Learning to Jointly Optimize System and User Demands", "comments": "To appear as a long paper in Proceedings of the 58th Annual Meeting\n  of the Association for Computational Linguistics (ACL 2020). Download our\n  code and simulated user models at github:\n  https://github.com/UKPLab/acl2020-empowering-active-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to active learning maximize the system performance by\nsampling unlabeled instances for annotation that yield the most efficient\ntraining. However, when active learning is integrated with an end-user\napplication, this can lead to frustration for participating users, as they\nspend time labeling instances that they would not otherwise be interested in\nreading. In this paper, we propose a new active learning approach that jointly\noptimizes the seemingly counteracting objectives of the active learning system\n(training efficiently) and the user (receiving useful instances). We study our\napproach in an educational application, which particularly benefits from this\ntechnique as the system needs to rapidly learn to predict the appropriateness\nof an exercise to a particular user, while the users should receive only\nexercises that match their skills. We evaluate multiple learning strategies and\nuser types with data from real users and find that our joint approach better\nsatisfies both objectives when alternative methods lead to many unsuitable\nexercises for end users.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 16:02:52 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 00:45:11 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Lee", "Ji-Ung", ""], ["Meyer", "Christian M.", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2005.04511", "submitter": "Ethan Chi", "authors": "Ethan A. Chi, John Hewitt, Christopher D. Manning", "title": "Finding Universal Grammatical Relations in Multilingual BERT", "comments": "To appear in ACL 2020; Farsi typo corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has found evidence that Multilingual BERT (mBERT), a\ntransformer-based multilingual masked language model, is capable of zero-shot\ncross-lingual transfer, suggesting that some aspects of its representations are\nshared cross-lingually. To better understand this overlap, we extend recent\nwork on finding syntactic trees in neural networks' internal representations to\nthe multilingual setting. We show that subspaces of mBERT representations\nrecover syntactic tree distances in languages other than English, and that\nthese subspaces are approximately shared across languages. Motivated by these\nresults, we present an unsupervised analysis method that provides evidence\nmBERT learns representations of syntactic dependency labels, in the form of\nclusters which largely agree with the Universal Dependencies taxonomy. This\nevidence suggests that even without explicit supervision, multilingual masked\nlanguage models learn certain linguistic universals.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 20:46:02 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 08:32:18 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Chi", "Ethan A.", ""], ["Hewitt", "John", ""], ["Manning", "Christopher D.", ""]]}, {"id": "2005.04518", "submitter": "Preslav Nakov", "authors": "Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov,\n  Ahmed Ali, James Glass, Preslav Nakov", "title": "What Was Written vs. Who Read It: News Media Profiling Using Text\n  Analysis and Social Media Context", "comments": "Factuality of reporting, fact-checking, political ideology, media\n  bias, disinformation, propaganda, social media, news media", "journal-ref": "ACL-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the political bias and the factuality of reporting of entire news\noutlets are critical elements of media profiling, which is an understudied but\nan increasingly important research direction. The present level of\nproliferation of fake, biased, and propagandistic content online, has made it\nimpossible to fact-check every single suspicious claim, either manually or\nautomatically. Alternatively, we can profile entire news outlets and look for\nthose that are likely to publish fake or biased content. This approach makes it\npossible to detect likely \"fake news\" the moment they are published, by simply\nchecking the reliability of their source. From a practical perspective,\npolitical bias and factuality of reporting have a linguistic aspect but also a\nsocial context. Here, we study the impact of both, namely (i) what was written\n(i.e., what was published by the target medium, and how it describes itself on\nTwitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium\non Facebook, Twitter, and YouTube). We further study (iii) what was written\nabout the target medium on Wikipedia. The evaluation results show that what was\nwritten matters most, and that putting all information sources together yields\nhuge improvements over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 22:00:08 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Baly", "Ramy", ""], ["Karadzhov", "Georgi", ""], ["An", "Jisun", ""], ["Kwak", "Haewoon", ""], ["Dinkov", "Yoan", ""], ["Ali", "Ahmed", ""], ["Glass", "James", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.04534", "submitter": "Kumar Ravi", "authors": "Vishal Vyas, Kumar Ravi, Vadlamani Ravi, V.Uma, Srirangaraj Setlur,\n  Venu Govindaraju", "title": "Article citation study: Context enhanced citation sentiment detection", "comments": "39 pages, 12 Tables, 5 Figures, Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation sentimet analysis is one of the little studied tasks for\nscientometric analysis. For citation analysis, we developed eight datasets\ncomprising citation sentences, which are manually annotated by us into three\nsentiment polarities viz. positive, negative, and neutral. Among eight\ndatasets, three were developed by considering the whole context of citations.\nFurthermore, we proposed an ensembled feature engineering method comprising\nword embeddings obtained for texts, parts-of-speech tags, and dependency\nrelationships together. Ensembled features were considered as input to deep\nlearning based approaches for citation sentiment classification, which is in\nturn compared with Bag-of-Words approach. Experimental results demonstrate that\ndeep learning is useful for higher number of samples, whereas support vector\nmachine is the winner for smaller number of samples. Moreover, context-based\nsamples are proved to be more effective than context-less samples for citation\nsentiment analysis.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 00:27:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Vyas", "Vishal", ""], ["Ravi", "Kumar", ""], ["Ravi", "Vadlamani", ""], ["Uma", "V.", ""], ["Setlur", "Srirangaraj", ""], ["Govindaraju", "Venu", ""]]}, {"id": "2005.04560", "submitter": "Xiang Lisa Li", "authors": "Xiang Lisa Li and Alexander M. Rush", "title": "Posterior Control of Blackbox Generation", "comments": "Accepted for publication at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text generation often requires high-precision output that obeys task-specific\nrules. This fine-grained control is difficult to enforce with off-the-shelf\ndeep learning models. In this work, we consider augmenting neural generation\nmodels with discrete control states learned through a structured\nlatent-variable approach. Under this formulation, task-specific knowledge can\nbe encoded through a range of rich, posterior constraints that are effectively\ntrained into the model. This approach allows users to ground internal model\ndecisions based on prior knowledge, without sacrificing the representational\npower of neural generative models. Experiments consider applications of this\napproach for text generation. We find that this method improves over standard\nbenchmarks, while also providing fine-grained control.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 03:22:45 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Xiang Lisa", ""], ["Rush", "Alexander M.", ""]]}, {"id": "2005.04588", "submitter": "Javed Qadrud-Din", "authors": "Javed Qadrud-Din, Ashraf Bah Rabiou, Ryan Walker, Ravi Soni, Martin\n  Gajek, Gabriel Pack, Akhil Rangaraj", "title": "Transformer Based Language Models for Similar Text Retrieval and Ranking", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches for similar text retrieval and ranking with long natural\nlanguage queries rely at some level on queries and responses having words in\ncommon with each other. Recent applications of transformer-based neural\nlanguage models to text retrieval and ranking problems have been very\npromising, but still involve a two-step process in which result candidates are\nfirst obtained through bag-of-words-based approaches, and then reranked by a\nneural transformer. In this paper, we introduce novel approaches for\neffectively applying neural transformer models to similar text retrieval and\nranking without an initial bag-of-words-based step. By eliminating the\nbag-of-words-based step, our approach is able to accurately retrieve and rank\nresults even when they have no non-stopwords in common with the query. We\naccomplish this by using bidirectional encoder representations from\ntransformers (BERT) to create vectorized representations of sentence-length\ntexts, along with a vector nearest neighbor search index. We demonstrate both\nsupervised and unsupervised means of using BERT to accomplish this task.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 06:12:53 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 04:21:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Qadrud-Din", "Javed", ""], ["Rabiou", "Ashraf Bah", ""], ["Walker", "Ryan", ""], ["Soni", "Ravi", ""], ["Gajek", "Martin", ""], ["Pack", "Gabriel", ""], ["Rangaraj", "Akhil", ""]]}, {"id": "2005.04611", "submitter": "Fabio Petroni", "authors": "Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\\"aschel,\n  Yuxiang Wu, Alexander H. Miller, Sebastian Riedel", "title": "How Context Affects Language Models' Factual Predictions", "comments": "accepted at AKBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When pre-trained on large unsupervised textual corpora, language models are\nable to store and retrieve factual knowledge to some extent, making it possible\nto use them directly for zero-shot cloze-style question answering. However,\nstoring factual knowledge in a fixed number of weights of a language model\nclearly has limitations. Previous approaches have successfully provided access\nto information outside the model weights using supervised architectures that\ncombine an information retrieval system with a machine reading component. In\nthis paper, we go a step further and integrate information from a retrieval\nsystem with a pre-trained language model in a purely unsupervised way. We\nreport that augmenting pre-trained language models in this way dramatically\nimproves performance and that the resulting system, despite being unsupervised,\nis competitive with a supervised machine reading baseline. Furthermore,\nprocessing query and context with different segment tokens allows BERT to\nutilize its Next Sentence Prediction pre-trained classifier to determine\nwhether the context is relevant or not, substantially improving BERT's\nzero-shot cloze-style question-answering performance and making its predictions\nrobust to noisy contexts.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 09:28:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Petroni", "Fabio", ""], ["Lewis", "Patrick", ""], ["Piktus", "Aleksandra", ""], ["Rockt\u00e4schel", "Tim", ""], ["Wu", "Yuxiang", ""], ["Miller", "Alexander H.", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2005.04625", "submitter": "Wang Zhu", "authors": "Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene\n  Ie, Fei Sha", "title": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby\n  Steps", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to follow instructions is of fundamental importance to autonomous\nagents for vision-and-language navigation (VLN). In this paper, we study how an\nagent can navigate long paths when learning from a corpus that consists of\nshorter ones. We show that existing state-of-the-art agents do not generalize\nwell. To this end, we propose BabyWalk, a new VLN agent that is learned to\nnavigate by decomposing long instructions into shorter ones (BabySteps) and\ncompleting them sequentially. A special design memory buffer is used by the\nagent to turn its past experiences into contexts for future steps. The learning\nprocess is composed of two phases. In the first phase, the agent uses imitation\nlearning from demonstration to accomplish BabySteps. In the second phase, the\nagent uses curriculum-based reinforcement learning to maximize rewards on\nnavigation tasks with increasingly longer instructions. We create two new\nbenchmark datasets (of long navigation tasks) and use them in conjunction with\nexisting ones to examine BabyWalk's generalization ability. Empirical results\nshow that BabyWalk achieves state-of-the-art results on several metrics, in\nparticular, is able to follow long instructions better. The codes and the\ndatasets are released on our project page https://github.com/Sha-Lab/babywalk.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 10:46:41 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 22:02:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhu", "Wang", ""], ["Hu", "Hexiang", ""], ["Chen", "Jiacheng", ""], ["Deng", "Zhiwei", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Sha", "Fei", ""]]}, {"id": "2005.04684", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao and Rui Yan", "title": "From Standard Summarization to New Tasks and Beyond: Summarization with\n  Manifold Information", "comments": "Accepted by IJCAI 2020 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization is the research area aiming at creating a short and\ncondensed version of the original document, which conveys the main idea of the\ndocument in a few words. This research topic has started to attract the\nattention of a large community of researchers, and it is nowadays counted as\none of the most promising research areas. In general, text summarization\nalgorithms aim at using a plain text document as input and then output a\nsummary. However, in real-world applications, most of the data is not in a\nplain text format. Instead, there is much manifold information to be\nsummarized, such as the summary for a web page based on a query in the search\nengine, extreme long document (e.g., academic paper), dialog history and so on.\nIn this paper, we focus on the survey of these new summarization tasks and\napproaches in the real-world application.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:59:36 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Ren", "Zhaochun", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2005.04690", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, Hanqing Lu", "title": "Non-Autoregressive Image Captioning with Counterfactuals-Critical\n  Multi-Agent Learning", "comments": "IJCAI 2020 (copyright held by IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image captioning models are autoregressive, i.e. they generate each word\nby conditioning on previously generated words, which leads to heavy latency\nduring inference. Recently, non-autoregressive decoding has been proposed in\nmachine translation to speed up the inference time by generating all words in\nparallel. Typically, these models use the word-level cross-entropy loss to\noptimize each word independently. However, such a learning process fails to\nconsider the sentence-level consistency, thus resulting in inferior generation\nquality of these non-autoregressive models. In this paper, we propose a\nNon-Autoregressive Image Captioning (NAIC) model with a novel training\nparadigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates\nNAIC as a multi-agent reinforcement learning system where positions in the\ntarget sequence are viewed as agents that learn to cooperatively maximize a\nsentence-level reward. Besides, we propose to utilize massive unlabeled images\nto boost captioning performance. Extensive experiments on MSCOCO image\ncaptioning benchmark show that our NAIC model achieves a performance comparable\nto state-of-the-art autoregressive models, while brings 13.9x decoding speedup.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 15:09:44 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["He", "Xingjian", ""], ["Jiang", "Jie", ""], ["Lu", "Hanqing", ""]]}, {"id": "2005.04712", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Masato Mimura, Tatsuya Kawahara", "title": "CTC-synchronous Training for Monotonic Attention Model", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotonic chunkwise attention (MoChA) has been studied for the online\nstreaming automatic speech recognition (ASR) based on a sequence-to-sequence\nframework. In contrast to connectionist temporal classification (CTC), backward\nprobabilities cannot be leveraged in the alignment marginalization process\nduring training due to left-to-right dependency in the decoder. This results in\nthe error propagation of alignments to subsequent token generation. To address\nthis problem, we propose CTC-synchronous training (CTC-ST), in which MoChA uses\nCTC alignments to learn optimal monotonic alignments. Reference CTC alignments\nare extracted from a CTC branch sharing the same encoder with the decoder. The\nentire model is jointly optimized so that the expected boundaries from MoChA\nare synchronized with the alignments. Experimental evaluations of the TEDLIUM\nrelease-2 and Librispeech corpora show that the proposed method significantly\nimproves recognition, especially for long utterances. We also show that CTC-ST\ncan bring out the full potential of SpecAugment for MoChA.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 16:48:23 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 01:53:33 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 10:07:04 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Mimura", "Masato", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2005.04724", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Thierry Dutoit", "title": "Chirp Complex Cepstrum-based Decomposition for Asynchronous Glottal\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that complex cepstrum can be effectively used for\nglottal flow estimation by separating the causal and anticausal components of\nspeech. In order to guarantee a correct estimation, some constraints on the\nwindow have been derived. Among these, the window has to be synchronized on a\nGlottal Closure Instant. This paper proposes an extension of the complex\ncepstrum-based decomposition by incorporating a chirp analysis. The resulting\nmethod is shown to give a reliable estimation of the glottal flow wherever the\nwindow is located. This technique is then suited for its integration in usual\nspeech processing systems, which generally operate in an asynchronous way.\nBesides its potential for automatic voice quality analysis is highlighted.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:33:48 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Drugman", "Thomas", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2005.04726", "submitter": "Shreyansh Bhatt", "authors": "Shreyansh Bhatt, Amit Sheth, Valerie Shalin, Jinjin Zhao", "title": "Knowledge Graph semantic enhancement of input data for improving AI", "comments": null, "journal-ref": "IEEE Internet Computing, 24(2), 66-72 (2020)", "doi": "10.1109/MIC.2020.2979620", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent systems designed using machine learning algorithms require a\nlarge number of labeled data. Background knowledge provides complementary, real\nworld factual information that can augment the limited labeled data to train a\nmachine learning algorithm. The term Knowledge Graph (KG) is in vogue as for\nmany practical applications, it is convenient and useful to organize this\nbackground knowledge in the form of a graph. Recent academic research and\nimplemented industrial intelligent systems have shown promising performance for\nmachine learning algorithms that combine training data with a knowledge graph.\nIn this article, we discuss the use of relevant KGs to enhance input data for\ntwo applications that use machine learning -- recommendation and community\ndetection. The KG improves both accuracy and explainability.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:37:38 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bhatt", "Shreyansh", ""], ["Sheth", "Amit", ""], ["Shalin", "Valerie", ""], ["Zhao", "Jinjin", ""]]}, {"id": "2005.04732", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Mohit Bansal", "title": "Towards Robustifying NLI Models Against Lexical Dataset Biases", "comments": "ACL 2020 (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning models are making fast progress on the task of Natural\nLanguage Inference, recent studies have also shown that these models achieve\nhigh accuracy by exploiting several dataset biases, and without deep\nunderstanding of the language semantics. Using contradiction-word bias and\nword-overlapping bias as our two bias examples, this paper explores both\ndata-level and model-level debiasing methods to robustify models against\nlexical dataset biases. First, we debias the dataset through data augmentation\nand enhancement, but show that the model bias cannot be fully removed via this\nmethod. Next, we also compare two ways of directly debiasing the model without\nknowing what the dataset biases are in advance. The first approach aims to\nremove the label bias at the embedding level. The second approach employs a\nbag-of-words sub-model to capture the features that are likely to exploit the\nbias and prevents the original model from learning these biased features by\nforcing orthogonality between these two sub-models. We performed evaluations on\nnew balanced datasets extracted from the original MNLI dataset as well as the\nNLI stress tests, and show that the orthogonality approach is better at\ndebiasing the model while maintaining competitive overall accuracy. Our code\nand data are available at: https://github.com/owenzx/LexicalDebias-ACL2020\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:56:10 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 23:44:17 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zhou", "Xiang", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.04749", "submitter": "Kaveri Anuranjana", "authors": "Vijjini Anvesh Rao, Kaveri Anuranjana and Radhika Mamidi", "title": "A SentiWordNet Strategy for Curriculum Learning in Sentiment Analysis", "comments": "Accepted Short Paper at 25th International Conference on Applications\n  of Natural Language to Information Systems, June 2020, DFKI Saarbr\\\"ucken,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curriculum Learning (CL) is the idea that learning on a training set\nsequenced or ordered in a manner where samples range from easy to difficult,\nresults in an increment in performance over otherwise random ordering. The idea\nparallels cognitive science's theory of how human brains learn, and that\nlearning a difficult task can be made easier by phrasing it as a sequence of\neasy to difficult tasks. This idea has gained a lot of traction in machine\nlearning and image processing for a while and recently in Natural Language\nProcessing (NLP). In this paper, we apply the ideas of curriculum learning,\ndriven by SentiWordNet in a sentiment analysis setting. In this setting, given\na text segment, our aim is to extract its sentiment or polarity. SentiWordNet\nis a lexical resource with sentiment polarity annotations. By comparing\nperformance with other curriculum strategies and with no curriculum, the\neffectiveness of the proposed strategy is presented. Convolutional, Recurrence,\nand Attention-based architectures are employed to assess this improvement. The\nmodels are evaluated on a standard sentiment dataset, Stanford Sentiment\nTreebank.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 18:50:40 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 19:47:35 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Rao", "Vijjini Anvesh", ""], ["Anuranjana", "Kaveri", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2005.04790", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet\n  Singh, Pratik Ringshia, Davide Testuggine", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new challenge set for multimodal classification,\nfocusing on detecting hate speech in multimodal memes. It is constructed such\nthat unimodal models struggle and only multimodal models can succeed: difficult\nexamples (\"benign confounders\") are added to the dataset to make it hard to\nrely on unimodal signals. The task requires subtle reasoning, yet is\nstraightforward to evaluate as a binary classification problem. We provide\nbaseline performance numbers for unimodal models, as well as for multimodal\nmodels with various degrees of sophistication. We find that state-of-the-art\nmethods perform poorly compared to humans (64.73% vs. 84.7% accuracy),\nillustrating the difficulty of the task and highlighting the challenge that\nthis important problem poses to the community.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 21:31:00 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:01:54 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 18:43:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kiela", "Douwe", ""], ["Firooz", "Hamed", ""], ["Mohan", "Aravind", ""], ["Goswami", "Vedanuj", ""], ["Singh", "Amanpreet", ""], ["Ringshia", "Pratik", ""], ["Testuggine", "Davide", ""]]}, {"id": "2005.04816", "submitter": "Aditya Siddhant", "authors": "Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha\n  Kudugunta, Naveen Arivazhagan and Yonghui Wu", "title": "Leveraging Monolingual Data with Self-Supervision for Multilingual\n  Neural Machine Translation", "comments": null, "journal-ref": "ACL 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years two promising research directions in low-resource\nneural machine translation (NMT) have emerged. The first focuses on utilizing\nhigh-resource languages to improve the quality of low-resource languages via\nmultilingual NMT. The second direction employs monolingual data with\nself-supervision to pre-train translation models, followed by fine-tuning on\nsmall amounts of supervised data. In this work, we join these two lines of\nresearch and demonstrate the efficacy of monolingual data with self-supervision\nin multilingual NMT. We offer three major results: (i) Using monolingual data\nsignificantly boosts the translation quality of low-resource languages in\nmultilingual models. (ii) Self-supervision improves zero-shot translation\nquality in multilingual models. (iii) Leveraging monolingual data with\nself-supervision provides a viable path towards adding new languages to\nmultilingual models, getting up to 33 BLEU on ro-en translation without any\nparallel data or back-translation.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 00:20:33 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Siddhant", "Aditya", ""], ["Bapna", "Ankur", ""], ["Cao", "Yuan", ""], ["Firat", "Orhan", ""], ["Chen", "Mia", ""], ["Kudugunta", "Sneha", ""], ["Arivazhagan", "Naveen", ""], ["Wu", "Yonghui", ""]]}, {"id": "2005.04862", "submitter": "Ye Bai", "authors": "Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi Wen, Shuai\n  Zhang", "title": "Listen Attentively, and Spell Once: Whole Sentence Generation via a\n  Non-Autoregressive Architecture for Low-Latency Speech Recognition", "comments": "accepted by INTERSPEECH2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although attention based end-to-end models have achieved promising\nperformance in speech recognition, the multi-pass forward computation in\nbeam-search increases inference time cost, which limits their practical\napplications. To address this issue, we propose a non-autoregressive end-to-end\nspeech recognition system called LASO (listen attentively, and spell once).\nBecause of the non-autoregressive property, LASO predicts a textual token in\nthe sequence without the dependence on other tokens. Without beam-search, the\none-pass propagation much reduces inference time cost of LASO. And because the\nmodel is based on the attention based feedforward structure, the computation\ncan be implemented in parallel efficiently. We conduct experiments on publicly\navailable Chinese dataset AISHELL-1. LASO achieves a character error rate of\n6.4%, which outperforms the state-of-the-art autoregressive transformer model\n(6.7%). The average inference latency is 21 ms, which is 1/50 of the\nautoregressive transformer model.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 04:45:02 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 15:32:41 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 09:52:53 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 01:26:15 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Bai", "Ye", ""], ["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Tian", "Zhengkun", ""], ["Wen", "Zhengqi", ""], ["Zhang", "Shuai", ""]]}, {"id": "2005.04929", "submitter": "Martha Lewis", "authors": "Martha Lewis", "title": "Towards logical negation for compositional distributional semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional model of meaning gives the\ncomposition of words into phrases and sentences pride of place. However, it has\nso far lacked a model of logical negation. This paper gives some steps towards\nproviding this operator, modelling it as a version of projection onto the\nsubspace orthogonal to a word. We give a small demonstration of the operators\nperformance in a sentence entailment task.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:51:30 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lewis", "Martha", ""]]}, {"id": "2005.04938", "submitter": "Tanik Saikh Mr", "authors": "Tanik Saikh, Arkadipta De, Asif Ekbal, Pushpak Bhattacharyya", "title": "A Deep Learning Approach for Automatic Detection of Fake News", "comments": null, "journal-ref": "Proceedings of the 16th International Conference on Natural\n  Language Processing (ICON 2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Fake news detection is a very prominent and essential task in the field of\njournalism. This challenging problem is seen so far in the field of politics,\nbut it could be even more challenging when it is to be determined in the\nmulti-domain platform. In this paper, we propose two effective models based on\ndeep learning for solving fake news detection problem in online news contents\nof multiple domains. We evaluate our techniques on the two recently released\ndatasets, namely FakeNews AMT and Celebrity for fake news detection. The\nproposed systems yield encouraging performance, outperforming the current\nhandcrafted feature engineering based state-of-the-art system with a\nsignificant margin of 3.08% and 9.3% by the two models, respectively. In order\nto exploit the datasets, available for the related tasks, we perform\ncross-domain analysis (i.e. model trained on FakeNews AMT and tested on\nCelebrity and vice versa) to explore the applicability of our systems across\nthe domains.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:07:46 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Saikh", "Tanik", ""], ["De", "Arkadipta", ""], ["Ekbal", "Asif", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2005.05114", "submitter": "Mohammad Hossein Manshaei", "authors": "Mohammad Amin Samadi, Mohammad Sadegh Akhondzadeh, Sayed Jalal Zahabi,\n  Mohammad Hossein Manshaei, Zeinab Maleki, Payman Adibi", "title": "Evaluating Sparse Interpretable Word Embeddings for Biomedical Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have found their way into a wide range of natural language\nprocessing tasks including those in the biomedical domain. While these vector\nrepresentations successfully capture semantic and syntactic word relations,\nhidden patterns and trends in the data, they fail to offer interpretability.\nInterpretability is a key means to justification which is an integral part when\nit comes to biomedical applications. We present an inclusive study on\ninterpretability of word embeddings in the medical domain, focusing on the role\nof sparse methods. Qualitative and quantitative measurements and metrics for\ninterpretability of word vector representations are provided. For the\nquantitative evaluation, we introduce an extensive categorized dataset that can\nbe used to quantify interpretability based on category theory. Intrinsic and\nextrinsic evaluation of the studied methods are also presented. As for the\nlatter, we propose datasets which can be utilized for effective extrinsic\nevaluation of word vectors in the biomedical domain. Based on our experiments,\nit is seen that sparse word vectors show far more interpretability while\npreserving the performance of their original vectors in downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:56:58 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Samadi", "Mohammad Amin", ""], ["Akhondzadeh", "Mohammad Sadegh", ""], ["Zahabi", "Sayed Jalal", ""], ["Manshaei", "Mohammad Hossein", ""], ["Maleki", "Zeinab", ""], ["Adibi", "Payman", ""]]}, {"id": "2005.05144", "submitter": "Edresson Casanova", "authors": "Edresson Casanova, Arnaldo Candido Junior, Christopher Shulby,\n  Frederico Santos de Oliveira, Jo\\~ao Paulo Teixeira, Moacir Antonelli Ponti,\n  Sandra Maria Aluisio", "title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian\n  Portuguese", "comments": "This paper is under consideration at Language Resources and\n  Evaluation (LREV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech provides a natural way for human-computer interaction. In particular,\nspeech synthesis systems are popular in different applications, such as\npersonal assistants, GPS applications, screen readers and accessibility tools.\nHowever, not all languages are on the same level when in terms of resources and\nsystems for speech synthesis. This work consists of creating publicly available\nresources for Brazilian Portuguese in the form of a novel dataset along with\ndeep learning models for end-to-end speech synthesis. Such dataset has 10.5\nhours from a single speaker, from which a Tacotron 2 model with the RTISI-LA\nvocoder presented the best performance, achieving a 4.03 MOS value. The\nobtained results are comparable to related works covering English language and\nthe state-of-the-art in Portuguese.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:36:44 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 12:09:08 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 18:27:46 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Casanova", "Edresson", ""], ["Junior", "Arnaldo Candido", ""], ["Shulby", "Christopher", ""], ["de Oliveira", "Frederico Santos", ""], ["Teixeira", "Jo\u00e3o Paulo", ""], ["Ponti", "Moacir Antonelli", ""], ["Aluisio", "Sandra Maria", ""]]}, {"id": "2005.05189", "submitter": "Yilin Niu", "authors": "Yilin Niu, Fangkai Jiao, Mantong Zhou, Ting Yao, Jingfang Xu, Minlie\n  Huang", "title": "A Self-Training Method for Machine Reading Comprehension with Soft\n  Evidence Extraction", "comments": "12 pages, accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have achieved great success on machine reading comprehension\n(MRC), many of which typically consist of two components: an evidence extractor\nand an answer predictor. The former seeks the most relevant information from a\nreference text, while the latter is to locate or generate answers from the\nextracted evidence. Despite the importance of evidence labels for training the\nevidence extractor, they are not cheaply accessible, particularly in many\nnon-extractive MRC tasks such as YES/NO question answering and multi-choice\nMRC.\n  To address this problem, we present a Self-Training method (STM), which\nsupervises the evidence extractor with auto-generated evidence labels in an\niterative process. At each iteration, a base MRC model is trained with golden\nanswers and noisy evidence labels. The trained model will predict pseudo\nevidence labels as extra supervision in the next iteration. We evaluate STM on\nseven datasets over three MRC tasks. Experimental results demonstrate the\nimprovement on existing MRC models, and we also analyze how and why such a\nself-training method works in MRC. The source code can be obtained from\nhttps://github.com/SparkJiao/Self-Training-MRC\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:26:07 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 04:02:57 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Niu", "Yilin", ""], ["Jiao", "Fangkai", ""], ["Zhou", "Mantong", ""], ["Yao", "Ting", ""], ["Xu", "Jingfang", ""], ["Huang", "Minlie", ""]]}, {"id": "2005.05240", "submitter": "Ye Liu", "authors": "Ye Liu, Tao Yang, Zeyu You, Wei Fan and Philip S. Yu", "title": "Commonsense Evidence Generation and Injection in Reading Comprehension", "comments": "Accepted by SIGDIAL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human tackle reading comprehension not only based on the given context itself\nbut often rely on the commonsense beyond. To empower the machine with\ncommonsense reasoning, in this paper, we propose a Commonsense Evidence\nGeneration and Injection framework in reading comprehension, named CEGI. The\nframework injects two kinds of auxiliary commonsense evidence into\ncomprehensive reading to equip the machine with the ability of rational\nthinking. Specifically, we build two evidence generators: the first generator\naims to generate textual evidence via a language model; the other generator\naims to extract factual evidence (automatically aligned text-triples) from a\ncommonsense knowledge graph after graph completion. Those evidences incorporate\ncontextual commonsense and serve as the additional inputs to the model.\nThereafter, we propose a deep contextual encoder to extract semantic\nrelationships among the paragraph, question, option, and evidence. Finally, we\nemploy a capsule network to extract different linguistic units (word and\nphrase) from the relations, and dynamically predict the optimal option based on\nthe extracted units. Experiments on the CosmosQA dataset demonstrate that the\nproposed CEGI model outperforms the current state-of-the-art approaches and\nachieves the accuracy (83.6%) on the leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:31:08 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Liu", "Ye", ""], ["Yang", "Tao", ""], ["You", "Zeyu", ""], ["Fan", "Wei", ""], ["Yu", "Philip S.", ""]]}, {"id": "2005.05255", "submitter": "Daphne Ippolito", "authors": "Daphne Ippolito, David Grangier, Douglas Eck, Chris Callison-Burch", "title": "Toward Better Storylines with Sentence-Level Language Models", "comments": "ACL 2020 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sentence-level language model which selects the next sentence in\na story from a finite set of fluent alternatives. Since it does not need to\nmodel fluency, the sentence-level language model can focus on longer range\ndependencies, which are crucial for multi-sentence coherence. Rather than\ndealing with individual words, our method treats the story so far as a list of\npre-trained sentence embeddings and predicts an embedding for the next\nsentence, which is more efficient than predicting word embeddings. Notably this\nallows us to consider a large number of candidates for the next sentence during\ntraining. We demonstrate the effectiveness of our approach with\nstate-of-the-art accuracy on the unsupervised Story Cloze task and with\npromising results on larger-scale next sentence prediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:54:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ippolito", "Daphne", ""], ["Grangier", "David", ""], ["Eck", "Douglas", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2005.05256", "submitter": "Abhilasha Sancheti", "authors": "Abhilasha Sancheti, Kundan Krishna, Balaji Vasan Srinivasan,\n  Anandhavelu Natarajan", "title": "Reinforced Rewards Framework for Text Style Transfer", "comments": "ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer deals with the algorithms to transfer the stylistic properties\nof a piece of text into that of another while ensuring that the core content is\npreserved. There has been a lot of interest in the field of text style transfer\ndue to its wide application to tailored text generation. Existing works\nevaluate the style transfer models based on content preservation and transfer\nstrength. In this work, we propose a reinforcement learning based framework\nthat directly rewards the framework on these target metrics yielding a better\ntransfer of the target style. We show the improved performance of our proposed\nframework based on automatic and human evaluation on three independent tasks:\nwherein we transfer the style of text from formal to informal, high excitement\nto low excitement, modern English to Shakespearean English, and vice-versa in\nall the three cases. Improved performance of the proposed framework over\nexisting state-of-the-art frameworks indicates the viability of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:54:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Sancheti", "Abhilasha", ""], ["Krishna", "Kundan", ""], ["Srinivasan", "Balaji Vasan", ""], ["Natarajan", "Anandhavelu", ""]]}, {"id": "2005.05257", "submitter": "Nils Holzenberger", "authors": "Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme", "title": "A Dataset for Statutory Reasoning in Tax Law Entailment and Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legislation can be viewed as a body of prescriptive rules expressed in\nnatural language. The application of legislation to facts of a case we refer to\nas statutory reasoning, where those facts are also expressed in natural\nlanguage. Computational statutory reasoning is distinct from most existing work\nin machine reading, in that much of the information needed for deciding a case\nis declared exactly once (a law), while the information needed in much of\nmachine reading tends to be learned through distributional language statistics.\nTo investigate the performance of natural language understanding approaches on\nstatutory reasoning, we introduce a dataset, together with a legal-domain text\ncorpus. Straightforward application of machine reading models exhibits low\nout-of-the-box performance on our questions, whether or not they have been\nfine-tuned to the legal domain. We contrast this with a hand-constructed\nProlog-based system, designed to fully solve the task. These experiments\nsupport a discussion of the challenges facing statutory reasoning moving\nforward, which we argue is an interesting real-world task that can motivate the\ndevelopment of models able to utilize prescriptive rules specified in natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:54:42 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 22:40:15 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 16:08:43 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Holzenberger", "Nils", ""], ["Blair-Stanek", "Andrew", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2005.05264", "submitter": "Ivan Vuli\\'c", "authors": "Daniela Gerz, Ivan Vuli\\'c, Marek Rei, Roi Reichart, Anna Korhonen", "title": "Multidirectional Associative Optimization of Function-Specific Word\n  Representations", "comments": "ACL 2020 (Long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a neural framework for learning associations between interrelated\ngroups of words such as the ones found in Subject-Verb-Object (SVO) structures.\nOur model induces a joint function-specific word vector space, where vectors of\ne.g. plausible SVO compositions lie close together. The model retains\ninformation about word group membership even in the joint space, and can\nthereby effectively be applied to a number of tasks reasoning over the SVO\nstructure. We show the robustness and versatility of the proposed framework by\nreporting state-of-the-art results on the tasks of estimating selectional\npreference and event similarity. The results indicate that the combinations of\nrepresentations learned with our task-independent model outperform\ntask-specific architectures from prior work, while reducing the number of\nparameters by up to 95%.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:07:20 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gerz", "Daniela", ""], ["Vuli\u0107", "Ivan", ""], ["Rei", "Marek", ""], ["Reichart", "Roi", ""], ["Korhonen", "Anna", ""]]}, {"id": "2005.05293", "submitter": "EPTCS", "authors": "Matthew Wilson, James Hefford, Guillaume Boisseau, Vincent Wang", "title": "The Safari of Update Structures: Visiting the Lens and Quantum\n  Enclosures", "comments": "In Proceedings ACT 2020, arXiv:2101.07888", "journal-ref": "EPTCS 333, 2021, pp. 1-18", "doi": "10.4204/EPTCS.333.1", "report-no": null, "categories": "quant-ph cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build upon our recently introduced concept of an update structure to show\nthat it is a generalisation of very-well-behaved lenses, that is, there is a\nbijection between a strict subset of update structures and vwb lenses in\ncartesian categories. We show that update structures are also sufficiently\ngeneral to capture quantum observables, pinpointing the additional assumptions\nrequired to make the two coincide. In doing so, we shift the focus from special\ncommutative dagger-Frobenius algebras to interacting (co)magma (co)module\npairs, showing that the algebraic properties of the (co)multiplication arise\nfrom the module-comodule interaction, rather than direct assumptions about the\nmagma-comagma pair. We then begin to investigate the zoo of possible update\nstructures, introducing the notions of classical security-flagged databases,\nand databases of quantum systems. This work is of foundational interest as\nupdate structures place previously distinct areas of research in a general\nclass of operationally motivated structures, we expect the taming of this class\nto illuminate novel relationships between separately studied topics in computer\nscience, physics and mathematics.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:51:38 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 15:14:01 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 00:00:42 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wilson", "Matthew", ""], ["Hefford", "James", ""], ["Boisseau", "Guillaume", ""], ["Wang", "Vincent", ""]]}, {"id": "2005.05298", "submitter": "Baolin Peng", "authors": "Baolin Peng and Chunyuan Li and Jinchao Li and Shahin Shayandeh and\n  Lars Liden and Jianfeng Gao", "title": "SOLOIST: Building Task Bots at Scale with Transfer Learning and Machine\n  Teaching", "comments": "18 pages; To appear at TACL; Project Website: https://aka.ms/soloist", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method SOLOIST that uses transfer learning and machine\nteaching to build task bots at scale. We parameterize classical modular\ntask-oriented dialog systems using a Transformer-based auto-regressive language\nmodel, which subsumes different dialog modules into a single neural model. We\npre-train, on heterogeneous dialog corpora, a task-grounded response generation\nmodel, which can generate dialog responses grounded in user goals and\nreal-world knowledge for task completion. The pre-trained model can be\nefficiently adapted to accomplish new tasks with a handful of task-specific\ndialogs via machine teaching, where training samples are generated by human\nteachers interacting with the system. Experiments show that (i) SOLOIST creates\nnew state-of-the-art on well-studied task-oriented dialog benchmarks, including\nCamRest676 and MultiWOZ; (ii) in the few-shot fine-tuning settings, SOLOIST\nsignificantly outperforms existing methods, and (iii) the use of machine\nteaching substantially reduces the labeling cost of fine-tuning. The\npre-trained models and codes are available at https://aka.ms/soloist.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:58:34 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:42:40 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 03:02:50 GMT"}, {"version": "v4", "created": "Fri, 9 Apr 2021 03:14:57 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Peng", "Baolin", ""], ["Li", "Chunyuan", ""], ["Li", "Jinchao", ""], ["Shayandeh", "Shahin", ""], ["Liden", "Lars", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2005.05339", "submitter": "Chris Donahue", "authors": "Chris Donahue, Mina Lee, Percy Liang", "title": "Enabling Language Models to Fill in the Blanks", "comments": "Published as a conference paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple approach for text infilling, the task of predicting\nmissing spans of text at any position in a document. While infilling could\nenable rich functionality especially for writing assistance tools, more\nattention has been devoted to language modeling---a special case of infilling\nwhere text is predicted at the end of a document. In this paper, we aim to\nextend the capabilities of language models (LMs) to the more general task of\ninfilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences\ncontaining the concatenation of artificially-masked text and the text which was\nmasked. We show that this approach, which we call infilling by language\nmodeling, can enable LMs to infill entire sentences effectively on three\ndifferent domains: short stories, scientific abstracts, and lyrics.\nFurthermore, we show that humans have difficulty identifying sentences infilled\nby our approach as machine-generated in the domain of short stories.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 18:00:03 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 18:03:11 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Donahue", "Chris", ""], ["Lee", "Mina", ""], ["Liang", "Percy", ""]]}, {"id": "2005.05402", "submitter": "Jie Lei", "authors": "Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L. Berg, Mohit\n  Bansal", "title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video\n  Paragraph Captioning", "comments": "ACL 2020 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating multi-sentence descriptions for videos is one of the most\nchallenging captioning tasks due to its high requirements for not only visual\nrelevance but also discourse-based coherence across the sentences in the\nparagraph. Towards this goal, we propose a new approach called Memory-Augmented\nRecurrent Transformer (MART), which uses a memory module to augment the\ntransformer architecture. The memory module generates a highly summarized\nmemory state from the video segments and the sentence history so as to help\nbetter prediction of the next sentence (w.r.t. coreference and repetition\naspects), thus encouraging coherent paragraph generation. Extensive\nexperiments, human evaluations, and qualitative analyses on two popular\ndatasets ActivityNet Captions and YouCookII show that MART generates more\ncoherent and less repetitive paragraph captions than baseline methods, while\nmaintaining relevance to the input video events. All code is available\nopen-source at: https://github.com/jayleicn/recurrent-transformer\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:01:41 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Lei", "Jie", ""], ["Wang", "Liwei", ""], ["Shen", "Yelong", ""], ["Yu", "Dong", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.05414", "submitter": "Soumya Banerjee", "authors": "Soumya Banerjee, Debarshi Kumar Sanyal, Samiran Chattopadhyay, Plaban\n  Kumar Bhowmick and Parthapratim Das", "title": "Segmenting Scientific Abstracts into Discourse Categories: A Deep\n  Learning-Based Approach for Sparse Labeled Data", "comments": "to appear in the proceedings of JCDL'2020", "journal-ref": null, "doi": "10.1145/3383583.3398598", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abstract of a scientific paper distills the contents of the paper into a\nshort paragraph. In the biomedical literature, it is customary to structure an\nabstract into discourse categories like BACKGROUND, OBJECTIVE, METHOD, RESULT,\nand CONCLUSION, but this segmentation is uncommon in other fields like computer\nscience. Explicit categories could be helpful for more granular, that is,\ndiscourse-level search and recommendation. The sparsity of labeled data makes\nit challenging to construct supervised machine learning solutions for automatic\ndiscourse-level segmentation of abstracts in non-bio domains. In this paper, we\naddress this problem using transfer learning. In particular, we define three\ndiscourse categories BACKGROUND, TECHNIQUE, OBSERVATION-for an abstract because\nthese three categories are the most common. We train a deep neural network on\nstructured abstracts from PubMed, then fine-tune it on a small hand-labeled\ncorpus of computer science papers. We observe an accuracy of 75% on the test\ncorpus. We perform an ablation study to highlight the roles of the different\nparts of the model. Our method appears to be a promising solution to the\nautomatic segmentation of abstracts, where the labeled data is sparse.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:21:25 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 08:35:08 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Banerjee", "Soumya", ""], ["Sanyal", "Debarshi Kumar", ""], ["Chattopadhyay", "Samiran", ""], ["Bhowmick", "Plaban Kumar", ""], ["Das", "Parthapratim", ""]]}, {"id": "2005.05442", "submitter": "Xingyi Yang", "authors": "Wenmian Yang, Guangtao Zeng, Bowen Tan, Zeqian Ju, Subrato\n  Chakravorty, Xuehai He, Shu Chen, Xingyi Yang, Qingyang Wu, Zhou Yu, Eric\n  Xing, Pengtao Xie", "title": "On the Generation of Medical Dialogues for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the pandemic of COVID-19, people experiencing COVID19-related symptoms\nor exposed to risk factors have a pressing need to consult doctors. Due to\nhospital closure, a lot of consulting services have been moved online. Because\nof the shortage of medical professionals, many people cannot receive online\nconsultations timely. To address this problem, we aim to develop a medical\ndialogue system that can provide COVID19-related consultations. We collected\ntwo dialogue datasets -- CovidDialog -- (in English and Chinese respectively)\ncontaining conversations between doctors and patients about COVID-19. On these\ntwo datasets, we train several dialogue generation models based on Transformer,\nGPT, and BERT-GPT. Since the two COVID-19 dialogue datasets are small in size,\nwhich bear high risk of overfitting, we leverage transfer learning to mitigate\ndata deficiency. Specifically, we take the pretrained models of Transformer,\nGPT, and BERT-GPT on dialog datasets and other large-scale texts, then finetune\nthem on our CovidDialog tasks. We perform both automatic and human evaluation\nof responses generated by these models. The results show that the generated\nresponses are promising in being doctor-like, relevant to the conversation\nhistory, and clinically informative. The data and code are available at\nhttps://github.com/UCSD-AI4H/COVID-Dialogue.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:23:43 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 02:06:58 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yang", "Wenmian", ""], ["Zeng", "Guangtao", ""], ["Tan", "Bowen", ""], ["Ju", "Zeqian", ""], ["Chakravorty", "Subrato", ""], ["He", "Xuehai", ""], ["Chen", "Shu", ""], ["Yang", "Xingyi", ""], ["Wu", "Qingyang", ""], ["Yu", "Zhou", ""], ["Xing", "Eric", ""], ["Xie", "Pengtao", ""]]}, {"id": "2005.05447", "submitter": "Irene Nandutu", "authors": "Irene Nandutu, Ernest Mwebaze", "title": "Luganda Text-to-Speech Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Uganda, Luganda is the most spoken native language. It is used for\ncommunication in informal as well as formal business transactions. The\ndevelopment of technology startups globally related to TTS has mainly been with\nlanguages like English, French, etc. These are added in TTS engines by Google,\nMicrosoft among others, allowing developers in these regions to innovate TTS\nproducts. Luganda is not supported because the language is not built and\ntrained on these engines. In this study, we analyzed the Luganda language\nstructure and constructions and then proposed and developed a Luganda TTS. The\nsystem was built and trained using locally sourced Luganda language text and\naudio. The engine is now able to capture text and reads it aloud. We tested the\naccuracy using MRT and MOS. MRT and MOS tests results are quite good with MRT\nhaving better results. The results general score was 71%. This study will\nenhance previous solutions to NLP gaps in Uganda, as well as provide raw data\nsuch that other research in this area can take place.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:33:33 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Nandutu", "Irene", ""], ["Mwebaze", "Ernest", ""]]}, {"id": "2005.05477", "submitter": "Francis Tyers", "authors": "Lane Schwartz, Francis Tyers, Lori Levin, Christo Kirov, Patrick\n  Littell, Chi-kiu Lo, Emily Prud'hommeaux, Hyunji Hayley Park, Kenneth\n  Steimel, Rebecca Knowles, Jeffrey Micher, Lonny Strunk, Han Liu, Coleman\n  Haley, Katherine J. Zhang, Robbie Jimmerson, Vasilisa Andriyanets, Aldrian\n  Obaja Muis, Naoki Otani, Jong Hyuk Park, and Zhisong Zhang", "title": "Neural Polysynthetic Language Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in natural language processing commonly assumes that approaches that\nwork well for English and and other widely-used languages are \"language\nagnostic\". In high-resource languages, especially those that are analytic, a\ncommon approach is to treat morphologically-distinct variants of a common root\nas completely independent word types. This assumes, that there are limited\nmorphological inflections per root, and that the majority will appear in a\nlarge enough corpus, so that the model can adequately learn statistics about\neach form. Approaches like stemming, lemmatization, or subword segmentation are\noften used when either of those assumptions do not hold, particularly in the\ncase of synthetic languages like Spanish or Russian that have more inflection\nthan English.\n  In the literature, languages like Finnish or Turkish are held up as extreme\nexamples of complexity that challenge common modelling assumptions. Yet, when\nconsidering all of the world's languages, Finnish and Turkish are closer to the\naverage case. When we consider polysynthetic languages (those at the extreme of\nmorphological complexity), approaches like stemming, lemmatization, or subword\nmodelling may not suffice. These languages have very high numbers of hapax\nlegomena, showing the need for appropriate morphological handling of words,\nwithout which it is not possible for a model to capture enough word statistics.\n  We examine the current state-of-the-art in language modelling, machine\ntranslation, and text prediction for four polysynthetic languages: Guaran\\'i,\nSt. Lawrence Island Yupik, Central Alaskan Yupik, and Inuktitut. We then\npropose a novel framework for language modelling that combines knowledge\nrepresentations from finite-state morphological analyzers with Tensor Product\nRepresentations in order to enable neural language models capable of handling\nthe full range of typologically variant languages.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 22:57:04 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 10:46:29 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Schwartz", "Lane", ""], ["Tyers", "Francis", ""], ["Levin", "Lori", ""], ["Kirov", "Christo", ""], ["Littell", "Patrick", ""], ["Lo", "Chi-kiu", ""], ["Prud'hommeaux", "Emily", ""], ["Park", "Hyunji Hayley", ""], ["Steimel", "Kenneth", ""], ["Knowles", "Rebecca", ""], ["Micher", "Jeffrey", ""], ["Strunk", "Lonny", ""], ["Liu", "Han", ""], ["Haley", "Coleman", ""], ["Zhang", "Katherine J.", ""], ["Jimmerson", "Robbie", ""], ["Andriyanets", "Vasilisa", ""], ["Muis", "Aldrian Obaja", ""], ["Otani", "Naoki", ""], ["Park", "Jong Hyuk", ""], ["Zhang", "Zhisong", ""]]}, {"id": "2005.05480", "submitter": "Shereen Oraby", "authors": "Yuheng Du, Shereen Oraby, Vittorio Perera, Minmin Shen, Anjali\n  Narayan-Chen, Tagyoung Chung, Anu Venkatesh, Dilek Hakkani-Tur", "title": "Schema-Guided Natural Language Generation", "comments": "Accepted as a long paper at INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based approaches to data-to-text natural language generation\n(NLG) have gained popularity in recent years, with the goal of generating a\nnatural language prompt that accurately realizes an input meaning\nrepresentation. To facilitate the training of neural network models,\nresearchers created large datasets of paired utterances and their meaning\nrepresentations. However, the creation of such datasets is an arduous task and\nthey mostly consist of simple meaning representations composed of slot and\nvalue tokens to be realized. These representations do not include any\ncontextual information that an NLG system can use when trying to generalize,\nsuch as domain information and descriptions of slots and values. In this paper,\nwe present the novel task of Schema-Guided Natural Language Generation\n(SG-NLG). Here, the goal is still to generate a natural language prompt, but in\nSG-NLG, the input MRs are paired with rich schemata providing contextual\ninformation. To generate a dataset for SG-NLG we re-purpose an existing dataset\nfor another task: dialog state tracking, which includes a large and rich schema\nspanning multiple different attributes, including information about the domain,\nuser intent, and slot descriptions. We train different state-of-the-art models\nfor neural natural language generation on this dataset and show that in many\ncases, including rich schema information allows our models to produce higher\nquality outputs both in terms of semantics and diversity. We also conduct\nexperiments comparing model performance on seen versus unseen domains, and\npresent a human evaluation demonstrating high ratings for overall output\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 23:01:22 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 20:33:09 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Du", "Yuheng", ""], ["Oraby", "Shereen", ""], ["Perera", "Vittorio", ""], ["Shen", "Minmin", ""], ["Narayan-Chen", "Anjali", ""], ["Chung", "Tagyoung", ""], ["Venkatesh", "Anu", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2005.05487", "submitter": "Takashi Morita", "authors": "Takashi Morita and Hiroki Koda", "title": "Exploring TTS without T Using Biologically/Psychologically Motivated\n  Neural Network Modules (ZeroSpeech 2020)", "comments": "Accepted in INTERSPEECH 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-3127", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we reported our exploration of Text-To-Speech without Text\n(TTS without T) in the Zero Resource Speech Challenge 2020, in which\nparticipants proposed an end-to-end, unsupervised system that learned speech\nrecognition and TTS together. We addressed the challenge using\nbiologically/psychologically motivated modules of Artificial Neural Networks\n(ANN), with a particular interest in unsupervised learning of human language as\na biological/psychological problem. The system first processes Mel Frequency\nCepstral Coefficient (MFCC) frames with an Echo-State Network (ESN), and\nsimulates computations in cortical microcircuits. The outcome is discretized by\nour original Variational Autoencoder (VAE) that implements the Dirichlet-based\nBayesian clustering widely accepted in computational linguistics and cognitive\nscience. The discretized signal is then reverted into sound waveform via a\nneural-network implementation of the source-filter model for speech production.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 23:44:37 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 09:18:57 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 09:13:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Morita", "Takashi", ""], ["Koda", "Hiroki", ""]]}, {"id": "2005.05507", "submitter": "Ion Madrazo Azpiazu", "authors": "Ion Madrazo Azpiazu, Maria Soledad Pera", "title": "A Framework for Hierarchical Multilingual Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual machine translation has recently been in vogue given its\npotential for improving machine translation performance for low-resource\nlanguages via transfer learning. Empirical examinations demonstrating the\nsuccess of existing multilingual machine translation strategies, however, are\nlimited to experiments in specific language groups. In this paper, we present a\nhierarchical framework for building multilingual machine translation strategies\nthat takes advantage of a typological language family tree for enabling\ntransfer among similar languages while avoiding the negative effects that\nresult from incorporating languages that are too different to each other.\nExhaustive experimentation on a dataset with 41 languages demonstrates the\nvalidity of the proposed framework, especially when it comes to improving the\nperformance of low-resource languages via the use of typologically related\nfamilies for which richer sets of resources are available.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:24:43 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Azpiazu", "Ion Madrazo", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "2005.05513", "submitter": "Tavpritesh Sethi", "authors": "Baani Leen Kaur Jolly, Palash Aggrawal, Amogh Gulati, Amarjit Singh\n  Sethi, Ponnurangam Kumaraguru, Tavpritesh Sethi", "title": "Psychometric Analysis and Coupling of Emotions Between State Bulletins\n  and Twitter in India during COVID-19 Infodemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 infodemic has been spreading faster than the pandemic itself. The\nmisinformation riding upon the infodemic wave poses a major threat to people's\nhealth and governance systems. Since social media is the largest source of\ninformation, managing the infodemic not only requires mitigating of\nmisinformation but also an early understanding of psychological patterns\nresulting from it. During the COVID-19 crisis, Twitter alone has seen a sharp\n45% increase in the usage of its curated events page, and a 30% increase in its\ndirect messaging usage, since March 6th 2020. In this study, we analyze the\npsychometric impact and coupling of the COVID-19 infodemic with the official\nbulletins related to COVID-19 at the national and state level in India. We look\nat these two sources with a psycho-linguistic lens of emotions and quantified\nthe extent and coupling between the two. We modified path, a deep skip-gram\nbased open-sourced lexicon builder for effective capture of health-related\nemotions. We were then able to capture the time-evolution of health-related\nemotions in social media and official bulletins. An analysis of lead-lag\nrelationships between the time series of extracted emotions from official\nbulletins and social media using Granger's causality showed that state\nbulletins were leading the social media for some emotions such as Medical\nEmergency. Further insights that are potentially relevant for the policymaker\nand the communicators actively engaged in mitigating misinformation are also\ndiscussed. Our paper also introduces CoronaIndiaDataset2, the first social\nmedia based COVID-19 dataset at national and state levels from India with over\n5.6 million national and 2.6 million state-level tweets. Finally, we present\nour findings as COVibes, an interactive web application capturing psychometric\ninsights captured upon the CoronaIndiaDataset, both at a national and state\nlevel.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:51:07 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:47:44 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Jolly", "Baani Leen Kaur", ""], ["Aggrawal", "Palash", ""], ["Gulati", "Amogh", ""], ["Sethi", "Amarjit Singh", ""], ["Kumaraguru", "Ponnurangam", ""], ["Sethi", "Tavpritesh", ""]]}, {"id": "2005.05525", "submitter": "Tomoki Hayashi", "authors": "Tomoki Hayashi and Shinji Watanabe", "title": "DiscreTalk: Text-to-Speech as a Machine Translation Problem", "comments": "Submitted to INTERSPEECH 2020. The demo is available on\n  https://kan-bayashi.github.io/DiscreTalk/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on\nneural machine translation (NMT). The proposed model consists of two\ncomponents; a non-autoregressive vector quantized variational autoencoder\n(VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model\nlearns a mapping function from a speech waveform into a sequence of discrete\nsymbols, and then the Transformer-NMT model is trained to estimate this\ndiscrete symbol sequence from a given input text. Since the VQ-VAE model can\nlearn such a mapping in a fully-data-driven manner, we do not need to consider\nhyperparameters of the feature extraction required in the conventional E2E-TTS\nmodels. Thanks to the use of discrete symbols, we can use various techniques\ndeveloped in NMT and automatic speech recognition (ASR) such as beam search,\nsubword units, and fusions with a language model. Furthermore, we can avoid an\nover smoothing problem of predicted features, which is one of the common issues\nin TTS. The experimental evaluation with the JSUT corpus shows that the\nproposed method outperforms the conventional Transformer-TTS model with a\nnon-autoregressive neural vocoder in naturalness, achieving the performance\ncomparable to the reconstruction of the VQ-VAE model.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 02:45:09 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Hayashi", "Tomoki", ""], ["Watanabe", "Shinji", ""]]}, {"id": "2005.05570", "submitter": "Rakesh Chada", "authors": "Rakesh Chada", "title": "Simultaneous paraphrasing and translation by fine-tuning Transformer\n  models", "comments": "Accepted to ACL 2020 4th workshop on Neural Generation and\n  Translation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the third place submission to the shared task on\nsimultaneous translation and paraphrasing for language education at the 4th\nworkshop on Neural Generation and Translation (WNGT) for ACL 2020. The final\nsystem leverages pre-trained translation models and uses a Transformer\narchitecture combined with an oversampling strategy to achieve a competitive\nperformance. This system significantly outperforms the baseline on Hungarian\n(27% absolute improvement in Weighted Macro F1 score) and Portuguese (33%\nabsolute improvement) languages.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 06:34:42 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Chada", "Rakesh", ""]]}, {"id": "2005.05592", "submitter": "Bo Xu", "authors": "Bo Xu, Cheng Lu, Yandong Guo and Jacob Wang", "title": "Discriminative Multi-modality Speech Recognition", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision is often used as a complementary modality for audio speech recognition\n(ASR), especially in the noisy environment where performance of solo audio\nmodality significantly deteriorates. After combining visual modality, ASR is\nupgraded to the multi-modality speech recognition (MSR). In this paper, we\npropose a two-stage speech recognition model. In the first stage, the target\nvoice is separated from background noises with help from the corresponding\nvisual information of lip movements, making the model 'listen' clearly. At the\nsecond stage, the audio modality combines visual modality again to better\nunderstand the speech by a MSR sub-network, further improving the recognition\nrate. There are some other key contributions: we introduce a pseudo-3D residual\nconvolution (P3D)-based visual front-end to extract more discriminative\nfeatures; we upgrade the temporal convolution block from 1D ResNet with the\ntemporal convolutional network (TCN), which is more suitable for the temporal\ntasks; the MSR sub-network is built on the top of Element-wise-Attention Gated\nRecurrent Unit (EleAtt-GRU), which is more effective than Transformer in long\nsequences. We conducted extensive experiments on the LRS3-TED and the LRW\ndatasets. Our two-stage model (audio enhanced multi-modality speech\nrecognition, AE-MSR) consistently achieves the state-of-the-art performance by\na significant margin, which demonstrates the necessity and effectiveness of\nAE-MSR.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 07:56:03 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 07:55:21 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Xu", "Bo", ""], ["Lu", "Cheng", ""], ["Guo", "Yandong", ""], ["Wang", "Jacob", ""]]}, {"id": "2005.05607", "submitter": "Yuting Wu", "authors": "Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang and Dongyan Zhao", "title": "Neighborhood Matching Network for Entity Alignment", "comments": "11 pages, accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural heterogeneity between knowledge graphs is an outstanding challenge\nfor entity alignment. This paper presents Neighborhood Matching Network (NMN),\na novel entity alignment framework for tackling the structural heterogeneity\nchallenge. NMN estimates the similarities between entities to capture both the\ntopological structure and the neighborhood difference. It provides two\ninnovative components for better learning representations for entity alignment.\nIt first uses a novel graph sampling method to distill a discriminative\nneighborhood for each entity. It then adopts a cross-graph neighborhood\nmatching module to jointly encode the neighborhood difference for a given\nentity pair. Such strategies allow NMN to effectively construct\nmatching-oriented entity representations while ignoring noisy neighbors that\nhave a negative impact on the alignment task. Extensive experiments performed\non three entity alignment datasets show that NMN can well estimate the\nneighborhood similarity in more tough cases and significantly outperforms 12\nprevious state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 08:26:15 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Wu", "Yuting", ""], ["Liu", "Xiao", ""], ["Feng", "Yansong", ""], ["Wang", "Zheng", ""], ["Zhao", "Dongyan", ""]]}, {"id": "2005.05635", "submitter": "Xinyan Xiao", "authors": "Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng\n  Wang, Feng Wu", "title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sentiment analysis has seen remarkable advance with the help of\npre-training approaches. However, sentiment knowledge, such as sentiment words\nand aspect-sentiment pairs, is ignored in the process of pre-training, despite\nthe fact that they are widely used in traditional sentiment analysis\napproaches. In this paper, we introduce Sentiment Knowledge Enhanced\nPre-training (SKEP) in order to learn a unified sentiment representation for\nmultiple sentiment analysis tasks. With the help of automatically-mined\nknowledge, SKEP conducts sentiment masking and constructs three sentiment\nknowledge prediction objectives, so as to embed sentiment information at the\nword, polarity and aspect level into pre-trained sentiment representation. In\nparticular, the prediction of aspect-sentiment pairs is converted into\nmulti-label classification, aiming to capture the dependency between words in a\npair. Experiments on three kinds of sentiment tasks show that SKEP\nsignificantly outperforms strong pre-training baseline, and achieves new\nstate-of-the-art results on most of the test datasets. We release our code at\nhttps://github.com/baidu/Senta.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:23:32 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 08:12:22 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Tian", "Hao", ""], ["Gao", "Can", ""], ["Xiao", "Xinyan", ""], ["Liu", "Hao", ""], ["He", "Bolei", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""], ["Wu", "Feng", ""]]}, {"id": "2005.05639", "submitter": "Michael Moortgat", "authors": "Michael Moortgat, Mehrnoosh Sadrzadeh, Gijs Wijnholds", "title": "A Frobenius Algebraic Analysis for Parasitic Gaps", "comments": "SemSpace 2019, to appear in Journal of Applied Logics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of parasitic gaps is an ostensible case of non-linearity\nin natural language composition. Existing categorial analyses, both in the\ntypelogical and in the combinatory traditions, rely on explicit forms of\nsyntactic copying. We identify two types of parasitic gapping where the\nduplication of semantic content can be confined to the lexicon. Parasitic gaps\nin adjuncts are analysed as forms of generalized coordination with a\npolymorphic type schema for the head of the adjunct phrase. For parasitic gaps\naffecting arguments of the same predicate, the polymorphism is associated with\nthe lexical item that introduces the primary gap. Our analysis is formulated in\nterms of Lambek calculus extended with structural control modalities. A\ncompositional translation relates syntactic types and derivations to the\ninterpreting compact closed category of finite dimensional vector spaces and\nlinear maps with Frobenius algebras over it. When interpreted over the\nnecessary semantic spaces, the Frobenius algebras provide the tools to model\nthe proposed instances of lexical polymorphism.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:36:15 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 16:41:57 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Moortgat", "Michael", ""], ["Sadrzadeh", "Mehrnoosh", ""], ["Wijnholds", "Gijs", ""]]}, {"id": "2005.05642", "submitter": "Zewang Zhang", "authors": "Zewang Zhang, Qiao Tian, Heng Lu, Ling-Hui Chen, Shan Liu", "title": "AdaDurIAN: Few-shot Adaptation for Neural Text-to-Speech with DurIAN", "comments": "Submitted to InterSpeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to leverage a DurIAN-based average model to\nenable a new speaker to have both accurate pronunciation and fluent\ncross-lingual speaking with very limited monolingual data. A weakness of the\nrecently proposed end-to-end text-to-speech (TTS) systems is that robust\nalignment is hard to achieve, which hinders it to scale well with very limited\ndata. To cope with this issue, we introduce AdaDurIAN by training an improved\nDurIAN-based average model and leverage it to few-shot learning with the shared\nspeaker-independent content encoder across different speakers. Several few-shot\nlearning tasks in our experiments show AdaDurIAN can outperform the baseline\nend-to-end system by a large margin. Subjective evaluations also show that\nAdaDurIAN yields higher mean opinion score (MOS) of naturalness and more\npreferences of speaker similarity. In addition, we also apply AdaDurIAN to\nemotion transfer tasks and demonstrate its promising performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:41:03 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Zewang", ""], ["Tian", "Qiao", ""], ["Lu", "Heng", ""], ["Chen", "Ling-Hui", ""], ["Liu", "Shan", ""]]}, {"id": "2005.05672", "submitter": "Sven Buechel", "authors": "Sven Buechel, Susanna R\\\"ucker, Udo Hahn", "title": "Learning and Evaluating Emotion Lexicons for 91 Languages", "comments": "ACL 2020 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion lexicons describe the affective meaning of words and thus constitute\na centerpiece for advanced sentiment and emotion analysis. Yet, manually\ncurated lexicons are only available for a handful of languages, leaving most\nlanguages of the world without such a precious resource for downstream\napplications. Even worse, their coverage is often limited both in terms of the\nlexical units they contain and the emotional variables they feature. In order\nto break this bottleneck, we here introduce a methodology for creating almost\narbitrarily large emotion lexicons for any target language. Our approach\nrequires nothing but a source language emotion lexicon, a bilingual word\ntranslation model, and a target language embedding model. Fulfilling these\nrequirements for 91 languages, we are able to generate representationally rich\nhigh-coverage lexicons comprising eight emotional variables with more than 100k\nlexical entries each. We evaluated the automatically generated lexicons against\nhuman judgment from 26 datasets, spanning 12 typologically diverse languages,\nand found that our approach produces results in line with state-of-the-art\nmonolingual approaches to lexicon creation and even surpasses human reliability\nfor some languages and variables. Code and data are available at\nhttps://github.com/JULIELab/MEmoLon archived under DOI\nhttps://doi.org/10.5281/zenodo.3779901.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 10:32:03 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Buechel", "Sven", ""], ["R\u00fccker", "Susanna", ""], ["Hahn", "Udo", ""]]}, {"id": "2005.05683", "submitter": "Fan Yin", "authors": "Fan Yin, Quanyu Long, Tao Meng, Kai-Wei Chang", "title": "On the Robustness of Language Encoders against Grammatical Errors", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a thorough study to diagnose the behaviors of pre-trained language\nencoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical\nerrors. Specifically, we collect real grammatical errors from non-native\nspeakers and conduct adversarial attacks to simulate these errors on clean text\ndata. We use this approach to facilitate debugging models on downstream\napplications. Results confirm that the performance of all tested models is\naffected but the degree of impact varies. To interpret model behaviors, we\nfurther design a linguistic acceptability task to reveal their abilities in\nidentifying ungrammatical sentences and the position of errors. We find that\nfixed contextual encoders with a simple classifier trained on the prediction of\nsentence correctness are able to locate error positions. We also design a cloze\ntest for BERT and discover that BERT captures the interaction between errors\nand specific tokens in context. Our results shed light on understanding the\nrobustness and behaviors of language encoders against grammatical errors.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 11:01:44 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Yin", "Fan", ""], ["Long", "Quanyu", ""], ["Meng", "Tao", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2005.05692", "submitter": "Ekaterina Kochmar", "authors": "Ekaterina Kochmar, Sian Gooding, and Matthew Shardlow", "title": "Detecting Multiword Expression Type Helps Lexical Complexity Assessment", "comments": "Accepted for publication at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiword expressions (MWEs) represent lexemes that should be treated as\nsingle lexical units due to their idiosyncratic nature. Multiple NLP\napplications have been shown to benefit from MWE identification, however the\nresearch on lexical complexity of MWEs is still an under-explored area. In this\nwork, we re-annotate the Complex Word Identification Shared Task 2018 dataset\nof Yimam et al. (2017), which provides complexity scores for a range of\nlexemes, with the types of MWEs. We release the MWE-annotated dataset with this\npaper, and we believe this dataset represents a valuable resource for the text\nsimplification community. In addition, we investigate which types of\nexpressions are most problematic for native and non-native readers. Finally, we\nshow that a lexical complexity assessment system benefits from the information\nabout MWE types.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 11:25:07 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Kochmar", "Ekaterina", ""], ["Gooding", "Sian", ""], ["Shardlow", "Matthew", ""]]}, {"id": "2005.05727", "submitter": "Ruiying Geng", "authors": "Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu", "title": "Dynamic Memory Induction Networks for Few-Shot Text Classification", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Dynamic Memory Induction Networks (DMIN) for few-shot\ntext classification. The model utilizes dynamic routing to provide more\nflexibility to memory-based few-shot learning in order to better adapt the\nsupport sets, which is a critical capacity of few-shot classification models.\nBased on that, we further develop induction models with query information,\naiming to enhance the generalization ability of meta-learning. The proposed\nmodel achieves new state-of-the-art results on the miniRCV1 and ODIC dataset,\nimproving the best performance (accuracy) by 2~4%. Detailed analysis is further\nperformed to show the effectiveness of each component.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 12:41:14 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Geng", "Ruiying", ""], ["Li", "Binhua", ""], ["Li", "Yongbin", ""], ["Sun", "Jian", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2005.05738", "submitter": "Antonio Toral", "authors": "Antonio Toral", "title": "Reassessing Claims of Human Parity and Super-Human Performance in\n  Machine Translation at WMT 2019", "comments": "Accepted at the 22nd Annual Conference of the European Association\n  for Machine Translation (EAMT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reassess the claims of human parity and super-human performance made at\nthe news shared task of WMT 2019 for three translation directions:\nEnglish-to-German, English-to-Russian and German-to-English. First we identify\nthree potential issues in the human evaluation of that shared task: (i) the\nlimited amount of intersentential context available, (ii) the limited\ntranslation proficiency of the evaluators and (iii) the use of a reference\ntranslation. We then conduct a modified evaluation taking these issues into\naccount. Our results indicate that all the claims of human parity and\nsuper-human performance made at WMT 2019 should be refuted, except the claim of\nhuman parity for English-to-German. Based on our findings, we put forward a set\nof recommendations and open questions for future assessments of human parity in\nmachine translation.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:09:29 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Toral", "Antonio", ""]]}, {"id": "2005.05754", "submitter": "Angrosh Mandya", "authors": "Angrosh Mandya, James O'Neill, Danushka Bollegala, and Frans Coenen", "title": "Do not let the history haunt you -- Mitigating Compounding Errors in\n  Conversational Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Conversational Question Answering (CoQA) task involves answering a\nsequence of inter-related conversational questions about a contextual\nparagraph. Although existing approaches employ human-written ground-truth\nanswers for answering conversational questions at test time, in a realistic\nscenario, the CoQA model will not have any access to ground-truth answers for\nthe previous questions, compelling the model to rely upon its own previously\npredicted answers for answering the subsequent questions. In this paper, we\nfind that compounding errors occur when using previously predicted answers at\ntest time, significantly lowering the performance of CoQA systems. To solve\nthis problem, we propose a sampling strategy that dynamically selects between\ntarget answers and model predictions during training, thereby closely\nsimulating the situation at test time. Further, we analyse the severity of this\nphenomena as a function of the question type, conversation length and domain\ntype.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:29:38 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Mandya", "Angrosh", ""], ["O'Neill", "James", ""], ["Bollegala", "Danushka", ""], ["Coenen", "Frans", ""]]}, {"id": "2005.05763", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Xinran Zhao, Yangqiu Song", "title": "WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for\n  Answering Winograd Schema Challenge", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the first comprehensive categorization of essential\ncommonsense knowledge for answering the Winograd Schema Challenge (WSC). For\neach of the questions, we invite annotators to first provide reasons for making\ncorrect decisions and then categorize them into six major knowledge categories.\nBy doing so, we better understand the limitation of existing methods (i.e.,\nwhat kind of knowledge cannot be effectively represented or inferred with\nexisting methods) and shed some light on the commonsense knowledge that we need\nto acquire in the future for better commonsense reasoning. Moreover, to\ninvestigate whether current WSC models can understand the commonsense or they\nsimply solve the WSC questions based on the statistical bias of the dataset, we\nleverage the collected reasons to develop a new task called WinoWhy, which\nrequires models to distinguish plausible reasons from very similar but wrong\nreasons for all WSC questions. Experimental results prove that even though\npre-trained language representation models have achieved promising progress on\nthe original WSC dataset, they are still struggling at WinoWhy. Further\nexperiments show that even though supervised models can achieve better\nperformance, the performance of these models can be sensitive to the dataset\ndistribution. WinoWhy and all codes are available at:\nhttps://github.com/HKUST-KnowComp/WinoWhy.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:40:06 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Hongming", ""], ["Zhao", "Xinran", ""], ["Song", "Yangqiu", ""]]}, {"id": "2005.05806", "submitter": "Bo Zheng", "authors": "Bo Zheng, Haoyang Wen, Yaobo Liang, Nan Duan, Wanxiang Che, Daxin\n  Jiang, Ming Zhou and Ting Liu", "title": "Document Modeling with Graph Attention Networks for Multi-grained\n  Machine Reading Comprehension", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Questions is a new challenging machine reading comprehension\nbenchmark with two-grained answers, which are a long answer (typically a\nparagraph) and a short answer (one or more entities inside the long answer).\nDespite the effectiveness of existing methods on this benchmark, they treat\nthese two sub-tasks individually during training while ignoring their\ndependencies. To address this issue, we present a novel multi-grained machine\nreading comprehension framework that focuses on modeling documents at their\nhierarchical nature, which are different levels of granularity: documents,\nparagraphs, sentences, and tokens. We utilize graph attention networks to\nobtain different levels of representations so that they can be learned\nsimultaneously. The long and short answers can be extracted from\nparagraph-level representation and token-level representation, respectively. In\nthis way, we can model the dependencies between the two-grained answers to\nprovide evidence for each other. We jointly train the two sub-tasks, and our\nexperiments show that our approach significantly outperforms previous systems\nat both long and short answer criteria.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:20:09 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 08:44:37 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zheng", "Bo", ""], ["Wen", "Haoyang", ""], ["Liang", "Yaobo", ""], ["Duan", "Nan", ""], ["Che", "Wanxiang", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""], ["Liu", "Ting", ""]]}, {"id": "2005.05814", "submitter": "Debanjan Ghosh", "authors": "Debanjan Ghosh and Avijit Vajpayee and Smaranda Muresan", "title": "A Report on the 2020 Sarcasm Detection Shared Task", "comments": "2nd Workshop on Figurative Language Processing (FigLang2020) at ACL\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting sarcasm and verbal irony is critical for understanding people's\nactual sentiments and beliefs. Thus, the field of sarcasm analysis has become a\npopular research problem in natural language processing. As the community\nworking on computational approaches for sarcasm detection is growing, it is\nimperative to conduct benchmarking studies to analyze the current\nstate-of-the-art, facilitating progress in this area. We report on the shared\ntask on sarcasm detection we conducted as a part of the 2nd Workshop on\nFigurative Language Processing (FigLang 2020) at ACL 2020.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:27:19 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 20:31:11 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ghosh", "Debanjan", ""], ["Vajpayee", "Avijit", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2005.05854", "submitter": "Preslav Nakov", "authors": "Giovanni Da San Martino, Shaden Shaar, Yifan Zhang, Seunghak Yu,\n  Alberto Barr\\'on-Cede\\~no, Preslav Nakov", "title": "Prta: A System to Support the Analysis of Propaganda Techniques in the\n  News", "comments": "propaganda, disinformation, fake news, media bias, COVID-19", "journal-ref": "ACL-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent events, such as the 2016 US Presidential Campaign, Brexit and the\nCOVID-19 \"infodemic\", have brought into the spotlight the dangers of online\ndisinformation. There has been a lot of research focusing on fact-checking and\ndisinformation detection. However, little attention has been paid to the\nspecific rhetorical and psychological techniques used to convey propaganda\nmessages. Revealing the use of such techniques can help promote media literacy\nand critical thinking, and eventually contribute to limiting the impact of\n\"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion\nTechniques Analyzer) allows users to explore the articles crawled on a regular\nbasis by highlighting the spans in which propaganda techniques occur and to\ncompare them on the basis of their use of propaganda techniques. The system\nfurther reports statistics about the use of such techniques, overall and over\ntime, or according to filtering criteria specified by the user based on time\ninterval, keywords, and/or political orientation of the media. Moreover, it\nallows users to analyze any text or URL through a dedicated interface or via an\nAPI. The system is available online: https://www.tanbih.org/prta\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:20:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Shaar", "Shaden", ""], ["Zhang", "Yifan", ""], ["Yu", "Seunghak", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.05864", "submitter": "Wenyu Du", "authors": "Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua\n  Bengio and Yue Zhang", "title": "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic\n  Distance Approach", "comments": "ACL20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly believed that knowledge of syntactic structure should improve\nlanguage modeling. However, effectively and computationally efficiently\nincorporating syntactic structure into neural language models has been a\nchallenging topic. In this paper, we make use of a multi-task objective, i.e.,\nthe models simultaneously predict words as well as ground truth parse trees in\na form called \"syntactic distances\", where information between these two\nseparate objectives shares the same intermediate representation. Experimental\nresults on the Penn Treebank and Chinese Treebank datasets show that when\nground truth parse trees are provided as additional training signals, the model\nis able to achieve lower perplexity and induce trees with better quality.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:35:00 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Du", "Wenyu", ""], ["Lin", "Zhouhan", ""], ["Shen", "Yikang", ""], ["O'Donnell", "Timothy J.", ""], ["Bengio", "Yoshua", ""], ["Zhang", "Yue", ""]]}, {"id": "2005.05909", "submitter": "John Morris", "authors": "John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin and\n  Yanjun Qi", "title": "TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and\n  Adversarial Training in NLP", "comments": "6 pages. More details are shared at\n  https://github.com/QData/TextAttack", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been substantial research using adversarial attacks to\nanalyze NLP models, each attack is implemented in its own code repository. It\nremains challenging to develop NLP attacks and utilize them to improve model\nperformance. This paper introduces TextAttack, a Python framework for\nadversarial attacks, data augmentation, and adversarial training in NLP.\nTextAttack builds attacks from four components: a goal function, a set of\nconstraints, a transformation, and a search method. TextAttack's modular design\nenables researchers to easily construct attacks from combinations of novel and\nexisting components. TextAttack provides implementations of 16 adversarial\nattacks from the literature and supports a variety of models and datasets,\nincluding BERT and other transformers, and all GLUE tasks. TextAttack also\nincludes data augmentation and adversarial training modules for using\ncomponents of adversarial attacks to improve model accuracy and robustness.\nTextAttack is democratizing NLP: anyone can try data augmentation and\nadversarial training on any model or dataset, with just a few lines of code.\nCode and tutorials are available at https://github.com/QData/TextAttack.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:33:35 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 17:37:21 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 19:33:10 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 00:10:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Morris", "John X.", ""], ["Lifland", "Eli", ""], ["Yoo", "Jin Yong", ""], ["Grigsby", "Jake", ""], ["Jin", "Di", ""], ["Qi", "Yanjun", ""]]}, {"id": "2005.05921", "submitter": "Jae Yeon Kim", "authors": "Jae Yeon Kim, Carlos Ortiz, Sarah Nam, Sarah Santiago, Vivek Datta", "title": "Intersectional Bias in Hate Speech and Abusive Language Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms are widely applied to detect hate speech and abusive language in\nsocial media. We investigated whether the human-annotated data used to train\nthese algorithms are biased. We utilized a publicly available annotated Twitter\ndataset (Founta et al. 2018) and classified the racial, gender, and party\nidentification dimensions of 99,996 tweets. The results showed that African\nAmerican tweets were up to 3.7 times more likely to be labeled as abusive, and\nAfrican American male tweets were up to 77% more likely to be labeled as\nhateful compared to the others. These patterns were statistically significant\nand robust even when party identification was added as a control variable. This\nstudy provides the first systematic evidence on intersectional bias in datasets\nof hate speech and abusive language.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:58:48 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 08:27:01 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 05:49:19 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Kim", "Jae Yeon", ""], ["Ortiz", "Carlos", ""], ["Nam", "Sarah", ""], ["Santiago", "Sarah", ""], ["Datta", "Vivek", ""]]}, {"id": "2005.05927", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Mitchell Stern, Dan Klein", "title": "Semantic Scaffolds for Pseudocode-to-Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for program generation based on semantic scaffolds,\nlightweight structures representing the high-level semantic and syntactic\ncomposition of a program. By first searching over plausible scaffolds then\nusing these as constraints for a beam search over programs, we achieve better\ncoverage of the search space when compared with existing techniques. We apply\nour hierarchical search method to the SPoC dataset for pseudocode-to-code\ngeneration, in which we are given line-level natural language pseudocode\nannotations and aim to produce a program satisfying execution-based test cases.\nBy using semantic scaffolds during inference, we achieve a 10% absolute\nimprovement in top-100 accuracy over the previous state-of-the-art.\nAdditionally, we require only 11 candidates to reach the top-3000 performance\nof the previous best approach when tested against unseen problems,\ndemonstrating a substantial improvement in efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:10:13 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Stern", "Mitchell", ""], ["Klein", "Dan", ""]]}, {"id": "2005.05954", "submitter": "Md. Tawkat Islam Khondaker", "authors": "Junaed Younus Khan, Md. Tawkat Islam Khondaker, Iram Tazim Hoque,\n  Hamada Al-Absi, Mohammad Saifur Rahman, Tanvir Alam, M. Sohel Rahman", "title": "COVID-19Base: A knowledgebase to explore biomedical entities related to\n  COVID-19", "comments": "10 pages, 3 figures", "journal-ref": "JMIR Med Inform 2020;8(11):e21648", "doi": "10.2196/21648", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting COVID-19Base, a knowledgebase highlighting the biomedical\nentities related to COVID-19 disease based on literature mining. To develop\nCOVID-19Base, we mine the information from publicly available scientific\nliterature and related public resources. We considered seven topic-specific\ndictionaries, including human genes, human miRNAs, human lncRNAs, diseases,\nProtein Databank, drugs, and drug side effects, are integrated to mine all\nscientific evidence related to COVID-19. We have employed an automated\nliterature mining and labeling system through a novel approach to measure the\neffectiveness of drugs against diseases based on natural language processing,\nsentiment analysis, and deep learning. To the best of our knowledge, this is\nthe first knowledgebase dedicated to COVID-19, which integrates such large\nvariety of related biomedical entities through literature mining. Proper\ninvestigation of the mined biomedical entities along with the identified\ninteractions among those, reported in COVID-19Base, would help the research\ncommunity to discover possible ways for the therapeutic treatment of COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:55:00 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Khan", "Junaed Younus", ""], ["Khondaker", "Md. Tawkat Islam", ""], ["Hoque", "Iram Tazim", ""], ["Al-Absi", "Hamada", ""], ["Rahman", "Mohammad Saifur", ""], ["Alam", "Tanvir", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "2005.05957", "submitter": "Rafael Valle", "authors": "Rafael Valle, Kevin Shih, Ryan Prenger, Bryan Catanzaro", "title": "Flowtron: an Autoregressive Flow-based Generative Network for\n  Text-to-Speech Synthesis", "comments": "10 pages, 7 pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose Flowtron: an autoregressive flow-based generative\nnetwork for text-to-speech synthesis with control over speech variation and\nstyle transfer. Flowtron borrows insights from IAF and revamps Tacotron in\norder to provide high-quality and expressive mel-spectrogram synthesis.\nFlowtron is optimized by maximizing the likelihood of the training data, which\nmakes training simple and stable. Flowtron learns an invertible mapping of data\nto a latent space that can be manipulated to control many aspects of speech\nsynthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores\n(MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech\nquality. In addition, we provide results on control of speech variation,\ninterpolation between samples and style transfer between speakers seen and\nunseen during training. Code and pre-trained models will be made publicly\navailable at https://github.com/NVIDIA/flowtron\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:57:17 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 01:54:35 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 15:10:18 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Valle", "Rafael", ""], ["Shih", "Kevin", ""], ["Prenger", "Ryan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2005.06012", "submitter": "Abdelrahim Elmadany", "authors": "Muhammad Abdul-Mageed, AbdelRahim Elmadany, El Moatez Billah Nagoudi,\n  Dinesh Pabbi, Kunal Verma, Rannie Lin", "title": "Mega-COV: A Billion-Scale Dataset of 100+ Languages for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Mega-COV, a billion-scale dataset from Twitter for studying\nCOVID-19. The dataset is diverse (covers 268 countries), longitudinal (goes as\nback as 2007), multilingual (comes in 100+ languages), and has a significant\nnumber of location-tagged tweets (~169M tweets). We release tweet IDs from the\ndataset. We also develop and release two powerful models, one for identifying\nwhether or not a tweet is related to the pandemic (best F1=97%) and another for\ndetecting misinformation about COVID-19 (best F1=92%). A human annotation study\nreveals the utility of our models on a subset of Mega-COV. Our data and models\ncan be useful for studying a wide host of phenomena related to the pandemic.\nMega-COV and our models are publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:23:27 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 23:57:50 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 19:15:42 GMT"}, {"version": "v4", "created": "Fri, 5 Feb 2021 22:19:06 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Elmadany", "AbdelRahim", ""], ["Nagoudi", "El Moatez Billah", ""], ["Pabbi", "Dinesh", ""], ["Verma", "Kunal", ""], ["Lin", "Rannie", ""]]}, {"id": "2005.06035", "submitter": "Chen Zheng", "authors": "Chen Zheng, Quan Guo, Parisa Kordjamshidi", "title": "Cross-Modality Relevance for Reasoning on Language and Vision", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the challenge of learning and reasoning over language\nand vision data for the related downstream tasks such as visual question\nanswering (VQA) and natural language for visual reasoning (NLVR). We design a\nnovel cross-modality relevance module that is used in an end-to-end framework\nto learn the relevance representation between components of various input\nmodalities under the supervision of a target task, which is more generalizable\nto unobserved data compared to merely reshaping the original representation\nspace. In addition to modeling the relevance between the textual entities and\nvisual entities, we model the higher-order relevance between entity relations\nin the text and object relations in the image. Our proposed approach shows\ncompetitive performance on two different language and vision tasks using public\nbenchmarks and improves the state-of-the-art published results. The learned\nalignments of input spaces and their relevance representations by NLVR task\nboost the training efficiency of VQA task.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:17:25 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zheng", "Chen", ""], ["Guo", "Quan", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2005.06058", "submitter": "Preslav Nakov", "authors": "Shaden Shaar, Giovanni Da San Martino, Nikolay Babulkov, Preslav Nakov", "title": "That is a Known Lie: Detecting Previously Fact-Checked Claims", "comments": "detecting previously fact-checked claims, fact-checking,\n  disinformation, fake news, social media, political debates", "journal-ref": "ACL-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of \"fake news\" has triggered a number of responses,\nmost notably the emergence of several manual fact-checking initiatives. As a\nresult and over time, a large number of fact-checked claims have been\naccumulated, which increases the likelihood that a new claim in social media or\na new statement by a politician might have already been fact-checked by some\ntrusted fact-checking organization, as viral claims often come back after a\nwhile in social media, and politicians like to repeat their favorite\nstatements, true or false, over and over again. As manual fact-checking is very\ntime-consuming (and fully automatic fact-checking has credibility issues), it\nis important to try to save this effort and to avoid wasting time on claims\nthat have already been fact-checked. Interestingly, despite the importance of\nthe task, it has been largely ignored by the research community so far. Here,\nwe aim to bridge this gap. In particular, we formulate the task and we discuss\nhow it relates to, but also differs from, previous work. We further create a\nspecialized dataset, which we release to the research community. Finally, we\npresent learning-to-rank experiments that demonstrate sizable improvements over\nstate-of-the-art retrieval and textual similarity approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:25:37 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Shaar", "Shaden", ""], ["Martino", "Giovanni Da San", ""], ["Babulkov", "Nikolay", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.06059", "submitter": "Carlo Lipizzi", "authors": "Carlo Lipizzi, Dario Borrelli, Fernanda de Oliveira Capela", "title": "A computational model implementing subjectivity with the 'Room Theory'.\n  The case of detecting Emotion from Text", "comments": "15 pages, 9 figures, 3 Tables - Under second round of review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a new method to consider subjectivity and general\ncontext dependency in text analysis and uses as example the detection of\nemotions conveyed in text. The proposed method takes into account subjectivity\nusing a computational version of the Framework Theory by Marvin Minsky (1974)\nleveraging on the Word2Vec approach to text vectorization by Mikolov et al.\n(2013), used to generate distributed representation of words based on the\ncontext where they appear. Our approach is based on three components: 1. a\nframework/'room' representing the point of view; 2. a benchmark representing\nthe criteria for the analysis - in this case the emotion classification, from a\nstudy of human emotions by Robert Plutchik (1980); and 3. the document to be\nanalyzed. By using similarity measure between words, we are able to extract the\nrelative relevance of the elements in the benchmark - intensities of emotions\nin our case study - for the document to be analyzed. Our method provides a\nmeasure that take into account the point of view of the entity reading the\ndocument. This method could be applied to all the cases where evaluating\nsubjectivity is relevant to understand the relative value or meaning of a text.\nSubjectivity can be not limited to human reactions, but it could be used to\nprovide a text with an interpretation related to a given domain (\"room\"). To\nevaluate our method, we used a test case in the political domain.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:26:04 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Lipizzi", "Carlo", ""], ["Borrelli", "Dario", ""], ["Capela", "Fernanda de Oliveira", ""]]}, {"id": "2005.06065", "submitter": "Ali Abavisani", "authors": "Ali Abavisani and Mark Hasegawa-Johnson", "title": "Automatic Estimation of Intelligibility Measure for Consonants in Speech", "comments": "5 pages, 1 figure, 7 tables, submitted to Inter Speech 2020\n  Conference", "journal-ref": null, "doi": "10.21437/Interspeech.2020-2121", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we provide a model to estimate a real-valued measure of the\nintelligibility of individual speech segments. We trained regression models\nbased on Convolutional Neural Networks (CNN) for stop consonants\n\\textipa{/p,t,k,b,d,g/} associated with vowel \\textipa{/A/}, to estimate the\ncorresponding Signal to Noise Ratio (SNR) at which the Consonant-Vowel (CV)\nsound becomes intelligible for Normal Hearing (NH) ears. The intelligibility\nmeasure for each sound is called SNR$_{90}$, and is defined to be the SNR level\nat which human participants are able to recognize the consonant at least 90\\%\ncorrectly, on average, as determined in prior experiments with NH subjects.\nPerformance of the CNN is compared to a baseline prediction based on automatic\nspeech recognition (ASR), specifically, a constant offset subtracted from the\nSNR at which the ASR becomes capable of correctly labeling the consonant.\nCompared to baseline, our models were able to accurately estimate the\nSNR$_{90}$~intelligibility measure with less than 2 [dB$^2$] Mean Squared Error\n(MSE) on average, while the baseline ASR-defined measure computes\nSNR$_{90}$~with a variance of 5.2 to 26.6 [dB$^2$], depending on the consonant.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:45:20 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 21:37:58 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Abavisani", "Ali", ""], ["Hasegawa-Johnson", "Mark", ""]]}, {"id": "2005.06070", "submitter": "Ali H\\\"urriyeto\\u{g}lu", "authors": "Ali H\\\"urriyeto\\u{g}lu, Vanni Zavarella, Hristo Tanev, Erdem\n  Y\\\"or\\\"uk, Ali Safaya, Osman Mutlu", "title": "Automated Extraction of Socio-political Events from News (AESPEN):\n  Workshop and Shared Task Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our effort on automated extraction of socio-political events from\nnews in the scope of a workshop and a shared task we organized at Language\nResources and Evaluation Conference (LREC 2020). We believe the event\nextraction studies in computational linguistics and social and political\nsciences should further support each other in order to enable large scale\nsocio-political event information collection across sources, countries, and\nlanguages. The event consists of regular research papers and a shared task,\nwhich is about event sentence coreference identification (ESCI), tracks. All\nsubmissions were reviewed by five members of the program committee. The\nworkshop attracted research papers related to evaluation of machine learning\nmethodologies, language resources, material conflict forecasting, and a shared\ntask participation report in the scope of socio-political event information\ncollection. It has shown us the volume and variety of both the data sources and\nevent information collection approaches related to socio-political events and\nthe need to fill the gap between automated text processing techniques and\nrequirements of social and political sciences.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 22:07:14 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["H\u00fcrriyeto\u011flu", "Ali", ""], ["Zavarella", "Vanni", ""], ["Tanev", "Hristo", ""], ["Y\u00f6r\u00fck", "Erdem", ""], ["Safaya", "Ali", ""], ["Mutlu", "Osman", ""]]}, {"id": "2005.06114", "submitter": "Mohammad Shoeybi", "authors": "Alex Boyd, Raul Puri, Mohammad Shoeybi, Mostofa Patwary, and Bryan\n  Catanzaro", "title": "Large Scale Multi-Actor Generative Dialog Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and\nengaging conversations with a user; however, they typically exhibit either\ninconsistent personality across conversations or the average personality of all\nusers. This paper addresses these issues by controlling an agent's persona upon\ngeneration via conditioning on prior conversations of a target actor. In doing\nso, we are able to utilize more abstract patterns within a person's speech and\nbetter emulate them in generated responses. This work introduces the Generative\nConversation Control model, an augmented and fine-tuned GPT-2 language model\nthat conditions on past reference conversations to probabilistically model\nmulti-turn conversations in the actor's persona. We introduce an accompanying\ndata collection procedure to obtain 10.3M conversations from 6 months worth of\nReddit comments. We demonstrate that scaling model sizes from 117M to 8.3B\nparameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held\nout Reddit conversations. Increasing model scale yielded similar improvements\nin human evaluations that measure preference of model samples to the held out\ntarget distribution in terms of realism (31% increased to 37% preference),\nstyle matching (37% to 42%), grammar and content quality (29% to 42%), and\nconversation coherency (32% to 40%). We find that conditionally modeling past\nconversations improves perplexity by 0.47 in automatic evaluations. Through\nhuman trials we identify positive trends between conditional modeling and style\nmatching and outline steps to further improve persona control.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 01:56:00 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Boyd", "Alex", ""], ["Puri", "Raul", ""], ["Shoeybi", "Mohammad", ""], ["Patwary", "Mostofa", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2005.06117", "submitter": "Vivek Gupta", "authors": "Vivek Gupta, Maitrey Mehta, Pegah Nokhiz and Vivek Srikumar", "title": "INFOTABS: Inference on Tables as Semi-structured Data", "comments": "16 pages, 6 figures, 14 Tables, ACL 2020, Project Page:\n  https://infotabs.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we observe that semi-structured tabulated text is ubiquitous;\nunderstanding them requires not only comprehending the meaning of text\nfragments, but also implicit relationships between them. We argue that such\ndata can prove as a testing ground for understanding how we reason about\ninformation. To study this, we introduce a new dataset called INFOTABS,\ncomprising of human-written textual hypotheses based on premises that are\ntables extracted from Wikipedia info-boxes. Our analysis shows that the\nsemi-structured, multi-domain and heterogeneous nature of the premises admits\ncomplex, multi-faceted reasoning. Experiments reveal that, while human\nannotators agree on the relationships between a table-hypothesis pair, several\nstandard modeling strategies are unsuccessful at the task, suggesting that\nreasoning about tables can pose a difficult modeling challenge.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 02:07:54 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Gupta", "Vivek", ""], ["Mehta", "Maitrey", ""], ["Nokhiz", "Pegah", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2005.06123", "submitter": "Ming-Chang Chiu", "authors": "Ming-Chang Chiu, Tiantian Feng, Xiang Ren, Shrikanth Narayanan", "title": "Screenplay Quality Assessment: Can We Predict Who Gets Nominated?", "comments": "4 pages, 3 figures, accepted to ACL NUSE workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding which scripts to turn into movies is a costly and time-consuming\nprocess for filmmakers. Thus, building a tool to aid script selection, an\ninitial phase in movie production, can be very beneficial. Toward that goal, in\nthis work, we present a method to evaluate the quality of a screenplay based on\nlinguistic cues. We address this in a two-fold approach: (1) we define the task\nas predicting nominations of scripts at major film awards with the hypothesis\nthat the peer-recognized scripts should have a greater chance to succeed. (2)\nbased on industry opinions and narratology, we extract and integrate\ndomain-specific features into common classification techniques. We face two\nchallenges (1) scripts are much longer than other document datasets (2)\nnominated scripts are limited and thus difficult to collect. However, with\nnarratology-inspired modeling and domain features, our approach offers clear\nimprovements over strong baselines. Our work provides a new approach for future\nwork in screenplay analysis.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 02:39:56 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Chiu", "Ming-Chang", ""], ["Feng", "Tiantian", ""], ["Ren", "Xiang", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2005.06128", "submitter": "Zhiliang Tian", "authors": "Zhiliang Tian, Wei Bi, Dongkyu Lee, Lanqing Xue, Yiping Song,\n  Xiaojiang Liu, Nevin L. Zhang", "title": "Response-Anticipated Memory for On-Demand Knowledge Integration in\n  Response Generation", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural conversation models are known to generate appropriate but\nnon-informative responses in general. A scenario where informativeness can be\nsignificantly enhanced is Conversing by Reading (CbR), where conversations take\nplace with respect to a given external document. In previous work, the external\ndocument is utilized by (1) creating a context-aware document memory that\nintegrates information from the document and the conversational context, and\nthen (2) generating responses referring to the memory. In this paper, we\npropose to create the document memory with some anticipated responses in mind.\nThis is achieved using a teacher-student framework. The teacher is given the\nexternal document, the context, and the ground-truth response, and learns how\nto build a response-aware document memory from three sources of information.\nThe student learns to construct a response-anticipated document memory from the\nfirst two sources, and the teacher's insight on memory creation. Empirical\nresults show that our model outperforms the previous state-of-the-art for the\nCbR task.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 03:09:58 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Tian", "Zhiliang", ""], ["Bi", "Wei", ""], ["Lee", "Dongkyu", ""], ["Xue", "Lanqing", ""], ["Song", "Yiping", ""], ["Liu", "Xiaojiang", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "2005.06166", "submitter": "Boliang Zhang", "authors": "Boliang Zhang, Ajay Nagesh, and Kevin Knight", "title": "Parallel Corpus Filtering via Pre-trained Language Models", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-crawled data provides a good source of parallel corpora for training\nmachine translation models. It is automatically obtained, but extremely noisy,\nand recent work shows that neural machine translation systems are more\nsensitive to noise than traditional statistical machine translation methods. In\nthis paper, we propose a novel approach to filter out noisy sentence pairs from\nweb-crawled corpora via pre-trained language models. We measure sentence\nparallelism by leveraging the multilingual capability of BERT and use the\nGenerative Pre-training (GPT) language model as a domain filter to balance data\ndomains. We evaluate the proposed method on the WMT 2018 Parallel Corpus\nFiltering shared task, and on our own web-crawled Japanese-Chinese parallel\ncorpus. Our method significantly outperforms baselines and achieves a new\nstate-of-the-art. In an unsupervised setting, our method achieves comparable\nperformance to the top-1 supervised method. We also evaluate on a web-crawled\nJapanese-Chinese parallel corpus that we make publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 06:06:23 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhang", "Boliang", ""], ["Nagesh", "Ajay", ""], ["Knight", "Kevin", ""]]}, {"id": "2005.06249", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Hai Zhao, Rui Wang", "title": "Machine Reading Comprehension: The Role of Contextualized Language\n  Models and Beyond", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension (MRC) aims to teach machines to read and\ncomprehend human languages, which is a long-standing goal of natural language\nprocessing (NLP). With the burst of deep neural networks and the evolution of\ncontextualized language models (CLMs), the research of MRC has experienced two\nsignificant breakthroughs. MRC and CLM, as a phenomenon, have a great impact on\nthe NLP community. In this survey, we provide a comprehensive and comparative\nreview on MRC covering overall research topics about 1) the origin and\ndevelopment of MRC and CLM, with a particular focus on the role of CLMs; 2) the\nimpact of MRC and CLM to the NLP community; 3) the definition, datasets, and\nevaluation of MRC; 4) general MRC architecture and technical methods in the\nview of two-stage Encoder-Decoder solving architecture from the insights of the\ncognitive process of humans; 5) previous highlights, emerging topics, and our\nempirical analysis, among which we especially focus on what works in different\nperiods of MRC researches. We propose a full-view categorization and new\ntaxonomies on these topics. The primary views we have arrived at are that 1)\nMRC boosts the progress from language processing to understanding; 2) the rapid\nimprovement of MRC systems greatly benefits from the development of CLMs; 3)\nthe theme of MRC is gradually moving from shallow text matching to cognitive\nreasoning.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 10:58:50 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""]]}, {"id": "2005.06251", "submitter": "Tao Meng", "authors": "Shengyu Jia, Tao Meng, Jieyu Zhao and Kai-Wei Chang", "title": "Mitigating Gender Bias Amplification in Distribution by Posterior\n  Regularization", "comments": "7 pages, 3 figures, published in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced machine learning techniques have boosted the performance of natural\nlanguage processing. Nevertheless, recent studies, e.g., Zhao et al. (2017)\nshow that these techniques inadvertently capture the societal bias hidden in\nthe corpus and further amplify it. However, their analysis is conducted only on\nmodels' top predictions. In this paper, we investigate the gender bias\namplification issue from the distribution perspective and demonstrate that the\nbias is amplified in the view of predicted probability distribution over\nlabels. We further propose a bias mitigation approach based on posterior\nregularization. With little performance loss, our method can almost remove the\nbias amplification in the distribution. Our study sheds the light on\nunderstanding the bias amplification.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:07:10 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Jia", "Shengyu", ""], ["Meng", "Tao", ""], ["Zhao", "Jieyu", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2005.06282", "submitter": "Sudipto Mukherjee", "authors": "Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed\n  Hassan Awadallah, Ryen White", "title": "Smart To-Do : Automatic Generation of To-Do Items from Emails", "comments": "58th annual meeting of the Association for Computational Linguistics\n  (ACL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent features in email service applications aim to increase\nproductivity by helping people organize their folders, compose their emails and\nrespond to pending tasks. In this work, we explore a new application,\nSmart-To-Do, that helps users with task management over emails. We introduce a\nnew task and dataset for automatically generating To-Do items from emails where\nthe sender has promised to perform an action. We design a two-stage process\nleveraging recent advances in neural text generation and sequence-to-sequence\nlearning, obtaining BLEU and ROUGE scores of 0:23 and 0:63 for this task. To\nthe best of our knowledge, this is the first work to address the problem of\ncomposing To-Do items from emails.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 02:21:40 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Mukherjee", "Sudipto", ""], ["Mukherjee", "Subhabrata", ""], ["Hasegawa", "Marcello", ""], ["Awadallah", "Ahmed Hassan", ""], ["White", "Ryen", ""]]}, {"id": "2005.06312", "submitter": "Guoshun Nan Dr", "authors": "Guoshun Nan, Zhijiang Guo, Ivan Sekuli\\'c, Wei Lu", "title": "Reasoning with Latent Structure Refinement for Document-Level Relation\n  Extraction", "comments": "Appeared in the proceedings of ACL 2020 (Long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction requires integrating information within\nand across multiple sentences of a document and capturing complex interactions\nbetween inter-sentence entities. However, effective aggregation of relevant\ninformation in the document remains a challenging research question. Existing\napproaches construct static document-level graphs based on syntactic trees,\nco-references or heuristics from the unstructured text to model the\ndependencies. Unlike previous methods that may not be able to capture rich\nnon-local interactions for inference, we propose a novel model that empowers\nthe relational reasoning across sentences by automatically inducing the latent\ndocument-level graph. We further develop a refinement strategy, which enables\nthe model to incrementally aggregate relevant information for multi-hop\nreasoning. Specifically, our model achieves an F1 score of 59.05 on a\nlarge-scale document-level dataset (DocRED), significantly improving over the\nprevious results, and also yields new state-of-the-art results on the CDR and\nGDA dataset. Furthermore, extensive analyses show that the model is able to\ndiscover more accurate inter-sentence relations.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:36:09 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 11:32:45 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 15:55:52 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Nan", "Guoshun", ""], ["Guo", "Zhijiang", ""], ["Sekuli\u0107", "Ivan", ""], ["Lu", "Wei", ""]]}, {"id": "2005.06370", "submitter": "Tomer Wullach", "authors": "Tomer Wullach, Amir Adler, Einat Minkov", "title": "Towards Hate Speech Detection at Large via Deep Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech detection is a critical problem in social media platforms, being\noften accused for enabling the spread of hatred and igniting physical violence.\nHate speech detection requires overwhelming resources including\nhigh-performance computing for online posts and tweets monitoring as well as\nthousands of human experts for daily screening of suspected posts or tweets.\nRecently, Deep Learning (DL)-based solutions have been proposed for automatic\ndetection of hate speech, using modest-sized training datasets of few thousands\nof hate speech sequences. While these methods perform well on the specific\ndatasets, their ability to detect new hate speech sequences is limited and has\nnot been investigated. Being a data-driven approach, it is well known that DL\nsurpasses other methods whenever a scale-up in train dataset size and diversity\nis achieved. Therefore, we first present a dataset of 1 million realistic hate\nand non-hate sequences, produced by a deep generative language model. We\nfurther utilize the generated dataset to train a well-studied DL-based hate\nspeech detector, and demonstrate consistent and significant performance\nimprovements across five public hate speech datasets. Therefore, the proposed\nsolution enables high sensitivity detection of a very large variety of hate\nspeech sequences, paving the way to a fully automatic solution.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:25:59 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Wullach", "Tomer", ""], ["Adler", "Amir", ""], ["Minkov", "Einat", ""]]}, {"id": "2005.06376", "submitter": "Petros Stavropoulos", "authors": "Petros Stavropoulos, Dimitris Pappas, Ion Androutsopoulos, Ryan\n  McDonald", "title": "BIOMRC: A Dataset for Biomedical Machine Reading Comprehension", "comments": "10 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care\nwas taken to reduce noise, compared to the previous BIOREAD dataset of Pappas\net al. (2018). Experiments show that simple heuristics do not perform well on\nthe new dataset, and that two neural MRC models that had been tested on BIOREAD\nperform much better on BIOMRC, indicating that the new dataset is indeed less\nnoisy or at least that its task is more feasible. Non-expert human performance\nis also higher on the new dataset compared to BIOREAD, and biomedical experts\nperform even better. We also introduce a new BERT-based MRC model, the best\nversion of which substantially outperforms all other methods tested, reaching\nor surpassing the accuracy of biomedical experts in some experiments. We make\nthe new dataset available in three different sizes, also releasing our code,\nand providing a leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:38:12 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Stavropoulos", "Petros", ""], ["Pappas", "Dimitris", ""], ["Androutsopoulos", "Ion", ""], ["McDonald", "Ryan", ""]]}, {"id": "2005.06377", "submitter": "Forrest Bao", "authors": "Forrest Sheng Bao, Hebi Li, Ge Luo, Cen Chen, Yinfei Yang, Youbiao He,\n  Minghui Qiu", "title": "End-to-end Semantics-based Summary Quality Assessment for\n  Single-document Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical automatic summary evaluation metrics, such as ROUGE, suffer from\ntwo drawbacks. First, semantic similarity and linguistic quality are not\ncaptured well. Second, a reference summary, which is expensive or impossible to\nobtain in many cases, is needed. Existing efforts to address the two drawbacks\nare done separately and have limitations. To holistically address them, we\nintroduce an end-to-end approach for summary quality assessment by leveraging\nsentence or document embedding and introducing two negative sampling approaches\nto create training data for this supervised approach. The proposed approach\nexhibits promising results on several summarization datasets of various domains\nincluding news, legislative bills, scientific papers, and patents. When rating\nmachine-generated summaries in TAC2010, our approach outperforms ROUGE in terms\nof linguistic quality, and achieves a correlation coefficient of up to 0.5702\nwith human evaluations in terms of modified pyramid scores. We hope our\napproach can facilitate summarization research or applications when reference\nsummaries are infeasible or costly to obtain, or when linguistic quality is a\nfocus.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:40:13 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 22:43:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bao", "Forrest Sheng", ""], ["Li", "Hebi", ""], ["Luo", "Ge", ""], ["Chen", "Cen", ""], ["Yang", "Yinfei", ""], ["He", "Youbiao", ""], ["Qiu", "Minghui", ""]]}, {"id": "2005.06383", "submitter": "Sriram Krishnan", "authors": "Sriram Krishnan and Amba Kulkarni", "title": "Sanskrit Segmentation Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally analyzing Sanskrit texts requires proper segmentation in the\ninitial stages. There have been various tools developed for Sanskrit text\nsegmentation. Of these, G\\'erard Huet's Reader in the Sanskrit Heritage Engine\nanalyzes the input text and segments it based on the word parameters - phases\nlike iic, ifc, Pr, Subst, etc., and sandhi (or transition) that takes place at\nthe end of a word with the initial part of the next word. And it enlists all\nthe possible solutions differentiating them with the help of the phases. The\nphases and their analyses have their use in the domain of sentential parsers.\nIn segmentation, though, they are not used beyond deciding whether the words\nformed with the phases are morphologically valid. This paper tries to modify\nthe above segmenter by ignoring the phase details (except for a few cases), and\nalso proposes a probability function to prioritize the list of solutions to\nbring up the most valid solutions at the top.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:50:03 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Krishnan", "Sriram", ""], ["Kulkarni", "Amba", ""]]}, {"id": "2005.06386", "submitter": "Peter Ormosi", "authors": "Ivan Slobozhan, Peter Ormosi, Rajesh Sharma", "title": "Which bills are lobbied? Predicting and interpreting lobbying activity\n  in the US", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CL cs.LG q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using lobbying data from OpenSecrets.org, we offer several experiments\napplying machine learning techniques to predict if a piece of legislation (US\nbill) has been subjected to lobbying activities or not. We also investigate the\ninfluence of the intensity of the lobbying activity on how discernible a\nlobbied bill is from one that was not subject to lobbying. We compare the\nperformance of a number of different models (logistic regression, random\nforest, CNN and LSTM) and text embedding representations (BOW, TF-IDF, GloVe,\nLaw2Vec). We report results of above 0.85% ROC AUC scores, and 78% accuracy.\nModel performance significantly improves (95% ROC AUC, and 88% accuracy) when\nbills with higher lobbying intensity are looked at. We also propose a method\nthat could be used for unlabelled data. Through this we show that there is a\nconsiderably large number of previously unlabelled US bills where our\npredictions suggest that some lobbying activity took place. We believe our\nmethod could potentially contribute to the enforcement of the US Lobbying\nDisclosure Act (LDA) by indicating the bills that were likely to have been\naffected by lobbying but were not filed as such.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:46:33 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Slobozhan", "Ivan", ""], ["Ormosi", "Peter", ""], ["Sharma", "Rajesh", ""]]}, {"id": "2005.06409", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Zineng Tang, Mohit Bansal", "title": "Dense-Caption Matching and Frame-Selection Gating for Temporal\n  Localization in VideoQA", "comments": "ACL 2020 (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos convey rich information. Dynamic spatio-temporal relationships between\npeople/objects, and diverse multimodal events are present in a video clip.\nHence, it is important to develop automated models that can accurately extract\nsuch information from videos. Answering questions on videos is one of the tasks\nwhich can evaluate such AI abilities. In this paper, we propose a video\nquestion answering model which effectively integrates multi-modal input sources\nand finds the temporally relevant information to answer questions.\nSpecifically, we first employ dense image captions to help identify objects and\ntheir detailed salient regions and actions, and hence give the model useful\nextra information (in explicit textual format to allow easier matching) for\nanswering questions. Moreover, our model is also comprised of dual-level\nattention (word/object and frame level), multi-head self/cross-integration for\ndifferent sources (video and dense captions), and gates which pass more\nrelevant information to the classifier. Finally, we also cast the frame\nselection problem as a multi-label classification task and introduce two loss\nfunctions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary\nCross-Entropy (BBCE), to better supervise the model with human importance\nannotations. We evaluate our model on the challenging TVQA dataset, where each\nof our model components provides significant gains, and our overall model\noutperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We\nalso present several word, object, and frame level visualization studies. Our\ncode is publicly available at:\nhttps://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:35:27 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Kim", "Hyounghun", ""], ["Tang", "Zineng", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.06420", "submitter": "James Henderson", "authors": "James Henderson", "title": "The Unstoppable Rise of Computational Linguistics in Deep Learning", "comments": "13 pages. Accepted for publication at ACL 2020, in the theme track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we trace the history of neural networks applied to natural\nlanguage understanding tasks, and identify key contributions which the nature\nof language has made to the development of neural network architectures. We\nfocus on the importance of variable binding and its instantiation in\nattention-based models, and argue that Transformer is not a sequence model but\nan induced-structure model. This perspective leads to predictions of the\nchallenges facing research in deep learning architectures for natural language\nunderstanding.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:51:02 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 10:07:40 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 07:58:28 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Henderson", "James", ""]]}, {"id": "2005.06517", "submitter": "Nicola Melluso", "authors": "Elena Coli, Nicola Melluso, Gualtiero Fantoni and Daniele Mazzei", "title": "Towards Automatic building of Human-Machine Conversational System to\n  support Maintenance Processes", "comments": null, "journal-ref": "R&D Management Conference 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Companies are dealing with many cognitive changes with the introduction of\nthe Industry 4.0 paradigm. In this constantly changing environment, knowledge\nmanagement is a key factor. Dialog systems, being able to hold a conversation\nwith humans, could support the knowledge management in business environment.\nAlthough, these systems are currently hand-coded and need the intervention of a\nhuman being in writing all the possible questions and answers, and then\nplanning the interactions. This process, besides being time-consuming, is not\nscalable. Conversely, a dialog system, also referred to as chatbot, can be\nbuilt from scratch by simply extracting rules from technical documentation. So,\nthe goal of this research is designing a methodology for automatic building of\nhuman-machine conversational system, able to interact in an industrial\nenvironment. An initial taxonomy, containing entities expected to be found in\nmaintenance manuals, is used to identify the relevant sentences of a manual\nprovided by the company BOBST SA and applying text mining techniques, it is\nautomatically expanded. The final result is a taxonomy network representing the\nentities and their relation, that will be used in future works for managing the\ninteractions of a maintenance chatbot.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:39:00 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Coli", "Elena", ""], ["Melluso", "Nicola", ""], ["Fantoni", "Gualtiero", ""], ["Mazzei", "Daniele", ""]]}, {"id": "2005.06527", "submitter": "Artuur Leeuwenberg", "authors": "Artuur Leeuwenberg, Marie-Francine Moens", "title": "A Survey on Temporal Reasoning for Temporal Information Extraction from\n  Text (Extended Abstract)", "comments": "Extended abstract of a JAIR article, which is to appear in the\n  proceedings of IJCAI 2020 (the copyright of this abstract is held by IJCAI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time is deeply woven into how people perceive, and communicate about the\nworld. Almost unconsciously, we provide our language utterances with temporal\ncues, like verb tenses, and we can hardly produce sentences without such cues.\nExtracting temporal cues from text, and constructing a global temporal view\nabout the order of described events is a major challenge of automatic natural\nlanguage understanding. Temporal reasoning, the process of combining different\ntemporal cues into a coherent temporal view, plays a central role in temporal\ninformation extraction. This article presents a comprehensive survey of the\nresearch from the past decades on temporal reasoning for automatic temporal\ninformation extraction from text, providing a case study on the integration of\nsymbolic reasoning with machine learning-based information extraction systems.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:53:15 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 11:35:51 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Leeuwenberg", "Artuur", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2005.06537", "submitter": "Hao Peng", "authors": "Hao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith", "title": "A Mixture of $h-1$ Heads is Better than $h$ Heads", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-head attentive neural architectures have achieved state-of-the-art\nresults on a variety of natural language processing tasks. Evidence has shown\nthat they are overparameterized; attention heads can be pruned without\nsignificant performance loss. In this work, we instead \"reallocate\" them -- the\nmodel learns to activate different heads on different inputs. Drawing\nconnections between multi-head attention and mixture of experts, we propose the\nmixture of attentive experts model (MAE). MAE is trained using a block\ncoordinate descent algorithm that alternates between updating (1) the\nresponsibilities of the experts and (2) their parameters. Experiments on\nmachine translation and language modeling show that MAE outperforms strong\nbaselines on both tasks. Particularly, on the WMT14 English to German\ntranslation dataset, MAE improves over \"transformer-base\" by 0.8 BLEU, with a\ncomparable number of parameters. Our analysis shows that our model learns to\nspecialize different experts to different inputs.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:05:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Peng", "Hao", ""], ["Schwartz", "Roy", ""], ["Li", "Dianqi", ""], ["Smith", "Noah A.", ""]]}, {"id": "2005.06540", "submitter": "Slava Jankin Mikhaylov", "authors": "Kakia Chatsiou and Slava Jankin Mikhaylov", "title": "Deep Learning for Political Science", "comments": null, "journal-ref": "The SAGE Handbook of Research Methods in Political Science and\n  International Relations, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political science, and social science in general, have traditionally been\nusing computational methods to study areas such as voting behavior, policy\nmaking, international conflict, and international development. More recently,\nincreasingly available quantities of data are being combined with improved\nalgorithms and affordable computational resources to predict, learn, and\ndiscover new insights from data that is large in volume and variety. New\ndevelopments in the areas of machine learning, deep learning, natural language\nprocessing (NLP), and, more generally, artificial intelligence (AI) are opening\nup new opportunities for testing theories and evaluating the impact of\ninterventions and programs in a more dynamic and effective way. Applications\nusing large volumes of structured and unstructured data are becoming common in\ngovernment and industry, and increasingly also in social science research. This\nchapter offers an introduction to such methods drawing examples from political\nscience. Focusing on the areas where the strengths of the methods coincide with\nchallenges in these fields, the chapter first presents an introduction to AI\nand its core technology - machine learning, with its rapidly developing\nsubfield of deep learning. The discussion of deep neural networks is\nillustrated with the NLP tasks that are relevant to political science. The\nlatest advances in deep learning methods for NLP are also reviewed, together\nwith their potential for improving information extraction and pattern\nrecognition from political science texts.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:14:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Chatsiou", "Kakia", ""], ["Mikhaylov", "Slava Jankin", ""]]}, {"id": "2005.06545", "submitter": "Sriram Krishnan", "authors": "Sriram Krishnan and Amba Kulkarni and G\\'erard Huet", "title": "Validation and Normalization of DCS corpus using Sanskrit Heritage tools\n  to build a tagged Gold Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Digital Corpus of Sanskrit records around 650,000 sentences along with\ntheir morphological and lexical tagging. But inconsistencies in morphological\nanalysis, and in providing crucial information like the segmented word, urges\nthe need for standardization and validation of this corpus. Automating the\nvalidation process requires efficient analyzers which also provide the missing\ninformation. The Sanskrit Heritage Engine's Reader produces all possible\nsegmentations with morphological and lexical analyses. Aligning these systems\nwould help us in recording the linguistic differences, which can be used to\nupdate these systems to produce standardized results and will also provide a\nGold corpus tagged with complete morphological and lexical information along\nwith the segmented words. Krishna et al. (2017) aligned 115,000 sentences,\nconsidering some of the linguistic differences. As both these systems have\nevolved significantly, the alignment is done again considering all the\nremaining linguistic differences between these systems. This paper describes\nthe modified alignment process in detail and records the additional linguistic\ndifferences observed.\n  Reference: Amrith Krishna, Pavankumar Satuluri, and Pawan Goyal. 2017. A\ndataset for Sanskrit word segmentation. In Proceedings of the Joint SIGHUM\nWorkshop on Computational Linguistics for Cultural Heritage, Social Sciences,\nHumanities and Literature, page 105-114. Association for Computational\nLinguistics, August.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:23:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Krishnan", "Sriram", ""], ["Kulkarni", "Amba", ""], ["Huet", "G\u00e9rard", ""]]}, {"id": "2005.06557", "submitter": "Ahmed Abdelali", "authors": "Ahmed Abdelali, Hamdy Mubarak, Younes Samih, Sabit Hassan, Kareem\n  Darwish", "title": "Arabic Dialect Identification in the Wild", "comments": "13 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present QADI, an automatically collected dataset of tweets belonging to a\nwide range of country-level Arabic dialects -covering 18 different countries in\nthe Middle East and North Africa region. Our method for building this dataset\nrelies on applying multiple filters to identify users who belong to different\ncountries based on their account descriptions and to eliminate tweets that are\neither written in Modern Standard Arabic or contain inappropriate language. The\nresultant dataset contains 540k tweets from 2,525 users who are evenly\ndistributed across 18 Arab countries. Using intrinsic evaluation, we show that\nthe labels of a set of randomly selected tweets are 91.5% accurate. For\nextrinsic evaluation, we are able to build effective country-level dialect\nidentification on tweets with a macro-averaged F1-score of 60.6% across 18\nclasses.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:46:41 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 08:23:17 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Abdelali", "Ahmed", ""], ["Mubarak", "Hamdy", ""], ["Samih", "Younes", ""], ["Hassan", "Sabit", ""], ["Darwish", "Kareem", ""]]}, {"id": "2005.06579", "submitter": "Xinya Du", "authors": "Xinya Du and Claire Cardie", "title": "Document-Level Event Role Filler Extraction using Multi-Granularity\n  Contextualized Encoding", "comments": "Accepted to ACL 2020 (long papers), 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few works in the literature of event extraction have gone beyond individual\nsentences to make extraction decisions. This is problematic when the\ninformation needed to recognize an event argument is spread across multiple\nsentences. We argue that document-level event extraction is a difficult task\nsince it requires a view of a larger context to determine which spans of text\ncorrespond to event role fillers. We first investigate how end-to-end neural\nsequence models (with pre-trained language model representations) perform on\ndocument-level role filler extraction, as well as how the length of context\ncaptured affects the models' performance. To dynamically aggregate information\ncaptured by neural representations learned at different levels of granularity\n(e.g., the sentence- and paragraph-level), we propose a novel multi-granularity\nreader. We evaluate our models on the MUC-4 event extraction dataset, and show\nthat our best system performs substantially better than prior work. We also\nreport findings on the relationship between context length and neural model\nperformance on the task.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 20:42:17 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Du", "Xinya", ""], ["Cardie", "Claire", ""]]}, {"id": "2005.06587", "submitter": "Bhanu Pratap Singh Rawat", "authors": "Bhanu Pratap Singh Rawat, Wei-Hung Weng, So Yeon Min, Preethi\n  Raghavan, Peter Szolovits", "title": "Entity-Enriched Neural Models for Clinical Question Answering", "comments": null, "journal-ref": "BioNLP Workshop, ACL'2020", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore state-of-the-art neural models for question answering on\nelectronic medical records and improve their ability to generalize better on\npreviously unseen (paraphrased) questions at test time. We enable this by\nlearning to predict logical forms as an auxiliary task along with the main task\nof answer span detection. The predicted logical forms also serve as a rationale\nfor the answer. Further, we also incorporate medical entity information in\nthese models via the ERNIE architecture. We train our models on the large-scale\nemrQA dataset and observe that our multi-task entity-enriched models generalize\nto paraphrased questions ~5% better than the baseline BERT model.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 21:04:29 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 00:50:27 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rawat", "Bhanu Pratap Singh", ""], ["Weng", "Wei-Hung", ""], ["Min", "So Yeon", ""], ["Raghavan", "Preethi", ""], ["Szolovits", "Peter", ""]]}, {"id": "2005.06588", "submitter": "Majid Asgari-Bidhendi", "authors": "Majid Asgari-Bidhendi, Mehrdad Nasser, Behrooz Janfada, Behrouz\n  Minaei-Bidgoli", "title": "PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction is the task of extracting semantic relations between\nentities in a sentence. It is an essential part of some natural language\nprocessing tasks such as information extraction, knowledge extraction, and\nknowledge base population. The main motivations of this research stem from a\nlack of a dataset for relation extraction in the Persian language as well as\nthe necessity of extracting knowledge from the growing big-data in the Persian\nlanguage for different applications. In this paper, we present \"PERLEX\" as the\nfirst Persian dataset for relation extraction, which is an expert-translated\nversion of the \"Semeval-2010-Task-8\" dataset. Moreover, this paper addresses\nPersian relation extraction utilizing state-of-the-art language-agnostic\nalgorithms. We employ six different models for relation extraction on the\nproposed bilingual dataset, including a non-neural model (as the baseline),\nthree neural models, and two deep learning models fed by multilingual-BERT\ncontextual word representations. The experiments result in the maximum f-score\n77.66% (provided by BERTEM-MTB method) as the state-of-the-art of relation\nextraction in the Persian language.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 21:06:59 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Asgari-Bidhendi", "Majid", ""], ["Nasser", "Mehrdad", ""], ["Janfada", "Behrooz", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2005.06600", "submitter": "Yu Wang", "authors": "Yu Wang, Yuelin Wang, Jie Liu, Zhuo Liu", "title": "A Comprehensive Survey of Grammar Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammar error correction (GEC) is an important application aspect of natural\nlanguage processing techniques. The past decade has witnessed significant\nprogress achieved in GEC for the sake of increasing popularity of machine\nlearning and deep learning, especially in late 2010s when near human-level GEC\nsystems are available. However, there is no prior work focusing on the whole\nrecapitulation of the progress. We present the first survey in GEC for a\ncomprehensive retrospect of the literature in this area. We first give the\nintroduction of five public datasets, data annotation schema, two important\nshared tasks and four standard evaluation metrics. More importantly, we discuss\nfour kinds of basic approaches, including statistical machine translation based\napproach, neural machine translation based approach, classification based\napproach and language model based approach, six commonly applied performance\nboosting techniques for GEC systems and two data augmentation methods. Since\nGEC is typically viewed as a sister task of machine translation, many GEC\nsystems are based on neural machine translation (NMT) approaches, where the\nneural sequence-to-sequence model is applied. Similarly, some performance\nboosting techniques are adapted from machine translation and are successfully\ncombined with GEC systems for enhancement on the final performance.\nFurthermore, we conduct an analysis in level of basic approaches, performance\nboosting techniques and integrated GEC systems based on their experiment\nresults respectively for more clear patterns and conclusions. Finally, we\ndiscuss five prospective directions for future GEC researches.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:46:52 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Wang", "Yu", ""], ["Wang", "Yuelin", ""], ["Liu", "Jie", ""], ["Liu", "Zhuo", ""]]}, {"id": "2005.06601", "submitter": "Tengteng Zhang", "authors": "Tengteng Zhang, Yiqin Yu, Jing Mei, Zefang Tang, Xiang Zhang, Shaochun\n  Li", "title": "Unlocking the Power of Deep PICO Extraction: Step-wise Medical NER\n  Identification", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PICO framework (Population, Intervention, Comparison, and Outcome) is\nusually used to formulate evidence in the medical domain. The major task of\nPICO extraction is to extract sentences from medical literature and classify\nthem into each class. However, in most circumstances, there will be more than\none evidences in an extracted sentence even it has been categorized to a\ncertain class. In order to address this problem, we propose a step-wise disease\nNamed Entity Recognition (DNER) extraction and PICO identification method. With\nour method, sentences in paper title and abstract are first classified into\ndifferent classes of PICO, and medical entities are then identified and\nclassified into P and O. Different kinds of deep learning frameworks are used\nand experimental results show that our method will achieve high performance and\nfine-grained extraction results comparing with conventional PICO extraction\nworks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:09:17 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zhang", "Tengteng", ""], ["Yu", "Yiqin", ""], ["Mei", "Jing", ""], ["Tang", "Zefang", ""], ["Zhang", "Xiang", ""], ["Li", "Shaochun", ""]]}, {"id": "2005.06602", "submitter": "Martin P\\\"omsl", "authors": "Martin P\\\"omsl (Osnabr\\\"uck University) and Roman Lyapin (Cogent Labs\n  Inc.)", "title": "CIRCE at SemEval-2020 Task 1: Ensembling Context-Free and\n  Context-Dependent Word Representations", "comments": "Accepted at SemEval-2020 Task 1 @ COLING 2020. Code available at\n  https://github.com/mpoemsl/circe", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the winning contribution to SemEval-2020 Task 1:\nUnsupervised Lexical Semantic Change Detection (Subtask 2) handed in by team UG\nStudent Intern. We present an ensemble model that makes predictions based on\ncontext-free and context-dependent word representations. The key findings are\nthat (1) context-free word representations are a powerful and robust baseline,\n(2) a sentence classification objective can be used to obtain useful\ncontext-dependent word representations, and (3) combining those representations\nincreases performance on some datasets while decreasing performance on others.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:18:29 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 10:10:05 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 13:50:47 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["P\u00f6msl", "Martin", "", "Osnabr\u00fcck University"], ["Lyapin", "Roman", "", "Cogent Labs\n  Inc."]]}, {"id": "2005.06605", "submitter": "Oren Halvani", "authors": "Oren Halvani and Lukas Graner", "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship\n  Analysis", "comments": "Paper has been accepted for publication in: The 16th International\n  Conference on Availability, Reliability and Security (ARES 2021)", "journal-ref": null, "doi": "10.1145/3465481.3470050", "report-no": null, "categories": "cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:10:24 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 09:16:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Halvani", "Oren", ""], ["Graner", "Lukas", ""]]}, {"id": "2005.06606", "submitter": "Xuanli He", "authors": "Xuanli He, Gholamreza Haffari, Mohammad Norouzi", "title": "Dynamic Programming Encoding for Subword Segmentation in Neural Machine\n  Translation", "comments": "update related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Dynamic Programming Encoding (DPE), a new segmentation\nalgorithm for tokenizing sentences into subword units. We view the subword\nsegmentation of output sentences as a latent variable that should be\nmarginalized out for learning and inference. A mixed character-subword\ntransformer is proposed, which enables exact log marginal likelihood estimation\nand exact MAP inference to find target segmentations with maximum posterior\nprobability. DPE uses a lightweight mixed character-subword transformer as a\nmeans of pre-processing parallel data to segment output sentences using dynamic\nprogramming. Empirical results on machine translation suggest that DPE is\neffective for segmenting output sentences and can be combined with BPE dropout\nfor stochastic segmentation of source sentences. DPE achieves an average\nimprovement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average\nimprovement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several\nWMT datasets including English <=> (German, Romanian, Estonian, Finnish,\nHungarian).\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 05:00:50 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 09:30:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["He", "Xuanli", ""], ["Haffari", "Gholamreza", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2005.06607", "submitter": "Soujanya Poria", "authors": "Navonil Majumder, Rishabh Bhardwaj, Soujanya Poria, Amir Zadeh,\n  Alexander Gelbukh, Amir Hussain, Louis-Philippe Morency", "title": "Improving Aspect-Level Sentiment Analysis with Aspect Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis (ABSA), a popular research area in NLP has\ntwo distinct parts -- aspect extraction (AE) and labeling the aspects with\nsentiment polarity (ALSA). Although distinct, these two tasks are highly\ncorrelated. The work primarily hypothesize that transferring knowledge from a\npre-trained AE model can benefit the performance of ALSA models. Based on this\nhypothesis, word embeddings are obtained during AE and subsequently, feed that\nto the ALSA model. Empirically, this work show that the added information\nsignificantly improves the performance of three different baseline ALSA models\non two distinct domains. This improvement also translates well across domains\nbetween AE and ALSA tasks.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 06:25:16 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Majumder", "Navonil", ""], ["Bhardwaj", "Rishabh", ""], ["Poria", "Soujanya", ""], ["Zadeh", "Amir", ""], ["Gelbukh", "Alexander", ""], ["Hussain", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2005.06608", "submitter": "El Moatez Billah Nagoudi", "authors": "Ali Alshehri, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed", "title": "Understanding and Detecting Dangerous Speech in Social Media", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media communication has become a significant part of daily activity in\nmodern societies. For this reason, ensuring safety in social media platforms is\na necessity. Use of dangerous language such as physical threats in online\nenvironments is a somewhat rare, yet remains highly important. Although several\nworks have been performed on the related issue of detecting offensive and\nhateful language, dangerous speech has not previously been treated in any\nsignificant way. Motivated by these observations, we report our efforts to\nbuild a labeled dataset for dangerous speech. We also exploit our dataset to\ndevelop highly effective models to detect dangerous content. Our best model\nperforms at 59.60% macro F1, significantly outperforming a competitive\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:42:09 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Alshehri", "Ali", ""], ["Nagoudi", "El Moatez Billah", ""], ["Abdul-Mageed", "Muhammad", ""]]}, {"id": "2005.06609", "submitter": "Laura Martinus", "authors": "Laura Martinus, Jason Webster, Joanne Moonsamy, Moses Shaba Jnr, Ridha\n  Moosa, Robert Fairon", "title": "Neural Machine Translation for South Africa's Official Languages", "comments": "workshop paper at AfricaNLP, ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural machine translation (NMT) have led to\nstate-of-the-art results for many European-based translation tasks. However,\ndespite these advances, there is has been little focus in applying these\nmethods to African languages. In this paper, we seek to address this gap by\ncreating an NMT benchmark BLEU score between English and the ten remaining\nofficial languages in South Africa.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 08:36:59 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Martinus", "Laura", ""], ["Webster", "Jason", ""], ["Moonsamy", "Joanne", ""], ["Jnr", "Moses Shaba", ""], ["Moosa", "Ridha", ""], ["Fairon", "Robert", ""]]}, {"id": "2005.06611", "submitter": "Dominique Mercier", "authors": "Dominique Mercier, Syed Tahseen Raza Rizvi, Vikas Rajashekar, Andreas\n  Dengel, Sheraz Ahmed", "title": "ImpactCite: An XLNet-based method for Citation Impact Analysis", "comments": "12 pages (10 + 2 references), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citations play a vital role in understanding the impact of scientific\nliterature. Generally, citations are analyzed quantitatively whereas\nqualitative analysis of citations can reveal deeper insights into the impact of\na scientific artifact in the community. Therefore, citation impact analysis\n(which includes sentiment and intent classification) enables us to quantify the\nquality of the citations which can eventually assist us in the estimation of\nranking and impact. The contribution of this paper is two-fold. First, we\nbenchmark the well-known language models like BERT and ALBERT along with\nseveral popular networks for both tasks of sentiment and intent classification.\nSecond, we provide ImpactCite, which is XLNet-based method for citation impact\nanalysis. All evaluations are performed on a set of publicly available citation\nanalysis datasets. Evaluation results reveal that ImpactCite achieves a new\nstate-of-the-art performance for both citation intent and sentiment\nclassification by outperforming the existing approaches by 3.44% and 1.33% in\nF1-score. Therefore, we emphasize ImpactCite (XLNet-based solution) for both\ntasks to better understand the impact of a citation. Additional efforts have\nbeen performed to come up with CSC-Clean corpus, which is a clean and reliable\ndataset for citation sentiment classification.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:31:54 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mercier", "Dominique", ""], ["Rizvi", "Syed Tahseen Raza", ""], ["Rajashekar", "Vikas", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2005.06616", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Varun Gupta, Ekaterina Kochmar, Dung D. Vu, Robert\n  Belfer, Joelle Pineau, Aaron Courville, Laurent Charlin, Yoshua Bengio", "title": "A Large-Scale, Open-Domain, Mixed-Interface Dialogue-Based ITS for STEM", "comments": "6 pages, 1 figure, 1 table, accepted for publication in the 21st\n  International Conference on Artificial Intelligence in Education (AIED 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Korbit, a large-scale, open-domain, mixed-interface,\ndialogue-based intelligent tutoring system (ITS). Korbit uses machine learning,\nnatural language processing and reinforcement learning to provide interactive,\npersonalized learning online. Korbit has been designed to easily scale to\nthousands of subjects, by automating, standardizing and simplifying the content\ncreation process. Unlike other ITS, a teacher can develop new learning modules\nfor Korbit in a matter of hours. To facilitate learning across a widerange of\nSTEM subjects, Korbit uses a mixed-interface, which includes videos,\ninteractive dialogue-based exercises, question-answering, conceptual diagrams,\nmathematical exercises and gamification elements. Korbit has been built to\nscale to millions of students, by utilizing a state-of-the-art cloud-based\nmicro-service architecture. Korbit launched its first course in 2019 on machine\nlearning, and since then over 7,000 students have enrolled. Although Korbit was\ndesigned to be open-domain and highly scalable, A/B testing experiments with\nreal-world students demonstrate that both student learning outcomes and student\nmotivation are substantially improved compared to typical online courses.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 02:45:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Gupta", "Varun", ""], ["Kochmar", "Ekaterina", ""], ["Vu", "Dung D.", ""], ["Belfer", "Robert", ""], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""], ["Charlin", "Laurent", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2005.06618", "submitter": "Procheta Sen", "authors": "Procheta Sen, Debasis Ganguly", "title": "Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective\n  Learning", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human society had a long history of suffering from cognitive biases leading\nto social prejudices and mass injustice. The prevalent existence of cognitive\nbiases in large volumes of historical data can pose a threat of being\nmanifested as unethical and seemingly inhuman predictions as outputs of AI\nsystems trained on such data. To alleviate this problem, we propose a\nbias-aware multi-objective learning framework that given a set of identity\nattributes (e.g. gender, ethnicity etc.) and a subset of sensitive categories\nof the possible classes of prediction outputs, learns to reduce the frequency\nof predicting certain combinations of them, e.g. predicting stereotypes such as\n`most blacks use abusive language', or `fear is a virtue of women'. Our\nexperiments conducted on an emotion prediction task with balanced class priors\nshows that a set of baseline bias-agnostic models exhibit cognitive biases with\nrespect to gender, such as women are prone to be afraid whereas men are more\nprone to be angry. In contrast, our proposed bias-aware multi-objective\nlearning methodology is shown to reduce such biases in the predictied emotions.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:01:53 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 07:20:09 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Sen", "Procheta", ""], ["Ganguly", "Debasis", ""]]}, {"id": "2005.06619", "submitter": "Ramit Debnath", "authors": "Ramit Debnath and Ronita Bardhan", "title": "India nudges to contain COVID-19 pandemic: a reactive public policy\n  analysis using machine-learning based topic modelling", "comments": "25 pages with 10 figures and 9 tables", "journal-ref": "PLoS ONE 15(9): e0238972 (2020)", "doi": "10.1371/journal.pone.0238972", "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  India locked down 1.3 billion people on March 25, 2020 in the wake of\nCOVID-19 pandemic. The economic cost of it was estimated at USD 98 billion,\nwhile the social costs are still unknown. This study investigated how\ngovernment formed reactive policies to fight coronavirus across its policy\nsectors. Primary data was collected from the Press Information Bureau (PIB) in\nthe form press releases of government plans, policies, programme initiatives\nand achievements. A text corpus of 260,852 words was created from 396 documents\nfrom the PIB. An unsupervised machine-based topic modelling using Latent\nDirichlet Allocation (LDA) algorithm was performed on the text corpus. It was\ndone to extract high probability topics in the policy sectors. The\ninterpretation of the extracted topics was made through a nudge theoretic lens\nto derive the critical policy heuristics of the government. Results showed that\nmost interventions were targeted to generate endogenous nudge by using external\ntriggers. Notably, the nudges from the Prime Minister of India was critical in\ncreating herd effect on lockdown and social distancing norms across the nation.\nA similar effect was also observed around the public health (e.g., masks in\npublic spaces; Yoga and Ayurveda for immunity), transport (e.g., old trains\nconverted to isolation wards), micro, small and medium enterprises (e.g., rapid\nproduction of PPE and masks), science and technology sector (e.g., diagnostic\nkits, robots and nano-technology), home affairs (e.g., surveillance and\nlockdown), urban (e.g. drones, GIS-tools) and education (e.g., online\nlearning). A conclusion was drawn on leveraging these heuristics are crucial\nfor lockdown easement planning.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 04:14:09 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 08:09:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Debnath", "Ramit", ""], ["Bardhan", "Ronita", ""]]}, {"id": "2005.06624", "submitter": "Aurelie Mascio", "authors": "Aurelie Mascio, Zeljko Kraljevic, Daniel Bean, Richard Dobson, Robert\n  Stewart, Rebecca Bendayan, Angus Roberts", "title": "Comparative Analysis of Text Classification Approaches in Electronic\n  Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification tasks which aim at harvesting and/or organizing\ninformation from electronic health records are pivotal to support clinical and\ntranslational research. However these present specific challenges compared to\nother classification tasks, notably due to the particular nature of the medical\nlexicon and language used in clinical records. Recent advances in embedding\nmethods have shown promising results for several clinical tasks, yet there is\nno exhaustive comparison of such approaches with other commonly used word\nrepresentations and classification models. In this work, we analyse the impact\nof various word representations, text pre-processing and classification\nalgorithms on the performance of four different text classification tasks. The\nresults show that traditional approaches, when tailored to the specific\nlanguage and structure of the text inherent to the classification task, can\nachieve or exceed the performance of more recent ones based on contextual\nembeddings such as BERT.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:04:18 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mascio", "Aurelie", ""], ["Kraljevic", "Zeljko", ""], ["Bean", "Daniel", ""], ["Dobson", "Richard", ""], ["Stewart", "Robert", ""], ["Bendayan", "Rebecca", ""], ["Roberts", "Angus", ""]]}, {"id": "2005.06625", "submitter": "Oguzhan Gencoglu", "authors": "Oguzhan Gencoglu", "title": "Cyberbullying Detection with Fairness Constraints", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": "10.1109/MIC.2020.3032461", "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying is a widespread adverse phenomenon among online social\ninteractions in today's digital society. While numerous computational studies\nfocus on enhancing the cyberbullying detection performance of machine learning\nalgorithms, proposed models tend to carry and reinforce unintended social\nbiases. In this study, we try to answer the research question of \"Can we\nmitigate the unintended bias of cyberbullying detection models by guiding the\nmodel training with fairness constraints?\". For this purpose, we propose a\nmodel training scheme that can employ fairness constraints and validate our\napproach with different datasets. We demonstrate that various types of\nunintended biases can be successfully mitigated without impairing the model\nquality. We believe our work contributes to the pursuit of unbiased,\ntransparent, and ethical machine learning solutions for cyber-social health.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 13:04:26 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 21:54:00 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gencoglu", "Oguzhan", ""]]}, {"id": "2005.06627", "submitter": "Junhua Liu", "authors": "Junhua Liu, Trisha Singhal, Lucienne T.M. Blessing, Kristin L. Wood\n  and Kwan Hui Lim", "title": "CrisisBERT: a Robust Transformer for Crisis Classification and\n  Contextual Crisis Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of crisis events, such as natural disasters, terrorist attacks\nand pandemics, is a crucial task to create early signals and inform relevant\nparties for spontaneous actions to reduce overall damage. Despite crisis such\nas natural disasters can be predicted by professional institutions, certain\nevents are first signaled by civilians, such as the recent COVID-19 pandemics.\nSocial media platforms such as Twitter often exposes firsthand signals on such\ncrises through high volume information exchange over half a billion tweets\nposted daily. Prior works proposed various crisis embeddings and classification\nusing conventional Machine Learning and Neural Network models. However, none of\nthe works perform crisis embedding and classification using state of the art\nattention-based deep neural networks models, such as Transformers and\ndocument-level contextual embeddings. This work proposes CrisisBERT, an\nend-to-end transformer-based model for two crisis classification tasks, namely\ncrisis detection and crisis recognition, which shows promising results across\naccuracy and f1 scores. The proposed model also demonstrates superior\nrobustness over benchmark, as it shows marginal performance compromise while\nextending from 6 to 36 events with only 51.4% additional data points. We also\nproposed Crisis2Vec, an attention-based, document-level contextual embedding\narchitecture for crisis embedding, which achieve better performance than\nconventional crisis embedding methods such as Word2Vec and GloVe. To the best\nof our knowledge, our works are first to propose using transformer-based crisis\nclassification and document-level contextual crisis embedding in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:57:24 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 07:58:23 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Junhua", ""], ["Singhal", "Trisha", ""], ["Blessing", "Lucienne T. M.", ""], ["Wood", "Kristin L.", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "2005.06628", "submitter": "Amazon Khetan", "authors": "Ashish Khetan, Zohar Karnin", "title": "schuBERT: Optimizing Elements of BERT", "comments": "11 pages, 6 figures, Accepted for publication in ACL 2020 as a long\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers \\citep{vaswani2017attention} have gradually become a key\ncomponent for many state-of-the-art natural language representation models. A\nrecent Transformer based model- BERT \\citep{devlin2018bert} achieved\nstate-of-the-art results on various natural language processing tasks,\nincluding GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is\ncomputationally prohibitive and has a huge number of parameters. In this work\nwe revisit the architecture choices of BERT in efforts to obtain a lighter\nmodel. We focus on reducing the number of parameters yet our methods can be\napplied towards other objectives such FLOPs or latency. We show that much\nefficient light BERT models can be obtained by reducing algorithmically chosen\ncorrect architecture design dimensions rather than reducing the number of\nTransformer encoder layers. In particular, our schuBERT gives $6.6\\%$ higher\naverage accuracy on GLUE and SQuAD datasets as compared to BERT with three\nencoder layers while having the same number of parameters.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 21:56:04 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Khetan", "Ashish", ""], ["Karnin", "Zohar", ""]]}, {"id": "2005.06632", "submitter": "Somaieh Goudarzvand", "authors": "Somaieh Goudarzvand, Gharib Gharibi, Yugyung Lee", "title": "SCAT: Second Chance Autoencoder for Textual Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a k-competitive learning approach for textual autoencoders named\nSecond Chance Autoencoder (SCAT). SCAT selects the $k$ largest and smallest\npositive activations as the winner neurons, which gain the activation values of\nthe loser neurons during the learning process, and thus focus on retrieving\nwell-representative features for topics. Our experiments show that SCAT\nachieves outstanding performance in classification, topic modeling, and\ndocument visualization compared to LDA, K-Sparse, NVCTM, and KATE.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 19:04:31 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 21:22:00 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 20:45:09 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Goudarzvand", "Somaieh", ""], ["Gharibi", "Gharib", ""], ["Lee", "Yugyung", ""]]}, {"id": "2005.06634", "submitter": "Lee Moore", "authors": "Amy Breden, Lee Moore", "title": "Detecting Adverse Drug Reactions from Twitter through Domain-Specific\n  Preprocessing and BERT Ensembling", "comments": "6 pages, 3 figures, uses acl2018.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automation of adverse drug reaction (ADR) detection in social media would\nrevolutionize the practice of pharmacovigilance, supporting drug regulators,\nthe pharmaceutical industry and the general public in ensuring the safety of\nthe drugs prescribed in daily practice. Following from the published\nproceedings of the Social Media Mining for Health (SMM4H) Applications Workshop\n& Shared Task in August 2019, we aimed to develop a deep learning model to\nclassify ADRs within Twitter tweets that contain drug mentions. Our approach\ninvolved fine-tuning $BERT_{LARGE}$ and two domain-specific BERT\nimplementations, $BioBERT$ and $Bio + clinicalBERT$, applying a domain-specific\npreprocessor, and developing a max-prediction ensembling approach. Our final\nmodel resulted in state-of-the-art performance on both $F_1$-score (0.6681) and\nrecall (0.7700) outperforming all models submitted in SMM4H 2019 and during\npost-evaluation to date.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:49:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Breden", "Amy", ""], ["Moore", "Lee", ""]]}, {"id": "2005.06641", "submitter": "Noga Zaslavsky", "authors": "Noga Zaslavsky, Jennifer Hu, Roger P. Levy", "title": "A Rate-Distortion view of human pragmatic reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What computational principles underlie human pragmatic reasoning? A prominent\napproach to pragmatics is the Rational Speech Act (RSA) framework, which\nformulates pragmatic reasoning as probabilistic speakers and listeners\nrecursively reasoning about each other. While RSA enjoys broad empirical\nsupport, it is not yet clear whether the dynamics of such recursive reasoning\nmay be governed by a general optimization principle. Here, we present a novel\nanalysis of the RSA framework that addresses this question. First, we show that\nRSA recursion implements an alternating maximization for optimizing a tradeoff\nbetween expected utility and communicative effort. On that basis, we study the\ndynamics of RSA recursion and disconfirm the conjecture that expected utility\nis guaranteed to improve with recursion depth. Second, we show that RSA can be\ngrounded in Rate-Distortion theory, while maintaining a similar ability to\naccount for human behavior and avoiding a bias of RSA toward random utterance\nproduction. This work furthers the mathematical understanding of RSA models,\nand suggests that general information-theoretic principles may give rise to\nhuman pragmatic reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 22:04:27 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zaslavsky", "Noga", ""], ["Hu", "Jennifer", ""], ["Levy", "Roger P.", ""]]}, {"id": "2005.06676", "submitter": "Xiaochuang Han", "authors": "Xiaochuang Han, Byron C. Wallace, Yulia Tsvetkov", "title": "Explaining Black Box Predictions and Unveiling Data Artifacts through\n  Influence Functions", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning models for NLP are notoriously opaque. This has\nmotivated the development of methods for interpreting such models, e.g., via\ngradient-based saliency maps or the visualization of attention weights. Such\napproaches aim to provide explanations for a particular model prediction by\nhighlighting important words in the corresponding input text. While this might\nbe useful for tasks where decisions are explicitly influenced by individual\ntokens in the input, we suspect that such highlighting is not suitable for\ntasks where model decisions should be driven by more complex reasoning. In this\nwork, we investigate the use of influence functions for NLP, providing an\nalternative approach to interpreting neural text classifiers. Influence\nfunctions explain the decisions of a model by identifying influential training\nexamples. Despite the promise of this approach, influence functions have not\nyet been extensively evaluated in the context of NLP, a gap addressed by this\nwork. We conduct a comparison between influence functions and common\nword-saliency methods on representative tasks. As suspected, we find that\ninfluence functions are particularly useful for natural language inference, a\ntask in which 'saliency maps' may not have clear interpretation. Furthermore,\nwe develop a new quantitative measure based on influence functions that can\nreveal artifacts in training data.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 00:45:23 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Han", "Xiaochuang", ""], ["Wallace", "Byron C.", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2005.06872", "submitter": "Riccardo Del Gratta Mr.", "authors": "Riccardo Del Gratta", "title": "A Category Theory Approach to Interoperability", "comments": "Paper submitted to Applied Category Theory 2020 and accepted for\n  Virtual Poster Session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we propose a Category Theory approach to (syntactic)\ninteroperability between linguistic tools. The resulting category consists of\ntextual documents, including any linguistic annotations, NLP tools that analyze\ntexts and add additional linguistic information, and format converters. Format\nconverters are necessary to make the tools both able to read and to produce\ndifferent output formats, which is the key to interoperability. The idea behind\nthis document is the parallelism between the concepts of composition and\nassociativity in Category Theory with the NLP pipelines. We show how pipelines\nof linguistic tools can be modeled into the conceptual framework of Category\nTheory and we successfully apply this method to two real-life examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 11:14:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 09:54:38 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Del Gratta", "Riccardo", ""]]}, {"id": "2005.06898", "submitter": "Susan Leavy Dr", "authors": "Susan Leavy, Gerardine Meaney, Karen Wade, Derek Greene", "title": "Mitigating Gender Bias in Machine Learning Data Sets", "comments": "10 pages, 5 figures, 5 Tables, Presented as Bias2020 workshop (as\n  part of the ECIR Conference) - http://bias.disim.univaq.it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence has the capacity to amplify and perpetuate societal\nbiases and presents profound ethical implications for society. Gender bias has\nbeen identified in the context of employment advertising and recruitment tools,\ndue to their reliance on underlying language processing and recommendation\nalgorithms. Attempts to address such issues have involved testing learned\nassociations, integrating concepts of fairness to machine learning and\nperforming more rigorous analysis of training data. Mitigating bias when\nalgorithms are trained on textual data is particularly challenging given the\ncomplex way gender ideology is embedded in language. This paper proposes a\nframework for the identification of gender bias in training data for machine\nlearning.The work draws upon gender theory and sociolinguistics to\nsystematically indicate levels of bias in textual training data and associated\nneural word embedding models, thus highlighting pathways for both removing bias\nfrom training data and critically assessing its impact.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:06:02 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 08:04:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Leavy", "Susan", ""], ["Meaney", "Gerardine", ""], ["Wade", "Karen", ""], ["Greene", "Derek", ""]]}, {"id": "2005.06901", "submitter": "Meishan Zhang", "authors": "Qiankun Fu and Yue Zhang and Jiangming Liu and Meishan Zhang", "title": "DRTS Parsing with Structure-Aware Encoding and Decoding", "comments": "ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse representation tree structure (DRTS) parsing is a novel semantic\nparsing task which has been concerned most recently. State-of-the-art\nperformance can be achieved by a neural sequence-to-sequence model, treating\nthe tree construction as an incremental sequence generation problem. Structural\ninformation such as input syntax and the intermediate skeleton of the partial\noutput has been ignored in the model, which could be potentially useful for the\nDRTS parsing. In this work, we propose a structural-aware model at both the\nencoder and decoder phase to integrate the structural information, where graph\nattention network (GAT) is exploited for effectively modeling. Experimental\nresults on a benchmark dataset show that our proposed model is effective and\ncan obtain the best performance in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:09:23 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Fu", "Qiankun", ""], ["Zhang", "Yue", ""], ["Liu", "Jiangming", ""], ["Zhang", "Meishan", ""]]}, {"id": "2005.06943", "submitter": "Anupam Jamatia Mr", "authors": "Steve Durairaj Swamy, Shubham Laddha, Basil Abdussalam, Debayan Datta\n  and Anupam Jamatia", "title": "NIT-Agartala-NLP-Team at SemEval-2020 Task 8: Building Multimodal\n  Classifiers to tackle Internet Humor", "comments": "Submitted to International Workshop on Semantic Evaluation\n  (SemEval)-2020 Task 8: Memotion Analysis,\n  http://alt.qcri.org/semeval2020/index.php?id=tasks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the systems submitted to SemEval-2020 Task 8: Memotion by\nthe `NIT-Agartala-NLP-Team'. A dataset of 8879 memes was made available by the\ntask organizers to train and test our models. Our systems include a Logistic\nRegression baseline, a BiLSTM + Attention-based learner and a transfer learning\napproach with BERT. For the three sub-tasks A, B and C, we attained ranks\n24/33, 11/29 and 15/26, respectively. We highlight our difficulties in\nharnessing image information as well as some techniques and handcrafted\nfeatures we employ to overcome these issues. We also discuss various modelling\nissues and theorize possible solutions and reasons as to why these problems\npersist.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 13:23:59 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 05:07:59 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Swamy", "Steve Durairaj", ""], ["Laddha", "Shubham", ""], ["Abdussalam", "Basil", ""], ["Datta", "Debayan", ""], ["Jamatia", "Anupam", ""]]}, {"id": "2005.06946", "submitter": "Tom De Smedt", "authors": "Pierre Vou\\'e, Tom De Smedt, Guy De Pauw", "title": "4chan & 8chan embeddings", "comments": "4 pages", "journal-ref": "Textgain Technical Reports 1 (2020)", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have collected over 30M messages from the publicly available /pol/ message\nboards on 4chan and 8chan, and compiled them into a model of toxic language\nuse. The trained word embeddings (0.4GB) are released for free and may be\nuseful for further study on toxic discourse or to boost hate speech detection\nsystems: https://textgain.com/8chan.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 10:17:55 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Vou\u00e9", "Pierre", ""], ["De Smedt", "Tom", ""], ["De Pauw", "Guy", ""]]}, {"id": "2005.06968", "submitter": "Xinsheng Wang", "authors": "Xinsheng Wang, Tingting Qiao, Jihua Zhu, Alan Hanjalic, Odette\n  Scharenborg", "title": "S2IGAN: Speech-to-Image Generation via Adversarial Learning", "comments": "Accepted to Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An estimated half of the world's languages do not have a written form, making\nit impossible for these languages to benefit from any existing text-based\ntechnologies. In this paper, a speech-to-image generation (S2IG) framework is\nproposed which translates speech descriptions to photo-realistic images without\nusing any text information, thus allowing unwritten languages to potentially\nbenefit from this technology. The proposed S2IG framework, named S2IGAN,\nconsists of a speech embedding network (SEN) and a relation-supervised\ndensely-stacked generative model (RDG). SEN learns the speech embedding with\nthe supervision of the corresponding visual information. Conditioned on the\nspeech embedding produced by SEN, the proposed RDG synthesizes images that are\nsemantically consistent with the corresponding speech descriptions. Extensive\nexperiments on two public benchmark datasets CUB and Oxford-102 demonstrate the\neffectiveness of the proposed S2IGAN on synthesizing high-quality and\nsemantically-consistent images from the speech signal, yielding a good\nperformance and a solid baseline for the S2IG task.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 13:39:56 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 08:17:22 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wang", "Xinsheng", ""], ["Qiao", "Tingting", ""], ["Zhu", "Jihua", ""], ["Hanjalic", "Alan", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2005.06980", "submitter": "Rajarshi Haldar", "authors": "Rajarshi Haldar, Lingfei Wu, Jinjun Xiong and Julia Hockenmaier", "title": "A Multi-Perspective Architecture for Semantic Code Search", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to match pieces of code to their corresponding natural language\ndescriptions and vice versa is fundamental for natural language search\ninterfaces to software repositories. In this paper, we propose a novel\nmulti-perspective cross-lingual neural framework for code--text matching,\ninspired in part by a previous model for monolingual text-to-text matching, to\ncapture both global and local similarities. Our experiments on the CoNaLa\ndataset show that our proposed model yields better performance on this\ncross-lingual text-to-code matching task than previous approaches that map code\nand text to a single joint embedding space.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 04:46:11 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Haldar", "Rajarshi", ""], ["Wu", "Lingfei", ""], ["Xiong", "Jinjun", ""], ["Hockenmaier", "Julia", ""]]}, {"id": "2005.07019", "submitter": "Zhijie Sasha Dong", "authors": "Zhijie Sasha Dong, Lingyu Meng, Lauren Christenson, Lawrence Fulton", "title": "Social Media Information Sharing for Natural Disaster Response", "comments": "24 pages, 14 figures", "journal-ref": "Nat Hazards 107, 2077 - 2104 (2021)", "doi": "10.1007/s11069-021-04528-9", "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media has become an essential channel for posting disaster-related\ninformation, which provide governments and relief agencies real-time data for\nbetter disaster management. However, research in this field has not received\nsufficient attention and extracting useful information is still challenging.\nThis paper aims to improve disaster relief efficiency via mining and analyzing\nsocial media data like public attitudes towards disaster response and public\ndemands for targeted relief supplies during different types of disasters. We\nfocus on different natural disasters based on properties such as types,\ndurations, and damages, which contains a total of 41,993 tweets. In this paper,\npublic perception is assessed qualitatively by manually classified tweets,\nwhich contain information like the demand for targeted relief supplies,\nsatisfactions of disaster response, and public fear. Public attitudes to\nnatural disasters are studied via a quantitative analysis using eight machine\nlearning models. To better provide decision-makers with the appropriate model,\nthe comparison of machine learning models based on computational time and\nprediction accuracy is conducted. The change of public opinion during different\nnatural disasters and the evolution of people's behavior of using social media\nfor disaster relief in the face of the identical type of natural disasters as\nTwitter continues to evolve are studied. The results in this paper demonstrate\nthe feasibility and validation of the proposed research approach and provide\nrelief agencies with insights into better disaster management.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 21:11:39 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 19:35:38 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 21:03:00 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 10:24:40 GMT"}, {"version": "v5", "created": "Sun, 11 Jul 2021 15:59:40 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dong", "Zhijie Sasha", ""], ["Meng", "Lingyu", ""], ["Christenson", "Lauren", ""], ["Fulton", "Lawrence", ""]]}, {"id": "2005.07025", "submitter": "Kun Zhou", "authors": "Kun Zhou, Berrak Sisman, Mingyang Zhang and Haizhou Li", "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice\n  Conversion", "comments": "Accepted by Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional voice conversion aims to convert the emotion of speech from one\nstate to another while preserving the linguistic content and speaker identity.\nThe prior studies on emotional voice conversion are mostly carried out under\nthe assumption that emotion is speaker-dependent. We consider that there is a\ncommon code between speakers for emotional expression in a spoken language,\ntherefore, a speaker-independent mapping between emotional states is possible.\nIn this paper, we propose a speaker-independent emotional voice conversion\nframework, that can convert anyone's emotion without the need for parallel\ndata. We propose a VAW-GAN based encoder-decoder structure to learn the\nspectrum and prosody mapping. We perform prosody conversion by using continuous\nwavelet transform (CWT) to model the temporal dependencies. We also investigate\nthe use of F0 as an additional input to the decoder to improve emotion\nconversion performance. Experiments show that the proposed speaker-independent\nframework achieves competitive results for both seen and unseen speakers.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:36:34 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 13:37:48 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 06:07:16 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Zhou", "Kun", ""], ["Sisman", "Berrak", ""], ["Zhang", "Mingyang", ""], ["Li", "Haizhou", ""]]}, {"id": "2005.07029", "submitter": "Yi-Chen Chen", "authors": "Yi-Chen Chen, Jui-Yang Hsu, Cheng-Kuang Lee, Hung-yi Lee", "title": "DARTS-ASR: Differentiable Architecture Search for Multilingual Speech\n  Recognition and Adaptation", "comments": "Accepted at INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous works, only parameter weights of ASR models are optimized under\nfixed-topology architecture. However, the design of successful model\narchitecture has always relied on human experience and intuition. Besides, many\nhyperparameters related to model architecture need to be manually tuned.\nTherefore in this paper, we propose an ASR approach with efficient\ngradient-based architecture search, DARTS-ASR. In order to examine the\ngeneralizability of DARTS-ASR, we apply our approach not only on many languages\nto perform monolingual ASR, but also on a multilingual ASR setting. Following\nprevious works, we conducted experiments on a multilingual dataset, IARPA\nBABEL. The experiment results show that our approach outperformed the baseline\nfixed-topology architecture by 10.2% and 10.0% relative reduction on character\nerror rates under monolingual and multilingual ASR settings respectively.\nFurthermore, we perform some analysis on the searched architectures by\nDARTS-ASR.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:32:27 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 02:40:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Yi-Chen", ""], ["Hsu", "Jui-Yang", ""], ["Lee", "Cheng-Kuang", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2005.07064", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman", "title": "Multi-agent Communication meets Natural Language: Synergies between\n  Functional and Structural Language Learning", "comments": "to appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for combining multi-agent communication and traditional\ndata-driven approaches to natural language learning, with an end goal of\nteaching agents to communicate with humans in natural language. Our starting\npoint is a language model that has been trained on generic, not task-specific\nlanguage data. We then place this model in a multi-agent self-play environment\nthat generates task-specific rewards used to adapt or modulate the model,\nturning it into a task-conditional language model. We introduce a new way for\ncombining the two types of learning based on the idea of reranking language\nmodel samples, and show that this method outperforms others in communicating\nwith humans in a visual referential communication task. Finally, we present a\ntaxonomy of different types of language drift that can occur alongside a set of\nmeasures to detect them.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 15:32:23 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Potapenko", "Anna", ""], ["Tieleman", "Olivier", ""]]}, {"id": "2005.07105", "submitter": "Colin Lockard", "authors": "Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi", "title": "ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured\n  Webpages", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many documents, such as semi-structured webpages, textual semantics are\naugmented with additional information conveyed using visual elements including\nlayout, font size, and color. Prior work on information extraction from\nsemi-structured websites has required learning an extraction model specific to\na given template via either manually labeled or distantly supervised data from\nthat template. In this work, we propose a solution for \"zero-shot\" open-domain\nrelation extraction from webpages with a previously unseen template, including\nfrom websites with little overlap with existing sources of knowledge for\ndistant supervision and websites in entirely new subject verticals. Our model\nuses a graph neural network-based approach to build a rich representation of\ntext fields on a webpage and the relationships between them, enabling\ngeneralization to new templates. Experiments show this approach provides a 31%\nF1 gain over a baseline for zero-shot extraction in a new subject vertical.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:15:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lockard", "Colin", ""], ["Shiralkar", "Prashant", ""], ["Dong", "Xin Luna", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2005.07111", "submitter": "Madhumita Sushil", "authors": "Madhumita Sushil and Simon \\v{S}uster and Walter Daelemans", "title": "Distilling neural networks into skipgram-level decision lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several previous studies on explanation for recurrent neural networks focus\non approaches that find the most important input segments for a network as its\nexplanations. In that case, the manner in which these input segments combine\nwith each other to form an explanatory pattern remains unknown. To overcome\nthis, some previous work tries to find patterns (called rules) in the data that\nexplain neural outputs. However, their explanations are often insensitive to\nmodel parameters, which limits the scalability of text explanations. To\novercome these limitations, we propose a pipeline to explain RNNs by means of\ndecision lists (also called rules) over skipgrams. For evaluation of\nexplanations, we create a synthetic sepsis-identification dataset, as well as\napply our technique on additional clinical and sentiment analysis datasets. We\nfind that our technique persistently achieves high explanation fidelity and\nqualitatively interpretable rules.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:25:42 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 08:43:42 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sushil", "Madhumita", ""], ["\u0160uster", "Simon", ""], ["Daelemans", "Walter", ""]]}, {"id": "2005.07150", "submitter": "Juntao Yu", "authors": "Juntao Yu and Bernd Bohnet and Massimo Poesio", "title": "Named Entity Recognition as Dependency Parsing", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing, concerned with identifying spans of text expressing references to\nentities. NER research is often focused on flat entities only (flat NER),\nignoring the fact that entity references can be nested, as in [Bank of [China]]\n(Finkel and Manning, 2009). In this paper, we use ideas from graph-based\ndependency parsing to provide our model a global view on the input via a\nbiaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of\nstart and end tokens in a sentence which we use to explore all spans, so that\nthe model is able to predict named entities accurately. We show that the model\nworks well for both nested and flat NER through evaluation on 8 corpora and\nachieving SoTA performance on all of them, with accuracy gains of up to 2.2\npercentage points.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:11:41 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 11:21:10 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 10:55:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yu", "Juntao", ""], ["Bohnet", "Bernd", ""], ["Poesio", "Massimo", ""]]}, {"id": "2005.07157", "submitter": "Aleksandr Laptev", "authors": "Aleksandr Laptev, Roman Korostik, Aleksey Svischev, Andrei Andrusenko,\n  Ivan Medennikov, Sergey Rybin", "title": "You Do Not Need More Data: Improving End-To-End Speech Recognition by\n  Text-To-Speech Data Augmentation", "comments": null, "journal-ref": null, "doi": "10.1109/CISP-BMEI51763.2020.9263564", "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is one of the most effective ways to make end-to-end\nautomatic speech recognition (ASR) perform close to the conventional hybrid\napproach, especially when dealing with low-resource tasks. Using recent\nadvances in speech synthesis (text-to-speech, or TTS), we build our TTS system\non an ASR training database and then extend the data with synthesized speech to\ntrain a recognition model. We argue that, when the training data amount is\nrelatively low, this approach can allow an end-to-end model to reach hybrid\nsystems' quality. For an artificial low-to-medium-resource setup, we compare\nthe proposed augmentation with the semi-supervised learning technique. We also\ninvestigate the influence of vocoder usage on final ASR performance by\ncomparing Griffin-Lim algorithm with our modified LPCNet. When applied with an\nexternal language model, our approach outperforms a semi-supervised setup for\nLibriSpeech test-clean and only 33% worse than a comparable supervised setup.\nOur system establishes a competitive result for end-to-end ASR trained on\nLibriSpeech train-clean-100 set with WER 4.3% for test-clean and 13.5% for\ntest-other.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:24:57 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 20:26:57 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Laptev", "Aleksandr", ""], ["Korostik", "Roman", ""], ["Svischev", "Aleksey", ""], ["Andrusenko", "Andrei", ""], ["Medennikov", "Ivan", ""], ["Rybin", "Sergey", ""]]}, {"id": "2005.07162", "submitter": "Marcin Namysl", "authors": "Marcin Namysl, Sven Behnke and Joachim K\\\"ohler", "title": "NAT: Noise-Aware Training for Robust Neural Sequence Labeling", "comments": "Accepted to appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence labeling systems should perform reliably not only under ideal\nconditions but also with corrupted inputs - as these systems often process\nuser-generated text or follow an error-prone upstream component. To this end,\nwe formulate the noisy sequence labeling problem, where the input may undergo\nan unknown noising process and propose two Noise-Aware Training (NAT)\nobjectives that improve robustness of sequence labeling performed on perturbed\ninput: Our data augmentation method trains a neural model using a mixture of\nclean and noisy samples, whereas our stability training algorithm encourages\nthe model to create a noise-invariant latent representation. We employ a\nvanilla noise model at training time. For evaluation, we use both the original\ndata and its variants perturbed with real OCR errors and misspellings.\nExtensive experiments on English and German named entity recognition benchmarks\nconfirmed that NAT consistently improved robustness of popular sequence\nlabeling models, preserving accuracy on the original input. We make our code\nand data publicly available for the research community.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:30:06 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Namysl", "Marcin", ""], ["Behnke", "Sven", ""], ["K\u00f6hler", "Joachim", ""]]}, {"id": "2005.07174", "submitter": "Elena Kochkina", "authors": "Elena Kochkina and Maria Liakata", "title": "Estimating predictive uncertainty for rumour verification models", "comments": "Accepted to the Annual Conference of the Association for\n  Computational Linguistics (ACL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The inability to correctly resolve rumours circulating online can have\nharmful real-world consequences. We present a method for incorporating model\nand data uncertainty estimates into natural language processing models for\nautomatic rumour verification. We show that these estimates can be used to\nfilter out model predictions likely to be erroneous, so that these difficult\ninstances can be prioritised by a human fact-checker. We propose two methods\nfor uncertainty-based instance rejection, supervised and unsupervised. We also\nshow how uncertainty estimates can be used to interpret model performance as a\nrumour unfolds.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:42:25 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kochkina", "Elena", ""], ["Liakata", "Maria", ""]]}, {"id": "2005.07202", "submitter": "Shoya Wada", "authors": "Shoya Wada, Toshihiro Takeda, Shiro Manabe, Shozo Konishi, Jun\n  Kamohara, and Yasushi Matsumura", "title": "Pre-training technique to localize medical BERT and enhance biomedical\n  BERT", "comments": "We made the pre-trained weights of ouBioBERT and the source code for\n  fine-tuning freely available at\n  https://github.com/sy-wada/blue_benchmark_with_transformers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training large-scale neural language models on raw texts has made a\nsignificant contribution to improving transfer learning in natural language\nprocessing (NLP). With the introduction of transformer-based language models,\nsuch as bidirectional encoder representations from transformers (BERT), the\nperformance of information extraction from a free text by NLP has significantly\nimproved for both the general domain and medical domain; however, it is\ndifficult to train specific BERT models that perform well for domains in which\nthere are few publicly available databases of high quality and large size. We\nhypothesized that this problem can be addressed by up-sampling a\ndomain-specific corpus and using it for pre-training with a larger corpus in a\nbalanced manner. Our proposed method consists of a single intervention with one\noption: simultaneous pre-training after up-sampling and amplified vocabulary.\nWe conducted three experiments and evaluated the resulting products. We\nconfirmed that our Japanese medical BERT outperformed conventional baselines\nand the other BERT models in terms of the medical document classification task\nand that our English BERT pre-trained using both the general and medical-domain\ncorpora performed sufficiently well for practical use in terms of the\nbiomedical language understanding evaluation (BLUE) benchmark. Moreover, our\nenhanced biomedical BERT model, in which clinical notes were not used during\npre-training, showed that both the clinical and biomedical scores of the BLUE\nbenchmark were 0.3 points above that of the ablation model trained without our\nproposed method. Well-balanced pre-training by up-sampling instances derived\nfrom a corpus appropriate for the target task allows us to construct a\nhigh-performance BERT model.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 18:00:01 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 04:22:24 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 07:00:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wada", "Shoya", ""], ["Takeda", "Toshihiro", ""], ["Manabe", "Shiro", ""], ["Konishi", "Shozo", ""], ["Kamohara", "Jun", ""], ["Matsumura", "Yasushi", ""]]}, {"id": "2005.07272", "submitter": "Ivan Medennikov", "authors": "Ivan Medennikov, Maxim Korenevsky, Tatiana Prisyach, Yuri Khokhlov,\n  Mariya Korenevskaya, Ivan Sorokin, Tatiana Timofeeva, Anton Mitrofanov,\n  Andrei Andrusenko, Ivan Podluzhny, Aleksandr Laptev, Aleksei Romanenko", "title": "Target-Speaker Voice Activity Detection: a Novel Approach for\n  Multi-Speaker Diarization in a Dinner Party Scenario", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1602", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker diarization for real-life scenarios is an extremely challenging\nproblem. Widely used clustering-based diarization approaches perform rather\npoorly in such conditions, mainly due to the limited ability to handle\noverlapping speech. We propose a novel Target-Speaker Voice Activity Detection\n(TS-VAD) approach, which directly predicts an activity of each speaker on each\ntime frame. TS-VAD model takes conventional speech features (e.g., MFCC) along\nwith i-vectors for each speaker as inputs. A set of binary classification\noutput layers produces activities of each speaker. I-vectors can be estimated\niteratively, starting with a strong clustering-based diarization. We also\nextend the TS-VAD approach to the multi-microphone case using a simple\nattention mechanism on top of hidden representations extracted from the\nsingle-channel TS-VAD model. Moreover, post-processing strategies for the\npredicted speaker activity probabilities are investigated. Experiments on the\nCHiME-6 unsegmented data show that TS-VAD achieves state-of-the-art results\noutperforming the baseline x-vector-based system by more than 30% Diarization\nError Rate (DER) abs.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 21:24:56 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:28:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Medennikov", "Ivan", ""], ["Korenevsky", "Maxim", ""], ["Prisyach", "Tatiana", ""], ["Khokhlov", "Yuri", ""], ["Korenevskaya", "Mariya", ""], ["Sorokin", "Ivan", ""], ["Timofeeva", "Tatiana", ""], ["Mitrofanov", "Anton", ""], ["Andrusenko", "Andrei", ""], ["Podluzhny", "Ivan", ""], ["Laptev", "Aleksandr", ""], ["Romanenko", "Aleksei", ""]]}, {"id": "2005.07287", "submitter": "Gregory Senay", "authors": "Gregory Senay, Badr Youbi Idrissi, Marine Haziza", "title": "VirAAL: Virtual Adversarial Active Learning For NLU", "comments": "To appear in Proc. IEEE SLT 2021, January 19-22, 2021, Shenzhen,\n  China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents VirAAL, an Active Learning framework based on Adversarial\nTraining. VirAAL aims to reduce the effort of annotation in Natural Language\nUnderstanding (NLU). VirAAL is based on Virtual Adversarial Training (VAT), a\nsemi-supervised approach that regularizes the model through Local\nDistributional Smoothness. With that, adversarial perturbations are added to\nthe inputs making the posterior distribution more consistent. Therefore,\nentropy-based Active Learning becomes robust by querying more informative\nsamples without requiring additional components. The first set of experiments\nstudies the impact of an adapted VAT for joint-NLU tasks within low labeled\ndata regimes. The second set shows the effect of VirAAL in an Active Learning\n(AL) process. Results demonstrate that VAT is robust even on multi-task\ntraining, where the adversarial noise is computed from multiple loss functions.\nSubstantial improvements are observed with entropy-based AL with VirAAL for\nquerying data to annotate. VirAAL is an inexpensive method in terms of AL\ncomputation with a positive impact on data sampling. Furthermore, VirAAL\ndecreases annotations in AL up to 80% and shows improvements over existing data\naugmentation methods. The code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 22:47:37 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 21:35:15 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Senay", "Gregory", ""], ["Idrissi", "Badr Youbi", ""], ["Haziza", "Marine", ""]]}, {"id": "2005.07297", "submitter": "Fatemah Husain", "authors": "Fatemah Husain", "title": "OSACT4 Shared Task on Offensive Language Detection: Intensive\n  Preprocessing-Based Approach", "comments": "Proceedings of the Twelfth International Conference on Language\n  Resources and Evaluation (LREC 2020), Marseille, France (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The preprocessing phase is one of the key phases within the text\nclassification pipeline. This study aims at investigating the impact of the\npreprocessing phase on text classification, specifically on offensive language\nand hate speech classification for Arabic text. The Arabic language used in\nsocial media is informal and written using Arabic dialects, which makes the\ntext classification task very complex. Preprocessing helps in dimensionality\nreduction and removing useless content. We apply intensive preprocessing\ntechniques to the dataset before processing it further and feeding it into the\nclassification model. An intensive preprocessing-based approach demonstrates\nits significant impact on offensive language detection and hate speech\ndetection shared tasks of the fourth workshop on Open-Source Arabic Corpora and\nCorpora Processing Tools (OSACT). Our team wins the third place (3rd) in the\nSub-Task A Offensive Language Detection division and wins the first place (1st)\nin the Sub-Task B Hate Speech Detection division, with an F1 score of 89% and\n95%, respectively, by providing the state-of-the-art performance in terms of\nF1, accuracy, recall, and precision for Arabic hate speech detection.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 23:46:10 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Husain", "Fatemah", ""]]}, {"id": "2005.07310", "submitter": "Jize Cao", "authors": "Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen and Jingjing\n  Liu", "title": "Behind the Scene: Revealing the Secrets of Pre-trained\n  Vision-and-Language Models", "comments": "Accepted by ECCV 2020 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Transformer-based large-scale pre-trained models have revolutionized\nvision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER\nhave significantly lifted state of the art across a wide range of V+L\nbenchmarks with joint image-text pre-training. However, little is known about\nthe inner mechanisms that destine their impressive success. To reveal the\nsecrets behind the scene of these powerful models, we present VALUE\n(Vision-And-Language Understanding Evaluation), a set of meticulously designed\nprobing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection,\nLinguistic Probing Tasks) generalizable to standard pre-trained V+L models,\naiming to decipher the inner workings of multimodal pre-training (e.g., the\nimplicit knowledge garnered in individual attention heads, the inherent\ncross-modal alignment learned through contextualized multimodal embeddings).\nThrough extensive analysis of each archetypal model architecture via these\nprobing tasks, our key observations are: (i) Pre-trained models exhibit a\npropensity for attending over text rather than images during inference. (ii)\nThere exists a subset of attention heads that are tailored for capturing\ncross-modal interactions. (iii) Learned attention matrix in pre-trained models\ndemonstrates patterns coherent with the latent alignment between image regions\nand textual words. (iv) Plotted attention patterns reveal\nvisually-interpretable relations among image regions. (v) Pure linguistic\nknowledge is also effectively encoded in the attention heads. These are\nvaluable insights serving to guide future work towards designing better model\narchitecture and objectives for multimodal pre-training.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 01:06:54 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 23:10:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cao", "Jize", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Yu", "Licheng", ""], ["Chen", "Yen-Chun", ""], ["Liu", "Jingjing", ""]]}, {"id": "2005.07355", "submitter": "James Warren PhD", "authors": "Chester Holt-Quick, Jim Warren, Karolina Stasiak, Ruth Williams, Grant\n  Christie, Sarah Hetrick, Sarah Hopkins, Tania Cargo, Sally Merry", "title": "A chatbot architecture for promoting youth resilience", "comments": "7 pages, 4 figures, submitted to Australian Health Informatics\n  Conference, Brisbane, October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-health technologies have the potential to provide scalable and accessible\ninterventions for youth mental health. As part of a developing an ecosystem of\ne-screening and e-therapy tools for New Zealand young people, a dialog agent,\nHeadstrong, has been designed to promote resilience with methods grounded in\ncognitive behavioral therapy and positive psychology. This paper describes the\narchitecture underlying the chatbot. The architecture supports a range of over\n20 activities delivered in a 4-week program by relatable personas. The\narchitecture provides a visual authoring interface to its content management\nsystem. In addition to supporting the original adolescent resilience chatbot,\nthe architecture has been reused to create a 3-week 'stress-detox' intervention\nfor undergraduates, and subsequently for a chatbot to support young people with\nthe impacts of the COVID-19 pandemic, with all three systems having been used\nin field trials. The Headstrong architecture illustrates the feasibility of\ncreating a domain-focused authoring environment in the context of e-therapy\nthat supports non-technical expert input and rapid deployment.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 04:36:06 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Holt-Quick", "Chester", ""], ["Warren", "Jim", ""], ["Stasiak", "Karolina", ""], ["Williams", "Ruth", ""], ["Christie", "Grant", ""], ["Hetrick", "Sarah", ""], ["Hopkins", "Sarah", ""], ["Cargo", "Tania", ""], ["Merry", "Sally", ""]]}, {"id": "2005.07362", "submitter": "Ryuichi Takanobu", "authors": "Ryuichi Takanobu, Qi Zhu, Jinchao Li, Baolin Peng, Jianfeng Gao,\n  Minlie Huang", "title": "Is Your Goal-Oriented Dialog Model Performing Really Well? Empirical\n  Analysis of System-wise Evaluation", "comments": "SIGDIAL 2020 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in developing goal-oriented dialog systems which\nserve users in accomplishing complex tasks through multi-turn conversations.\nAlthough many methods are devised to evaluate and improve the performance of\nindividual dialog components, there is a lack of comprehensive empirical study\non how different components contribute to the overall performance of a dialog\nsystem. In this paper, we perform a system-wise evaluation and present an\nempirical analysis on different types of dialog systems which are composed of\ndifferent modules in different settings. Our results show that (1) a pipeline\ndialog system trained using fine-grained supervision signals at different\ncomponent levels often obtains better performance than the systems that use\njoint or end-to-end models trained on coarse-grained labels, (2)\ncomponent-wise, single-turn evaluation results are not always consistent with\nthe overall performance of a dialog system, and (3) despite the discrepancy\nbetween simulators and human users, simulated evaluation is still a valid\nalternative to the costly human evaluation especially in the early stage of\ndevelopment.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 05:20:06 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Takanobu", "Ryuichi", ""], ["Zhu", "Qi", ""], ["Li", "Jinchao", ""], ["Peng", "Baolin", ""], ["Gao", "Jianfeng", ""], ["Huang", "Minlie", ""]]}, {"id": "2005.07394", "submitter": "Da-Rong Liu", "authors": "Da-Rong Liu, Chunxi Liu, Frank Zhang, Gabriel Synnaeve, Yatharth\n  Saraf, Geoffrey Zweig", "title": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network\n  Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos uploaded on social media are often accompanied with textual\ndescriptions. In building automatic speech recognition (ASR) systems for\nvideos, we can exploit the contextual information provided by such video\nmetadata. In this paper, we explore ASR lattice rescoring by selectively\nattending to the video descriptions. We first use an attention based method to\nextract contextual vector representations of video metadata, and use these\nrepresentations as part of the inputs to a neural language model during lattice\nrescoring. Secondly, we propose a hybrid pointer network approach to explicitly\ninterpolate the word probabilities of the word occurrences in metadata. We\nperform experimental evaluations on both language modeling and ASR tasks, and\ndemonstrate that both proposed methods provide performance improvements by\nselectively leveraging the video metadata.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:47:33 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Liu", "Da-Rong", ""], ["Liu", "Chunxi", ""], ["Zhang", "Frank", ""], ["Synnaeve", "Gabriel", ""], ["Saraf", "Yatharth", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "2005.07421", "submitter": "Haoran Huang", "authors": "Shaohua Zhang, Haoran Huang, Jicong Liu and Hang Li", "title": "Spelling Error Correction with Soft-Masked BERT", "comments": "To be published at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spelling error correction is an important yet challenging task because a\nsatisfactory solution of it essentially needs human-level language\nunderstanding ability. Without loss of generality we consider Chinese spelling\nerror correction (CSC) in this paper. A state-of-the-art method for the task\nselects a character from a list of candidates for correction (including\nnon-correction) at each position of the sentence on the basis of BERT, the\nlanguage representation model. The accuracy of the method can be sub-optimal,\nhowever, because BERT does not have sufficient capability to detect whether\nthere is an error at each position, apparently due to the way of pre-training\nit using mask language modeling. In this work, we propose a novel neural\narchitecture to address the aforementioned issue, which consists of a network\nfor error detection and a network for error correction based on BERT, with the\nformer being connected to the latter with what we call soft-masking technique.\nOur method of using `Soft-Masked BERT' is general, and it may be employed in\nother language detection-correction problems. Experimental results on two\ndatasets demonstrate that the performance of our proposed method is\nsignificantly better than the baselines including the one solely based on BERT.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:02:38 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Zhang", "Shaohua", ""], ["Huang", "Haoran", ""], ["Liu", "Jicong", ""], ["Li", "Hang", ""]]}, {"id": "2005.07456", "submitter": "Marko Robnik-Sikonja", "authors": "Marko Robnik-Sikonja, Kristjan Reba, Igor Mozetic", "title": "Cross-lingual Transfer of Sentiment Classifiers", "comments": "18 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings represent words in a numeric space so that semantic relations\nbetween words are represented as distances and directions in the vector space.\nCross-lingual word embeddings transform vector spaces of different languages so\nthat similar words are aligned. This is done by constructing a mapping between\nvector spaces of two languages or learning a joint vector space for multiple\nlanguages. Cross-lingual embeddings can be used to transfer machine learning\nmodels between languages, thereby compensating for insufficient data in\nless-resourced languages. We use cross-lingual word embeddings to transfer\nmachine learning prediction models for Twitter sentiment between 13 languages.\nWe focus on two transfer mechanisms that recently show superior transfer\nperformance. The first mechanism uses the trained models whose input is the\njoint numerical space for many languages as implemented in the LASER library.\nThe second mechanism uses large pretrained multilingual BERT language models.\nOur experiments show that the transfer of models between similar languages is\nsensible, even with no target language data. The performance of cross-lingual\nmodels obtained with the multilingual BERT and LASER library is comparable, and\nthe differences are language-dependent. The transfer with CroSloEngual BERT,\npretrained on only three languages, is superior on these and some closely\nrelated languages.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:15:27 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 06:29:45 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 15:18:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Robnik-Sikonja", "Marko", ""], ["Reba", "Kristjan", ""], ["Mozetic", "Igor", ""]]}, {"id": "2005.07473", "submitter": "Fabricio Murai", "authors": "B\\'arbara Silveira, Henrique S. Silva, Fabricio Murai, Ana Paula Couto\n  da Silva", "title": "Predicting User Emotional Tone in Mental Disorder Online Communities", "comments": "8 pages, 3 figures, 3 tables", "journal-ref": "Future Generation Computer Systems, Volume 125, 2021, Pages\n  641-651, ISSN 0167-739X", "doi": "10.1016/j.future.2021.07.014", "report-no": null, "categories": "cs.LG cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Online Social Networks have become an important medium for\npeople who suffer from mental disorders to share moments of hardship, and\nreceive emotional and informational support. In this work, we analyze how\ndiscussions in Reddit communities related to mental disorders can help improve\nthe health conditions of their users. Using the emotional tone of users'\nwriting as a proxy for emotional state, we uncover relationships between user\ninteractions and state changes. First, we observe that authors of negative\nposts often write rosier comments after engaging in discussions, indicating\nthat users' emotional state can improve due to social support. Second, we build\nmodels based on SOTA text embedding techniques and RNNs to predict shifts in\nemotional tone. This differs from most of related work, which focuses primarily\non detecting mental disorders from user activity. We demonstrate the\nfeasibility of accurately predicting the users' reactions to the interactions\nexperienced in these platforms, and present some examples which illustrate that\nthe models are correctly capturing the effects of comments on the author's\nemotional tone. Our models hold promising implications for interventions to\nprovide support for people struggling with mental illnesses.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 11:25:08 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 12:48:29 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Silveira", "B\u00e1rbara", ""], ["Silva", "Henrique S.", ""], ["Murai", "Fabricio", ""], ["da Silva", "Ana Paula Couto", ""]]}, {"id": "2005.07486", "submitter": "Prajjwal Bhargava", "authors": "Prajjwal Bhargava", "title": "Adaptive Transformers for Learning Multimodal Representations", "comments": "Accepted at ACL SRW 2020. Code can be found here\n  https://github.com/prajjwal1/adaptive_transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of transformers has grown from learning about language semantics to\nforming meaningful visiolinguistic representations. These architectures are\noften over-parametrized, requiring large amounts of computation. In this work,\nwe extend adaptive approaches to learn more about model interpretability and\ncomputational efficiency. Specifically, we study attention spans, sparse, and\nstructured dropout methods to help understand how their attention mechanism\nextends for vision and language tasks. We further show that these approaches\ncan help us learn more about how the network perceives the complexity of input\nsequences, sparsity preferences for different modalities, and other related\nphenomena.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 12:12:57 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 06:40:52 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 12:26:12 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Bhargava", "Prajjwal", ""]]}, {"id": "2005.07493", "submitter": "Shubham Agarwal", "authors": "Shubham Agarwal, Trung Bui, Joon-Young Lee, Ioannis Konstas, Verena\n  Rieser", "title": "History for Visual Dialog: Do we really need it?", "comments": "ACL'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Dialog involves \"understanding\" the dialog history (what has been\ndiscussed previously) and the current question (what is asked), in addition to\ngrounding information in the image, to generate the correct response. In this\npaper, we show that co-attention models which explicitly encode dialog history\noutperform models that don't, achieving state-of-the-art performance (72 % NDCG\non val set). However, we also expose shortcomings of the crowd-sourcing dataset\ncollection procedure by showing that history is indeed only required for a\nsmall amount of the data and that the current evaluation metric encourages\ngeneric replies. To that end, we propose a challenging subset (VisDialConv) of\nthe VisDial val set and provide a benchmark of 63% NDCG.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:58:09 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Agarwal", "Shubham", ""], ["Bui", "Trung", ""], ["Lee", "Joon-Young", ""], ["Konstas", "Ioannis", ""], ["Rieser", "Verena", ""]]}, {"id": "2005.07503", "submitter": "Martin Muller", "authors": "Martin M\\\"uller, Marcel Salath\\'e, Per E Kummervold", "title": "COVID-Twitter-BERT: A Natural Language Processing Model to Analyse\n  COVID-19 Content on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based\nmodel, pretrained on a large corpus of Twitter messages on the topic of\nCOVID-19. Our model shows a 10-30% marginal improvement compared to its base\nmodel, BERT-Large, on five different classification datasets. The largest\nimprovements are on the target domain. Pretrained transformer models, such as\nCT-BERT, are trained on a specific target domain and can be used for a wide\nvariety of natural language processing tasks, including classification,\nquestion-answering and chatbots. CT-BERT is optimised to be used on COVID-19\ncontent, in particular social media posts from Twitter.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 12:40:46 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["M\u00fcller", "Martin", ""], ["Salath\u00e9", "Marcel", ""], ["Kummervold", "Per E", ""]]}, {"id": "2005.07505", "submitter": "Jean-Baptiste Camps", "authors": "Jean-Baptiste Camps, Simon Gabay, Paul Fi\\`evre, Thibault Cl\\'erice,\n  Florian Cafiero", "title": "Corpus and Models for Lemmatisation and POS-tagging of Classical French\n  Theatre", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, 2021, Digital\n  humanities in languages (February 14, 2021) jdmdh:7161", "doi": "10.46298/jdmdh.6485", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes the process of building an annotated corpus and training\nmodels for classical French literature, with a focus on theatre, and\nparticularly comedies in verse. It was originally developed as a preliminary\nstep to the stylometric analyses presented in Cafiero and Camps [2019]. The use\nof a recent lemmatiser based on neural networks and a CRF tagger allows to\nachieve accuracies beyond the current state-of-the art on the in-domain test,\nand proves to be robust during out-of-domain tests, i.e.up to 20th c.novels.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 12:47:54 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 15:32:28 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Camps", "Jean-Baptiste", ""], ["Gabay", "Simon", ""], ["Fi\u00e8vre", "Paul", ""], ["Cl\u00e9rice", "Thibault", ""], ["Cafiero", "Florian", ""]]}, {"id": "2005.07522", "submitter": "Yi Zhang", "authors": "Yi Zhang, Tao Ge, Xu Sun", "title": "Parallel Data Augmentation for Formality Style Transfer", "comments": "Accepted by ACL 2020. arXiv admin note: text overlap with\n  arXiv:1909.06002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main barrier to progress in the task of Formality Style Transfer is the\ninadequacy of training data. In this paper, we study how to augment parallel\ndata and propose novel and simple data augmentation methods for this task to\nobtain useful sentence pairs with easily accessible models and systems.\nExperiments demonstrate that our augmented parallel data largely helps improve\nformality style transfer when it is used to pre-train the model, leading to the\nstate-of-the-art results in the GYAFC benchmark dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 04:05:29 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Zhang", "Yi", ""], ["Ge", "Tao", ""], ["Sun", "Xu", ""]]}, {"id": "2005.07604", "submitter": "Felix Hamann", "authors": "Nadja Kurz, Felix Hamann, Adrian Ulges", "title": "Neural Entity Linking on Technical Service Tickets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking, the task of mapping textual mentions to known entities, has\nrecently been tackled using contextualized neural networks. We address the\nquestion whether these results -- reported for large, high-quality datasets\nsuch as Wikipedia -- transfer to practical business use cases, where labels are\nscarce, text is low-quality, and terminology is highly domain-specific. Using\nan entity linking model based on BERT, a popular transformer network in natural\nlanguage processing, we show that a neural approach outperforms and complements\nhand-coded heuristics, with improvements of about 20% top-1 accuracy. Also, the\nbenefits of transfer learning on a large corpus are demonstrated, while\nfine-tuning proves difficult. Finally, we compare different BERT-based\narchitectures and show that a simple sentence-wise encoding (Bi-Encoder) offers\na fast yet efficient search in practice.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:47:02 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:11:16 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kurz", "Nadja", ""], ["Hamann", "Felix", ""], ["Ulges", "Adrian", ""]]}, {"id": "2005.07617", "submitter": "Roman Klinger", "authors": "David Helbig and Enrica Troiano and Roman Klinger", "title": "Challenges in Emotion Style Transfer: An Exploration with a Lexical\n  Substitution Pipeline", "comments": "Accepted at the SocialNLP Workshop at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose the task of emotion style transfer, which is particularly\nchallenging, as emotions (here: anger, disgust, fear, joy, sadness, surprise)\nare on the fence between content and style. To understand the particular\ndifficulties of this task, we design a transparent emotion style transfer\npipeline based on three steps: (1) select the words that are promising to be\nsubstituted to change the emotion (with a brute-force approach and selection\nbased on the attention mechanism of an emotion classifier), (2) find sets of\nwords as candidates for substituting the words (based on lexical and\ndistributional semantics), and (3) select the most promising combination of\nsubstitutions with an objective function which consists of components for\ncontent (based on BERT sentence embeddings), emotion (based on an emotion\nclassifier), and fluency (based on a neural language model). This comparably\nstraight-forward setup enables us to explore the task and understand in what\ncases lexical substitution can vary the emotional load of texts, how changes in\ncontent and style interact and if they are at odds. We further evaluate our\npipeline quantitatively in an automated and an annotation study based on Tweets\nand find, indeed, that simultaneous adjustments of content and emotion are\nconflicting objectives: as we show in a qualitative analysis motivated by\nScherer's emotion component model, this is particularly the case for implicit\nemotion expressions based on cognitive appraisal or descriptions of bodily\nreactions.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 16:11:33 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Helbig", "David", ""], ["Troiano", "Enrica", ""], ["Klinger", "Roman", ""]]}, {"id": "2005.07647", "submitter": "Xavier Suau Cuadros", "authors": "Xavier Suau, Luca Zappella, Nicholas Apostoloff", "title": "Finding Experts in Transformer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work we study the presence of expert units in pre-trained Transformer\nModels (TM), and how they impact a model's performance. We define expert units\nto be neurons that are able to classify a concept with a given average\nprecision, where a concept is represented by a binary set of sentences\ncontaining the concept (or not). Leveraging the OneSec dataset (Scarlini et\nal., 2019), we compile a dataset of 1641 concepts that allows diverse expert\nunits in TM to be discovered. We show that expert units are important in\nseveral ways: (1) The presence of expert units is correlated ($r^2=0.833$) with\nthe generalization power of TM, which allows ranking TM without requiring\nfine-tuning on suites of downstream tasks. We further propose an empirical\nmethod to decide how accurate such experts should be to evaluate\ngeneralization. (2) The overlap of top experts between concepts provides a\nsensible way to quantify concept co-learning, which can be used for\nexplainability of unknown concepts. (3) We show how to self-condition\noff-the-shelf pre-trained language models to generate text with a given concept\nby forcing the top experts to be active, without requiring re-training the\nmodel or using additional parameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:07:02 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Suau", "Xavier", ""], ["Zappella", "Luca", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "2005.07648", "submitter": "Pierre Sermanet", "authors": "Corey Lynch and Pierre Sermanet", "title": "Language Conditioned Imitation Learning over Unstructured Data", "comments": "Published at RSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:08:50 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 23:43:24 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lynch", "Corey", ""], ["Sermanet", "Pierre", ""]]}, {"id": "2005.07654", "submitter": "Asan Agibetov", "authors": "Asan Agibetov, Matthias Samwald", "title": "Benchmarking neural embeddings for link prediction in knowledge graphs\n  under semantic and structural changes", "comments": null, "journal-ref": null, "doi": "10.1016/j.websem.2020.100590", "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, link prediction algorithms based on neural embeddings have gained\ntremendous popularity in the Semantic Web community, and are extensively used\nfor knowledge graph completion. While algorithmic advances have strongly\nfocused on efficient ways of learning embeddings, fewer attention has been\ndrawn to the different ways their performance and robustness can be evaluated.\nIn this work we propose an open-source evaluation pipeline, which benchmarks\nthe accuracy of neural embeddings in situations where knowledge graphs may\nexperience semantic and structural changes. We define relation-centric\nconnectivity measures that allow us to connect the link prediction capacity to\nthe structure of the knowledge graph. Such an evaluation pipeline is especially\nimportant to simulate the accuracy of embeddings for knowledge graphs that are\nexpected to be frequently updated.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:15:45 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 08:11:29 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Agibetov", "Asan", ""], ["Samwald", "Matthias", ""]]}, {"id": "2005.07655", "submitter": "Steven Wilson", "authors": "Steven R. Wilson, Walid Magdy, Barbara McGillivray, Gareth Tyson", "title": "Analyzing Temporal Relationships between Trending Terms on Twitter and\n  Urban Dictionary Activity", "comments": "Accepted at The Web Science Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an online, crowd-sourced, open English-language slang dictionary, the\nUrban Dictionary platform contains a wealth of opinions, jokes, and definitions\nof terms, phrases, acronyms, and more. However, it is unclear exactly how\nactivity on this platform relates to larger conversations happening elsewhere\non the web, such as discussions on larger, more popular social media platforms.\nIn this research, we study the temporal activity trends on Urban Dictionary and\nprovide the first analysis of how this activity relates to content being\ndiscussed on a major social network: Twitter. By collecting the whole of Urban\nDictionary, as well as a large sample of tweets over seven years, we explore\nthe connections between the words and phrases that are defined and searched for\non Urban Dictionary and the content that is talked about on Twitter. Through a\nseries of cross-correlation calculations, we identify cases in which Urban\nDictionary activity closely reflects the larger conversation happening on\nTwitter. Then, we analyze the types of terms that have a stronger connection to\ndiscussions on Twitter, finding that Urban Dictionary activity that is\npositively correlated with Twitter is centered around terms related to memes,\npopular public figures, and offline events. Finally, We explore the\nrelationship between periods of time when terms are trending on Twitter and the\ncorresponding activity on Urban Dictionary, revealing that new definitions are\nmore likely to be added to Urban Dictionary for terms that are currently\ntrending on Twitter.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:17:01 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 15:18:18 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wilson", "Steven R.", ""], ["Magdy", "Walid", ""], ["McGillivray", "Barbara", ""], ["Tyson", "Gareth", ""]]}, {"id": "2005.07667", "submitter": "Jovan Kalajdjieski", "authors": "Jovan Kalajdjieski, Martina Toshevska, Frosina Stojanovska", "title": "Recent Advances in SQL Query Generation: A Survey", "comments": "Part of the 17th International Conference on Informatics and\n  Information Technologies. Received best paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural language is hypothetically the best user interface for many domains.\nHowever, general models that provide an interface between natural language and\nany other domain still do not exist. Providing natural language interface to\nrelational databases could possibly attract a vast majority of users that are\nor are not proficient with query languages. With the rise of deep learning\ntechniques, there is extensive ongoing research in designing a suitable natural\nlanguage interface to relational databases.\n  This survey aims to overview some of the latest methods and models proposed\nin the area of SQL query generation from natural language. We describe models\nwith various architectures such as convolutional neural networks, recurrent\nneural networks, pointer networks, reinforcement learning, etc. Several\ndatasets intended to address the problem of SQL query generation are\ninterpreted and briefly overviewed. In the end, evaluation metrics utilized in\nthe field are presented mainly as a combination of execution accuracy and\nlogical form accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:31:29 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Kalajdjieski", "Jovan", ""], ["Toshevska", "Martina", ""], ["Stojanovska", "Frosina", ""]]}, {"id": "2005.07683", "submitter": "Victor Sanh", "authors": "Victor Sanh, Thomas Wolf, Alexander M. Rush", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning", "comments": "14 pages, 6 figures, 3 tables. Published at NeurIPS2020. Code:\n  \\url{huggingface.co/mvp}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnitude pruning is a widely used strategy for reducing model size in pure\nsupervised learning; however, it is less effective in the transfer learning\nregime that has become standard for state-of-the-art natural language\nprocessing applications. We propose the use of movement pruning, a simple,\ndeterministic first-order weight pruning method that is more adaptive to\npretrained model fine-tuning. We give mathematical foundations to the method\nand compare it to existing zeroth- and first-order pruning methods. Experiments\nshow that when pruning large pretrained language models, movement pruning shows\nsignificant improvements in high-sparsity regimes. When combined with\ndistillation, the approach achieves minimal accuracy loss with down to only 3%\nof the model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:54:15 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:14:58 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sanh", "Victor", ""], ["Wolf", "Thomas", ""], ["Rush", "Alexander M.", ""]]}, {"id": "2005.07692", "submitter": "Altan Cakir", "authors": "Gizem Aras, Didem Makaroglu, Seniz Demir, Altan Cakir", "title": "An Evaluation of Recent Neural Sequence Tagging Models in Turkish Named\n  Entity Recognition", "comments": "Submitted to Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": "ITUAI08", "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is an extensively studied task that extracts\nand classifies named entities in a text. NER is crucial not only in downstream\nlanguage processing applications such as relation extraction and question\nanswering but also in large scale big data operations such as real-time\nanalysis of online digital media content. Recent research efforts on Turkish, a\nless studied language with morphologically rich nature, have demonstrated the\neffectiveness of neural architectures on well-formed texts and yielded\nstate-of-the art results by formulating the task as a sequence tagging problem.\nIn this work, we empirically investigate the use of recent neural architectures\n(Bidirectional long short-term memory and Transformer-based networks) proposed\nfor Turkish NER tagging in the same setting. Our results demonstrate that\ntransformer-based networks which can model long-range context overcome the\nlimitations of BiLSTM networks where different input features at the character,\nsubword, and word levels are utilized. We also propose a transformer-based\nnetwork with a conditional random field (CRF) layer that leads to the\nstate-of-the-art result (95.95\\% f-measure) on a common dataset. Our study\ncontributes to the literature that quantifies the impact of transfer learning\non processing morphologically rich languages.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 06:54:07 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 05:53:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Aras", "Gizem", ""], ["Makaroglu", "Didem", ""], ["Demir", "Seniz", ""], ["Cakir", "Altan", ""]]}, {"id": "2005.07734", "submitter": "Susan Leavy Dr", "authors": "Susan Leavy", "title": "Uncovering Gender Bias in Media Coverage of Politicians with Machine\n  Learning", "comments": "24 pages, 1 figures, 14 tables, Digital Scholarship in Humanities\n  Journal", "journal-ref": "Digital Scholarship in the Humanities 34.1 (2019): 48-63", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents research uncovering systematic gender bias in the\nrepresentation of political leaders in the media, using artificial\nintelligence. Newspaper coverage of Irish ministers over a fifteen year period\nwas gathered and analysed with natural language processing techniques and\nmachine learning. Findings demonstrate evidence of gender bias in the portrayal\nof female politicians, the kind of policies they were associated with and how\nthey were evaluated in terms of their performance as political leaders. This\npaper also sets out a methodology whereby media content may be analysed on a\nlarge scale utilising techniques from artificial intelligence within a\ntheoretical framework founded in gender theory and feminist linguistics.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:37:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Leavy", "Susan", ""]]}, {"id": "2005.07751", "submitter": "Ruben Kruiper", "authors": "Ruben Kruiper, Julian F.V. Vincent, Jessica Chen-Burger, Marc P.Y.\n  Desmulliez, Ioannis Konstas", "title": "In Layman's Terms: Semi-Open Relation Extraction from Scientific Texts", "comments": "To be published in ACL 2020 conference proceedings. Updated dataset\n  statistics, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information Extraction (IE) from scientific texts can be used to guide\nreaders to the central information in scientific documents. But narrow IE\nsystems extract only a fraction of the information captured, and Open IE\nsystems do not perform well on the long and complex sentences encountered in\nscientific texts. In this work we combine the output of both types of systems\nto achieve Semi-Open Relation Extraction, a new task that we explore in the\nBiology domain. First, we present the Focused Open Biological Information\nExtraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow\nscientific IE system to extract trade-off relations and arguments that are\ncentral to biology texts. We then run both the narrow IE system and a\nstate-of-the-art Open IE system on a corpus of 10k open-access scientific\nbiological texts. We show that a significant amount (65%) of erroneous and\nuninformative Open IE extractions can be filtered using narrow IE extractions.\nFurthermore, we show that the retained extractions are significantly more often\ninformative to a reader.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 19:18:26 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:55:40 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kruiper", "Ruben", ""], ["Vincent", "Julian F. V.", ""], ["Chen-Burger", "Jessica", ""], ["Desmulliez", "Marc P. Y.", ""], ["Konstas", "Ioannis", ""]]}, {"id": "2005.07753", "submitter": "Ruben Kruiper", "authors": "Ruben Kruiper, Julian F.V. Vincent, Jessica Chen-Burger, Marc P.Y.\n  Desmulliez, Ioannis Konstas", "title": "A Scientific Information Extraction Dataset for Nature Inspired\n  Engineering", "comments": "Published in Proceedings of the 12th Conference on Language Resources\n  and Evaluation (LREC 2020). Updated dataset statistics, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nature has inspired various ground-breaking technological developments in\napplications ranging from robotics to aerospace engineering and the\nmanufacturing of medical devices. However, accessing the information captured\nin scientific biology texts is a time-consuming and hard task that requires\ndomain-specific knowledge. Improving access for outsiders can help\ninterdisciplinary research like Nature Inspired Engineering. This paper\ndescribes a dataset of 1,500 manually-annotated sentences that express\ndomain-independent relations between central concepts in a scientific biology\ntext, such as trade-offs and correlations. The arguments of these relations can\nbe Multi Word Expressions and have been annotated with modifying phrases to\nform non-projective graphs. The dataset allows for training and evaluating\nRelation Extraction algorithms that aim for coarse-grained typing of scientific\nbiological documents, enabling a high-level filter for engineers.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 19:25:12 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:47:47 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kruiper", "Ruben", ""], ["Vincent", "Julian F. V.", ""], ["Chen-Burger", "Jessica", ""], ["Desmulliez", "Marc P. Y.", ""], ["Konstas", "Ioannis", ""]]}, {"id": "2005.07799", "submitter": "Dan Lim", "authors": "Dan Lim, Won Jang, Gyeonghwan O, Heayoung Park, Bongwan Kim, Jaesam\n  Yoon", "title": "JDI-T: Jointly trained Duration Informed Transformer for Text-To-Speech\n  without Explicit Alignment", "comments": "Accepted for publication in Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Jointly trained Duration Informed Transformer (JDI-T), a\nfeed-forward Transformer with a duration predictor jointly trained without\nexplicit alignments in order to generate an acoustic feature sequence from an\ninput text. In this work, inspired by the recent success of the duration\ninformed networks such as FastSpeech and DurIAN, we further simplify its\nsequential, two-stage training pipeline to a single-stage training.\nSpecifically, we extract the phoneme duration from the autoregressive\nTransformer on the fly during the joint training instead of pretraining the\nautoregressive model and using it as a phoneme duration extractor. To our best\nknowledge, it is the first implementation to jointly train the feed-forward\nTransformer without relying on a pre-trained phoneme duration extractor in a\nsingle training pipeline. We evaluate the effectiveness of the proposed model\non the publicly available Korean Single speaker Speech (KSS) dataset compared\nto the baseline text-to-speech (TTS) models trained by ESPnet-TTS.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 22:06:13 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 01:42:11 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 02:48:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lim", "Dan", ""], ["Jang", "Won", ""], ["O", "Gyeonghwan", ""], ["Park", "Heayoung", ""], ["Kim", "Bongwan", ""], ["Yoon", "Jaesam", ""]]}, {"id": "2005.07809", "submitter": "Zhuohao Chen", "authors": "Zhuohao Chen, Nikolaos Flemotomos, Victor Ardulov, Torrey A. Creed,\n  Zac E. Imel, David C. Atkins, Shrikanth Narayanan", "title": "Feature Fusion Strategies for End-to-End Evaluation of Cognitive\n  Behavior Therapy Sessions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Behavioral Therapy (CBT) is a goal-oriented psychotherapy for\nmental health concerns implemented in a conversational setting with broad\nempirical support for its effectiveness across a range of presenting problems\nand client populations. The quality of a CBT session is typically assessed by\ntrained human raters who manually assign pre-defined session-level behavioral\ncodes. In this paper, we develop an end-to-end pipeline that converts speech\naudio to diarized and transcribed text and extracts linguistic features to code\nthe CBT sessions automatically. We investigate both word-level and\nutterance-level features and propose feature fusion strategies to combine them.\nThe utterance level features include dialog act tags as well as behavioral\ncodes drawn from another well-known talk psychotherapy called Motivational\nInterviewing (MI). We propose a novel method to augment the word-based features\nwith the utterance level tags for subsequent CBT code estimation. Experiments\nshow that our new fusion strategy outperforms all the studied features, both\nwhen used individually and when fused by direct concatenation. We also find\nthat incorporating a sentence segmentation module can further improve the\noverall system given the preponderance of multi-utterance conversational turns\nin CBT sessions.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 22:26:58 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 20:53:36 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Chen", "Zhuohao", ""], ["Flemotomos", "Nikolaos", ""], ["Ardulov", "Victor", ""], ["Creed", "Torrey A.", ""], ["Imel", "Zac E.", ""], ["Atkins", "David C.", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2005.07817", "submitter": "Yanpei Shi", "authors": "Yanpei Shi, Qiang Huang, Thomas Hain", "title": "Weakly Supervised Training of Hierarchical Attention Networks for\n  Speaker Identification", "comments": "Acceptted for presentation at Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying multiple speakers without knowing where a speaker's voice is in a\nrecording is a challenging task. In this paper, a hierarchical attention\nnetwork is proposed to solve a weakly labelled speaker identification problem.\nThe use of a hierarchical structure, consisting of a frame-level encoder and a\nsegment-level encoder, aims to learn speaker related information locally and\nglobally. Speech streams are segmented into fragments. The frame-level encoder\nwith attention learns features and highlights the target related frames\nlocally, and output a fragment based embedding. The segment-level encoder works\nwith a second attention layer to emphasize the fragments probably related to\ntarget speakers. The global information is finally collected from segment-level\nmodule to predict speakers via a classifier. To evaluate the effectiveness of\nthe proposed approach, artificial datasets based on Switchboard Cellular part1\n(SWBC) and Voxceleb1 are constructed in two conditions, where speakers' voices\nare overlapped and not overlapped. Comparing to two baselines the obtained\nresults show that the proposed approach can achieve better performances.\nMoreover, further experiments are conducted to evaluate the impact of utterance\nsegmentation. The results show that a reasonable segmentation can slightly\nimprove identification performances.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 22:57:53 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 15:56:00 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 07:38:52 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Shi", "Yanpei", ""], ["Huang", "Qiang", ""], ["Hain", "Thomas", ""]]}, {"id": "2005.07818", "submitter": "Yanpei Shi", "authors": "Yanpei Shi, Qiang Huang, Thomas Hain", "title": "Speaker Re-identification with Speaker Dependent Speech Enhancement", "comments": "Acceptted for presentation at Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the use of deep neural networks has significantly boosted speaker\nrecognition performance, it is still challenging to separate speakers in poor\nacoustic environments. Here speech enhancement methods have traditionally\nallowed improved performance. The recent works have shown that adapting speech\nenhancement can lead to further gains. This paper introduces a novel approach\nthat cascades speech enhancement and speaker recognition. In the first step, a\nspeaker embedding vector is generated , which is used in the second step to\nenhance the speech quality and re-identify the speakers. Models are trained in\nan integrated framework with joint optimisation. The proposed approach is\nevaluated using the Voxceleb1 dataset, which aims to assess speaker recognition\nin real world situations. In addition three types of noise at different\nsignal-noise-ratios were added for this work. The obtained results show that\nthe proposed approach using speaker dependent speech enhancement can yield\nbetter speaker recognition and speech enhancement performances than two\nbaselines in various noise conditions.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 23:02:10 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 15:54:03 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 07:36:12 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Shi", "Yanpei", ""], ["Huang", "Qiang", ""], ["Hain", "Thomas", ""]]}, {"id": "2005.07820", "submitter": "Saja Khaled Tawalbeh", "authors": "Saja Khaled Tawalbeh, Mahmoud Hammad and Mohammad AL-Smadi", "title": "KEIS@JUST at SemEval-2020 Task 12: Identifying Multilingual Offensive\n  Tweets Using Weighted Ensemble and Fine-Tuned BERT", "comments": "8 pages without references, 4 figures, SemEval 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research presents our team KEIS@JUST participation at SemEval-2020 Task\n12 which represents shared task on multilingual offensive language. We\nparticipated in all the provided languages for all subtasks except sub-task-A\nfor the English language. Two main approaches have been developed the first is\nperformed to tackle both languages Arabic and English, a weighted ensemble\nconsists of Bi-GRU and CNN followed by Gaussian noise and global pooling layer\nmultiplied by weights to improve the overall performance. The second is\nperformed for other languages, a transfer learning from BERT beside the\nrecurrent neural networks such as Bi-LSTM and Bi-GRU followed by a global\naverage pooling layer. Word embedding and contextual embedding have been used\nas features, moreover, data augmentation has been used only for the Arabic\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 23:11:03 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Tawalbeh", "Saja Khaled", ""], ["Hammad", "Mahmoud", ""], ["AL-Smadi", "Mohammad", ""]]}, {"id": "2005.07845", "submitter": "Zitao Liu", "authors": "Gale Yan Huang, Jiahao Chen, Haochen Liu, Weiping Fu, Wenbiao Ding,\n  Jiliang Tang, Songfan Yang, Guoliang Li, Zitao Liu", "title": "Neural Multi-Task Learning for Teacher Question Detection in Online\n  Classrooms", "comments": "The 21th International Conference on Artificial Intelligence in\n  Education(AIED), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asking questions is one of the most crucial pedagogical techniques used by\nteachers in class. It not only offers open-ended discussions between teachers\nand students to exchange ideas but also provokes deeper student thought and\ncritical analysis. Providing teachers with such pedagogical feedback will\nremarkably help teachers improve their overall teaching quality over time in\nclassrooms. Therefore, in this work, we build an end-to-end neural framework\nthat automatically detects questions from teachers' audio recordings. Compared\nwith traditional methods, our approach not only avoids cumbersome feature\nengineering, but also adapts to the task of multi-class question detection in\nreal education scenarios. By incorporating multi-task learning techniques, we\nare able to strengthen the understanding of semantic relations among different\ntypes of questions. We conducted extensive experiments on the question\ndetection tasks in a real-world online classroom dataset and the results\ndemonstrate the superiority of our model in terms of various evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 02:17:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Huang", "Gale Yan", ""], ["Chen", "Jiahao", ""], ["Liu", "Haochen", ""], ["Fu", "Weiping", ""], ["Ding", "Wenbiao", ""], ["Tang", "Jiliang", ""], ["Yang", "Songfan", ""], ["Li", "Guoliang", ""], ["Liu", "Zitao", ""]]}, {"id": "2005.07850", "submitter": "Abdelrahman Mohamed", "authors": "Kritika Singh, Vimal Manohar, Alex Xiao, Sergey Edunov, Ross Girshick,\n  Vitaliy Liptchinsky, Christian Fuegen, Yatharth Saraf, Geoffrey Zweig,\n  Abdelrahman Mohamed", "title": "Large scale weakly and semi-supervised learning for low-resource video\n  ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many semi- and weakly-supervised approaches have been investigated for\novercoming the labeling cost of building high quality speech recognition\nsystems. On the challenging task of transcribing social media videos in\nlow-resource conditions, we conduct a large scale systematic comparison between\ntwo self-labeling methods on one hand, and weakly-supervised pretraining using\ncontextual metadata on the other. We investigate distillation methods at the\nframe level and the sequence level for hybrid, encoder-only CTC-based, and\nencoder-decoder speech recognition systems on Dutch and Romanian languages\nusing 27,000 and 58,000 hours of unlabeled audio respectively. Although all\napproaches improved upon their respective baseline WERs by more than 8%,\nsequence-level distillation for encoder-decoder models provided the largest\nrelative WER reduction of 20% compared to the strongest data-augmented\nsupervised baseline.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 03:08:45 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 01:17:55 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Singh", "Kritika", ""], ["Manohar", "Vimal", ""], ["Xiao", "Alex", ""], ["Edunov", "Sergey", ""], ["Girshick", "Ross", ""], ["Liptchinsky", "Vitaliy", ""], ["Fuegen", "Christian", ""], ["Saraf", "Yatharth", ""], ["Zweig", "Geoffrey", ""], ["Mohamed", "Abdelrahman", ""]]}, {"id": "2005.07877", "submitter": "Zhongxia Yan", "authors": "Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han", "title": "MicroNet for Efficient Language Modeling", "comments": "Accepted by PMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to design compact language models for efficient deployment.\nWe improve upon recent advances in both the language modeling domain and the\nmodel-compression domain to construct parameter and computation efficient\nlanguage models. We use an efficient transformer-based architecture with\nadaptive embedding and softmax, differentiable non-parametric cache, Hebbian\nsoftmax, knowledge distillation, network pruning, and low-bit quantization. In\nthis paper, we provide the winning solution to the NeurIPS 2019 MicroNet\nChallenge in the language modeling track. Compared to the baseline language\nmodel provided by the MicroNet Challenge, our model is 90 times more\nparameter-efficient and 36 times more computation-efficient while achieving the\nrequired test perplexity of 35 on the Wikitext-103 dataset. We hope that this\nwork will aid future research into efficient language models, and we have\nreleased our full source code at\nhttps://github.com/mit-han-lab/neurips-micronet.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 05:42:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yan", "Zhongxia", ""], ["Wang", "Hanrui", ""], ["Guo", "Demi", ""], ["Han", "Song", ""]]}, {"id": "2005.07886", "submitter": "Lei Zhong", "authors": "Lei Zhong, Juan Cao, Qiang Sheng, Junbo Guo, Ziang Wang", "title": "Integrating Semantic and Structural Information with Graph Convolutional\n  Network for Controversy Detection", "comments": "12 pages, 3 figures, 6 tables; To appear in ACL 2020 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying controversial posts on social media is a fundamental task for\nmining public sentiment, assessing the influence of events, and alleviating the\npolarized views. However, existing methods fail to 1) effectively incorporate\nthe semantic information from content-related posts; 2) preserve the structural\ninformation for reply relationship modeling; 3) properly handle posts from\ntopics dissimilar to those in the training set. To overcome the first two\nlimitations, we propose Topic-Post-Comment Graph Convolutional Network\n(TPC-GCN), which integrates the information from the graph structure and\ncontent of topics, posts, and comments for post-level controversy detection. As\nto the third limitation, we extend our model to Disentangled TPC-GCN\n(DTPC-GCN), to disentangle topic-related and topic-unrelated features and then\nfuse dynamically. Extensive experiments on two real-world datasets demonstrate\nthat our models outperform existing methods. Analysis of the results and cases\nproves that our models can integrate both semantic and structural information\nwith significant generalizability.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 06:29:14 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhong", "Lei", ""], ["Cao", "Juan", ""], ["Sheng", "Qiang", ""], ["Guo", "Junbo", ""], ["Wang", "Ziang", ""]]}, {"id": "2005.07897", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Baris Bozkurt, Thierry Dutoit", "title": "Glottal Source Estimation using an Automatic Chirp Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work, we showed that the glottal source can be estimated from\nspeech signals by computing the Zeros of the Z-Transform (ZZT). Decomposition\nwas achieved by separating the roots inside (causal contribution) and outside\n(anticausal contribution) the unit circle. In order to guarantee a correct\ndeconvolution, time alignment on the Glottal Closure Instants (GCIs) was shown\nto be essential. This paper extends the formalism of ZZT by evaluating the\nZ-transform on a contour possibly different from the unit circle. A method is\nproposed for determining automatically this contour by inspecting the root\ndistribution. The derived Zeros of the Chirp Z-Transform (ZCZT)-based technique\nturns out to be much more robust to GCI location errors.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:10:38 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Drugman", "Thomas", ""], ["Bozkurt", "Baris", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2005.07901", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Thierry Dutoit", "title": "Oscillating Statistical Moments for Speech Polarity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An inversion of the speech polarity may have a dramatic detrimental effect on\nthe performance of various techniques of speech processing. An automatic method\nfor determining the speech polarity (which is dependent upon the recording\nsetup) is thus required as a preliminary step for ensuring the well-behaviour\nof such techniques. This paper proposes a new approach of polarity detection\nrelying on oscillating statistical moments. These moments have the property to\noscillate at the local fundamental frequency and to exhibit a phase shift which\ndepends on the speech polarity. This dependency stems from the introduction of\nnon-linearity or higher-order statistics in the moment calculation. The\nresulting method is shown on 10 speech corpora to provide a substantial\nimprovement compared to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:16:43 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Drugman", "Thomas", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2005.07903", "submitter": "Zhengkun Tian", "authors": "Zhengkun Tian and Jiangyan Yi and Jianhua Tao and Ye Bai and Shuai\n  Zhang and Zhengqi Wen", "title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech\n  Recognition", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-autoregressive transformer models have achieved extremely fast inference\nspeed and comparable performance with autoregressive sequence-to-sequence\nmodels in neural machine translation. Most of the non-autoregressive\ntransformers decode the target sequence from a predefined-length mask sequence.\nIf the predefined length is too long, it will cause a lot of redundant\ncalculations. If the predefined length is shorter than the length of the target\nsequence, it will hurt the performance of the model. To address this problem\nand improve the inference speed, we propose a spike-triggered\nnon-autoregressive transformer model for end-to-end speech recognition, which\nintroduces a CTC module to predict the length of the target sequence and\naccelerate the convergence. All the experiments are conducted on a public\nChinese mandarin dataset AISHELL-1. The results show that the proposed model\ncan accurately predict the length of the target sequence and achieve a\ncompetitive performance with the advanced transformers. What's more, the model\neven achieves a real-time factor of 0.0056, which exceeds all mainstream speech\nrecognition models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:27:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Tian", "Zhengkun", ""], ["Yi", "Jiangyan", ""], ["Tao", "Jianhua", ""], ["Bai", "Ye", ""], ["Zhang", "Shuai", ""], ["Wen", "Zhengqi", ""]]}, {"id": "2005.07920", "submitter": "Burin Naowarat", "authors": "Burin Naowarat, Thananchai Kongthaworn, Korrawe Karunratanakul, Sheng\n  Hui Wu, Ekapol Chuangsuwanich", "title": "Reducing Spelling Inconsistencies in Code-Switching ASR using\n  Contextualized CTC Loss", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-Switching (CS) remains a challenge for Automatic Speech Recognition\n(ASR), especially character-based models. With the combined choice of\ncharacters from multiple languages, the outcome from character-based models\nsuffers from phoneme duplication, resulting in language-inconsistent spellings.\nWe propose Contextualized Connectionist Temporal Classification (CCTC) loss to\nencourage spelling consistencies of a character-based non-autoregressive ASR\nwhich allows for faster inference. The CCTC loss conditions the main prediction\non the predicted contexts to ensure language consistency in the spellings. In\ncontrast to existing CTC-based approaches, CCTC loss does not require\nframe-level alignments, since the context ground truth is obtained from the\nmodel's estimated path. Compared to the same model trained with regular CTC\nloss, our method consistently improved the ASR performance on both CS and\nmonolingual corpora.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 09:36:58 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 11:10:25 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 18:21:30 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Naowarat", "Burin", ""], ["Kongthaworn", "Thananchai", ""], ["Karunratanakul", "Korrawe", ""], ["Wu", "Sheng Hui", ""], ["Chuangsuwanich", "Ekapol", ""]]}, {"id": "2005.07923", "submitter": "Junfeng Jiang", "authors": "Chao Xiong, Che Liu, Zijun Xu, Junfeng Jiang, Jieping Ye", "title": "Sequential Sentence Matching Network for Multi-turn Response Selection\n  in Retrieval-based Chatbots", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, open domain multi-turn chatbots have attracted much interest from\nlots of researchers in both academia and industry. The dominant retrieval-based\nmethods use context-response matching mechanisms for multi-turn response\nselection. Specifically, the state-of-the-art methods perform the\ncontext-response matching by word or segment similarity. However, these models\nlack a full exploitation of the sentence-level semantic information, and make\nsimple mistakes that humans can easily avoid. In this work, we propose a\nmatching network, called sequential sentence matching network (S2M), to use the\nsentence-level semantic information to address the problem. Firstly and most\nimportantly, we find that by using the sentence-level semantic information, the\nnetwork successfully addresses the problem and gets a significant improvement\non matching, resulting in a state-of-the-art performance. Furthermore, we\nintegrate the sentence matching we introduced here and the usual word\nsimilarity matching reported in the current literature, to match at different\nsemantic levels. Experiments on three public data sets show that such\nintegration further improves the model performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 09:47:19 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Xiong", "Chao", ""], ["Liu", "Che", ""], ["Xu", "Zijun", ""], ["Jiang", "Junfeng", ""], ["Ye", "Jieping", ""]]}, {"id": "2005.07934", "submitter": "Dawid Jurkiewicz", "authors": "Dawid Jurkiewicz, {\\L}ukasz Borchmann, Izabela Kosmala, Filip\n  Grali\\'nski", "title": "ApplicaAI at SemEval-2020 Task 11: On RoBERTa-CRF, Span CLS and Whether\n  Self-Training Helps Them", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the winning system for the propaganda Technique\nClassification (TC) task and the second-placed system for the propaganda Span\nIdentification (SI) task. The purpose of TC task was to identify an applied\npropaganda technique given propaganda text fragment. The goal of SI task was to\nfind specific text fragments which contain at least one propaganda technique.\nBoth of the developed solutions used semi-supervised learning technique of\nself-training. Interestingly, although CRF is barely used with\ntransformer-based language models, the SI task was approached with RoBERTa-CRF\narchitecture. An ensemble of RoBERTa-based models was proposed for the TC task,\nwith one of them making use of Span CLS layers we introduce in the present\npaper. In addition to describing the submitted systems, an impact of\narchitectural decisions and training schemes is investigated along with remarks\nregarding training models of the same or better quality with lower\ncomputational budget. Finally, the results of error analysis are presented.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:15:49 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 13:04:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Jurkiewicz", "Dawid", ""], ["Borchmann", "\u0141ukasz", ""], ["Kosmala", "Izabela", ""], ["Grali\u0144ski", "Filip", ""]]}, {"id": "2005.07954", "submitter": "Izumi Haruta", "authors": "Izumi Haruta, Koji Mineshima, Daisuke Bekki", "title": "Logical Inferences with Comparatives and Generalized Quantifiers", "comments": "To appear in the Proceedings of the Association for Computational\n  Linguistics: Student Research Workshop (ACL-SRW 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative constructions pose a challenge in Natural Language Inference\n(NLI), which is the task of determining whether a text entails a hypothesis.\nComparatives are structurally complex in that they interact with other\nlinguistic phenomena such as quantifiers, numerals, and lexical antonyms. In\nformal semantics, there is a rich body of work on comparatives and gradable\nexpressions using the notion of degree. However, a logical inference system for\ncomparatives has not been sufficiently developed for use in the NLI task. In\nthis paper, we present a compositional semantics that maps various comparative\nconstructions in English to semantic representations via Combinatory Categorial\nGrammar (CCG) parsers and combine it with an inference system based on\nautomated theorem proving. We evaluate our system on three NLI datasets that\ncontain complex logical inferences with comparatives, generalized quantifiers,\nand numerals. We show that the system outperforms previous logic-based systems\nas well as recent deep learning-based models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 11:11:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Haruta", "Izumi", ""], ["Mineshima", "Koji", ""], ["Bekki", "Daisuke", ""]]}, {"id": "2005.07973", "submitter": "Afroz Ahamad Siddiqui", "authors": "Afroz Ahamad, Ankit Anand, Pranesh Bhargava", "title": "AccentDB: A Database of Non-Native English Accents to Assist Neural\n  Speech Recognition", "comments": "Proceedings of the 12th Language Resources and Evaluation Conference\n  - LREC, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Automatic Speech Recognition (ASR) technology has evolved to identify\nthe speech spoken by native speakers of a language very well. However,\nidentification of the speech spoken by non-native speakers continues to be a\nmajor challenge for it. In this work, we first spell out the key requirements\nfor creating a well-curated database of speech samples in non-native accents\nfor training and testing robust ASR systems. We then introduce AccentDB, one\nsuch database that contains samples of 4 Indian-English accents collected by\nus, and a compilation of samples from 4 native-English, and a metropolitan\nIndian-English accent. We also present an analysis on separability of the\ncollected accent data. Further, we present several accent classification models\nand evaluate them thoroughly against human-labelled accent classes. We test the\ngeneralization of our classifier models in a variety of setups of seen and\nunseen data. Finally, we introduce the task of accent neutralization of\nnon-native accents to native accents using autoencoder models with\ntask-specific architectures. Thus, our work aims to aid ASR systems at every\nstage of development with a database for training, classification models for\nfeature augmentation, and neutralization systems for acoustic transformations\nof non-native accents of English.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 12:38:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ahamad", "Afroz", ""], ["Anand", "Ankit", ""], ["Bhargava", "Pranesh", ""]]}, {"id": "2005.07979", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Christoph Ringlstetter and Hinrich Sch\\\"utze", "title": "Unsupervised Embedding-based Detection of Lexical Semantic Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes EmbLexChange, a system introduced by the \"Life-Language\"\nteam for SemEval-2020 Task 1, on unsupervised detection of lexical-semantic\nchanges. EmbLexChange is defined as the divergence between the embedding based\nprofiles of word w (calculated with respect to a set of reference words) in the\nsource and the target domains (source and target domains can be simply two time\nframes t1 and t2). The underlying assumption is that the lexical-semantic\nchange of word w would affect its co-occurring words and subsequently alters\nthe neighborhoods in the embedding spaces. We show that using a resampling\nframework for the selection of reference words, we can reliably detect\nlexical-semantic changes in English, German, Swedish, and Latin. EmbLexChange\nachieved second place in the binary detection of semantic changes in the\nSemEval-2020.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 13:05:47 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Ringlstetter", "Christoph", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2005.07988", "submitter": "Xiao Li", "authors": "Xiao Li, Kees van Deemter, and Chenghua Lin", "title": "A Text Reassembling Approach to Natural Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a number of proposals for performing Natural Language\nGeneration (NLG) based in large part on statistical techniques. Despite having\nmany attractive features, we argue that these existing approaches nonetheless\nhave some important drawbacks, sometimes because the approach in question is\nnot fully statistical (i.e., relies on a certain amount of handcrafting),\nsometimes because the approach in question lacks transparency. Focussing on\nsome of the key NLG tasks (namely Content Selection, Lexical Choice, and\nLinguistic Realisation), we propose a novel approach, called the Text\nReassembling approach to NLG (TRG), which approaches the ideal of a purely\nstatistical approach very closely, and which is at the same time highly\ntransparent. We evaluate the TRG approach and discuss how TRG may be extended\nto deal with other NLG tasks, such as Document Structuring, and Aggregation. We\ndiscuss the strengths and limitations of TRG, concluding that the method may\nhold particular promise for domain experts who want to build an NLG system\ndespite having little expertise in linguistics and NLG.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 13:28:17 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 10:33:57 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 15:31:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Li", "Xiao", ""], ["van Deemter", "Kees", ""], ["Lin", "Chenghua", ""]]}, {"id": "2005.08024", "submitter": "Tao Tu", "authors": "Tao Tu, Yuan-Jui Chen, Alexander H. Liu, Hung-yi Lee", "title": "Semi-supervised Learning for Multi-speaker Text-to-speech Synthesis\n  Using Discrete Speech Representation", "comments": "Interspeech 2020, https://github.com/ttaoREtw/semi-tts", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end multi-speaker text-to-speech (TTS) systems gain success\nin the situation where a lot of high-quality speech plus their corresponding\ntranscriptions are available. However, laborious paired data collection\nprocesses prevent many institutes from building multi-speaker TTS systems of\ngreat performance. In this work, we propose a semi-supervised learning approach\nfor multi-speaker TTS. A multi-speaker TTS model can learn from the\nuntranscribed audio via the proposed encoder-decoder framework with discrete\nspeech representation. The experiment results demonstrate that with only an\nhour of paired speech data, no matter the paired data is from multiple speakers\nor a single speaker, the proposed model can generate intelligible speech in\ndifferent voices. We found the model can benefit from the proposed\nsemi-supervised learning approach even when part of the unpaired speech data is\nnoisy. In addition, our analysis reveals that different speaker characteristics\nof the paired data have an impact on the effectiveness of semi-supervised TTS.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 15:47:11 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 07:55:34 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Tu", "Tao", ""], ["Chen", "Yuan-Jui", ""], ["Liu", "Alexander H.", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2005.08025", "submitter": "Alexey Svyatkovskiy", "authors": "Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, Neel Sundaresan", "title": "IntelliCode Compose: Code Generation Using Transformer", "comments": "Accepted for publication at ESEC/FSE conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In software development through integrated development environments (IDEs),\ncode completion is one of the most widely used features. Nevertheless, majority\nof integrated development environments only support completion of methods and\nAPIs, or arguments.\n  In this paper, we introduce IntelliCode Compose $-$ a general-purpose\nmultilingual code completion tool which is capable of predicting sequences of\ncode tokens of arbitrary types, generating up to entire lines of syntactically\ncorrect code. It leverages state-of-the-art generative transformer model\ntrained on 1.2 billion lines of source code in Python, $C\\#$, JavaScript and\nTypeScript programming languages. IntelliCode Compose is deployed as a\ncloud-based web service. It makes use of client-side tree-based caching,\nefficient parallel implementation of the beam search decoder, and compute graph\noptimizations to meet edit-time completion suggestion requirements in the\nVisual Studio Code IDE and Azure Notebook.\n  Our best model yields an average edit similarity of $86.7\\%$ and a perplexity\nof 1.82 for Python programming language.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 15:47:53 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:40:12 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Svyatkovskiy", "Alexey", ""], ["Deng", "Shao Kun", ""], ["Fu", "Shengyu", ""], ["Sundaresan", "Neel", ""]]}, {"id": "2005.08042", "submitter": "Yongqiang Wang", "authors": "Chunyang Wu, Yongqiang Wang, Yangyang Shi, Ching-Feng Yeh, Frank Zhang", "title": "Streaming Transformer-based Acoustic Models Using Self-attention with\n  Augmented Memory", "comments": "submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based acoustic modeling has achieved great suc-cess for both\nhybrid and sequence-to-sequence speech recogni-tion. However, it requires\naccess to the full sequence, and thecomputational cost grows quadratically with\nrespect to the in-put sequence length. These factors limit its adoption for\nstream-ing applications. In this work, we proposed a novel augmentedmemory\nself-attention, which attends on a short segment of theinput sequence and a\nbank of memories. The memory bankstores the embedding information for all the\nprocessed seg-ments. On the librispeech benchmark, our proposed\nmethodoutperforms all the existing streamable transformer methods bya large\nmargin and achieved over 15% relative error reduction,compared with the widely\nused LC-BLSTM baseline. Our find-ings are also confirmed on some large internal\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 16:54:52 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wu", "Chunyang", ""], ["Wang", "Yongqiang", ""], ["Shi", "Yangyang", ""], ["Yeh", "Ching-Feng", ""], ["Zhang", "Frank", ""]]}, {"id": "2005.08053", "submitter": "Qiang Huang", "authors": "Qiang Huang and Thomas Hain", "title": "Exploration of Audio Quality Assessment and Anomaly Localisation Using\n  Attention Models", "comments": "Submitted to InterSpeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of speech technology require more and more audio data.\nAutomatic assessment of the quality of the collected recordings is important to\nensure they meet the requirements of the related applications. However,\neffective and high performing assessment remains a challenging task without a\nclean reference. In this paper, a novel model for audio quality assessment is\nproposed by jointly using bidirectional long short-term memory and an attention\nmechanism. The former is to mimic a human auditory perception ability to learn\ninformation from a recording, and the latter is to further discriminate\ninterferences from desired signals by highlighting target related features. To\nevaluate our proposed approach, the TIMIT dataset is used and augmented by\nmixing with various natural sounds. In our experiments, two tasks are explored.\nThe first task is to predict an utterance quality score, and the second is to\nidentify where an anomalous distortion takes place in a recording. The obtained\nresults show that the use of our proposed approach outperforms a strong\nbaseline method and gains about 5% improvements after being measured by three\nmetrics, Linear Correlation Coefficient and Spearman Rank Correlation\nCoefficient, and F1.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 17:54:07 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Huang", "Qiang", ""], ["Hain", "Thomas", ""]]}, {"id": "2005.08056", "submitter": "Hongyu Gong", "authors": "Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, Dong Yu", "title": "Recurrent Chunking Mechanisms for Long-Text Machine Reading\n  Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study machine reading comprehension (MRC) on long texts,\nwhere a model takes as inputs a lengthy document and a question and then\nextracts a text span from the document as an answer. State-of-the-art models\ntend to use a pretrained transformer model (e.g., BERT) to encode the joint\ncontextual information of document and question. However, these\ntransformer-based models can only take a fixed-length (e.g., 512) text as its\ninput. To deal with even longer text inputs, previous approaches usually chunk\nthem into equally-spaced segments and predict answers based on each segment\nindependently without considering the information from other segments. As a\nresult, they may form segments that fail to cover the correct answer span or\nretain insufficient contexts around it, which significantly degrades the\nperformance. Moreover, they are less capable of answering questions that need\ncross-segment information.\n  We propose to let a model learn to chunk in a more flexible way via\nreinforcement learning: a model can decide the next segment that it wants to\nprocess in either direction. We also employ recurrent mechanisms to enable\ninformation to flow across segments. Experiments on three MRC datasets -- CoQA,\nQuAC, and TriviaQA -- demonstrate the effectiveness of our proposed recurrent\nchunking mechanisms: we can obtain segments that are more likely to contain\ncomplete answers and at the same time provide sufficient contexts around the\nground truth answers for better predictions.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 18:08:58 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:29:08 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gong", "Hongyu", ""], ["Shen", "Yelong", ""], ["Yu", "Dian", ""], ["Chen", "Jianshu", ""], ["Yu", "Dong", ""]]}, {"id": "2005.08081", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xuancheng Ren, Guangxiang Zhao, Xu Sun", "title": "Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In sequence-to-sequence learning, the decoder relies on the attention\nmechanism to efficiently extract information from the encoder. While it is\ncommon practice to draw information from only the last encoder layer, recent\nwork has proposed to use representations from different encoder layers for\ndiversified levels of information. Nonetheless, the decoder still obtains only\na single view of the source sequences, which might lead to insufficient\ntraining of the encoder layer stack due to the hierarchy bypassing problem. In\nthis work, we propose layer-wise cross-view decoding, where for each decoder\nlayer, together with the representations from the last encoder layer, which\nserve as a global view, those from other encoder layers are supplemented for a\nstereoscopic view of the source sequences. Systematic experiments show that we\nsuccessfully address the hierarchy bypassing problem and substantially improve\nthe performance of sequence-to-sequence learning with deep representations on\ndiverse tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:00:39 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 10:21:33 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 05:58:38 GMT"}, {"version": "v4", "created": "Sat, 15 May 2021 08:52:22 GMT"}, {"version": "v5", "created": "Sun, 4 Jul 2021 08:38:40 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Liu", "Fenglin", ""], ["Ren", "Xuancheng", ""], ["Zhao", "Guangxiang", ""], ["Sun", "Xu", ""]]}, {"id": "2005.08105", "submitter": "Mingda Chen", "authors": "Mingda Chen, Kevin Gimpel", "title": "Learning Probabilistic Sentence Representations from Paraphrases", "comments": "Repl4NLP at ACL 2020, short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic word embeddings have shown effectiveness in capturing notions\nof generality and entailment, but there is very little work on doing the\nanalogous type of investigation for sentences. In this paper we define\nprobabilistic models that produce distributions for sentences. Our\nbest-performing model treats each word as a linear transformation operator\napplied to a multivariate Gaussian distribution. We train our models on\nparaphrases and demonstrate that they naturally capture sentence specificity.\nWhile our proposed model achieves the best performance overall, we also show\nthat specificity is represented by simpler architectures via the norm of the\nsentence vectors. Qualitative analysis shows that our probabilistic model\ncaptures sentential entailment and provides ways to analyze the specificity and\npreciseness of individual words.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:10:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chen", "Mingda", ""], ["Gimpel", "Kevin", ""]]}, {"id": "2005.08113", "submitter": "Xuhui Zhou", "authors": "Xuhui Zhou, Zaixiang Zheng, Shujian Huang", "title": "RPD: A Distance Function Between Word Embeddings", "comments": "ACL Student Research Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-understood that different algorithms, training processes, and\ncorpora produce different word embeddings. However, less is known about the\nrelation between different embedding spaces, i.e. how far different sets of\nembeddings deviate from each other. In this paper, we propose a novel metric\ncalled Relative pairwise inner Product Distance (RPD) to quantify the distance\nbetween different sets of word embeddings. This metric has a unified scale for\ncomparing different sets of word embeddings. Based on the properties of RPD, we\nstudy the relations of word embeddings of different algorithms systematically\nand investigate the influence of different training processes and corpora. The\nresults shed light on the poorly understood word embeddings and justify RPD as\na measure of the distance of embedding spaces.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:53:31 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhou", "Xuhui", ""], ["Zheng", "Zaixiang", ""], ["Huang", "Shujian", ""]]}, {"id": "2005.08118", "submitter": "Piotr \\.Zelasko", "authors": "Piotr \\.Zelasko, Laureano Moro-Vel\\'azquez, Mark Hasegawa-Johnson,\n  Odette Scharenborg, Najim Dehak", "title": "That Sounds Familiar: an Analysis of Phonetic Representations Transfer\n  Across Languages", "comments": "Submitted to Interspeech 2020. For some reason, the ArXiv Latex\n  engine rendered it in more than 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only a handful of the world's languages are abundant with the resources that\nenable practical applications of speech processing technologies. One of the\nmethods to overcome this problem is to use the resources existing in other\nlanguages to train a multilingual automatic speech recognition (ASR) model,\nwhich, intuitively, should learn some universal phonetic representations. In\nthis work, we focus on gaining a deeper understanding of how general these\nrepresentations might be, and how individual phones are getting improved in a\nmultilingual setting. To that end, we select a phonetically diverse set of\nlanguages, and perform a series of monolingual, multilingual and crosslingual\n(zero-shot) experiments. The ASR is trained to recognize the International\nPhonetic Alphabet (IPA) token sequences. We observe significant improvements\nacross all languages in the multilingual setting, and stark degradation in the\ncrosslingual setting, where the model, among other errors, considers Javanese\nas a tone language. Notably, as little as 10 hours of the target language\ntraining data tremendously reduces ASR error rates. Our analysis uncovered that\neven the phones that are unique to a single language can benefit greatly from\nadding training data from other languages - an encouraging result for the\nlow-resource speech community.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 22:28:09 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["\u017belasko", "Piotr", ""], ["Moro-Vel\u00e1zquez", "Laureano", ""], ["Hasegawa-Johnson", "Mark", ""], ["Scharenborg", "Odette", ""], ["Dehak", "Najim", ""]]}, {"id": "2005.08146", "submitter": "Somin Wadhwa", "authors": "Somin Wadhwa, Kanhua Yin, Kevin S. Hughes, Byron C. Wallace", "title": "Semi-Automating Knowledge Base Construction for Cancer Genetics", "comments": "In proceedings of the Conference on Automated Knowledge Base\n  Construction (AKBC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the exponentially growing subarea of genetics in\ncancer. The need to synthesize and centralize this evidence for dissemination\nhas motivated a team of physicians to manually construct and maintain a\nknowledge base that distills key results reported in the literature. This is a\nlaborious process that entails reading through full-text articles to understand\nthe study design, assess study quality, and extract the reported cancer risk\nestimates associated with particular hereditary cancer genes (i.e.,\npenetrance). In this work, we propose models to automatically surface key\nelements from full-text cancer genetics articles, with the ultimate aim of\nexpediting the manual workflow currently in place.\n  We propose two challenging tasks that are critical for characterizing the\nfindings reported cancer genetics studies: (i) Extracting snippets of text that\ndescribe \\emph{ascertainment mechanisms}, which in turn inform whether the\npopulation studied may introduce bias owing to deviations from the target\npopulation; (ii) Extracting reported risk estimates (e.g., odds or hazard\nratios) associated with specific germline mutations. The latter task may be\nviewed as a joint entity tagging and relation extraction problem. To train\nmodels for these tasks, we induce distant supervision over tokens and snippets\nin full-text articles using the manually constructed knowledge base. We propose\nand evaluate several model variants, including a transformer-based joint entity\nand relation extraction model to extract <germline mutation, risk-estimate>}\npairs. We observe strong empirical performance, highlighting the practical\npotential for such models to aid KB construction in this space. We ablate\ncomponents of our model, observing, e.g., that a joint model for <germline\nmutation, risk-estimate> fares substantially better than a pipelined approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 02:01:43 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 00:47:33 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wadhwa", "Somin", ""], ["Yin", "Kanhua", ""], ["Hughes", "Kevin S.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.08156", "submitter": "Lis Pereira", "authors": "Lis Pereira, Xiaodong Liu, Fei Cheng, Masayuki Asahara, Ichiro\n  Kobayashi", "title": "Adversarial Training for Commonsense Inference", "comments": "6 pages, Accepted to ACL2020 RepL4NLP workshop", "journal-ref": "ACL2020 RepL4NLP workshop", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an AdversariaL training algorithm for commonsense InferenCE\n(ALICE). We apply small perturbations to word embeddings and minimize the\nresultant adversarial risk to regularize the model. We exploit a novel\ncombination of two different approaches to estimate these perturbations: 1)\nusing the true label and 2) using the model prediction. Without relying on any\nhuman-crafted features, knowledge bases, or additional datasets other than the\ntarget datasets, our model boosts the fine-tuning performance of RoBERTa,\nachieving competitive results on multiple reading comprehension datasets that\nrequire commonsense inference.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 03:28:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pereira", "Lis", ""], ["Liu", "Xiaodong", ""], ["Cheng", "Fei", ""], ["Asahara", "Masayuki", ""], ["Kobayashi", "Ichiro", ""]]}, {"id": "2005.08177", "submitter": "Tyler Chang", "authors": "Tyler A. Chang and Anna N. Rafferty", "title": "Encodings of Source Syntax: Similarities in NMT Representations Across\n  Target Languages", "comments": "To appear at the 5th Workshop on Representation Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train neural machine translation (NMT) models from English to six target\nlanguages, using NMT encoder representations to predict ancestor constituent\nlabels of source language words. We find that NMT encoders learn similar source\nsyntax regardless of NMT target language, relying on explicit morphosyntactic\ncues to extract syntactic features from source sentences. Furthermore, the NMT\nencoders outperform RNNs trained directly on several of the constituent label\nprediction tasks, suggesting that NMT encoder representations can be used\neffectively for natural language tasks involving syntax. However, both the NMT\nencoders and the directly-trained RNNs learn substantially different syntactic\ninformation from a probabilistic context-free grammar (PCFG) parser. Despite\nlower overall accuracy scores, the PCFG often performs well on sentences for\nwhich the RNN-based models perform poorly, suggesting that RNN architectures\nare constrained in the types of syntax they can learn.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 06:41:32 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chang", "Tyler A.", ""], ["Rafferty", "Anna N.", ""]]}, {"id": "2005.08178", "submitter": "Keshav Kolluru", "authors": "Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam and Soumen\n  Chakrabarti", "title": "IMoJIE: Iterative Memory-Based Joint Open Information Extraction", "comments": null, "journal-ref": "ACL 2020, Long paper", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional systems for Open Information Extraction were statistical\nand rule-based, recently neural models have been introduced for the task. Our\nwork builds upon CopyAttention, a sequence generation OpenIE model (Cui et.\nal., 2018). Our analysis reveals that CopyAttention produces a constant number\nof extractions per sentence, and its extracted tuples often express redundant\ninformation.\n  We present IMoJIE, an extension to CopyAttention, which produces the next\nextraction conditioned on all previously extracted tuples. This approach\novercomes both shortcomings of CopyAttention, resulting in a variable number of\ndiverse extractions per sentence. We train IMoJIE on training data bootstrapped\nfrom extractions of several non-neural systems, which have been automatically\nfiltered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by\nabout 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a\nnew state of the art for the task.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 07:04:08 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kolluru", "Keshav", ""], ["Aggarwal", "Samarth", ""], ["Rathore", "Vipul", ""], ["Mausam", "", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2005.08182", "submitter": "Manraj Singh Grover", "authors": "Manraj Singh Grover, Yaman Kumar, Sumit Sarin, Payman Vafaee, Mika\n  Hama, Rajiv Ratn Shah", "title": "Multi-modal Automated Speech Scoring using Attention Fusion", "comments": "Submitted to INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel multi-modal end-to-end neural approach for\nautomated assessment of non-native English speakers' spontaneous speech using\nattention fusion. The pipeline employs Bi-directional Recurrent Convolutional\nNeural Networks and Bi-directional Long Short-Term Memory Neural Networks to\nencode acoustic and lexical cues from spectrograms and transcriptions,\nrespectively. Attention fusion is performed on these learned predictive\nfeatures to learn complex interactions between different modalities before\nfinal scoring. We compare our model with strong baselines and find combined\nattention to both lexical and acoustic cues significantly improves the overall\nperformance of the system. Further, we present a qualitative and quantitative\nanalysis of our model.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 07:53:15 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Grover", "Manraj Singh", ""], ["Kumar", "Yaman", ""], ["Sarin", "Sumit", ""], ["Vafaee", "Payman", ""], ["Hama", "Mika", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2005.08188", "submitter": "Juntao Li", "authors": "Juntao Li, Chang Liu, Jian Wang, Lidong Bing, Hongsong Li, Xiaozhong\n  Liu, Dongyan Zhao, Rui Yan", "title": "Cross-Lingual Low-Resource Set-to-Description Retrieval for Global\n  E-Commerce", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prosperous of cross-border e-commerce, there is an urgent demand for\ndesigning intelligent approaches for assisting e-commerce sellers to offer\nlocal products for consumers from all over the world. In this paper, we explore\na new task of cross-lingual information retrieval, i.e., cross-lingual\nset-to-description retrieval in cross-border e-commerce, which involves\nmatching product attribute sets in the source language with persuasive product\ndescriptions in the target language. We manually collect a new and high-quality\npaired dataset, where each pair contains an unordered product attribute set in\nthe source language and an informative product description in the target\nlanguage. As the dataset construction process is both time-consuming and\ncostly, the new dataset only comprises of 13.5k pairs, which is a low-resource\nsetting and can be viewed as a challenging testbed for model development and\nevaluation in cross-border e-commerce. To tackle this cross-lingual\nset-to-description retrieval task, we propose a novel cross-lingual matching\nnetwork (CLMN) with the enhancement of context-dependent cross-lingual mapping\nupon the pre-trained monolingual BERT representations. Experimental results\nindicate that our proposed CLMN yields impressive results on the challenging\ntask and the context-dependent cross-lingual mapping on BERT yields noticeable\nimprovement over the pre-trained multi-lingual BERT model.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 08:10:51 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Juntao", ""], ["Liu", "Chang", ""], ["Wang", "Jian", ""], ["Bing", "Lidong", ""], ["Li", "Hongsong", ""], ["Liu", "Xiaozhong", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2005.08199", "submitter": "Sumeet Agarwal", "authors": "Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal", "title": "How much complexity does an RNN architecture need to learn\n  syntax-sensitive dependencies?", "comments": "11 pages, 5 figures (including appendix); to appear at ACL SRW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long short-term memory (LSTM) networks and their variants are capable of\nencapsulating long-range dependencies, which is evident from their performance\non a variety of linguistic tasks. On the other hand, simple recurrent networks\n(SRNs), which appear more biologically grounded in terms of synaptic\nconnections, have generally been less successful at capturing long-range\ndependencies as well as the loci of grammatical errors in an unsupervised\nsetting. In this paper, we seek to develop models that bridge the gap between\nbiological plausibility and linguistic competence. We propose a new\narchitecture, the Decay RNN, which incorporates the decaying nature of neuronal\nactivations and models the excitatory and inhibitory connections in a\npopulation of neurons. Besides its biological inspiration, our model also shows\ncompetitive performance relative to LSTMs on subject-verb agreement, sentence\ngrammaticality, and language modeling tasks. These results provide some\npointers towards probing the nature of the inductive biases required for RNN\narchitectures to model linguistic phenomena successfully.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 09:13:28 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 10:18:27 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bhatt", "Gantavya", ""], ["Bansal", "Hritik", ""], ["Singh", "Rishubh", ""], ["Agarwal", "Sumeet", ""]]}, {"id": "2005.08206", "submitter": "Ben Eyal", "authors": "Ben Eyal and Michael Elhadad", "title": "Building a Hebrew Semantic Role Labeling Lexical Resource from Parallel\n  Movie Subtitles", "comments": "9 pages, 7 figures, accepted to LREC 2020", "journal-ref": "Proceedings of The 12th Language Resources and Evaluation\n  Conference (2020) 5936-5944", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a semantic role labeling resource for Hebrew built\nsemi-automatically through annotation projection from English. This corpus is\nderived from the multilingual OpenSubtitles dataset and includes short informal\nsentences, for which reliable linguistic annotations have been computed. We\nprovide a fully annotated version of the data including morphological analysis,\ndependency syntax and semantic role labeling in both FrameNet and PropBank\nstyles. Sentences are aligned between English and Hebrew, both sides include\nfull annotations and the explicit mapping from the English arguments to the\nHebrew ones. We train a neural SRL model on this Hebrew resource exploiting the\npre-trained multilingual BERT transformer model, and provide the first\navailable baseline model for Hebrew SRL as a reference point. The code we\nprovide is generic and can be adapted to other languages to bootstrap SRL\nresources.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 10:03:42 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Eyal", "Ben", ""], ["Elhadad", "Michael", ""]]}, {"id": "2005.08213", "submitter": "Won Ik Cho", "authors": "Won Ik Cho, Donghyun Kwak, Ji Won Yoon, Nam Soo Kim", "title": "Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation", "comments": "Interspeech 2020 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech is one of the most effective means of communication and is full of\ninformation that helps the transmission of utterer's thoughts. However, mainly\ndue to the cumbersome processing of acoustic features, phoneme or word\nposterior probability has frequently been discarded in understanding the\nnatural language. Thus, some recent spoken language understanding (SLU) modules\nhave utilized end-to-end structures that preserve the uncertainty information.\nThis further reduces the propagation of speech recognition error and guarantees\ncomputational efficiency. We claim that in this process, the speech\ncomprehension can benefit from the inference of massive pre-trained language\nmodels (LMs). We transfer the knowledge from a concrete Transformer-based text\nLM to an SLU module which can face a data shortage, based on recent cross-modal\ndistillation methodologies. We demonstrate the validity of our proposal upon\nthe performance on Fluent Speech Command, an English SLU benchmark. Thereby, we\nexperimentally verify our hypothesis that the knowledge could be shared from\nthe top layer of the LM to a fully speech-based module, in which the abstracted\nspeech is expected to meet the semantic representation.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 10:50:15 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 07:43:49 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cho", "Won Ik", ""], ["Kwak", "Donghyun", ""], ["Yoon", "Ji Won", ""], ["Kim", "Nam Soo", ""]]}, {"id": "2005.08223", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno, Luis-Gil Moreno-Jim\\'enez", "title": "LiSSS: A toy corpus of Spanish Literary Sentences for Emotions detection", "comments": "8 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a new small data-set in Computational Creativity (CC)\nfield, the Spanish Literary Sentences for emotions detection corpus (LISSS). We\naddress this corpus of literary sentences in order to evaluate or design\nalgorithms of emotions classification and detection. We have constitute this\ncorpus by manually classifying the sentences in a set of emotions: Love, Fear,\nHappiness, Anger and Sadness/Pain. We also present some baseline classification\nalgorithms applied on our corpus. The LISSS corpus will be available to the\ncommunity as a free resource to evaluate or create CC-like algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 11:14:30 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 10:30:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Moreno-Jim\u00e9nez", "Luis-Gil", ""]]}, {"id": "2005.08224", "submitter": "Deval Mehta", "authors": "Xin Pei, Deval Mehta", "title": "#Coronavirus or #Chinesevirus?!: Understanding the negative sentiment\n  reflected in Tweets with racist hashtags across the development of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situated in the global outbreak of COVID-19, our study enriches the\ndiscussion concerning the emergent racism and xenophobia on social media. With\nbig data extracted from Twitter, we focus on the analysis of negative sentiment\nreflected in tweets marked with racist hashtags, as racism and xenophobia are\nmore likely to be delivered via the negative sentiment. Especially, we propose\na stage-based approach to capture how the negative sentiment changes along with\nthe three development stages of COVID-19, under which it transformed from a\ndomestic epidemic into an international public health emergency and later, into\nthe global pandemic. At each stage, sentiment analysis enables us to recognize\nthe negative sentiment from tweets with racist hashtags, and keyword extraction\nallows for the discovery of themes in the expression of negative sentiment by\nthese tweets. Under this public health crisis of human beings, this stage-based\napproach enables us to provide policy suggestions for the enactment of\nstage-specific intervention strategies to combat racism and xenophobia on\nsocial media in a more effective way.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 11:15:50 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pei", "Xin", ""], ["Mehta", "Deval", ""]]}, {"id": "2005.08238", "submitter": "Zhibing Zhao", "authors": "Zhibing Zhao, Yingce Xia, Tao Qin, Lirong Xia, Tie-Yan Liu", "title": "Dual Learning: Theoretical Study and an Algorithmic Extension", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual learning has been successfully applied in many machine learning\napplications including machine translation, image-to-image transformation, etc.\nThe high-level idea of dual learning is very intuitive: if we map an $x$ from\none domain to another and then map it back, we should recover the original $x$.\nAlthough its effectiveness has been empirically verified, theoretical\nunderstanding of dual learning is still very limited. In this paper, we aim at\nunderstanding why and when dual learning works. Based on our theoretical\nanalysis, we further extend dual learning by introducing more related mappings\nand propose multi-step dual learning, in which we leverage feedback signals\nfrom additional domains to improve the qualities of the mappings. We prove that\nmulti-step dual learn-ing can boost the performance of standard dual learning\nunder mild conditions. Experiments on WMT 14 English$\\leftrightarrow$German and\nMultiUNEnglish$\\leftrightarrow$French translations verify our theoretical\nfindings on dual learning, and the results on the translations among English,\nFrench, and Spanish of MultiUN demonstrate the effectiveness of multi-step dual\nlearning.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 12:14:35 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhao", "Zhibing", ""], ["Xia", "Yingce", ""], ["Qin", "Tao", ""], ["Xia", "Lirong", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2005.08259", "submitter": "Mohammed Belkhatir", "authors": "Mohammed Maree, Israa Noor, Khaled Rabayah, Mohammed Belkhatir, and\n  Saadat M. Alhashmi", "title": "On the Combined Use of Extrinsic Semantic Resources for Medical\n  Information Search", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.2987568", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic concepts and relations encoded in domain-specific ontologies and\nother medical semantic resources play a crucial role in deciphering terms in\nmedical queries and documents. The exploitation of these resources for tackling\nthe semantic gap issue has been widely studied in the literature. However,\nthere are challenges that hinder their widespread use in real-world\napplications. Among these challenges is the insufficient knowledge individually\nencoded in existing medical ontologies, which is magnified when users express\ntheir information needs using long-winded natural language queries. In this\ncontext, many of the users query terms are either unrecognized by the used\nontologies, or cause retrieving false positives that degrade the quality of\ncurrent medical information search approaches. In this article, we explore the\ncombination of multiple extrinsic semantic resources in the development of a\nfull-fledged medical information search framework to: i) highlight and expand\nhead medical concepts in verbose medical queries (i.e. concepts among query\nterms that significantly contribute to the informativeness and intent of a\ngiven query), ii) build semantically enhanced inverted index documents, iii)\ncontribute to a heuristical weighting technique in the query document matching\nprocess. To demonstrate the effectiveness of the proposed approach, we\nconducted several experiments over the CLEF eHealth 2014 dataset. Findings\nindicate that the proposed method combining several extrinsic semantic\nresources proved to be more effective than related approaches in terms of\nprecision measure.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:18:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Maree", "Mohammed", ""], ["Noor", "Israa", ""], ["Rabayah", "Khaled", ""], ["Belkhatir", "Mohammed", ""], ["Alhashmi", "Saadat M.", ""]]}, {"id": "2005.08271", "submitter": "Vladimir Iashin", "authors": "Vladimir Iashin and Esa Rahtu", "title": "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal\n  Transformer", "comments": "Accepted by BMVC 2020. More experiments. Code:\n  https://github.com/v-iashin/bmt Project page: https://v-iashin.github.io/bmt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense video captioning aims to localize and describe important events in\nuntrimmed videos. Existing methods mainly tackle this task by exploiting only\nvisual features, while completely neglecting the audio track. Only a few prior\nworks have utilized both modalities, yet they show poor results or demonstrate\nthe importance on a dataset with a specific domain. In this paper, we introduce\nBi-modal Transformer which generalizes the Transformer architecture for a\nbi-modal input. We show the effectiveness of the proposed model with audio and\nvisual modalities on the dense video captioning task, yet the module is capable\nof digesting any two modalities in a sequence-to-sequence task. We also show\nthat the pre-trained bi-modal encoder as a part of the bi-modal transformer can\nbe used as a feature extractor for a simple proposal generation module. The\nperformance is demonstrated on a challenging ActivityNet Captions dataset where\nour model achieves outstanding performance. The code is available:\nv-iashin.github.io/bmt\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:00:05 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 09:17:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Iashin", "Vladimir", ""], ["Rahtu", "Esa", ""]]}, {"id": "2005.08294", "submitter": "Bhaskar Sen", "authors": "Bhaskar Sen, Nikhil Gopal, and Xinwei Xue", "title": "Support-BERT: Predicting Quality of Question-Answer Pairs in MSDN using\n  Deep Bidirectional Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality of questions and answers from community support websites (e.g.\nMicrosoft Developers Network, Stackoverflow, Github, etc.) is difficult to\ndefine and a prediction model of quality questions and answers is even more\nchallenging to implement. Previous works have addressed the question quality\nmodels and answer quality models separately using meta-features like number of\nup-votes, trustworthiness of the person posting the questions or answers,\ntitles of the post, and context naive natural language processing features.\nHowever, there is a lack of an integrated question-answer quality model for\ncommunity question answering websites in the literature. In this brief paper,\nwe tackle the quality Q&A modeling problems from the community support websites\nusing a recently developed deep learning model using bidirectional\ntransformers. We investigate the applicability of transfer learning on Q&A\nquality modeling using Bidirectional Encoder Representations from Transformers\n(BERT) trained on a separate tasks originally using Wikipedia. It is found that\na further pre-training of BERT model along with finetuning on the Q&As\nextracted from Microsoft Developer Network (MSDN) can boost the performance of\nautomated quality prediction to more than 80%. Furthermore, the implementations\nare carried out for deploying the finetuned model in real-time scenario using\nAzureML in Azure knowledge base system.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 16:50:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sen", "Bhaskar", ""], ["Gopal", "Nikhil", ""], ["Xue", "Xinwei", ""]]}, {"id": "2005.08314", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel", "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "comments": "To Appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed the burgeoning of pretrained language models\n(LMs) for text-based natural language (NL) understanding tasks. Such models are\ntypically trained on free-form NL text, hence may not be suitable for tasks\nlike semantic parsing over structured data, which require reasoning over both\nfree-form NL questions and structured tabular data (e.g., database tables). In\nthis paper we present TaBERT, a pretrained LM that jointly learns\nrepresentations for NL sentences and (semi-)structured tables. TaBERT is\ntrained on a large corpus of 26 million tables and their English contexts. In\nexperiments, neural semantic parsers using TaBERT as feature representation\nlayers achieve new best results on the challenging weakly-supervised semantic\nparsing benchmark WikiTableQuestions, while performing competitively on the\ntext-to-SQL dataset Spider. Implementation of the model will be available at\nhttp://fburl.com/TaBERT .\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 17:26:40 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yin", "Pengcheng", ""], ["Neubig", "Graham", ""], ["Yih", "Wen-tau", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2005.08319", "submitter": "Tao Chen", "authors": "Ansel MacLaughlin, Tao Chen, Burcu Karagol Ayan, Dan Roth", "title": "Context-Based Quotation Recommendation", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While composing a new document, anything from a news article to an email or\nessay, authors often utilize direct quotes from a variety of sources. Although\nan author may know what point they would like to make, selecting an appropriate\nquote for the specific context may be time-consuming and difficult. We\ntherefore propose a novel context-aware quote recommendation system which\nutilizes the content an author has already written to generate a ranked list of\nquotable paragraphs and spans of tokens from a given source document.\n  We approach quote recommendation as a variant of open-domain question\nanswering and adapt the state-of-the-art BERT-based methods from open-QA to our\ntask. We conduct experiments on a collection of speech transcripts and\nassociated news articles, evaluating models' paragraph ranking and span\nprediction performances. Our experiments confirm the strong performance of\nBERT-based methods on this task, which outperform bag-of-words and neural\nranking baselines by more than 30% relative across all ranking metrics.\nQualitative analyses show the difficulty of the paragraph and span\nrecommendation tasks and confirm the quotability of the best BERT model's\npredictions, even if they are not the true selected quotes from the original\nnews articles.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 17:49:53 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 05:31:13 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["MacLaughlin", "Ansel", ""], ["Chen", "Tao", ""], ["Ayan", "Burcu Karagol", ""], ["Roth", "Dan", ""]]}, {"id": "2005.08340", "submitter": "Carlos G\\'omez-Rodr\\'iguez", "authors": "Elmurod Kuriyozov, Yerai Doval, Carlos G\\'omez-Rodr\\'iguez", "title": "Cross-Lingual Word Embeddings for Turkic Languages", "comments": "Final version, published in the proceedings of LREC 2020", "journal-ref": "Proceedings of The 12th Language Resources and Evaluation\n  Conference, Marseille, France, 2020, pp. 4047-4055", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing interest in learning cross-lingual word\nembeddings to transfer knowledge obtained from a resource-rich language, such\nas English, to lower-resource languages for which annotated data is scarce,\nsuch as Turkish, Russian, and many others. In this paper, we present the first\nviability study of established techniques to align monolingual embedding spaces\nfor Turkish, Uzbek, Azeri, Kazakh and Kyrgyz, members of the Turkic family\nwhich is heavily affected by the low-resource constraint. Those techniques are\nknown to require little explicit supervision, mainly in the form of bilingual\ndictionaries, hence being easily adaptable to different domains, including\nlow-resource ones. We obtain new bilingual dictionaries and new word embeddings\nfor these languages and show the steps for obtaining cross-lingual word\nembeddings using state-of-the-art techniques. Then, we evaluate the results\nusing the bilingual dictionary induction task. Our experiments confirm that the\nobtained bilingual dictionaries outperform previously-available ones, and that\nword embeddings from a low-resource language can benefit from resource-rich\nclosely-related languages when they are aligned together. Furthermore,\nevaluation on an extrinsic task (Sentiment analysis on Uzbek) proves that\nmonolingual word embeddings can, although slightly, benefit from cross-lingual\nalignments.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 18:57:23 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kuriyozov", "Elmurod", ""], ["Doval", "Yerai", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2005.08347", "submitter": "Yiming Wang", "authors": "Yiming Wang, Hang Lv, Daniel Povey, Lei Xie, Sanjeev Khudanpur", "title": "Wake Word Detection with Alignment-Free Lattice-Free MMI", "comments": "Accepted at Interspeech 2020. 5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Always-on spoken language interfaces, e.g. personal digital assistants, rely\non a wake word to start processing spoken input. We present novel methods to\ntrain a hybrid DNN/HMM wake word detection system from partially labeled\ntraining data, and to use it in on-line applications: (i) we remove the\nprerequisite of frame-level alignments in the LF-MMI training algorithm,\npermitting the use of un-transcribed training examples that are annotated only\nfor the presence/absence of the wake word; (ii) we show that the classical\nkeyword/filler model must be supplemented with an explicit non-speech (silence)\nmodel for good performance; (iii) we present an FST-based decoder to perform\nonline detection. We evaluate our methods on two real data sets, showing\n50%--90% reduction in false rejection rates at pre-specified false alarm rates\nover the best previously published figures, and re-validate them on a third\n(large) data set.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 19:22:25 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 05:52:20 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 22:06:14 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Yiming", ""], ["Lv", "Hang", ""], ["Povey", "Daniel", ""], ["Xie", "Lei", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2005.08365", "submitter": "Xiang Gao", "authors": "Xiang Gao, Michel Galley, Bill Dolan", "title": "MixingBoard: a Knowledgeable Stylized Integrated Text Generation\n  Platform", "comments": "accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MixingBoard, a platform for quickly building demos with a focus on\nknowledge grounded stylized text generation. We unify existing text generation\nalgorithms in a shared codebase and further adapt earlier algorithms for\nconstrained generation. To borrow advantages from different models, we\nimplement strategies for cross-model integration, from the token probability\nlevel to the latent space level. An interface to external knowledge is provided\nvia a module that retrieves on-the-fly relevant knowledge from passages on the\nweb or any document collection. A user interface for local development, remote\nwebpage access, and a RESTful API are provided to make it simple for users to\nbuild their own demos.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 20:29:27 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 18:40:26 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Gao", "Xiang", ""], ["Galley", "Michel", ""], ["Dolan", "Bill", ""]]}, {"id": "2005.08384", "submitter": "Christian Anti\\'c", "authors": "Christian Anti\\'c", "title": "Fixed Point Semantics for Stream Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning over streams of input data is an essential part of human\nintelligence. During the last decade {\\em stream reasoning} has emerged as a\nresearch area within the AI-community with many potential applications. In\nfact, the increased availability of streaming data via services like Google and\nFacebook has raised the need for reasoning engines coping with data that\nchanges at high rate. Recently, the rule-based formalism {\\em LARS} for\nnon-monotonic stream reasoning under the answer set semantics has been\nintroduced. Syntactically, LARS programs are logic programs with negation\nincorporating operators for temporal reasoning, most notably {\\em window\noperators} for selecting relevant time points. Unfortunately, by preselecting\n{\\em fixed} intervals for the semantic evaluation of programs, the rigid\nsemantics of LARS programs is not flexible enough to {\\em constructively} cope\nwith rapidly changing data dependencies. Moreover, we show that defining the\nanswer set semantics of LARS in terms of FLP reducts leads to undesirable\ncircular justifications similar to other ASP extensions. This paper fixes all\nof the aforementioned shortcomings of LARS. More precisely, we contribute to\nthe foundations of stream reasoning by providing an operational fixed point\nsemantics for a fully flexible variant of LARS and we show that our semantics\nis sound and constructive in the sense that answer sets are derivable bottom-up\nand free of circular justifications.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 22:25:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Anti\u0107", "Christian", ""]]}, {"id": "2005.08392", "submitter": "Yu-An Chung", "authors": "Yu-An Chung, Hao Tang, James Glass", "title": "Vector-Quantized Autoregressive Predictive Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive Predictive Coding (APC), as a self-supervised objective, has\nenjoyed success in learning representations from large amounts of unlabeled\ndata, and the learned representations are rich for many downstream tasks.\nHowever, the connection between low self-supervised loss and strong performance\nin downstream tasks remains unclear. In this work, we propose Vector-Quantized\nAutoregressive Predictive Coding (VQ-APC), a novel model that produces\nquantized representations, allowing us to explicitly control the amount of\ninformation encoded in the representations. By studying a sequence of\nincreasingly limited models, we reveal the constituents of the learned\nrepresentations. In particular, we confirm the presence of information with\nprobing tasks, while showing the absence of information with mutual\ninformation, uncovering the model's preference in preserving speech information\nas its capacity becomes constrained. We find that there exists a point where\nphonetic and speaker information are amplified to maximize a self-supervised\nobjective. As a byproduct, the learned codes for a particular model capacity\ncorrespond well to English phones.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 23:06:09 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chung", "Yu-An", ""], ["Tang", "Hao", ""], ["Glass", "James", ""]]}, {"id": "2005.08400", "submitter": "Pedram Hosseini", "authors": "Pedram Hosseini and Poorya Hosseini and David A. Broniatowski", "title": "Content analysis of Persian/Farsi Tweets during COVID-19 pandemic in\n  Iran using NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iran, along with China, South Korea, and Italy was among the countries that\nwere hit hard in the first wave of the COVID-19 spread. Twitter is one of the\nwidely-used online platforms by Iranians inside and abroad for sharing their\nopinion, thoughts, and feelings about a wide range of issues. In this study,\nusing more than 530,000 original tweets in Persian/Farsi on COVID-19, we\nanalyzed the topics discussed among users, who are mainly Iranians, to gauge\nand track the response to the pandemic and how it evolved over time. We applied\na combination of manual annotation of a random sample of tweets and topic\nmodeling tools to classify the contents and frequency of each category of\ntopics. We identified the top 25 topics among which living experience under\nhome quarantine emerged as a major talking point. We additionally categorized\nbroader content of tweets that shows satire, followed by news, is the dominant\ntweet type among the Iranian users. While this framework and methodology can be\nused to track public response to ongoing developments related to COVID-19, a\ngeneralization of this framework can become a useful framework to gauge Iranian\npublic reaction to ongoing policy measures or events locally and\ninternationally.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 23:47:08 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hosseini", "Pedram", ""], ["Hosseini", "Poorya", ""], ["Broniatowski", "David A.", ""]]}, {"id": "2005.08417", "submitter": "Ashutosh Kumar", "authors": "Ashutosh Kumar, Kabir Ahuja, Raghuram Vadapalli, Partha Talukdar", "title": "Syntax-guided Controlled Generation of Paraphrases", "comments": "16 pages, 3 figures, Accepted to TACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sentence (e.g., \"I like mangoes\") and a constraint (e.g., sentiment\nflip), the goal of controlled text generation is to produce a sentence that\nadapts the input sentence to meet the requirements of the constraint (e.g., \"I\nhate mangoes\"). Going beyond such simple constraints, recent works have started\nexploring the incorporation of complex syntactic-guidance as constraints in the\ntask of controlled paraphrase generation. In these methods, syntactic-guidance\nis sourced from a separate exemplar sentence. However, these prior works have\nonly utilized limited syntactic information available in the parse tree of the\nexemplar sentence. We address this limitation in the paper and propose Syntax\nGuided Controlled Paraphraser (SGCP), an end-to-end framework for syntactic\nparaphrase generation. We find that SGCP can generate syntax conforming\nsentences while not compromising on relevance. We perform extensive automated\nand human evaluations over multiple real-world English language datasets to\ndemonstrate the efficacy of SGCP over state-of-the-art baselines. To drive\nfuture research, we have made SGCP's source code available\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 01:31:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kumar", "Ashutosh", ""], ["Ahuja", "Kabir", ""], ["Vadapalli", "Raghuram", ""], ["Talukdar", "Partha", ""]]}, {"id": "2005.08433", "submitter": "Shi-Yan Weng", "authors": "Tien-Hong Lo, Fu-An Chao, Shi-Yan Weng, Berlin Chen", "title": "The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR\n  Challenge", "comments": "Submitted to Interspeech 2020 Special Session: Shared Task on\n  Automatic Speech Recognition for Non-Native Children's Speech", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the NTNU ASR system participating in the Interspeech\n2020 Non-Native Children's Speech ASR Challenge supported by the SIG-CHILD\ngroup of ISCA. This ASR shared task is made much more challenging due to the\ncoexisting diversity of non-native and children speaking characteristics. In\nthe setting of closed-track evaluation, all participants were restricted to\ndevelop their systems merely based on the speech and text corpora provided by\nthe organizer. To work around this under-resourced issue, we built our ASR\nsystem on top of CNN-TDNNF-based acoustic models, meanwhile harnessing the\nsynergistic power of various data augmentation strategies, including both\nutterance- and word-level speed perturbation and spectrogram augmentation,\nalongside a simple yet effective data-cleansing approach. All variants of our\nASR system employed an RNN-based language model to rescore the first-pass\nrecognition hypotheses, which was trained solely on the text dataset released\nby the organizer. Our system with the best configuration came out in second\nplace, resulting in a word error rate (WER) of 17.59 %, while those of the\ntop-performing, second runner-up and official baseline systems are 15.67%,\n18.71%, 35.09%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 02:51:26 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 19:07:08 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Lo", "Tien-Hong", ""], ["Chao", "Fu-An", ""], ["Weng", "Shi-Yan", ""], ["Chen", "Berlin", ""]]}, {"id": "2005.08440", "submitter": "Shi-Yan Weng", "authors": "Tien-Hong Lo, Shi-Yan Weng, Hsiu-Jui Chang, and Berlin Chen", "title": "An Effective End-to-End Modeling Approach for Mispronunciation Detection", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, end-to-end (E2E) automatic speech recognition (ASR) systems have\ngarnered tremendous attention because of their great success and unified\nmodeling paradigms in comparison to conventional hybrid DNN-HMM ASR systems.\nDespite the widespread adoption of E2E modeling frameworks on ASR, there still\nis a dearth of work on investigating the E2E frameworks for use in\ncomputer-assisted pronunciation learning (CAPT), particularly for\nMispronunciation detection (MD). In response, we first present a novel use of\nhybrid CTCAttention approach to the MD task, taking advantage of the strengths\nof both CTC and the attention-based model meanwhile getting around the need for\nphone-level forced alignment. Second, we perform input augmentation with text\nprompt information to make the resulting E2E model more tailored for the MD\ntask. On the other hand, we adopt two MD decision methods so as to better\ncooperate with the proposed framework: 1) decision-making based on a\nrecognition confidence measure or 2) simply based on speech recognition\nresults. A series of Mandarin MD experiments demonstrate that our approach not\nonly simplifies the processing pipeline of existing hybrid DNN-HMM systems but\nalso brings about systematic and substantial performance improvements.\nFurthermore, input augmentation with text prompts seems to hold excellent\npromise for the E2E-based MD approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 03:37:21 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Lo", "Tien-Hong", ""], ["Weng", "Shi-Yan", ""], ["Chang", "Hsiu-Jui", ""], ["Chen", "Berlin", ""]]}, {"id": "2005.08469", "submitter": "Abhijit Mahabal", "authors": "Abhijit Mahabal, Jason Baldridge, Burcu Karagol Ayan, Vincent Perot,\n  Dan Roth", "title": "Text Classification with Few Examples using Controlled Generalization", "comments": null, "journal-ref": "Proceedings of NAACL-HLT 2019", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data for text classification is often limited in practice,\nespecially for applications with many output classes or involving many related\nclassification problems. This means classifiers must generalize from limited\nevidence, but the manner and extent of generalization is task dependent.\nCurrent practice primarily relies on pre-trained word embeddings to map words\nunseen in training to similar seen ones. Unfortunately, this squishes many\ncomponents of meaning into highly restricted capacity. Our alternative begins\nwith sparse pre-trained representations derived from unlabeled parsed corpora;\nbased on the available training data, we select features that offers the\nrelevant generalizations. This produces task-specific semantic vectors; here,\nwe show that a feed-forward network over these vectors is especially effective\nin low-data scenarios, compared to existing state-of-the-art methods. By\nfurther pairing this network with a convolutional neural network, we keep this\nedge in low data scenarios and remain competitive when using full training\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 06:04:58 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mahabal", "Abhijit", ""], ["Baldridge", "Jason", ""], ["Ayan", "Burcu Karagol", ""], ["Perot", "Vincent", ""], ["Roth", "Dan", ""]]}, {"id": "2005.08497", "submitter": "Bin Wang", "authors": "Bin Wang, Yan Yin, Hui Lin", "title": "Attention-based Transducer for Online Speech Recognition", "comments": "submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies reveal the potential of recurrent neural network transducer\n(RNN-T) for end-to-end (E2E) speech recognition. Among some most popular E2E\nsystems including RNN-T, Attention Encoder-Decoder (AED), and Connectionist\nTemporal Classification (CTC), RNN-T has some clear advantages given that it\nsupports streaming recognition and does not have frame-independency assumption.\nAlthough significant progresses have been made for RNN-T research, it is still\nfacing performance challenges in terms of training speed and accuracy. We\npropose attention-based transducer with modification over RNN-T in two aspects.\nFirst, we introduce chunk-wise attention in the joint network. Second,\nself-attention is introduced in the encoder. Our proposed model outperforms\nRNN-T for both training speed and accuracy. For training, we achieves over 1.7x\nspeedup. With 500 hours LAIX non-native English training data, attention-based\ntransducer yields ~10.6% WER reduction over baseline RNN-T. Trained with full\nset of over 10K hours data, our final system achieves ~5.5% WER reduction over\nthat trained with the best Kaldi TDNN-f recipe. After 8-bit weight quantization\nwithout WER degradation, RTF and latency drop to 0.34~0.36 and 268~409\nmilliseconds respectively on a single CPU core of a production server.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 07:26:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Bin", ""], ["Yin", "Yan", ""], ["Lin", "Hui", ""]]}, {"id": "2005.08516", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva and\n  Chitta Baral", "title": "Towards Question Format Independent Numerical Reasoning: A Set of\n  Prerequisite Tasks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical reasoning is often important to accurately understand the world.\nRecently, several format-specific datasets have been proposed, such as\nnumerical reasoning in the settings of Natural Language Inference (NLI),\nReading Comprehension (RC), and Question Answering (QA). Several\nformat-specific models and architectures in response to those datasets have\nalso been proposed. However, there exists a strong need for a benchmark which\ncan evaluate the abilities of models, in performing question format independent\nnumerical reasoning, as (i) the numerical reasoning capabilities we want to\nteach are not controlled by question formats, (ii) for numerical reasoning\ntechnology to have the best possible application, it must be able to process\nlanguage and reason in a way that is not exclusive to a single format, task,\ndataset or domain. In pursuit of this goal, we introduce NUMBERGAME, a\nmultifaceted benchmark to evaluate model performance across numerical reasoning\ntasks of eight diverse formats. We add four existing question types in our\ncompilation. Two of the new types we add are about questions that require\nexternal numerical knowledge, commonsense knowledge and domain knowledge. For\nbuilding a more practical numerical reasoning system, NUMBERGAME demands four\ncapabilities beyond numerical reasoning: (i) detecting question format directly\nfrom data (ii) finding intermediate common format to which every format can be\nconverted (iii) incorporating commonsense knowledge (iv) handling data\nimbalance across formats. We build several baselines, including a new model\nbased on knowledge hunting using a cheatsheet. However, all baselines perform\npoorly in contrast to the human baselines, indicating the hardness of our\nbenchmark. Our work takes forward the recent progress in generic system\ndevelopment, demonstrating the scope of these under-explored tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:14:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mishra", "Swaroop", ""], ["Mitra", "Arindam", ""], ["Varshney", "Neeraj", ""], ["Sachdeva", "Bhavdeep", ""], ["Baral", "Chitta", ""]]}, {"id": "2005.08519", "submitter": "Hanna Abi Akl", "authors": "Hanna Abi Akl, Dominique Mariko, Estelle Labidurie", "title": "Yseop at SemEval-2020 Task 5: Cascaded BERT Language Model for\n  Counterfactual Statement Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore strategies to detect and evaluate counterfactual\nsentences. We describe our system for SemEval-2020 Task 5: Modeling Causal\nReasoning in Language: Detecting Counterfactuals. We use a BERT base model for\nthe classification task and build a hybrid BERT Multi-Layer Perceptron system\nto handle the sequence identification task. Our experiments show that while\nintroducing syntactic and semantic features does little in improving the system\nin the classification task, using these types of features as cascaded linear\ninputs to fine-tune the sequence-delimiting ability of the model ensures it\noutperforms other similar-purpose complex systems like BiLSTM-CRF in the second\ntask. Our system achieves an F1 score of 85.00% in Task 1 and 83.90% in Task 2.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:19:18 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 17:35:42 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Akl", "Hanna Abi", ""], ["Mariko", "Dominique", ""], ["Labidurie", "Estelle", ""]]}, {"id": "2005.08520", "submitter": "Adrian Lancucki", "authors": "Adrian {\\L}a\\'ncucki, Jan Chorowski, Guillaume Sanchez, Ricard Marxer,\n  Nanxin Chen, Hans J.G.A. Dolfing, Sameer Khurana, Tanel Alum\\\"ae, Antoine\n  Laurent", "title": "Robust Training of Vector Quantized Bottleneck Models", "comments": "Published at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate methods for reliable and efficient training of\ndiscrete representation using Vector-Quantized Variational Auto-Encoder models\n(VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial\nrepresentations of speech, applicable to unsupervised voice conversion and\nreaching state-of-the-art performance on unit discovery tasks. For unsupervised\nrepresentation learning, they became viable alternatives to continuous latent\nvariable models such as the Variational Auto-Encoder (VAE). However, training\ndeep discrete variable models is challenging, due to the inherent\nnon-differentiability of the discretization operation. In this paper we focus\non VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par\nwith its continuous counterparts. It quantizes encoder outputs with on-line\n$k$-means clustering. We show that the codebook learning can suffer from poor\ninitialization and non-stationarity of clustered encoder outputs. We\ndemonstrate that these can be successfully overcome by increasing the learning\nrate for the codebook and periodic date-dependent codeword re-initialization.\nAs a result, we achieve more robust training across different tasks, and\nsignificantly increase the usage of latent codewords even for large codebooks.\nThis has practical benefit, for instance, in unsupervised representation\nlearning, where large codebooks may lead to disentanglement of latent\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:23:41 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["\u0141a\u0144cucki", "Adrian", ""], ["Chorowski", "Jan", ""], ["Sanchez", "Guillaume", ""], ["Marxer", "Ricard", ""], ["Chen", "Nanxin", ""], ["Dolfing", "Hans J. G. A.", ""], ["Khurana", "Sameer", ""], ["Alum\u00e4e", "Tanel", ""], ["Laurent", "Antoine", ""]]}, {"id": "2005.08571", "submitter": "Jianwei Yu", "authors": "Jianwei Yu, Bo Wu, Rongzhi Gu, Shi-Xiong Zhang, Lianwu Chen, Yong Xu.\n  Meng Yu, Dan Su, Dong Yu, Xunying Liu, Helen Meng", "title": "Audio-visual Multi-channel Recognition of Overlapped Speech", "comments": "submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) of overlapped speech remains a highly\nchallenging task to date. To this end, multi-channel microphone array data are\nwidely used in state-of-the-art ASR systems. Motivated by the invariance of\nvisual modality to acoustic signal corruption, this paper presents an\naudio-visual multi-channel overlapped speech recognition system featuring\ntightly integrated separation front-end and recognition back-end. A series of\naudio-visual multi-channel speech separation front-end components based on\n\\textit{TF masking}, \\textit{filter\\&sum} and \\textit{mask-based MVDR}\nbeamforming approaches were developed. To reduce the error cost mismatch\nbetween the separation and recognition components, they were jointly fine-tuned\nusing the connectionist temporal classification (CTC) loss function, or a\nmulti-task criterion interpolation with scale-invariant signal to noise ratio\n(Si-SNR) error cost. Experiments suggest that the proposed multi-channel AVSR\nsystem outperforms the baseline audio-only ASR system by up to 6.81\\% (26.83\\%\nrelative) and 22.22\\% (56.87\\% relative) absolute word error rate (WER)\nreduction on overlapped speech constructed using either simulation or replaying\nof the lipreading sentence 2 (LRS2) dataset respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 10:31:19 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 12:30:54 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Yu", "Jianwei", ""], ["Wu", "Bo", ""], ["Gu", "Rongzhi", ""], ["Zhang", "Shi-Xiong", ""], ["Chen", "Lianwu", ""], ["Yu", "Yong Xu. Meng", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""], ["Liu", "Xunying", ""], ["Meng", "Helen", ""]]}, {"id": "2005.08575", "submitter": "Pohan Chi", "authors": "Po-Han Chi, Pei-Hung Chung, Tsung-Han Wu, Chun-Cheng Hsieh, Yen-Hao\n  Chen, Shang-Wen Li, Hung-yi Lee", "title": "Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio\n  Representation", "comments": "Accepted by IEEE Spoken Language Technology Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For self-supervised speech processing, it is crucial to use pretrained models\nas speech representation extractors. In recent works, increasing the size of\nthe model has been utilized in acoustic model training in order to achieve\nbetter performance. In this paper, we propose Audio ALBERT, a lite version of\nthe self-supervised speech representation model. We use the representations\nwith two downstream tasks, speaker identification, and phoneme classification.\nWe show that Audio ALBERT is capable of achieving competitive performance with\nthose huge models in the downstream tasks while utilizing 91\\% fewer\nparameters. Moreover, we use some simple probing models to measure how much the\ninformation of the speaker and phoneme is encoded in latent representations. In\nprobing experiments, we find that the latent representations encode richer\ninformation of both phoneme and speaker than that of the last layer.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 10:42:44 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 05:35:28 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 00:56:58 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 17:51:46 GMT"}, {"version": "v5", "created": "Mon, 3 May 2021 09:33:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chi", "Po-Han", ""], ["Chung", "Pei-Hung", ""], ["Wu", "Tsung-Han", ""], ["Hsieh", "Chun-Cheng", ""], ["Chen", "Yen-Hao", ""], ["Li", "Shang-Wen", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2005.08595", "submitter": "Maha Elbayad", "authors": "Maha Elbayad, Laurent Besacier, Jakob Verbeek", "title": "Efficient Wait-k Models for Simultaneous Machine Translation", "comments": "Accepted at INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous machine translation consists in starting output generation\nbefore the entire input sequence is available. Wait-k decoders offer a simple\nbut efficient approach for this problem. They first read k source tokens, after\nwhich they alternate between producing a target token and reading another\nsource token. We investigate the behavior of wait-k decoding in low resource\nsettings for spoken corpora using IWSLT datasets. We improve training of these\nmodels using unidirectional encoders, and training across multiple values of k.\nExperiments with Transformer and 2D-convolutional architectures show that our\nwait-k models generalize well across a wide range of latency levels. We also\nshow that the 2D-convolution architecture is competitive with Transformers for\nsimultaneous translation of spoken language.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:14:23 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 01:10:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Elbayad", "Maha", ""], ["Besacier", "Laurent", ""], ["Verbeek", "Jakob", ""]]}, {"id": "2005.08601", "submitter": "Brij Mohan Lal Srivastava", "authors": "Brij Mohan Lal Srivastava, Natalia Tomashenko, Xin Wang, Emmanuel\n  Vincent, Junichi Yamagishi, Mohamed Maouche, Aur\\'elien Bellet, Marc Tommasi", "title": "Design Choices for X-vector Based Speaker Anonymization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed x-vector based anonymization scheme converts any input\nvoice into that of a random pseudo-speaker. In this paper, we present a\nflexible pseudo-speaker selection technique as a baseline for the first\nVoicePrivacy Challenge. We explore several design choices for the distance\nmetric between speakers, the region of x-vector space where the pseudo-speaker\nis picked, and gender selection. To assess the strength of anonymization\nachieved, we consider attackers using an x-vector based speaker verification\nsystem who may use original or anonymized speech for enrollment, depending on\ntheir knowledge of the anonymization scheme. The Equal Error Rate (EER)\nachieved by the attackers and the decoding Word Error Rate (WER) over\nanonymized data are reported as the measures of privacy and utility.\nExperiments are performed using datasets derived from LibriSpeech to find the\noptimal combination of design choices in terms of privacy and utility.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:32:14 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Srivastava", "Brij Mohan Lal", ""], ["Tomashenko", "Natalia", ""], ["Wang", "Xin", ""], ["Vincent", "Emmanuel", ""], ["Yamagishi", "Junichi", ""], ["Maouche", "Mohamed", ""], ["Bellet", "Aur\u00e9lien", ""], ["Tommasi", "Marc", ""]]}, {"id": "2005.08612", "submitter": "Zolt\\'an Kmetty", "authors": "Zolt\\'an Kmetty, Julia Koltai, Tam\\'as Rudas", "title": "The presence of occupational structure in online texts based on word\n  embedding NLP models", "comments": "34 pages, 2 figures, 4 tables. Paper presented at IC2S2 2019 and RC28\n  summer meeting 2019 (Columbia University)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on social stratification is closely linked to analysing the prestige\nassociated with different occupations. This research focuses on the positions\nof occupations in the semantic space represented by large amounts of textual\ndata. The results are compared to standard results in social stratification to\nsee whether the classical results are reproduced and if additional insights can\nbe gained into the social positions of occupations. The paper gives an\naffirmative answer to both questions. The results show fundamental similarity\nof the occupational structure obtained from text analysis to the structure\ndescribed by prestige and social distance scales. While our research reinforces\nmany theories and empirical findings of the traditional body of literature on\nsocial stratification and, in particular, occupational hierarchy, it pointed to\nthe importance of a factor not discussed in the main line of stratification\nliterature so far: the power and organizational aspect.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:49:35 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 10:51:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kmetty", "Zolt\u00e1n", ""], ["Koltai", "Julia", ""], ["Rudas", "Tam\u00e1s", ""]]}, {"id": "2005.08650", "submitter": "Marek Rychlik", "authors": "Marek Rychlik, and Dwight Nwaigwe and Yan Han and Dylan Murphy", "title": "Development of a New Image-to-text Conversion System for Pashto, Farsi\n  and Traditional Chinese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report upon the results of a research and prototype building project\n\\emph{Worldly~OCR} dedicated to developing new, more accurate image-to-text\nconversion software for several languages and writing systems. These include\nthe cursive scripts Farsi and Pashto, and Latin cursive scripts. We also\ndescribe approaches geared towards Traditional Chinese, which is non-cursive,\nbut features an extremely large character set of 65,000 characters. Our\nmethodology is based on Machine Learning, especially Deep Learning, and Data\nScience, and is directed towards vast quantities of original documents,\nexceeding a billion pages. The target audience of this paper is a general\naudience with interest in Digital Humanities or in retrieval of accurate\nfull-text and metadata from digital images.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:58:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Rychlik", "Marek", ""], ["Nwaigwe", "Dwight", ""], ["Han", "Yan", ""], ["Murphy", "Dylan", ""]]}, {"id": "2005.08658", "submitter": "Avishek Anand", "authors": "Avishek Anand, Lawrence Cavedon, Matthias Hagen, Hideo Joho, Mark\n  Sanderson, and Benno Stein", "title": "Conversational Search -- A Report from Dagstuhl Seminar 19461", "comments": "contains arXiv:2001.06910, arXiv:2001.02912", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dagstuhl Seminar 19461 \"Conversational Search\" was held on 10-15 November\n2019. 44~researchers in Information Retrieval and Web Search, Natural Language\nProcessing, Human Computer Interaction, and Dialogue Systems were invited to\nshare the latest development in the area of Conversational Search and discuss\nits research agenda and future directions. A 5-day program of the seminar\nconsisted of six introductory and background sessions, three visionary talk\nsessions, one industry talk session, and seven working groups and reporting\nsessions. The seminar also had three social events during the program. This\nreport provides the executive summary, overview of invited talks, and findings\nfrom the seven working groups which cover the definition, evaluation,\nmodelling, explanation, scenarios, applications, and prototype of\nConversational Search. The ideas and findings presented in this report should\nserve as one of the main sources for diverse research programs on\nConversational Search.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:48:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Anand", "Avishek", ""], ["Cavedon", "Lawrence", ""], ["Hagen", "Matthias", ""], ["Joho", "Hideo", ""], ["Sanderson", "Mark", ""], ["Stein", "Benno", ""]]}, {"id": "2005.08742", "submitter": "Van Tung Pham", "authors": "Tingzhi Mao, Yerbolat Khassanov, Van Tung Pham, Haihua Xu, Hao Huang,\n  Eng Siong Chng", "title": "Approaches to Improving Recognition of Underrepresented Named Entities\n  in Hybrid ASR Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a series of complementary approaches to improve the\nrecognition of underrepresented named entities (NE) in hybrid ASR systems\nwithout compromising overall word error rate performance. The underrepresented\nwords correspond to rare or out-of-vocabulary (OOV) words in the training data,\nand thereby can't be modeled reliably. We begin with graphemic lexicon which\nallows to drop the necessity of phonetic models in hybrid ASR. We study it\nunder different settings and demonstrate its effectiveness in dealing with\nunderrepresented NEs. Next, we study the impact of neural language model (LM)\nwith letter-based features derived to handle infrequent words. After that, we\nattempt to enrich representations of underrepresented NEs in pretrained neural\nLM by borrowing the embedding representations of rich-represented words. This\nlet us gain significant performance improvement on underrepresented NE\nrecognition. Finally, we boost the likelihood scores of utterances containing\nNEs in the word lattices rescored by neural LMs and gain further performance\nimprovement. The combination of the aforementioned approaches improves NE\nrecognition by up to 42% relatively.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:11:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mao", "Tingzhi", ""], ["Khassanov", "Yerbolat", ""], ["Pham", "Van Tung", ""], ["Xu", "Haihua", ""], ["Huang", "Hao", ""], ["Chng", "Eng Siong", ""]]}, {"id": "2005.08746", "submitter": "Javier Velasco-Mata", "authors": "Mhd Wesam Al-Nabki, Francisco Ja\\~nez-Martino, Roberto A.\n  Vasco-Carofilis, Eduardo Fidalgo, Javier Velasco-Mata", "title": "Improving Named Entity Recognition in Tor Darknet with Local Distance\n  Neighbor Feature", "comments": "2 pages, 1 figure, to be published in conference JNIC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Name entity recognition in noisy user-generated texts is a difficult task\nusually enhanced by incorporating an external resource of information, such as\ngazetteers. However, gazetteers are task-specific, and they are expensive to\nbuild and maintain. This paper adopts and improves the approach of Aguilar et\nal. by presenting a novel feature, called Local Distance Neighbor, which\nsubstitutes gazetteers. We tested the new approach on the W-NUT-2017 dataset,\nobtaining state-of-the-art results for the Group, Person and Product categories\nof Named Entities. Next, we added 851 manually labeled samples to the\nW-NUT-2017 dataset to account for named entities in the Tor Darknet related to\nweapons and drug selling. Finally, our proposal achieved an entity and surface\nF1 scores of 52.96% and 50.57% on this extended dataset, demonstrating its\nusefulness for Law Enforcement Agencies to detect named entities in the Tor\nhidden services.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:21:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Al-Nabki", "Mhd Wesam", ""], ["Ja\u00f1ez-Martino", "Francisco", ""], ["Vasco-Carofilis", "Roberto A.", ""], ["Fidalgo", "Eduardo", ""], ["Velasco-Mata", "Javier", ""]]}, {"id": "2005.08773", "submitter": "Francisco J\\'a\\~nez-Martino", "authors": "Francisco J\\'a\\~nez-Martino, Eduardo Fidalgo, Santiago\n  Gonz\\'alez-Mart\\'inez, Javier Velasco-Mata", "title": "Classification of Spam Emails through Hierarchical Clustering and\n  Supervised Learning", "comments": "4 pages, 2 figures, to be published in conference JNIC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spammers take advantage of email popularity to send indiscriminately\nunsolicited emails. Although researchers and organizations continuously develop\nanti-spam filters based on binary classification, spammers bypass them through\nnew strategies, like word obfuscation or image-based spam. For the first time\nin literature, we propose to classify spam email in categories to improve the\nhandle of already detected spam emails, instead of just using a binary model.\nFirst, we applied a hierarchical clustering algorithm to create SPEMC-$11$K\n(SPam EMail Classification), the first multi-class dataset, which contains\nthree types of spam emails: Health and Technology, Personal Scams, and Sexual\nContent. Then, we used SPEMC-$11$K to evaluate the combination of TF-IDF and\nBOW encodings with Na\\\"ive Bayes, Decision Trees and SVM classifiers. Finally,\nwe recommend for the task of multi-class spam classification the use of (i)\nTF-IDF combined with SVM for the best micro F1 score performance, $95.39\\%$,\nand (ii) TD-IDF along with NB for the fastest spam classification, analyzing an\nemail in $2.13$ms.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:41:22 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 15:36:25 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["J\u00e1\u00f1ez-Martino", "Francisco", ""], ["Fidalgo", "Eduardo", ""], ["Gonz\u00e1lez-Mart\u00ednez", "Santiago", ""], ["Velasco-Mata", "Javier", ""]]}, {"id": "2005.08793", "submitter": "Sergey Zinin", "authors": "Sergey Zinin, Yang Xu", "title": "Corpus of Chinese Dynastic Histories: Gender Analysis over Two Millennia", "comments": "12th Conference on Language Resources and Evaluation (LREC 2020), 9\n  pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese dynastic histories form a large continuous linguistic space of\napproximately 2000 years, from the 3rd century BCE to the 18th century CE. The\nhistories are documented in Classical (Literary) Chinese in a corpus of over 20\nmillion characters, suitable for the computational analysis of historical\nlexicon and semantic change. However, there is no freely available open-source\ncorpus of these histories, making Classical Chinese low-resource. This project\nintroduces a new open-source corpus of twenty-four dynastic histories covered\nby Creative Commons license. An original list of Classical Chinese\ngender-specific terms was developed as a case study for analyzing the\nhistorical linguistic use of male and female terms. The study demonstrates\nconsiderable stability in the usage of these terms, with dominance of male\nterms. Exploration of word meanings uses keyword analysis of focus corpora\ncreated for genderspecific terms. This method yields meaningful semantic\nrepresentations that can be used for future studies of diachronic semantics.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:14:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zinin", "Sergey", ""], ["Xu", "Yang", ""]]}, {"id": "2005.08805", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Franck Dernoncourt, Walter Chang, Nazli Goharian,\n  Ophir Frieder", "title": "Interaction Matching for Long-Tail Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an elegant and effective approach for addressing limitations in\nexisting multi-label classification models by incorporating interaction\nmatching, a concept shown to be useful for ad-hoc search result ranking. By\nperforming soft n-gram interaction matching, we match labels with natural\nlanguage descriptions (which are common to have in most multi-labeling tasks).\nOur approach can be used to enhance existing multi-label classification\napproaches, which are biased toward frequently-occurring labels. We evaluate\nour approach on two challenging tasks: automatic medical coding of clinical\nnotes and automatic labeling of entities from software tutorial text. Our\nresults show that our method can yield up to an 11% relative improvement in\nmacro performance, with most of the gains stemming labels that appear\ninfrequently in the training set (i.e., the long tail of labels).\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:27:55 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["MacAvaney", "Sean", ""], ["Dernoncourt", "Franck", ""], ["Chang", "Walter", ""], ["Goharian", "Nazli", ""], ["Frieder", "Ophir", ""]]}, {"id": "2005.08817", "submitter": "Jia Xue", "authors": "Jia Xue, Junxiang Chen, Chen Chen, Chengda Zheng, Sijia Li, Tingshao\n  Zhu", "title": "Public discourse and sentiment during the COVID-19 pandemic: using\n  Latent Dirichlet Allocation for topic modeling on Twitter", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0239441", "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study aims to understand Twitter users' discourse and psychological\nreactions to COVID-19. We use machine learning techniques to analyze about 1.9\nmillion Tweets (written in English) related to coronavirus collected from\nJanuary 23 to March 7, 2020. A total of salient 11 topics are identified and\nthen categorized into ten themes, including \"updates about confirmed cases,\"\n\"COVID-19 related death,\" \"cases outside China (worldwide),\" \"COVID-19 outbreak\nin South Korea,\" \"early signs of the outbreak in New York,\" \"Diamond Princess\ncruise,\" \"economic impact,\" \"Preventive measures,\" \"authorities,\" and \"supply\nchain.\" Results do not reveal treatments and symptoms related messages as\nprevalent topics on Twitter. Sentiment analysis shows that fear for the unknown\nnature of the coronavirus is dominant in all topics. Implications and\nlimitations of the study are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:50:38 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 17:22:01 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 13:52:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Xue", "Jia", ""], ["Chen", "Junxiang", ""], ["Chen", "Chen", ""], ["Zheng", "Chengda", ""], ["Li", "Sijia", ""], ["Zhu", "Tingshao", ""]]}, {"id": "2005.08820", "submitter": "Azzam Mourad Dr.", "authors": "Azzam Mourad, Ali Srour, Haidar Harmanani, Cathia Jenainatiy, Mohamad\n  Arafeh", "title": "Critical Impact of Social Networks Infodemic on Defeating Coronavirus\n  COVID-19 Pandemic: Twitter-Based Study and Research Directions", "comments": "11 pages, 10 figures, Journal Article", "journal-ref": "https://ieeexplore.ieee.org/document/9223699 (2020)", "doi": "10.1109/TNSM.2020.3031034", "report-no": "In the IEEE Transactions on Network and Service Management (Early\n  Access, 2020)", "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News creation and consumption has been changing since the advent of social\nmedia. An estimated 2.95 billion people in 2019 used social media worldwide.\nThe widespread of the Coronavirus COVID-19 resulted with a tsunami of social\nmedia. Most platforms were used to transmit relevant news, guidelines and\nprecautions to people. According to WHO, uncontrolled conspiracy theories and\npropaganda are spreading faster than the COVID-19 pandemic itself, creating an\ninfodemic and thus causing psychological panic, misleading medical advises, and\neconomic disruption. Accordingly, discussions have been initiated with the\nobjective of moderating all COVID-19 communications, except those initiated\nfrom trusted sources such as the WHO and authorized governmental entities. This\npaper presents a large-scale study based on data mined from Twitter. Extensive\nanalysis has been performed on approximately one million COVID-19 related\ntweets collected over a period of two months. Furthermore, the profiles of\n288,000 users were analyzed including unique users profiles, meta-data and\ntweets context. The study noted various interesting conclusions including the\ncritical impact of the (1) exploitation of the COVID-19 crisis to redirect\nreaders to irrelevant topics and (2) widespread of unauthentic medical\nprecautions and information. Further data analysis revealed the importance of\nusing social networks in a global pandemic crisis by relying on credible users\nwith variety of occupations, content developers and influencers in specific\nfields. In this context, several insights and findings have been provided while\nelaborating computing and non-computing implications and research directions\nfor potential solutions and social networks management strategies during crisis\nperiods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:53:13 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mourad", "Azzam", ""], ["Srour", "Ali", ""], ["Harmanani", "Haidar", ""], ["Jenainatiy", "Cathia", ""], ["Arafeh", "Mohamad", ""]]}, {"id": "2005.08826", "submitter": "Kate McCurdy", "authors": "Kate McCurdy, Sharon Goldwater, Adam Lopez", "title": "Inflecting when there's no majority: Limitations of encoder-decoder\n  neural networks as cognitive models for German plurals", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.159", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can artificial neural networks learn to represent inflectional morphology and\ngeneralize to new words as human speakers do? Kirov and Cotterell (2018) argue\nthat the answer is yes: modern Encoder-Decoder (ED) architectures learn\nhuman-like behavior when inflecting English verbs, such as extending the\nregular past tense form -(e)d to novel words. However, their work does not\naddress the criticism raised by Marcus et al. (1995): that neural models may\nlearn to extend not the regular, but the most frequent class -- and thus fail\non tasks like German number inflection, where infrequent suffixes like -s can\nstill be productively generalized.\n  To investigate this question, we first collect a new dataset from German\nspeakers (production and ratings of plural forms for novel nouns) that is\ndesigned to avoid sources of information unavailable to the ED model. The\nspeaker data show high variability, and two suffixes evince 'regular' behavior,\nappearing more often with phonologically atypical inputs. Encoder-decoder\nmodels do generalize the most frequently produced plural class, but do not show\nhuman-like variability or 'regular' extension of these other plural markers. We\nconclude that modern neural models may still struggle with minority-class\ngeneralization.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:58:28 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["McCurdy", "Kate", ""], ["Goldwater", "Sharon", ""], ["Lopez", "Adam", ""]]}, {"id": "2005.08864", "submitter": "Kate McCurdy", "authors": "Katherine McCurdy and Oguz Serbetci", "title": "Grammatical gender associations outweigh topical gender bias in\n  crosslinguistic word embeddings", "comments": "Extended abstract presented at the WiNLP workshop, ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated that vector space models of semantics can\nreflect undesirable biases in human culture. Our investigation of\ncrosslinguistic word embeddings reveals that topical gender bias interacts\nwith, and is surpassed in magnitude by, the effect of grammatical gender\nassociations, and both may be attenuated by corpus lemmatization. This finding\nhas implications for downstream applications such as machine translation.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:39:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["McCurdy", "Katherine", ""], ["Serbetci", "Oguz", ""]]}, {"id": "2005.08866", "submitter": "Ivan Vuli\\'c", "authors": "Sam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli\\'c, Matthew\n  Henderson", "title": "Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained\n  Conversational Representations", "comments": "ACL 2020 (updated version with errata)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Span-ConveRT, a light-weight model for dialog slot-filling which\nframes the task as a turn-based span extraction task. This formulation allows\nfor a simple integration of conversational knowledge coded in large pretrained\nconversational models such as ConveRT (Henderson et al., 2019). We show that\nleveraging such knowledge in Span-ConveRT is especially useful for few-shot\nlearning scenarios: we report consistent gains over 1) a span extractor that\ntrains representations from scratch in the target domain, and 2) a BERT-based\nspan extractor. In order to inspire more work on span extraction for the\nslot-filling task, we also release RESTAURANTS-8K, a new challenging data set\nof 8,198 utterances, compiled from actual conversations in the restaurant\nbooking domain.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:40:30 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 16:29:17 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Coope", "Sam", ""], ["Farghly", "Tyler", ""], ["Gerz", "Daniela", ""], ["Vuli\u0107", "Ivan", ""], ["Henderson", "Matthew", ""]]}, {"id": "2005.08932", "submitter": "Johnathan Avery", "authors": "Johnathan E. Avery, Robert L. Goldstone, Michael N. Jones", "title": "Reconstructing Maps from Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous research has demonstrated that Distributional Semantic Models (DSMs)\nare capable of reconstructing maps from news corpora (Louwerse & Zwaan, 2009)\nand novels (Louwerse & Benesh, 2012). The capacity for reproducing maps is\nsurprising since DSMs notoriously lack perceptual grounding (De Vega et al.,\n2012). In this paper we investigate the statistical sources required in\nlanguage to infer maps, and resulting constraints placed on mechanisms of\nsemantic representation. Study 1 brings word co-occurrence under experimental\ncontrol to demonstrate that direct co-occurrence in language is necessary for\ntraditional DSMs to successfully reproduce maps. Study 2 presents an\ninstance-based DSM that is capable of reconstructing maps independent of the\nfrequency of co-occurrence of city names.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:57:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Avery", "Johnathan E.", ""], ["Goldstone", "Robert L.", ""], ["Jones", "Michael N.", ""]]}, {"id": "2005.08946", "submitter": "Fatemah Husain", "authors": "Fatemah Husain", "title": "Arabic Offensive Language Detection Using Machine Learning and Ensemble\n  Machine Learning Approaches", "comments": "5 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2005.07297", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aims at investigating the effect of applying single learner\nmachine learning approach and ensemble machine learning approach for offensive\nlanguage detection on Arabic language. Classifying Arabic social media text is\na very challenging task due to the ambiguity and informality of the written\nformat of the text. Arabic language has multiple dialects with diverse\nvocabularies and structures, which increase the complexity of obtaining high\nclassification performance. Our study shows significant impact for applying\nensemble machine learning approach over the single learner machine learning\napproach. Among the trained ensemble machine learning classifiers, bagging\nperforms the best in offensive language detection with F1 score of 88%, which\nexceeds the score obtained by the best single learner classifier by 6%. Our\nfindings highlight the great opportunities of investing more efforts in\npromoting the ensemble machine learning approach solutions for offensive\nlanguage detection models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 06:40:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Husain", "Fatemah", ""]]}, {"id": "2005.09067", "submitter": "Max Savery", "authors": "Max Savery, Asma Ben Abacha, Soumya Gayen, Dina Demner-Fushman", "title": "Question-Driven Summarization of Answers to Consumer Health Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic summarization of natural language is a widely studied area in\ncomputer science, one that is broadly applicable to anyone who routinely needs\nto understand large quantities of information. For example, in the medical\ndomain, recent developments in deep learning approaches to automatic\nsummarization have the potential to make health information more easily\naccessible to patients and consumers. However, to evaluate the quality of\nautomatically generated summaries of health information, gold-standard, human\ngenerated summaries are required. Using answers provided by the National\nLibrary of Medicine's consumer health question answering system, we present the\nMEDIQA Answer Summarization dataset, the first summarization collection\ncontaining question-driven summaries of answers to consumer health questions.\nThis dataset can be used to evaluate single or multi-document summaries\ngenerated by algorithms using extractive or abstractive approaches. In order to\nbenchmark the dataset, we include results of baseline and state-of-the-art deep\nlearning summarization models, demonstrating that this dataset can be used to\neffectively evaluate question-driven machine-generated summaries and promote\nfurther machine learning research in medical question answering.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 20:36:11 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:18:05 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Savery", "Max", ""], ["Abacha", "Asma Ben", ""], ["Gayen", "Soumya", ""], ["Demner-Fushman", "Dina", ""]]}, {"id": "2005.09069", "submitter": "Vivek Gupta", "authors": "Vivek Gupta, Ankit Saw, Pegah Nokhiz, Praneeth Netrapalli, Piyush Rai,\n  Partha Talukdar", "title": "P-SIF: Document Embeddings Using Partition Averaging", "comments": "15 Pages, 3 Figures, 13 Tables, AAAI 2020, Blog :\n  http://vivgupt.blogspot.com/2019/06/document-vector-estimation-using.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simple weighted averaging of word vectors often yields effective\nrepresentations for sentences which outperform sophisticated seq2seq neural\nmodels in many tasks. While it is desirable to use the same method to represent\ndocuments as well, unfortunately, the effectiveness is lost when representing\nlong documents involving multiple sentences. One of the key reasons is that a\nlonger document is likely to contain words from many different topics; hence,\ncreating a single vector while ignoring all the topical structure is unlikely\nto yield an effective document representation. This problem is less acute in\nsingle sentences and other short text fragments where the presence of a single\ntopic is most likely. To alleviate this problem, we present P-SIF, a\npartitioned word averaging model to represent long documents. P-SIF retains the\nsimplicity of simple weighted word averaging while taking a document's topical\nstructure into account. In particular, P-SIF learns topic-specific vectors from\na document and finally concatenates them all to represent the overall document.\nWe provide theoretical justifications on the correctness of P-SIF. Through a\ncomprehensive set of experiments, we demonstrate P-SIF's effectiveness compared\nto simple weighted averaging and many other baselines.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 20:41:12 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gupta", "Vivek", ""], ["Saw", "Ankit", ""], ["Nokhiz", "Pegah", ""], ["Netrapalli", "Praneeth", ""], ["Rai", "Piyush", ""], ["Talukdar", "Partha", ""]]}, {"id": "2005.09093", "submitter": "Shijie Wu", "authors": "Shijie Wu, Mark Dredze", "title": "Are All Languages Created Equal in Multilingual BERT?", "comments": "RepL4NLP Workshop 2020 (Best Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly\ngood cross-lingual performance on several NLP tasks, even without explicit\ncross-lingual signals. However, these evaluations have focused on cross-lingual\ntransfer with high-resource languages, covering only a third of the languages\ncovered by mBERT. We explore how mBERT performs on a much wider set of\nlanguages, focusing on the quality of representation for low-resource\nlanguages, measured by within-language performance. We consider three tasks:\nNamed Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency\nParsing (54 languages each). mBERT does better than or comparable to baselines\non high resource languages but does much worse for low resource languages.\nFurthermore, monolingual BERT models for these languages do even worse. Paired\nwith similar languages, the performance gap between monolingual BERT and mBERT\ncan be narrowed. We find that better models for low resource languages require\nmore efficient pretraining techniques or more data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:15:39 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 02:46:15 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Wu", "Shijie", ""], ["Dredze", "Mark", ""]]}, {"id": "2005.09099", "submitter": "Sean Trott", "authors": "Sean Trott, Tiago Timponi Torrent, Nancy Chang, Nathan Schneider", "title": "(Re)construing Meaning in NLP", "comments": "ACL 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human speakers have an extensive toolkit of ways to express themselves. In\nthis paper, we engage with an idea largely absent from discussions of meaning\nin natural language understanding--namely, that the way something is expressed\nreflects different ways of conceptualizing or construing the information being\nconveyed. We first define this phenomenon more precisely, drawing on\nconsiderable prior work in theoretical cognitive semantics and\npsycholinguistics. We then survey some dimensions of construed meaning and show\nhow insights from construal could inform theoretical and practical work in NLP.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:21:34 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Trott", "Sean", ""], ["Torrent", "Tiago Timponi", ""], ["Chang", "Nancy", ""], ["Schneider", "Nathan", ""]]}, {"id": "2005.09117", "submitter": "Avner May", "authors": "Simran Arora, Avner May, Jian Zhang, Christopher R\\'e", "title": "Contextual Embeddings: When Are They Worth It?", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the settings for which deep contextual embeddings (e.g., BERT) give\nlarge improvements in performance relative to classic pretrained embeddings\n(e.g., GloVe), and an even simpler baseline---random word embeddings---focusing\non the impact of the training set size and the linguistic properties of the\ntask. Surprisingly, we find that both of these simpler baselines can match\ncontextual embeddings on industry-scale data, and often perform within 5 to 10%\naccuracy (absolute) on benchmark tasks. Furthermore, we identify properties of\ndata for which contextual embeddings give particularly large gains: language\ncontaining complex structure, ambiguous word usage, and words unseen in\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 22:20:17 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Arora", "Simran", ""], ["May", "Avner", ""], ["Zhang", "Jian", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2005.09123", "submitter": "Manuel Mager", "authors": "Manuel Mager, Ramon Fernandez Astudillo, Tahira Naseem, Md Arafat\n  Sultan, Young-Suk Lee, Radu Florian and Salim Roukos", "title": "GPT-too: A language-model-first approach for AMR-to-text generation", "comments": "Paper accepted to the Annual Meeting of the Association for\n  Computational Linguistics (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Meaning Representations (AMRs) are broad-coverage sentence-level semantic\ngraphs. Existing approaches to generating text from AMR have focused on\ntraining sequence-to-sequence or graph-to-sequence models on AMR annotated data\nonly. In this paper, we propose an alternative approach that combines a strong\npre-trained language model with cycle consistency-based re-scoring. Despite the\nsimplicity of the approach, our experimental results show these models\noutperform all previous techniques on the English LDC2017T10dataset, including\nthe recent use of transformer architectures. In addition to the standard\nevaluation metrics, we provide human evaluation experiments that further\nsubstantiate the strength of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 22:50:26 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 11:24:29 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Mager", "Manuel", ""], ["Astudillo", "Ramon Fernandez", ""], ["Naseem", "Tahira", ""], ["Sultan", "Md Arafat", ""], ["Lee", "Young-Suk", ""], ["Florian", "Radu", ""], ["Roukos", "Salim", ""]]}, {"id": "2005.09125", "submitter": "EPTCS", "authors": "Yong Li (State Key Laboratory of Computer Science, Institute of\n  Software, Chinese Academy of Sciences), Moshe Y. Vardi (Rice University),\n  Lijun Zhang (State Key Laboratory of Computer Science, Institute of Software,\n  Chinese Academy of Sciences)", "title": "On the Power of Unambiguity in B\\\"uchi Complementation", "comments": "In Proceedings GandALF 2020, arXiv:2009.09360", "journal-ref": "EPTCS 326, 2020, pp. 182-198", "doi": "10.4204/EPTCS.326.12", "report-no": null, "categories": "cs.FL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we exploit the power of \\emph{unambiguity} for the\ncomplementation problem of B\\\"uchi automata by utilizing reduced run directed\nacyclic graphs (DAGs) over infinite words, in which each vertex has at most one\npredecessor. We then show how to use this type of reduced run DAGs as a\n\\emph{unified tool} to optimize \\emph{both} rank-based and slice-based\ncomplementation constructions for B\\\"uchi automata with a finite degree of\nambiguity. As a result, given a B\\\"uchi automaton with $n$ states and a finite\ndegree of ambiguity, the number of states in the complementary B\\\"uchi\nautomaton constructed by the classical rank-based and slice-based\ncomplementation constructions can be improved, respectively, to $2^{O(n)}$ from\n$2^{O(n\\log n)}$ and to $O(4^n)$ from $O((3n)^n)$.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 22:51:34 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 01:27:07 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Li", "Yong", "", "State Key Laboratory of Computer Science, Institute of\n  Software, Chinese Academy of Sciences"], ["Vardi", "Moshe Y.", "", "Rice University"], ["Zhang", "Lijun", "", "State Key Laboratory of Computer Science, Institute of Software,\n  Chinese Academy of Sciences"]]}, {"id": "2005.09128", "submitter": "Matthew Roddy", "authors": "Matthew Roddy and Naomi Harte", "title": "Neural Generation of Dialogue Response Timings", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timings of spoken response offsets in human dialogue have been shown to\nvary based on contextual elements of the dialogue. We propose neural models\nthat simulate the distributions of these response offsets, taking into account\nthe response turn as well as the preceding turn. The models are designed to be\nintegrated into the pipeline of an incremental spoken dialogue system (SDS). We\nevaluate our models using offline experiments as well as human listening tests.\nWe show that human listeners consider certain response timings to be more\nnatural based on the dialogue context. The introduction of these models into\nSDS pipelines could increase the perceived naturalness of interactions.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 23:00:57 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Roddy", "Matthew", ""], ["Harte", "Naomi", ""]]}, {"id": "2005.09133", "submitter": "Boxiang Liu", "authors": "Boxiang Liu and Liang Huang", "title": "NEJM-enzh: A Parallel Corpus for English-Chinese Translation in the\n  Biomedical Domain", "comments": "11 pages, 11 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine translation requires large amounts of parallel text. While such\ndatasets are abundant in domains such as newswire, they are less accessible in\nthe biomedical domain. Chinese and English are two of the most widely spoken\nlanguages, yet to our knowledge a parallel corpus in the biomedical domain does\nnot exist for this language pair. In this study, we develop an effective\npipeline to acquire and process an English-Chinese parallel corpus, consisting\nof about 100,000 sentence pairs and 3,000,000 tokens on each side, from the New\nEngland Journal of Medicine (NEJM). We show that training on out-of-domain data\nand fine-tuning with as few as 4,000 NEJM sentence pairs improve translation\nquality by 25.3 (13.4) BLEU for en$\\to$zh (zh$\\to$en) directions. Translation\nquality continues to improve at a slower pace on larger in-domain datasets,\nwith an increase of 33.0 (24.3) BLEU for en$\\to$zh (zh$\\to$en) directions on\nthe full dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 23:25:15 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Boxiang", ""], ["Huang", "Liang", ""]]}, {"id": "2005.09137", "submitter": "Yongqiang Wang", "authors": "Yangyang Shi, Yongqiang Wang, Chunyang Wu, Christian Fuegen, Frank\n  Zhang, Duc Le, Ching-Feng Yeh, Michael L. Seltzer", "title": "Weak-Attention Suppression For Transformer Based Speech Recognition", "comments": "submitted to interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers, originally proposed for natural language processing (NLP)\ntasks, have recently achieved great success in automatic speech recognition\n(ASR). However, adjacent acoustic units (i.e., frames) are highly correlated,\nand long-distance dependencies between them are weak, unlike text units. It\nsuggests that ASR will likely benefit from sparse and localized attention. In\nthis paper, we propose Weak-Attention Suppression (WAS), a method that\ndynamically induces sparsity in attention probabilities. We demonstrate that\nWAS leads to consistent Word Error Rate (WER) improvement over strong\ntransformer baselines. On the widely used LibriSpeech benchmark, our proposed\nmethod reduced WER by 10%$ on test-clean and 5% on test-other for streamable\ntransformers, resulting in a new state-of-the-art among streaming models.\nFurther analysis shows that WAS learns to suppress attention of non-critical\nand redundant continuous acoustic frames, and is more likely to suppress past\nframes rather than future ones. It indicates the importance of lookahead in\nattention-based ASR models.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 23:49:40 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Shi", "Yangyang", ""], ["Wang", "Yongqiang", ""], ["Wu", "Chunyang", ""], ["Fuegen", "Christian", ""], ["Zhang", "Frank", ""], ["Le", "Duc", ""], ["Yeh", "Ching-Feng", ""], ["Seltzer", "Michael L.", ""]]}, {"id": "2005.09150", "submitter": "Frank Zhang", "authors": "Frank Zhang, Yongqiang Wang, Xiaohui Zhang, Chunxi Liu, Yatharth\n  Saraf, Geoffrey Zweig", "title": "Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces", "comments": "In proceedings Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we first show that on the widely used LibriSpeech benchmark,\nour transformer-based context-dependent connectionist temporal classification\n(CTC) system produces state-of-the-art results. We then show that using\nwordpieces as modeling units combined with CTC training, we can greatly\nsimplify the engineering pipeline compared to conventional frame-based\ncross-entropy training by excluding all the GMM bootstrapping, decision tree\nbuilding and force alignment steps, while still achieving very competitive\nword-error-rate. Additionally, using wordpieces as modeling units can\nsignificantly improve runtime efficiency since we can use larger stride without\nlosing accuracy. We further confirm these findings on two internal VideoASR\ndatasets: German, which is similar to English as a fusional language, and\nTurkish, which is an agglutinative language.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:43:17 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 21:22:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhang", "Frank", ""], ["Wang", "Yongqiang", ""], ["Zhang", "Xiaohui", ""], ["Liu", "Chunxi", ""], ["Saraf", "Yatharth", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "2005.09183", "submitter": "Seito Kasai", "authors": "Seito Kasai, Yuchi Ishikawa, Masaki Hayashi, Yoshimitsu Aoki, Kensho\n  Hara, Hirokatsu Kataoka", "title": "Retrieving and Highlighting Action with Spatiotemporal Reference", "comments": "Accepted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework that jointly retrieves and\nspatiotemporally highlights actions in videos by enhancing current deep\ncross-modal retrieval methods. Our work takes on the novel task of action\nhighlighting, which visualizes where and when actions occur in an untrimmed\nvideo setting. Action highlighting is a fine-grained task, compared to\nconventional action recognition tasks which focus on classification or\nwindow-based localization. Leveraging weak supervision from annotated captions,\nour framework acquires spatiotemporal relevance maps and generates local\nembeddings which relate to the nouns and verbs in captions. Through\nexperiments, we show that our model generates various maps conditioned on\ndifferent actions, in which conventional visual reasoning methods only go as\nfar as to show a single deterministic saliency map. Also, our model improves\nretrieval recall over our baseline without alignment by 2-3% on the MSR-VTT\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 03:12:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kasai", "Seito", ""], ["Ishikawa", "Yuchi", ""], ["Hayashi", "Masaki", ""], ["Aoki", "Yoshimitsu", ""], ["Hara", "Kensho", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "2005.09198", "submitter": "Ozgur Ozmen", "authors": "James Nutaro and Ozgur Ozmen", "title": "Quantifying the Uncertainty of Precision Estimates for Rule based Text\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule based classifiers that use the presence and absence of key sub-strings\nto make classification decisions have a natural mechanism for quantifying the\nuncertainty of their precision. For a binary classifier, the key insight is to\ntreat partitions of the sub-string set induced by the documents as Bernoulli\nrandom variables. The mean value of each random variable is an estimate of the\nclassifier's precision when presented with a document inducing that partition.\nThese means can be compared, using standard statistical tests, to a desired or\nexpected classifier precision. A set of binary classifiers can be combined into\na single, multi-label classifier by an application of the Dempster-Shafer\ntheory of evidence. The utility of this approach is demonstrated with a\nbenchmark problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 03:51:47 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Nutaro", "James", ""], ["Ozmen", "Ozgur", ""]]}, {"id": "2005.09207", "submitter": "Zhiyu Chen", "authors": "Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, Brian D. Davison", "title": "Table Search Using a Deep Contextualized Language Model", "comments": "Accepted at SIGIR 2020 (Long)", "journal-ref": null, "doi": "10.1145/3397271.3401044", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained contextualized language models such as BERT have achieved\nimpressive results on various natural language processing benchmarks.\nBenefiting from multiple pretraining tasks and large scale training corpora,\npretrained models can capture complex syntactic word relations. In this paper,\nwe use the deep contextualized language model BERT for the task of ad hoc table\nretrieval. We investigate how to encode table content considering the table\nstructure and input length limit of BERT. We also propose an approach that\nincorporates features from prior literature on table retrieval and jointly\ntrains them with BERT. In experiments on public datasets, we show that our best\napproach can outperform the previous state-of-the-art method and BERT baselines\nwith a large margin under different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 04:18:04 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 23:07:15 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Chen", "Zhiyu", ""], ["Trabelsi", "Mohamed", ""], ["Heflin", "Jeff", ""], ["Xu", "Yinan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2005.09225", "submitter": "David Wadden", "authors": "David Wadden, Tal August, Qisheng Li, Tim Althoff", "title": "The Effect of Moderation on Online Mental Health Conversations", "comments": "Accepted as a full paper at ICWSM 2021. 12 pages, 12 figures, 3\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people struggling with mental health issues are unable to access\nadequate care due to high costs and a shortage of mental health professionals,\nleading to a global mental health crisis. Online mental health communities can\nhelp mitigate this crisis by offering a scalable, easily accessible alternative\nto in-person sessions with therapists or support groups. However, people\nseeking emotional or psychological support online may be especially vulnerable\nto the kinds of antisocial behavior that sometimes occur in online discussions.\nModeration can improve online discourse quality, but we lack an understanding\nof its effects on online mental health conversations. In this work, we\nleveraged a natural experiment, occurring across 200,000 messages from 7,000\nonline mental health conversations, to evaluate the effects of moderation on\nonline mental health discussions. We found that participation in group mental\nhealth discussions led to improvements in psychological perspective, and that\nthese improvements were larger in moderated conversations. The presence of a\nmoderator increased user engagement, encouraged users to discuss negative\nemotions more candidly, and dramatically reduced bad behavior among chat\nparticipants. Moderation also encouraged stronger linguistic coordination,\nwhich is indicative of trust building. In addition, moderators who remained\nactive in conversations were especially successful in keeping conversations on\ntopic. Our findings suggest that moderation can serve as a valuable tool to\nimprove the efficacy and safety of online mental health conversations. Based on\nthese findings, we discuss implications and trade-offs involved in designing\neffective online spaces for mental health support.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 05:40:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 20:55:46 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 22:08:16 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2020 19:20:09 GMT"}, {"version": "v5", "created": "Tue, 6 Apr 2021 23:35:09 GMT"}, {"version": "v6", "created": "Wed, 14 Apr 2021 04:14:48 GMT"}, {"version": "v7", "created": "Thu, 22 Apr 2021 22:00:35 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wadden", "David", ""], ["August", "Tal", ""], ["Li", "Qisheng", ""], ["Althoff", "Tim", ""]]}, {"id": "2005.09246", "submitter": "Rajeev Bhatt Ambati", "authors": "Rajeev Bhatt Ambati, Ahmed Ada Hanifi, Ramya Vunikili, Puneet Sharma,\n  and Oladimeji Farri", "title": "Assertion Detection in Multi-Label Clinical Text using Scope\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label sentences (text) in the clinical domain result from the rich\ndescription of scenarios during patient care. The state-of-theart methods for\nassertion detection mostly address this task in the setting of a single\nassertion label per sentence (text). In addition, few rules based and deep\nlearning methods perform negation/assertion scope detection on single-label\ntext. It is a significant challenge extending these methods to address\nmulti-label sentences without diminishing performance. Therefore, we developed\na convolutional neural network (CNN) architecture to localize multiple labels\nand their scopes in a single stage end-to-end fashion, and demonstrate that our\nmodel performs atleast 12% better than the state-of-the-art on multi-label\nclinical text.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 06:56:02 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ambati", "Rajeev Bhatt", ""], ["Hanifi", "Ahmed Ada", ""], ["Vunikili", "Ramya", ""], ["Sharma", "Puneet", ""], ["Farri", "Oladimeji", ""]]}, {"id": "2005.09256", "submitter": "Kohei Matsuura", "authors": "Kohei Matsuura, Masato Mimura, Shinsuke Sakai, Tatsuya Kawahara", "title": "Generative Adversarial Training Data Adaptation for Very Low-resource\n  Automatic Speech Recognition", "comments": "Accepted for Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to transcribe and archive speech data of endangered languages\nfor preserving heritages of verbal culture and automatic speech recognition\n(ASR) is a powerful tool to facilitate this process. However, since endangered\nlanguages do not generally have large corpora with many speakers, the\nperformance of ASR models trained on them are considerably poor in general.\nNevertheless, we are often left with a lot of recordings of spontaneous speech\ndata that have to be transcribed. In this work, for mitigating this speaker\nsparsity problem, we propose to convert the whole training speech data and make\nit sound like the test speaker in order to develop a highly accurate ASR system\nfor this speaker. For this purpose, we utilize a CycleGAN-based non-parallel\nvoice conversion technology to forge a labeled training data that is close to\nthe test speaker's speech. We evaluated this speaker adaptation approach on two\nlow-resource corpora, namely, Ainu and Mboshi. We obtained 35-60% relative\nimprovement in phone error rate on the Ainu corpus, and 40% relative\nimprovement was attained on the Mboshi corpus. This approach outperformed two\nconventional methods namely unsupervised adaptation and multilingual training\nwith these two corpora.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:35:14 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 08:38:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Matsuura", "Kohei", ""], ["Mimura", "Masato", ""], ["Sakai", "Shinsuke", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2005.09260", "submitter": "Pavel Kral", "authors": "Ji\\v{r}\\'i Mart\\'inek, Christophe Cerisara, Pavel Kr\\'al and Ladislav\n  Lenc", "title": "Cross-lingual Approaches for Task-specific Dialogue Act Recognition", "comments": "Accepted for 17th International Conference on Artificial Intelligence\n  Applications and Innovations (AIAI 2021), 25-27 June", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we exploit cross-lingual models to enable dialogue act\nrecognition for specific tasks with a small number of annotations. We design a\ntransfer learning approach for dialogue act recognition and validate it on two\ndifferent target languages and domains. We compute dialogue turn embeddings\nwith both a CNN and multi-head self-attention model and show that the best\nresults are obtained by combining all sources of transferred information. We\nfurther demonstrate that the proposed methods significantly outperform related\ncross-lingual DA recognition approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:44:48 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 06:27:08 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mart\u00ednek", "Ji\u0159\u00ed", ""], ["Cerisara", "Christophe", ""], ["Kr\u00e1l", "Pavel", ""], ["Lenc", "Ladislav", ""]]}, {"id": "2005.09267", "submitter": "Qiantong Xu", "authors": "Qiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel\n  Synnaeve, Ronan Collobert", "title": "Iterative Pseudo-Labeling for Speech Recognition", "comments": "INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-labeling has recently shown promise in end-to-end automatic speech\nrecognition (ASR). We study Iterative Pseudo-Labeling (IPL), a semi-supervised\nalgorithm which efficiently performs multiple iterations of pseudo-labeling on\nunlabeled data as the acoustic model evolves. In particular, IPL fine-tunes an\nexisting model at each iteration using both labeled data and a subset of\nunlabeled data. We study the main components of IPL: decoding with a language\nmodel and data augmentation. We then demonstrate the effectiveness of IPL by\nachieving state-of-the-art word-error rate on the Librispeech test sets in both\nstandard and low-resource setting. We also study the effect of language models\ntrained on different corpora to show IPL can effectively utilize additional\ntext. Finally, we release a new large in-domain text corpus which does not\noverlap with the Librispeech training transcriptions to foster research in\nlow-resource, semi-supervised ASR\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:56:21 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 01:30:10 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Xu", "Qiantong", ""], ["Likhomanenko", "Tatiana", ""], ["Kahn", "Jacob", ""], ["Hannun", "Awni", ""], ["Synnaeve", "Gabriel", ""], ["Collobert", "Ronan", ""]]}, {"id": "2005.09271", "submitter": "Benlai Tang", "authors": "Wenjie Li, Benlai Tang, Xiang Yin, Yushi Zhao, Wei Li, Kang Wang, Hao\n  Huang, Yuxuan Wang, Zejun Ma", "title": "Improving Accent Conversion with Reference Encoder and End-To-End\n  Text-To-Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accent conversion (AC) transforms a non-native speaker's accent into a native\naccent while maintaining the speaker's voice timbre. In this paper, we propose\napproaches to improving accent conversion applicability, as well as quality.\nFirst of all, we assume no reference speech is available at the conversion\nstage, and hence we employ an end-to-end text-to-speech system that is trained\non native speech to generate native reference speech. To improve the quality\nand accent of the converted speech, we introduce reference encoders which make\nus capable of utilizing multi-source information. This is motivated by acoustic\nfeatures extracted from native reference and linguistic information, which are\ncomplementary to conventional phonetic posteriorgrams (PPGs), so they can be\nconcatenated as features to improve a baseline system based only on PPGs.\nMoreover, we optimize model architecture using GMM-based attention instead of\nwindowed attention to elevate synthesized performance. Experimental results\nindicate when the proposed techniques are applied the integrated system\nsignificantly raises the scores of acoustic quality (30$\\%$ relative increase\nin mean opinion score) and native accent (68$\\%$ relative preference) while\nretaining the voice identity of the non-native speaker.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:09:58 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Li", "Wenjie", ""], ["Tang", "Benlai", ""], ["Yin", "Xiang", ""], ["Zhao", "Yushi", ""], ["Li", "Wei", ""], ["Wang", "Kang", ""], ["Huang", "Hao", ""], ["Wang", "Yuxuan", ""], ["Ma", "Zejun", ""]]}, {"id": "2005.09276", "submitter": "Qi Jia", "authors": "Qi Jia, Mengxue Zhang, Shengyao Zhang, Kenny Q. Zhu", "title": "Matching Questions and Answers in Dialogues from Online Forums", "comments": "Accepted at ECAI2020", "journal-ref": "ECAI 2020: 24th European Conference on Artificial Intelligence", "doi": "10.3233/FAIA200326", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching question-answer relations between two turns in conversations is not\nonly the first step in analyzing dialogue structures, but also valuable for\ntraining dialogue systems. This paper presents a QA matching model considering\nboth distance information and dialogue history by two simultaneous attention\nmechanisms called mutual attention. Given scores computed by the trained model\nbetween each non-question turn with its candidate questions, a greedy matching\nstrategy is used for final predictions. Because existing dialogue datasets such\nas the Ubuntu dataset are not suitable for the QA matching task, we further\ncreate a dataset with 1,000 labeled dialogues and demonstrate that our proposed\nmodel outperforms the state-of-the-art and other strong baselines, particularly\nfor matching long-distance QA pairs.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:18:52 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 02:44:07 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Jia", "Qi", ""], ["Zhang", "Mengxue", ""], ["Zhang", "Shengyao", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2005.09282", "submitter": "Bolaji Yusuf", "authors": "Bolaji Yusuf and Lucas Ondel", "title": "Bayesian Subspace HMM for the Zerospeech 2020 Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our submission to the Zerospeech 2020 challenge,\nwhere the participants are required to discover latent representations from\nunannotated speech, and to use those representations to perform speech\nsynthesis, with synthesis quality used as a proxy metric for the unit quality.\nIn our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit\ndiscovery. The SHMM models each unit as an HMM whose parameters are constrained\nto lie in a low dimensional subspace of the total parameter space which is\ntrained to model phonetic variability. Our system compares favorably with the\nbaseline on the human-evaluated character error rate while maintaining\nsignificantly lower unit bitrate.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:28:38 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 12:24:11 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yusuf", "Bolaji", ""], ["Ondel", "Lucas", ""]]}, {"id": "2005.09284", "submitter": "William Caicedo-Torres", "authors": "William Caicedo-Torres, Jairo Gutierrez", "title": "ISeeU2: Visually Interpretable ICU mortality prediction using deep\n  learning and free-text medical notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Accurate mortality prediction allows Intensive Care Units (ICUs) to\nadequately benchmark clinical practice and identify patients with unexpected\noutcomes. Traditionally, simple statistical models have been used to assess\npatient death risk, many times with sub-optimal performance. On the other hand\ndeep learning holds promise to positively impact clinical practice by\nleveraging medical data to assist diagnosis and prediction, including mortality\nprediction. However, as the question of whether powerful Deep Learning models\nattend correlations backed by sound medical knowledge when generating\npredictions remains open, additional interpretability tools are needed to\nfoster trust and encourage the use of AI by clinicians. In this work we show a\nDeep Learning model trained on MIMIC-III to predict mortality using raw nursing\nnotes, together with visual explanations for word importance. Our model reaches\na ROC of 0.8629 (+/-0.0058), outperforming the traditional SAPS-II score and\nproviding enhanced interpretability when compared with similar Deep Learning\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:30:34 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Caicedo-Torres", "William", ""], ["Gutierrez", "Jairo", ""]]}, {"id": "2005.09336", "submitter": "Albert Zeyer", "authors": "Mohammad Zeineldeen, Albert Zeyer, Wei Zhou, Thomas Ng, Ralf\n  Schl\\\"uter, Hermann Ney", "title": "A systematic comparison of grapheme-based vs. phoneme-based label units\n  for encoder-decoder-attention models", "comments": "5 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the rationale of end-to-end modeling, CTC, RNN-T or\nencoder-decoder-attention models for automatic speech recognition (ASR) use\ngraphemes or grapheme-based subword units based on e.g. byte-pair encoding\n(BPE). The mapping from pronunciation to spelling is learned completely from\ndata. In contrast to this, classical approaches to ASR employ secondary\nknowledge sources in the form of phoneme lists to define phonetic output labels\nand pronunciation lexica. In this work, we do a systematic comparison between\ngrapheme- and phoneme-based output labels for an encoder-decoder-attention ASR\nmodel. We investigate the use of single phonemes as well as BPE-based phoneme\ngroups as output labels of our model. To preserve a simplified and efficient\ndecoder design, we also extend the phoneme set by auxiliary units to be able to\ndistinguish homophones. Experiments performed on the Switchboard 300h and\nLibriSpeech benchmarks show that phoneme-based modeling is competitive to\ngrapheme-based encoder-decoder-attention modeling.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:54:17 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 22:05:17 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 16:59:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zeineldeen", "Mohammad", ""], ["Zeyer", "Albert", ""], ["Zhou", "Wei", ""], ["Ng", "Thomas", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2005.09379", "submitter": "Martin Tutek", "authors": "Martin Tutek, Jan \\v{S}najder", "title": "Staying True to Your Word: (How) Can Attention Become Explanation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attention mechanism has quickly become ubiquitous in NLP. In addition to\nimproving performance of models, attention has been widely used as a glimpse\ninto the inner workings of NLP models. The latter aspect has in the recent\nyears become a common topic of discussion, most notably in work of Jain and\nWallace, 2019; Wiegreffe and Pinter, 2019. With the shortcomings of using\nattention weights as a tool of transparency revealed, the attention mechanism\nhas been stuck in a limbo without concrete proof when and whether it can be\nused as an explanation. In this paper, we provide an explanation as to why\nattention has seen rightful critique when used with recurrent networks in\nsequence classification tasks. We propose a remedy to these issues in the form\nof a word level objective and our findings give credibility for attention to\nprovide faithful interpretations of recurrent models.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 11:55:11 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Tutek", "Martin", ""], ["\u0160najder", "Jan", ""]]}, {"id": "2005.09382", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Sona Mokra, Nathaniel Wong, Tim Harley", "title": "Human Instruction-Following with Deep Reinforcement Learning via\n  Transfer-Learning from Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has described neural-network-based agents that are trained with\nreinforcement learning (RL) to execute language-like commands in simulated\nworlds, as a step towards an intelligent agent or robot that can be instructed\nby human users. However, the optimisation of multi-goal motor policies via deep\nRL from scratch requires many episodes of experience. Consequently,\ninstruction-following with deep RL typically involves language generated from\ntemplates (by an environment simulator), which does not reflect the varied or\nambiguous expressions of real users. Here, we propose a conceptually simple\nmethod for training instruction-following agents with deep RL that are robust\nto natural human instructions. By applying our method with a state-of-the-art\npre-trained text-based language model (BERT), on tasks requiring agents to\nidentify and position everyday objects relative to other objects in a\nnaturalistic 3D simulated room, we demonstrate substantially-above-chance\nzero-shot transfer from synthetic template commands to natural instructions\ngiven by humans. Our approach is a general recipe for training any deep\nRL-based system to interface with human users, and bridges the gap between two\nresearch directions of notable recent success: agent-centric motor behavior and\ntext-based representation learning.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:16:58 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Hill", "Felix", ""], ["Mokra", "Sona", ""], ["Wong", "Nathaniel", ""], ["Harley", "Tim", ""]]}, {"id": "2005.09389", "submitter": "Lukas Lange", "authors": "Lukas Lange, Heike Adel, Jannik Str\\\"otgen", "title": "On the Choice of Auxiliary Languages for Improved Sequence Tagging", "comments": "RepL4NLP at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work showed that embeddings from related languages can improve the\nperformance of sequence tagging, even for monolingual models. In this analysis\npaper, we investigate whether the best auxiliary language can be predicted\nbased on language distances and show that the most related language is not\nalways the best auxiliary language. Further, we show that attention-based\nmeta-embeddings can effectively combine pre-trained embeddings from different\nlanguages for sequence tagging and set new state-of-the-art results for\npart-of-speech tagging in five languages.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:32:20 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lange", "Lukas", ""], ["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""]]}, {"id": "2005.09392", "submitter": "Lukas Lange", "authors": "Lukas Lange, Anastasiia Iurshina, Heike Adel, Jannik Str\\\"otgen", "title": "Adversarial Alignment of Multilingual Models for Extracting Temporal\n  Expressions from Text", "comments": "RepL4NLP at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although temporal tagging is still dominated by rule-based systems, there\nhave been recent attempts at neural temporal taggers. However, all of them\nfocus on monolingual settings. In this paper, we explore multilingual methods\nfor the extraction of temporal expressions from text and investigate\nadversarial training for aligning embedding spaces to one common space. With\nthis, we create a single multilingual model that can also be transferred to\nunseen languages and set the new state of the art in those cross-lingual\ntransfer experiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:37:04 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lange", "Lukas", ""], ["Iurshina", "Anastasiia", ""], ["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""]]}, {"id": "2005.09394", "submitter": "Hirofumi Inaguma", "authors": "Hirofumi Inaguma, Masato Mimura, Tatsuya Kawahara", "title": "Enhancing Monotonic Multihead Attention for Streaming ASR", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a monotonic multihead attention (MMA) by extending hard\nmonotonic attention to Transformer-based automatic speech recognition (ASR) for\nonline streaming applications. For streaming inference, all monotonic attention\n(MA) heads should learn proper alignments because the next token is not\ngenerated until all heads detect the corresponding token boundaries. However,\nwe found not all MA heads learn alignments with a na\\\"ive implementation. To\nencourage every head to learn alignments properly, we propose HeadDrop\nregularization by masking out a part of heads stochastically during training.\nFurthermore, we propose to prune redundant heads to improve consensus among\nheads for boundary detection and prevent delayed token generation caused by\nsuch heads. Chunkwise attention on each MA head is extended to the multihead\ncounterpart. Finally, we propose head-synchronous beam search decoding to\nguarantee stable streaming inference.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:39:38 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 11:11:27 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 12:20:25 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Inaguma", "Hirofumi", ""], ["Mimura", "Masato", ""], ["Kawahara", "Tatsuya", ""]]}, {"id": "2005.09397", "submitter": "Lukas Lange", "authors": "Lukas Lange, Heike Adel, Jannik Str\\\"otgen", "title": "Closing the Gap: Joint De-Identification and Concept Extraction in the\n  Clinical Domain", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting natural language processing in the clinical domain requires\nde-identification, i.e., anonymization of personal information in texts.\nHowever, current research considers de-identification and downstream tasks,\nsuch as concept extraction, only in isolation and does not study the effects of\nde-identification on other tasks. In this paper, we close this gap by reporting\nconcept extraction performance on automatically anonymized data and\ninvestigating joint models for de-identification and concept extraction. In\nparticular, we propose a stacked model with restricted access to\nprivacy-sensitive information and a multitask model. We set the new state of\nthe art on benchmark datasets in English (96.1% F1 for de-identification and\n88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 12:44:41 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lange", "Lukas", ""], ["Adel", "Heike", ""], ["Str\u00f6tgen", "Jannik", ""]]}, {"id": "2005.09406", "submitter": "Sebastian Garcia-Valencia", "authors": "Sebastian Garcia-Valencia", "title": "Embeddings as representation for symbolic music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A representation technique that allows encoding music in a way that contains\nmusical meaning would improve the results of any model trained for computer\nmusic tasks like generation of melodies and harmonies of better quality. The\nfield of natural language processing has done a lot of work in finding a way to\ncapture the semantic meaning of words and sentences, and word embeddings have\nsuccessfully shown the capabilities for such a task. In this paper, we\nexperiment with embeddings to represent musical notes from 3 different\nvariations of a dataset and analyze if the model can capture useful musical\npatterns. To do this, the resulting embeddings are visualized in projections\nusing the t-SNE technique.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:04:02 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Garcia-Valencia", "Sebastian", ""]]}, {"id": "2005.09409", "submitter": "Benjamin Van Niekerk", "authors": "Benjamin van Niekerk, Leanne Nortje, Herman Kamper", "title": "Vector-quantized neural networks for acoustic unit discovery in the\n  ZeroSpeech 2020 challenge", "comments": "5 pages, 3 figures, 2 tables, accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore vector quantization for acoustic unit discovery.\nLeveraging unlabelled data, we aim to learn discrete representations of speech\nthat separate phonetic content from speaker-specific details. We propose two\nneural models to tackle this challenge - both use vector quantization to map\ncontinuous features to a finite set of codes. The first model is a type of\nvector-quantized variational autoencoder (VQ-VAE). The VQ-VAE encodes speech\ninto a sequence of discrete units before reconstructing the audio waveform. Our\nsecond model combines vector quantization with contrastive predictive coding\n(VQ-CPC). The idea is to learn a representation of speech by predicting future\nacoustic units. We evaluate the models on English and Indonesian data for the\nZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models\noutperform all submissions to the 2019 and 2020 challenges, with a relative\nimprovement of more than 30%. The models also perform competitively on a\ndownstream voice conversion task. Of the two, VQ-CPC performs slightly better\nin general and is simpler and faster to train. Finally, probing experiments\nshow that vector quantization is an effective bottleneck, forcing the models to\ndiscard speaker information.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:06:17 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 12:41:55 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["van Niekerk", "Benjamin", ""], ["Nortje", "Leanne", ""], ["Kamper", "Herman", ""]]}, {"id": "2005.09439", "submitter": "EPTCS", "authors": "Giovanni de Felice (University of Oxford), Elena Di Lavore (Tallinn\n  University of Technology), Mario Rom\\'an (Tallinn University of Technology),\n  Alexis Toumi (University of Oxford)", "title": "Functorial Language Games for Question Answering", "comments": "In Proceedings ACT 2020, arXiv:2101.07888", "journal-ref": "EPTCS 333, 2021, pp. 311-321", "doi": "10.4204/EPTCS.333.21", "report-no": null, "categories": "cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some categorical investigations into Wittgenstein's\nlanguage-games, with applications to game-theoretic pragmatics and\nquestion-answering in natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:35:13 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 00:09:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["de Felice", "Giovanni", "", "University of Oxford"], ["Di Lavore", "Elena", "", "Tallinn\n  University of Technology"], ["Rom\u00e1n", "Mario", "", "Tallinn University of Technology"], ["Toumi", "Alexis", "", "University of Oxford"]]}, {"id": "2005.09471", "submitter": "Danny Merkx", "authors": "Danny Merkx and Stefan L. Frank", "title": "Human Sentence Processing: Recurrence or Attention?", "comments": "This paper will appear in the proceedings of CMCL 2021 to be held\n  June 10th", "journal-ref": null, "doi": "10.18653/v1/2021.cmcl-1.2", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have long been an architecture of interest\nfor computational models of human sentence processing. The recently introduced\nTransformer architecture outperforms RNNs on many natural language processing\ntasks but little is known about its ability to model human language processing.\nWe compare Transformer- and RNN-based language models' ability to account for\nmeasures of human reading effort. Our analysis shows Transformers to outperform\nRNNs in explaining self-paced reading times and neural activity during reading\nEnglish sentences, challenging the widely held idea that human sentence\nprocessing involves recurrent and immediate processing and provides evidence\nfor cue-based retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:17:49 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 12:49:14 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Merkx", "Danny", ""], ["Frank", "Stefan L.", ""]]}, {"id": "2005.09606", "submitter": "Sudha Rao", "authors": "Angela S. Lin, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Chris\n  Brockett, Debadeepta Dey, Bill Dolan", "title": "A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks", "comments": "This paper has been accepted to be published at ACL 2020", "journal-ref": "Association of Computational Linguistics 2020", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high-level procedural tasks can be decomposed into sequences of\ninstructions that vary in their order and choice of tools. In the cooking\ndomain, the web offers many partially-overlapping text and video recipes (i.e.\nprocedures) that describe how to make the same dish (i.e. high-level task).\nAligning instructions for the same dish across different sources can yield\ndescriptive visual explanations that are far richer semantically than\nconventional textual instructions, providing commonsense insight into how\nreal-world procedures are structured. Learning to align these different\ninstruction sets is challenging because: a) different recipes vary in their\norder of instructions and use of ingredients; and b) video instructions can be\nnoisy and tend to contain far more information than text instructions. To\naddress these challenges, we first use an unsupervised alignment algorithm that\nlearns pairwise alignments between instructions of different recipes for the\nsame dish. We then use a graph algorithm to derive a joint alignment between\nmultiple text and multiple video recipes for the same dish. We release the\nMicrosoft Research Multimodal Aligned Recipe Corpus containing 150K pairwise\nalignments between recipes across 4,262 dishes with rich commonsense\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:27:00 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lin", "Angela S.", ""], ["Rao", "Sudha", ""], ["Celikyilmaz", "Asli", ""], ["Nouri", "Elnaz", ""], ["Brockett", "Chris", ""], ["Dey", "Debadeepta", ""], ["Dolan", "Bill", ""]]}, {"id": "2005.09649", "submitter": "Ammar Rashed", "authors": "Ammar Rashed, Mucahid Kutlu, Kareem Darwish, Tamer Elsayed, Cans{\\i}n\n  Bayrak", "title": "Embeddings-Based Clustering for Target Specific Stances: The Case of a\n  Polarized Turkey", "comments": "arXiv admin note: text overlap with arXiv:1909.10213", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On June 24, 2018, Turkey conducted a highly consequential election in which\nthe Turkish people elected their president and parliament in the first election\nunder a new presidential system. During the election period, the Turkish people\nextensively shared their political opinions on Twitter. One aspect of\npolarization among the electorate was support for or opposition to the\nreelection of Recep Tayyip Erdo\\u{g}an. In this paper, we present an\nunsupervised method for target-specific stance detection in a polarized\nsetting, specifically Turkish politics, achieving 90% precision in identifying\nuser stances, while maintaining more than 80% recall. The method involves\nrepresenting users in an embedding space using Google's Convolutional Neural\nNetwork (CNN) based multilingual universal sentence encoder. The\nrepresentations are then projected onto a lower dimensional space in a manner\nthat reflects similarities and are consequently clustered. We show the\neffectiveness of our method in properly clustering users of divergent groups\nacross multiple targets that include political figures, different groups, and\nparties. We perform our analysis on a large dataset of 108M Turkish\nelection-related tweets along with the timeline tweets of 168k Turkish users,\nwho authored 213M tweets. Given the resultant user stances, we are able to\nobserve correlations between topics and compute topic polarization.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:52:15 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Rashed", "Ammar", ""], ["Kutlu", "Mucahid", ""], ["Darwish", "Kareem", ""], ["Elsayed", "Tamer", ""], ["Bayrak", "Cans\u0131n", ""]]}, {"id": "2005.09684", "submitter": "Liang Lu", "authors": "Liang Lu, Changliang Liu, Jinyu Li and Yifan Gong", "title": "Exploring Transformers for Large-Scale Speech Recognition", "comments": "5 pages, 1 figure, Interspeech 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recurrent neural networks still largely define state-of-the-art speech\nrecognition systems, the Transformer network has been proven to be a\ncompetitive alternative, especially in the offline condition. Most studies with\nTransformers have been constrained in a relatively small scale setting, and\nsome forms of data argumentation approaches are usually applied to combat the\ndata sparsity issue. In this paper, we aim at understanding the behaviors of\nTransformers in the large-scale speech recognition setting, where we have used\naround 65,000 hours of training data. We investigated various aspects on\nscaling up Transformers, including model initialization, warmup training as\nwell as different Layer Normalization strategies. In the streaming condition,\nwe compared the widely used attention mask based future context lookahead\napproach to the Transformer-XL network. From our experiments, we show that\nTransformers can achieve around 6% relative word error rate (WER) reduction\ncompared to the BLSTM baseline in the offline fashion, while in the streaming\nfashion, Transformer-XL is comparable to LC-BLSTM with 800 millisecond latency\nconstraint.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:07:14 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 18:51:37 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Lu", "Liang", ""], ["Liu", "Changliang", ""], ["Li", "Jinyu", ""], ["Gong", "Yifan", ""]]}, {"id": "2005.09740", "submitter": "Javad Rafiei Asl", "authors": "Javad Rafiei Asl, Juan M. Banda", "title": "GLEAKE: Global and Local Embedding Automatic Keyphrase Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for granular categorization of large corpora of text\ndocuments have become increasingly more important with the rate scientific,\nnews, medical, and web documents are growing in the last few years. Automatic\nkeyphrase extraction (AKE) aims to automatically detect a small set of single\nor multi-words from within a single textual document that captures the main\ntopics of the document. AKE plays an important role in various NLP and\ninformation retrieval tasks such as document summarization and categorization,\nfull-text indexing, and article recommendation. Due to the lack of sufficient\nhuman-labeled data in different textual contents, supervised learning\napproaches are not ideal for automatic detection of keyphrases from the content\nof textual bodies. With the state-of-the-art advances in text embedding\ntechniques, NLP researchers have focused on developing unsupervised methods to\nobtain meaningful insights from raw datasets. In this work, we introduce Global\nand Local Embedding Automatic Keyphrase Extractor (GLEAKE) for the task of AKE.\nGLEAKE utilizes single and multi-word embedding techniques to explore the\nsyntactic and semantic aspects of the candidate phrases and then combines them\ninto a series of embedding-based graphs. Moreover, GLEAKE applies network\nanalysis techniques on each embedding-based graph to refine the most\nsignificant phrases as a final set of keyphrases. We demonstrate the high\nperformance of GLEAKE by evaluating its results on five standard AKE datasets\nfrom different domains and writing styles and by showing its superiority with\nregards to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:24:02 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Asl", "Javad Rafiei", ""], ["Banda", "Juan M.", ""]]}, {"id": "2005.09803", "submitter": "Aman Tyagi", "authors": "Aman Tyagi, Anjalie Field, Priyank Lathwal, Yulia Tsvetkov, Kathleen\n  M. Carley", "title": "A Computational Analysis of Polarization on Indian and Pakistani Social\n  Media", "comments": null, "journal-ref": "Social Informatics - 12th International Conference, SocInfo 2020,\n  Pisa, Italy", "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Between February 14, 2019 and March 4, 2019, a terrorist attack in Pulwama,\nKashmir followed by retaliatory airstrikes led to rising tensions between India\nand Pakistan, two nuclear-armed countries. In this work, we examine polarizing\nmessaging on Twitter during these events, particularly focusing on the\npositions of Indian and Pakistani politicians. We use a label propagation\ntechnique focused on hashtag co-occurrences to find polarizing tweets and\nusers. Our analysis reveals that politicians in the ruling political party in\nIndia (BJP) used polarized hashtags and called for escalation of conflict more\nso than politicians from other parties. Our work offers the first analysis of\nhow escalating tensions between India and Pakistan manifest on Twitter and\nprovides a framework for studying polarizing messages.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 00:44:38 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 02:09:14 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Tyagi", "Aman", ""], ["Field", "Anjalie", ""], ["Lathwal", "Priyank", ""], ["Tsvetkov", "Yulia", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "2005.09824", "submitter": "Yiwen Shao", "authors": "Yiwen Shao, Yiming Wang, Daniel Povey, Sanjeev Khudanpur", "title": "PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for\n  End-to-End ASR", "comments": "Submtted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PyChain, a fully parallelized PyTorch implementation of end-to-end\nlattice-free maximum mutual information (LF-MMI) training for the so-called\n\\emph{chain models} in the Kaldi automatic speech recognition (ASR) toolkit.\nUnlike other PyTorch and Kaldi based ASR toolkits, PyChain is designed to be as\nflexible and light-weight as possible so that it can be easily plugged into new\nASR projects, or other existing PyTorch-based ASR tools, as exemplified\nrespectively by a new project PyChain-example, and Espresso, an existing\nend-to-end ASR toolkit. PyChain's efficiency and flexibility is demonstrated\nthrough such novel features as full GPU training on numerator/denominator\ngraphs, and support for unequal length sequences. Experiments on the WSJ\ndataset show that with simple neural networks and commonly used machine\nlearning techniques, PyChain can achieve competitive results that are\ncomparable to Kaldi and better than other end-to-end ASR systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 02:10:21 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Shao", "Yiwen", ""], ["Wang", "Yiming", ""], ["Povey", "Daniel", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "2005.09837", "submitter": "Jichang Zhao", "authors": "Di Weng, Jichang Zhao", "title": "Positive emotions help rank negative reviews in e-commerce", "comments": "Emotion lexicons are publicly available at\n  https://doi.org/10.6084/m9.figshare.12327680.v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative reviews, the poor ratings in postpurchase evaluation, play an\nindispensable role in e-commerce, especially in shaping future sales and firm\nequities. However, extant studies seldom examine their potential value for\nsellers and producers in enhancing capabilities of providing better services\nand products. For those who exploited the helpfulness of reviews in the view of\ne-commerce keepers, the ranking approaches were developed for customers\ninstead. To fill this gap, in terms of combining description texts and emotion\npolarities, the aim of the ranking method in this study is to provide the most\nhelpful negative reviews under a certain product attribute for online sellers\nand producers. By applying a more reasonable evaluating procedure, experts with\nrelated backgrounds are hired to vote for the ranking approaches. Our ranking\nmethod turns out to be more reliable for ranking negative reviews for sellers\nand producers, demonstrating a better performance than the baselines like BM25\nwith a result of 8% higher. In this paper, we also enrich the previous\nunderstandings of emotions in valuing reviews. Specifically, it is surprisingly\nfound that positive emotions are more helpful rather than negative emotions in\nranking negative reviews. The unexpected strengthening from positive emotions\nin ranking suggests that less polarized reviews on negative experience in fact\noffer more rational feedbacks and thus more helpfulness to the sellers and\nproducers. The presented ranking method could provide e-commerce practitioners\nwith an efficient and effective way to leverage negative reviews from online\nconsumers.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 03:34:20 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Weng", "Di", ""], ["Zhao", "Jichang", ""]]}, {"id": "2005.09862", "submitter": "Wei Zou", "authors": "Dongwei Jiang, Wubo Li, Ruixiong Zhang, Miao Cao, Ne Luo, Yang Han,\n  Wei Zou, Xiangang Li", "title": "A Further Study of Unsupervised Pre-training for Transformer Based\n  Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Building a good speech recognition system usually requires large amounts of\ntranscribed data, which is expensive to collect. To tackle this problem, many\nunsupervised pre-training methods have been proposed. Among these methods,\nMasked Predictive Coding achieved significant improvements on various speech\nrecognition datasets with BERT-like Masked Reconstruction loss and Transformer\nbackbone. However, many aspects of MPC have not been fully investigated. In\nthis paper, we conduct a further study on MPC and focus on three important\naspects: the effect of pre-training data speaking style, its extension on\nstreaming model, and how to better transfer learned knowledge from pre-training\nstage to downstream tasks. Experiments reveled that pre-training data with\nmatching speaking style is more useful on downstream recognition tasks. A\nunified training objective with APC and MPC provided 8.46% relative error\nreduction on streaming model trained on HKUST. Also, the combination of target\ndata adaption and layer-wise discriminative training helped the knowledge\ntransfer of MPC, which achieved 3.99% relative error reduction on AISHELL over\na strong baseline.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 06:22:29 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 03:57:48 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Jiang", "Dongwei", ""], ["Li", "Wubo", ""], ["Zhang", "Ruixiong", ""], ["Cao", "Miao", ""], ["Luo", "Ne", ""], ["Han", "Yang", ""], ["Zou", "Wei", ""], ["Li", "Xiangang", ""]]}, {"id": "2005.09921", "submitter": "Shota Horiguchi", "authors": "Shota Horiguchi, Yusuke Fujita, Shinji Watanabe, Yawen Xue, Kenji\n  Nagamatsu", "title": "End-to-End Speaker Diarization for an Unknown Number of Speakers with\n  Encoder-Decoder Based Attractors", "comments": "Accepted to INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end speaker diarization for an unknown number of speakers is addressed\nin this paper. Recently proposed end-to-end speaker diarization outperformed\nconventional clustering-based speaker diarization, but it has one drawback: it\nis less flexible in terms of the number of speakers. This paper proposes a\nmethod for encoder-decoder based attractor calculation (EDA), which first\ngenerates a flexible number of attractors from a speech embedding sequence.\nThen, the generated multiple attractors are multiplied by the speech embedding\nsequence to produce the same number of speaker activities. The speech embedding\nsequence is extracted using the conventional self-attentive end-to-end neural\nspeaker diarization (SA-EEND) network. In a two-speaker condition, our method\nachieved a 2.69 % diarization error rate (DER) on simulated mixtures and a 8.07\n% DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained\n4.56 % and 9.54 %, respectively. In unknown numbers of speakers conditions, our\nmethod attained a 15.29 % DER on CALLHOME, while the x-vector-based clustering\nmethod achieved a 19.43 % DER.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 09:08:41 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 10:31:15 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 07:12:42 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Horiguchi", "Shota", ""], ["Fujita", "Yusuke", ""], ["Watanabe", "Shinji", ""], ["Xue", "Yawen", ""], ["Nagamatsu", "Kenji", ""]]}, {"id": "2005.09940", "submitter": "Ngoc Quan Pham", "authors": "Ngoc-Quan Pham, Thanh-Le Ha, Tuan-Nam Nguyen, Thai-Son Nguyen,\n  Elizabeth Salesky, Sebastian Stueker, Jan Niehues and Alexander Waibel", "title": "Relative Positional Encoding for Speech Recognition and Direct\n  Translation", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models are powerful sequence-to-sequence architectures that are\ncapable of directly mapping speech inputs to transcriptions or translations.\nHowever, the mechanism for modeling positions in this model was tailored for\ntext modeling, and thus is less ideal for acoustic inputs. In this work, we\nadapt the relative position encoding scheme to the Speech Transformer, where\nthe key addition is relative distance between input states in the\nself-attention network. As a result, the network can better adapt to the\nvariable distributions present in speech data. Our experiments show that our\nresulting model achieves the best recognition result on the Switchboard\nbenchmark in the non-augmentation condition, and the best published result in\nthe MuST-C speech translation benchmark. We also show that this model is able\nto better utilize synthetic data than the Transformer, and adapts better to\nvariable sentence segmentation quality for speech translation.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 09:53:06 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Pham", "Ngoc-Quan", ""], ["Ha", "Thanh-Le", ""], ["Nguyen", "Tuan-Nam", ""], ["Nguyen", "Thai-Son", ""], ["Salesky", "Elizabeth", ""], ["Stueker", "Sebastian", ""], ["Niehues", "Jan", ""], ["Waibel", "Alexander", ""]]}, {"id": "2005.09946", "submitter": "Pierpaolo Basile", "authors": "Pierluigi Cassotti, Annalina Caputo, Marco Polignano, Pierpaolo Basile", "title": "GM-CTSC at SemEval-2020 Task 1: Gaussian Mixtures Cross Temporal\n  Similarity Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes the system proposed for the SemEval-2020 Task 1:\nUnsupervised Lexical Semantic Change Detection. We focused our approach on the\ndetection problem. Given the semantics of words captured by temporal word\nembeddings in different time periods, we investigate the use of unsupervised\nmethods to detect when the target word has gained or loosed senses. To this\nend, we defined a new algorithm based on Gaussian Mixture Models to cluster the\ntarget similarities computed over the two periods. We compared the proposed\napproach with a number of similarity-based thresholds. We found that, although\nthe performance of the detection methods varies across the word embedding\nalgorithms, the combination of Gaussian Mixture with Temporal Referencing\nresulted in our best system.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 10:14:01 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Cassotti", "Pierluigi", ""], ["Caputo", "Annalina", ""], ["Polignano", "Marco", ""], ["Basile", "Pierpaolo", ""]]}, {"id": "2005.09980", "submitter": "Nils K\\\"obis C", "authors": "Nils K\\\"obis, Luca Mossink", "title": "Artificial Intelligence versus Maya Angelou: Experimental evidence that\n  people cannot differentiate AI-generated from human-written poetry", "comments": "Computers in Human Behavior 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The release of openly available, robust natural language generation\nalgorithms (NLG) has spurred much public attention and debate. One reason lies\nin the algorithms' purported ability to generate human-like text across various\ndomains. Empirical evidence using incentivized tasks to assess whether people\n(a) can distinguish and (b) prefer algorithm-generated versus human-written\ntext is lacking. We conducted two experiments assessing behavioral reactions to\nthe state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal =\n830). Using the identical starting lines of human poems, GPT-2 produced samples\nof poems. From these samples, either a random poem was chosen\n(Human-out-of-the-loop) or the best one was selected (Human-in-the-loop) and in\nturn matched with a human-written poem. In a new incentivized version of the\nTuring Test, participants failed to reliably detect the\nalgorithmically-generated poems in the Human-in-the-loop treatment, yet\nsucceeded in the Human-out-of-the-loop treatment. Further, people reveal a\nslight aversion to algorithm-generated poetry, independent on whether\nparticipants were informed about the algorithmic origin of the poem\n(Transparency) or not (Opacity). We discuss what these results convey about the\nperformance of NLG algorithms to produce human-like text and propose\nmethodologies to study such learning algorithms in human-agent experimental\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:52:28 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 19:31:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["K\u00f6bis", "Nils", ""], ["Mossink", "Luca", ""]]}, {"id": "2005.10043", "submitter": "Wei Li", "authors": "Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du", "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs that capture relations between textual units have great benefits for\ndetecting salient information from multiple documents and generating overall\ncoherent summaries. In this paper, we develop a neural abstractive\nmulti-document summarization (MDS) model which can leverage well-known graph\nrepresentations of documents such as similarity graph and discourse graph, to\nmore effectively process multiple input documents and produce abstractive\nsummaries. Our model utilizes graphs to encode documents in order to capture\ncross-document relations, which is crucial to summarizing long documents. Our\nmodel can also take advantage of graphs to guide the summary generation\nprocess, which is beneficial for generating coherent and concise summaries.\nFurthermore, pre-trained language models can be easily combined with our model,\nwhich further improve the summarization performance significantly. Empirical\nresults on the WikiSum and MultiNews dataset show that the proposed\narchitecture brings substantial improvements over several strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:39:47 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Wei", ""], ["Xiao", "Xinyan", ""], ["Liu", "Jiachen", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""], ["Du", "Junping", ""]]}, {"id": "2005.10048", "submitter": "Magdalena Biesialska", "authors": "Magdalena Biesialska, Bardia Rafieian, Marta R. Costa-juss\\`a", "title": "Enhancing Word Embeddings with Knowledge Extracted from Lexical\n  Resources", "comments": "Accepted to ACL 2020 SRW", "journal-ref": null, "doi": "10.18653/v1/2020.acl-srw.36", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an effective method for semantic specialization of\nword vector representations. To this end, we use traditional word embeddings\nand apply specialization methods to better capture semantic relations between\nwords. In our approach, we leverage external knowledge from rich lexical\nresources such as BabelNet. We also show that our proposed post-specialization\nmethod based on an adversarial neural network with the Wasserstein distance\nallows to gain improvements over state-of-the-art methods on two tasks: word\nsimilarity and dialog state tracking.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:45:49 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Biesialska", "Magdalena", ""], ["Rafieian", "Bardia", ""], ["Costa-juss\u00e0", "Marta R.", ""]]}, {"id": "2005.10049", "submitter": "Wilfried Michel", "authors": "Wilfried Michel and Ralf Schl\\\"uter and Hermann Ney", "title": "Early Stage LM Integration Using Local and Global Log-Linear Combination", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models with an implicit alignment mechanism (e.g.\nattention) are closing the performance gap towards traditional hybrid hidden\nMarkov models (HMM) for the task of automatic speech recognition. One important\nfactor to improve word error rate in both cases is the use of an external\nlanguage model (LM) trained on large text-only corpora. Language model\nintegration is straightforward with the clear separation of acoustic model and\nlanguage model in classical HMM-based modeling. In contrast, multiple\nintegration schemes have been proposed for attention models. In this work, we\npresent a novel method for language model integration into implicit-alignment\nbased sequence-to-sequence models. Log-linear model combination of acoustic and\nlanguage model is performed with a per-token renormalization. This allows us to\ncompute the full normalization term efficiently both in training and in\ntesting. This is compared to a global renormalization scheme which is\nequivalent to applying shallow fusion in training. The proposed methods show\ngood improvements over standard model combination (shallow fusion) on our\nstate-of-the-art Librispeech system. Furthermore, the improvements are\npersistent even if the LM is exchanged for a more powerful one after training.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:49:55 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Michel", "Wilfried", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2005.10058", "submitter": "Sergey A. Slavnov", "authors": "Sergey Slavnov", "title": "On embedding Lambek calculus into commutative categorial grammars", "comments": "Much better presentation in the new version. Added pictures and\n  examples, simplified proofs, improved notation and terminology, computational\n  part reduced almost to zero", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.CL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider tensor grammars, which are an example of \\commutative\" grammars,\nbased on the classical (rather than intuitionistic) linear logic. They can be\nseen as a surface representation of abstract categorial grammars ACG in the\nsense that derivations of ACG translate to derivations of tensor grammars and\nthis translation is isomorphic on the level of string languages. The basic\ningredient are tensor terms, which can be seen as encoding and generalizing\nproof-nets. Using tensor terms makes the syntax extremely simple and a direct\ngeometric meaning becomes transparent. Then we address the problem of encoding\nnoncommutative operations in our setting. This turns out possible after\nenriching the system with new unary operators. The resulting system allows\nrepresenting both ACG and Lambek grammars as conservative fragments, while the\nformalism remains, as it seems to us, rather simple and intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:08:56 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 18:23:42 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 08:55:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Slavnov", "Sergey", ""]]}, {"id": "2005.10070", "submitter": "Demian Gholipour Ghalandari", "authors": "Demian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John\n  Glover, Georgiana Ifrim", "title": "A Large-Scale Multi-Document Summarization Dataset from the Wikipedia\n  Current Events Portal", "comments": "Camera-ready version for ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-document summarization (MDS) aims to compress the content in large\ndocument collections into short summaries and has important applications in\nstory clustering for newsfeeds, presentation of search results, and timeline\ngeneration. However, there is a lack of datasets that realistically address\nsuch use cases at a scale large enough for training supervised models for this\ntask. This work presents a new dataset for MDS that is large both in the total\nnumber of document clusters and in the size of individual clusters. We build\nthis dataset by leveraging the Wikipedia Current Events Portal (WCEP), which\nprovides concise and neutral human-written summaries of news events, with links\nto external source articles. We also automatically extend these source articles\nby looking for related articles in the Common Crawl archive. We provide a\nquantitative analysis of the dataset and empirical results for several\nstate-of-the-art MDS techniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:33:33 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ghalandari", "Demian Gholipour", ""], ["Hokamp", "Chris", ""], ["Pham", "Nghia The", ""], ["Glover", "John", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "2005.10089", "submitter": "Yingbo Gao", "authors": "Jingjing Huo, Yingbo Gao, Weiyue Wang, Ralf Schl\\\"uter, Hermann Ney", "title": "Investigation of Large-Margin Softmax in Neural Language Modeling", "comments": "Proceedings of INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To encourage intra-class compactness and inter-class separability among\ntrainable feature vectors, large-margin softmax methods are developed and\nwidely applied in the face recognition community. The introduction of the\nlarge-margin concept into the softmax is reported to have good properties such\nas enhanced discriminative power, less overfitting and well-defined geometric\nintuitions. Nowadays, language modeling is commonly approached with neural\nnetworks using softmax and cross entropy. In this work, we are curious to see\nif introducing large-margins to neural language models would improve the\nperplexity and consequently word error rate in automatic speech recognition.\nSpecifically, we first implement and test various types of conventional margins\nfollowing the previous works in face recognition. To address the distribution\nof natural language data, we then compare different strategies for word vector\nnorm-scaling. After that, we apply the best norm-scaling setup in combination\nwith various margins and conduct neural language models rescoring experiments\nin automatic speech recognition. We find that although perplexity is slightly\ndeteriorated, neural language models with large-margin softmax can yield word\nerror rate similar to that of the standard softmax baseline. Finally, expected\nmargins are analyzed through visualization of word vectors, showing that the\nsyntactic and semantic relationships are also preserved.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:53:19 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:45:20 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Huo", "Jingjing", ""], ["Gao", "Yingbo", ""], ["Wang", "Weiyue", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2005.10107", "submitter": "Demian Gholipour Ghalandari", "authors": "Demian Gholipour Ghalandari and Georgiana Ifrim", "title": "Examining the State-of-the-Art in News Timeline Summarization", "comments": "Camera-ready version for ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on automatic news timeline summarization (TLS) leaves an\nunclear picture about how this task can generally be approached and how well it\nis currently solved. This is mostly due to the focus on individual subtasks,\nsuch as date selection and date summarization, and to the previous lack of\nappropriate evaluation metrics for the full TLS task. In this paper, we compare\ndifferent TLS strategies using appropriate evaluation frameworks, and propose a\nsimple and effective combination of methods that improves over the\nstate-of-the-art on all tested benchmarks. For a more robust evaluation, we\nalso present a new TLS dataset, which is larger and spans longer time periods\nthan previous datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:06:39 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ghalandari", "Demian Gholipour", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "2005.10113", "submitter": "Linhao Dong", "authors": "Linhao Dong, Cheng Yi, Jianzong Wang, Shiyu Zhou, Shuang Xu, Xueli\n  Jia, Bo Xu", "title": "A Comparison of Label-Synchronous and Frame-Synchronous End-to-End\n  Models for Speech Recognition", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models are gaining wider attention in the field of automatic\nspeech recognition (ASR). One of their advantages is the simplicity of building\nthat directly recognizes the speech frame sequence into the text label sequence\nby neural networks. According to the driving end in the recognition process,\nend-to-end ASR models could be categorized into two types: label-synchronous\nand frame-synchronous, each of which has unique model behaviour and\ncharacteristic. In this work, we make a detailed comparison on a representative\nlabel-synchronous model (transformer) and a soft frame-synchronous model\n(continuous integrate-and-fire (CIF) based model). The results on three public\ndataset and a large-scale dataset with 12000 hours of training data show that\nthe two types of models have respective advantages that are consistent with\ntheir synchronous mode.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:10:35 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 08:56:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dong", "Linhao", ""], ["Yi", "Cheng", ""], ["Wang", "Jianzong", ""], ["Zhou", "Shiyu", ""], ["Xu", "Shuang", ""], ["Jia", "Xueli", ""], ["Xu", "Bo", ""]]}, {"id": "2005.10125", "submitter": "Mariflor Vega-Carrasco", "authors": "Mariflor Vega-Carrasco, Jason O'sullivan, Rosie Prior, Ioanna\n  Manolopoulou, Mirco Musolesi", "title": "Modelling Grocery Retail Topic Distributions: Evaluation,\n  Interpretability and Stability", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the shopping motivations behind market baskets has high\ncommercial value in the grocery retail industry. Analyzing shopping\ntransactions demands techniques that can cope with the volume and\ndimensionality of grocery transactional data while keeping interpretable\noutcomes. Latent Dirichlet Allocation (LDA) provides a suitable framework to\nprocess grocery transactions and to discover a broad representation of\ncustomers' shopping motivations. However, summarizing the posterior\ndistribution of an LDA model is challenging, while individual LDA draws may not\nbe coherent and cannot capture topic uncertainty. Moreover, the evaluation of\nLDA models is dominated by model-fit measures which may not adequately capture\nthe qualitative aspects such as interpretability and stability of topics.\n  In this paper, we introduce clustering methodology that post-processes\nposterior LDA draws to summarise the entire posterior distribution and identify\nsemantic modes represented as recurrent topics. Our approach is an alternative\nto standard label-switching techniques and provides a single posterior summary\nset of topics, as well as associated measures of uncertainty. Furthermore, we\nestablish a more holistic definition for model evaluation, which assesses topic\nmodels based not only on their likelihood but also on their coherence,\ndistinctiveness and stability. By means of a survey, we set thresholds for the\ninterpretation of topic coherence and topic similarity in the domain of grocery\nretail data. We demonstrate that the selection of recurrent topics through our\nclustering methodology not only improves model likelihood but also outperforms\nthe qualitative aspects of LDA such as interpretability and stability. We\nillustrate our methods on an example from a large UK supermarket chain.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:23:36 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:29:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Vega-Carrasco", "Mariflor", ""], ["O'sullivan", "Jason", ""], ["Prior", "Rosie", ""], ["Manolopoulou", "Ioanna", ""], ["Musolesi", "Mirco", ""]]}, {"id": "2005.10200", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen", "title": "BERTweet: A pre-trained language model for English Tweets", "comments": "In Proceedings of EMNLP 2020: System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BERTweet, the first public large-scale pre-trained language model\nfor English Tweets. Our BERTweet, having the same architecture as BERT-base\n(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu\net al., 2019). Experiments show that BERTweet outperforms strong baselines\nRoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better\nperformance results than the previous state-of-the-art models on three Tweet\nNLP tasks: Part-of-speech tagging, Named-entity recognition and text\nclassification. We release BERTweet under the MIT License to facilitate future\nresearch and applications on Tweet data. Our BERTweet is available at\nhttps://github.com/VinAIResearch/BERTweet\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:05:57 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 10:00:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Vu", "Thanh", ""], ["Nguyen", "Anh Tuan", ""]]}, {"id": "2005.10213", "submitter": "Shijie Wu", "authors": "Shijie Wu, Ryan Cotterell, Mans Hulden", "title": "Applying the Transformer to Character-level Transduction", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transformer has been shown to outperform recurrent neural network-based\nsequence-to-sequence models in various word-level NLP tasks. Yet for\ncharacter-level transduction tasks, e.g. morphological inflection generation\nand historical text normalization, there are few works that outperform\nrecurrent models using the transformer. In an empirical study, we uncover that,\nin contrast to recurrent sequence-to-sequence models, the batch size plays a\ncrucial role in the performance of the transformer on character-level tasks,\nand we show that with a large enough batch size, the transformer does indeed\noutperform recurrent models. We also introduce a simple technique to handle\nfeature-guided character-level transduction that further improves performance.\nWith these insights, we achieve state-of-the-art performance on morphological\ninflection and historical text normalization. We also show that the transformer\noutperforms a strong baseline on two other character-level transduction tasks:\ngrapheme-to-phoneme conversion and transliteration.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:25:43 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:59:30 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wu", "Shijie", ""], ["Cotterell", "Ryan", ""], ["Hulden", "Mans", ""]]}, {"id": "2005.10219", "submitter": "Jack Weston", "authors": "Abhishek Shivkumar, Jack Weston, Raphael Lenain, Emil Fristed", "title": "BlaBla: Linguistic Feature Extraction for Clinical Analysis in Multiple\n  Languages", "comments": "5 pages. 1 figure. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BlaBla, an open-source Python library for extracting linguistic\nfeatures with proven clinical relevance to neurological and psychiatric\ndiseases across many languages. BlaBla is a unifying framework for accelerating\nand simplifying clinical linguistic research. The library is built on\nstate-of-the-art NLP frameworks and supports multithreaded/GPU-enabled feature\nextraction via both native Python calls and a command line interface. We\ndescribe BlaBla's architecture and clinical validation of its features across\n12 diseases. We further demonstrate the application of BlaBla to a task\nvisualizing and classifying language disorders in three languages on real\nclinical data from the AphasiaBank dataset. We make the codebase freely\navailable to researchers with the hope of providing a consistent,\nwell-validated foundation for the next generation of clinical linguistic\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:31:35 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Shivkumar", "Abhishek", ""], ["Weston", "Jack", ""], ["Lenain", "Raphael", ""], ["Fristed", "Emil", ""]]}, {"id": "2005.10232", "submitter": "Anil Ramakrishna", "authors": "Anil Ramakrishna, Shrikanth Narayanan", "title": "Sentence level estimation of psycholinguistic norms using joint\n  multidimensional annotations", "comments": "5 pages, 4 figures, submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psycholinguistic normatives represent various affective and mental constructs\nusing numeric scores and are used in a variety of applications in natural\nlanguage processing. They are commonly used at the sentence level, the scores\nof which are estimated by extrapolating word level scores using simple\naggregation strategies, which may not always be optimal. In this work, we\npresent a novel approach to estimate the psycholinguistic norms at sentence\nlevel. We apply a multidimensional annotation fusion model on annotations at\nthe word level to estimate a parameter which captures relationships between\ndifferent norms. We then use this parameter at sentence level to estimate the\nnorms. We evaluate our approach by predicting sentence level scores for various\nnormative dimensions and compare with standard word aggregation schemes.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:47:56 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ramakrishna", "Anil", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2005.10283", "submitter": "Bryan Eikema", "authors": "Bryan Eikema and Wilker Aziz", "title": "Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural\n  Machine Translation", "comments": "COLING 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have revealed a number of pathologies of neural machine\ntranslation (NMT) systems. Hypotheses explaining these mostly suggest there is\nsomething fundamentally wrong with NMT as a model or its training algorithm,\nmaximum likelihood estimation (MLE). Most of this evidence was gathered using\nmaximum a posteriori (MAP) decoding, a decision rule aimed at identifying the\nhighest-scoring translation, i.e. the mode. We argue that the evidence\ncorroborates the inadequacy of MAP decoding more than casts doubt on the model\nand its training algorithm. In this work, we show that translation\ndistributions do reproduce various statistics of the data well, but that beam\nsearch strays from such statistics. We show that some of the known pathologies\nand biases of NMT are due to MAP decoding and not to NMT's statistical\nassumptions nor MLE. In particular, we show that the most likely translations\nunder the model accumulate so little probability mass that the mode can be\nconsidered essentially arbitrary. We therefore advocate for the use of decision\nrules that take into account the translation distribution holistically. We show\nthat an approximation to minimum Bayes risk decoding gives competitive results\nconfirming that NMT models do capture important aspects of translation well in\nexpectation.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:05:51 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 11:29:52 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Eikema", "Bryan", ""], ["Aziz", "Wilker", ""]]}, {"id": "2005.10331", "submitter": "Yutao Zhu", "authors": "Yutao Zhu, Ruihua Song, Zhicheng Dou, Jian-Yun Nie, Jin Zhou", "title": "ScriptWriter: Narrative-Guided Script Generation", "comments": "ACL 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is appealing to have a system that generates a story or scripts\nautomatically from a story-line, even though this is still out of our reach. In\ndialogue systems, it would also be useful to drive dialogues by a dialogue\nplan. In this paper, we address a key problem involved in these applications --\nguiding a dialogue by a narrative. The proposed model ScriptWriter selects the\nbest response among the candidates that fit the context as well as the given\nnarrative. It keeps track of what in the narrative has been said and what is to\nbe said. A narrative plays a different role than the context (i.e., previous\nutterances), which is generally used in current dialogue systems. Due to the\nunavailability of data for this new application, we construct a new large-scale\ndata collection GraphMovie from a movie website where end-users can upload\ntheir narratives freely when watching a movie. Experimental results on the\ndataset show that our proposed approach based on narratives significantly\noutperforms the baselines that simply use the narrative as a kind of context.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:48:50 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 15:49:40 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhu", "Yutao", ""], ["Song", "Ruihua", ""], ["Dou", "Zhicheng", ""], ["Nie", "Jian-Yun", ""], ["Zhou", "Jin", ""]]}, {"id": "2005.10389", "submitter": "Dan Iter", "authors": "Dan Iter, Kelvin Guu, Larry Lansing, Dan Jurafsky", "title": "Pretraining with Contrastive Sentence Objectives Improves Discourse\n  Performance of Language Models", "comments": "AC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent models for unsupervised representation learning of text have employed\na number of techniques to improve contextual word representations but have put\nlittle focus on discourse-level representations. We propose CONPONO, an\ninter-sentence objective for pretraining language models that models discourse\ncoherence and the distance between sentences. Given an anchor sentence, our\nmodel is trained to predict the text k sentences away using a sampled-softmax\nobjective where the candidates consist of neighboring sentences and sentences\nrandomly sampled from the corpus. On the discourse representation benchmark\nDiscoEval, our model improves over the previous state-of-the-art by up to 13%\nand on average 4% absolute across 7 tasks. Our model is the same size as\nBERT-Base, but outperforms the much larger BERT- Large model and other more\nrecent approaches that incorporate discourse. We also show that CONPONO yields\ngains of 2%-6% absolute even for tasks that do not explicitly evaluate\ndiscourse: textual entailment (RTE), common sense reasoning (COPA) and reading\ncomprehension (ReCoRD).\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 23:21:43 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Iter", "Dan", ""], ["Guu", "Kelvin", ""], ["Lansing", "Larry", ""], ["Jurafsky", "Dan", ""]]}, {"id": "2005.10390", "submitter": "Yusuke Yasuda", "authors": "Yusuke Yasuda, Xin Wang, Junichi Yamagishi", "title": "Investigation of learning abilities on linguistic features in\n  sequence-to-sequence text-to-speech synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence-to-sequence text-to-speech synthesis (TTS) can produce\nhigh-quality speech directly from text or simple linguistic features such as\nphonemes. Unlike traditional pipeline TTS, the neural sequence-to-sequence TTS\ndoes not require manually annotated and complicated linguistic features such as\npart-of-speech tags and syntactic structures for system training. However, it\nmust be carefully designed and well optimized so that it can implicitly extract\nuseful linguistic features from the input features. In this paper we\ninvestigate under what conditions the neural sequence-to-sequence TTS can work\nwell in Japanese and English along with comparisons with deep neural network\n(DNN) based pipeline TTS systems. Unlike past comparative studies, the pipeline\nsystems also use autoregressive probabilistic modeling and a neural vocoder. We\ninvestigated systems from three aspects: a) model architecture, b) model\nparameter size, and c) language. For the model architecture aspect, we adopt\nmodified Tacotron systems that we previously proposed and their variants using\nan encoder from Tacotron or Tacotron2. For the model parameter size aspect, we\ninvestigate two model parameter sizes. For the language aspect, we conduct\nlistening tests in both Japanese and English to see if our findings can be\ngeneralized across languages. Our experiments suggest that a) a neural\nsequence-to-sequence TTS system should have a sufficient number of model\nparameters to produce high quality speech, b) it should also use a powerful\nencoder when it takes characters as inputs, and c) the encoder still has a room\nfor improvement and needs to have an improved architecture to learn\nsupra-segmental features more appropriately.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 23:26:14 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 04:18:56 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yasuda", "Yusuke", ""], ["Wang", "Xin", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "2005.10406", "submitter": "Andrew Hard", "authors": "Andrew Hard, Kurt Partridge, Cameron Nguyen, Niranjan Subrahmanya,\n  Aishanee Shah, Pai Zhu, Ignacio Lopez Moreno, Rajiv Mathews", "title": "Training Keyword Spotting Models on Non-IID Data with Federated Learning", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate that a production-quality keyword-spotting model can be\ntrained on-device using federated learning and achieve comparable false accept\nand false reject rates to a centrally-trained model. To overcome the\nalgorithmic constraints associated with fitting on-device data (which are\ninherently non-independent and identically distributed), we conduct thorough\nempirical studies of optimization algorithms and hyperparameter configurations\nusing large-scale federated simulations. To overcome resource constraints, we\nreplace memory intensive MTR data augmentation with SpecAugment, which reduces\nthe false reject rate by 56%. Finally, to label examples (given the zero\nvisibility into on-device data), we explore teacher-student training.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 00:53:33 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:52:52 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Hard", "Andrew", ""], ["Partridge", "Kurt", ""], ["Nguyen", "Cameron", ""], ["Subrahmanya", "Niranjan", ""], ["Shah", "Aishanee", ""], ["Zhu", "Pai", ""], ["Moreno", "Ignacio Lopez", ""], ["Mathews", "Rajiv", ""]]}, {"id": "2005.10410", "submitter": "Jude Khouja", "authors": "Jude Khouja", "title": "Stance Prediction and Claim Verification: An Arabic Perspective", "comments": "To be presented at FEVER workshop at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the application of textual entailment in news claim\nverification and stance prediction using a new corpus in Arabic. The publicly\navailable corpus comes in two perspectives: a version consisting of 4,547 true\nand false claims and a version consisting of 3,786 pairs (claim, evidence). We\ndescribe the methodology for creating the corpus and the annotation process.\nUsing the introduced corpus, we also develop two machine learning baselines for\ntwo proposed tasks: claim verification and stance prediction. Our best model\nutilizes pretraining (BERT) and achieves 76.7 F1 on the stance prediction task\nand 64.3 F1 on the claim verification task. Our preliminary experiments shed\nsome light on the limits of automatic claim verification that relies on claims\ntext only. Results hint that while the linguistic features and world knowledge\nlearned during pretraining are useful for stance prediction, such learned\nrepresentations from pretraining are insufficient for verifying claims without\naccess to context or evidence.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 01:17:46 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Khouja", "Jude", ""]]}, {"id": "2005.10416", "submitter": "Abdelrahman Abdallah", "authors": "Abdelrahman Abdallah, Mahmoud Kasem, Mohamed Hamada, and Shaymaa Sdeek", "title": "Automated Question Answer medical model based on Deep Learning\n  Technology", "comments": null, "journal-ref": "ICEMIS'20: Proceedings of the 6th International Conference on\n  Engineering & MIS 2020", "doi": "10.1145/3410352.3410744", "report-no": "13", "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence can now provide more solutions for different\nproblems, especially in the medical field. One of those problems the lack of\nanswers to any given medical/health-related question. The Internet is full of\nforums that allow people to ask some specific questions and get great answers\nfor them. Nevertheless, browsing these questions in order to locate one similar\nto your own, also finding a satisfactory answer is a difficult and\ntime-consuming task. This research will introduce a solution to this problem by\nautomating the process of generating qualified answers to these questions and\ncreating a kind of digital doctor. Furthermore, this research will train an\nend-to-end model using the framework of RNN and the encoder-decoder to generate\nsensible and useful answers to a small set of medical/health issues. The\nproposed model was trained and evaluated using data from various online\nservices, such as WebMD, HealthTap, eHealthForums, and iCliniq.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 01:40:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Abdallah", "Abdelrahman", ""], ["Kasem", "Mahmoud", ""], ["Hamada", "Mohamed", ""], ["Sdeek", "Shaymaa", ""]]}, {"id": "2005.10433", "submitter": "Mihir Kale", "authors": "Mihir Kale, Abhinav Rastogi", "title": "Text-to-Text Pre-Training for Data-to-Text Tasks", "comments": "Accepted to INLG-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the pre-train + fine-tune strategy for data-to-text tasks. Our\nexperiments indicate that text-to-text pre-training in the form of T5, enables\nsimple, end-to-end transformer based models to outperform pipelined neural\narchitectures tailored for data-to-text generation, as well as alternative\nlanguage model based pre-training techniques such as BERT and GPT-2.\nImportantly, T5 pre-training leads to better generalization, as evidenced by\nlarge improvements on out-of-domain test sets. We hope our work serves as a\nuseful baseline for future research, as transfer learning becomes ever more\nprevalent for data-to-text tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:46:15 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 00:29:30 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 00:42:32 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kale", "Mihir", ""], ["Rastogi", "Abhinav", ""]]}, {"id": "2005.10450", "submitter": "Feng Ji", "authors": "Shuke Peng, Feng Ji, Zehao Lin, Shaobo Cui, Haiqing Chen, Yin Zhang", "title": "MTSS: Learn from Multiple Domain Teachers and Become a Multi-domain\n  Dialogue Expert", "comments": "AAAI 2020, Spotlight Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How to build a high-quality multi-domain dialogue system is a challenging\nwork due to its complicated and entangled dialogue state space among each\ndomain, which seriously limits the quality of dialogue policy, and further\naffects the generated response. In this paper, we propose a novel method to\nacquire a satisfying policy and subtly circumvent the knotty dialogue state\nrepresentation problem in the multi-domain setting. Inspired by real school\nteaching scenarios, our method is composed of multiple domain-specific teachers\nand a universal student. Each individual teacher only focuses on one specific\ndomain and learns its corresponding domain knowledge and dialogue policy based\non a precisely extracted single domain dialogue state representation. Then,\nthese domain-specific teachers impart their domain knowledge and policies to a\nuniversal student model and collectively make this student model a multi-domain\ndialogue expert. Experiment results show that our method reaches competitive\nresults with SOTAs in both multi-domain and single domain setting.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 03:40:02 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Peng", "Shuke", ""], ["Ji", "Feng", ""], ["Lin", "Zehao", ""], ["Cui", "Shaobo", ""], ["Chen", "Haiqing", ""], ["Zhang", "Yin", ""]]}, {"id": "2005.10454", "submitter": "Curtis Murray", "authors": "Curtis Murray, Lewis Mitchell, Jonathan Tuke, Mark Mackay", "title": "Symptom extraction from the narratives of personal experiences with\n  COVID-19 on Reddit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media discussion of COVID-19 provides a rich source of information\ninto how the virus affects people's lives that is qualitatively different from\ntraditional public health datasets. In particular, when individuals self-report\ntheir experiences over the course of the virus on social media, it can allow\nfor identification of the emotions each stage of symptoms engenders in the\npatient. Posts to the Reddit forum r/COVID19Positive contain first-hand\naccounts from COVID-19 positive patients, giving insight into personal\nstruggles with the virus. These posts often feature a temporal structure\nindicating the number of days after developing symptoms the text refers to.\nUsing topic modelling and sentiment analysis, we quantify the change in\ndiscussion of COVID-19 throughout individuals' experiences for the first 14\ndays since symptom onset. Discourse on early symptoms such as fever, cough, and\nsore throat was concentrated towards the beginning of the posts, while language\nindicating breathing issues peaked around ten days. Some conversation around\ncritical cases was also identified and appeared at a roughly constant rate. We\nidentified two clear clusters of positive and negative emotions associated with\nthe evolution of these symptoms and mapped their relationships. Our results\nprovide a perspective on the patient experience of COVID-19 that complements\nother medical data streams and can potentially reveal when mental health issues\nmight appear.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 03:54:51 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Murray", "Curtis", ""], ["Mitchell", "Lewis", ""], ["Tuke", "Jonathan", ""], ["Mackay", "Mark", ""]]}, {"id": "2005.10463", "submitter": "Haoneng Luo", "authors": "Haoneng Luo, Shiliang Zhang, Ming Lei, Lei Xie", "title": "Simplified Self-Attention for Transformer-based End-to-End Speech\n  Recognition", "comments": "Accepted to SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have been introduced into end-to-end speech recognition\nwith state-of-the-art performance on various tasks owing to their superiority\nin modeling long-term dependencies. However, such improvements are usually\nobtained through the use of very large neural networks. Transformer models\nmainly include two submodules - position-wise feedforward layers and\nself-attention (SAN) layers. In this paper, to reduce the model complexity\nwhile maintaining good performance, we propose a simplified self-attention\n(SSAN) layer which employs FSMN memory block instead of projection layers to\nform query and key vectors for transformer-based end-to-end speech recognition.\nWe evaluate the SSAN-based and the conventional SAN-based transformers on the\npublic AISHELL-1, internal 1000-hour and 20,000-hour large-scale Mandarin\ntasks. Results show that our proposed SSAN-based transformer model can achieve\nover 20% relative reduction in model parameters and 6.7% relative CER reduction\non the AISHELL-1 task. With impressively 20% parameter reduction, our model\nshows no loss of recognition performance on the 20,000-hour large-scale task.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 04:55:59 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 09:58:44 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Luo", "Haoneng", ""], ["Zhang", "Shiliang", ""], ["Lei", "Ming", ""], ["Xie", "Lei", ""]]}, {"id": "2005.10464", "submitter": "Ashutosh Baheti", "authors": "Ashutosh Baheti, Alan Ritter, Kevin Small", "title": "Fluent Response Generation for Conversational Question Answering", "comments": "2020 Annual Conference of the Association for Computational\n  Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) is an important aspect of open-domain conversational\nagents, garnering specific research focus in the conversational QA (ConvQA)\nsubtask. One notable limitation of recent ConvQA efforts is the response being\nanswer span extraction from the target corpus, thus ignoring the natural\nlanguage generation (NLG) aspect of high-quality conversational agents. In this\nwork, we propose a method for situating QA responses within a SEQ2SEQ NLG\napproach to generate fluent grammatical answer responses while maintaining\ncorrectness. From a technical perspective, we use data augmentation to generate\ntraining data for an end-to-end system. Specifically, we develop Syntactic\nTransformations (STs) to produce question-specific candidate answer responses\nand rank them using a BERT-based classifier (Devlin et al., 2019). Human\nevaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the\nproposed model outperforms baseline CoQA and QuAC models in generating\nconversational responses. We further show our model's scalability by conducting\ntests on the CoQA dataset. The code and data are available at\nhttps://github.com/abaheti95/QADialogSystem.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 04:57:01 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:56:09 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Baheti", "Ashutosh", ""], ["Ritter", "Alan", ""], ["Small", "Kevin", ""]]}, {"id": "2005.10469", "submitter": "Kyu Han", "authors": "Jing Pan, Joshua Shapiro, Jeremy Wohlwend, Kyu J. Han, Tao Lei and Tao\n  Ma", "title": "ASAPP-ASR: Multistream CNN and Self-Attentive SRU for SOTA Speech\n  Recognition", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present state-of-the-art (SOTA) performance on the\nLibriSpeech corpus with two novel neural network architectures, a multistream\nCNN for acoustic modeling and a self-attentive simple recurrent unit (SRU) for\nlanguage modeling. In the hybrid ASR framework, the multistream CNN acoustic\nmodel processes an input of speech frames in multiple parallel pipelines where\neach stream has a unique dilation rate for diversity. Trained with the\nSpecAugment data augmentation method, it achieves relative word error rate\n(WER) improvements of 4% on test-clean and 14% on test-other. We further\nimprove the performance via N-best rescoring using a 24-layer self-attentive\nSRU language model, achieving WERs of 1.75% on test-clean and 4.46% on\ntest-other.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 05:18:34 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Pan", "Jing", ""], ["Shapiro", "Joshua", ""], ["Wohlwend", "Jeremy", ""], ["Han", "Kyu J.", ""], ["Lei", "Tao", ""], ["Ma", "Tao", ""]]}, {"id": "2005.10470", "submitter": "Kyu Han", "authors": "Kyu J. Han, Jing Pan, Venkata Krishna Naveen Tadala, Tao Ma and Dan\n  Povey", "title": "Multistream CNN for Robust Acoustic Modeling", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes multistream CNN, a novel neural network architecture for\nrobust acoustic modeling in speech recognition tasks. The proposed architecture\nprocesses input speech with diverse temporal resolutions by applying different\ndilation rates to convolutional neural networks across multiple streams to\nachieve the robustness. The dilation rates are selected from the multiples of a\nsub-sampling rate of 3 frames. Each stream stacks TDNN-F layers (a variant of\n1D CNN), and output embedding vectors from the streams are concatenated then\nprojected to the final layer. We validate the effectiveness of the proposed\nmultistream CNN architecture by showing consistent improvements against Kaldi's\nbest TDNN-F model across various data sets. Multistream CNN improves the WER of\nthe test-other set in the LibriSpeech corpus by 12% (relative). On custom data\nfrom ASAPP's production ASR system for a contact center, it records a relative\nWER improvement of 11% for customer channel audio to prove its robustness to\ndata in the wild. In terms of real-time factor, multistream CNN outperforms the\nbaseline TDNN-F by 15%, which also suggests its practicality on production\nsystems. When combined with self-attentive SRU LM rescoring, multistream CNN\ncontributes for ASAPP to achieve the best WER of 1.75% on test-clean in\nLibriSpeech.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 05:26:15 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 05:47:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Han", "Kyu J.", ""], ["Pan", "Jing", ""], ["Tadala", "Venkata Krishna Naveen", ""], ["Ma", "Tao", ""], ["Povey", "Dan", ""]]}, {"id": "2005.10527", "submitter": "Maria Pia Di Buono", "authors": "Carola Carlino, Gennaro Nolano, Maria Pia di Buono, Johanna Monti", "title": "LaCulturaNonSiFerma -- Report su uso e la diffusione degli hashtag delle\n  istituzioni culturali italiane durante il periodo di lockdown", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report presents an analysis of #hashtags used by Italian Cultural\nHeritage institutions to promote and communicate cultural content during the\nCOVID-19 lock-down period in Italy. Several activities to support and engage\nusers' have been proposed using social media. Most of these activities present\none or more #hashtags which help to aggregate content and create a community on\nspecific topics. Results show that on one side Italian institutions have been\nvery proactive in adapting to the pandemic scenario and on the other side\nusers' reacted very positively increasing their participation in the proposed\nactivities.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 09:03:03 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Carlino", "Carola", ""], ["Nolano", "Gennaro", ""], ["di Buono", "Maria Pia", ""], ["Monti", "Johanna", ""]]}, {"id": "2005.10583", "submitter": "Lifeng Han", "authors": "Lifeng Han, Gareth J.F. Jones and Alan F. Smeaton", "title": "MultiMWE: Building a Multi-lingual Multi-Word Expression (MWE) Parallel\n  Corpora", "comments": "Accepted to LREC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-word expressions (MWEs) are a hot topic in research in natural language\nprocessing (NLP), including topics such as MWE detection, MWE decomposition,\nand research investigating the exploitation of MWEs in other NLP fields such as\nMachine Translation. However, the availability of bilingual or multi-lingual\nMWE corpora is very limited. The only bilingual MWE corpora that we are aware\nof is from the PARSEME (PARSing and Multi-word Expressions) EU Project. This is\na small collection of only 871 pairs of English-German MWEs. In this paper, we\npresent multi-lingual and bilingual MWE corpora that we have extracted from\nroot parallel corpora. Our collections are 3,159,226 and 143,042 bilingual MWE\npairs for German-English and Chinese-English respectively after filtering. We\nexamine the quality of these extracted bilingual MWEs in MT experiments. Our\ninitial experiments applying MWEs in MT show improved translation performances\non MWE terms in qualitative analysis and better general evaluation scores in\nquantitative analysis, on both German-English and Chinese-English language\npairs. We follow a standard experimental pipeline to create our MultiMWE\ncorpora which are available online. Researchers can use this free corpus for\ntheir own models or use them in a knowledge base as model features.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 11:46:44 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Han", "Lifeng", ""], ["Jones", "Gareth J. F.", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2005.10608", "submitter": "Marina Fomicheva", "authors": "Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr\\'ed\\'eric Blain,\n  Francisco Guzm\\'an, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, Lucia\n  Specia", "title": "Unsupervised Quality Estimation for Neural Machine Translation", "comments": "Accepted for publication in TACL. Authors' final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality Estimation (QE) is an important component in making Machine\nTranslation (MT) useful in real-world applications, as it is aimed to inform\nthe user on the quality of the MT output at test time. Existing approaches\nrequire large amounts of expert annotated data, computation and time for\ntraining. As an alternative, we devise an unsupervised approach to QE where no\ntraining or access to additional resources besides the MT system itself is\nrequired. Different from most of the current work that treats the MT system as\na black box, we explore useful information that can be extracted from the MT\nsystem as a by-product of translation. By employing methods for uncertainty\nquantification, we achieve very good correlation with human judgments of\nquality, rivalling state-of-the-art supervised QE models. To evaluate our\napproach we collect the first dataset that enables work on both black-box and\nglass-box approaches to QE.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 12:38:06 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 08:37:22 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fomicheva", "Marina", ""], ["Sun", "Shuo", ""], ["Yankovskaya", "Lisa", ""], ["Blain", "Fr\u00e9d\u00e9ric", ""], ["Guzm\u00e1n", "Francisco", ""], ["Fishel", "Mark", ""], ["Aletras", "Nikolaos", ""], ["Chaudhary", "Vishrav", ""], ["Specia", "Lucia", ""]]}, {"id": "2005.10627", "submitter": "Zhaofeng Wu", "authors": "Zhaofeng Wu, Ding Zhao, Qiao Liang, Jiahui Yu, Anmol Gulati, Ruoming\n  Pang", "title": "Dynamic Sparsity Neural Networks for Automatic Speech Recognition", "comments": "ICASSP 2021. (c) 2021 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses, in any\n  current or future media, including reprinting/republishing this material for\n  advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic speech recognition (ASR), model pruning is a widely adopted\ntechnique that reduces model size and latency to deploy neural network models\non edge devices with resource constraints. However, multiple models with\ndifferent sparsity levels usually need to be separately trained and deployed to\nheterogeneous target hardware with different resource specifications and for\napplications that have various latency requirements. In this paper, we present\nDynamic Sparsity Neural Networks (DSNN) that, once trained, can instantly\nswitch to any predefined sparsity configuration at run-time. We demonstrate the\neffectiveness and flexibility of DSNN using experiments on internal production\ndatasets with Google Voice Search data, and show that the performance of a DSNN\nmodel is on par with that of individually trained single sparsity networks. Our\ntrained DSNN model, therefore, can greatly ease the training process and\nsimplify deployment in diverse scenarios with resource constraints.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 22:08:54 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 20:58:33 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 08:01:58 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wu", "Zhaofeng", ""], ["Zhao", "Ding", ""], ["Liang", "Qiao", ""], ["Yu", "Jiahui", ""], ["Gulati", "Anmol", ""], ["Pang", "Ruoming", ""]]}, {"id": "2005.10629", "submitter": "Elie Azeraf", "authors": "Elie Azeraf, Emmanuel Monfrini, Emmanuel Vignon, Wojciech Pieczynski", "title": "Hidden Markov Chains, Entropic Forward-Backward, and Part-Of-Speech\n  Tagging", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to take into account the characteristics - also called features -\nof observations is essential in Natural Language Processing (NLP) problems.\nHidden Markov Chain (HMC) model associated with classic Forward-Backward\nprobabilities cannot handle arbitrary features like prefixes or suffixes of any\nsize, except with an independence condition. For twenty years, this default has\nencouraged the development of other sequential models, starting with the\nMaximum Entropy Markov Model (MEMM), which elegantly integrates arbitrary\nfeatures. More generally, it led to neglect HMC for NLP. In this paper, we show\nthat the problem is not due to HMC itself, but to the way its restoration\nalgorithms are computed. We present a new way of computing HMC based\nrestorations using original Entropic Forward and Entropic Backward (EFB)\nprobabilities. Our method allows taking into account features in the HMC\nframework in the same way as in the MEMM framework. We illustrate the\nefficiency of HMC using EFB in Part-Of-Speech Tagging, showing its superiority\nover MEMM based restoration. We also specify, as a perspective, how HMCs with\nEFB might appear as an alternative to Recurrent Neural Networks to treat\nsequential data with a deep architecture.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 13:31:11 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Azeraf", "Elie", ""], ["Monfrini", "Emmanuel", ""], ["Vignon", "Emmanuel", ""], ["Pieczynski", "Wojciech", ""]]}, {"id": "2005.10636", "submitter": "Michihiro Yasunaga", "authors": "Michihiro Yasunaga, Percy Liang", "title": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback", "comments": "ICML 2020. Code & data available at\n  https://github.com/michiyasunaga/DrRepair", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to repair programs from diagnostic\nfeedback (e.g., compiler error messages). Program repair is challenging for two\nreasons: First, it requires reasoning and tracking symbols across source code\nand diagnostic feedback. Second, labeled datasets available for program repair\nare relatively small. In this work, we propose novel solutions to these two\nchallenges. First, we introduce a program-feedback graph, which connects\nsymbols relevant to program repair in source code and diagnostic feedback, and\nthen apply a graph neural network on top to model the reasoning process.\nSecond, we present a self-supervised learning paradigm for program repair that\nleverages unlabeled programs available online to create a large amount of extra\nprogram repair examples, which we use to pre-train our models. We evaluate our\nproposed approach on two applications: correcting introductory programming\nassignments (DeepFix dataset) and correcting the outputs of program synthesis\n(SPoC dataset). Our final system, DrRepair, significantly outperforms prior\nwork, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best),\nand 48.4% synthesis success rate on SPoC (+3.7% over the prior best).\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 07:24:28 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:30:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Yasunaga", "Michihiro", ""], ["Liang", "Percy", ""]]}, {"id": "2005.10652", "submitter": "Sina Ahmadi", "authors": "Sina Ahmadi, Hossein Hassani", "title": "Towards Finite-State Morphology of Kurdish", "comments": "Manuscript submitted to ACM-TALLIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Morphological analysis is the study of the formation and structure of words.\nIt plays a crucial role in various tasks in Natural Language Processing (NLP)\nand Computational Linguistics (CL) such as machine translation and text and\nspeech generation. Kurdish is a less-resourced multi-dialect Indo-European\nlanguage with highly inflectional morphology. In this paper, as the first\nattempt of its kind, the morphology of the Kurdish language (Sorani dialect) is\ndescribed from a computational point of view. We extract morphological rules\nwhich are transformed into finite-state transducers for generating and\nanalyzing words. The result of this research assists in conducting studies on\nlanguage generation for Kurdish and enhances the Information Retrieval (IR)\ncapacity for the language while leveraging the Kurdish NLP and CL into a more\nadvanced computational level.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 13:55:07 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Ahmadi", "Sina", ""], ["Hassani", "Hossein", ""]]}, {"id": "2005.10659", "submitter": "Pavel Braslavski", "authors": "Vladislav Korablinov and Pavel Braslavski", "title": "RuBQ: A Russian Dataset for Question Answering over Wikidata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper presents RuBQ, the first Russian knowledge base question answering\n(KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of\nvarying complexity, their English machine translations, SPARQL queries to\nWikidata, reference answers, as well as a Wikidata sample of triples containing\nentities with Russian labels. The dataset creation started with a large\ncollection of question-answer pairs from online quizzes. The data underwent\nautomatic filtering, crowd-assisted entity linking, automatic generation of\nSPARQL queries, and their subsequent in-house verification.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 14:06:15 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Korablinov", "Vladislav", ""], ["Braslavski", "Pavel", ""]]}, {"id": "2005.10678", "submitter": "Shun-Po Chuang", "authors": "Shun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, Hung-yi Lee", "title": "Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in\n  Multitask End-to-End Speech Translation", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech translation (ST) aims to learn transformations from speech in the\nsource language to the text in the target language. Previous works show that\nmultitask learning improves the ST performance, in which the recognition\ndecoder generates the text of the source language, and the translation decoder\nobtains the final translations based on the output of the recognition decoder.\nBecause whether the output of the recognition decoder has the correct semantics\nis more critical than its accuracy, we propose to improve the multitask ST\nmodel by utilizing word embedding as the intermediate.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 14:22:35 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 01:04:58 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chuang", "Shun-Po", ""], ["Sung", "Tzu-Wei", ""], ["Liu", "Alexander H.", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2005.10716", "submitter": "Weixin Liang", "authors": "Weixin Liang, James Zou, Zhou Yu", "title": "Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for\n  Automatic Dialog Evaluation", "comments": null, "journal-ref": "ACL 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Domain dialog system evaluation is one of the most important challenges\nin dialog research. Existing automatic evaluation metrics, such as BLEU are\nmostly reference-based. They calculate the difference between the generated\nresponse and a limited number of available references. Likert-score based\nself-reported user rating is widely adopted by social conversational systems,\nsuch as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\nfrom bias and variance among different users. To alleviate this problem, we\nformulate dialog evaluation as a comparison task. We also propose an automatic\nevaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\nautomatically cleans self-reported user ratings as it trains on them.\nSpecifically, we first use a self-supervised method to learn better dialog\nfeature representation, and then use KNN and Shapley to remove confusing\nsamples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\ncomparison task.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:14:49 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 04:05:58 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Liang", "Weixin", ""], ["Zou", "James", ""], ["Yu", "Zhou", ""]]}, {"id": "2005.10786", "submitter": "Evgeniy Shishkin", "authors": "Evgeny Shishkin and Evgeny Kislitsyn", "title": "SafeComp: Protocol For Certifying Cloud Computations Integrity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a problem of certifying computation integrity performed by some\nremote party we do not necessarily trust. We present a multi-party interactive\nprotocol called SafeComp that solves this problem under specified constraints.\nComparing to the nearest related work, our protocol reduces a proof\nconstruction complexity from $O(n \\log{n})$ to $O(n)$, turning a communication\ncomplexity to exactly one round using a certificate of a comparable length.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:08:39 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 12:18:38 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Shishkin", "Evgeny", ""], ["Kislitsyn", "Evgeny", ""]]}, {"id": "2005.10790", "submitter": "Alexander Mehler", "authors": "Alexander Mehler, Bernhard Jussen, Tim Geelhaar, Alexander Henlein,\n  Giuseppe Abrami, Daniel Baumartz, Tolga Uslu, Wahed Hemati", "title": "The Frankfurt Latin Lexicon: From Morphological Expansion and Word\n  Embeddings to SemioGraphs", "comments": "22 pages, 7 figures, 5 tabels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present the Frankfurt Latin Lexicon (FLL), a lexical\nresource for Medieval Latin that is used both for the lemmatization of Latin\ntexts and for the post-editing of lemmatizations. We describe recent advances\nin the development of lemmatizers and test them against the Capitularies corpus\n(comprising Frankish royal edicts, mid-6th to mid-9th century), a corpus\ncreated as a reference for processing Medieval Latin. We also consider the\npost-correction of lemmatizations using a limited crowdsourcing process aimed\nat continuous review and updating of the FLL. Starting from the texts resulting\nfrom this lemmatization process, we describe the extension of the FLL by means\nof word embeddings, whose interactive traversing by means of SemioGraphs\ncompletes the digital enhanced hermeneutic circle. In this way, the article\nargues for a more comprehensive understanding of lemmatization, encompassing\nclassical machine learning as well as intellectual post-corrections and, in\nparticular, human computation in the form of interpretation processes based on\ngraph representations of the underlying lexical resources.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:16:53 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Mehler", "Alexander", ""], ["Jussen", "Bernhard", ""], ["Geelhaar", "Tim", ""], ["Henlein", "Alexander", ""], ["Abrami", "Giuseppe", ""], ["Baumartz", "Daniel", ""], ["Uslu", "Tolga", ""], ["Hemati", "Wahed", ""]]}, {"id": "2005.10865", "submitter": "Benjamin Nye", "authors": "Benjamin E. Nye, Ani Nenkova, Iain J. Marshall, Byron C. Wallace", "title": "Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Trialstreamer, a living database of clinical trial reports. Here\nwe mainly describe the evidence extraction component; this extracts from\nbiomedical abstracts key pieces of information that clinicians need when\nappraising the literature, and also the relations between these. Specifically,\nthe system extracts descriptions of trial participants, the treatments compared\nin each arm (the interventions), and which outcomes were measured. The system\nthen attempts to infer which interventions were reported to work best by\ndetermining their relationship with identified trial outcome measures. In\naddition to summarizing individual trials, these extracted data elements allow\nautomatic synthesis of results across many trials on the same topic. We apply\nthe system at scale to all reports of randomized controlled trials indexed in\nMEDLINE, powering the automatic generation of evidence maps, which provide a\nglobal view of the efficacy of different interventions combining data from all\nrelevant clinical trials on a topic. We make all code and models freely\navailable alongside a demonstration of the web interface.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 19:32:04 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Nye", "Benjamin E.", ""], ["Nenkova", "Ani", ""], ["Marshall", "Iain J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.10893", "submitter": "Ashim Gupta", "authors": "Ashim Gupta, Amrith Krishna, Pawan Goyal, Oliver Hellwig", "title": "Evaluating Neural Morphological Taggers for Sanskrit", "comments": "Accepted to SIGMORPHON Workshop at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural sequence labelling approaches have achieved state of the art results\nin morphological tagging. We evaluate the efficacy of four standard sequence\nlabelling models on Sanskrit, a morphologically rich, fusional Indian language.\nAs its label space can theoretically contain more than 40,000 labels, systems\nthat explicitly model the internal structure of a label are more suited for the\ntask, because of their ability to generalise to labels not seen during\ntraining. We find that although some neural models perform better than others,\none of the common causes for error for all of these models is mispredictions\ndue to syncretism.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:36:32 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Gupta", "Ashim", ""], ["Krishna", "Amrith", ""], ["Goyal", "Pawan", ""], ["Hellwig", "Oliver", ""]]}, {"id": "2005.10899", "submitter": "Diwakar Mahajan", "authors": "Diwakar Mahajan, Jennifer J. Liang, Ching-Huei Tsou", "title": "Extracting Daily Dosage from Medication Instructions in EHRs: An\n  Automated Approach and Lessons Learned", "comments": "10 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a patient's medication history is essential for physicians to\nprovide appropriate treatment recommendations. A medication's prescribed daily\ndosage is a key element of the medication history; however, it is generally not\nprovided as a discrete quantity and needs to be derived from free text\nmedication instructions (Sigs) in the structured electronic health record\n(EHR). Existing works in daily dosage extraction are narrow in scope, dealing\nwith dosage extraction for a single drug from clinical notes. Here, we present\nan automated approach to calculate daily dosage for all medications in EHR\nstructured data. We describe and characterize the variable language used in\nSigs, and present our hybrid system for calculating daily dosage combining deep\nlearning-based named entity extractor with lexicon dictionaries and regular\nexpressions. Our system achieves 0.98 precision and 0.95 recall on an\nexpert-generated dataset of 1000 Sigs, demonstrating its effectiveness on the\ngeneral purpose daily dosage calculation task.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:55:22 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Mahajan", "Diwakar", ""], ["Liang", "Jennifer J.", ""], ["Tsou", "Ching-Huei", ""]]}, {"id": "2005.10915", "submitter": "Sourya Dipta Das", "authors": "Sourya Dipta Das, Soumil Mandal", "title": "Team Neuro at SemEval-2020 Task 8: Multi-Modal Fine Grain Emotion\n  Classification of Memes using Multitask Learning", "comments": "Proceedings of the International Workshop on Semantic Evaluation\n  (SemEval)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe the system that we used for the memotion\nanalysis challenge, which is Task 8 of SemEval-2020. This challenge had three\nsubtasks where affect based sentiment classification of the memes was required\nalong with intensities. The system we proposed combines the three tasks into a\nsingle one by representing it as multi-label hierarchical classification\nproblem.Here,Multi-Task learning or Joint learning Procedure is used to train\nour model.We have used dual channels to extract text and image based features\nfrom separate Deep Neural Network Backbone and aggregate them to create task\nspecific features. These task specific aggregated feature vectors ware then\npassed on to smaller networks with dense layers, each one assigned for\npredicting one type of fine grain sentiment label. Our Proposed method show the\nsuperiority of this system in few tasks to other best models from the\nchallenge.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:29:44 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Mandal", "Soumil", ""]]}, {"id": "2005.11004", "submitter": "Hieu-Thi Luong", "authors": "Hieu-Thi Luong, Junichi Yamagishi", "title": "NAUTILUS: a Versatile Voice Cloning System", "comments": "Submitted to The IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel speech synthesis system, called NAUTILUS, that can\ngenerate speech with a target voice either from a text input or a reference\nutterance of an arbitrary source speaker. By using a multi-speaker speech\ncorpus to train all requisite encoders and decoders in the initial training\nstage, our system can clone unseen voices using untranscribed speech of target\nspeakers on the basis of the backpropagation algorithm. Moreover, depending on\nthe data circumstance of the target speaker, the cloning strategy can be\nadjusted to take advantage of additional data and modify the behaviors of\ntext-to-speech (TTS) and/or voice conversion (VC) systems to accommodate the\nsituation. We test the performance of the proposed framework by using deep\nconvolution layers to model the encoders, decoders and WaveNet vocoder.\nEvaluations show that it achieves comparable quality with state-of-the-art TTS\nand VC systems when cloning with just five minutes of untranscribed speech.\nMoreover, it is demonstrated that the proposed framework has the ability to\nswitch between TTS and VC with high speaker consistency, which will be useful\nfor many applications.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 05:00:20 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 01:12:22 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Luong", "Hieu-Thi", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "2005.11009", "submitter": "Liang Wang", "authors": "Liang Wang, Jinlong Liu, Jingming Liu", "title": "Investigating Label Bias in Beam Search for Open-ended Text Generation", "comments": "10 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beam search is an effective and widely used decoding algorithm in many\nsequence-to-sequence (seq2seq) text generation tasks. However, in open-ended\ntext generation, beam search is often found to produce repetitive and generic\ntexts, sampling-based decoding algorithms like top-k sampling and nucleus\nsampling are more preferred. Standard seq2seq models suffer from label bias due\nto its locally normalized probability formulation. This paper provides a series\nof empirical evidence that label bias is a major reason for such degenerate\nbehaviors of beam search. By combining locally normalized maximum likelihood\nestimation and globally normalized sequence-level training, label bias can be\nreduced with almost no sacrifice in perplexity. To quantitatively measure label\nbias, we test the model's ability to discriminate the groundtruth text and a\nset of context-agnostic distractors. We conduct experiments on large-scale\nresponse generation datasets. Results show that beam search can produce more\ndiverse and meaningful texts with our approach, in terms of both automatic and\nhuman evaluation metrics. Our analysis also suggests several future working\ndirections towards the grand challenge of open-ended text generation.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 05:17:53 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wang", "Liang", ""], ["Liu", "Jinlong", ""], ["Liu", "Jingming", ""]]}, {"id": "2005.11014", "submitter": "Ajay Chatterjee", "authors": "Ajay Chatterjee and Shubhashis Sengupta", "title": "Intent Mining from past conversations for conversational agent", "comments": "8 pages, 2 figures", "journal-ref": "Proceedings of the 28th International Conference on Computational\n  Linguistics, 2020", "doi": null, "report-no": "https://www.aclweb.org/anthology/2020.coling-main.366", "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational systems are of primary interest in the AI community. Chatbots\nare increasingly being deployed to provide round-the-clock support and to\nincrease customer engagement. Many of the commercial bot building frameworks\nfollow a standard approach that requires one to build and train an intent model\nto recognize a user input. Intent models are trained in a supervised setting\nwith a collection of textual utterance and intent label pairs. Gathering a\nsubstantial and wide coverage of training data for different intent is a\nbottleneck in the bot building process. Moreover, the cost of labeling a\nhundred to thousands of conversations with intent is a time consuming and\nlaborious job. In this paper, we present an intent discovery framework that\ninvolves 4 primary steps: Extraction of textual utterances from a conversation\nusing a pre-trained domain agnostic Dialog Act Classifier (Data Extraction),\nautomatic clustering of similar user utterances (Clustering), manual annotation\nof clusters with an intent label (Labeling) and propagation of intent labels to\nthe utterances from the previous step, which are not mapped to any cluster\n(Label Propagation); to generate intent training data from raw conversations.\nWe have introduced a novel density-based clustering algorithm ITER-DBSCAN for\nunbalanced data clustering. Subject Matter Expert (Annotators with domain\nexpertise) manually looks into the clustered user utterances and provides an\nintent label for discovery. We conducted user studies to validate the\neffectiveness of the trained intent model generated in terms of coverage of\nintents, accuracy and time saving concerning manual annotation. Although the\nsystem is developed for building an intent model for the conversational system,\nthis framework can also be used for a short text clustering or as a labeling\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 05:29:13 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 04:45:07 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 07:44:22 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2021 13:45:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chatterjee", "Ajay", ""], ["Sengupta", "Shubhashis", ""]]}, {"id": "2005.11017", "submitter": "Mengxi Wei", "authors": "Mengxi Wei, Yifan He, Qiong Zhang", "title": "Robust Layout-aware IE for Visually Rich Documents with Pre-trained\n  Language Models", "comments": "10 pages, to appear in SIGIR 2020 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many business documents processed in modern NLP and IR pipelines are visually\nrich: in addition to text, their semantics can also be captured by visual\ntraits such as layout, format, and fonts. We study the problem of information\nextraction from visually rich documents (VRDs) and present a model that\ncombines the power of large pre-trained language models and graph neural\nnetworks to efficiently encode both textual and visual information in business\ndocuments. We further introduce new fine-tuning objectives to improve in-domain\nunsupervised fine-tuning to better utilize large amount of unlabeled in-domain\ndata. We experiment on real world invoice and resume data sets and show that\nthe proposed method outperforms strong text-based RoBERTa baselines by 6.3%\nabsolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a\nfew-shot setting, our method requires up to 30x less annotation data than the\nbaseline to achieve the same level of performance at ~90% F1.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:04:50 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wei", "Mengxi", ""], ["He", "Yifan", ""], ["Zhang", "Qiong", ""]]}, {"id": "2005.11021", "submitter": "Moritz Schubotz", "authors": "Philipp Scharpf, Moritz Schubotz, Abdou Youssef, Felix Hamborg, Norman\n  Meuschke, Bela Gipp", "title": "Classification and Clustering of arXiv Documents, Sections, and\n  Abstracts, Comparing Encodings of Natural and Mathematical Language", "comments": null, "journal-ref": "Proceedings of the ACM/IEEE Joint Conference on Digital Libraries\n  JCDL 2020", "doi": "10.1145/3383583.3398529", "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we show how selecting and combining encodings of natural and\nmathematical language affect classification and clustering of documents with\nmathematical content. We demonstrate this by using sets of documents, sections,\nand abstracts from the arXiv preprint server that are labeled by their subject\nclass (mathematics, computer science, physics, etc.) to compare different\nencodings of text and formulae and evaluate the performance and runtimes of\nselected classification and clustering algorithms. Our encodings achieve\nclassification accuracies up to $82.8\\%$ and cluster purities up to $69.4\\%$\n(number of clusters equals number of classes), and $99.9\\%$ (unspecified number\nof clusters) respectively. We observe a relatively low correlation between text\nand math similarity, which indicates the independence of text and formulae and\nmotivates treating them as separate features of a document. The classification\nand clustering can be employed, e.g., for document search and recommendation.\nFurthermore, we show that the computer outperforms a human expert when\nclassifying documents. Finally, we evaluate and discuss multi-label\nclassification and formula semantification.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:16:32 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Scharpf", "Philipp", ""], ["Schubotz", "Moritz", ""], ["Youssef", "Abdou", ""], ["Hamborg", "Felix", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2005.11055", "submitter": "Abhirut Gupta", "authors": "Kushal Chauhan and Abhirut Gupta", "title": "Improving Segmentation for Technical Support Problems", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Technical support problems are often long and complex. They typically contain\nuser descriptions of the problem, the setup, and steps for attempted\nresolution. Often they also contain various non-natural language text elements\nlike outputs of commands, snippets of code, error messages or stack traces.\nThese elements contain potentially crucial information for problem resolution.\nHowever, they cannot be correctly parsed by tools designed for natural\nlanguage. In this paper, we address the problem of segmentation for technical\nsupport questions. We formulate the problem as a sequence labelling task, and\nstudy the performance of state of the art approaches. We compare this against\nan intuitive contextual sentence-level classification baseline, and a state of\nthe art supervised text-segmentation approach. We also introduce a novel\ncomponent of combining contextual embeddings from multiple language models\npre-trained on different data sources, which achieves a marked improvement over\nusing embeddings from a single pre-trained language model. Finally, we also\ndemonstrate the usefulness of such segmentation with improvements on the\ndownstream task of answer retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:29:06 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Chauhan", "Kushal", ""], ["Gupta", "Abhirut", ""]]}, {"id": "2005.11067", "submitter": "Diego Antognini", "authors": "Diego Antognini and Claudiu Musat and Boi Faltings", "title": "Interacting with Explanations through Critiquing", "comments": "Accepted at IJCAI 2021. 15 pages, 10 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using personalized explanations to support recommendations has been shown to\nincrease trust and perceived quality. However, to actually obtain better\nrecommendations, there needs to be a means for users to modify the\nrecommendation criteria by interacting with the explanation. We present a novel\ntechnique using aspect markers that learns to generate personalized\nexplanations of recommendations from review texts, and we show that human users\nsignificantly prefer these explanations over those produced by state-of-the-art\ntechniques. Our work's most important innovation is that it allows users to\nreact to a recommendation by critiquing the textual explanation: removing\n(symmetrically adding) certain aspects they dislike or that are no longer\nrelevant (symmetrically that are of interest). The system updates its user\nmodel and the resulting recommendations according to the critique. This is\nbased on a novel unsupervised critiquing method for single- and multi-step\ncritiquing with textual explanations. Experiments on two real-world datasets\nshow that our system is the first to achieve good performance in adapting to\nthe preferences expressed in multi-step critiquing.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 09:03:06 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 09:07:21 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 10:32:26 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Antognini", "Diego", ""], ["Musat", "Claudiu", ""], ["Faltings", "Boi", ""]]}, {"id": "2005.11075", "submitter": "Leonhard Hennig", "authors": "Hanchu Zhang, Leonhard Hennig, Christoph Alt, Changjian Hu, Yao Meng,\n  Chao Wang", "title": "Bootstrapping Named Entity Recognition in E-Commerce with Positive\n  Unlabeled Learning", "comments": "Accepted at ECNLP 3 (ACL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) in domains like e-commerce is an understudied\nproblem due to the lack of annotated datasets. Recognizing novel entity types\nin this domain, such as products, components, and attributes, is challenging\nbecause of their linguistic complexity and the low coverage of existing\nknowledge resources. To address this problem, we present a bootstrapped\npositive-unlabeled learning algorithm that integrates domain-specific\nlinguistic features to quickly and efficiently expand the seed dictionary. The\nmodel achieves an average F1 score of 72.02% on a novel dataset of product\ndescriptions, an improvement of 3.63% over a baseline BiLSTM classifier, and in\nparticular exhibits better recall (4.96% on average).\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 09:35:30 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Zhang", "Hanchu", ""], ["Hennig", "Leonhard", ""], ["Alt", "Christoph", ""], ["Hu", "Changjian", ""], ["Meng", "Yao", ""], ["Wang", "Chao", ""]]}, {"id": "2005.11140", "submitter": "Mariona Coll Ardanuy", "authors": "Mariona Coll Ardanuy, Federico Nanni, Kaspar Beelen, Kasra Hosseini,\n  Ruth Ahnert, Jon Lawrence, Katherine McDonough, Giorgia Tolfo, Daniel CS\n  Wilson, Barbara McGillivray", "title": "Living Machines: A study of atypical animacy", "comments": "12 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new approach to animacy detection, the task of\ndetermining whether an entity is represented as animate in a text. In\nparticular, this work is focused on atypical animacy and examines the scenario\nin which typically inanimate objects, specifically machines, are given animate\nattributes. To address it, we have created the first dataset for atypical\nanimacy detection, based on nineteenth-century sentences in English, with\nmachines represented as either animate or inanimate. Our method builds on\nrecent innovations in language modeling, specifically BERT contextualized word\nembeddings, to better capture fine-grained contextual properties of words. We\npresent a fully unsupervised pipeline, which can be easily adapted to different\ncontexts, and report its performance on an established animacy dataset and our\nnewly introduced resource. We show that our method provides a substantially\nmore accurate characterization of atypical animacy, especially when applied to\nhighly complex forms of language use.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 12:35:18 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 14:55:51 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ardanuy", "Mariona Coll", ""], ["Nanni", "Federico", ""], ["Beelen", "Kaspar", ""], ["Hosseini", "Kasra", ""], ["Ahnert", "Ruth", ""], ["Lawrence", "Jon", ""], ["McDonough", "Katherine", ""], ["Tolfo", "Giorgia", ""], ["Wilson", "Daniel CS", ""], ["McGillivray", "Barbara", ""]]}, {"id": "2005.11153", "submitter": "Hongyin Luo", "authors": "Hongyin Luo, Shang-Wen Li, James Glass", "title": "Prototypical Q Networks for Automatic Conversational Diagnosis and\n  Few-Shot New Disease Adaption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken dialog systems have seen applications in many domains, including\nmedical for automatic conversational diagnosis. State-of-the-art dialog\nmanagers are usually driven by deep reinforcement learning models, such as deep\nQ networks (DQNs), which learn by interacting with a simulator to explore the\nentire action space since real conversations are limited. However, the\nDQN-based automatic diagnosis models do not achieve satisfying performances\nwhen adapted to new, unseen diseases with only a few training samples. In this\nwork, we propose the Prototypical Q Networks (ProtoQN) as the dialog manager\nfor the automatic diagnosis systems. The model calculates prototype embeddings\nwith real conversations between doctors and patients, learning from them and\nsimulator-augmented dialogs more efficiently. We create both supervised and\nfew-shot learning tasks with the Muzhi corpus. Experiments showed that the\nProtoQN significantly outperformed the baseline DQN model in both supervised\nand few-shot learning scenarios, and achieves state-of-the-art few-shot\nlearning performances.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:10:49 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Luo", "Hongyin", ""], ["Li", "Shang-Wen", ""], ["Glass", "James", ""]]}, {"id": "2005.11176", "submitter": "Irina Nikishina", "authors": "Irina Nikishina and Varvara Logacheva and Alexander Panchenko and\n  Natalia Loukachevitch", "title": "RUSSE'2020: Findings of the First Taxonomy Enrichment Task for the\n  Russian language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the results of the first shared task on taxonomy\nenrichment for the Russian language. The participants were asked to extend an\nexisting taxonomy with previously unseen words: for each new word their systems\nshould provide a ranked list of possible (candidate) hypernyms. In comparison\nto the previous tasks for other languages, our competition has a more realistic\ntask setting: new words were provided without definitions. Instead, we provided\na textual corpus where these new terms occurred. For this evaluation campaign,\nwe developed a new evaluation dataset based on unpublished RuWordNet data. The\nshared task features two tracks: \"nouns\" and \"verbs\". 16 teams participated in\nthe task demonstrating high results with more than half of them outperforming\nthe provided baseline.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:30:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Nikishina", "Irina", ""], ["Logacheva", "Varvara", ""], ["Panchenko", "Alexander", ""], ["Loukachevitch", "Natalia", ""]]}, {"id": "2005.11177", "submitter": "Muhammad Imran", "authors": "Umair Qazi, Muhammad Imran, Ferda Ofli", "title": "GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19\n  Tweets with Location Information", "comments": "10 pages, 5 figures, accepted at ACM SIGSPATIAL Special May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past several years have witnessed a huge surge in the use of social media\nplatforms during mass convergence events such as health emergencies, natural or\nhuman-induced disasters. These non-traditional data sources are becoming vital\nfor disease forecasts and surveillance when preparing for epidemic and pandemic\noutbreaks. In this paper, we present GeoCoV19, a large-scale Twitter dataset\ncontaining more than 524 million multilingual tweets posted over a period of 90\ndays since February 1, 2020. Moreover, we employ a gazetteer-based approach to\ninfer the geolocation of tweets. We postulate that this large-scale,\nmultilingual, geolocated social media data can empower the research communities\nto evaluate how societies are collectively coping with this unprecedented\nglobal crisis as well as to develop computational methods to address challenges\nsuch as identifying fake news, understanding communities' knowledge gaps,\nbuilding disease forecast and surveillance models, among others.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:30:42 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Qazi", "Umair", ""], ["Imran", "Muhammad", ""], ["Ofli", "Ferda", ""]]}, {"id": "2005.11184", "submitter": "Hemant Yadav", "authors": "Hemant Yadav, Sreyan Ghosh, Yi Yu, Rajiv Ratn Shah", "title": "End-to-end Named Entity Recognition from English Speech", "comments": "submitted to Interspeech-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) from text has been a widely studied problem\nand usually extracts semantic information from text. Until now, NER from speech\nis mostly studied in a two-step pipeline process that includes first applying\nan automatic speech recognition (ASR) system on an audio sample and then\npassing the predicted transcript to a NER tagger. In such cases, the error does\nnot propagate from one step to another as both the tasks are not optimized in\nan end-to-end (E2E) fashion. Recent studies confirm that integrated approaches\n(e.g., E2E ASR) outperform sequential ones (e.g., phoneme based ASR). In this\npaper, we introduce a first publicly available NER annotated dataset for\nEnglish speech and present an E2E approach, which jointly optimizes the ASR and\nNER tagger components. Experimental results show that the proposed E2E approach\noutperforms the classical two-step approach. We also discuss how NER from\nspeech can be used to handle out of vocabulary (OOV) words in an ASR system.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:39:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Yadav", "Hemant", ""], ["Ghosh", "Sreyan", ""], ["Yu", "Yi", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2005.11185", "submitter": "Danni Liu", "authors": "Danni Liu, Gerasimos Spanakis, Jan Niehues", "title": "Low-Latency Sequence-to-Sequence Speech Recognition and Translation by\n  Partial Hypothesis Selection", "comments": "Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder models provide a generic architecture for\nsequence-to-sequence tasks such as speech recognition and translation. While\noffline systems are often evaluated on quality metrics like word error rates\n(WER) and BLEU, latency is also a crucial factor in many practical use-cases.\nWe propose three latency reduction techniques for chunk-based incremental\ninference and evaluate their efficiency in terms of accuracy-latency trade-off.\nOn the 300-hour How2 dataset, we reduce latency by 83% to 0.8 second by\nsacrificing 1% WER (6% rel.) compared to offline transcription. Although our\nexperiments use the Transformer, the hypothesis selection strategies are\napplicable to other encoder-decoder models. To avoid expensive re-computation,\nwe use a unidirectionally-attending encoder. After an adaptation procedure to\npartial sequences, the unidirectional model performs on-par with the original\nmodel. We further show that our approach is also applicable to low-latency\nspeech translation. On How2 English-Portuguese speech translation, we reduce\nlatency to 0.7 second (-84% rel.) while incurring a loss of 2.4 BLEU points (5%\nrel.) compared to the offline system.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:42:54 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 16:18:44 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liu", "Danni", ""], ["Spanakis", "Gerasimos", ""], ["Niehues", "Jan", ""]]}, {"id": "2005.11197", "submitter": "Sneha Mehta", "authors": "Sneha Mehta, Bahareh Azarnoush, Boris Chen, Avneesh Saluja, Vinith\n  Misra, Ballav Bihani, Ritwik Kumar", "title": "Simplify-then-Translate: Automatic Preprocessing for Black-Box Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Black-box machine translation systems have proven incredibly useful for a\nvariety of applications yet by design are hard to adapt, tune to a specific\ndomain, or build on top of. In this work, we introduce a method to improve such\nsystems via automatic pre-processing (APP) using sentence simplification. We\nfirst propose a method to automatically generate a large in-domain paraphrase\ncorpus through back-translation with a black-box MT system, which is used to\ntrain a paraphrase model that \"simplifies\" the original sentence to be more\nconducive for translation. The model is used to preprocess source sentences of\nmultiple low-resource language pairs. We show that this preprocessing leads to\nbetter translation performance as compared to non-preprocessed source\nsentences. We further perform side-by-side human evaluation to verify that\ntranslations of the simplified sentences are better than the original ones.\nFinally, we provide some guidance on recommended language pairs for generating\nthe simplification model corpora by investigating the relationship between ease\nof translation of a language pair (as measured by BLEU) and quality of the\nresulting simplification model from back-translations of this language pair (as\nmeasured by SARI), and tie this into the downstream task of low-resource\ntranslation.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 14:15:53 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 15:37:50 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Mehta", "Sneha", ""], ["Azarnoush", "Bahareh", ""], ["Chen", "Boris", ""], ["Saluja", "Avneesh", ""], ["Misra", "Vinith", ""], ["Bihani", "Ballav", ""], ["Kumar", "Ritwik", ""]]}, {"id": "2005.11216", "submitter": "Anjalie Field", "authors": "Anjalie Field, Sascha Rothe, Simon Baumgartner, Cong Yu, and Abe\n  Ittycheriah", "title": "A Generative Approach to Titling and Clustering Wikipedia Sections", "comments": "Accepted to WNGT Workshop at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the performance of transformer encoders with various decoders for\ninformation organization through a new task: generation of section headings for\nWikipedia articles. Our analysis shows that decoders containing attention\nmechanisms over the encoder output achieve high-scoring results by generating\nextractive text. In contrast, a decoder without attention better facilitates\nsemantic encoding and can be used to generate section embeddings. We\nadditionally introduce a new loss function, which further encourages the\ndecoder to generate high-quality embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 14:49:07 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Field", "Anjalie", ""], ["Rothe", "Sascha", ""], ["Baumgartner", "Simon", ""], ["Yu", "Cong", ""], ["Ittycheriah", "Abe", ""]]}, {"id": "2005.11223", "submitter": "Yunchang Zhu", "authors": "Yunchang Zhu, Liang Pang, Yanyan Lan, Xueqi Cheng", "title": "L2R2: Leveraging Ranking for Abductive Reasoning", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The abductive natural language inference task ($\\alpha$NLI) is proposed to\nevaluate the abductive reasoning ability of a learning system. In the\n$\\alpha$NLI task, two observations are given and the most plausible hypothesis\nis asked to pick out from the candidates. Existing methods simply formulate it\nas a classification problem, thus a cross-entropy log-loss objective is used\nduring training. However, discriminating true from false does not measure the\nplausibility of a hypothesis, for all the hypotheses have a chance to happen,\nonly the probabilities are different. To fill this gap, we switch to a ranking\nperspective that sorts the hypotheses in order of their plausibilities. With\nthis new perspective, a novel $L2R^2$ approach is proposed under the\nlearning-to-rank framework. Firstly, training samples are reorganized into a\nranking form, where two observations and their hypotheses are treated as the\nquery and a set of candidate documents respectively. Then, an ESIM model or\npre-trained language model, e.g. BERT or RoBERTa, is obtained as the scoring\nfunction. Finally, the loss functions for the ranking task can be either\npair-wise or list-wise for training. The experimental results on the ART\ndataset reach the state-of-the-art in the public leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:01:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Zhu", "Yunchang", ""], ["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2005.11239", "submitter": "Nikolay Banar", "authors": "Nikolay Banar, Walter Daelemans and Mike Kestemont", "title": "Character-level Transformer-based Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation (NMT) is nowadays commonly applied at the subword\nlevel, using byte-pair encoding. A promising alternative approach focuses on\ncharacter-level translation, which simplifies processing pipelines in NMT\nconsiderably. This approach, however, must consider relatively longer\nsequences, rendering the training process prohibitively expensive. In this\npaper, we discuss a novel, Transformer-based approach, that we compare, both in\nspeed and in quality to the Transformer at subword and character levels, as\nwell as previously developed character-level models. We evaluate our models on\n4 language pairs from WMT'15: DE-EN, CS-EN, FI-EN and RU-EN. The proposed novel\narchitecture can be trained on a single GPU and is 34% percent faster than the\ncharacter-level Transformer; still, the obtained results are at least on par\nwith it. In addition, our proposed model outperforms the subword-level model in\nFI-EN and shows close results in CS-EN. To stimulate further research in this\narea and close the gap with subword-level NMT, we make all our code and models\npublicly available.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:40:43 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Banar", "Nikolay", ""], ["Daelemans", "Walter", ""], ["Kestemont", "Mike", ""]]}, {"id": "2005.11267", "submitter": "Tom Williams", "authors": "Poulomi Pal, Lixiao Zhu, Andrea Golden-Lasher, Akshay Swaminathan, Tom\n  Williams", "title": "Givenness Hierarchy Theoretic Cognitive Status Filtering", "comments": "To be published in the proceedings of the 2020 Annual Meeting of the\n  Cognitive Science Society (COGSCI). Supplemental materials available at\n  https://osf.io/qse7y/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For language-capable interactive robots to be effectively introduced into\nhuman society, they must be able to naturally and efficiently communicate about\nthe objects, locations, and people found in human environments. An important\naspect of natural language communication is the use of pronouns. Ac-cording to\nthe linguistic theory of the Givenness Hierarchy(GH), humans use pronouns due\nto implicit assumptions about the cognitive statuses their referents have in\nthe minds of their conversational partners. In previous work, Williams et al.\npresented the first computational implementation of the full GH for the purpose\nof robot language understanding, leveraging a set of rules informed by the GH\nliterature. However, that approach was designed specifically for language\nunderstanding,oriented around GH-inspired memory structures used to assess what\nentities are candidate referents given a particular cognitive status. In\ncontrast, language generation requires a model in which cognitive status can be\nassessed for a given entity. We present and compare two such models of\ncognitive status: a rule-based Finite State Machine model directly informed by\nthe GH literature and a Cognitive Status Filter designed to more flexibly\nhandle uncertainty. The models are demonstrated and evaluated using a\nsilver-standard English subset of the OFAI Multimodal Task Description Corpus.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:44:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Pal", "Poulomi", ""], ["Zhu", "Lixiao", ""], ["Golden-Lasher", "Andrea", ""], ["Swaminathan", "Akshay", ""], ["Williams", "Tom", ""]]}, {"id": "2005.11313", "submitter": "Param Raval", "authors": "Devshree Patel, Param Raval, Ratnam Parikh, Yesha Shastri", "title": "Comparative Study of Machine Learning Models and BERT on SQuAD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to provide a comparative analysis of performance of certain\nmodels popular in machine learning and the BERT model on the Stanford Question\nAnswering Dataset (SQuAD). The analysis shows that the BERT model, which was\nonce state-of-the-art on SQuAD, gives higher accuracy in comparison to other\nmodels. However, BERT requires a greater execution time even when only 100\nsamples are used. This shows that with increasing accuracy more amount of time\nis invested in training the data. Whereas in case of preliminary machine\nlearning models, execution time for full data is lower but accuracy is\ncompromised.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:58:30 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Patel", "Devshree", ""], ["Raval", "Param", ""], ["Parikh", "Ratnam", ""], ["Shastri", "Yesha", ""]]}, {"id": "2005.11344", "submitter": "Luca Lugini", "authors": "Christopher Olshefski, Luca Lugini, Ravneet Singh, Diane Litman,\n  Amanda Godley", "title": "The Discussion Tracker Corpus of Collaborative Argumentation", "comments": "In Proceedings of The 12th Language Resources and Evaluation\n  Conference (LREC), Marseille, France, May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although Natural Language Processing (NLP) research on argument mining has\nadvanced considerably in recent years, most studies draw on corpora of\nasynchronous and written texts, often produced by individuals. Few published\ncorpora of synchronous, multi-party argumentation are available. The Discussion\nTracker corpus, collected in American high school English classes, is an\nannotated dataset of transcripts of spoken, multi-party argumentation. The\ncorpus consists of 29 multi-party discussions of English literature transcribed\nfrom 985 minutes of audio. The transcripts were annotated for three dimensions\nof collaborative argumentation: argument moves (claims, evidence, and\nexplanations), specificity (low, medium, high) and collaboration (e.g.,\nextensions of and disagreements about others' ideas). In addition to providing\ndescriptive statistics on the corpus, we provide performance benchmarks and\nassociated code for predicting each dimension separately, illustrate the use of\nthe multiple annotations in the corpus to improve performance via multi-task\nlearning, and finally discuss other ways the corpus might be used to further\nNLP research.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 18:27:28 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Olshefski", "Christopher", ""], ["Lugini", "Luca", ""], ["Singh", "Ravneet", ""], ["Litman", "Diane", ""], ["Godley", "Amanda", ""]]}, {"id": "2005.11347", "submitter": "Li Zhang", "authors": "Li Zhang, Han Wang, Lingxiao Li", "title": "SentPWNet: A Unified Sentence Pair Weighting Network for Task-specific\n  Sentence Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pair-based metric learning has been widely adopted to learn sentence\nembedding in many NLP tasks such as semantic text similarity due to its\nefficiency in computation. Most existing works employed a sequence encoder\nmodel and utilized limited sentence pairs with a pair-based loss to learn\ndiscriminating sentence representation. However, it is known that the sentence\nrepresentation can be biased when the sampled sentence pairs deviate from the\ntrue distribution of all sentence pairs. In this paper, our theoretical\nanalysis shows that existing works severely suffered from a good pair sampling\nand instance weighting strategy. Instead of one time pair selection and\nlearning on equal weighted pairs, we propose a unified locality weighting and\nlearning framework to learn task-specific sentence embedding. Our model,\nSentPWNet, exploits the neighboring spatial distribution of each sentence as\nlocality weight to indicate the informative level of sentence pair. Such weight\nis updated along with pair-loss optimization in each round, ensuring the model\nkeep learning the most informative sentence pairs. Extensive experiments on\nfour public available datasets and a self-collected place search benchmark with\n1.4 million places clearly demonstrate that our model consistently outperforms\nexisting sentence embedding methods with comparable efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 18:32:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhang", "Li", ""], ["Wang", "Han", ""], ["Li", "Lingxiao", ""]]}, {"id": "2005.11355", "submitter": "Aakanksha Naik", "authors": "Aakanksha Naik, Carolyn Ros\\'e", "title": "Towards Open Domain Event Trigger Identification using Adversarial\n  Domain Adaptation", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We tackle the task of building supervised event trigger identification models\nwhich can generalize better across domains. Our work leverages the adversarial\ndomain adaptation (ADA) framework to introduce domain-invariance. ADA uses\nadversarial training to construct representations that are predictive for\ntrigger identification, but not predictive of the example's domain. It requires\nno labeled data from the target domain, making it completely unsupervised.\nExperiments with two domains (English literature and news) show that ADA leads\nto an average F1 score improvement of 3.9 on out-of-domain data. Our best\nperforming model (BERT-A) reaches 44-49 F1 across both domains, using no\nlabeled target data. Preliminary experiments reveal that finetuning on 1%\nlabeled data, followed by self-training leads to substantial improvement,\nreaching 51.5 and 67.2 F1 on literature and news respectively.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 19:19:50 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Naik", "Aakanksha", ""], ["Ros\u00e9", "Carolyn", ""]]}, {"id": "2005.11364", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft and Mohit\n  Iyyer", "title": "Open-Retrieval Conversational Question Answering", "comments": "Accepted to SIGIR'20", "journal-ref": null, "doi": "10.1145/3397271.3401110", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is one of the ultimate goals of information retrieval.\nRecent research approaches conversational search by simplified settings of\nresponse ranking and conversational question answering, where an answer is\neither selected from a given candidate set or extracted from a given passage.\nThese simplifications neglect the fundamental role of retrieval in\nconversational search. To address this limitation, we introduce an\nopen-retrieval conversational question answering (ORConvQA) setting, where we\nlearn to retrieve evidence from a large collection before extracting answers,\nas a further step towards building functional conversational search systems. We\ncreate a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an\nend-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader\nthat are all based on Transformers. Our extensive experiments on OR-QuAC\ndemonstrate that a learnable retriever is crucial for ORConvQA. We further show\nthat our system can make a substantial improvement when we enable history\nmodeling in all system components. Moreover, we show that the reranker\ncomponent contributes to the model performance by providing a regularization\neffect. Finally, further in-depth analyses are performed to provide new\ninsights into ORConvQA.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 19:39:50 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Chen", "Cen", ""], ["Qiu", "Minghui", ""], ["Croft", "W. Bruce", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2005.11401", "submitter": "Patrick Lewis", "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\n  Karpukhin, Naman Goyal, Heinrich K\\\"uttler, Mike Lewis, Wen-tau Yih, Tim\n  Rockt\\\"aschel, Sebastian Riedel, Douwe Kiela", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "comments": "Accepted at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 21:34:34 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 16:23:06 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 10:12:16 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 15:42:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lewis", "Patrick", ""], ["Perez", "Ethan", ""], ["Piktus", "Aleksandra", ""], ["Petroni", "Fabio", ""], ["Karpukhin", "Vladimir", ""], ["Goyal", "Naman", ""], ["K\u00fcttler", "Heinrich", ""], ["Lewis", "Mike", ""], ["Yih", "Wen-tau", ""], ["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""], ["Kiela", "Douwe", ""]]}, {"id": "2005.11424", "submitter": "Xiangjue Dong", "authors": "Xiangjue Dong, Changmao Li, Jinho D. Choi", "title": "Transformer-based Context-aware Sarcasm Detection in Conversation\n  Threads from Social Media", "comments": "To be published in ACL FigLang2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transformer-based sarcasm detection model that accounts for the\ncontext from the entire conversation thread for more robust predictions. Our\nmodel uses deep transformer layers to perform multi-head attentions among the\ntarget utterance and the relevant context in the thread. The context-aware\nmodels are evaluated on two datasets from social media, Twitter and Reddit, and\nshow 3.1% and 7.0% improvements over their baselines. Our best models give the\nF1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively,\nbecoming one of the highest performing systems among 36 participants in this\nshared task.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 23:41:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dong", "Xiangjue", ""], ["Li", "Changmao", ""], ["Choi", "Jinho D.", ""]]}, {"id": "2005.11458", "submitter": "Boyi Liu", "authors": "Yixian Zhang, Jieren Chen, Boyi Liu, Yifan Yang, Haocheng Li, Xinyi\n  Zheng, Xi Chen, Tenglong Ren and Naixue Xiong", "title": "COVID-19 Public Opinion and Emotion Monitoring System Based on Time\n  Series Thermal New Word Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the spread and development of new epidemics, it is of great reference\nvalue to identify the changing trends of epidemics in public emotions. We\ndesigned and implemented the COVID-19 public opinion monitoring system based on\ntime series thermal new word mining. A new word structure discovery scheme\nbased on the timing explosion of network topics and a Chinese sentiment\nanalysis method for the COVID-19 public opinion environment is proposed.\nEstablish a \"Scrapy-Redis-Bloomfilter\" distributed crawler framework to collect\ndata. The system can judge the positive and negative emotions of the reviewer\nbased on the comments, and can also reflect the depth of the seven emotions\nsuch as Hopeful, Happy, and Depressed. Finally, we improved the sentiment\ndiscriminant model of this system and compared the sentiment discriminant error\nof COVID-19 related comments with the Jiagu deep learning model. The results\nshow that our model has better generalization ability and smaller discriminant\nerror. We designed a large data visualization screen, which can clearly show\nthe trend of public emotions, the proportion of various emotion categories,\nkeywords, hot topics, etc., and fully and intuitively reflect the development\nof public opinion.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 03:42:10 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhang", "Yixian", ""], ["Chen", "Jieren", ""], ["Liu", "Boyi", ""], ["Yang", "Yifan", ""], ["Li", "Haocheng", ""], ["Zheng", "Xinyi", ""], ["Chen", "Xi", ""], ["Ren", "Tenglong", ""], ["Xiong", "Naixue", ""]]}, {"id": "2005.11490", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Zhuyun Dai and Krisztian Balog and Jamie Callan", "title": "Summarizing and Exploring Tabular Data in Conversational Search", "comments": "Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR 2020), 2020", "journal-ref": null, "doi": "10.1145/3397271.3401205", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular data provide answers to a significant portion of search queries.\nHowever, reciting an entire result table is impractical in conversational\nsearch systems. We propose to generate natural language summaries as answers to\ndescribe the complex information contained in a table. Through crowdsourcing\nexperiments, we build a new conversation-oriented, open-domain table\nsummarization dataset. It includes annotated table summaries, which not only\nanswer questions but also help people explore other information in the table.\nWe utilize this dataset to develop automatic table summarization systems as\nSOTA baselines. Based on the experimental results, we identify challenges and\npoint out future research directions that this resource will support.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 08:29:51 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:34:53 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 15:56:31 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Zhang", "Shuo", ""], ["Dai", "Zhuyun", ""], ["Balog", "Krisztian", ""], ["Callan", "Jamie", ""]]}, {"id": "2005.11494", "submitter": "Roland Roller", "authors": "Laura Seiffe, Oliver Marten, Michael Mikhailov, Sven Schmeier,\n  Sebastian M\\\"oller, Roland Roller", "title": "From Witch's Shot to Music Making Bones -- Resources for Medical Laymen\n  to Technical Language and Vice Versa", "comments": "In Proceedings of LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people share information in social media or forums, like food they eat,\nsports activities they do or events which have been visited. This also applies\nto information about a person's health status. Information we share online\nunveils directly or indirectly information about our lifestyle and health\nsituation and thus provides a valuable data resource. If we can make advantage\nof that data, applications can be created that enable e.g. the detection of\npossible risk factors of diseases or adverse drug reactions of medications.\nHowever, as most people are not medical experts, language used might be more\ndescriptive rather than the precise medical expression as medics do. To detect\nand use those relevant information, laymen language has to be translated and/or\nlinked to the corresponding medical concept. This work presents baseline data\nsources in order to address this challenge for German. We introduce a new data\nset which annotates medical laymen and technical expressions in a patient\nforum, along with a set of medical synonyms and definitions, and present first\nbaseline results on the data.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 08:56:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Seiffe", "Laura", ""], ["Marten", "Oliver", ""], ["Mikhailov", "Michael", ""], ["Schmeier", "Sven", ""], ["M\u00f6ller", "Sebastian", ""], ["Roller", "Roland", ""]]}, {"id": "2005.11640", "submitter": "Su Zhu", "authors": "Chen Liu, Su Zhu, Zijian Zhao, Ruisheng Cao, Lu Chen and Kai Yu", "title": "Jointly Encoding Word Confusion Network and Dialogue Context with BERT\n  for Spoken Language Understanding", "comments": "Accepted to INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Understanding (SLU) converts hypotheses from automatic speech\nrecognizer (ASR) into structured semantic representations. ASR recognition\nerrors can severely degenerate the performance of the subsequent SLU module. To\naddress this issue, word confusion networks (WCNs) have been used to encode the\ninput for SLU, which contain richer information than 1-best or n-best\nhypotheses list. To further eliminate ambiguity, the last system act of\ndialogue context is also utilized as additional input. In this paper, a novel\nBERT based SLU model (WCN-BERT SLU) is proposed to encode WCNs and the dialogue\ncontext jointly. It can integrate both structural information and ASR posterior\nprobabilities of WCNs in the BERT architecture. Experiments on DSTC2, a\nbenchmark of SLU, show that the proposed method is effective and can outperform\nprevious state-of-the-art models significantly.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 02:26:13 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 13:24:06 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 02:45:43 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Liu", "Chen", ""], ["Zhu", "Su", ""], ["Zhao", "Zijian", ""], ["Cao", "Ruisheng", ""], ["Chen", "Lu", ""], ["Yu", "Kai", ""]]}, {"id": "2005.11665", "submitter": "Xiuyu Wu", "authors": "Xiuyu Wu, Nan Jiang and Yunfang Wu", "title": "A Question Type Driven and Copy Loss Enhanced Frameworkfor\n  Answer-Agnostic Neural Question Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The answer-agnostic question generation is a significant and challenging\ntask, which aims to automatically generate questions for a given sentence but\nwithout an answer. In this paper, we propose two new strategies to deal with\nthis task: question type prediction and copy loss mechanism. The question type\nmodule is to predict the types of questions that should be asked, which allows\nour model to generate multiple types of questions for the same source sentence.\nThe new copy loss enhances the original copy mechanism to make sure that every\nimportant word in the source sentence has been copied when generating\nquestions. Our integrated model outperforms the state-of-the-art approach in\nanswer-agnostic question generation, achieving a BLEU-4 score of 13.9 on SQuAD.\nHuman evaluation further validates the high quality of our generated questions.\nWe will make our code public available for further research.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 07:09:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wu", "Xiuyu", ""], ["Jiang", "Nan", ""], ["Wu", "Yunfang", ""]]}, {"id": "2005.11676", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura", "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis:\n  ZeroSpeech 2020 Challenge", "comments": "Submitted to INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report our submitted system for the ZeroSpeech 2020\nchallenge on Track 2019. The main theme in this challenge is to build a speech\nsynthesizer without any textual information or phonetic labels. In order to\ntackle those challenges, we build a system that must address two major\ncomponents such as 1) given speech audio, extract subword units in an\nunsupervised way and 2) re-synthesize the audio from novel speakers. The system\nalso needs to balance the codebook performance between the ABX error rate and\nthe bitrate compression rate. Our main contribution here is we proposed\nTransformer-based VQ-VAE for unsupervised unit discovery and Transformer-based\ninverter for the speech synthesis given the extracted codebook. Additionally,\nwe also explored several regularization methods to improve performance even\nfurther.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 07:42:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "2005.11682", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Thomas Dubuisson, Alexis Moinet, Nicolas D'Alessandro,\n  Thierry Dutoit", "title": "Glottal source estimation robustness: A comparison of sensitivity of\n  voice source estimation techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating the voice source directly from\nspeech waveforms. A novel principle based on Anticausality Dominated Regions\n(ACDR) is used to estimate the glottal open phase. This technique is compared\nto two other state-of-the-art well-known methods, namely the Zeros of the\nZ-Transform (ZZT) and the Iterative Adaptive Inverse Filtering (IAIF)\nalgorithms. Decomposition quality is assessed on synthetic signals through two\nobjective measures: the spectral distortion and a glottal formant determination\nrate. Technique robustness is tested by analyzing the influence of noise and\nGlottal Closure Instant (GCI) location errors. Besides impacts of the\nfundamental frequency and the first formant on the performance are evaluated.\nOur proposed approach shows significant improvement in robustness, which could\nbe of a great interest when decomposing real speech.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 08:13:47 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Drugman", "Thomas", ""], ["Dubuisson", "Thomas", ""], ["Moinet", "Alexis", ""], ["D'Alessandro", "Nicolas", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2005.11687", "submitter": "Nikola Milo\\v{s}evi\\'c Dr", "authors": "Nikola Milosevic, Gangamma Kalappa, Hesam Dadafarin, Mahmoud Azimaee,\n  Goran Nenadic", "title": "MASK: A flexible framework to facilitate de-identification of clinical\n  texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Medical health records and clinical summaries contain a vast amount of\nimportant information in textual form that can help advancing research on\ntreatments, drugs and public health. However, the majority of these information\nis not shared because they contain private information about patients, their\nfamilies, or medical staff treating them. Regulations such as HIPPA in the US,\nPHIPPA in Canada and GDPR regulate the protection, processing and distribution\nof this information. In case this information is de-identified and personal\ninformation are replaced or redacted, they could be distributed to the research\ncommunity. In this paper, we present MASK, a software package that is designed\nto perform the de-identification task. The software is able to perform named\nentity recognition using some of the state-of-the-art techniques and then mask\nor redact recognized entities. The user is able to select named entity\nrecognition algorithm (currently implemented are two versions of CRF-based\ntechniques and BiLSTM-based neural network with pre-trained GLoVe and ELMo\nembedding) and masking algorithm (e.g. shift dates, replace names/locations,\ntotally redact entity).\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 08:53:00 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 20:09:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Milosevic", "Nikola", ""], ["Kalappa", "Gangamma", ""], ["Dadafarin", "Hesam", ""], ["Azimaee", "Mahmoud", ""], ["Nenadic", "Goran", ""]]}, {"id": "2005.11694", "submitter": "Ye Ma", "authors": "Ye Ma and Lu Zong", "title": "Integrated Node Encoder for Labelled Textual Networks", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Voluminous works have been implemented to exploit content-enhanced network\nembedding models, with little focus on the labelled information of nodes.\nAlthough TriDNR leverages node labels by treating them as node attributes, it\nfails to enrich unlabelled node vectors with the labelled information, which\nleads to the weaker classification result on the test set in comparison to\nexisting unsupervised textual network embedding models. In this study, we\ndesign an integrated node encoder (INE) for textual networks which is jointly\ntrained on the structure-based and label-based objectives. As a result, the\nnode encoder preserves the integrated knowledge of not only the network text\nand structure, but also the labelled information. Furthermore, INE allows the\ncreation of label-enhanced vectors for unlabelled nodes by entering their node\ncontents. Our node embedding achieves state-of-the-art performances in the\nclassification task on two public citation networks, namely Cora and DBLP,\npushing benchmarks up by 10.0\\% and 12.1\\%, respectively, with the 70\\%\ntraining ratio. Additionally, a feasible solution that generalizes our model\nfrom textual networks to a broader range of networks is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 09:20:34 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ma", "Ye", ""], ["Zong", "Lu", ""]]}, {"id": "2005.11700", "submitter": "Zequn Liu", "authors": "Zequn Liu, Ruiyi Zhang, Yiping Song, Ming Zhang", "title": "When does MAML Work the Best? An Empirical Study on Model-Agnostic\n  Meta-Learning in NLP Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Agnostic Meta-Learning (MAML), a model-agnostic meta-learning method,\nis successfully employed in NLP applications including few-shot text\nclassification and multi-domain low-resource language generation. Many\nimpacting factors, including data quantity, similarity among tasks, and the\nbalance between general language model and task-specific adaptation, can affect\nthe performance of MAML in NLP, but few works have thoroughly studied them. In\nthis paper, we conduct an empirical study to investigate these impacting\nfactors and conclude when MAML works the best based on the experimental\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 09:29:36 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Liu", "Zequn", ""], ["Zhang", "Ruiyi", ""], ["Song", "Yiping", ""], ["Zhang", "Ming", ""]]}, {"id": "2005.11706", "submitter": "Ye Ma", "authors": "Ye Ma, Lu Zong, Peiwan Wang", "title": "A Novel Distributed Representation of News (DRNews) for Stock Market\n  Predictions", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this study, a novel Distributed Representation of News (DRNews) model is\ndeveloped and applied in deep learning-based stock market predictions. With the\nmerit of integrating contextual information and cross-documental knowledge, the\nDRNews model creates news vectors that describe both the semantic information\nand potential linkages among news events through an attributed news network.\nTwo stock market prediction tasks, namely the short-term stock movement\nprediction and stock crises early warning, are implemented in the framework of\nthe attention-based Long Short Term-Memory (LSTM) network. It is suggested that\nDRNews substantially enhances the results of both tasks comparing with five\nbaselines of news embedding models. Further, the attention mechanism suggests\nthat short-term stock trend and stock market crises both receive influences\nfrom daily news with the former demonstrates more critical responses on the\ninformation related to the stock market {\\em per se}, whilst the latter draws\nmore concerns on the banking sector and economic policies.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 10:01:27 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ma", "Ye", ""], ["Zong", "Lu", ""], ["Wang", "Peiwan", ""]]}, {"id": "2005.11723", "submitter": "Nikos Voskarides", "authors": "Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten de\n  Rijke", "title": "Query Resolution for Conversational Search with Limited Supervision", "comments": "SIGIR 2020 full conference paper", "journal-ref": null, "doi": "10.1145/3397271.3401130", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on multi-turn passage retrieval as a crucial component\nof conversational search. One of the key challenges in multi-turn passage\nretrieval comes from the fact that the current turn query is often\nunderspecified due to zero anaphora, topic change, or topic return. Context\nfrom the conversational history can be used to arrive at a better expression of\nthe current turn query, defined as the task of query resolution. In this paper,\nwe model the query resolution task as a binary term classification problem: for\neach term appearing in the previous turns of the conversation decide whether to\nadd it to the current turn query or not. We propose QuReTeC (Query Resolution\nby Term Classification), a neural query resolution model based on bidirectional\ntransformers. We propose a distant supervision method to automatically generate\ntraining data by using query-passage relevance labels. Such labels are often\nreadily available in a collection either as human annotations or inferred from\nuser interactions. We show that QuReTeC outperforms state-of-the-art models,\nand furthermore, that our distant supervision method can be used to\nsubstantially reduce the amount of human-curated data required to train\nQuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval\narchitecture and demonstrate its effectiveness on the TREC CAsT dataset.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:37:22 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Voskarides", "Nikos", ""], ["Li", "Dan", ""], ["Ren", "Pengjie", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2005.11729", "submitter": "Jianfeng Liu", "authors": "Jianfeng Liu, Feiyang Pan, Ling Luo", "title": "GoChat: Goal-oriented Chatbots with Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A chatbot that converses like a human should be goal-oriented (i.e., be\npurposeful in conversation), which is beyond language generation. However,\nexisting dialogue systems often heavily rely on cumbersome hand-crafted rules\nor costly labelled datasets to reach the goals. In this paper, we propose\nGoal-oriented Chatbots (GoChat), a framework for end-to-end training chatbots\nto maximize the longterm return from offline multi-turn dialogue datasets. Our\nframework utilizes hierarchical reinforcement learning (HRL), where the\nhigh-level policy guides the conversation towards the final goal by determining\nsome sub-goals, and the low-level policy fulfills the sub-goals by generating\nthe corresponding utterance for response. In our experiments on a real-world\ndialogue dataset for anti-fraud in financial, our approach outperforms previous\nmethods on both the quality of response generation as well as the success rate\nof accomplishing the goal.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 12:14:19 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 04:03:33 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Liu", "Jianfeng", ""], ["Pan", "Feiyang", ""], ["Luo", "Ling", ""]]}, {"id": "2005.11739", "submitter": "Mario Alberto Barrantes Quesada", "authors": "Mario Barrantes and Benedikt Herudek and Richard Wang", "title": "Adversarial NLI for Factual Correctness in Text Summarisation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the Adversarial NLI dataset to train the NLI model and show that the\nmodel has the potential to enhance factual correctness in abstract\nsummarization. We follow the work of Falke et al. (2019), which rank multiple\ngenerated summaries based on the entailment probabilities between an source\ndocument and summaries and select the summary that has the highest entailment\nprobability. The authors' earlier study concluded that current NLI models are\nnot sufficiently accurate for the ranking task. We show that the Transformer\nmodels fine-tuned on the new dataset achieve significantly higher accuracy and\nhave the potential of selecting a coherent summary.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 13:02:57 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Barrantes", "Mario", ""], ["Herudek", "Benedikt", ""], ["Wang", "Richard", ""]]}, {"id": "2005.11768", "submitter": "Xinting Huang", "authors": "Jiajing Wan and Xinting Huang", "title": "KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for\n  Comprehension And Generation", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our strategies in SemEval 2020 Task 4: Commonsense\nValidation and Explanation. We propose a novel way to search for evidence and\nchoose the different large-scale pre-trained models as the backbone for three\nsubtasks. The results show that our evidence-searching approach improves model\nperformance on commonsense explanation task. Our team ranks 2nd in subtask C\naccording to human evaluation score.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:09:21 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 06:39:14 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Wan", "Jiajing", ""], ["Huang", "Xinting", ""]]}, {"id": "2005.11769", "submitter": "Shang-Yi Chuang", "authors": "Shang-Yi Chuang, Yu Tsao, Chen-Chou Lo and Hsin-Min Wang", "title": "Lite Audio-Visual Speech Enhancement", "comments": "Accepted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have confirmed the effectiveness of incorporating visual\ninformation into speech enhancement (SE) systems. Despite improved denoising\nperformance, two problems may be encountered when implementing an audio-visual\nSE (AVSE) system: (1) additional processing costs are incurred to incorporate\nvisual input and (2) the use of face or lip images may cause privacy problems.\nIn this study, we propose a Lite AVSE (LAVSE) system to address these problems.\nThe system includes two visual data compression techniques and removes the\nvisual feature extraction network from the training model, yielding better\nonline computation efficiency. Our experimental results indicate that the\nproposed LAVSE system can provide notably better performance than an audio-only\nSE system with a similar number of model parameters. In addition, the\nexperimental results confirm the effectiveness of the two techniques for visual\ndata compression.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:09:42 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 16:04:56 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 13:33:54 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Chuang", "Shang-Yi", ""], ["Tsao", "Yu", ""], ["Lo", "Chen-Chou", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "2005.11777", "submitter": "Murong Ma", "authors": "Murong Ma, Haiwei Wu, Xuyang Wang, Lin Yang, Junjie Wang and Ming Li", "title": "Acoustic Word Embedding System for Code-Switching Query-by-example\n  Spoken Term Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep convolutional neural network-based acoustic\nword embedding system on code-switching query by example spoken term detection.\nDifferent from previous configurations, we combine audio data in two languages\nfor training instead of only using one single language. We transform the\nacoustic features of keyword templates and searching content to\nfixed-dimensional vectors and calculate the distances between keyword segments\nand searching content segments obtained in a sliding manner. An auxiliary\nvariability-invariant loss is also applied to training data within the same\nword but different speakers. This strategy is used to prevent the extractor\nfrom encoding undesired speaker- or accent-related information into the\nacoustic word embeddings. Experimental results show that our proposed system\nproduces promising searching results in the code-switching test scenario. With\nthe increased number of templates and the employment of variability-invariant\nloss, the searching performance is further enhanced.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:27:56 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ma", "Murong", ""], ["Wu", "Haiwei", ""], ["Wang", "Xuyang", ""], ["Yang", "Lin", ""], ["Wang", "Junjie", ""], ["Li", "Ming", ""]]}, {"id": "2005.11787", "submitter": "Nikolai Rozanov", "authors": "Anne Lauscher and Olga Majewska and Leonardo F. R. Ribeiro and Iryna\n  Gurevych and Nikolai Rozanov and Goran Glava\\v{s}", "title": "Common Sense or World Knowledge? Investigating Adapter-Based Knowledge\n  Injection into Pretrained Transformers", "comments": "EMNLP 2020 - DeeLIO, ECML 2020 - DECODEML, 5 pages, 4 tables, 3\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Following the major success of neural language models (LMs) such as BERT or\nGPT-2 on a variety of language understanding tasks, recent work focused on\ninjecting (structured) knowledge from external resources into these models.\nWhile on the one hand, joint pretraining (i.e., training from scratch, adding\nobjectives based on external knowledge to the primary LM objective) may be\nprohibitively computationally expensive, post-hoc fine-tuning on external\nknowledge, on the other hand, may lead to the catastrophic forgetting of\ndistributional knowledge. In this work, we investigate models for complementing\nthe distributional knowledge of BERT with conceptual knowledge from ConceptNet\nand its corresponding Open Mind Common Sense (OMCS) corpus, respectively, using\nadapter training. While overall results on the GLUE benchmark paint an\ninconclusive picture, a deeper analysis reveals that our adapter-based models\nsubstantially outperform BERT (up to 15-20 performance points) on inference\ntasks that require the type of conceptual knowledge explicitly present in\nConceptNet and OMCS. All code and experiments are open sourced under:\nhttps://github.com/wluper/retrograph .\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:49:57 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 11:31:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lauscher", "Anne", ""], ["Majewska", "Olga", ""], ["Ribeiro", "Leonardo F. R.", ""], ["Gurevych", "Iryna", ""], ["Rozanov", "Nikolai", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2005.11838", "submitter": "Aviad Elyashar", "authors": "Aviad Elyashar, Rami Puzis, Michael Fire", "title": "How Does That Sound? Multi-Language SpokenName2Vec Algorithm Using\n  Speech Generation and Deep Learning", "comments": "arXiv admin note: text overlap with arXiv:1912.04003", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for information about a specific person is an online activity\nfrequently performed by many users. In most cases, users are aided by queries\ncontaining a name and sending back to the web search engines for finding their\nwill. Typically, Web search engines provide just a few accurate results\nassociated with a name-containing query. Currently, most solutions for\nsuggesting synonyms in online search are based on pattern matching and phonetic\nencoding, however very often, the performance of such solutions is less than\noptimal. In this paper, we propose SpokenName2Vec, a novel and generic approach\nwhich addresses the similar name suggestion problem by utilizing automated\nspeech generation, and deep learning to produce spoken name embeddings. This\nsophisticated and innovative embeddings captures the way people pronounce names\nin any language and accent. Utilizing the name pronunciation can be helpful for\nboth differentiating and detecting names that sound alike, but are written\ndifferently. The proposed approach was demonstrated on a large-scale dataset\nconsisting of 250,000 forenames and evaluated using a machine learning\nclassifier and 7,399 names with their verified synonyms. The performance of the\nproposed approach was found to be superior to 10 other algorithms evaluated in\nthis study, including well used phonetic and string similarity algorithms, and\ntwo recently proposed algorithms. The results obtained suggest that the\nproposed approach could serve as a useful and valuable tool for solving the\nsimilar name suggestion problem.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 20:39:00 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 18:20:24 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Elyashar", "Aviad", ""], ["Puzis", "Rami", ""], ["Fire", "Michael", ""]]}, {"id": "2005.11849", "submitter": "Mamoru Komachi", "authors": "Satoru Katsumata and Mamoru Komachi", "title": "Stronger Baselines for Grammatical Error Correction Using Pretrained\n  Encoder-Decoder Model", "comments": "6 pages; AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studies on grammatical error correction (GEC) have reported the effectiveness\nof pretraining a Seq2Seq model with a large amount of pseudodata. However, this\napproach requires time-consuming pretraining for GEC because of the size of the\npseudodata. In this study, we explore the utility of bidirectional and\nauto-regressive transformers (BART) as a generic pretrained encoder-decoder\nmodel for GEC. With the use of this generic pretrained model for GEC, the\ntime-consuming pretraining can be eliminated. We find that monolingual and\nmultilingual BART models achieve high performance in GEC, with one of the\nresults being comparable to the current strong results in English GEC. Our\nimplementations are publicly available at GitHub\n(https://github.com/Katsumata420/generic-pretrained-GEC).\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 22:13:24 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 02:57:04 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Katsumata", "Satoru", ""], ["Komachi", "Mamoru", ""]]}, {"id": "2005.11861", "submitter": "Ha Nguyen", "authors": "Maha Elbayad, Ha Nguyen, Fethi Bougares, Natalia Tomashenko, Antoine\n  Caubri\\`ere, Benjamin Lecouteux, Yannick Est\\`eve, Laurent Besacier", "title": "ON-TRAC Consortium for End-to-End and Simultaneous Speech Translation\n  Challenge Tasks at IWSLT 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the ON-TRAC Consortium translation systems developed for\ntwo challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline\nspeech translation and simultaneous speech translation. ON-TRAC Consortium is\ncomposed of researchers from three French academic laboratories: LIA (Avignon\nUniversit\\'e), LIG (Universit\\'e Grenoble Alpes), and LIUM (Le Mans\nUniversit\\'e). Attention-based encoder-decoder models, trained end-to-end, were\nused for our submissions to the offline speech translation track. Our\ncontributions focused on data augmentation and ensembling of multiple models.\nIn the simultaneous speech translation track, we build on Transformer-based\nwait-k models for the text-to-text subtask. For speech-to-text simultaneous\ntranslation, we attach a wait-k MT system to a hybrid ASR system. We propose an\nalgorithm to control the latency of the ASR+MT cascade and achieve a good\nlatency-quality trade-off on both subtasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 23:44:45 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Elbayad", "Maha", ""], ["Nguyen", "Ha", ""], ["Bougares", "Fethi", ""], ["Tomashenko", "Natalia", ""], ["Caubri\u00e8re", "Antoine", ""], ["Lecouteux", "Benjamin", ""], ["Est\u00e8ve", "Yannick", ""], ["Besacier", "Laurent", ""]]}, {"id": "2005.11882", "submitter": "Saif M. Mohammad Dr.", "authors": "Saif M. Mohammad", "title": "Sentiment Analysis: Automatically Detecting Valence, Emotions, and Other\n  Affectual States from Text", "comments": "This is the author's manuscript of what is slated to appear in the\n  Second Edition of Emotion Measurement, 2021", "journal-ref": "Second Edition of Emotion Measurement, 2021", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in machine learning have led to computer systems that are\nhuman-like in behaviour. Sentiment analysis, the automatic determination of\nemotions in text, is allowing us to capitalize on substantial previously\nunattainable opportunities in commerce, public health, government policy,\nsocial sciences, and art. Further, analysis of emotions in text, from news to\nsocial media posts, is improving our understanding of not just how people\nconvey emotions through language but also how emotions shape our behaviour.\nThis article presents a sweeping overview of sentiment analysis research that\nincludes: the origins of the field, the rich landscape of tasks, challenges, a\nsurvey of the methods and resources used, and applications. We also discuss\ndiscuss how, without careful fore-thought, sentiment analysis has the potential\nfor harmful outcomes. We outline the latest lines of research in pursuit of\nfairness in sentiment analysis.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 01:37:31 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 03:18:48 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Mohammad", "Saif M.", ""]]}, {"id": "2005.11888", "submitter": "Dongjun Wei", "authors": "Dongjun Wei and Yaxin Liu and Fuqing Zhu and Liangjun Zang and Wei\n  Zhou and Yijun Lu and Songlin Hu", "title": "AutoSUM: Automating Feature Extraction and Multi-user Preference\n  Simulation for Entity Summarization", "comments": "11 pages, accepted in PAKDD'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Withthegrowthofknowledgegraphs, entity descriptions are becoming extremely\nlengthy. Entity summarization task, aiming to generate diverse, comprehensive,\nand representative summaries for entities, has received increasing interest\nrecently. In most previous methods, features are usually extracted by the\nhandcrafted templates. Then the feature selection and multi-user preference\nsimulation take place, depending too much on human expertise. In this paper, a\nnovel integration method called AutoSUM is proposed for automatic feature\nextraction and multi-user preference simulation to overcome the drawbacks of\nprevious methods. There are two modules in AutoSUM: extractor and simulator.\nThe extractor module operates automatic feature extraction based on a BiLSTM\nwith a combined input representation including word embeddings and graph\nembeddings. Meanwhile, the simulator module automates multi-user preference\nsimulation based on a well-designed two-phase attention mechanism (i.e.,\nentity-phase attention and user-phase attention). Experimental results\ndemonstrate that AutoSUM produces state-of-the-art performance on two widely\nused datasets (i.e., DBpedia and LinkedMDB) in both F-measure and MAP.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 02:20:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wei", "Dongjun", ""], ["Liu", "Yaxin", ""], ["Zhu", "Fuqing", ""], ["Zang", "Liangjun", ""], ["Zhou", "Wei", ""], ["Lu", "Yijun", ""], ["Hu", "Songlin", ""]]}, {"id": "2005.11950", "submitter": "Bi-Cheng Yan", "authors": "Bi-Cheng Yan, Meng-Che Wu, Hsiao-Tsung Hung, Berlin Chen", "title": "An End-to-End Mispronunciation Detection System for L2 English Speech\n  Leveraging Novel Anti-Phone Modeling", "comments": "Accepted by Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mispronunciation detection and diagnosis (MDD) is a core component of\ncomputer-assisted pronunciation training (CAPT). Most of the existing MDD\napproaches focus on dealing with categorical errors (viz. one canonical phone\nis substituted by another one, aside from those mispronunciations caused by\ndeletions or insertions). However, accurate detection and diagnosis of\nnon-categorial or distortion errors (viz. approximating L2 phones with L1\n(first-language) phones, or erroneous pronunciations in between) still seems\nout of reach. In view of this, we propose to conduct MDD with a novel end-\nto-end automatic speech recognition (E2E-based ASR) approach. In particular, we\nexpand the original L2 phone set with their corresponding anti-phone set,\nmaking the E2E-based MDD approach have a better capability to take in both\ncategorical and non-categorial mispronunciations, aiming to provide better\nmispronunciation detection and diagnosis feedback. Furthermore, a novel\ntransfer-learning paradigm is devised to obtain the initial model estimate of\nthe E2E-based MDD system without resource to any phonological rules. Extensive\nsets of experimental results on the L2-ARCTIC dataset show that our best system\ncan outperform the existing E2E baseline system and pronunciation scoring based\nmethod (GOP) in terms of the F1-score, by 11.05% and 27.71%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 07:27:47 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 07:41:49 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Yan", "Bi-Cheng", ""], ["Wu", "Meng-Che", ""], ["Hung", "Hsiao-Tsung", ""], ["Chen", "Berlin", ""]]}, {"id": "2005.11988", "submitter": "Pirmin Lemberger", "authors": "Pirmin Lemberger", "title": "Deep Learning Models for Automatic Summarization", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization is an NLP task which aims to convert a textual document\ninto a shorter one while keeping as much meaning as possible. This pedagogical\narticle reviews a number of recent Deep Learning architectures that have helped\nto advance research in this field. We will discuss in particular applications\nof pointer networks, hierarchical Transformers and Reinforcement Learning. We\nassume basic knowledge of Seq2Seq architecture and Transformer networks within\nNLP.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:12:37 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lemberger", "Pirmin", ""]]}, {"id": "2005.11996", "submitter": "Hannah Chen", "authors": "Hannah Chen, Yangfeng Ji, David Evans", "title": "Pointwise Paraphrase Appraisal is Potentially Problematic", "comments": "ACL 2020 Student Research Workshop", "journal-ref": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics: Student Research Workshop (2020) 150-155", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevailing approach for training and evaluating paraphrase identification\nmodels is constructed as a binary classification problem: the model is given a\npair of sentences, and is judged by how accurately it classifies pairs as\neither paraphrases or non-paraphrases. This pointwise-based evaluation method\ndoes not match well the objective of most real world applications, so the goal\nof our work is to understand how models which perform well under pointwise\nevaluation may fail in practice and find better methods for evaluating\nparaphrase identification models. As a first step towards that goal, we show\nthat although the standard way of fine-tuning BERT for paraphrase\nidentification by pairing two sentences as one sequence results in a model with\nstate-of-the-art performance, that model may perform poorly on simple tasks\nlike identifying pairs with two identical sentences. Moreover, we show that\nthese models may even predict a pair of randomly-selected sentences with higher\nparaphrase score than a pair of identical ones.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:27:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 03:18:28 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Chen", "Hannah", ""], ["Ji", "Yangfeng", ""], ["Evans", "David", ""]]}, {"id": "2005.12040", "submitter": "Georgios Sidiropoulos", "authors": "Georgios Sidiropoulos, Nikos Voskarides and Evangelos Kanoulas", "title": "Knowledge Graph Simple Question Answering for Unseen Domains", "comments": "Accepted at AKBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph simple question answering (KGSQA), in its standard form, does\nnot take into account that human-curated question answering training data only\ncover a small subset of the relations that exist in a Knowledge Graph (KG), or\neven worse, that new domains covering unseen and rather different to existing\ndomains relations are added to the KG. In this work, we study KGSQA in a\npreviously unstudied setting where new, unseen domains are added during test\ntime. In this setting, question-answer pairs of the new domain do not appear\nduring training, thus making the task more challenging. We propose a\ndata-centric domain adaptation framework that consists of a KGSQA system that\nis applicable to new domains, and a sequence to sequence question generation\nmethod that automatically generates question-answer pairs for the new domain.\nSince the effectiveness of question generation for KGSQA can be restricted by\nthe limited lexical variety of the generated questions, we use distant\nsupervision to extract a set of keywords that express each relation of the\nunseen domain and incorporate those in the question generation method.\nExperimental results demonstrate that our framework significantly improves over\nzero-shot baselines and is robust across domains.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 11:34:54 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Sidiropoulos", "Georgios", ""], ["Voskarides", "Nikos", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2005.12078", "submitter": "Sandeep Mathias", "authors": "Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra,\n  Pushpak Bhattacharyya", "title": "Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach\n  to Grade Essays Using Gaze Behaviour", "comments": "This paper was accepted for publication at AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gaze behaviour of a reader is helpful in solving several NLP tasks such\nas automatic essay grading. However, collecting gaze behaviour from readers is\ncostly in terms of time and money. In this paper, we propose a way to improve\nautomatic essay grading using gaze behaviour, which is learnt at run time using\na multi-task learning framework. To demonstrate the efficacy of this multi-task\nlearning based approach to automatic essay grading, we collect gaze behaviour\nfor 48 essays across 4 essay sets, and learn gaze behaviour for the rest of the\nessays, numbering over 7000 essays. Using the learnt gaze behaviour, we can\nachieve a statistically significant improvement in performance over the\nstate-of-the-art system for the essay sets where we have gaze data. We also\nachieve a statistically significant improvement for 4 other essay sets,\nnumbering about 6000 essays, where we have no gaze behaviour data available.\nOur approach establishes that learning gaze behaviour improves automatic essay\ngrading.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:38:47 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 11:43:00 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Mathias", "Sandeep", ""], ["Murthy", "Rudra", ""], ["Kanojia", "Diptesh", ""], ["Mishra", "Abhijit", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "2005.12086", "submitter": "Joosung Lee", "authors": "Joosung Lee", "title": "Stable Style Transformer: Delete and Generate Approach with\n  Encoder-Decoder for Text Style Transfer", "comments": "10 pages, 3 figures, INLG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text style transfer is the task that generates a sentence by preserving the\ncontent of the input sentence and transferring the style. Most existing studies\nare progressing on non-parallel datasets because parallel datasets are limited\nand hard to construct. In this work, we introduce a method that follows two\nstages in non-parallel datasets. The first stage is to delete attribute markers\nof a sentence directly through a classifier. The second stage is to generate a\ntransferred sentence by combining the content tokens and the target style. We\nexperiment on two benchmark datasets and evaluate context, style, fluency, and\nsemantic. It is difficult to select the best system using only these automatic\nmetrics, but it is possible to select stable systems. We consider only robust\nsystems in all automatic evaluation metrics to be the minimum conditions that\ncan be used in real applications. Many previous systems are difficult to use in\ncertain situations because performance is significantly lower in several\nevaluation metrics. However, our system is stable in all automatic evaluation\nmetrics and has results comparable to other models. Also, we compare the\nperformance results of our system and the unstable system through human\nevaluation. Our code and data are available at the link\n(https://github.com/rungjoo/Stable-Style-Transformer).\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:04:54 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 08:04:13 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 09:14:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lee", "Joosung", ""]]}, {"id": "2005.12094", "submitter": "Daniel Hershcovich", "authors": "Daniel Hershcovich, Miryam de Lhoneux, Artur Kulmizev, Elham Pejhan,\n  Joakim Nivre", "title": "K{\\o}psala: Transition-Based Graph Parsing via Efficient Training and\n  Effective Encoding", "comments": "IWPT shared task 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present K{\\o}psala, the Copenhagen-Uppsala system for the Enhanced\nUniversal Dependencies Shared Task at IWPT 2020. Our system is a pipeline\nconsisting of off-the-shelf models for everything but enhanced graph parsing,\nand for the latter, a transition-based graph parser adapted from Che et al.\n(2019). We train a single enhanced parser model per language, using gold\nsentence splitting and tokenization for training, and rely only on tokenized\nsurface forms and multilingual BERT for encoding. While a bug introduced just\nbefore submission resulted in a severe drop in precision, its post-submission\nfix would bring us to 4th place in the official ranking, according to average\nELAS. Our parser demonstrates that a unified pipeline is effective for both\nMeaning Representation Parsing and Enhanced Universal Dependencies.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:17:09 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 11:45:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Hershcovich", "Daniel", ""], ["de Lhoneux", "Miryam", ""], ["Kulmizev", "Artur", ""], ["Pejhan", "Elham", ""], ["Nivre", "Joakim", ""]]}, {"id": "2005.12116", "submitter": "Sawan Kumar", "authors": "Sawan Kumar and Partha Talukdar", "title": "NILE : Natural Language Inference with Faithful Natural Language\n  Explanations", "comments": "13 pages, 3 figures, Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent growth in the popularity and success of deep learning models on\nNLP classification tasks has accompanied the need for generating some form of\nnatural language explanation of the predicted labels. Such generated natural\nlanguage (NL) explanations are expected to be faithful, i.e., they should\ncorrelate well with the model's internal decision making. In this work, we\nfocus on the task of natural language inference (NLI) and address the following\nquestion: can we build NLI systems which produce labels with high accuracy,\nwhile also generating faithful explanations of its decisions? We propose\nNatural-language Inference over Label-specific Explanations (NILE), a novel NLI\nmethod which utilizes auto-generated label-specific NL explanations to produce\nlabels along with its faithful explanation. We demonstrate NILE's effectiveness\nover previously reported methods through automated and human evaluation of the\nproduced labels and explanations. Our evaluation of NILE also supports the\nclaim that accurate systems capable of providing testable explanations of their\ndecisions can be designed. We discuss the faithfulness of NILE's explanations\nin terms of sensitivity of the decisions to the corresponding explanations. We\nargue that explicit evaluation of faithfulness, in addition to label and\nexplanation accuracy, is an important step in evaluating model's explanations.\nFurther, we demonstrate that task-specific probes are necessary to establish\nsuch sensitivity.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:56:03 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kumar", "Sawan", ""], ["Talukdar", "Partha", ""]]}, {"id": "2005.12132", "submitter": "Weixin Zeng", "authors": "Weixin Zeng, Xiang Zhao, Wei Wang, Jiuyang Tang, and Zhen Tan", "title": "Degree-Aware Alignment for Entities in Tail", "comments": "Accepted by SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) is to discover equivalent entities in knowledge graphs\n(KGs), which bridges heterogeneous sources of information and facilitates the\nintegration of knowledge. Existing EA solutions mainly rely on structural\ninformation to align entities, typically through KG embedding. Nonetheless, in\nreal-life KGs, only a few entities are densely connected to others, and the\nrest majority possess rather sparse neighborhood structure. We refer to the\nlatter as long-tail entities, and observe that such phenomenon arguably limits\nthe use of structural information for EA. To mitigate the issue, we revisit and\ninvestigate into the conventional EA pipeline in pursuit of elegant\nperformance. For pre-alignment, we propose to amplify long-tail entities, which\nare of relatively weak structural information, with entity name information\nthat is generally available (but overlooked) in the form of concatenated power\nmean word embeddings. For alignment, under a novel complementary framework of\nconsolidating structural and name signals, we identify entity's degree as\nimportant guidance to effectively fuse two different sources of information. To\nthis end, a degree-aware co-attention network is conceived, which dynamically\nadjusts the significance of features in a degree-aware manner. For\npost-alignment, we propose to complement original KGs with facts from their\ncounterparts by using confident EA results as anchors via iterative training.\nComprehensive experimental evaluations validate the superiority of our proposed\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:15:49 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zeng", "Weixin", ""], ["Zhao", "Xiang", ""], ["Wang", "Wei", ""], ["Tang", "Jiuyang", ""], ["Tan", "Zhen", ""]]}, {"id": "2005.12142", "submitter": "Kuan-Yu Chen", "authors": "Chia-Chih Kuo, Shang-Bao Luo, Kuan-Yu Chen", "title": "An Audio-enriched BERT-based Framework for Spoken Multiple-choice\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a spoken multiple-choice question answering (SMCQA) task, given a passage,\na question, and multiple choices all in the form of speech, the machine needs\nto pick the correct choice to answer the question. While the audio could\ncontain useful cues for SMCQA, usually only the auto-transcribed text is\nutilized in system development. Thanks to the large-scaled pre-trained language\nrepresentation models, such as the bidirectional encoder representations from\ntransformers (BERT), systems with only auto-transcribed text can still achieve\na certain level of performance. However, previous studies have evidenced that\nacoustic-level statistics can offset text inaccuracies caused by the automatic\nspeech recognition systems or representation inadequacy lurking in word\nembedding generators, thereby making the SMCQA system robust. Along the line of\nresearch, this study concentrates on designing a BERT-based SMCQA framework,\nwhich not only inherits the advantages of contextualized language\nrepresentations learned by BERT, but integrates the complementary\nacoustic-level information distilled from audio with the text-level\ninformation. Consequently, an audio-enriched BERT-based SMCQA framework is\nproposed. A series of experiments demonstrates remarkable improvements in\naccuracy over selected baselines and SOTA systems on a published Chinese SMCQA\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:41:28 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kuo", "Chia-Chih", ""], ["Luo", "Shang-Bao", ""], ["Chen", "Kuan-Yu", ""]]}, {"id": "2005.12143", "submitter": "Danni Liu", "authors": "Danni Liu, Jan Niehues, Gerasimos Spanakis", "title": "Adapting End-to-End Speech Recognition for Readable Subtitles", "comments": "IWSLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition (ASR) systems are primarily evaluated on\ntranscription accuracy. However, in some use cases such as subtitling, verbatim\ntranscription would reduce output readability given limited screen size and\nreading time. Therefore, this work focuses on ASR with output compression, a\ntask challenging for supervised approaches due to the scarcity of training\ndata. We first investigate a cascaded system, where an unsupervised compression\nmodel is used to post-edit the transcribed speech. We then compare several\nmethods of end-to-end speech recognition under output length constraints. The\nexperiments show that with limited data far less than needed for training a\nmodel from scratch, we can adapt a Transformer-based ASR model to incorporate\nboth transcription and compression capabilities. Furthermore, the best\nperformance in terms of WER and ROUGE scores is achieved by explicitly modeling\nthe length constraints within the end-to-end ASR system.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:42:26 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Liu", "Danni", ""], ["Niehues", "Jan", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "2005.12147", "submitter": "Mayank Singh", "authors": "Mayank Kumar Singh, Sayan Banerjee, Shubhasis Chaudhuri", "title": "NENET: An Edge Learnable Network for Link Prediction in Scene Text", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text detection in scenes based on deep neural networks have shown promising\nresults. Instead of using word bounding box regression, recent state-of-the-art\nmethods have started focusing on character bounding box and pixel-level\nprediction. This necessitates the need to link adjacent characters, which we\npropose in this paper using a novel Graph Neural Network (GNN) architecture\nthat allows us to learn both node and edge features as opposed to only the node\nfeatures under the typical GNN. The main advantage of using GNN for link\nprediction lies in its ability to connect characters which are spatially\nseparated and have an arbitrary orientation. We show our concept on the well\nknown SynthText dataset, achieving top results as compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:47:16 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Singh", "Mayank Kumar", ""], ["Banerjee", "Sayan", ""], ["Chaudhuri", "Shubhasis", ""]]}, {"id": "2005.12187", "submitter": "Juri Opitz", "authors": "Juri Opitz", "title": "AMR Quality Rating with a Lightweight CNN", "comments": "AACL-IJCNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured semantic sentence representations such as Abstract Meaning\nRepresentations (AMRs) are potentially useful in various NLP tasks. However,\nthe quality of automatic parses can vary greatly and jeopardizes their\nusefulness. This can be mitigated by models that can accurately rate AMR\nquality in the absence of costly gold data, allowing us to inform downstream\nsystems about an incorporated parse's trustworthiness or select among different\ncandidate parses.\n  In this work, we propose to transfer the AMR graph to the domain of images.\nThis allows us to create a simple convolutional neural network (CNN) that\nimitates a human judge tasked with rating graph quality. Our experiments show\nthat the method can rate quality more accurately than strong baselines, in\nseveral quality dimensions. Moreover, the method proves to be efficient and\nreduces the incurred energy consumption.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:58:00 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 17:15:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Opitz", "Juri", ""]]}, {"id": "2005.12240", "submitter": "Oumaima Oueslati", "authors": "Oumaima Oueslati, Erik Cambria, Moez Ben HajHmida, and Habib Ounelli", "title": "A review of sentiment analysis research in Arabic language", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2020.05.034", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a task of natural language processing which has\nrecently attracted increasing attention. However, sentiment analysis research\nhas mainly been carried out for the English language. Although Arabic is\nramping up as one of the most used languages on the Internet, only a few\nstudies have focused on Arabic sentiment analysis so far. In this paper, we\ncarry out an in-depth qualitative study of the most important research works in\nthis context by presenting limits and strengths of existing approaches. In\nparticular, we survey both approaches that leverage machine translation or\ntransfer learning to adapt English resources to Arabic and approaches that stem\ndirectly from the Arabic language.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 17:26:02 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Oueslati", "Oumaima", ""], ["Cambria", "Erik", ""], ["HajHmida", "Moez Ben", ""], ["Ounelli", "Habib", ""]]}, {"id": "2005.12246", "submitter": "Mengzhou Xia", "authors": "Mengzhou Xia, Anjalie Field, Yulia Tsvetkov", "title": "Demoting Racial Bias in Hate Speech Detection", "comments": "Accepted at SocialNLP Workshop @ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current hate speech datasets, there exists a high correlation between\nannotators' perceptions of toxicity and signals of African American English\n(AAE). This bias in annotated training data and the tendency of machine\nlearning models to amplify it cause AAE text to often be mislabeled as\nabusive/offensive/hate speech with a high false positive rate by current hate\nspeech classifiers. In this paper, we use adversarial training to mitigate this\nbias, introducing a hate speech classifier that learns to detect toxic\nsentences while demoting confounds corresponding to AAE texts. Experimental\nresults on a hate speech dataset and an AAE dataset suggest that our method is\nable to substantially reduce the false positive rate for AAE text while only\nminimally affecting the performance of hate speech classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 17:43:22 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Xia", "Mengzhou", ""], ["Field", "Anjalie", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2005.12339", "submitter": "Dan Roth", "authors": "Dan Roth", "title": "Incidental Supervision: Moving beyond Supervised Learning", "comments": "6 pages, 1 figure. Appeared in AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning and Inference methods have become ubiquitous in our attempt\nto induce more abstract representations of natural language text, visual\nscenes, and other messy, naturally occurring data, and support decisions that\ndepend on it. However, learning models for these tasks is difficult partly\nbecause generating the necessary supervision signals for it is costly and does\nnot scale. This paper describes several learning paradigms that are designed to\nalleviate the supervision bottleneck. It will illustrate their benefit in the\ncontext of multiple problems, all pertaining to inducing various levels of\nsemantic representations from text.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 18:44:53 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Roth", "Dan", ""]]}, {"id": "2005.12368", "submitter": "Marija Stepanovi\\'c", "authors": "Andreas Kirkedal, Marija Stepanovi\\'c, Barbara Plank", "title": "FT Speech: Danish Parliament Speech Corpus", "comments": "Accepted at Interspeech 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-3164", "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces FT Speech, a new speech corpus created from the\nrecorded meetings of the Danish Parliament, otherwise known as the Folketing\n(FT). The corpus contains over 1,800 hours of transcribed speech by a total of\n434 speakers. It is significantly larger in duration, vocabulary, and amount of\nspontaneous speech than the existing public speech corpora for Danish, which\nare largely limited to read-aloud and dictation data. We outline design\nconsiderations, including the preprocessing methods and the alignment\nprocedure. To evaluate the quality of the corpus, we train automatic speech\nrecognition systems on the new resource and compare them to the systems trained\non the Danish part of Spr\\r{a}kbanken, the largest public ASR corpus for Danish\nto date. Our baseline results show that we achieve a 14.01 WER on the new\ncorpus. A combination of FT Speech with in-domain language data provides\ncomparable results to models trained specifically on Spr\\r{a}kbanken, showing\nthat FT Speech transfers well to this data set. Interestingly, our results\ndemonstrate that the opposite is not the case. This shows that FT Speech\nprovides a valuable resource for promoting research on Danish ASR with more\nspontaneous speech.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 19:51:18 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 13:36:44 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kirkedal", "Andreas", ""], ["Stepanovi\u0107", "Marija", ""], ["Plank", "Barbara", ""]]}, {"id": "2005.12385", "submitter": "Ning Wang", "authors": "Ning Wang, Fan Luo, Vishal Peddagangireddy, K.P. Subbalakshmi and R.\n  Chandramouli", "title": "Personalized Early Stage Alzheimer's Disease Detection: A Case Study of\n  President Reagan's Speeches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer`s disease (AD)-related global healthcare cost is estimated to be $1\ntrillion by 2050. Currently, there is no cure for this disease; however,\nclinical studies show that early diagnosis and intervention helps to extend the\nquality of life and inform technologies for personalized mental healthcare.\nClinical research indicates that the onset and progression of Alzheimer`s\ndisease lead to dementia and other mental health issues. As a result, the\nlanguage capabilities of patient start to decline. In this paper, we show that\nmachine learning-based unsupervised clustering of and anomaly detection with\nlinguistic biomarkers are promising approaches for intuitive visualization and\npersonalized early stage detection of Alzheimer`s disease. We demonstrate this\napproach on 10 year`s (1980 to 1989) of President Ronald Reagan`s speech data\nset. Key linguistic biomarkers that indicate early-stage AD are identified.\nExperimental results show that Reagan had early onset of Alzheimer`s sometime\nbetween 1983 and 1987. This finding is corroborated by prior work that analyzed\nhis interviews using a statistical technique. The proposed technique also\nidentifies the exact speeches that reflect linguistic biomarkers for early\nstage AD.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:26:52 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Ning", ""], ["Luo", "Fan", ""], ["Peddagangireddy", "Vishal", ""], ["Subbalakshmi", "K. P.", ""], ["Chandramouli", "R.", ""]]}, {"id": "2005.12398", "submitter": "Marzieh Fadaee", "authors": "Marzieh Fadaee and Christof Monz", "title": "The Unreasonable Volatility of Neural Machine Translation Models", "comments": "Accepted to Neural Generation and Translation Workshop (WNGT) at ACL\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that Neural Machine Translation (NMT) models achieve\nimpressive performance, however, questions about understanding the behavior of\nthese models remain unanswered. We investigate the unexpected volatility of NMT\nmodels where the input is semantically and syntactically correct. We discover\nthat with trivial modifications of source sentences, we can identify cases\nwhere \\textit{unexpected changes} happen in the translation and in the worst\ncase lead to mistranslations. This volatile behavior of translating extremely\nsimilar sentences in surprisingly different ways highlights the underlying\ngeneralization problem of current NMT models. We find that both RNN and\nTransformer models display volatile behavior in 26% and 19% of sentence\nvariations, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 20:54:23 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Fadaee", "Marzieh", ""], ["Monz", "Christof", ""]]}, {"id": "2005.12411", "submitter": "Manuel Mager", "authors": "Manuel Mager and Katharina Kann", "title": "The IMS-CUBoulder System for the SIGMORPHON 2020 Shared Task on\n  Unsupervised Morphological Paradigm Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we present the systems of the University of Stuttgart IMS and\nthe University of Colorado Boulder (IMS-CUBoulder) for SIGMORPHON 2020 Task 2\non unsupervised morphological paradigm completion (Kann et al., 2020). The task\nconsists of generating the morphological paradigms of a set of lemmas, given\nonly the lemmas themselves and unlabeled text. Our proposed system is a\nmodified version of the baseline introduced together with the task. In\nparticular, we experiment with substituting the inflection generation component\nwith an LSTM sequence-to-sequence model and an LSTM pointer-generator network.\nOur pointer-generator system obtains the best score of all seven submitted\nsystems on average over all languages, and outperforms the official baseline,\nwhich was best overall, on Bulgarian and Kannada.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 21:23:52 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Mager", "Manuel", ""], ["Kann", "Katharina", ""]]}, {"id": "2005.12423", "submitter": "Srijan Kumar", "authors": "Caleb Ziems, Bing He, Sandeep Soni, Srijan Kumar", "title": "Racism is a Virus: Anti-Asian Hate and Counterhate in Social Media\n  during the COVID-19 Crisis", "comments": "The COVID-HATE dataset, classifier, and demo are available at\n  http://claws.cc.gatech.edu/covid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of COVID-19 has sparked racism, hate, and xenophobia in social\nmedia targeted at Chinese and broader Asian communities. However, little is\nknown about how racial hate spreads during a pandemic and the role of\ncounterhate speech in mitigating the spread. Here we study the evolution and\nspread of anti-Asian hate speech through the lens of Twitter. We create\nCOVID-HATE, the largest dataset of anti-Asian hate and counterhate spanning\nthree months, containing over 30 million tweets, and a social network with over\n87 million nodes. By creating a novel hand-labeled dataset of 2,400 tweets, we\ntrain a text classifier to identify hate and counterhate tweets that achieves\nan average AUROC of 0.852. We identify 891,204 hate and 200,198 counterhate\ntweets in COVID-HATE. Using this data to conduct longitudinal analysis, we find\nthat while hateful users are less engaged in the COVID-19 discussions prior to\ntheir first anti-Asian tweet, they become more vocal and engaged afterwards\ncompared to counterhate users. We find that bots comprise 10.4% of hateful\nusers and are more vocal and hateful compared to non-bot users. Comparing bot\naccounts, we show that hateful bots are more successful in attracting followers\ncompared to counterhate bots. Analysis of the social network reveals that\nhateful and counterhate users interact and engage extensively with one another,\ninstead of living in isolated polarized communities. Furthermore, we find that\nhate is contagious and nodes are highly likely to become hateful after being\nexposed to hateful content. Importantly, our analysis reveals that counterhate\nmessages can discourage users from turning hateful in the first place. Overall,\nthis work presents a comprehensive overview of anti-Asian hate and counterhate\ncontent during a pandemic. The COVID-HATE dataset is available at\nhttp://claws.cc.gatech.edu/covid.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 21:58:09 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Ziems", "Caleb", ""], ["He", "Bing", ""], ["Soni", "Sandeep", ""], ["Kumar", "Srijan", ""]]}, {"id": "2005.12443", "submitter": "Marcos Zampieri", "authors": "Farhad Akhbardeh, Travis Desell, Marcos Zampieri", "title": "MaintNet: A Collaborative Open-Source Library for Predictive Maintenance\n  Language Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintenance record logbooks are an emerging text type in NLP. They typically\nconsist of free text documents with many domain specific technical terms,\nabbreviations, as well as non-standard spelling and grammar, which poses\ndifficulties to NLP pipelines trained on standard corpora. Analyzing and\nannotating such documents is of particular importance in the development of\npredictive maintenance systems, which aim to provide operational efficiencies,\nprevent accidents and save lives. In order to facilitate and encourage research\nin this area, we have developed MaintNet, a collaborative open-source library\nof technical and domain-specific language datasets. MaintNet provides novel\nlogbook data from the aviation, automotive, and facilities domains along with\ntools to aid in their (pre-)processing and clustering. Furthermore, it provides\na way to encourage discussion on and sharing of new datasets and tools for\nlogbook data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 23:44:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Akhbardeh", "Farhad", ""], ["Desell", "Travis", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2005.12484", "submitter": "Yifan Gao", "authors": "Yifan Gao, Chien-Sheng Wu, Shafiq Joty, Caiming Xiong, Richard Socher,\n  Irwin King, Michael R. Lyu, and Steven C.H. Hoi", "title": "Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational\n  Machine Reading", "comments": "ACL 2020, 11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of conversational machine reading is to answer user questions given\na knowledge base text which may require asking clarification questions.\nExisting approaches are limited in their decision making due to struggles in\nextracting question-related rules and reasoning about them. In this paper, we\npresent a new framework of conversational machine reading that comprises a\nnovel Explicit Memory Tracker (EMT) to track whether conditions listed in the\nrule text have already been satisfied to make a decision. Moreover, our\nframework generates clarification questions by adopting a coarse-to-fine\nreasoning strategy, utilizing sentence-level entailment scores to weight\ntoken-level distributions. On the ShARC benchmark (blind, held-out) testset,\nEMT achieves new state-of-the-art results of 74.6% micro-averaged decision\naccuracy and 49.5 BLEU4. We also show that EMT is more interpretable by\nvisualizing the entailment-oriented reasoning process as the conversation\nflows. Code and models are released at\nhttps://github.com/Yifan-Gao/explicit_memory_tracker.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 02:21:31 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 06:14:38 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Gao", "Yifan", ""], ["Wu", "Chien-Sheng", ""], ["Joty", "Shafiq", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2005.12501", "submitter": "Benjamin Kane", "authors": "Benjamin Kane, Georgiy Platonov, and Lenhart K. Schubert", "title": "History-Aware Question Answering in a Blocks World Dialogue System", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential for dialogue-based spatial reasoning systems to maintain\nmemory of historical states of the world. In addition to conveying that the\ndialogue agent is mentally present and engaged with the task, referring to\nhistorical states may be crucial for enabling collaborative planning (e.g., for\nplanning to return to a previous state, or diagnosing a past misstep). In this\npaper, we approach the problem of spatial memory in a multi-modal spoken\ndialogue system capable of answering questions about interaction history in a\nphysical blocks world setting. This work builds upon a full spatial\nquestion-answering pipeline consisting of a vision system, speech input and\noutput mediated by an animated avatar, a dialogue system that robustly\ninterprets spatial queries, and a constraint solver that derives answers based\non 3-D spatial modelling. The contributions of this work include a symbolic\ndialogue context registering knowledge about discourse history and changes in\nthe world, as well as a natural language understanding module capable of\ninterpreting free-form historical questions and querying the dialogue context\nto form an answer.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 03:16:11 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kane", "Benjamin", ""], ["Platonov", "Georgiy", ""], ["Schubert", "Lenhart K.", ""]]}, {"id": "2005.12503", "submitter": "Jihyung Moon", "authors": "Jihyung Moon, Won Ik Cho, Junbum Lee", "title": "BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection", "comments": "To be published in SocialNLP@ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Toxic comments in online platforms are an unavoidable social issue under the\ncloak of anonymity. Hate speech detection has been actively done for languages\nsuch as English, German, or Italian, where manually labeled corpus has been\nreleased. In this work, we first present 9.4K manually labeled entertainment\nnews comments for identifying Korean toxic speech, collected from a widely used\nonline news platform in Korea. The comments are annotated regarding social bias\nand hate speech since both aspects are correlated. The inter-annotator\nagreement Krippendorff's alpha score is 0.492 and 0.496, respectively. We\nprovide benchmarks using CharCNN, BiLSTM, and BERT, where BERT achieves the\nhighest score on all tasks. The models generally display better performance on\nbias identification, since the hate speech detection is a more subjective\nissue. Additionally, when BERT is trained with bias label for hate speech\ndetection, the prediction score increases, implying that bias and hate are\nintertwined. We make our dataset publicly available and open competitions with\nthe corpus and benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 03:34:01 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Moon", "Jihyung", ""], ["Cho", "Won Ik", ""], ["Lee", "Junbum", ""]]}, {"id": "2005.12515", "submitter": "Mehrdad Farahani", "authors": "Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad\n  Manthouri", "title": "ParsBERT: Transformer-based Model for Persian Language Understanding", "comments": "10 pages, 5 figures, 7 tables, table 7 corrected and some refs\n  related to table 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surge of pre-trained language models has begun a new era in the field of\nNatural Language Processing (NLP) by allowing us to build powerful language\nmodels. Among these models, Transformer-based models such as BERT have become\nincreasingly popular due to their state-of-the-art performance. However, these\nmodels are usually focused on English, leaving other languages to multilingual\nmodels with limited resources. This paper proposes a monolingual BERT for the\nPersian language (ParsBERT), which shows its state-of-the-art performance\ncompared to other architectures and multilingual models. Also, since the amount\nof data available for NLP tasks in Persian is very restricted, a massive\ndataset for different NLP tasks as well as pre-training the model is composed.\nParsBERT obtains higher scores in all datasets, including existing ones as well\nas composed ones and improves the state-of-the-art performance by outperforming\nboth multilingual BERT and other prior works in Sentiment Analysis, Text\nClassification and Named Entity Recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 05:05:32 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 13:09:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Farahani", "Mehrdad", ""], ["Gharachorloo", "Mohammad", ""], ["Farahani", "Marzieh", ""], ["Manthouri", "Mohammad", ""]]}, {"id": "2005.12522", "submitter": "Jerry Wei", "authors": "Jerry Wei, Chengyu Huang, Soroush Vosoughi, Jason Wei", "title": "What Are People Asking About COVID-19? A Question Classification Dataset", "comments": "Published in Proceedings of the 1st Workshop on NLP for COVID-19 at\n  ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources,\nwhich we annotate into 15 question categories and 207 question clusters. The\nmost common questions in our dataset asked about transmission, prevention, and\nsocietal effects of COVID, and we found that many questions that appeared in\nmultiple sources were not answered by any FAQ websites of reputable\norganizations such as the CDC and FDA. We post our dataset publicly at\nhttps://github.com/JerryWei03/COVID-Q. For classifying questions into 15\ncategories, a BERT baseline scored 58.1% accuracy when trained on 20 examples\nper category, and for a question clustering task, a BERT + triplet loss\nbaseline achieved 49.5% accuracy. We hope COVID-Q can help either for direct\nuse in developing applied systems or as a domain-specific resource for model\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 05:41:58 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 01:16:53 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wei", "Jerry", ""], ["Huang", "Chengyu", ""], ["Vosoughi", "Soroush", ""], ["Wei", "Jason", ""]]}, {"id": "2005.12529", "submitter": "Behnam Hedayatnia", "authors": "Behnam Hedayatnia, Karthik Gopalakrishnan, Seokhwan Kim, Yang Liu,\n  Mihail Eric, Dilek Hakkani-Tur", "title": "Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue\n  Systems", "comments": "Link to public dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain dialogue systems aim to generate relevant, informative and\nengaging responses. Seq2seq neural response generation approaches do not have\nexplicit mechanisms to control the content or style of the generated response,\nand frequently result in uninformative utterances. In this paper, we propose\nusing a dialogue policy to plan the content and style of target responses in\nthe form of an action plan, which includes knowledge sentences related to the\ndialogue context, targeted dialogue acts, topic information, etc. The\nattributes within the action plan are obtained by automatically annotating the\npublicly released Topical-Chat dataset. We condition neural response generators\non the action plan which is then realized as target utterances at the turn and\nsentence levels. We also investigate different dialogue policy models to\npredict an action plan given the dialogue context. Through automated and human\nevaluation, we measure the appropriateness of the generated responses and check\nif the generation models indeed learn to realize the given action plans. We\ndemonstrate that a basic dialogue policy that operates at the sentence level\ngenerates better responses in comparison to turn level generation as well as\nbaseline models with no action plan. Additionally the basic dialogue policy has\nthe added effect of controllability.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 06:09:57 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 04:10:35 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 20:24:04 GMT"}, {"version": "v4", "created": "Mon, 24 Aug 2020 21:06:46 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Hedayatnia", "Behnam", ""], ["Gopalakrishnan", "Karthik", ""], ["Kim", "Seokhwan", ""], ["Liu", "Yang", ""], ["Eric", "Mihail", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2005.12531", "submitter": "Dongyang Dai", "authors": "Dongyang Dai, Li Chen, Yuping Wang, Mu Wang, Rui Xia, Xuchen Song,\n  Zhiyong Wu, Yuxuan Wang", "title": "Noise Robust TTS for Low Resource Speakers using Pre-trained Model and\n  Speech Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of deep neural network, speech synthesis task has\nachieved significant improvements based on the end-to-end encoder-decoder\nframework in the recent days. More and more applications relying on speech\nsynthesis technology have been widely used in our daily life. Robust speech\nsynthesis model depends on high quality and customized data which needs lots of\ncollecting efforts. It is worth investigating how to take advantage of\nlow-quality and low resource voice data which can be easily obtained from the\nInternet for usage of synthesizing personalized voice. In this paper, the\nproposed end-to-end speech synthesis model uses both speaker embedding and\nnoise representation as conditional inputs to model speaker and noise\ninformation respectively. Firstly, the speech synthesis model is pre-trained\nwith both multi-speaker clean data and noisy augmented data; then the\npre-trained model is adapted on noisy low-resource new speaker data; finally,\nby setting the clean speech condition, the model can synthesize the new\nspeaker's clean voice. Experimental results show that the speech generated by\nthe proposed approach has better subjective evaluation results than the method\ndirectly fine-tuning pre-trained multi-speaker speech synthesis model with\ndenoised new speaker data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 06:14:06 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 11:36:56 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Dai", "Dongyang", ""], ["Chen", "Li", ""], ["Wang", "Yuping", ""], ["Wang", "Mu", ""], ["Xia", "Rui", ""], ["Song", "Xuchen", ""], ["Wu", "Zhiyong", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2005.12533", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel, Andres Suarez Madrigal, Gino Yu", "title": "Guiding Symbolic Natural Language Grammar Induction via\n  Transformer-Based Sequence Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach to automated learning of syntactic rules governing natural\nlanguages is proposed, based on using probabilities assigned to sentences (and\npotentially longer word sequences) by transformer neural network language\nmodels to guide symbolic learning processes like clustering and rule induction.\nThis method exploits the learned linguistic knowledge in transformers, without\nany reference to their inner representations; hence, the technique is readily\nadaptable to the continuous appearance of more powerful language models. We\nshow a proof-of-concept example of our proposed technique, using it to guide\nunsupervised symbolic link-grammar induction methods drawn from our prior\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 06:18:47 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Goertzel", "Ben", ""], ["Madrigal", "Andres Suarez", ""], ["Yu", "Gino", ""]]}, {"id": "2005.12535", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel, Mike Duncan, Debbie Duong, Nil Geisweiller, Hedra Seid,\n  Abdulrahman Semrie, Man Hin Leung, Matthew Ikle'", "title": "Embedding Vector Differences Can Be Aligned With Uncertain Intensional\n  Logic Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DeepWalk algorithm is used to assign embedding vectors to nodes in the\nAtomspace weighted, labeled hypergraph that is used to represent knowledge in\nthe OpenCog AGI system, in the context of an application to probabilistic\ninference regarding the causes of longevity based on data from biological\nontologies and genomic analyses. It is shown that vector difference operations\nbetween embedding vectors are, in appropriate conditions, approximately\nalignable with \"intensional difference\" operations between the hypergraph nodes\ncorresponding to the embedding vectors. This relationship hints at a broader\nfunctorial mapping between uncertain intensional logic and vector arithmetic,\nand opens the door for using embedding vector algebra to guide intensional\ninference control.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 06:20:32 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Goertzel", "Ben", ""], ["Duncan", "Mike", ""], ["Duong", "Debbie", ""], ["Geisweiller", "Nil", ""], ["Seid", "Hedra", ""], ["Semrie", "Abdulrahman", ""], ["Leung", "Man Hin", ""], ["Ikle'", "Matthew", ""]]}, {"id": "2005.12565", "submitter": "Saadullah Amin", "authors": "Saadullah Amin, Katherine Ann Dunfield, Anna Vechkaeva and G\\\"unter\n  Neumann", "title": "A Data-driven Approach for Noise Reduction in Distantly Supervised\n  Biomedical Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact triples are a common form of structured knowledge used within the\nbiomedical domain. As the amount of unstructured scientific texts continues to\ngrow, manual annotation of these texts for the task of relation extraction\nbecomes increasingly expensive. Distant supervision offers a viable approach to\ncombat this by quickly producing large amounts of labeled, but considerably\nnoisy, data. We aim to reduce such noise by extending an entity-enriched\nrelation classification BERT model to the problem of multiple instance\nlearning, and defining a simple data encoding scheme that significantly reduces\nnoise, reaching state-of-the-art performance for distantly-supervised\nbiomedical relation extraction. Our approach further encodes knowledge about\nthe direction of relation triples, allowing for increased focus on relation\nlearning by reducing noise and alleviating the need for joint learning with\nknowledge graph completion.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 08:15:32 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Amin", "Saadullah", ""], ["Dunfield", "Katherine Ann", ""], ["Vechkaeva", "Anna", ""], ["Neumann", "G\u00fcnter", ""]]}, {"id": "2005.12588", "submitter": "Pierre-Loic Garoche", "authors": "Rapha\\\"el Cohen, Eric F\\'eron, Pierre-Lo\\\"ic Garoche (ENAC)", "title": "Verification and Validation of Convex Optimization Algorithms for Model\n  Predictive Control", "comments": null, "journal-ref": "Journal of Aerospace Information Systems, American Institute of\n  Aeronautics and Astronautics, 2020, 17 (5), pp.257-270", "doi": null, "report-no": null, "categories": "cs.CL cs.FL math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced embedded algorithms are growing in complexity and they are an\nessential contributor to the growth of autonomy in many areas. However, the\npromise held by these algorithms cannot be kept without proper attention to the\nconsiderably stronger design constraints that arise when the applications of\ninterest, such as aerospace systems, are safety-critical. Formal verification\nis the process of proving or disproving the ''correctness'' of an algorithm\nwith respect to a certain mathematical description of it by means of a\ncomputer. This article discusses the formal verification of the Ellipsoid\nmethod, a convex optimization algorithm, and its code implementation as it\napplies to receding horizon control. Options for encoding code properties and\ntheir proofs are detailed. The applicability and limitations of those code\nproperties and proofs are presented as well. Finally, floating-point errors are\ntaken into account in a numerical analysis of the Ellipsoid algorithm.\nModifications to the algorithm are presented which can be used to control its\nnumerical stability.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 09:18:14 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Cohen", "Rapha\u00ebl", "", "ENAC"], ["F\u00e9ron", "Eric", "", "ENAC"], ["Garoche", "Pierre-Lo\u00efc", "", "ENAC"]]}, {"id": "2005.12592", "submitter": "Kostiantyn Omelianchuk", "authors": "Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub, Oleksandr\n  Skurzhanskyi", "title": "GECToR -- Grammatical Error Correction: Tag, Not Rewrite", "comments": "Accepted for publication in BEA workshop (15th Workshop on Innovative\n  Use of NLP for Building Educational Applications; co-located with ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple and efficient GEC sequence tagger using a\nTransformer encoder. Our system is pre-trained on synthetic data and then\nfine-tuned in two stages: first on errorful corpora, and second on a\ncombination of errorful and error-free parallel corpora. We design custom\ntoken-level transformations to map input tokens to target corrections. Our best\nsingle-model/ensemble GEC tagger achieves an $F_{0.5}$ of 65.3/66.5 on\nCoNLL-2014 (test) and $F_{0.5}$ of 72.4/73.6 on BEA-2019 (test). Its inference\nspeed is up to 10 times as fast as a Transformer-based seq2seq GEC system. The\ncode and trained models are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 09:33:02 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 09:15:53 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Omelianchuk", "Kostiantyn", ""], ["Atrasevych", "Vitaliy", ""], ["Chernodub", "Artem", ""], ["Skurzhanskyi", "Oleksandr", ""]]}, {"id": "2005.12696", "submitter": "Yi Zhu", "authors": "Yi Zhu, Yiwei Zhou and Menglin Xia", "title": "Generating Semantically Valid Adversarial Questions for TableQA", "comments": "AAAI 2021 Workshop on Towards Robust, Secure and Efficient Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack on question answering systems over tabular data (TableQA)\ncan help evaluate to what extent they can understand natural language questions\nand reason with tables. However, generating natural language adversarial\nquestions is difficult, because even a single character swap could lead to huge\nsemantic difference in human perception. In this paper, we propose SAGE\n(Semantically valid Adversarial GEnerator), a Wasserstein sequence-to-sequence\nmodel for TableQA white-box attack. To preserve meaning of original questions,\nwe apply minimum risk training with SIMILE and entity delexicalization. We use\nGumbel-Softmax to incorporate adversarial loss for end-to-end training. Our\nexperiments show that SAGE outperforms existing local attack models on semantic\nvalidity and fluency while achieving a good attack success rate. Finally, we\ndemonstrate that adversarial training with SAGE augmented data can improve\nperformance and robustness of TableQA systems.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 13:17:36 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 05:29:19 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 14:40:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhu", "Yi", ""], ["Zhou", "Yiwei", ""], ["Xia", "Menglin", ""]]}, {"id": "2005.12762", "submitter": "Bel\\'en Sald\\'ias", "authors": "Belen Saldias and Deb Roy", "title": "Exploring aspects of similarity between spoken personal narratives by\n  disentangling them into narrative clause types", "comments": "9 pages, Proceedings of the 2020 ACL Workshop on Narrative\n  Understanding, Storylines, and Events (NUSE). ACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sharing personal narratives is a fundamental aspect of human social behavior\nas it helps share our life experiences. We can tell stories and rely on our\nbackground to understand their context, similarities, and differences. A\nsubstantial effort has been made towards developing storytelling machines or\ninferring characters' features. However, we don't usually find models that\ncompare narratives. This task is remarkably challenging for machines since\nthey, as sometimes we do, lack an understanding of what similarity means. To\naddress this challenge, we first introduce a corpus of real-world spoken\npersonal narratives comprising 10,296 narrative clauses from 594 video\ntranscripts. Second, we ask non-narrative experts to annotate those clauses\nunder Labov's sociolinguistic model of personal narratives (i.e., action,\norientation, and evaluation clause types) and train a classifier that reaches\n84.7% F-score for the highest-agreed clauses. Finally, we match stories and\nexplore whether people implicitly rely on Labov's framework to compare\nnarratives. We show that actions followed by the narrator's evaluation of these\nare the aspects non-experts consider the most. Our approach is intended to help\ninform machine learning methods aimed at studying or representing personal\nnarratives.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:34:07 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 13:32:15 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Saldias", "Belen", ""], ["Roy", "Deb", ""]]}, {"id": "2005.12766", "submitter": "Hongchao Fang", "authors": "Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, Pengtao Xie", "title": "CERT: Contrastive Self-supervised Learning for Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained language models such as BERT, GPT have shown great effectiveness\nin language understanding. The auxiliary predictive tasks in existing\npretraining approaches are mostly defined on tokens, thus may not be able to\ncapture sentence-level semantics very well. To address this issue, we propose\nCERT: Contrastive self-supervised Encoder Representations from Transformers,\nwhich pretrains language representation models using contrastive\nself-supervised learning at the sentence level. CERT creates augmentations of\noriginal sentences using back-translation. Then it finetunes a pretrained\nlanguage encoder (e.g., BERT) by predicting whether two augmented sentences\noriginate from the same sentence. CERT is simple to use and can be flexibly\nplugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11\nnatural language understanding tasks in the GLUE benchmark where CERT\noutperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks,\nand performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks,\nCERT outperforms BERT. The data and code are available at\nhttps://github.com/UCSD-AI4H/CERT\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 16:20:38 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 12:47:18 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Fang", "Hongchao", ""], ["Wang", "Sicheng", ""], ["Zhou", "Meng", ""], ["Ding", "Jiayuan", ""], ["Xie", "Pengtao", ""]]}, {"id": "2005.12801", "submitter": "Kiant\\'e Brantley", "authors": "Kiant\\'e Brantley, Amr Sharaf, Hal Daum\\'e III", "title": "Active Imitation Learning with Noisy Guidance", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning algorithms provide state-of-the-art results on many\nstructured prediction tasks by learning near-optimal search policies. Such\nalgorithms assume training-time access to an expert that can provide the\noptimal action at any queried state; unfortunately, the number of such queries\nis often prohibitive, frequently rendering these approaches impractical. To\ncombat this query complexity, we consider an active learning setting in which\nthe learning algorithm has additional access to a much cheaper noisy heuristic\nthat provides noisy guidance. Our algorithm, LEAQI, learns a difference\nclassifier that predicts when the expert is likely to disagree with the\nheuristic, and queries the expert only when necessary. We apply LEAQI to three\nsequence labeling tasks, demonstrating significantly fewer queries to the\nexpert and comparable (or better) accuracies over a passive approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:35:46 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Brantley", "Kiant\u00e9", ""], ["Sharaf", "Amr", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2005.12816", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Manos Tsagkias, Ernest Pusateri, Ilya Oparin", "title": "Predicting Entity Popularity to Improve Spoken Entity Recognition by\n  Virtual Assistants", "comments": "SIGIR '20. The 43rd International ACM SIGIR Conference on Research &\n  Development in Information Retrieval", "journal-ref": null, "doi": "10.1145/3397271.3401298", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on improving the effectiveness of a Virtual Assistant (VA) in\nrecognizing emerging entities in spoken queries. We introduce a method that\nuses historical user interactions to forecast which entities will gain in\npopularity and become trending, and it subsequently integrates the predictions\nwithin the Automated Speech Recognition (ASR) component of the VA. Experiments\nshow that our proposed approach results in a 20% relative reduction in errors\non emerging entity name utterances without degrading the overall recognition\nquality of the system.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:47:42 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Tsagkias", "Manos", ""], ["Pusateri", "Ernest", ""], ["Oparin", "Ilya", ""]]}, {"id": "2005.12830", "submitter": "Jia Xue", "authors": "Jia Xue (University of Toronto), Junxiang Chen (University of\n  Pittsburgh), Ran Hu (University of Toronto), Chen Chen (University of\n  Toronto), ChengDa Zheng (University of Toronto), Xiaoqian Liu (Chinese\n  Academy of Sciences), Tingshao Zhu (China Academy of Science)", "title": "Twitter discussions and emotions about COVID-19 pandemic: a machine\n  learning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the study is to examine coronavirus disease (COVID-19)\nrelated discussions, concerns, and sentiments that emerged from tweets posted\nby Twitter users. We analyze 4 million Twitter messages related to the COVID-19\npandemic using a list of 25 hashtags such as \"coronavirus,\" \"COVID-19,\"\n\"quarantine\" from March 1 to April 21 in 2020. We use a machine learning\napproach, Latent Dirichlet Allocation (LDA), to identify popular unigram,\nbigrams, salient topics and themes, and sentiments in the collected Tweets.\nPopular unigrams include \"virus,\" \"lockdown,\" and \"quarantine.\" Popular bigrams\ninclude \"COVID-19,\" \"stay home,\" \"corona virus,\" \"social distancing,\" and \"new\ncases.\" We identify 13 discussion topics and categorize them into five\ndifferent themes, such as \"public health measures to slow the spread of\nCOVID-19,\" \"social stigma associated with COVID-19,\" \"coronavirus news cases\nand deaths,\" \"COVID-19 in the United States,\" and \"coronavirus cases in the\nrest of the world\". Across all identified topics, the dominant sentiments for\nthe spread of coronavirus are anticipation that measures that can be taken,\nfollowed by a mixed feeling of trust, anger, and fear for different topics. The\npublic reveals a significant feeling of fear when they discuss the coronavirus\nnew cases and deaths than other topics. The study shows that Twitter data and\nmachine learning approaches can be leveraged for infodemiology study by\nstudying the evolving public discussions and sentiments during the COVID-19.\nReal-time monitoring and assessment of the Twitter discussion and concerns can\nbe promising for public health emergency responses and planning. Already\nemerged pandemic fear, stigma, and mental health concerns may continue to\ninfluence public trust when there occurs a second wave of COVID-19 or a new\nsurge of the imminent pandemic.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 16:10:02 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 02:43:13 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Xue", "Jia", "", "University of Toronto"], ["Chen", "Junxiang", "", "University of\n  Pittsburgh"], ["Hu", "Ran", "", "University of Toronto"], ["Chen", "Chen", "", "University of\n  Toronto"], ["Zheng", "ChengDa", "", "University of Toronto"], ["Liu", "Xiaoqian", "", "Chinese\n  Academy of Sciences"], ["Zhu", "Tingshao", "", "China Academy of Science"]]}, {"id": "2005.12833", "submitter": "Laila Rasmy", "authors": "Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao and Degui Zhi", "title": "Med-BERT: pre-trained contextualized embeddings on large-scale\n  structured electronic health records for disease prediction", "comments": "L.R., X.Y., and Z.X. share first authorship of this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) based predictive models from electronic health records\n(EHR) deliver impressive performance in many clinical tasks. Large training\ncohorts, however, are often required to achieve high accuracy, hindering the\nadoption of DL-based models in scenarios with limited training data size.\nRecently, bidirectional encoder representations from transformers (BERT) and\nrelated models have achieved tremendous successes in the natural language\nprocessing domain. The pre-training of BERT on a very large training corpus\ngenerates contextualized embeddings that can boost the performance of models\ntrained on smaller datasets. We propose Med-BERT, which adapts the BERT\nframework for pre-training contextualized embedding models on structured\ndiagnosis data from 28,490,650 patients EHR dataset. Fine-tuning experiments\nare conducted on two disease-prediction tasks: (1) prediction of heart failure\nin patients with diabetes and (2) prediction of pancreatic cancer from two\nclinical databases. Med-BERT substantially improves prediction accuracy,\nboosting the area under receiver operating characteristics curve (AUC) by\n2.02-7.12%. In particular, pre-trained Med-BERT substantially improves the\nperformance of tasks with very small fine-tuning training sets (300-500\nsamples) boosting the AUC by more than 20% or equivalent to the AUC of 10 times\nlarger training set. We believe that Med-BERT will benefit disease-prediction\nstudies with small local training datasets, reduce data collection expenses,\nand accelerate the pace of artificial intelligence aided healthcare.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 05:07:17 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Rasmy", "Laila", ""], ["Xiang", "Yang", ""], ["Xie", "Ziqian", ""], ["Tao", "Cui", ""], ["Zhi", "Degui", ""]]}, {"id": "2005.12889", "submitter": "Ruixiang Cui", "authors": "Ruixiang Cui, Daniel Hershcovich", "title": "Refining Implicit Argument Annotation for UCCA", "comments": "DMR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicate-argument structure analysis is a central component in meaning\nrepresentations of text. The fact that some arguments are not explicitly\nmentioned in a sentence gives rise to ambiguity in language understanding, and\nrenders it difficult for machines to interpret text correctly. However, only\nfew resources represent implicit roles for NLU, and existing studies in NLP\nonly make coarse distinctions between categories of arguments omitted from\nlinguistic form. This paper proposes a typology for fine-grained implicit\nargument annotation on top of Universal Conceptual Cognitive Annotation's\nfoundational layer. The proposed implicit argument categorisation is driven by\ntheories of implicit role interpretation and consists of six types: Deictic,\nGeneric, Genre-based, Type-identifiable, Non-specific, and Iterated-set. We\nexemplify our design by revisiting part of the UCCA EWT corpus, providing a new\ndataset annotated with the refinement layer, and making a comparative analysis\nwith other schemes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:24:15 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 16:07:33 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 13:33:36 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 10:00:17 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Cui", "Ruixiang", ""], ["Hershcovich", "Daniel", ""]]}, {"id": "2005.12898", "submitter": "Han He", "authors": "Tae Hwan Oh, Ji Yoon Han, Hyonsu Choe, Seokwon Park, Han He, Jinho D.\n  Choi, Na-Rae Han, Jena D. Hwang, Hansaem Kim", "title": "Analysis of the Penn Korean Universal Dependency Treebank (PKT-UD):\n  Manual Revision to Build Robust Parsing Model in Korean", "comments": "Accepted by The 16th International Conference on Parsing\n  Technologies, IWPT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first open on important issues regarding the Penn Korean\nUniversal Treebank (PKT-UD) and address these issues by revising the entire\ncorpus manually with the aim of producing cleaner UD annotations that are more\nfaithful to Korean grammar. For compatibility to the rest of UD corpora, we\nfollow the UDv2 guidelines, and extensively revise the part-of-speech tags and\nthe dependency relations to reflect morphological features and flexible\nword-order aspects in Korean. The original and the revised versions of PKT-UD\nare experimented with transformer-based parsing models using biaffine\nattention. The parsing model trained on the revised corpus shows a significant\nimprovement of 3.0% in labeled attachment score over the model trained on the\nprevious corpus. Our error analysis demonstrates that this revision allows the\nparsing model to learn relations more robustly, reducing several critical\nerrors that used to be made by the previous model.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:46:46 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Oh", "Tae Hwan", ""], ["Han", "Ji Yoon", ""], ["Choe", "Hyonsu", ""], ["Park", "Seokwon", ""], ["He", "Han", ""], ["Choi", "Jinho D.", ""], ["Han", "Na-Rae", ""], ["Hwang", "Jena D.", ""], ["Kim", "Hansaem", ""]]}, {"id": "2005.12962", "submitter": "Tuan Dinh", "authors": "Huy Kinh Phan, Viet Lam Phung, Tuan Anh Dinh, Bao Quoc Nguyen", "title": "A comparison of Vietnamese Statistical Parametric Speech Synthesis\n  Systems", "comments": "9 pages, submitted to KSE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, statistical parametric speech synthesis (SPSS) systems have\nbeen widely utilized in many interactive speech-based systems (e.g.~Amazon's\nAlexa, Bose's headphones). To select a suitable SPSS system, both speech\nquality and performance efficiency (e.g.~decoding time) must be taken into\naccount. In the paper, we compared four popular Vietnamese SPSS techniques\nusing: 1) hidden Markov models (HMM), 2) deep neural networks (DNN), 3)\ngenerative adversarial networks (GAN), and 4) end-to-end (E2E) architectures,\nwhich consists of Tacontron~2 and WaveGlow vocoder in terms of speech quality\nand performance efficiency. We showed that the E2E systems accomplished the\nbest quality, but required the power of GPU to achieve real-time performance.\nWe also showed that the HMM-based system had inferior speech quality, but it\nwas the most efficient system. Surprisingly, the E2E systems were more\nefficient than the DNN and GAN in inference on GPU. Surprisingly, the GAN-based\nsystem did not outperform the DNN in term of quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 18:32:03 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Phan", "Huy Kinh", ""], ["Phung", "Viet Lam", ""], ["Dinh", "Tuan Anh", ""], ["Nguyen", "Bao Quoc", ""]]}, {"id": "2005.12994", "submitter": "Puxuan Yu", "authors": "Puxuan Yu and James Allan", "title": "A Study of Neural Matching Models for Cross-lingual IR", "comments": "4 pages, 1 figure, accepted at SIGIR'20", "journal-ref": null, "doi": "10.1145/3397271.3401322", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate interaction-based neural matching models for\nad-hoc cross-lingual information retrieval (CLIR) using cross-lingual word\nembeddings (CLWEs). With experiments conducted on the CLEF collection over four\nlanguage pairs, we evaluate and provide insight into different neural model\narchitectures, different ways to represent query-document interactions and\nword-pair similarity distributions in CLIR. This study paves the way for\nlearning an end-to-end CLIR system using CLWEs.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 19:21:57 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Yu", "Puxuan", ""], ["Allan", "James", ""]]}, {"id": "2005.13012", "submitter": "Eduardo C\\'esar Garrido-Merch\\'an", "authors": "Santiago Gonz\\'alez-Carvajal and Eduardo C. Garrido-Merch\\'an", "title": "Comparing BERT against traditional machine learning text classification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BERT model has arisen as a popular state-of-the-art machine learning\nmodel in the recent years that is able to cope with multiple NLP tasks such as\nsupervised text classification without human supervision. Its flexibility to\ncope with any type of corpus delivering great results has make this approach\nvery popular not only in academia but also in the industry. Although, there are\nlots of different approaches that have been used throughout the years with\nsuccess. In this work, we first present BERT and include a little review on\nclassical NLP approaches. Then, we empirically test with a suite of experiments\ndealing different scenarios the behaviour of BERT against the traditional\nTF-IDF vocabulary fed to machine learning algorithms. Our purpose of this work\nis to add empirical evidence to support or refuse the use of BERT as a default\non NLP tasks. Experiments show the superiority of BERT and its independence of\nfeatures of the NLP problem such as the language of the text adding empirical\nevidence to use BERT as a default technique to be used in NLP problems.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:14:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 15:48:52 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gonz\u00e1lez-Carvajal", "Santiago", ""], ["Garrido-Merch\u00e1n", "Eduardo C.", ""]]}, {"id": "2005.13013", "submitter": "Jason Phang", "authors": "Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun\n  Liu, Clara Vania, Katharina Kann, Samuel R. Bowman", "title": "English Intermediate-Task Training Improves Zero-Shot Cross-Lingual\n  Transfer Too", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermediate-task training---fine-tuning a pretrained model on an\nintermediate task before fine-tuning again on the target task---often improves\nmodel performance substantially on language understanding tasks in monolingual\nEnglish settings. We investigate whether English intermediate-task training is\nstill helpful on non-English target tasks. Using nine intermediate\nlanguage-understanding tasks, we evaluate intermediate-task transfer in a\nzero-shot cross-lingual setting on the XTREME benchmark. We see large\nimprovements from intermediate training on the BUCC and Tatoeba sentence\nretrieval tasks and moderate improvements on question-answering target tasks.\nMNLI, SQuAD and HellaSwag achieve the best overall results as intermediate\ntasks, while multi-task intermediate offers small additional improvements.\nUsing our best intermediate-task models for each target task, we obtain a 5.4\npoint improvement over XLM-R Large on the XTREME benchmark, setting the state\nof the art as of June 2020. We also investigate continuing multilingual MLM\nduring intermediate-task training and using machine-translated\nintermediate-task data, but neither consistently outperforms simply performing\nEnglish intermediate-task training.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:17:29 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 18:01:49 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Phang", "Jason", ""], ["Calixto", "Iacer", ""], ["Htut", "Phu Mon", ""], ["Pruksachatkun", "Yada", ""], ["Liu", "Haokun", ""], ["Vania", "Clara", ""], ["Kann", "Katharina", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2005.13041", "submitter": "Thomas Davidson", "authors": "Thomas Davidson and Debasmita Bhattacharya", "title": "Examining Racial Bias in an Online Abuse Corpus with Structural Topic\n  Modeling", "comments": "Please cite the published version, see proceedings of ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use structural topic modeling to examine racial bias in data collected to\ntrain models to detect hate speech and abusive language in social media posts.\nWe augment the abusive language dataset by adding an additional feature\nindicating the predicted probability of the tweet being written in\nAfrican-American English. We then use structural topic modeling to examine the\ncontent of the tweets and how the prevalence of different topics is related to\nboth abusiveness annotation and dialect prediction. We find that certain topics\nare disproportionately racialized and considered abusive. We discuss how topic\nmodeling may be a useful approach for identifying bias in annotated data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 21:02:43 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Davidson", "Thomas", ""], ["Bhattacharya", "Debasmita", ""]]}, {"id": "2005.13084", "submitter": "Kai Shu", "authors": "Kai Shu, Subhabrata Mukherjee, Guoqing Zheng, Ahmed Hassan Awadallah,\n  Milad Shokouhi, Susan Dumais", "title": "Learning with Weak Supervision for Email Intent Detection", "comments": "10 pages, 3 figures", "journal-ref": "ACM SIGIR 2020", "doi": "10.1145/3397271.3401121", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email remains one of the most frequently used means of online communication.\nPeople spend a significant amount of time every day on emails to exchange\ninformation, manage tasks and schedule events. Previous work has studied\ndifferent ways for improving email productivity by prioritizing emails,\nsuggesting automatic replies or identifying intents to recommend appropriate\nactions. The problem has been mostly posed as a supervised learning problem\nwhere models of different complexities were proposed to classify an email\nmessage into a predefined taxonomy of intents or classes. The need for labeled\ndata has always been one of the largest bottlenecks in training supervised\nmodels. This is especially the case for many real-world tasks, such as email\nintent classification, where large scale annotated examples are either hard to\nacquire or unavailable due to privacy or data access constraints. Email users\noften take actions in response to intents expressed in an email (e.g., setting\nup a meeting in response to an email with a scheduling request). Such actions\ncan be inferred from user interaction logs. In this paper, we propose to\nleverage user actions as a source of weak supervision, in addition to a limited\nset of annotated examples, to detect intents in emails. We develop an\nend-to-end robust deep neural network model for email intent identification\nthat leverages both clean annotated data and noisy weak supervision along with\na self-paced learning mechanism. Extensive experiments on three different\nintent detection tasks show that our approach can effectively leverage the\nweakly supervised data to improve intent detection in emails.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 23:41:05 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Shu", "Kai", ""], ["Mukherjee", "Subhabrata", ""], ["Zheng", "Guoqing", ""], ["Awadallah", "Ahmed Hassan", ""], ["Shokouhi", "Milad", ""], ["Dumais", "Susan", ""]]}, {"id": "2005.13111", "submitter": "Lili Yu", "authors": "Kyle Swanson, Lili Yu, Tao Lei", "title": "Rationalizing Text Matching: Learning Sparse Alignments via Optimal\n  Transport", "comments": "To appear at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selecting input features of top relevance has become a popular method for\nbuilding self-explaining models. In this work, we extend this selective\nrationalization approach to text matching, where the goal is to jointly select\nand align text pieces, such as tokens or sentences, as a justification for the\ndownstream prediction. Our approach employs optimal transport (OT) to find a\nminimal cost alignment between the inputs. However, directly applying OT often\nproduces dense and therefore uninterpretable alignments. To overcome this\nlimitation, we introduce novel constrained variants of the OT problem that\nresult in highly sparse alignments with controllable sparsity. Our model is\nend-to-end differentiable using the Sinkhorn algorithm for OT and can be\ntrained without any alignment annotations. We evaluate our model on the\nStackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very\nsparse rationale selections with high fidelity while preserving prediction\naccuracy compared to strong attention baseline models.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:20:49 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Swanson", "Kyle", ""], ["Yu", "Lili", ""], ["Lei", "Tao", ""]]}, {"id": "2005.13119", "submitter": "Zehao Lin", "authors": "Zehao Lin, Shaobo Cui, Xiaoming Kang, Guodun Li, Feng Ji, Haiqing\n  Chen, Yin Zhang", "title": "Should Answer Immediately or Wait for Further Information? A Novel\n  Wait-or-Answer Task and Its Predictive Approach", "comments": "This previously appeared as arXiv:2002.09616v2, which was mistakenly\n  submitted as a replacement. arXiv admin note: text overlap with\n  arXiv:2002.09616v3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different people have different habits of describing their intents in\nconversations. Some people may tend to deliberate their full intents in several\nsuccessive utterances, i.e., they use several consistent messages for\nreadability instead of a long sentence to express their question. This creates\na predicament faced by dialogue systems' application, especially in real-world\nindustrial scenarios, in which the dialogue system is unsure that whether it\nshould answer the user's query immediately or wait for users' further\nsupplementary input. Motivated by such interesting quandary, we define a novel\ntask: Wait-or-Answer to better tackle this dilemma faced by dialogue systems.\nWe shed light on a new research topic about how the dialogue system can be more\ncompetent to behave in this Wait-or-Answer quandary. Further, we propose a\npredictive approach dubbed Imagine-then-Arbitrate (ITA) to resolve this\nWait-or-Answer task. More specifically, we take advantage of an arbitrator\nmodel to help the dialogue system decide to wait or answer. The arbitrator's\ndecision is made with the assistance of two ancillary imaginator models: a wait\nimaginator and an answer imaginator. The wait imaginator tries to predict what\nthe user would supplement and use its prediction to persuade the arbitrator\nthat the user has some information to add, so the dialogue system should wait.\nThe answer imaginator, nevertheless, struggles to predict the answer of the\ndialogue system and convince the arbitrator that it's a superior choice to\nanswer the users' query immediately. To our best knowledge, our paper is the\nfirst work to explicitly define the Wait-or-Answer task in the dialogue system.\nAdditionally, our proposed ITA approach significantly outperforms the existing\nmodels in solving this Wait-or-Answer problem.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:48:54 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Lin", "Zehao", ""], ["Cui", "Shaobo", ""], ["Kang", "Xiaoming", ""], ["Li", "Guodun", ""], ["Ji", "Feng", ""], ["Chen", "Haiqing", ""], ["Zhang", "Yin", ""]]}, {"id": "2005.13125", "submitter": "Kelechi Nwaike Mr.", "authors": "Kelechi Nwaike and Licheng Jiao", "title": "Counterfactual Detection meets Transfer Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can consider Counterfactuals as belonging in the domain of Discourse\nstructure and semantics, A core area in Natural Language Understanding and in\nthis paper, we introduce an approach to resolving counterfactual detection as\nwell as the indexing of the antecedents and consequents of Counterfactual\nstatements. While Transfer learning is already being applied to several NLP\ntasks, It has the characteristics to excel in a novel number of tasks. We show\nthat detecting Counterfactuals is a straightforward Binary Classification Task\nthat can be implemented with minimal adaptation on already existing model\nArchitectures, thanks to a well annotated training data set,and we introduce a\nnew end to end pipeline to process antecedents and consequents as an entity\nrecognition task, thus adapting them into Token Classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 02:02:57 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Nwaike", "Kelechi", ""], ["Jiao", "Licheng", ""]]}, {"id": "2005.13156", "submitter": "Marta R. Costa-juss\\`a", "authors": "Marta R. Costa-juss\\`a, Roger Creus, Oriol Domingo, Albert\n  Dom\\'inguez, Miquel Escobar, Cayetana L\\'opez, Marina Garcia and Margarita\n  Geleta", "title": "MT-Adapted Datasheets for Datasets: Template and Repository", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we are taking the standardized model proposed by Gebru et al.\n(2018) for documenting the popular machine translation datasets of the EuroParl\n(Koehn, 2005) and News-Commentary (Barrault et al., 2019). Within this\ndocumentation process, we have adapted the original datasheet to the particular\ncase of data consumers within the Machine Translation area. We are also\nproposing a repository for collecting the adapted datasheets in this research\narea\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 04:56:08 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Costa-juss\u00e0", "Marta R.", ""], ["Creus", "Roger", ""], ["Domingo", "Oriol", ""], ["Dom\u00ednguez", "Albert", ""], ["Escobar", "Miquel", ""], ["L\u00f3pez", "Cayetana", ""], ["Garcia", "Marina", ""], ["Geleta", "Margarita", ""]]}, {"id": "2005.13170", "submitter": "Haochen Liu", "authors": "Haochen Liu, Zhiwei Wang, Tyler Derr and Jiliang Tang", "title": "Chat as Expected: Learning to Manipulate Black-box Neural Dialogue\n  Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural network based dialogue systems have become ubiquitous in our\nincreasingly digitalized society. However, due to their inherent opaqueness,\nsome recently raised concerns about using neural models are starting to be\ntaken seriously. In fact, intentional or unintentional behaviors could lead to\na dialogue system to generate inappropriate responses. Thus, in this paper, we\ninvestigate whether we can learn to craft input sentences that result in a\nblack-box neural dialogue model being manipulated into having its outputs\ncontain target words or match target sentences. We propose a reinforcement\nlearning based model that can generate such desired inputs automatically.\nExtensive experiments on a popular well-trained state-of-the-art neural\ndialogue model show that our method can successfully seek out desired inputs\nthat lead to the target outputs in a considerable portion of cases.\nConsequently, our work reveals the potential of neural dialogue models to be\nmanipulated, which inspires and opens the door towards developing strategies to\ndefend them.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 05:34:12 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Liu", "Haochen", ""], ["Wang", "Zhiwei", ""], ["Derr", "Tyler", ""], ["Tang", "Jiliang", ""]]}, {"id": "2005.13213", "submitter": "Jey Han Lau", "authors": "Kobi Leins and Jey Han Lau and Timothy Baldwin", "title": "Give Me Convenience and Give Her Death: Who Should Decide What Uses of\n  NLP are Appropriate, and on What Basis?", "comments": "6 pages; accepted for ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of growing NLP capabilities, coupled with an awareness of the ethical\ndimensions of research, questions have been raised about whether particular\ndatasets and tasks should be deemed off-limits for NLP research. We examine\nthis question with respect to a paper on automatic legal sentencing from EMNLP\n2019 which was a source of some debate, in asking whether the paper should have\nbeen allowed to be published, who should have been charged with making such a\ndecision, and on what basis. We focus in particular on the role of data\nstatements in ethically assessing research, but also discuss the topic of dual\nuse, and examine the outcomes of similar debates in other scientific\ndisciplines.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 07:31:57 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Leins", "Kobi", ""], ["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""]]}, {"id": "2005.13236", "submitter": "Pedro Ortiz Suarez", "authors": "Pedro Javier Ortiz Su\\'arez (ALMAnaCH, SU), Yoann Dupont (ALMAnaCH,\n  SU), Benjamin Muller (ALMAnaCH, SU), Laurent Romary (ALMAnaCH), Beno\\^it\n  Sagot (ALMAnaCH)", "title": "Establishing a New State-of-the-Art for French Named Entity Recognition", "comments": null, "journal-ref": "LREC 2020 - 12th Language Resources and Evaluation Conference, May\n  2020, Marseille, France", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The French TreeBank developed at the University Paris 7 is the main source of\nmorphosyntactic and syntactic annotations for French. However, it does not\ninclude explicit information related to named entities, which are among the\nmost useful information for several natural language processing tasks and\napplications. Moreover, no large-scale French corpus with named entity\nannotations contain referential information, which complement the type and the\nspan of each mention with an indication of the entity it refers to. We have\nmanually annotated the French TreeBank with such information, after an\nautomatic pre-annotation step. We sketch the underlying annotation guidelines\nand we provide a few figures about the resulting annotations.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:44:09 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Su\u00e1rez", "Pedro Javier Ortiz", "", "ALMAnaCH, SU"], ["Dupont", "Yoann", "", "ALMAnaCH,\n  SU"], ["Muller", "Benjamin", "", "ALMAnaCH, SU"], ["Romary", "Laurent", "", "ALMAnaCH"], ["Sagot", "Beno\u00eet", "", "ALMAnaCH"]]}, {"id": "2005.13263", "submitter": "Tanner Bohn", "authors": "Tanner Bohn, Charles X. Ling", "title": "Catching Attention with Automatic Pull Quote Selection", "comments": "Accepted to COLING-2020. 15 pages (~9 for content + refs + appendix),\n  6 figures, 5 tables (+ 5 appendix tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To advance understanding on how to engage readers, we advocate the novel task\nof automatic pull quote selection. Pull quotes are a component of articles\nspecifically designed to catch the attention of readers with spans of text\nselected from the article and given more salient presentation. This task\ndiffers from related tasks such as summarization and clickbait identification\nby several aspects. We establish a spectrum of baseline approaches to the task,\nranging from handcrafted features to a neural mixture-of-experts to cross-task\nmodels. By examining the contributions of individual features and embedding\ndimensions from these models, we uncover unexpected properties of pull quotes\nto help answer the important question of what engages readers. Human evaluation\nalso supports the uniqueness of this task and the suitability of our selection\nmodels. The benefits of exploring this problem further are clear: pull quotes\nincrease enjoyment and readability, shape reader perceptions, and facilitate\nlearning. Code to reproduce this work is available at\nhttps://github.com/tannerbohn/AutomaticPullQuoteSelection.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 09:59:34 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:49:35 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Bohn", "Tanner", ""], ["Ling", "Charles X.", ""]]}, {"id": "2005.13316", "submitter": "Sascha Wolfer", "authors": "Sascha Wolfer and Alexander Koplenig and Frank Michaelis and Carolin\n  M\\\"uller-Spitzer", "title": "Tracking, exploring and analyzing recent developments in German-language\n  online press in the face of the coronavirus crisis: cOWIDplus Analysis and\n  cOWIDplus Viewer", "comments": "13 pages, 6 figures, 1 table, 3852 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus pandemic may be the largest crisis the world has had to face\nsince World War II. It does not come as a surprise that it is also having an\nimpact on language as our primary communication tool. We present three\ninter-connected resources that are designed to capture and illustrate these\neffects on a subset of the German language: An RSS corpus of German-language\nnewsfeeds (with freely available untruncated unigram frequency lists), a static\nbut continuously updated HTML page tracking the diversity of the used\nvocabulary and a web application that enables other researchers and the broader\npublic to explore these effects without any or with little knowledge of corpus\nrepresentation/exploration or statistical analyses.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 12:21:36 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wolfer", "Sascha", ""], ["Koplenig", "Alexander", ""], ["Michaelis", "Frank", ""], ["M\u00fcller-Spitzer", "Carolin", ""]]}, {"id": "2005.13334", "submitter": "Daniel Fern\\'andez-Gonz\\'alez", "authors": "Daniel Fern\\'andez-Gonz\\'alez and Carlos G\\'omez-Rodr\\'iguez", "title": "Enriched In-Order Linearization for Faster Sequence-to-Sequence\n  Constituent Parsing", "comments": "Proceedings of ACL 2020. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence constituent parsing requires a linearization to\nrepresent trees as sequences. Top-down tree linearizations, which can be based\non brackets or shift-reduce actions, have achieved the best accuracy to date.\nIn this paper, we show that these results can be improved by using an in-order\nlinearization instead. Based on this observation, we implement an enriched\nin-order shift-reduce linearization inspired by Vinyals et al. (2015)'s\napproach, achieving the best accuracy to date on the English PTB dataset among\nfully-supervised single-model sequence-to-sequence constituent parsers.\nFinally, we apply deterministic attention mechanisms to match the speed of\nstate-of-the-art transition-based parsers, thus showing that\nsequence-to-sequence models can match them, not only in accuracy, but also in\nspeed.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:01:02 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Fern\u00e1ndez-Gonz\u00e1lez", "Daniel", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2005.13342", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Till Blume, Saskia Ostendorff", "title": "Towards an Open Platform for Legal Information", "comments": "Accepted at ACM/IEEE Joint Conference on Digital Libraries (JCDL)\n  2020", "journal-ref": null, "doi": "10.1145/3383583.3398616", "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the area of legal information systems have led to a\nvariety of applications that promise support in processing and accessing legal\ndocuments. Unfortunately, these applications have various limitations, e.g.,\nregarding scope or extensibility. Furthermore, we do not observe a trend\ntowards open access in digital libraries in the legal domain as we observe in\nother domains, e.g., economics of computer science. To improve open access in\nthe legal domain, we present our approach for an open source platform to\ntransparently process and access Legal Open Data. This enables the sustainable\ndevelopment of legal applications by offering a single technology stack.\nMoreover, the approach facilitates the development and deployment of new\ntechnologies. As proof of concept, we implemented six technologies and\ngenerated metadata for more than 250,000 German laws and court decisions. Thus,\nwe can provide users of our platform not only access to legal documents, but\nalso the contained information.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:16:19 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ostendorff", "Malte", ""], ["Blume", "Till", ""], ["Ostendorff", "Saskia", ""]]}, {"id": "2005.13344", "submitter": "Daniel Fern\\'andez-Gonz\\'alez", "authors": "Daniel Fern\\'andez-Gonz\\'alez and Carlos G\\'omez-Rodr\\'iguez", "title": "Transition-based Semantic Dependency Parsing with Pointer Networks", "comments": "Proceedings of ACL 2020. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transition-based parsers implemented with Pointer Networks have become the\nnew state of the art in dependency parsing, excelling in producing labelled\nsyntactic trees and outperforming graph-based models in this task. In order to\nfurther test the capabilities of these powerful neural networks on a harder NLP\nproblem, we propose a transition system that, thanks to Pointer Networks, can\nstraightforwardly produce labelled directed acyclic graphs and perform semantic\ndependency parsing. In addition, we enhance our approach with deep\ncontextualized word embeddings extracted from BERT. The resulting system not\nonly outperforms all existing transition-based models, but also matches the\nbest fully-supervised accuracy to date on the SemEval 2015 Task 18 English\ndatasets among previous state-of-the-art graph-based parsers.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:18:27 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 11:10:31 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Fern\u00e1ndez-Gonz\u00e1lez", "Daniel", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2005.13362", "submitter": "Edison Marrese-Taylor", "authors": "Edison Marrese-Taylor, Cristian Rodriguez-Opazo, Jorge A. Balazs,\n  Stephen Gould, Yutaka Matsuo", "title": "A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews", "comments": "Second Grand Challenge and Workshop on Multimodal Language ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in opinion mining for written reviews, few works\nhave tackled the problem on other sources of reviews. In light of this issue,\nwe propose a multi-modal approach for mining fine-grained opinions from video\nreviews that is able to determine the aspects of the item under review that are\nbeing discussed and the sentiment orientation towards them. Our approach works\nat the sentence level without the need for time annotations and uses features\nderived from the audio, video and language transcriptions of its contents. We\nevaluate our approach on two datasets and show that leveraging the video and\naudio modalities consistently provides increased performance over text-only\nbaselines, providing evidence these extra modalities are key in better\nunderstanding video reviews.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:46:11 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 03:13:49 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Marrese-Taylor", "Edison", ""], ["Rodriguez-Opazo", "Cristian", ""], ["Balazs", "Jorge A.", ""], ["Gould", "Stephen", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2005.13399", "submitter": "Lasha Abzianidze", "authors": "Lasha Abzianidze, Rik van Noord, Hessel Haagsma and Johan Bos", "title": "The First Shared Task on Discourse Representation Structure Parsing", "comments": "International Conference on Computational Semantics (IWCS)", "journal-ref": "Proceedings of the IWCS Shared Task on Semantic Parsing, IWCS,\n  SIGSEM, 2019, Association for Computational Linguistics", "doi": "10.18653/v1/W19-1201", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper presents the IWCS 2019 shared task on semantic parsing where the\ngoal is to produce Discourse Representation Structures (DRSs) for English\nsentences. DRSs originate from Discourse Representation Theory and represent\nscoped meaning representations that capture the semantics of negation, modals,\nquantification, and presupposition triggers. Additionally, concepts and\nevent-participants in DRSs are described with WordNet synsets and the thematic\nroles from VerbNet. To measure similarity between two DRSs, they are\nrepresented in a clausal form, i.e. as a set of tuples. Participant systems\nwere expected to produce DRSs in this clausal form. Taking into account the\nrich lexical information, explicit scope marking, a high number of shared\nvariables among clauses, and highly-constrained format of valid DRSs, all these\nmakes the DRS parsing a challenging NLP task. The results of the shared task\ndisplayed improvements over the existing state-of-the-art parser.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:52:21 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Abzianidze", "Lasha", ""], ["van Noord", "Rik", ""], ["Haagsma", "Hessel", ""], ["Bos", "Johan", ""]]}, {"id": "2005.13407", "submitter": "Amir Feder", "authors": "Amir Feder, Nadav Oved, Uri Shalit, Roi Reichart", "title": "CausaLM: Causal Model Explanation Through Counterfactual Language Models", "comments": "Our code and data are available at:\n  https://amirfeder.github.io/CausaLM/ Accepted for publication in\n  Computational Linguistics journal", "journal-ref": null, "doi": "10.1162/coli_a_00404", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding predictions made by deep neural networks is notoriously\ndifficult, but also crucial to their dissemination. As all machine learning\nbased methods, they are as good as their training data, and can also capture\nunwanted biases. While there are tools that can help understand whether such\nbiases exist, they do not distinguish between correlation and causation, and\nmight be ill-suited for text-based models and for reasoning about high level\nlanguage concepts. A key problem of estimating the causal effect of a concept\nof interest on a given model is that this estimation requires the generation of\ncounterfactual examples, which is challenging with existing generation\ntechnology. To bridge that gap, we propose CausaLM, a framework for producing\ncausal model explanations using counterfactual language representation models.\nOur approach is based on fine-tuning of deep contextualized embedding models\nwith auxiliary adversarial tasks derived from the causal graph of the problem.\nConcretely, we show that by carefully choosing auxiliary adversarial\npre-training tasks, language representation models such as BERT can effectively\nlearn a counterfactual representation for a given concept of interest, and be\nused to estimate its true causal effect on model performance. A byproduct of\nour method is a language representation model that is unaffected by the tested\nconcept, which can be useful in mitigating unwanted bias ingrained in the data.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:06:35 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 06:59:21 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2020 21:06:43 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 12:34:18 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Feder", "Amir", ""], ["Oved", "Nadav", ""], ["Shalit", "Uri", ""], ["Reichart", "Roi", ""]]}, {"id": "2005.13421", "submitter": "Lasha Abzianidze", "authors": "Johan Bos, Lasha Abzianidze", "title": "Thirty Musts for Meaning Banking", "comments": "https://www.aclweb.org/anthology/W19-3302/", "journal-ref": "Proceedings of the First International Workshop on Designing\n  Meaning Representations, 2019, Association for Computational Linguistics", "doi": "10.18653/v1/W19-3302", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meaning banking--creating a semantically annotated corpus for the purpose of\nsemantic parsing or generation--is a challenging task. It is quite simple to\ncome up with a complex meaning representation, but it is hard to design a\nsimple meaning representation that captures many nuances of meaning. This paper\nlists some lessons learned in nearly ten years of meaning annotation during the\ndevelopment of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel\nMeaning Bank (Abzianidze et al., 2017). The paper's format is rather\nunconventional: there is no explicit related work, no methodology section, no\nresults, and no discussion (and the current snippet is not an abstract but\nactually an introductory preface). Instead, its structure is inspired by work\nof Traum (2000) and Bender (2013). The list starts with a brief overview of the\nexisting meaning banks (Section 1) and the rest of the items are roughly\ndivided into three groups: corpus collection (Section 2 and 3, annotation\nmethods (Section 4-11), and design of meaning representations (Section 12-30).\nWe hope this overview will give inspiration and guidance in creating improved\nmeaning banks in the future.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:28:12 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Bos", "Johan", ""], ["Abzianidze", "Lasha", ""]]}, {"id": "2005.13455", "submitter": "Anhad Mohananey", "authors": "Anhad Mohananey, Katharina Kann, Samuel R. Bowman", "title": "Self-Training for Unsupervised Parsing with PRPN", "comments": "Accepted for publication at the 16th International Conference on\n  Parsing Technologies (IWPT), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural unsupervised parsing (UP) models learn to parse without access to\nsyntactic annotations, while being optimized for another task like language\nmodeling. In this work, we propose self-training for neural UP models: we\nleverage aggregated annotations predicted by copies of our model as supervision\nfor future copies. To be able to use our model's predictions during training,\nwe extend a recent neural UP architecture, the PRPN (Shen et al., 2018a) such\nthat it can be trained in a semi-supervised fashion. We then add examples with\nparses predicted by our model to our unlabeled UP training data. Our\nself-trained model outperforms the PRPN by 8.1% F1 and the previous state of\nthe art by 1.6% F1. In addition, we show that our architecture can also be\nhelpful for semi-supervised parsing in ultra-low-resource settings.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:11:09 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Mohananey", "Anhad", ""], ["Kann", "Katharina", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2005.13482", "submitter": "Adhiguna Kuncoro", "authors": "Adhiguna Kuncoro, Lingpeng Kong, Daniel Fried, Dani Yogatama, Laura\n  Rimell, Chris Dyer, Phil Blunsom", "title": "Syntactic Structure Distillation Pretraining For Bidirectional Encoders", "comments": "17 pages, 6 tables, 2 figures. AK and LK contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual representation learners trained on large amounts of data have\nachieved notable success on downstream tasks; intriguingly, they have also\nperformed well on challenging tests of syntactic competence. Given this\nsuccess, it remains an open question whether scalable learners like BERT can\nbecome fully proficient in the syntax of natural language by virtue of data\nscale alone, or whether they still benefit from more explicit syntactic biases.\nTo answer this question, we introduce a knowledge distillation strategy for\ninjecting syntactic biases into BERT pretraining, by distilling the\nsyntactically informative predictions of a hierarchical---albeit harder to\nscale---syntactic language model. Since BERT models masked words in\nbidirectional context, we propose to distill the approximate marginal\ndistribution over words in context from the syntactic LM. Our approach reduces\nrelative error by 2-21% on a diverse set of structured prediction tasks,\nalthough we obtain mixed results on the GLUE benchmark. Our findings\ndemonstrate the benefits of syntactic biases, even in representation learners\nthat exploit large amounts of data, and contribute to a better understanding of\nwhere syntactic biases are most helpful in benchmarks of natural language\nunderstanding.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:44:01 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kuncoro", "Adhiguna", ""], ["Kong", "Lingpeng", ""], ["Fried", "Daniel", ""], ["Yogatama", "Dani", ""], ["Rimell", "Laura", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "2005.13485", "submitter": "Ruisheng Cao", "authors": "Ruisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu\n  Chen and Kai Yu", "title": "Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing", "comments": "accepted by ACL 2020 Long, 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One daunting problem for semantic parsing is the scarcity of annotation.\nAiming to reduce nontrivial human labor, we propose a two-stage semantic\nparsing framework, where the first stage utilizes an unsupervised paraphrase\nmodel to convert an unlabeled natural language utterance into the canonical\nutterance. The downstream naive semantic parser accepts the intermediate output\nand returns the target logical form. Furthermore, the entire training process\nis split into two phases: pre-training and cycle learning. Three tailored\nself-supervised tasks are introduced throughout training to activate the\nunsupervised paraphrase model. Experimental results on benchmarks Overnight and\nGeoGranno demonstrate that our framework is effective and compatible with\nsupervised training.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:47:44 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 09:19:15 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 11:45:03 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cao", "Ruisheng", ""], ["Zhu", "Su", ""], ["Yang", "Chenyu", ""], ["Liu", "Chen", ""], ["Ma", "Rao", ""], ["Zhao", "Yanbin", ""], ["Chen", "Lu", ""], ["Yu", "Kai", ""]]}, {"id": "2005.13486", "submitter": "Lixing Zhu", "authors": "Lixing Zhu and Yulan He and Deyu Zhou", "title": "Neural Temporal Opinion Modelling for Opinion Prediction on Twitter", "comments": "To appear at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion prediction on Twitter is challenging due to the transient nature of\ntweet content and neighbourhood context. In this paper, we model users' tweet\nposting behaviour as a temporal point process to jointly predict the posting\ntime and the stance label of the next tweet given a user's historical tweet\nsequence and tweets posted by their neighbours. We design a topic-driven\nattention mechanism to capture the dynamic topic shifts in the neighbourhood\ncontext. Experimental results show that the proposed model predicts both the\nposting time and the stance labels of future tweets more accurately compared to\na number of competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:49:04 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Zhu", "Lixing", ""], ["He", "Yulan", ""], ["Zhou", "Deyu", ""]]}, {"id": "2005.13575", "submitter": "Chundra Cathcart", "authors": "Chundra A. Cathcart, Florian Wandl", "title": "In search of isoglosses: continuous and discrete language embeddings in\n  Slavic historical phonology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the ability of neural network architectures to\neffectively learn diachronic phonological generalizations in a multilingual\nsetting. We employ models using three different types of language embedding\n(dense, sigmoid, and straight-through). We find that the Straight-Through model\noutperforms the other two in terms of accuracy, but the Sigmoid model's\nlanguage embeddings show the strongest agreement with the traditional\nsubgrouping of the Slavic languages. We find that the Straight-Through model\nhas learned coherent, semi-interpretable information about sound change, and\noutline directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 18:10:46 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cathcart", "Chundra A.", ""], ["Wandl", "Florian", ""]]}, {"id": "2005.13619", "submitter": "Howey Qiu", "authors": "Brian Cheang, Bailey Wei, David Kogan, Howey Qiu, Masud Ahmed", "title": "Language Representation Models for Fine-Grained Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment classification is a quickly advancing field of study with\napplications in almost any field. While various models and datasets have shown\nhigh accuracy inthe task of binary classification, the task of fine-grained\nsentiment classification is still an area with room for significant\nimprovement. Analyzing the SST-5 dataset,previous work by Munikar et al. (2019)\nshowed that the embedding tool BERT allowed a simple model to achieve\nstate-of-the-art accuracy. Since that paper, several BERT alternatives have\nbeen published, with three primary ones being AlBERT (Lan et al., 2019),\nDistilBERT (Sanh et al. 2019), and RoBERTa (Liu etal. 2019). While these models\nreport some improvement over BERT on the popular benchmarks GLUE, SQuAD, and\nRACE, they have not been applied to the fine-grained classification task. In\nthis paper, we examine whether the improvements hold true when applied to a\nnovel task, by replicating the BERT model from Munikar et al., and swapping the\nembedding layer to the alternative models. Over the experiments, we found that\nAlBERT suffers significantly more accuracy loss than reported on other tasks,\nDistilBERT has accuracy loss similar to their reported loss on other tasks\nwhile being the fastest model to train, and RoBERTa reaches anew\nstate-of-the-art accuracy for prediction on the SST-5 root level (60.2%).\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:01:56 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cheang", "Brian", ""], ["Wei", "Bailey", ""], ["Kogan", "David", ""], ["Qiu", "Howey", ""], ["Ahmed", "Masud", ""]]}, {"id": "2005.13681", "submitter": "Elizabeth Salesky", "authors": "Elizabeth Salesky and Alan W Black", "title": "Phone Features Improve Speech Translation", "comments": "Accepted to ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end models for speech translation (ST) more tightly couple speech\nrecognition (ASR) and machine translation (MT) than a traditional cascade of\nseparate ASR and MT models, with simpler model architectures and the potential\nfor reduced error propagation. Their performance is often assumed to be\nsuperior, though in many conditions this is not yet the case. We compare\ncascaded and end-to-end models across high, medium, and low-resource\nconditions, and show that cascades remain stronger baselines. Further, we\nintroduce two methods to incorporate phone features into ST models. We show\nthat these features improve both architectures, closing the gap between\nend-to-end models and cascades, and outperforming previous academic work -- by\nup to 9 BLEU on our low-resource setting.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 22:05:10 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Salesky", "Elizabeth", ""], ["Black", "Alan W", ""]]}, {"id": "2005.13751", "submitter": "Iraklis Moutidis", "authors": "Iraklis Moutidis and Hywel T.P. Williams", "title": "Complex networks for event detection in heterogeneous high volume news\n  streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting important events in high volume news streams is an important task\nfor a variety of purposes.The volume and rate of online news increases the need\nfor automated event detection methods thatcan operate in real time. In this\npaper we develop a network-based approach that makes the workingassumption that\nimportant news events always involve named entities (such as persons,\nlocationsand organizations) that are linked in news articles. Our approach uses\nnatural language processingtechniques to detect these entities in a stream of\nnews articles and then creates a time-stamped seriesof networks in which the\ndetected entities are linked by co-occurrence in articles and sentences. Inthis\nprototype, weighted node degree is tracked over time and change-point detection\nused to locateimportant events. Potential events are characterized and\ndistinguished using community detectionon KeyGraphs that relate named entities\nand informative noun-phrases from related articles. Thismethodology already\nproduces promising results and will be extended in future to include a\nwidervariety of complex network analysis techniques.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:45:43 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Moutidis", "Iraklis", ""], ["Williams", "Hywel T. P.", ""]]}, {"id": "2005.13756", "submitter": "Katharina Kann", "authors": "Katharina Kann, Arya McCarthy, Garrett Nicolai, Mans Hulden", "title": "The SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm\n  Completion", "comments": "SIGMORPHON 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the findings of the SIGMORPHON 2020 shared task on\nunsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a\nnovel task in the field of inflectional morphology. Participants were asked to\nsubmit systems which take raw text and a list of lemmas as input, and output\nall inflected forms, i.e., the entire morphological paradigm, of each lemma. In\norder to simulate a realistic use case, we first released data for 5\ndevelopment languages. However, systems were officially evaluated on 9 surprise\nlanguages, which were only revealed a few days before the submission deadline.\nWe provided a modular baseline system, which is a pipeline of 4 components. 3\nteams submitted a total of 7 systems, but, surprisingly, none of the submitted\nsystems was able to improve over the baseline on average over all 9 test\nlanguages. Only on 3 languages did a submitted system obtain the best results.\nThis shows that unsupervised morphological paradigm completion is still largely\nunsolved. We present an analysis here, so that this shared task will ground\nfurther research on the topic.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 03:09:58 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Kann", "Katharina", ""], ["McCarthy", "Arya", ""], ["Nicolai", "Garrett", ""], ["Hulden", "Mans", ""]]}, {"id": "2005.13783", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand and Surya Kallumadi and Faizan Javed and Eugene\n  Agichtein", "title": "JointMap: Joint Query Intent Understanding For Modeling Intent\n  Hierarchies in E-commerce Search", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate understanding of a user's query intent can help improve the\nperformance of downstream tasks such as query scoping and ranking. In the\ne-commerce domain, recent work in query understanding focuses on the query to\nproduct-category mapping. But, a small yet significant percentage of queries\n(in our website 1.5% or 33M queries in 2019) have non-commercial intent\nassociated with them. These intents are usually associated with non-commercial\ninformation seeking needs such as discounts, store hours, installation guides,\netc. In this paper, we introduce Joint Query Intent Understanding (JointMap), a\ndeep learning model to simultaneously learn two different high-level user\nintent tasks: 1) identifying a query's commercial vs. non-commercial intent,\nand 2) associating a set of relevant product categories in taxonomy to a\nproduct query. JointMap model works by leveraging the transfer bias that exists\nbetween these two related tasks through a joint-learning process. As curating a\nlabeled data set for these tasks can be expensive and time-consuming, we\npropose a distant supervision approach in conjunction with an active learning\nmodel to generate high-quality training data sets. To demonstrate the\neffectiveness of JointMap, we use search queries collected from a large\ncommercial website. Our results show that JointMap significantly improves both\n\"commercial vs. non-commercial\" intent prediction and product category mapping\nby 2.3% and 10% on average over state-of-the-art deep learning methods. Our\nfindings suggest a promising direction to model the intent hierarchies in an\ne-commerce search engine.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 05:20:00 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 21:05:35 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Kallumadi", "Surya", ""], ["Javed", "Faizan", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2005.13798", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Harshita Sahijwani, Jason Ingyu Choi, Eugene Agichtein", "title": "ConCET: Entity-Aware Topic Classification for Open-Domain Conversational\n  Agents", "comments": "CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the topic (domain) of each user's utterance in open-domain\nconversational systems is a crucial step for all subsequent language\nunderstanding and response tasks. In particular, for complex domains, an\nutterance is often routed to a single component responsible for that domain.\nThus, correctly mapping a user utterance to the right domain is critical. To\naddress this problem, we introduce ConCET: a Concurrent Entity-aware\nconversational Topic classifier, which incorporates entity-type information\ntogether with the utterance content features. Specifically, ConCET utilizes\nentity information to enrich the utterance representation, combining character,\nword, and entity-type embeddings into a single representation. However, for\nrich domains with millions of available entities, unrealistic amounts of\nlabeled training data would be required. To complement our model, we propose a\nsimple and effective method for generating synthetic training data, to augment\nthe typically limited amounts of labeled training data, using commonly\navailable knowledge bases to generate additional labeled utterances. We\nextensively evaluate ConCET and our proposed training method first on an openly\navailable human-human conversational dataset called Self-Dialogue, to calibrate\nour approach against previous state-of-the-art methods; second, we evaluate\nConCET on a large dataset of human-machine conversations with real users,\ncollected as part of the Amazon Alexa Prize. Our results show that ConCET\nsignificantly improves topic classification performance on both datasets,\nincluding 8-10% improvements over state-of-the-art deep learning methods. We\ncomplement our quantitative results with detailed analysis of system\nperformance, which could be used for further improvements of conversational\nagents.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:29:08 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Sahijwani", "Harshita", ""], ["Choi", "Jason Ingyu", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2005.13803", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Harshita Sahijwani, Eugene Agichtein", "title": "Would you Like to Talk about Sports Now? Towards Contextual Topic\n  Suggestion for Open-Domain Conversational Agents", "comments": "CHIIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To hold a true conversation, an intelligent agent should be able to\noccasionally take initiative and recommend the next natural conversation topic.\nThis is a challenging task. A topic suggested by the agent should be relevant\nto the person, appropriate for the conversation context, and the agent should\nhave something interesting to say about it. Thus, a scripted, or\none-size-fits-all, popularity-based topic suggestion is doomed to fail.\nInstead, we explore different methods for a personalized, contextual topic\nsuggestion for open-domain conversations. We formalize the Conversational Topic\nSuggestion problem (CTS) to more clearly identify the assumptions and\nrequirements. We also explore three possible approaches to solve this problem:\n(1) model-based sequential topic suggestion to capture the conversation context\n(CTS-Seq), (2) Collaborative Filtering-based suggestion to capture previous\nsuccessful conversations from similar users (CTS-CF), and (3) a hybrid approach\ncombining both conversation context and collaborative filtering. To evaluate\nthe effectiveness of these methods, we use real conversations collected as part\nof the Amazon Alexa Prize 2018 Conversational AI challenge. The results are\npromising: the CTS-Seq model suggests topics with 23% higher accuracy than the\nbaseline, and incorporating collaborative filtering signals into a hybrid\nCTS-Seq-CF model further improves recommendation accuracy by 12%. Together, our\nproposed models, experiments, and analysis significantly advance the study of\nopen-domain conversational agents, and suggest promising directions for future\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:41:18 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Sahijwani", "Harshita", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2005.13804", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Jason Ingyu Choi, Eugene Agichtein", "title": "Contextual Dialogue Act Classification for Open-Domain Conversational\n  Agents", "comments": "SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying the general intent of the user utterance in a conversation, also\nknown as Dialogue Act (DA), e.g., open-ended question, statement of opinion, or\nrequest for an opinion, is a key step in Natural Language Understanding (NLU)\nfor conversational agents. While DA classification has been extensively studied\nin human-human conversations, it has not been sufficiently explored for the\nemerging open-domain automated conversational agents. Moreover, despite\nsignificant advances in utterance-level DA classification, full understanding\nof dialogue utterances requires conversational context. Another challenge is\nthe lack of available labeled data for open-domain human-machine conversations.\nTo address these problems, we propose a novel method, CDAC (Contextual Dialogue\nAct Classifier), a simple yet effective deep learning approach for contextual\ndialogue act classification. Specifically, we use transfer learning to adapt\nmodels trained on human-human conversations to predict dialogue acts in\nhuman-machine dialogues. To investigate the effectiveness of our method, we\ntrain our model on the well-known Switchboard human-human dialogue dataset, and\nfine-tune it for predicting dialogue acts in human-machine conversation data,\ncollected as part of the Amazon Alexa Prize 2018 competition. The results show\nthat the CDAC model outperforms an utterance-level state of the art baseline by\n8.0% on the Switchboard dataset, and is comparable to the latest reported\nstate-of-the-art contextual DA classification results. Furthermore, our results\nshow that fine-tuning the CDAC model on a small sample of manually labeled\nhuman-machine conversations allows CDAC to more accurately predict dialogue\nacts in real users' conversations, suggesting a promising direction for future\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:48:10 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Choi", "Jason Ingyu", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2005.13808", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand", "title": "User Intent Inference for Web Search and Conversational Agents", "comments": "WSDM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User intent understanding is a crucial step in designing both conversational\nagents and search engines. Detecting or inferring user intent is challenging,\nsince the user utterances or queries can be short, ambiguous, and contextually\ndependent. To address these research challenges, my thesis work focuses on: 1)\nUtterance topic and intent classification for conversational agents 2) Query\nintent mining and classification for Web search engines, focusing on the\ne-commerce domain. To address the first topic, I proposed novel models to\nincorporate entity information and conversation-context clues to predict both\ntopic and intent of the user's utterances. For the second research topic, I\nplan to extend the existing state of the art methods in Web search intent\nprediction to the e-commerce domain, via: 1) Developing a joint learning model\nto predict search queries' intents and the product categories associated with\nthem, 2) Discovering new hidden users' intents. All the models will be\nevaluated on the real queries available from a major e-commerce site search\nengine. The results from these studies can be leveraged to improve performance\nof various tasks such as natural language understanding, query scoping, query\nsuggestion, and ranking, resulting in an enriched user experience.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:04:42 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 16:13:38 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ahmadvand", "Ali", ""]]}, {"id": "2005.13827", "submitter": "Mittul Singh", "authors": "Mittul Singh, Sami Virpioja, Peter Smit, Mikko Kurimo", "title": "Subword RNNLM Approximations for Out-Of-Vocabulary Keyword Search", "comments": "INTERSPEECH 2019", "journal-ref": null, "doi": "10.21437/Interspeech.2019-1329", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spoken Keyword Search, the query may contain out-of-vocabulary (OOV) words\nnot observed when training the speech recognition system. Using subword\nlanguage models (LMs) in the first-pass recognition makes it possible to\nrecognize the OOV words, but even the subword n-gram LMs suffer from data\nsparsity. Recurrent Neural Network (RNN) LMs alleviate the sparsity problems\nbut are not suitable for first-pass recognition as such. One way to solve this\nis to approximate the RNNLMs by back-off n-gram models. In this paper, we\npropose to interpolate the conventional n-gram models and the RNNLM\napproximation for better OOV recognition. Furthermore, we develop a new RNNLM\napproximation method suitable for subword units: It produces variable-order\nn-grams to include long-span approximations and considers also n-grams that\nwere not originally observed in the training corpus. To evaluate these models\non OOVs, we setup Arabic and Finnish Keyword Search tasks concentrating only on\nOOV words. On these tasks, interpolating the baseline RNNLM approximation and a\nconventional LM outperforms the conventional LM in terms of the Maximum Term\nWeighted Value for single-character subwords. Moreover, replacing the baseline\napproximation with the proposed method achieves the best performance on both\nmulti- and single-character subwords.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:59:06 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 12:33:57 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Singh", "Mittul", ""], ["Virpioja", "Sami", ""], ["Smit", "Peter", ""], ["Kurimo", "Mikko", ""]]}, {"id": "2005.13837", "submitter": "Seanie Lee", "authors": "Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim, Sung Ju Hwang", "title": "Generating Diverse and Consistent QA pairs from Contexts with\n  Information-Maximizing Hierarchical Conditional VAEs", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most crucial challenges in question answering (QA) is the scarcity\nof labeled data, since it is costly to obtain question-answer (QA) pairs for a\ntarget text domain with human annotation. An alternative approach to tackle the\nproblem is to use automatically generated QA pairs from either the problem\ncontext or from large amount of unstructured texts (e.g. Wikipedia). In this\nwork, we propose a hierarchical conditional variational autoencoder (HCVAE) for\ngenerating QA pairs given unstructured texts as contexts, while maximizing the\nmutual information between generated QA pairs to ensure their consistency. We\nvalidate our Information Maximizing Hierarchical Conditional Variational\nAutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the\nperformance of the QA model (BERT-base) using only the generated QA pairs\n(QA-based evaluation) or by using both the generated and human-labeled pairs\n(semi-supervised learning) for training, against state-of-the-art baseline\nmodels. The results show that our model obtains impressive performance gains\nover all baselines on both tasks, using only a fraction of data for training.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 08:26:06 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 01:02:24 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 07:58:35 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 06:58:33 GMT"}, {"version": "v5", "created": "Mon, 15 Jun 2020 02:55:11 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lee", "Dong Bok", ""], ["Lee", "Seanie", ""], ["Jeong", "Woo Tae", ""], ["Kim", "Donghwan", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2005.13867", "submitter": "Mao Ye", "authors": "Chenpeng Zhang (1), Shuai Li (2), Mao Ye (1), Ce Zhu (2), Xue Li (3)\n  ((1) School of Computer Science and Engineering, University of Electronic\n  Science and Technology of China, (2) School of Information and Communication\n  Engineering, University of Electronic Science and Technology of China, (3)\n  School of Information Technology and Electronic Engineering, The University\n  of Queensland)", "title": "Learning Various Length Dependence by Dual Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are widely used as a memory model for\nsequence-related problems. Many variants of RNN have been proposed to solve the\ngradient problems of training RNNs and process long sequences. Although some\nclassical models have been proposed, capturing long-term dependence while\nresponding to short-term changes remains a challenge. To this problem, we\npropose a new model named Dual Recurrent Neural Networks (DuRNN). The DuRNN\nconsists of two parts to learn the short-term dependence and progressively\nlearn the long-term dependence. The first part is a recurrent neural network\nwith constrained full recurrent connections to deal with short-term dependence\nin sequence and generate short-term memory. Another part is a recurrent neural\nnetwork with independent recurrent connections which helps to learn long-term\ndependence and generate long-term memory. A selection mechanism is added\nbetween two parts to help the needed long-term information transfer to the\nindependent neurons. Multiple modules can be stacked to form a multi-layer\nmodel for better performance. Our contributions are: 1) a new recurrent model\ndeveloped based on the divide-and-conquer strategy to learn long and short-term\ndependence separately, and 2) a selection mechanism to enhance the separating\nand learning of different temporal scales of dependence. Both theoretical\nanalysis and extensive experiments are conducted to validate the performance of\nour model, and we also conduct simple visualization experiments and ablation\nanalyses for the model interpretability. Experimental results indicate that the\nproposed DuRNN model can handle not only very long sequences (over 5000 time\nsteps), but also short sequences very well. Compared with many state-of-the-art\nRNN models, our model has demonstrated efficient and better performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 09:30:01 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhang", "Chenpeng", ""], ["Li", "Shuai", ""], ["Ye", "Mao", ""], ["Zhu", "Ce", ""], ["Li", "Xue", ""]]}, {"id": "2005.13895", "submitter": "Shucong Zhang", "authors": "Shucong Zhang, Erfan Loweimi, Peter Bell, Steve Renals", "title": "When Can Self-Attention Be Replaced by Feed Forward Layers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-attention models such as Transformers have given competitive\nresults compared to recurrent neural network systems in speech recognition. The\nkey factor for the outstanding performance of self-attention models is their\nability to capture temporal relationships without being limited by the distance\nbetween two related events. However, we note that the range of the learned\ncontext progressively increases from the lower to upper self-attention layers,\nwhilst acoustic events often happen within short time spans in a left-to-right\norder. This leads to a question: for speech recognition, is a global view of\nthe entire sequence still important for the upper self-attention layers in the\nencoder of Transformers? To investigate this, we replace these self-attention\nlayers with feed forward layers. In our speech recognition experiments (Wall\nStreet Journal and Switchboard), we indeed observe an interesting result:\nreplacing the upper self-attention layers in the encoder with feed forward\nlayers leads to no performance drop, and even minor gains. Our experiments\noffer insights to how self-attention layers process the speech signal, leading\nto the conclusion that the lower self-attention layers of the encoder encode a\nsufficiently wide range of inputs, hence learning further contextual\ninformation in the upper layers is unnecessary.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:35:49 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhang", "Shucong", ""], ["Loweimi", "Erfan", ""], ["Bell", "Peter", ""], ["Renals", "Steve", ""]]}, {"id": "2005.13930", "submitter": "Benedikt Boenninghoff", "authors": "Benedikt Boenninghoff, Steffen Zeiler, Robert M. Nickel, Dorothea\n  Kolossa", "title": "Variational Autoencoder with Embedded Student-$t$ Mixture Model for\n  Authorship Attribution", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional computational authorship attribution describes a classification\ntask in a closed-set scenario. Given a finite set of candidate authors and\ncorresponding labeled texts, the objective is to determine which of the authors\nhas written another set of anonymous or disputed texts. In this work, we\npropose a probabilistic autoencoding framework to deal with this supervised\nclassification task. More precisely, we are extending a variational autoencoder\n(VAE) with embedded Gaussian mixture model to a Student-$t$ mixture model.\nAutoencoders have had tremendous success in learning latent representations.\nHowever, existing VAEs are currently still bound by limitations imposed by the\nassumed Gaussianity of the underlying probability distributions in the latent\nspace. In this work, we are extending the Gaussian model for the VAE to a\nStudent-$t$ model, which allows for an independent control of the \"heaviness\"\nof the respective tails of the implied probability densities. Experiments over\nan Amazon review dataset indicate superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:52:32 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Boenninghoff", "Benedikt", ""], ["Zeiler", "Steffen", ""], ["Nickel", "Robert M.", ""], ["Kolossa", "Dorothea", ""]]}, {"id": "2005.13962", "submitter": "Elizabeth Salesky", "authors": "Elizabeth Salesky, Eleanor Chodroff, Tiago Pimentel, Matthew Wiesner,\n  Ryan Cotterell, Alan W Black and Jason Eisner", "title": "A Corpus for Large-Scale Phonetic Typology", "comments": "Accepted to ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major hurdle in data-driven research on typology is having sufficient data\nin many languages to draw meaningful conclusions. We present VoxClamantis v1.0,\nthe first large-scale corpus for phonetic typology, with aligned segments and\nestimated phoneme-level labels in 690 readings spanning 635 languages, along\nwith acoustic-phonetic measures of vowels and sibilants. Access to such data\ncan greatly facilitate investigation of phonetic typology at a large scale and\nacross many languages. However, it is non-trivial and computationally intensive\nto obtain such alignments for hundreds of languages, many of which have few to\nno resources presently available. We describe the methodology to create our\ncorpus, discuss caveats with current methods and their impact on the utility of\nthis data, and illustrate possible research directions through a series of case\nstudies on the 48 highest-quality readings. Our corpus and scripts are publicly\navailable for non-commercial use at https://voxclamantisproject.github.io.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:03:51 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Salesky", "Elizabeth", ""], ["Chodroff", "Eleanor", ""], ["Pimentel", "Tiago", ""], ["Wiesner", "Matthew", ""], ["Cotterell", "Ryan", ""], ["Black", "Alan W", ""], ["Eisner", "Jason", ""]]}, {"id": "2005.13978", "submitter": "Hendra Setiawan", "authors": "Hendra Setiawan, Matthias Sperber, Udhay Nallasamy, Matthias Paulik", "title": "Variational Neural Machine Translation with Normalizing Flows", "comments": "To appear in 2020 Association for Computational Linguistics (ACL) as\n  a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Neural Machine Translation (VNMT) is an attractive framework for\nmodeling the generation of target translations, conditioned not only on the\nsource sentence but also on some latent random variables. The latent variable\nmodeling may introduce useful statistical dependencies that can improve\ntranslation accuracy. Unfortunately, learning informative latent variables is\nnon-trivial, as the latent space can be prohibitively large, and the latent\ncodes are prone to be ignored by many translation models at training time.\nPrevious works impose strong assumptions on the distribution of the latent code\nand limit the choice of the NMT architecture. In this paper, we propose to\napply the VNMT framework to the state-of-the-art Transformer and introduce a\nmore flexible approximate posterior based on normalizing flows. We demonstrate\nthe efficacy of our proposal under both in-domain and out-of-domain conditions,\nsignificantly outperforming strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:30:53 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Setiawan", "Hendra", ""], ["Sperber", "Matthias", ""], ["Nallasamy", "Udhay", ""], ["Paulik", "Matthias", ""]]}, {"id": "2005.14028", "submitter": "Santhosh Rajamanickam", "authors": "Santhosh Rajamanickam, Pushkar Mishra, Helen Yannakoudakis, Ekaterina\n  Shutova", "title": "Joint Modelling of Emotion and Abusive Language Detection", "comments": "Proceedings of the 58th Annual Meeting of the Association for\n  Computational Linguistics, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of online communication platforms has been accompanied by some\nundesirable effects, such as the proliferation of aggressive and abusive\nbehaviour online. Aiming to tackle this problem, the natural language\nprocessing (NLP) community has experimented with a range of techniques for\nabuse detection. While achieving substantial success, these methods have so far\nonly focused on modelling the linguistic properties of the comments and the\nonline communities of users, disregarding the emotional state of the users and\nhow this might affect their language. The latter is, however, inextricably\nlinked to abusive behaviour. In this paper, we present the first joint model of\nemotion and abusive language detection, experimenting in a multi-task learning\nframework that allows one task to inform the other. Our results demonstrate\nthat incorporating affective features leads to significant improvements in\nabuse detection performance across datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:08:40 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Rajamanickam", "Santhosh", ""], ["Mishra", "Pushkar", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2005.14050", "submitter": "Hanna Wallach", "authors": "Su Lin Blodgett and Solon Barocas and Hal Daum\\'e III and Hanna\n  Wallach", "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey 146 papers analyzing \"bias\" in NLP systems, finding that their\nmotivations are often vague, inconsistent, and lacking in normative reasoning,\ndespite the fact that analyzing \"bias\" is an inherently normative process. We\nfurther find that these papers' proposed quantitative techniques for measuring\nor mitigating \"bias\" are poorly matched to their motivations and do not engage\nwith the relevant literature outside of NLP. Based on these findings, we\ndescribe the beginnings of a path forward by proposing three recommendations\nthat should guide work analyzing \"bias\" in NLP systems. These recommendations\nrest on a greater recognition of the relationships between language and social\nhierarchies, encouraging researchers and practitioners to articulate their\nconceptualizations of \"bias\"---i.e., what kinds of system behaviors are\nharmful, in what ways, to whom, and why, as well as the normative reasoning\nunderlying these statements---and to center work around the lived experiences\nof members of communities affected by NLP systems, while interrogating and\nreimagining the power relations between technologists and such communities.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:32:08 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 16:44:18 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Blodgett", "Su Lin", ""], ["Barocas", "Solon", ""], ["Daum\u00e9", "Hal", "III"], ["Wallach", "Hanna", ""]]}, {"id": "2005.14108", "submitter": "Aminul Huq", "authors": "Aminul Huq, Mst. Tasnim Pervin", "title": "Adversarial Attacks and Defense on Texts: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been used widely for various purposes in recent\nyears in object recognition, self-driving cars, face recognition, speech\nrecognition, sentiment analysis, and many others. However, in recent years it\nhas been shown that these models possess weakness to noises which force the\nmodel to misclassify. This issue has been studied profoundly in the image and\naudio domain. Very little has been studied on this issue concerning textual\ndata. Even less survey on this topic has been performed to understand different\ntypes of attacks and defense techniques. In this manuscript, we accumulated and\nanalyzed different attacking techniques and various defense models to provide a\nmore comprehensive idea. Later we point out some of the interesting findings of\nall papers and challenges that need to be overcome to move forward in this\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:58:45 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 18:10:13 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2020 04:58:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Huq", "Aminul", ""], ["Pervin", "Mst. Tasnim", ""]]}, {"id": "2005.14134", "submitter": "Martha Lewis", "authors": "Gemma De las Cuevas and Andreas Klingler and Martha Lewis and Tim\n  Netzer", "title": "Cats climb entails mammals move: preserving hyponymy in compositional\n  distributional semantics", "comments": "Submitted to SemSpace 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To give vector-based representations of meaning more structure, one approach\nis to use positive semidefinite (psd) matrices. These allow us to model\nsimilarity of words as well as the hyponymy or is-a relationship. Psd matrices\ncan be learnt relatively easily in a given vector space $M\\otimes M^*$, but to\ncompose words to form phrases and sentences, we need representations in larger\nspaces. In this paper, we introduce a generic way of composing the psd matrices\ncorresponding to words. We propose that psd matrices for verbs, adjectives, and\nother functional words be lifted to completely positive (CP) maps that match\ntheir grammatical type. This lifting is carried out by our composition rule\ncalled Compression, Compr. In contrast to previous composition rules like Fuzz\nand Phaser (a.k.a. KMult and BMult), Compr preserves hyponymy. Mathematically,\nCompr is itself a CP map, and is therefore linear and generally\nnon-commutative. We give a number of proposals for the structure of Compr,\nbased on spiders, cups and caps, and generate a range of composition rules. We\ntest these rules on a small sentence entailment dataset, and see some\nimprovements over the performance of Fuzz and Phaser.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:31:59 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 06:44:28 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Cuevas", "Gemma De las", ""], ["Klingler", "Andreas", ""], ["Lewis", "Martha", ""], ["Netzer", "Tim", ""]]}, {"id": "2005.14165", "submitter": "Tom B Brown", "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\n  Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\n  Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\n  Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\n  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\n  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n  Radford, Ilya Sutskever, Dario Amodei", "title": "Language Models are Few-Shot Learners", "comments": "40+32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:29:03 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:08:53 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 02:52:35 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2020 19:47:17 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Brown", "Tom B.", ""], ["Mann", "Benjamin", ""], ["Ryder", "Nick", ""], ["Subbiah", "Melanie", ""], ["Kaplan", "Jared", ""], ["Dhariwal", "Prafulla", ""], ["Neelakantan", "Arvind", ""], ["Shyam", "Pranav", ""], ["Sastry", "Girish", ""], ["Askell", "Amanda", ""], ["Agarwal", "Sandhini", ""], ["Herbert-Voss", "Ariel", ""], ["Krueger", "Gretchen", ""], ["Henighan", "Tom", ""], ["Child", "Rewon", ""], ["Ramesh", "Aditya", ""], ["Ziegler", "Daniel M.", ""], ["Wu", "Jeffrey", ""], ["Winter", "Clemens", ""], ["Hesse", "Christopher", ""], ["Chen", "Mark", ""], ["Sigler", "Eric", ""], ["Litwin", "Mateusz", ""], ["Gray", "Scott", ""], ["Chess", "Benjamin", ""], ["Clark", "Jack", ""], ["Berner", "Christopher", ""], ["McCandlish", "Sam", ""], ["Radford", "Alec", ""], ["Sutskever", "Ilya", ""], ["Amodei", "Dario", ""]]}, {"id": "2005.14187", "submitter": "Hanrui Wang", "authors": "Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang\n  Gan, Song Han", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language\n  Processing", "comments": "Accepted to ACL 2020. 14 pages, 12 figures. Code available at\n  http://github.com/mit-han-lab/hardware-aware-transformers.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but\nthey are difficult to be deployed on hardware due to the intensive computation.\nTo enable low-latency inference on resource-constrained hardware platforms, we\npropose to design Hardware-Aware Transformers (HAT) with neural architecture\nsearch. We first construct a large design space with $\\textit{arbitrary\nencoder-decoder attention}$ and $\\textit{heterogeneous layers}$. Then we train\na $\\textit{SuperTransformer}$ that covers all candidates in the design space,\nand efficiently produces many $\\textit{SubTransformers}$ with weight sharing.\nFinally, we perform an evolutionary search with a hardware latency constraint\nto find a specialized $\\textit{SubTransformer}$ dedicated to run fast on the\ntarget hardware. Extensive experiments on four machine translation tasks\ndemonstrate that HAT can discover efficient models for different hardware (CPU,\nGPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT\ncan achieve $\\textbf{3}\\times$ speedup, $\\textbf{3.7}\\times$ smaller size over\nbaseline Transformer; $\\textbf{2.7}\\times$ speedup, $\\textbf{3.6}\\times$\nsmaller size over Evolved Transformer with $\\textbf{12,041}\\times$ less search\ncost and no performance loss. HAT code is\nhttps://github.com/mit-han-lab/hardware-aware-transformers.git\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:58:56 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Wang", "Hanrui", ""], ["Wu", "Zhanghao", ""], ["Liu", "Zhijian", ""], ["Cai", "Han", ""], ["Zhu", "Ligeng", ""], ["Gan", "Chuang", ""], ["Han", "Song", ""]]}, {"id": "2005.14253", "submitter": "Nicholas FitzGerald", "authors": "Thibault F\\'evry, Nicholas FitzGerald, Livio Baldini Soares, Tom\n  Kwiatkowski", "title": "Empirical Evaluation of Pretraining Strategies for Supervised Entity\n  Linking", "comments": "11 pages, 8 figures, appearing at AKBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an entity linking model which combines a Transformer\narchitecture with large scale pretraining from Wikipedia links. Our model\nachieves the state-of-the-art on two commonly used entity linking datasets:\n96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand\nwhat design choices are important for entity linking, including choices of\nnegative entity candidates, Transformer architecture, and input perturbations.\nLastly, we present promising results on more challenging settings such as\nend-to-end entity linking and entity linking without in-domain training data.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:32:52 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["F\u00e9vry", "Thibault", ""], ["FitzGerald", "Nicholas", ""], ["Soares", "Livio Baldini", ""], ["Kwiatkowski", "Tom", ""]]}, {"id": "2005.14299", "submitter": "Oskar Wysocki", "authors": "Oskar Wysocki, Malina Florea, Andre Freitas", "title": "What is SemEval evaluating? A Systematic Analysis of Evaluation\n  Campaigns in NLP", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SemEval is the primary venue in the NLP community for the proposal of new\nchallenges and for the systematic empirical evaluation of NLP systems. This\npaper provides a systematic quantitative analysis of SemEval aiming to evidence\nthe patterns of the contributions behind SemEval. By understanding the\ndistribution of task types, metrics, architectures, participation and citations\nover time we aim to answer the question on what is being evaluated by SemEval.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:17:43 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Wysocki", "Oskar", ""], ["Florea", "Malina", ""], ["Freitas", "Andre", ""]]}, {"id": "2005.14315", "submitter": "Nikita Moghe", "authors": "Nikita Moghe, Priyesh Vijayan, Balaraman Ravindran, Mitesh M. Khapra", "title": "On Incorporating Structural Information to improve Dialogue Response\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of generating dialogue responses from background\nknowledge comprising of domain specific resources. Specifically, given a\nconversation around a movie, the task is to generate the next response based on\nbackground knowledge about the movie such as the plot, review, Reddit comments\netc. This requires capturing structural, sequential and semantic information\nfrom the conversation context and the background resources. This is a new task\nand has not received much attention from the community. We propose a new\narchitecture that uses the ability of BERT to capture deep contextualized\nrepresentations in conjunction with explicit structure and sequence\ninformation. More specifically, we use (i) Graph Convolutional Networks (GCNs)\nto capture structural information, (ii) LSTMs to capture sequential information\nand (iii) BERT for the deep contextualized representations that capture\nsemantic information. We analyze the proposed architecture extensively. To this\nend, we propose a plug-and-play Semantics-Sequences-Structures (SSS) framework\nwhich allows us to effectively combine such linguistic information. Through a\nseries of experiments we make some interesting observations. First, we observe\nthat the popular adaptation of the GCN model for NLP tasks where structural\ninformation (GCNs) was added on top of sequential information (LSTMs) performs\npoorly on our task. This leads us to explore interesting ways of combining\nsemantic and structural information to improve the performance. Second, we\nobserve that while BERT already outperforms other deep contextualized\nrepresentations such as ELMo, it still benefits from the additional structural\ninformation explicitly added using GCNs. This is a bit surprising given the\nrecent claims that BERT already captures structural information. Lastly, the\nproposed SSS framework gives an improvement of 7.95% over the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:06:03 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Moghe", "Nikita", ""], ["Vijayan", "Priyesh", ""], ["Ravindran", "Balaraman", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "2005.14327", "submitter": "Jinyu Li", "authors": "Jinyu Li, Yu Wu, Yashesh Gaur, Chengyi Wang, Rui Zhao, Shujie Liu", "title": "On the Comparison of Popular End-to-End Models for Large Scale Speech\n  Recognition", "comments": "Accepted by Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a strong push to transition from hybrid models to\nend-to-end (E2E) models for automatic speech recognition. Currently, there are\nthree promising E2E methods: recurrent neural network transducer (RNN-T), RNN\nattention-based encoder-decoder (AED), and Transformer-AED. In this study, we\nconduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models,\nin both non-streaming and streaming modes. We use 65 thousand hours of\nMicrosoft anonymized training data to train these models. As E2E models are\nmore data hungry, it is better to compare their effectiveness with large amount\nof training data. To the best of our knowledge, no such comprehensive study has\nbeen conducted yet. We show that although AED models are stronger than RNN-T in\nthe non-streaming mode, RNN-T is very competitive in streaming mode if its\nencoder can be properly initialized. Among all three E2E models,\ntransformer-AED achieved the best accuracy in both streaming and non-streaming\nmode. We show that both streaming RNN-T and transformer-AED models can obtain\nbetter accuracy than a highly-optimized hybrid model.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:30:57 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 01:57:22 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Li", "Jinyu", ""], ["Wu", "Yu", ""], ["Gaur", "Yashesh", ""], ["Wang", "Chengyi", ""], ["Zhao", "Rui", ""], ["Liu", "Shujie", ""]]}, {"id": "2005.14386", "submitter": "Ruotian Luo", "authors": "Ruotian Luo and Greg Shakhnarovich", "title": "Controlling Length in Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate captioning models that allow control of caption\nlength. Our models can leverage this control to generate captions of different\nstyle and descriptiveness.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:03:15 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Luo", "Ruotian", ""], ["Shakhnarovich", "Greg", ""]]}, {"id": "2005.14408", "submitter": "Xiao Yang", "authors": "Deepak Muralidharan, Joel Ruben Antony Moniz, Sida Gao, Xiao Yang, Lin\n  Li, Justine Kao, Stephen Pulman, Atish Kothari, Ray Shen, Yinying Pan, Vivek\n  Kaul, Mubarak Seyed Ibrahim, Gang Xiang, Nan Dun, Yidan Zhou, Andy O, Yuan\n  Zhang, Pooja Chitkara, Xuan Wang, Alkesh Patel, Kushal Tayal, Roger Zheng,\n  Peter Grasch, Jason Williams", "title": "Noise-robust Named Entity Understanding for Virtual Assistants", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Understanding (NEU) plays an essential role in interactions\nbetween users and voice assistants, since successfully identifying entities and\ncorrectly linking them to their standard forms is crucial to understanding the\nuser's intent. NEU is a challenging task in voice assistants due to the\nambiguous nature of natural language and because noise introduced by speech\ntranscription and user errors occur frequently in spoken natural language\nqueries. In this paper, we propose an architecture with novel features that\njointly solves the recognition of named entities (a.k.a. Named Entity\nRecognition, or NER) and the resolution to their canonical forms (a.k.a. Entity\nLinking, or EL). We show that by combining NER and EL information in a joint\nreranking module, our proposed framework improves accuracy in both tasks. This\nimproved performance and the features that enable it, also lead to better\naccuracy in downstream tasks, such as domain classification and semantic\nparsing.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 06:14:53 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 20:32:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Muralidharan", "Deepak", ""], ["Moniz", "Joel Ruben Antony", ""], ["Gao", "Sida", ""], ["Yang", "Xiao", ""], ["Li", "Lin", ""], ["Kao", "Justine", ""], ["Pulman", "Stephen", ""], ["Kothari", "Atish", ""], ["Shen", "Ray", ""], ["Pan", "Yinying", ""], ["Kaul", "Vivek", ""], ["Ibrahim", "Mubarak Seyed", ""], ["Xiang", "Gang", ""], ["Dun", "Nan", ""], ["Zhou", "Yidan", ""], ["O", "Andy", ""], ["Zhang", "Yuan", ""], ["Chitkara", "Pooja", ""], ["Wang", "Xuan", ""], ["Patel", "Alkesh", ""], ["Tayal", "Kushal", ""], ["Zheng", "Roger", ""], ["Grasch", "Peter", ""], ["Williams", "Jason", ""]]}, {"id": "2005.14424", "submitter": "Mao Ye", "authors": "Mao Ye, Chengyue Gong, Qiang Liu", "title": "SAFER: A Structure-free Approach for Certified Robustness to Adversarial\n  Word Substitutions", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art NLP models can often be fooled by human-unaware\ntransformations such as synonymous word substitution. For security reasons, it\nis of critical importance to develop models with certified robustness that can\nprovably guarantee that the prediction is can not be altered by any possible\nsynonymous word substitution. In this work, we propose a certified robust\nmethod based on a new randomized smoothing technique, which constructs a\nstochastic ensemble by applying random word substitutions on the input\nsentences, and leverage the statistical properties of the ensemble to provably\ncertify the robustness. Our method is simple and structure-free in that it only\nrequires the black-box queries of the model outputs, and hence can be applied\nto any pre-trained models (such as BERT) and any types of models (world-level\nor subword-level). Our method significantly outperforms recent state-of-the-art\nmethods for certified robustness on both IMDB and Amazon text classification\ntasks. To the best of our knowledge, we are the first work to achieve certified\nrobustness on large systems such as BERT with practically meaningful certified\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 07:15:19 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Ye", "Mao", ""], ["Gong", "Chengyue", ""], ["Liu", "Qiang", ""]]}, {"id": "2005.14435", "submitter": "Xiang Hao", "authors": "Xiang Hao, Shixue Wen, Xiangdong Su, Yun Liu, Guanglai Gao and Xiaofei\n  Li", "title": "Sub-Band Knowledge Distillation Framework for Speech Enhancement", "comments": "Published in Interspeech 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1539", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single-channel speech enhancement, methods based on full-band spectral\nfeatures have been widely studied. However, only a few methods pay attention to\nnon-full-band spectral features. In this paper, we explore a knowledge\ndistillation framework based on sub-band spectral mapping for single-channel\nspeech enhancement. Specifically, we divide the full frequency band into\nmultiple sub-bands and pre-train an elite-level sub-band enhancement model\n(teacher model) for each sub-band. These teacher models are dedicated to\nprocessing their own sub-bands. Next, under the teacher models' guidance, we\ntrain a general sub-band enhancement model (student model) that works for all\nsub-bands. Without increasing the number of model parameters and computational\ncomplexity, the student model's performance is further improved. To evaluate\nour proposed method, we conducted a large number of experiments on an\nopen-source data set. The final experimental results show that the guidance\nfrom the elite-level teacher models dramatically improves the student model's\nperformance, which exceeds the full-band model by employing fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 07:55:12 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 12:14:59 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hao", "Xiang", ""], ["Wen", "Shixue", ""], ["Su", "Xiangdong", ""], ["Liu", "Yun", ""], ["Gao", "Guanglai", ""], ["Li", "Xiaofei", ""]]}, {"id": "2005.14441", "submitter": "Xiang Hao", "authors": "Xiang Hao, Xiangdong Su, Zhiyu Wang, Qiang Zhang, Huali Xu and\n  Guanglai Gao", "title": "SNR-Based Teachers-Student Technique for Speech Enhancement", "comments": "Published in 2020 IEEE International Conference on Multimedia and\n  Expo (ICME 2020)", "journal-ref": null, "doi": "10.1109/ICME46284.2020.9102846", "report-no": null, "categories": "eess.AS cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is very challenging for speech enhancement methods to achieves robust\nperformance under both high signal-to-noise ratio (SNR) and low SNR\nsimultaneously. In this paper, we propose a method that integrates an SNR-based\nteachers-student technique and time-domain U-Net to deal with this problem.\nSpecifically, this method consists of multiple teacher models and a student\nmodel. We first train the teacher models under multiple small-range SNRs that\ndo not coincide with each other so that they can perform speech enhancement\nwell within the specific SNR range. Then, we choose different teacher models to\nsupervise the training of the student model according to the SNR of the\ntraining data. Eventually, the student model can perform speech enhancement\nunder both high SNR and low SNR. To evaluate the proposed method, we\nconstructed a dataset with an SNR ranging from -20dB to 20dB based on the\npublic dataset. We experimentally analyzed the effectiveness of the SNR-based\nteachers-student technique and compared the proposed method with several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 08:13:01 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 12:12:20 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hao", "Xiang", ""], ["Su", "Xiangdong", ""], ["Wang", "Zhiyu", ""], ["Zhang", "Qiang", ""], ["Xu", "Huali", ""], ["Gao", "Guanglai", ""]]}, {"id": "2005.14464", "submitter": "Jiwei Li", "authors": "Xiaoya Li, Mingxin Zhou, Jiawei Wu, Arianna Yuan, Fei Wu and Jiwei Li", "title": "Analyzing COVID-19 on Online Social Media: Trends, Sentiments and\n  Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the time of writing, the ongoing pandemic of coronavirus disease\n(COVID-19) has caused severe impacts on society, economy and people's daily\nlives. People constantly express their opinions on various aspects of the\npandemic on social media, making user-generated content an important source for\nunderstanding public emotions and concerns. In this paper, we perform a\ncomprehensive analysis on the affective trajectories of the American people and\nthe Chinese people based on Twitter and Weibo posts between January 20th, 2020\nand May 11th 2020. Specifically, by identifying people's sentiments, emotions\n(i.e., anger, disgust, fear, happiness, sadness, surprise) and the emotional\ntriggers (e.g., what a user is angry/sad about) we are able to depict the\ndynamics of public affect in the time of COVID-19. By contrasting two very\ndifferent countries, China and the Unites States, we reveal sharp differences\nin people's views on COVID-19 in different cultures. Our study provides a\ncomputational approach to unveiling public emotions and concerns on the\npandemic in real-time, which would potentially help policy-makers better\nunderstand people's need and thus make optimal policy.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:24:38 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 09:41:10 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 06:36:29 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Li", "Xiaoya", ""], ["Zhou", "Mingxin", ""], ["Wu", "Jiawei", ""], ["Yuan", "Arianna", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""]]}, {"id": "2005.14489", "submitter": "Patrick Wilken", "authors": "Patrick Wilken, Tamer Alkhouli, Evgeny Matusov, Pavel Golik", "title": "Neural Simultaneous Speech Translation Using Alignment-Based Chunking", "comments": "IWSLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In simultaneous machine translation, the objective is to determine when to\nproduce a partial translation given a continuous stream of source words, with a\ntrade-off between latency and quality. We propose a neural machine translation\n(NMT) model that makes dynamic decisions when to continue feeding on input or\ngenerate output words. The model is composed of two main components: one to\ndynamically decide on ending a source chunk, and another that translates the\nconsumed chunk. We train the components jointly and in a manner consistent with\nthe inference conditions. To generate chunked training data, we propose a\nmethod that utilizes word alignment while also preserving enough context. We\ncompare models with bidirectional and unidirectional encoders of different\ndepths, both on real speech and text input. Our results on the IWSLT 2020\nEnglish-to-German task outperform a wait-k baseline by 2.6 to 3.7% BLEU\nabsolute.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 10:20:48 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Wilken", "Patrick", ""], ["Alkhouli", "Tamer", ""], ["Matusov", "Evgeny", ""], ["Golik", "Pavel", ""]]}, {"id": "2005.14576", "submitter": "Dieter Schn\\\"app", "authors": "Susanne Arndt, Dieter Schn\\\"app", "title": "Harbsafe-162. A Domain-Specific Data Set for the Intrinsic Evaluation of\n  Semantic Representations for Terminological Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The article presents Harbsafe-162, a domain-specific data set for evaluating\ndistributional semantic models. It originates from a cooperation by Technische\nUniversit\\\"at Braunschweig and the German Commission for Electrical, Electronic\n& Information Technologies of DIN and VDE, the Harbsafe project. One objective\nof the project is to apply distributional semantic models to terminological\nentries, that is, complex lexical data comprising of at least one or several\nterms, term phrases and a definition. This application is needed to solve a\nmore complex problem: the harmonization of terminologies of standards and\nstandards bodies (i.e. resolution of doublettes and inconsistencies). Due to a\nlack of evaluation data sets for terminological entries, the creation of\nHarbsafe-162 was a necessary step towards harmonization assistance.\nHarbsafe-162 covers data from nine electrotechnical standards in the domain of\nfunctional safety, IT security, and dependability. An intrinsic evaluation\nmethod in the form of a similarity rating task has been applied in which two\nlinguists and three domain experts from standardization participated. The data\nset is used to evaluate a specific implementation of an established sentence\nembedding model. This implementation proves to be satisfactory for the\ndomain-specific data so that further implementations for harmonization\nassistance may be brought forward by the project. Considering recent criticism\non intrinsic evaluation methods, the article concludes with an evaluation of\nHarbsafe-162 and joins a more general discussion about the nature of similarity\nrating tasks. Harbsafe-162 has been made available for the community.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 13:56:31 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Arndt", "Susanne", ""], ["Schn\u00e4pp", "Dieter", ""]]}, {"id": "2005.14578", "submitter": "Benjamin Milde", "authors": "Benjamin Milde, Chris Biemann", "title": "Improving Unsupervised Sparsespeech Acoustic Models with Categorical\n  Reparameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sparsespeech model is an unsupervised acoustic model that can generate\ndiscrete pseudo-labels for untranscribed speech. We extend the Sparsespeech\nmodel to allow for sampling over a random discrete variable, yielding\npseudo-posteriorgrams. The degree of sparsity in this posteriorgram can be\nfully controlled after the model has been trained. We use the Gumbel-Softmax\ntrick to approximately sample from a discrete distribution in the neural\nnetwork and this allows us to train the network efficiently with standard\nbackpropagation. The new and improved model is trained and evaluated on the\nLibri-Light corpus, a benchmark for ASR with limited or no supervision. The\nmodel is trained on 600h and 6000h of English read speech. We evaluate the\nimproved model using the ABX error measure and a semi-supervised setting with\n10h of transcribed speech. We observe a relative improvement of up to 31.4% on\nABX error rates across speakers on the test set with the improved Sparsespeech\nmodel on 600h of speech data and further improvements when we scale the model\nto 6000h.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 13:58:36 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Milde", "Benjamin", ""], ["Biemann", "Chris", ""]]}, {"id": "2005.14613", "submitter": "Kalyani Roy", "authors": "Kalyani Roy (1), Smit Shah (1), Nithish Pai (2), Jaidam Ramtej (2),\n  Prajit Prashant Nadkarn (2), Jyotirmoy Banerjee (2), Pawan Goyal (1), and\n  Surender Kumar (2) ((1) Indian Institute of Technology Kharagpur, (2)\n  Flipkart)", "title": "Using Large Pretrained Language Models for Answering User Queries from\n  Product Specifications", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While buying a product from the e-commerce websites, customers generally have\na plethora of questions. From the perspective of both the e-commerce service\nprovider as well as the customers, there must be an effective question\nanswering system to provide immediate answers to the user queries. While\ncertain questions can only be answered after using the product, there are many\nquestions which can be answered from the product specification itself. Our work\ntakes a first step in this direction by finding out the relevant product\nspecifications, that can help answering the user questions. We propose an\napproach to automatically create a training dataset for this problem. We\nutilize recently proposed XLNet and BERT architectures for this problem and\nfind that they provide much better performance than the Siamese model,\npreviously applied for this problem. Our model gives a good performance even\nwhen trained on one vertical and tested across different verticals.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 14:52:33 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Roy", "Kalyani", ""], ["Shah", "Smit", ""], ["Pai", "Nithish", ""], ["Ramtej", "Jaidam", ""], ["Nadkarn", "Prajit Prashant", ""], ["Banerjee", "Jyotirmoy", ""], ["Goyal", "Pawan", ""], ["Kumar", "Surender", ""]]}, {"id": "2005.14627", "submitter": "Md Gulzar Hussain", "authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, and\n  Sakib Al Hasan", "title": "Detection of Bangla Fake News using MNB and SVM Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:38:54 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hussain", "Md Gulzar", ""], ["Hasan", "Md Rashidul", ""], ["Rahman", "Mahmuda", ""], ["Protim", "Joy", ""], ["Hasan", "Sakib Al", ""]]}, {"id": "2005.14662", "submitter": "Yusuke Takimoto", "authors": "Yusuke Takimoto, Yosuke Fukuchi, Shoya Matsumori, Michita Imai", "title": "SLAM-Inspired Simultaneous Contextualization and Interpreting for\n  Incremental Conversation Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representation of words has improved the performance for many\nnatural language tasks. In many methods, however, only one meaning is\nconsidered for one label of a word, and multiple meanings of polysemous words\ndepending on the context are rarely handled. Although research works have dealt\nwith polysemous words, they determine the meanings of such words according to a\nbatch of large documents. Hence, there are two problems with applying these\nmethods to sequential sentences, as in a conversation that contains ambiguous\nexpressions. The first problem is that the methods cannot sequentially deal\nwith the interdependence between context and word interpretation, in which\ncontext is decided by word interpretations and the word interpretations are\ndecided by the context. Context estimation must thus be performed in parallel\nto pursue multiple interpretations. The second problem is that the previous\nmethods use large-scale sets of sentences for offline learning of new\ninterpretations, and the steps of learning and inference are clearly separated.\nSuch methods using offline learning cannot obtain new interpretations during a\nconversation. Hence, to dynamically estimate the conversation context and\ninterpretations of polysemous words in sequential sentences, we propose a\nmethod of Simultaneous Contextualization And INterpreting (SCAIN) based on the\ntraditional Simultaneous Localization And Mapping (SLAM) algorithm. By using\nthe SCAIN algorithm, we can sequentially optimize the interdependence between\ncontext and word interpretation while obtaining new interpretations online. For\nexperimental evaluation, we created two datasets: one from Wikipedia's\ndisambiguation pages and the other from real conversations. For both datasets,\nthe results confirmed that SCAIN could effectively achieve sequential\noptimization of the interdependence and acquisition of new interpretations.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:40:27 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Takimoto", "Yusuke", ""], ["Fukuchi", "Yosuke", ""], ["Matsumori", "Shoya", ""], ["Imai", "Michita", ""]]}, {"id": "2005.14672", "submitter": "Rob van der Goot", "authors": "Rob van der Goot, Ahmet \\\"Ust\\\"un, Alan Ramponi, Ibrahim Sharaf,\n  Barbara Plank", "title": "Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning\n  in NLP", "comments": "EACL demo version (MaChAmp 0.2) https://machamp-nlp.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning, particularly approaches that combine multi-task learning\nwith pre-trained contextualized embeddings and fine-tuning, have advanced the\nfield of Natural Language Processing tremendously in recent years. In this\npaper we present MaChAmp, a toolkit for easy fine-tuning of contextualized\nembeddings in multi-task settings. The benefits of MaChAmp are its flexible\nconfiguration options, and the support of a variety of natural language\nprocessing tasks in a uniform toolkit, from text classification and sequence\nlabeling to dependency parsing, masked language modeling, and text generation.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:54:50 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:48:02 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 17:30:21 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 16:30:11 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["van der Goot", "Rob", ""], ["\u00dcst\u00fcn", "Ahmet", ""], ["Ramponi", "Alan", ""], ["Sharaf", "Ibrahim", ""], ["Plank", "Barbara", ""]]}, {"id": "2005.14690", "submitter": "Prashant Kapil", "authors": "Prashant Kapil, Asif Ekbal, Dipankar Das", "title": "Investigating Deep Learning Approaches for Hate Speech Detection in\n  Social Media", "comments": "12 pages, 2 figures, 8 tables. Accepted in CICLing: International\n  Conference on Computational Linguistics and Intelligent Text Processing,\n  2019. Modified after reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The phenomenal growth on the internet has helped in empowering individual's\nexpressions, but the misuse of freedom of expression has also led to the\nincrease of various cyber crimes and anti-social activities. Hate speech is one\nsuch issue that needs to be addressed very seriously as otherwise, this could\npose threats to the integrity of the social fabrics.\n  In this paper, we proposed deep learning approaches utilizing various\nembeddings for detecting various types of hate speeches in social media.\nDetecting hate speech from a large volume of text, especially tweets which\ncontains limited contextual information also poses several practical\nchallenges.\n  Moreover, the varieties in user-generated data and the presence of various\nforms of hate speech makes it very challenging to identify the degree and\nintention of the message. Our experiments on three publicly available datasets\nof different domains shows a significant improvement in accuracy and F1-score.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:28:46 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Kapil", "Prashant", ""], ["Ekbal", "Asif", ""], ["Das", "Dipankar", ""]]}, {"id": "2005.14709", "submitter": "Viktor Schlegel", "authors": "Viktor Schlegel, Goran Nenadic, Riza Batista-Navarro", "title": "Beyond Leaderboards: A survey of methods for revealing weaknesses in\n  Natural Language Inference data and models", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a growing number of publications that analyse Natural\nLanguage Inference (NLI) datasets for superficial cues, whether they undermine\nthe complexity of the tasks underlying those datasets and how they impact those\nmodels that are optimised and evaluated on this data. This structured survey\nprovides an overview of the evolving research area by categorising reported\nweaknesses in models and datasets and the methods proposed to reveal and\nalleviate those weaknesses for the English language. We summarise and discuss\nthe findings and conclude with a set of recommendations for possible future\nresearch directions. We hope it will be a useful resource for researchers who\npropose new datasets, to have a set of tools to assess the suitability and\nquality of their data to evaluate various phenomena of interest, as well as\nthose who develop novel architectures, to further understand the implications\nof their improvements with respect to their model's acquired capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:55:57 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Schlegel", "Viktor", ""], ["Nenadic", "Goran", ""], ["Batista-Navarro", "Riza", ""]]}, {"id": "2005.14714", "submitter": "Sebastian Bischoff", "authors": "Sebastian Bischoff, Niklas Deckers, Marcel Schliebs, Ben Thies,\n  Matthias Hagen, Efstathios Stamatatos, Benno Stein, Martin Potthast", "title": "The Importance of Suppressing Domain Style in Authorship Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prerequisite of many approaches to authorship analysis is a\nrepresentation of writing style. But despite decades of research, it still\nremains unclear to what extent commonly used and widely accepted\nrepresentations like character trigram frequencies actually represent an\nauthor's writing style, in contrast to more domain-specific style components or\neven topic. We address this shortcoming for the first time in a novel\nexperimental setup of fixed authors but swapped domains between training and\ntesting. With this setup, we reveal that approaches using character trigram\nfeatures are highly susceptible to favor domain information when applied\nwithout attention to domains, suffering drops of up to 55.4 percentage points\nin classification accuracy under domain swapping. We further propose a new\nremedy based on domain-adversarial learning and compare it to ones from the\nliterature based on heuristic rules. Both can work well, reducing accuracy\nlosses under domain swapping to 3.6% and 3.9%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:58:19 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Bischoff", "Sebastian", ""], ["Deckers", "Niklas", ""], ["Schliebs", "Marcel", ""], ["Thies", "Ben", ""], ["Hagen", "Matthias", ""], ["Stamatatos", "Efstathios", ""], ["Stein", "Benno", ""], ["Potthast", "Martin", ""]]}, {"id": "2005.14716", "submitter": "Kevin Tang", "authors": "Kevin Tang, Jason A. Shaw", "title": "Prosody leaks into the memories of words", "comments": "41 pages, 1 figure", "journal-ref": "Cognition 210. 104601 (2021)", "doi": "10.1016/j.cognition.2021.104601", "report-no": null, "categories": "cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average predictability (aka informativity) of a word in context has been\nshown to condition word duration (Seyfarth, 2014). All else being equal, words\nthat tend to occur in more predictable environments are shorter than words that\ntend to occur in less predictable environments. One account of the\ninformativity effect on duration is that the acoustic details of probabilistic\nreduction are stored as part of a word's mental representation. Other research\nhas argued that predictability effects are tied to prosodic structure in\nintegral ways. With the aim of assessing a potential prosodic basis for\ninformativity effects in speech production, this study extends past work in two\ndirections; it investigated informativity effects in another large language,\nMandarin Chinese, and broadened the study beyond word duration to additional\nacoustic dimensions, pitch and intensity, known to index prosodic prominence.\nThe acoustic information of content words was extracted from a large telephone\nconversation speech corpus with over 400,000 tokens and 6,000 word types spoken\nby 1,655 individuals and analyzed for the effect of informativity using\nfrequency statistics estimated from a 431 million word subtitle corpus. Results\nindicated that words with low informativity have shorter durations, replicating\nthe effect found in English. In addition, informativity had significant effects\non maximum pitch and intensity, two phonetic dimensions related to prosodic\nprominence. Extending this interpretation, these results suggest that\npredictability is closely linked to prosodic prominence, and that the lexical\nrepresentation of a word includes phonetic details associated with its average\nprosodic prominence in discourse. In other words, the lexicon absorbs prosodic\ninfluences on speech production.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:58:33 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 16:09:11 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tang", "Kevin", ""], ["Shaw", "Jason A.", ""]]}]