[{"id": "1511.00040", "submitter": "Sta\\v{s}a Milojevi\\'c", "authors": "Sta\\v{s}a Milojevi\\'c", "title": "Quantifying the Cognitive Extent of Science", "comments": "Accepted for publication in Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL astro-ph.IM cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the modern science is characterized by an exponential growth in\nscientific literature, the increase in publication volume clearly does not\nreflect the expansion of the cognitive boundaries of science. Nevertheless,\nmost of the metrics for assessing the vitality of science or for making funding\nand policy decisions are based on productivity. Similarly, the increasing level\nof knowledge production by large science teams, whose results often enjoy\ngreater visibility, does not necessarily mean that \"big science\" leads to\ncognitive expansion. Here we present a novel, big-data method to quantify the\nextents of cognitive domains of different bodies of scientific literature\nindependently from publication volume, and apply it to 20 million articles\npublished over 60-130 years in physics, astronomy, and biomedicine. The method\nis based on the lexical diversity of titles of fixed quotas of research\narticles. Owing to large size of quotas, the method overcomes the inherent\nstochasticity of article titles to achieve <1% precision. We show that the\nperiods of cognitive growth do not necessarily coincide with the trends in\npublication volume. Furthermore, we show that the articles produced by larger\nteams cover significantly smaller cognitive territory than (the same quota of)\narticles from smaller teams. Our findings provide a new perspective on the role\nof small teams and individual researchers in expanding the cognitive boundaries\nof science. The proposed method of quantifying the extent of the cognitive\nterritory can also be applied to study many other aspects of \"science of\nscience.\"\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 22:21:45 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 16:15:01 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "1511.00060", "submitter": "Xingxing Zhang", "authors": "Xingxing Zhang, Liang Lu, Mirella Lapata", "title": "Top-down Tree Long Short-Term Memory Networks", "comments": "to appear in NAACL 2016; code available at\n  https://github.com/XingxingZhang/td-treelstm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) networks, a type of recurrent neural network\nwith a more complex computational unit, have been successfully applied to a\nvariety of sequence modeling tasks. In this paper we develop Tree Long\nShort-Term Memory (TreeLSTM), a neural network model based on LSTM, which is\ndesigned to predict a tree rather than a linear sequence. TreeLSTM defines the\nprobability of a sentence by estimating the generation probability of its\ndependency tree. At each time step, a node is generated based on the\nrepresentation of the generated sub-tree. We further enhance the modeling power\nof TreeLSTM by explicitly representing the correlations between left and right\ndependents. Application of our model to the MSR sentence completion challenge\nachieves results beyond the current state of the art. We also report results on\ndependency parsing reranking achieving competitive performance.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 02:05:28 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 00:48:42 GMT"}, {"version": "v3", "created": "Sun, 3 Apr 2016 23:30:17 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Zhang", "Xingxing", ""], ["Lu", "Liang", ""], ["Lapata", "Mirella", ""]]}, {"id": "1511.00215", "submitter": "Peilu Wang", "authors": "Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao", "title": "A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network\n  with Word Embedding", "comments": "Rejected by EMNLP 2015, score: 4,3,3 (full is 5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has\nbeen shown to be very effective for modeling and predicting sequential data,\ne.g. speech utterances or handwritten documents. In this study, we propose to\nuse BLSTM-RNN for a unified tagging solution that can be applied to various\ntagging tasks including part-of-speech tagging, chunking and named entity\nrecognition. Instead of exploiting specific features carefully optimized for\neach task, our solution only uses one set of task-independent features and\ninternal representations learnt from unlabeled text for all tasks.Requiring no\ntask specific knowledge or sophisticated feature engineering, our approach gets\nnearly state-of-the-art performance in all these three tagging tasks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 07:59:48 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Wang", "Peilu", ""], ["Qian", "Yao", ""], ["Soong", "Frank K.", ""], ["He", "Lei", ""], ["Zhao", "Hai", ""]]}, {"id": "1511.00352", "submitter": "Abhinav Maurya", "authors": "Abhinav Maurya", "title": "Spatial Semantic Scan: Jointly Detecting Subtle Events and their Spatial\n  Footprint", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods have been proposed for detecting emerging events in text streams\nusing topic modeling. However, these methods have shortcomings that make them\nunsuitable for rapid detection of locally emerging events on massive text\nstreams. We describe Spatially Compact Semantic Scan (SCSS) that has been\ndeveloped specifically to overcome the shortcomings of current methods in\ndetecting new spatially compact events in text streams. SCSS employs\nalternating optimization between using semantic scan to estimate contrastive\nforeground topics in documents, and discovering spatial neighborhoods with high\noccurrence of documents containing the foreground topics. We evaluate our\nmethod on Emergency Department chief complaints dataset (ED dataset) to verify\nthe effectiveness of our method in detecting real-world disease outbreaks from\nfree-text ED chief complaint data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 01:45:41 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 03:01:41 GMT"}, {"version": "v3", "created": "Sat, 28 May 2016 18:59:48 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Maurya", "Abhinav", ""]]}, {"id": "1511.00360", "submitter": "Lei Xie", "authors": "Chuang Ding, Lei Xie, Jie Yan, Weini Zhang, Yang Liu", "title": "Automatic Prosody Prediction for Chinese Speech Synthesis using\n  BLSTM-RNN and Embedding Features", "comments": "5 pages, 4 figures, ASRU 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prosody affects the naturalness and intelligibility of speech. However,\nautomatic prosody prediction from text for Chinese speech synthesis is still a\ngreat challenge and the traditional conditional random fields (CRF) based\nmethod always heavily relies on feature engineering. In this paper, we propose\nto use neural networks to predict prosodic boundary labels directly from\nChinese characters without any feature engineering. Experimental results show\nthat stacking feed-forward and bidirectional long short-term memory (BLSTM)\nrecurrent network layers achieves superior performance over the CRF-based\nmethod. The embedding features learned from raw text further enhance the\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 02:34:12 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ding", "Chuang", ""], ["Xie", "Lei", ""], ["Yan", "Jie", ""], ["Zhang", "Weini", ""], ["Liu", "Yang", ""]]}, {"id": "1511.00622", "submitter": "Steffen Eger", "authors": "Steffen Eger", "title": "On the Number of Many-to-Many Alignments of Multiple Sequences", "comments": null, "journal-ref": "Journal of Automata, Languages and Combinatorics 20 (2015) 1,\n  53-65", "doi": null, "report-no": null, "categories": "math.CO cs.CL cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We count the number of alignments of $N \\ge 1$ sequences when match-up types\nare from a specified set $S\\subseteq \\mathbb{N}^N$. Equivalently, we count the\nnumber of nonnegative integer matrices whose rows sum to a given fixed vector\nand each of whose columns lie in $S$. We provide a new asymptotic formula for\nthe case $S=\\{(s_1,\\ldots,s_N) \\:|\\: 1\\le s_i\\le 2\\}$.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 18:17:26 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 11:31:27 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Eger", "Steffen", ""]]}, {"id": "1511.01042", "submitter": "Junyoung Chung", "authors": "Junyoung Chung and Jacob Devlin and Hany Hassan Awadalla", "title": "Detecting Interrogative Utterances with Recurrent Neural Networks", "comments": "6 pages, accepted to NIPS 2015 Workshop on Machine Learning for\n  Spoken Language Understanding and Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore different neural network architectures that can\npredict if a speaker of a given utterance is asking a question or making a\nstatement. We com- pare the outcomes of regularization methods that are\npopularly used to train deep neural networks and study how different context\nfunctions can affect the classification performance. We also compare the\nefficacy of gated activation functions that are favorably used in recurrent\nneural networks and study how to combine multimodal inputs. We evaluate our\nmodels on two multimodal datasets: MSR-Skype and CALLHOME.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 19:26:16 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 03:54:19 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Chung", "Junyoung", ""], ["Devlin", "Jacob", ""], ["Awadalla", "Hany Hassan", ""]]}, {"id": "1511.01158", "submitter": "Minwei Feng", "authors": "Minwei Feng, Bing Xiang, Bowen Zhou", "title": "Distributed Deep Learning for Question Answering", "comments": "This paper will appear in the Proceeding of The 25th ACM\n  International Conference on Information and Knowledge Management (CIKM 2016),\n  Indianapolis, USA", "journal-ref": null, "doi": "10.1145/2983323.2983377", "report-no": null, "categories": "cs.LG cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an empirical study of the distributed deep learning for\nquestion answering subtasks: answer selection and question classification.\nComparison studies of SGD, MSGD, ADADELTA, ADAGRAD, ADAM/ADAMAX, RMSPROP,\nDOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results\nshow that the distributed framework based on the message passing interface can\naccelerate the convergence speed at a sublinear scale. This paper demonstrates\nthe importance of distributed training. For example, with 48 workers, a 24x\nspeedup is achievable for the answer selection task and running time is\ndecreased from 138.2 hours to 5.81 hours, which will increase the productivity\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 23:18:35 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 15:41:54 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 16:41:37 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Feng", "Minwei", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1511.01259", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO), Karima Rafes (TAO, LRI)", "title": "Transforming Wikipedia into an Ontology-based Information Retrieval\n  Search Engine for Local Experts using a Third-Party Taxonomy", "comments": "Joint Second Workshop on Language and Ontology \\& Terminology and\n  Knowledge Structures (LangOnto2 + TermiKS) LO2TKS, May 2016, Portoroz,\n  Slovenia. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is widely used for finding general information about a wide variety\nof topics. Its vocation is not to provide local information. For example, it\nprovides plot, cast, and production information about a given movie, but not\nshowing times in your local movie theatre. Here we describe how we can connect\nlocal information to Wikipedia, without altering its content. The case study we\npresent involves finding local scientific experts. Using a third-party\ntaxonomy, independent from Wikipedia's category hierarchy, we index information\nconnected to our local experts, present in their activity reports, and we\nre-index Wikipedia content using the same taxonomy. The connections between\nWikipedia pages and local expert reports are stored in a relational database,\naccessible through as public SPARQL endpoint. A Wikipedia gadget (or plugin)\nactivated by the interested user, accesses the endpoint as each Wikipedia page\nis accessed. An additional tab on the Wikipedia page allows the user to open up\na list of teams of local experts associated with the subject matter in the\nWikipedia page. The technique, though presented here as a way to identify local\nexperts, is generic, in that any third party taxonomy, can be used in this to\nconnect Wikipedia to any non-Wikipedia data source.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 09:41:31 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 14:45:27 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"], ["Rafes", "Karima", "", "TAO, LRI"]]}, {"id": "1511.01432", "submitter": "Andrew Dai", "authors": "Andrew M. Dai and Quoc V. Le", "title": "Semi-supervised Sequence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two approaches that use unlabeled data to improve sequence\nlearning with recurrent networks. The first approach is to predict what comes\nnext in a sequence, which is a conventional language model in natural language\nprocessing. The second approach is to use a sequence autoencoder, which reads\nthe input sequence into a vector and predicts the input sequence again. These\ntwo algorithms can be used as a \"pretraining\" step for a later supervised\nsequence learning algorithm. In other words, the parameters obtained from the\nunsupervised step can be used as a starting point for other supervised training\nmodels. In our experiments, we find that long short term memory recurrent\nnetworks after being pretrained with the two approaches are more stable and\ngeneralize better. With pretraining, we are able to train long short term\nmemory recurrent networks up to a few hundred timesteps, thereby achieving\nstrong performance in many text classification tasks, such as IMDB, DBpedia and\n20 Newsgroups.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 18:48:36 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Dai", "Andrew M.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1511.01480", "submitter": "Maurizio Naldi", "authors": "Maurizio Naldi", "title": "Approximation of the truncated Zeta distribution and Zipf's law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law appears in many application areas but does not have a closed form\nexpression, which may make its use cumbersome. Since it coincides with the\ntruncated version of the Zeta distribution, in this paper we propose three\napproximate closed form expressions for the truncated Zeta distribution, which\nmay be employed for Zipf's law as well. The three approximations are based on\nthe replacement of the sum occurring in Zipf's law with an integral, and are\nnamed respectively the integral approximation, the average integral\napproximation, and the trapezoidal approximation. While the first one is shown\nto be of little use, the trapezoidal approximation exhibits an error which is\ntypically lower than 1\\%, but is as low as 0.1\\% for the range of values of the\nZipf parameter below 1.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 19:30:27 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Naldi", "Maurizio", ""]]}, {"id": "1511.01556", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Chih-Kai Huang, Hongsu Wang, Peter K. Bol", "title": "Mining Local Gazetteers of Literary Chinese with CRF and Pattern based\n  Methods for Biographical Information in Chinese History", "comments": "11 pages, 5 figures, 5 tables, the Third Workshop on Big Humanities\n  Data (2015 IEEE BigData), the 29th Pacific Asia Conference on Language,\n  Information and Computation (PACLIC 29)", "journal-ref": null, "doi": "10.1109/BigData.2015.7363931", "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person names and location names are essential building blocks for identifying\nevents and social networks in historical documents that were written in\nliterary Chinese. We take the lead to explore the research on algorithmically\nrecognizing named entities in literary Chinese for historical studies with\nlanguage-model based and conditional-random-field based methods, and extend our\nwork to mining the document structures in historical documents. Practical\nevaluations were conducted with texts that were extracted from more than 220\nvolumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single\nmost important collection that contains information about officers who served\nin local government in Chinese history. Our methods performed very well on\nthese realistic tests. Thousands of names and addresses were identified from\nthe texts. A good portion of the extracted names match the biographical\ninformation currently recorded in the China Biographical Database (CBDB) of\nHarvard University, and many others can be verified by historians and will\nbecome as new additions to CBDB.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 23:39:46 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Huang", "Chih-Kai", ""], ["Wang", "Hongsu", ""], ["Bol", "Peter K.", ""]]}, {"id": "1511.01559", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Hongsu Wang, Wen-Huei Cheng, Chu-Ting Hsu, Wei-Yun Chiu", "title": "Color Aesthetics and Social Networks in Complete Tang Poems:\n  Explorations and Discoveries", "comments": "10 pages, 1 figure, 8 tables, The 29th Pacific Asia Conference on\n  Language, Information and Computation (PACLIC 29), The 27th Conference on\n  Computational Linguistics and Speech Analysis (ROCLING XXVII, Chinese\n  version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Complete Tang Poems (CTP) is the most important source to study Tang\npoems. We look into CTP with computational tools from specific linguistic\nperspectives, including distributional semantics and collocational analysis.\nFrom such quantitative viewpoints, we compare the usage of \"wind\" and \"moon\" in\nthe poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,\nand play a crucial role in the imageries of poems. Thus, words for colors are\nstudied, and \"white\" is the main focus because it is the most frequent color in\nCTP. We also explore some cases of using colored words in antithesis pairs that\nwere central for fostering the imageries of the poems. CTP also contains useful\nhistorical information, and we extract person names in CTP to study the social\nnetworks of the Tang poets. Such information can then be integrated with the\nChina Biographical Database of Harvard University.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 00:21:40 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Wang", "Hongsu", ""], ["Cheng", "Wen-Huei", ""], ["Hsu", "Chu-Ting", ""], ["Chiu", "Wei-Yun", ""]]}, {"id": "1511.01574", "submitter": "Ciprian Chelba", "authors": "Ciprian Chelba and Fernando Pereira", "title": "Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix\n  Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Sparse Non-negative Matrix (SNM) language model estimation using\nmultinomial loss on held-out data.\n  Being able to train on held-out data is important in practical situations\nwhere the training data is usually mismatched from the held-out/test data. It\nis also less constrained than the previous training algorithm using\nleave-one-out on training data: it allows the use of richer meta-features in\nthe adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing\nwhich would be difficult to deal with correctly in leave-one-out training.\n  In experiments on the one billion words language modeling benchmark, we are\nable to slightly improve on our previous results which use a different loss\nfunction, and employ leave-one-out training on a subset of the main training\nset. Surprisingly, an adjustment model with meta-features that discard all\nlexical information can perform as well as lexicalized meta-features. We find\nthat fairly small amounts of held-out data (on the order of 30-70 thousand\nwords) are sufficient for training the adjustment model.\n  In a real-life scenario where the training data is a mix of data sources that\nare imbalanced in size, and of different degrees of relevance to the held-out\nand test data, taking into account the data source for a given skip-/n-gram\nfeature and combining them for best performance on held-out/test data improves\nover skip-/n-gram SNM models trained on pooled data by about 8% in the SMT\nsetup, or as much as 15% in the ASR/IME setup.\n  The ability to mix various data sources based on how relevant they are to a\nmismatched held-out set is probably the most attractive feature of the new\nestimation method for SNM LM.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:45:29 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 19:15:19 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Chelba", "Ciprian", ""], ["Pereira", "Fernando", ""]]}, {"id": "1511.01665", "submitter": "Yiou Lin", "authors": "Yiou Lin, Hang Lei, Jia Wu and Xiaoyu Li", "title": "An Empirical Study on Sentiment Classification of Chinese Review using\n  Word Embedding", "comments": "The 29th Pacific Asia Conference on Language, Information and\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, how word embeddings can be used as features in Chinese\nsentiment classification is presented. Firstly, a Chinese opinion corpus is\nbuilt with a million comments from hotel review websites. Then the word\nembeddings which represent each comment are used as input in different machine\nlearning methods for sentiment classification, including SVM, Logistic\nRegression, Convolutional Neural Network (CNN) and ensemble methods. These\nmethods get better performance compared with N-gram models using Naive Bayes\n(NB) and Maximum Entropy (ME). Finally, a combination of machine learning\nmethods is proposed which presents an outstanding performance in precision,\nrecall and F1 score. After selecting the most useful methods to construct the\ncombinational model and testing over the corpus, the final F1 score is 0.920.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 09:25:21 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Lin", "Yiou", ""], ["Lei", "Hang", ""], ["Wu", "Jia", ""], ["Li", "Xiaoyu", ""]]}, {"id": "1511.01666", "submitter": "Abhinav Tushar", "authors": "Abhinav Tushar and Abhinav Dahiya", "title": "Comparing Writing Styles using Word Embedding and Dynamic Time Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The development of plot or story in novels is reflected in the content and\nthe words used. The flow of sentiments, which is one aspect of writing style,\ncan be quantified by analyzing the flow of words. This study explores literary\nworks as signals in word embedding space and tries to compare writing styles of\npopular classic novels using dynamic time warping.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 09:25:41 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Tushar", "Abhinav", ""], ["Dahiya", "Abhinav", ""]]}, {"id": "1511.01756", "submitter": "Suzanne Patience Mpouli Njanga Seh", "authors": "Suzanne Mpouli (ACASA), Jean-Gabriel Ganascia (ACASA)", "title": "\"Pale as death\" or \"p\\^ale comme la mort\" : Frozen similes used as\n  literary clich\\'es", "comments": "EUROPHRAS2015:COMPUTERISED AND CORPUS-BASED APPROACHES TO\n  PHRASEOLOGY: MONOLINGUAL AND MULTILINGUAL PERSPECTIVES, Jun 2015, Malaga,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study is focused on the automatic identification and description\nof frozen similes in British and French novels written between the 19 th\ncentury and the beginning of the 20 th century. Two main patterns of frozen\nsimiles were considered: adjectival ground + simile marker + nominal vehicle\n(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.\nsleep like a top). All potential similes and their components were first\nextracted using a rule-based algorithm. Then, frozen similes were identified\nbased on reference lists of existing similes and semantic distance between the\ntenor and the vehicle. The results obtained tend to confirm the fact that\nfrozen similes are not used haphazardly in literary texts. In addition,\ncontrary to how they are often presented, frozen similes often go beyond the\nground or the eventuality and the vehicle to also include the tenor.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 14:20:01 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 07:35:38 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Mpouli", "Suzanne", "", "ACASA"], ["Ganascia", "Jean-Gabriel", "", "ACASA"]]}, {"id": "1511.01974", "submitter": "Xu Chen", "authors": "Xu Chen, Han Zhang, Judith Gelernter", "title": "Multi-lingual Geoparsing based on Machine Translation", "comments": "7 pages, 4 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our method for multi-lingual geoparsing uses monolingual tools and resources\nalong with machine translation and alignment to return location words in many\nlanguages. Not only does our method save the time and cost of developing\ngeoparsers for each language separately, but also it allows the possibility of\na wide range of language capabilities within a single interface. We evaluated\nour method in our LanguageBridge prototype on location named entities using\nnewswire, broadcast news and telephone conversations in English, Arabic and\nChinese data from the Linguistic Data Consortium (LDC). Our results for\ngeoparsing Chinese and Arabic text using our multi-lingual geoparsing method\nare comparable to our results for geoparsing English text with our English\ntools. Furthermore, experiments using our machine translation approach results\nin accuracy comparable to results from the same data that was translated\nmanually.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 03:07:20 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Chen", "Xu", ""], ["Zhang", "Han", ""], ["Gelernter", "Judith", ""]]}, {"id": "1511.02014", "submitter": "Alexander Koplenig", "authors": "Alexander Koplenig and Carolin Mueller-Spitzer", "title": "Population size predicts lexical diversity, but so does the mean sea\n  level - why it is important to correctly account for the structure of\n  temporal data", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0150771", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to demonstrate why it is important to correctly account for the\n(serial dependent) structure of temporal data, we document an apparently\nspectacular relationship between population size and lexical diversity: for\nfive out of seven investigated languages, there is a strong relationship\nbetween population size and lexical diversity of the primary language in this\ncountry. We show that this relationship is the result of a misspecified model\nthat does not consider the temporal aspect of the data by presenting a similar\nbut nonsensical relationship between the global annual mean sea level and\nlexical diversity. Given the fact that in the recent past, several studies were\npublished that present surprising links between different economic, cultural,\npolitical and (socio-)demographical variables on the one hand and cultural or\nlinguistic characteristics on the other hand, but seem to suffer from exactly\nthis problem, we explain the cause of the misspecification and show that it has\nprofound consequences. We demonstrate how simple transformation of the time\nseries can often solve problems of this type and argue that the evaluation of\nthe plausibility of a relationship is important in this context. We hope that\nour paper will help both researchers and reviewers to understand why it is\nimportant to use special models for the analysis of data with a natural\ntemporal ordering.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 09:39:24 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 07:48:26 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Koplenig", "Alexander", ""], ["Mueller-Spitzer", "Carolin", ""]]}, {"id": "1511.02024", "submitter": "Tobias Schnabel", "authors": "S. Sathiya Keerthi, Tobias Schnabel, Rajiv Khanna", "title": "Towards a Better Understanding of Predict and Count Models", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, Levy and Goldberg pointed out an interesting connection\nbetween prediction-based word embedding models and count models based on\npointwise mutual information. Under certain conditions, they showed that both\nmodels end up optimizing equivalent objective functions. This paper explores\nthis connection in more detail and lays out the factors leading to differences\nbetween these models. We find that the most relevant differences from an\noptimization perspective are (i) predict models work in a low dimensional space\nwhere embedding vectors can interact heavily; (ii) since predict models have\nfewer parameters, they are less prone to overfitting.\n  Motivated by the insight of our analysis, we show how count models can be\nregularized in a principled manner and provide closed-form solutions for L1 and\nL2 regularization. Finally, we propose a new embedding model with a convex\nobjective and the additional benefit of being intelligible.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 10:29:26 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Keerthi", "S. Sathiya", ""], ["Schnabel", "Tobias", ""], ["Khanna", "Rajiv", ""]]}, {"id": "1511.02117", "submitter": "Kerry Fultz", "authors": "Kerry Fultz and Seth Filip", "title": "Introducing SKYSET - a Quintuple Approach for Improving Instructions", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach called SKYSET (Synthetic Knowledge Yield Social Entities\nTranslation) is proposed to validate completeness and to reduce ambiguity from\nwritten instructional documentation. SKYSET utilizes a quintuple set of\nstandardized categories, which differs from traditional approaches that\ntypically use triples. The SKYSET System defines the categories required to\nform a standard template for representing information that is portable across\ndifferent domains. It provides a standardized framework that enables sentences\nfrom written instructions to be translated into sets of category typed entities\non a table or database. The SKYSET entities contain conceptual units or phrases\nthat represent information from the original source documentation. SKYSET\nenables information concatenation where multiple documents from different\ndomains can be translated and combined into a single common filterable and\nsearchable table of entities.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 15:36:54 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Fultz", "Kerry", ""], ["Filip", "Seth", ""]]}, {"id": "1511.02274", "submitter": "Zichao Yang", "authors": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola", "title": "Stacked Attention Networks for Image Question Answering", "comments": "test-dev/standard results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents stacked attention networks (SANs) that learn to answer\nnatural language questions from images. SANs use semantic representation of a\nquestion as query to search for the regions in an image that are related to the\nanswer. We argue that image question answering (QA) often requires multiple\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\nimage multiple times to infer the answer progressively. Experiments conducted\non four image QA data sets demonstrate that the proposed SANs significantly\noutperform previous state-of-the-art approaches. The visualization of the\nattention layers illustrates the progress that the SAN locates the relevant\nvisual clues that lead to the answer of the question layer-by-layer.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 00:43:32 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 20:37:49 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Yang", "Zichao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""], ["Smola", "Alex", ""]]}, {"id": "1511.02283", "submitter": "Junhua Mao", "authors": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan\n  Yuille, Kevin Murphy", "title": "Generation and Comprehension of Unambiguous Object Descriptions", "comments": "We have released the Google Refexp dataset together with a toolbox\n  for visualization and evaluation, see\n  https://github.com/mjhucla/Google_Refexp_toolbox. Camera ready version for\n  CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that can generate an unambiguous description (known as a\nreferring expression) of a specific object or region in an image, and which can\nalso comprehend or interpret such an expression to infer which object is being\ndescribed. We show that our method outperforms previous methods that generate\ndescriptions of objects without taking into account other potentially ambiguous\nobjects in the scene. Our model is inspired by recent successes of deep\nlearning methods for image captioning, but while image captioning is difficult\nto evaluate, our task allows for easy objective evaluation. We also present a\nnew large-scale dataset for referring expressions, based on MS-COCO. We have\nreleased the dataset and a toolbox for visualization and evaluation, see\nhttps://github.com/mjhucla/Google_Refexp_toolbox\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 02:17:36 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 08:58:08 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 01:11:56 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Mao", "Junhua", ""], ["Huang", "Jonathan", ""], ["Toshev", "Alexander", ""], ["Camburu", "Oana", ""], ["Yuille", "Alan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1511.02301", "submitter": "Jason  Weston", "authors": "Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new test of how well language models capture meaning in\nchildren's books. Unlike standard language modelling benchmarks, it\ndistinguishes the task of predicting syntactic function words from that of\npredicting lower-frequency words, which carry greater semantic content. We\ncompare a range of state-of-the-art models, each with a different way of\nencoding what has been previously read. We show that models which store\nexplicit representations of long-term contexts outperform state-of-the-art\nneural language models at predicting semantic content words, although this\nadvantage is not observed for syntactic function words. Interestingly, we find\nthat the amount of text encoded in a single memory representation is highly\ninfluential to the performance: there is a sweet-spot, not too big and not too\nsmall, between single words and full sentences that allows the most meaningful\ninformation in a text to be effectively retained and recalled. Further, the\nattention over such window-based memories can be trained effectively through\nself-supervision. We then assess the generality of this principle by applying\nit to the CNN QA benchmark, which involves identifying named entities in\nparaphrased summaries of news articles, and achieve state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 04:36:20 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 23:21:58 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 21:10:21 GMT"}, {"version": "v4", "created": "Fri, 1 Apr 2016 05:31:33 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Hill", "Felix", ""], ["Bordes", "Antoine", ""], ["Chopra", "Sumit", ""], ["Weston", "Jason", ""]]}, {"id": "1511.02385", "submitter": "Syvester Olubolu Orimaye Dr", "authors": "Sylvester Olubolu Orimaye, Saadat M. Alhashmi, Eu-Gene Siew and Sang\n  Jung Kang", "title": "Review-Level Sentiment Classification with Sentence-Level Polarity\n  Correction", "comments": "15 pages. This paper is based on the same sentence-level technique\n  proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy\n  it: sentiment classification on Amazon reviews using sentence polarity shift.\n  In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer\n  Berlin Heidelberg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective technique to solving review-level sentiment\nclassification problem by using sentence-level polarity correction. Our\npolarity correction technique takes into account the consistency of the\npolarities (positive and negative) of sentences within each product review\nbefore performing the actual machine learning task. While sentences with\ninconsistent polarities are removed, sentences with consistent polarities are\nused to learn state-of-the-art classifiers. The technique achieved better\nresults on different types of products reviews and outperforms baseline models\nwithout the correction technique. Experimental results show an average of 82%\nF-measure on four different product review domains.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 18:38:22 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Orimaye", "Sylvester Olubolu", ""], ["Alhashmi", "Saadat M.", ""], ["Siew", "Eu-Gene", ""], ["Kang", "Sang Jung", ""]]}, {"id": "1511.02435", "submitter": "Son-Il Kwak", "authors": "Son-Il Kwak, O-Chol Kown, Chang-Sin Kim, Yong-Il Pak, Gum-Chol Son,\n  Chol-Jun Hwang, Hyon-Chol Kim, Hyok-Chol Sin, Gyong-Il Hyon, Sok-Min Han", "title": "A Chinese POS Decision Method Using Korean Translation Information", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method that imitates a translation expert using\nthe Korean translation information and analyse the performance. Korean is good\nat tagging than Chinese, so we can use this property in Chinese POS tagging.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 03:44:26 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Kwak", "Son-Il", ""], ["Kown", "O-Chol", ""], ["Kim", "Chang-Sin", ""], ["Pak", "Yong-Il", ""], ["Son", "Gum-Chol", ""], ["Hwang", "Chol-Jun", ""], ["Kim", "Hyon-Chol", ""], ["Sin", "Hyok-Chol", ""], ["Hyon", "Gyong-Il", ""], ["Han", "Sok-Min", ""]]}, {"id": "1511.02436", "submitter": "Syvester Olubolu Orimaye Dr", "authors": "Sylvester Olubolu Orimaye, Kah Yee Tai, Jojo Sze-Meng Wong and Chee\n  Piau Wong", "title": "Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment\n  using Compound Skip-grams", "comments": "Accepted and presented at the 2015 NIPS Workshop on Machine Learning\n  in Healthcare (MLHC), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting Mild Cognitive Impairment (MCI) is currently a challenge as\nexisting diagnostic criteria rely on neuropsychological examinations. Automated\nMachine Learning (ML) models that are trained on verbal utterances of MCI\npatients can aid diagnosis. Using a combination of skip-gram features, our\nmodel learned several linguistic biomarkers to distinguish between 19 patients\nwith MCI and 19 healthy control individuals from the DementiaBank language\ntranscript clinical dataset. Results show that a model with compound of\nskip-grams has better AUC and could help ML prediction on small MCI data\nsample.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 03:45:49 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 03:25:54 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Orimaye", "Sylvester Olubolu", ""], ["Tai", "Kah Yee", ""], ["Wong", "Jojo Sze-Meng", ""], ["Wong", "Chee Piau", ""]]}, {"id": "1511.02506", "submitter": "Yi-Hsiu Liao", "authors": "Yi-Hsiu Liao, Hung-yi Lee, Lin-shan Lee", "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition", "comments": "arXiv admin note: text overlap with arXiv:1506.01163", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the Structured Deep Neural Network (structured DNN)\nas a structured and deep learning framework. This approach can learn to find\nthe best structured object (such as a label sequence) given a structured input\n(such as a vector sequence) by globally considering the mapping relationships\nbetween the structures rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learn utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning approach. This approach was\nshown to beat structured SVM in preliminary experiments on TIMIT.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 17:08:54 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Liao", "Yi-Hsiu", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1511.02556", "submitter": "Hao Wang", "authors": "Hao Wang, Jorge A. Castanon", "title": "Sentiment Expression via Emoticons on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis and\nother NLP tasks as features to ma- chine learning algorithms or as entries of\nsentiment lexicons. In this paper, we argue that while emoticons are strong and\ncommon signals of sentiment expression on social media the relationship between\nemoticons and sentiment polarity are not always clear. Thus, any algorithm that\ndeals with sentiment polarity should take emoticons into account but extreme\ncau- tion should be exercised in which emoticons to depend on. First, to\ndemonstrate the prevalence of emoticons on social media, we analyzed the\nfrequency of emoticons in a large re- cent Twitter data set. Then we carried\nout four analyses to examine the relationship between emoticons and sentiment\npolarity as well as the contexts in which emoticons are used. The first\nanalysis surveyed a group of participants for their perceived sentiment\npolarity of the most frequent emoticons. The second analysis examined\nclustering of words and emoti- cons to better understand the meaning conveyed\nby the emoti- cons. The third analysis compared the sentiment polarity of\nmicroblog posts before and after emoticons were removed from the text. The last\nanalysis tested the hypothesis that removing emoticons from text hurts\nsentiment classification by training two machine learning models with and\nwithout emoticons in the text respectively. The results confirms the arguments\nthat: 1) a few emoticons are strong and reliable signals of sentiment polarity\nand one should take advantage of them in any senti- ment analysis; 2) a large\ngroup of the emoticons conveys com- plicated sentiment hence they should be\ntreated with extreme caution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 02:31:31 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Wang", "Hao", ""], ["Castanon", "Jorge A.", ""]]}, {"id": "1511.02570", "submitter": "Chunhua Shen", "authors": "Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick", "title": "Explicit Knowledge-based Reasoning for Visual Question Answering", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for visual question answering which is capable of\nreasoning about contents of an image on the basis of information extracted from\na large-scale knowledge base. The method not only answers natural language\nquestions using concepts not contained in the image, but can provide an\nexplanation of the reasoning by which it developed its answer. The method is\ncapable of answering far more complex questions than the predominant long\nshort-term memory-based approach, and outperforms it significantly in the\ntesting. We also provide a dataset and a protocol by which to evaluate such\nmethods, thus addressing one of the key issues in general visual ques- tion\nanswering.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 05:25:57 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 01:10:38 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wang", "Peng", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Dick", "Anthony", ""]]}, {"id": "1511.02669", "submitter": "Adrian Groza", "authors": "Adrian Groza and Roxana Szabo", "title": "Enacting textual entailment and ontologies for automated essay grading\n  in chemical domain", "comments": "16th Int. Symposium on Computational Intelligence and Informatics\n  (CINTI2015), Budapest, Hungary, 19-21 November, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system for automated essay grading using ontologies and textual\nentailment. The process of textual entailment is guided by hypotheses, which\nare extracted from a domain ontology. Textual entailment checks if the truth of\nthe hypothesis follows from a given text. We enact textual entailment to\ncompare students answer to a model answer obtained from ontology. We validated\nthe solution against various essays written by students in the chemistry\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 13:21:02 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Groza", "Adrian", ""], ["Szabo", "Roxana", ""]]}, {"id": "1511.02799", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein", "title": "Neural Module Networks", "comments": "Corrects an error in the evaluation of the NMN-only ablation\n  experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is fundamentally compositional in nature---a\nquestion like \"where is the dog?\" shares substructure with questions like \"what\ncolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously\nexploit the representational capacity of deep networks and the compositional\nlinguistic structure of questions. We describe a procedure for constructing and\nlearning *neural module networks*, which compose collections of jointly-trained\nneural \"modules\" into deep networks for question answering. Our approach\ndecomposes questions into their linguistic substructures, and uses these\nstructures to dynamically instantiate modular networks (with reusable\ncomponents for recognizing dogs, classifying colors, etc.). The resulting\ncompound networks are jointly trained. We evaluate our approach on two\nchallenging datasets for visual question answering, achieving state-of-the-art\nresults on both the VQA natural image dataset and a new dataset of complex\nquestions about abstract shapes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:48:39 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 06:36:22 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 18:26:40 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 17:15:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Klein", "Dan", ""]]}, {"id": "1511.03012", "submitter": "Adrian Groza", "authors": "Adrian Groza and Lidia Corde", "title": "Information retrieval in folktales using natural language processing", "comments": "IEEE 11 International Conference on Intelligent Computer\n  Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to extract information about literary characters in unstructured\ntexts. We employ natural language processing and reasoning on domain\nontologies. The first task is to identify the main characters and the parts of\nthe story where these characters are described or act. We illustrate the system\nin a scenario in the folktale domain. The system relies on a folktale ontology\nthat we have developed based on Propp's model for folktales morphology.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 08:13:49 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Corde", "Lidia", ""]]}, {"id": "1511.03053", "submitter": "Suzanne Mpouli", "authors": "Suzanne Mpouli (ACASA), Jean-Gabriel Ganascia (ACASA)", "title": "Investigating the stylistic relevance of adjective and verb simile\n  markers", "comments": null, "journal-ref": "Corpus Linguistics 2015, Jul 2015, Lancaster, United Kingdom", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similes play an important role in literary texts not only as rhetorical\ndevices and as figures of speech but also because of their evocative power,\ntheir aptness for description and the relative ease with which they can be\ncombined with other figures of speech (Israel et al. 2004). Detecting all types\nof simile constructions in a particular text therefore seems crucial when\nanalysing the style of an author. Few research studies however have been\ndedicated to the study of less prominent simile markers in fictional prose and\ntheir relevance for stylistic studies. The present paper studies the frequency\nof adjective and verb simile markers in a corpus of British and French novels\nin order to determine which ones are really informative and worth including in\na stylistic analysis. Furthermore, are those adjectives and verb simile markers\nused differently in both languages?\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 10:33:47 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Mpouli", "Suzanne", "", "ACASA"], ["Ganascia", "Jean-Gabriel", "", "ACASA"]]}, {"id": "1511.03088", "submitter": "Isabelle Augenstein", "authors": "Leon Derczynski and Isabelle Augenstein and Kalina Bontcheva", "title": "USFD: Twitter NER with Drift Compensation and Linked Data", "comments": "Paper in ACL anthology:\n  https://aclweb.org/anthology/W/W15/W15-4306.bib", "journal-ref": "Proceedings of the ACL Workshop on Noisy User-generated Text\n  (2015), pp. 48--53", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a pilot NER system for Twitter, comprising the USFD\nsystem entry to the W-NUT 2015 NER shared task. The goal is to correctly label\nentities in a tweet dataset, using an inventory of ten types. We employ\nstructured learning, drawing on gazetteers taken from Linked Data, and on\nunsupervised clustering features, and attempting to compensate for stylistic\nand topic drift - a key challenge in social media text. Our result is\ncompetitive; we provide an analysis of the components of our methodology, and\nan examination of the target dataset in the context of this task.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 12:34:47 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Derczynski", "Leon", ""], ["Augenstein", "Isabelle", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1511.03292", "submitter": "Yezhou Yang", "authors": "Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis\n  Aloimonos", "title": "From Images to Sentences through Scene Description Graphs using\n  Commonsense Reasoning and Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the construction of linguistic descriptions of\nimages. This is achieved through the extraction of scene description graphs\n(SDGs) from visual scenes using an automatically constructed knowledge base.\nSDGs are constructed using both vision and reasoning. Specifically, commonsense\nreasoning is applied on (a) detections obtained from existing perception\nmethods on given images, (b) a \"commonsense\" knowledge base constructed using\nnatural language processing of image annotations and (c) lexical ontological\nknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based\nevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most\ncases, sentences auto-constructed from SDGs obtained by our method give a more\nrelevant and thorough description of an image than a recent state-of-the-art\nimage caption based approach. Our Image-Sentence Alignment Evaluation results\nare also comparable to that of the recent state-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 21:14:51 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1511.03546", "submitter": "Guorui Zhou", "authors": "Guorui Zhou, Guang Chen", "title": "Hierarchical Latent Semantic Mapping for Automated Topic Generation", "comments": "9 pages, 3 figures, Under Review as a conference at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of information sits in an unprecedented amount of text data. Managing\nallocation of these large scale text data is an important problem for many\nareas. Topic modeling performs well in this problem. The traditional generative\nmodels (PLSA,LDA) are the state-of-the-art approaches in topic modeling and\nmost recent research on topic generation has been focusing on improving or\nextending these models. However, results of traditional generative models are\nsensitive to the number of topics K, which must be specified manually. The\nproblem of generating topics from corpus resembles community detection in\nnetworks. Many effective algorithms can automatically detect communities from\nnetworks without a manually specified number of the communities. Inspired by\nthese algorithms, in this paper, we propose a novel method named Hierarchical\nLatent Semantic Mapping (HLSM), which automatically generates topics from\ncorpus. HLSM calculates the association between each pair of words in the\nlatent topic space, then constructs a unipartite network of words with this\nassociation and hierarchically generates topics from this network. We apply\nHLSM to several document collections and the experimental comparisons against\nseveral state-of-the-art approaches demonstrate the promising performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 15:58:30 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 13:47:53 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 05:23:58 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2015 01:35:58 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Zhou", "Guorui", ""], ["Chen", "Guang", ""]]}, {"id": "1511.03683", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Sharad Vikram, Julian McAuley", "title": "Generative Concatenative Nets Jointly Learn to Write and Classify\n  Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system's basic task is to estimate how users will respond to\nunseen items. This is typically modeled in terms of how a user might rate a\nproduct, but here we aim to extend such approaches to model how a user would\nwrite about the product. To do so, we design a character-level Recurrent Neural\nNetwork (RNN) that generates personalized product reviews. The network\nconvincingly learns styles and opinions of nearly 1000 distinct authors, using\na large corpus of reviews from BeerAdvocate.com. It also tailors reviews to\ndescribe specific items, categories, and star ratings. Using a simple input\nreplication strategy, the Generative Concatenative Network (GCN) preserves the\nsignal of static auxiliary inputs across wide sequence intervals. Without any\nadditional training, the generative model can classify reviews, identifying the\nauthor of the review, the product category, and the sentiment (rating), with\nremarkable accuracy. Our evaluation shows the GCN captures complex dynamics in\ntext, such as the effect of negation, misspellings, slang, and large\nvocabularies gracefully absent any machinery explicitly dedicated to the\npurpose.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:16:59 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 10:27:27 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 08:20:05 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2015 19:17:07 GMT"}, {"version": "v5", "created": "Thu, 7 Apr 2016 07:08:42 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Vikram", "Sharad", ""], ["McAuley", "Julian", ""]]}, {"id": "1511.03690", "submitter": "David Harwath", "authors": "David Harwath and James Glass", "title": "Deep Multimodal Semantic Embeddings for Speech and Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a model which takes as input a corpus of images\nwith relevant spoken captions and finds a correspondence between the two\nmodalities. We employ a pair of convolutional neural networks to model visual\nobjects and speech signals at the word level, and tie the networks together\nwith an embedding and alignment model which learns a joint semantic space over\nboth modalities. We evaluate our model using image search and annotation tasks\non the Flickr8k dataset, which we augmented by collecting a corpus of 40,000\nspoken captions using Amazon Mechanical Turk.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:30:10 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Harwath", "David", ""], ["Glass", "James", ""]]}, {"id": "1511.03729", "submitter": "Tian Wang", "authors": "Tian Wang and Kyunghyun Cho", "title": "Larger-Context Language Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel method to incorporate corpus-level discourse\ninformation into language modelling. We call this larger-context language\nmodel. We introduce a late fusion approach to a recurrent language model based\non long short-term memory units (LSTM), which helps the LSTM unit keep\nintra-sentence dependencies and inter-sentence dependencies separate from each\nother. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank),\nwe demon- strate that the proposed model improves perplexity significantly. In\nthe experi- ments, we evaluate the proposed approach while varying the number\nof context sentences and observe that the proposed late fusion is superior to\nthe usual way of incorporating additional inputs to the LSTM. By analyzing the\ntrained larger- context language model, we discover that content words,\nincluding nouns, adjec- tives and verbs, benefit most from an increasing number\nof context sentences. This analysis suggests that larger-context language model\nimproves the unconditional language model by capturing the theme of a document\nbetter and more easily.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 23:24:29 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 17:51:01 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Wang", "Tian", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1511.03745", "submitter": "Marcus Rohrbach", "authors": "Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt\n  Schiele", "title": "Grounding of Textual Phrases in Images by Reconstruction", "comments": "published at ECCV 2016 (oral); updated to final version", "journal-ref": null, "doi": "10.1007/978-3-319-46448-0_49", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual\ncontent is a challenging problem with many applications for human-computer\ninteraction and image-text reference resolution. Few datasets provide the\nground truth spatial localization of phrases, thus it is desirable to learn\nfrom data with no or little grounding supervision. We propose a novel approach\nwhich learns grounding by reconstructing a given phrase using an attention\nmechanism, which can be either latent or optimized directly. During training\nour approach encodes the phrase using a recurrent network language model and\nthen learns to attend to the relevant image region in order to reconstruct the\ninput phrase. At test time, the correct attention, i.e., the grounding, is\nevaluated. If grounding supervision is available it can be directly applied via\na loss over the attention mechanism. We demonstrate the effectiveness of our\napproach on the Flickr 30k Entities and ReferItGame datasets with different\nlevels of supervision, ranging from no supervision over partial supervision to\nfull supervision. Our supervised variant improves by a large margin over the\nstate-of-the-art on both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 01:13:47 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 18:59:11 GMT"}, {"version": "v3", "created": "Fri, 18 Mar 2016 04:03:15 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2017 21:02:05 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Hu", "Ronghang", ""], ["Darrell", "Trevor", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.03924", "submitter": "Normunds Gruzitis", "authors": "Normunds Gruzitis and Dana Dann\\'ells", "title": "A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural\n  Language", "comments": null, "journal-ref": "Language Resources and Evaluation 51(1), 37-66, 2017", "doi": "10.1007/s10579-015-9321-8", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Berkeley FrameNet is a lexico-semantic resource for English based on the\ntheory of frame semantics. It has been exploited in a range of natural language\nprocessing applications and has inspired the development of framenets for many\nlanguages. We present a methodological approach to the extraction and\ngeneration of a computational multilingual FrameNet-based grammar and lexicon.\nThe approach leverages FrameNet-annotated corpora to automatically extract a\nset of cross-lingual semantico-syntactic valence patterns. Based on data from\nBerkeley FrameNet and Swedish FrameNet, the proposed approach has been\nimplemented in Grammatical Framework (GF), a categorial grammar formalism\nspecialized for multilingual grammars. The implementation of the grammar and\nlexicon is supported by the design of FrameNet, providing a frame semantic\nabstraction layer, an interlingual semantic API (application programming\ninterface), over the interlingual syntactic API already provided by GF Resource\nGrammar Library. The evaluation of the acquired grammar and lexicon shows the\nfeasibility of the approach. Additionally, we illustrate how the FrameNet-based\ngrammar and lexicon are exploited in two distinct multilingual controlled\nnatural language applications. The produced resources are available under an\nopen source license.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 15:23:37 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Gruzitis", "Normunds", ""], ["Dann\u00e9lls", "Dana", ""]]}, {"id": "1511.03962", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, Jacob Eisenstein", "title": "Document Context Language Models", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text documents are structured on multiple levels of detail: individual words\nare related by syntax, but larger units of text are related by discourse\nstructure. Existing language models generally fail to account for discourse\nstructure, but it is crucial if we are to have language models that reward\ncoherence and generate coherent texts. We present and empirically evaluate a\nset of multi-level recurrent neural network language models, called\nDocument-Context Language Models (DCLM), which incorporate contextual\ninformation both within and beyond the sentence. In comparison with word-level\nrecurrent neural network language models, the DCLM models obtain slightly\nbetter predictive likelihoods, and considerably better assessments of document\ncoherence.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 16:53:50 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 19:40:50 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 03:26:47 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2016 23:46:44 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Ji", "Yangfeng", ""], ["Cohn", "Trevor", ""], ["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1511.04024", "submitter": "Zachary Seymour", "authors": "Zachary Seymour, Yingming Li, Zhongfei Zhang", "title": "Multimodal Skip-gram Using Convolutional Pseudowords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the representational mapping across multimodal data such\nthat given a piece of the raw data in one modality the corresponding semantic\ndescription in terms of the raw data in another modality is immediately\nobtained. Such a representational mapping can be found in a wide spectrum of\nreal-world applications including image/video retrieval, object recognition,\naction/behavior recognition, and event understanding and prediction. To that\nend, we introduce a simplified training objective for learning multimodal\nembeddings using the skip-gram architecture by introducing convolutional\n\"pseudowords:\" embeddings composed of the additive combination of distributed\nword representations and image features from convolutional neural networks\nprojected into the multimodal space. We present extensive results of the\nrepresentational properties of these embeddings on various word similarity\nbenchmarks to show the promise of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 19:32:08 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 19:09:38 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Seymour", "Zachary", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1511.04108", "submitter": "Ming Tan", "authors": "Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou", "title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "comments": "added new experiments on TREC-QA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply a general deep learning (DL) framework for the answer\nselection task, which does not depend on manually defined features or\nlinguistic tools. The basic framework is to build the embeddings of questions\nand answers based on bidirectional long short-term memory (biLSTM) models, and\nmeasure their closeness by cosine similarity. We further extend this basic\nmodel in two directions. One direction is to define a more composite\nrepresentation for questions and answers by combining convolutional neural\nnetwork with the basic framework. The other direction is to utilize a simple\nbut efficient attention mechanism in order to generate the answer\nrepresentation according to the question context. Several variations of models\nare provided. The models are examined by two datasets, including TREC-QA and\nInsuranceQA. Experimental results demonstrate that the proposed models\nsubstantially outperform several strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 22:01:54 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:00:46 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 17:56:29 GMT"}, {"version": "v4", "created": "Mon, 28 Mar 2016 04:12:45 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Tan", "Ming", ""], ["Santos", "Cicero dos", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1511.04164", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko,\n  Trevor Darrell", "title": "Natural Language Object Retrieval", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of natural language object retrieval, to\nlocalize a target object within a given image based on a natural language query\nof the object. Natural language object retrieval differs from text-based image\nretrieval task as it involves spatial information about objects within the\nscene and global scene context. To address this issue, we propose a novel\nSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate\nboxes for object retrieval, integrating spatial configurations and global\nscene-level contextual information into the network. Our model processes query\ntext, local image descriptors, spatial configurations and global context\nfeatures through a recurrent network, outputs the probability of the query text\nconditioned on each candidate box as a score for the box, and can transfer\nvisual-linguistic knowledge from image captioning domain to our task.\nExperimental results demonstrate that our method effectively utilizes both\nlocal and global information, outperforming previous baseline methods\nsignificantly on different datasets and scenarios, and can exploit large scale\nvision and language datasets for knowledge transfer.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 05:53:37 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 20:12:44 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 03:36:58 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Hu", "Ronghang", ""], ["Xu", "Huazhe", ""], ["Rohrbach", "Marcus", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.04401", "submitter": "Federico Raue", "authors": "Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki", "title": "Symbol Grounding Association in Multimodal Sequences with Missing\n  Elements", "comments": "Under review on Journal of Artificial Intelligence Research (JAIR) --\n  Special Track on Deep Learning, Knowledge Representation, and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend a symbolic association framework for being able to\nhandle missing elements in multimodal sequences. The general scope of the work\nis the symbolic associations of object-word mappings as it happens in language\ndevelopment in infants. In other words, two different representations of the\nsame abstract concepts can associate in both directions. This scenario has been\nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In\nthis work, we extend a recent approach for multimodal sequences (visual and\naudio) to also cope with missing elements in one or both modalities. Our method\nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based\non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We\npropose to include an extra step for the combination with the max operation for\nexploiting the common elements between both sequences. The motivation behind is\nthat the combination acts as a condition selector for choosing the best\nrepresentation from both LSTMs. We evaluated the proposed extension in the\nfollowing scenarios: missing elements in one modality (visual or audio) and\nmissing elements in both modalities (visual and sound). The performance of our\nextension reaches better results than the original model and similar results to\nindividual LSTM trained in each modality.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:59:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:59:02 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 11:36:59 GMT"}, {"version": "v4", "created": "Fri, 16 Dec 2016 14:17:02 GMT"}, {"version": "v5", "created": "Thu, 7 Dec 2017 10:14:23 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Raue", "Federico", ""], ["Dengel", "Andreas", ""], ["Breuel", "Thomas M.", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1511.04586", "submitter": "Wang Ling", "authors": "Wang Ling and Isabel Trancoso and Chris Dyer and Alan W Black", "title": "Character-based Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural machine translation model that views the input and\noutput sentences as sequences of characters rather than words. Since word-level\ninformation provides a crucial source of bias, our input model composes\nrepresentations of character sequences into representations of words (as\ndetermined by whitespace boundaries), and then these are translated using a\njoint attention/translation model. In the target language, the translation is\nmodeled as a sequence of word vectors, but each word is generated one character\nat a time, conditional on the previous character generations in each word. As\nthe representation and generation of words is performed at the character level,\nour model is capable of interpreting and generating unseen word forms. A\nsecondary benefit of this approach is that it alleviates much of the challenges\nassociated with preprocessing/tokenization of the source and target languages.\nWe show that our model can achieve translation results that are on par with\nconventional word-based models.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 17:36:43 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Ling", "Wang", ""], ["Trancoso", "Isabel", ""], ["Dyer", "Chris", ""], ["Black", "Alan W", ""]]}, {"id": "1511.04590", "submitter": "Li Yao", "authors": "Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio", "title": "Oracle performance for visual captioning", "comments": "BMVC2016 (Oral paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of associating images and videos with a natural language description\nhas attracted a great amount of attention recently. Rapid progress has been\nmade in terms of both developing novel algorithms and releasing new datasets.\nIndeed, the state-of-the-art results on some of the standard datasets have been\npushed into the regime where it has become more and more difficult to make\nsignificant improvements. Instead of proposing new models, this work\ninvestigates the possibility of empirically establishing performance upper\nbounds on various visual captioning datasets without extra data labelling\neffort or human evaluation. In particular, it is assumed that visual captioning\nis decomposed into two steps: from visual inputs to visual concepts, and from\nvisual concepts to natural language descriptions. One would be able to obtain\nan upper bound when assuming the first step is perfect and only requiring\ntraining a conditional language model for the second step. We demonstrate the\nconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination\nof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used\nfor visual concept extraction in the first step and the simplicity of the\nlanguage model for the second step, we show that current state-of-the-art\nmodels fall short when being compared with the learned upper bounds.\nFurthermore, with such a bound, we quantify several important factors\nconcerning image and video captioning: the number of visual concepts captured\nby different models, the trade-off between the amount of visual elements\ncaptured and their accuracy, and the intrinsic difficulty and blessing of\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 18:02:39 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 04:20:08 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2016 04:55:57 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2016 23:38:25 GMT"}, {"version": "v5", "created": "Wed, 14 Sep 2016 16:55:29 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Yao", "Li", ""], ["Ballas", "Nicolas", ""], ["Cho", "Kyunghyun", ""], ["Smith", "John R.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1511.04623", "submitter": "Kazuya Kawakami", "authors": "Kazuya Kawakami, Chris Dyer", "title": "Learning to Represent Words in Context with Multilingual Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural network architecture based on bidirectional LSTMs to\ncompute representations of words in the sentential contexts. These\ncontext-sensitive word representations are suitable for, e.g., distinguishing\ndifferent word senses and other context-modulated variations in meaning. To\nlearn the parameters of our model, we use cross-lingual supervision,\nhypothesizing that a good representation of a word in context will be one that\nis sufficient for selecting the correct translation into a second language. We\nevaluate the quality of our representations as features in three downstream\ntasks: prediction of semantic supersenses (which assign nouns and verbs into a\nfew dozen semantic classes), low resource machine translation, and a lexical\nsubstitution task, and obtain state-of-the-art results on all of these.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 21:36:38 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:35:42 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Kawakami", "Kazuya", ""], ["Dyer", "Chris", ""]]}, {"id": "1511.04636", "submitter": "Ji He", "authors": "Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng,\n  Mari Ostendorf", "title": "Deep Reinforcement Learning with a Natural Language Action Space", "comments": "accepted by ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel architecture for reinforcement learning with\ndeep neural networks designed to handle state and action spaces characterized\nby natural language, as found in text-based games. Termed a deep reinforcement\nrelevance network (DRRN), the architecture represents action and state spaces\nwith separate embedding vectors, which are combined with an interaction\nfunction to approximate the Q-function in reinforcement learning. We evaluate\nthe DRRN on two popular text games, showing superior performance over other\ndeep Q-learning architectures. Experiments with paraphrased action descriptions\nshow that the model is extracting meaning rather than simply memorizing strings\nof text.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 23:30:39 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 20:24:12 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 01:51:20 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2016 23:43:40 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2016 05:58:34 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["He", "Ji", ""], ["Chen", "Jianshu", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Deng", "Li", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1511.04646", "submitter": "Yikang Shen", "authors": "Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng, Jie Tang, Zhang Xiong", "title": "Word Embedding based Correlation Model for Question/Answer Matching", "comments": "8 pages, 2 figures", "journal-ref": "AAAI (2017) 3511--3517", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of community based question answering (Q&A) services, a\nlarge scale of Q&A archives have been accumulated and are an important\ninformation and knowledge resource on the web. Question and answer matching has\nbeen attached much importance to for its ability to reuse knowledge stored in\nthese systems: it can be useful in enhancing user experience with recurrent\nquestions. In this paper, we try to improve the matching accuracy by overcoming\nthe lexical gap between question and answer pairs. A Word Embedding based\nCorrelation (WEC) model is proposed by integrating advantages of both the\ntranslation model and word embedding, given a random pair of words, WEC can\nscore their co-occurrence probability in Q&A pairs and it can also leverage the\ncontinuity and smoothness of continuous space word representation to deal with\nnew pairs of words that are rare in the training parallel text. An experimental\nstudy on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new\nmethod's promising potential.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 02:59:22 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 02:40:12 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Shen", "Yikang", ""], ["Rong", "Wenge", ""], ["Jiang", "Nan", ""], ["Peng", "Baolin", ""], ["Tang", "Jie", ""], ["Xiong", "Zhang", ""]]}, {"id": "1511.04661", "submitter": "Hao Wang", "authors": "Hao Wang, Vijay R. Bommireddipalli, Ayman Hanafy, Mohamed Bahgat, Sara\n  Noeman and Ossama S. Emam", "title": "A System for Extracting Sentiment from Large-Scale Arabic Social Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media data in Arabic language is becoming more and more abundant. It\nis a consensus that valuable information lies in social media data. Mining this\ndata and making the process easier are gaining momentum in the industries. This\npaper describes an enterprise system we developed for extracting sentiment from\nlarge volumes of social data in Arabic dialects. First, we give an overview of\nthe Big Data system for information extraction from multilingual social data\nfrom a variety of sources. Then, we focus on the Arabic sentiment analysis\ncapability that was built on top of the system including normalizing written\nArabic dialects, building sentiment lexicons, sentiment classification, and\nperformance evaluation. Lastly, we demonstrate the value of enriching sentiment\nresults with user profiles in understanding sentiments of a specific user\ngroup.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 05:53:13 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Wang", "Hao", ""], ["Bommireddipalli", "Vijay R.", ""], ["Hanafy", "Ayman", ""], ["Bahgat", "Mohamed", ""], ["Noeman", "Sara", ""], ["Emam", "Ossama S.", ""]]}, {"id": "1511.04747", "submitter": "Sayan Ghosh", "authors": "Sayan Ghosh, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer", "title": "Learning Representations of Affect from Speech", "comments": "This is a submission for the ICLR (International Conference on\n  Learning Representations) Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of prior work on representation learning for speech\nrecognition applications, but not much emphasis has been given to an\ninvestigation of effective representations of affect from speech, where the\nparalinguistic elements of speech are separated out from the verbal content. In\nthis paper, we explore denoising autoencoders for learning paralinguistic\nattributes i.e. categorical and dimensional affective traits from speech. We\nshow that the representations learnt by the bottleneck layer of the autoencoder\nare highly discriminative of activation intensity and at separating out\nnegative valence (sadness and anger) from positive valence (happiness). We\nexperiment with different input speech features (such as FFT and log-mel\nspectrograms with temporal context windows), and different autoencoder\narchitectures (such as stacked and deep autoencoders). We also learn utterance\nspecific representations by a combination of denoising autoencoders and BLSTM\nbased recurrent autoencoders. Emotion classification is performed with the\nlearnt temporal/dynamic representations to evaluate the quality of the\nrepresentations. Experiments on a well-established real-life speech dataset\n(IEMOCAP) show that the learnt representations are comparable to state of the\nart feature extractors (such as voice quality features and MFCCs) and are\ncompetitive with state-of-the-art approaches at emotion and dimensional affect\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 18:16:20 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:37:01 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 20:44:51 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 20:36:36 GMT"}, {"version": "v5", "created": "Tue, 19 Jan 2016 04:05:50 GMT"}, {"version": "v6", "created": "Sun, 14 Feb 2016 18:11:46 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Ghosh", "Sayan", ""], ["Laksana", "Eugene", ""], ["Morency", "Louis-Philippe", ""], ["Scherer", "Stefan", ""]]}, {"id": "1511.04834", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Quoc V. Le, Ilya Sutskever", "title": "Neural Programmer: Inducing Latent Programs with Gradient Descent", "comments": "Accepted as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive supervised classification\nperformance in many tasks including image recognition, speech recognition, and\nsequence to sequence learning. However, this success has not been translated to\napplications like question answering that may involve complex arithmetic and\nlogic reasoning. A major limitation of these models is in their inability to\nlearn even simple arithmetic and logic operations. For example, it has been\nshown that neural networks fail to learn to add two binary numbers reliably. In\nthis work, we propose Neural Programmer, an end-to-end differentiable neural\nnetwork augmented with a small set of basic arithmetic and logic operations.\nNeural Programmer can call these augmented operations over several steps,\nthereby inducing compositional programs that are more complex than the built-in\noperations. The model learns from a weak supervision signal which is the result\nof execution of the correct program, hence it does not require expensive\nannotation of the correct program itself. The decisions of what operations to\ncall, and what data segments to apply to are inferred by Neural Programmer.\nSuch decisions, during training, are done in a differentiable fashion so that\nthe entire network can be trained jointly by gradient descent. We find that\ntraining the model is difficult, but it can be greatly improved by adding\nrandom noise to the gradient. On a fairly complex synthetic table-comprehension\ndataset, traditional recurrent networks and attentional models perform poorly\nwhile Neural Programmer typically obtains nearly perfect accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 06:03:58 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 07:00:28 GMT"}, {"version": "v3", "created": "Thu, 4 Aug 2016 18:23:03 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Le", "Quoc V.", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1511.04868", "submitter": "Navdeep Jaitly", "authors": "Navdeep Jaitly, David Sussillo, Quoc V. Le, Oriol Vinyals, Ilya\n  Sutskever and Samy Bengio", "title": "A Neural Transducer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have achieved impressive results on various\ntasks. However, they are unsuitable for tasks that require incremental\npredictions to be made as more data arrives or tasks that have long input\nsequences and output sequences. This is because they generate an output\nsequence conditioned on an entire input sequence. In this paper, we present a\nNeural Transducer that can make incremental predictions as more input arrives,\nwithout redoing the entire computation. Unlike sequence-to-sequence models, the\nNeural Transducer computes the next-step distribution conditioned on the\npartially observed input sequence and the partially generated sequence. At each\ntime step, the transducer can decide to emit zero to many output symbols. The\ndata can be processed using an encoder and presented as input to the\ntransducer. The discrete decision to emit a symbol at every time step makes it\ndifficult to learn with conventional backpropagation. It is however possible to\ntrain the transducer by using a dynamic programming algorithm to generate\ntarget discrete decisions. Our experiments show that the Neural Transducer\nworks well in settings where it is required to produce output predictions as\ndata come in. We also find that the Neural Transducer performs well for long\nsequences even when attention mechanisms are not used.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 08:53:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 19:56:58 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 19:27:14 GMT"}, {"version": "v4", "created": "Thu, 4 Aug 2016 23:31:46 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Jaitly", "Navdeep", ""], ["Sussillo", "David", ""], ["Le", "Quoc V.", ""], ["Vinyals", "Oriol", ""], ["Sutskever", "Ilya", ""], ["Bengio", "Samy", ""]]}, {"id": "1511.04891", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed\n  Elgammal", "title": "Sherlock: Scalable Fact Learning in Images", "comments": "Jan 7 Update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study scalable and uniform understanding of facts in images. Existing\nvisual recognition systems are typically modeled differently for each fact type\nsuch as objects, actions, and interactions. We propose a setting where all\nthese facts can be modeled simultaneously with a capacity to understand\nunbounded number of facts in a structured way. The training data comes as\nstructured facts in images, including (1) objects (e.g., $<$boy$>$), (2)\nattributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and\n(4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic\nlanguage view (e.g., $<$ boy, playing$>$) and a visual view (an image with this\nfact). We show that learning visual facts in a structured way enables not only\na uniform but also generalizable visual understanding. We propose and\ninvestigate recent and strong approaches from the multiview learning literature\nand also introduce two learning representation models as potential baselines.\nWe applied the investigated methods on several datasets that we augmented with\nstructured facts and a large scale dataset of more than 202,000 facts and\n814,000 images. Our experiments show the advantage of relating facts by the\nstructure by the proposed models compared to the designed baselines on\nbidirectional fact retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 09:56:04 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:36:55 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 02:56:24 GMT"}, {"version": "v4", "created": "Sat, 2 Apr 2016 05:26:39 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Cohen", "Scott", ""], ["Chang", "Walter", ""], ["Price", "Brian", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1511.04970", "submitter": "Bruno Gon\\c{c}alves", "authors": "Bruno Gon\\c{c}alves and David S\\'anchez", "title": "Learning about Spanish dialects through Twitter", "comments": "16 pages, 5 figures, 1 table", "journal-ref": "RILI, XVI 2 (28), 65-75 (2016)", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper maps the large-scale variation of the Spanish language by\nemploying a corpus based on geographically tagged Twitter messages. Lexical\ndialects are extracted from an analysis of variants of tens of concepts. The\nresulting maps show linguistic variation on an unprecedented scale across the\nglobe. We discuss the properties of the main dialects within a machine learning\napproach and find that varieties spoken in urban areas have an international\ncharacter in contrast to country areas where dialects show a more regional\nuniformity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 14:29:38 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 00:51:34 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1511.05076", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain", "title": "Latent Dirichlet Allocation Based Organisation of Broadcast Media\n  Archives for Deep Neural Network Adaptation", "comments": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU\n  2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "journal-ref": null, "doi": "10.1109/ASRU.2015.7404785", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for the discovery of latent domains in\ndiverse speech data, for the use of adaptation of Deep Neural Networks (DNNs)\nfor Automatic Speech Recognition. Our work focuses on transcription of\nmulti-genre broadcast media, which is often only categorised broadly in terms\nof high level genres such as sports, news, documentary, etc. However, in terms\nof acoustic modelling these categories are coarse. Instead, it is expected that\na mixture of latent domains can better represent the complex and diverse\nbehaviours within a TV show, and therefore lead to better and more robust\nperformance. We propose a new method, whereby these latent domains are\ndiscovered with Latent Dirichlet Allocation, in an unsupervised manner. These\nare used to adapt DNNs using the Unique Binary Code (UBIC) representation for\nthe LDA domains. Experiments conducted on a set of BBC TV broadcasts, with more\nthan 2,000 shows for training and 47 shows for testing, show that the use of\nLDA-UBIC DNNs reduces the error up to 13% relative compared to the baseline\nhybrid DNN models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 18:25:33 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Ng", "Raymond W. M.", ""], ["Hain", "Thomas", ""]]}, {"id": "1511.05099", "submitter": "Peng Zhang", "authors": "Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh", "title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex compositional structure of language makes problems at the\nintersection of vision and language challenging. But language also provides a\nstrong prior that can result in good superficial performance, without the\nunderlying models truly understanding the visual content. This can hinder\nprogress in pushing state of art in the computer vision aspects of multi-modal\nAI. In this paper, we address binary Visual Question Answering (VQA) on\nabstract scenes. We formulate this problem as visual verification of concepts\ninquired in the questions. Specifically, we convert the question to a tuple\nthat concisely summarizes the visual concept to be detected in the image. If\nthe concept can be found in the image, the answer to the question is \"yes\", and\notherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on\nthe high-level semantics of the VQA task as opposed to the low-level\nrecognition problems, and perhaps more importantly, (2) They provide us the\nmodality to balance the dataset such that language priors are controlled, and\nthe role of vision is essential. In particular, we collect fine-grained pairs\nof scenes for every question, such that the answer to the question is \"yes\" for\none scene, and \"no\" for the other for the exact same question. Indeed, language\npriors alone do not perform better than chance on our balanced dataset.\nMoreover, our proposed approach matches the performance of a state-of-the-art\nVQA approach on the unbalanced dataset, and outperforms it on the balanced\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 19:38:14 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 20:54:47 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2015 20:54:35 GMT"}, {"version": "v4", "created": "Sun, 31 Jan 2016 20:58:39 GMT"}, {"version": "v5", "created": "Tue, 19 Apr 2016 19:30:00 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Zhang", "Peng", ""], ["Goyal", "Yash", ""], ["Summers-Stay", "Douglas", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1511.05234", "submitter": "Huijuan Xu", "authors": "Huijuan Xu and Kate Saenko", "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\n  Visual Question Answering", "comments": "include test-standard result on VQA full release (V1.0) dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 01:00:04 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 03:06:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Xu", "Huijuan", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05284", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond\n  Mooney, Kate Saenko, Trevor Darrell", "title": "Deep Compositional Captioning: Describing Novel Object Categories\n  without Paired Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent deep neural network models have achieved promising results on\nthe image captioning task, they rely largely on the availability of corpora\nwith paired image and sentence captions to describe objects in context. In this\nwork, we propose the Deep Compositional Captioner (DCC) to address the task of\ngenerating descriptions of novel objects which are not present in paired\nimage-sentence datasets. Our method achieves this by leveraging large object\nrecognition datasets and external text corpora and by transferring knowledge\nbetween semantically similar concepts. Current deep caption models can only\ndescribe objects contained in paired image-sentence corpora, despite the fact\nthat they are pre-trained with large object recognition datasets, namely\nImageNet. In contrast, our model can compose sentences that describe novel\nobjects and their interactions with other objects. We demonstrate our model's\nability to describe novel concepts by empirically evaluating its performance on\nMSCOCO and show qualitative results on ImageNet images of objects for which no\npaired image-caption data exist. Further, we extend our approach to generate\ndescriptions of objects in video clips. Our results show that DCC has distinct\nadvantages over existing image and video captioning approaches for generating\ndescriptions of new objects in context.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 06:44:48 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 23:40:55 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Venugopalan", "Subhashini", ""], ["Rohrbach", "Marcus", ""], ["Mooney", "Raymond", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.05389", "submitter": "Imran Sheikh", "authors": "Imran Sheikh, Irina Illina, Dominique Fohr, Georges Linar\\`es", "title": "Learning to retrieve out-of-vocabulary words in speech recognition", "comments": "Updated references, added appendix discussing more results; added\n  more discussion, replaced simple phone search results with KWS results; added\n  KWS results for both training phase, probably last update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech\nrecognition systems used to process diachronic audio data. To help recovery of\nthe PNs missed by the system, relevant OOV PNs can be retrieved out of the many\nOOVs by exploiting semantic context of the spoken content. In this paper, we\npropose two neural network models targeted to retrieve OOV PNs relevant to an\naudio document: (a) Document level Continuous Bag of Words (D-CBOW), (b)\nDocument level Continuous Bag of Weighted Words (D-CBOW2). Both these models\ntake document words as input and learn with an objective to maximise the\nretrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new\napproach in which the input embedding layer is augmented with a context anchor\nlayer. This layer learns to assign importance to input words and has the\nability to capture (task specific) key-words in a bag-of-word neural network\nmodel. With experiments on French broadcast news videos we show that these two\nmodels outperform the baseline methods based on raw embeddings from LDA,\nSkip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives\nfaster convergence during training.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:18:07 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 09:39:29 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:32:55 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 14:03:44 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Sheikh", "Imran", ""], ["Illina", "Irina", ""], ["Fohr", "Dominique", ""], ["Linar\u00e8s", "Georges", ""]]}, {"id": "1511.05392", "submitter": "Eric Nalisnick", "authors": "Eric Nalisnick, Sachin Ravi", "title": "Learning the Dimensionality of Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for learning word embeddings with data-dependent\ndimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic\nDimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of\nMikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is\nmade dynamic by employing techniques used by Cote & Larochelle (2016) to define\nan RBM with an infinite number of hidden units. We show qualitatively and\nquantitatively that SD-SG and SD-CBOW are competitive with their\nfixed-dimension counterparts while providing a distribution over embedding\ndimensionalities, which offers a window into how semantics distribute across\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:28:55 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 04:43:11 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 17:44:37 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Nalisnick", "Eric", ""], ["Ravi", "Sachin", ""]]}, {"id": "1511.05526", "submitter": "Matthew Walter", "authors": "Zhengyang Wu and Mohit Bansal and Matthew R. Walter", "title": "Learning Articulated Motion Models from Visual and Lingual Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for robots to operate effectively in homes and workplaces, they must\nbe able to manipulate the articulated objects common within environments built\nfor and by humans. Previous work learns kinematic models that prescribe this\nmanipulation from visual demonstrations. Lingual signals, such as natural\nlanguage descriptions and instructions, offer a complementary means of\nconveying knowledge of such manipulation models and are suitable to a wide\nrange of interactions (e.g., remote manipulation). In this paper, we present a\nmultimodal learning framework that incorporates both visual and lingual\ninformation to estimate the structure and parameters that define kinematic\nmodels of articulated objects. The visual signal takes the form of an RGB-D\nimage stream that opportunistically captures object motion in an unprepared\nscene. Accompanying natural language descriptions of the motion constitute the\nlingual signal. We present a probabilistic language model that uses word\nembeddings to associate lingual verbs with their corresponding kinematic\nstructures. By exploiting the complementary nature of the visual and lingual\ninput, our method infers correct kinematic structures for various multiple-part\nobjects on which the previous state-of-the-art, visual-only system fails. We\nevaluate our multimodal learning framework on a dataset comprised of a variety\nof household objects, and demonstrate a 36% improvement in model accuracy over\nthe vision-only baseline.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 19:55:34 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 14:53:28 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Wu", "Zhengyang", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1511.05756", "submitter": "Hyeonwoo Noh", "authors": "Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han", "title": "Image Question Answering using Convolutional Neural Network with Dynamic\n  Parameter Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle image question answering (ImageQA) problem by learning a\nconvolutional neural network (CNN) with a dynamic parameter layer whose weights\nare determined adaptively based on questions. For the adaptive parameter\nprediction, we employ a separate parameter prediction network, which consists\nof gated recurrent unit (GRU) taking a question as its input and a\nfully-connected layer generating a set of candidate weights as its output.\nHowever, it is challenging to construct a parameter prediction network for a\nlarge number of parameters in the fully-connected dynamic parameter layer of\nthe CNN. We reduce the complexity of this problem by incorporating a hashing\ntechnique, where the candidate weights given by the parameter prediction\nnetwork are selected using a predefined hash function to determine individual\nweights in the dynamic parameter layer. The proposed network---joint network\nwith the CNN for ImageQA and the parameter prediction network---is trained\nend-to-end through back-propagation, where its weights are initialized using a\npre-trained CNN and GRU. The proposed algorithm illustrates the\nstate-of-the-art performance on all available public ImageQA benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 12:30:57 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["Seo", "Paul Hongsuck", ""], ["Han", "Bohyung", ""]]}, {"id": "1511.05926", "submitter": "Thien Nguyen", "authors": "Thien Huu Nguyen and Ralph Grishman", "title": "Combining Neural Networks and Log-linear Models to Improve Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed the success of the traditional feature-based\nmethod on exploiting the discrete structures such as words or lexical patterns\nto extract relations from text. Recently, convolutional and recurrent neural\nnetworks has provided very effective mechanisms to capture the hidden\nstructures within sentences via continuous representations, thereby\nsignificantly advancing the performance of relation extraction. The advantage\nof convolutional neural networks is their capacity to generalize the\nconsecutive k-grams in the sentences while recurrent neural networks are\neffective to encode long ranges of sentence context. This paper proposes to\ncombine the traditional feature-based method, the convolutional and recurrent\nneural networks to simultaneously benefit from their advantages. Our systematic\nevaluation of different network architectures and combination methods\ndemonstrates the effectiveness of this approach and results in the\nstate-of-the-art performance on the ACE 2005 and SemEval dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:17:39 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Nguyen", "Thien Huu", ""], ["Grishman", "Ralph", ""]]}, {"id": "1511.06018", "submitter": "Lingpeng Kong", "authors": "Lingpeng Kong, Chris Dyer, Noah A. Smith", "title": "Segmental Recurrent Neural Networks", "comments": "10 pages, published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce segmental recurrent neural networks (SRNNs) which define, given\nan input sequence, a joint probability distribution over segmentations of the\ninput and labelings of the segments. Representations of the input segments\n(i.e., contiguous subsequences of the input) are computed by encoding their\nconstituent tokens using bidirectional recurrent neural nets, and these\n\"segment embeddings\" are used to define compatibility scores with output\nlabels. These local compatibility scores are integrated using a global\nsemi-Markov conditional random field. Both fully supervised training -- in\nwhich segment boundaries and labels are observed -- as well as partially\nsupervised training -- in which segment boundaries are latent -- are\nstraightforward. Experiments on handwriting recognition and joint Chinese word\nsegmentation/POS tagging show that, compared to models that do not explicitly\nrepresent segments such as BIO tagging schemes and connectionist temporal\nclassification (CTC), SRNNs obtain substantially higher accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 23:02:45 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 22:46:37 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kong", "Lingpeng", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1511.06038", "submitter": "Yishu Miao", "authors": "Yishu Miao, Lei Yu and Phil Blunsom", "title": "Neural Variational Inference for Text Processing", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural variational inference have spawned a renaissance in\ndeep latent variable models. In this paper we introduce a generic variational\ninference framework for generative and conditional models of text. While\ntraditional variational methods derive an analytic approximation for the\nintractable distributions over latent variables, here we construct an inference\nnetwork conditioned on the discrete text input to provide the variational\ndistribution. We validate this framework on two very different text modelling\napplications, generative document modelling and supervised question answering.\nOur neural variational document model combines a continuous stochastic document\nrepresentation with a bag-of-words generative model and achieves the lowest\nreported perplexities on two standard test corpora. The neural answer selection\nmodel employs a stochastic representation layer within an attention mechanism\nto extract the semantics between a question and answer pair. On two question\nanswering benchmarks this model exceeds all previous published benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 01:23:28 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 14:35:48 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:49:17 GMT"}, {"version": "v4", "created": "Sat, 4 Jun 2016 06:41:58 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Miao", "Yishu", ""], ["Yu", "Lei", ""], ["Blunsom", "Phil", ""]]}, {"id": "1511.06052", "submitter": "Yi Yang", "authors": "Yi Yang and Jacob Eisenstein", "title": "Overcoming Language Variation in Sentiment Analysis with Social\n  Attention", "comments": "Published in Transactions of the Association for Computational\n  Linguistics (TACL), 2017. Please cite the TACL version:\n  https://transacl.org/ojs/index.php/tacl/article/view/1024", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variation in language is ubiquitous, particularly in newer forms of writing\nsuch as social media. Fortunately, variation is not random, it is often linked\nto social properties of the author. In this paper, we show how to exploit\nsocial networks to make sentiment analysis more robust to social language\nvariation. The key idea is linguistic homophily: the tendency of socially\nlinked individuals to use language in similar ways. We formalize this idea in a\nnovel attention-based neural network architecture, in which attention is\ndivided among several basis models, depending on the author's position in the\nsocial network. This has the effect of smoothing the classification function\nacross the social network, and makes it possible to induce personalized\nclassifiers even for authors for whom there is no labeled data or demographic\nmetadata. This model significantly improves the accuracies of sentiment\nanalysis on Twitter and on review data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 03:54:15 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 15:36:48 GMT"}, {"version": "v3", "created": "Wed, 28 Dec 2016 22:07:53 GMT"}, {"version": "v4", "created": "Sat, 26 Aug 2017 15:11:01 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yang", "Yi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1511.06066", "submitter": "Dong Wang", "authors": "Dong Wang and Thomas Fang Zheng", "title": "Transfer Learning for Speech and Language Processing", "comments": "13 pages, APSIPA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a vital technique that generalizes models trained for\none setting or task to other settings or tasks. For example in speech\nrecognition, an acoustic model trained for one language can be used to\nrecognize speech in another language, with little or no re-training data.\nTransfer learning is closely related to multi-task learning (cross-lingual vs.\nmultilingual), and is traditionally studied in the name of `model adaptation'.\nRecent advance in deep learning shows that transfer learning becomes much\neasier and more effective with high-level abstract features learned by deep\nmodels, and the `transfer' can be conducted not only between data distributions\nand data types, but also between model structures (e.g., shallow nets and deep\nnets) or even model types (e.g., Bayesian models and neural models). This\nreview paper summarizes some recent prominent research towards this direction,\nparticularly for speech and language processing. We also report some results\nfrom our group and highlight the potential of this very interesting research\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 05:54:45 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Wang", "Dong", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1511.06078", "submitter": "Liwei Wang", "authors": "Liwei Wang, Yin Li, Svetlana Lazebnik", "title": "Learning Deep Structure-Preserving Image-Text Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for learning joint embeddings of images and text\nusing a two-branch neural network with multiple layers of linear projections\nfollowed by nonlinearities. The network is trained using a large margin\nobjective that combines cross-view ranking constraints with within-view\nneighborhood structure preservation constraints inspired by metric learning\nliterature. Extensive experiments show that our approach gains significant\nimprovements in accuracy for image-to-text and text-to-image retrieval. Our\nmethod achieves new state-of-the-art results on the Flickr30K and MSCOCO\nimage-sentence datasets and shows promise on the new task of phrase\nlocalization on the Flickr30K Entities dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 07:17:49 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 03:10:04 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wang", "Liwei", ""], ["Li", "Yin", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1511.06114", "submitter": "Minh-Thang Luong", "authors": "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz\n  Kaiser", "title": "Multi-task Sequence to Sequence Learning", "comments": "10 pages, 4 figures, ICLR 2016 camera-ready, added parsing SOTA\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence to sequence learning has recently emerged as a new paradigm in\nsupervised learning. To date, most of its applications focused on only one task\nand not much work explored this framework for multiple tasks. This paper\nexamines three multi-task learning (MTL) settings for sequence to sequence\nmodels: (a) the oneto-many setting - where the encoder is shared between\nseveral tasks such as machine translation and syntactic parsing, (b) the\nmany-to-one setting - useful when only the decoder can be shared, as in the\ncase of translation and image caption generation, and (c) the many-to-many\nsetting - where multiple encoders and decoders are shared, which is the case\nwith unsupervised objectives and translation. Our results show that training on\na small amount of parsing and image caption data can improve the translation\nquality between English and German by up to 1.5 BLEU points over strong\nsingle-task baselines on the WMT benchmarks. Furthermore, we have established a\nnew state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we\nreveal interesting properties of the two unsupervised learning objectives,\nautoencoder and skip-thought, in the MTL context: autoencoder helps less in\nterms of perplexities but more on BLEU scores compared to skip-thought.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 10:24:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 06:46:29 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 08:10:59 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 10:55:58 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Luong", "Minh-Thang", ""], ["Le", "Quoc V.", ""], ["Sutskever", "Ilya", ""], ["Vinyals", "Oriol", ""], ["Kaiser", "Lukasz", ""]]}, {"id": "1511.06219", "submitter": "Lucas Sterckx", "authors": "Lucas Sterckx and Thomas Demeester and Johannes Deleu and Chris\n  Develder", "title": "Knowledge Base Population using Semantic Label Propagation", "comments": "Submitted to Knowledge Based Systems, special issue on Knowledge\n  Bases for Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial aspect of a knowledge base population system that extracts new\nfacts from text corpora, is the generation of training data for its relation\nextractors. In this paper, we present a method that maximizes the effectiveness\nof newly trained relation extractors at a minimal annotation cost. Manual\nlabeling can be significantly reduced by Distant Supervision, which is a method\nto construct training data automatically by aligning a large text corpus with\nan existing knowledge base of known facts. For example, all sentences\nmentioning both 'Barack Obama' and 'US' may serve as positive training\ninstances for the relation born_in(subject,object). However, distant\nsupervision typically results in a highly noisy training set: many training\nsentences do not really express the intended relation. We propose to combine\ndistant supervision with minimal manual supervision in a technique called\nfeature labeling, to eliminate noise from the large and noisy initial training\nset, resulting in a significant increase of precision. We further improve on\nthis approach by introducing the Semantic Label Propagation method, which uses\nthe similarity between low-dimensional representations of candidate training\ninstances, to extend the training set in order to increase recall while\nmaintaining high precision. Our proposed strategy for generating training data\nis studied and evaluated on an established test collection designed for\nknowledge base population tasks. The experimental results show that the\nSemantic Label Propagation strategy leads to substantial performance gains when\ncompared to existing approaches, while requiring an almost negligible manual\nannotation effort.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:51:31 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 11:52:14 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Sterckx", "Lucas", ""], ["Demeester", "Thomas", ""], ["Deleu", "Johannes", ""], ["Develder", "Chris", ""]]}, {"id": "1511.06246", "submitter": "Xinchi Chen", "authors": "Xinchi Chen, Xipeng Qiu, Jingxiang Jiang, Xuanjing Huang", "title": "Gaussian Mixture Embeddings for Multiple Word Prototypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, word representation has been increasingly focused on for its\nexcellent properties in representing the word semantics. Previous works mainly\nsuffer from the problem of polysemy phenomenon. To address this problem, most\nof previous models represent words as multiple distributed vectors. However, it\ncannot reflect the rich relations between words by representing words as points\nin the embedded space. In this paper, we propose the Gaussian mixture skip-gram\n(GMSG) model to learn the Gaussian mixture embeddings for words based on\nskip-gram framework. Each word can be regarded as a gaussian mixture\ndistribution in the embedded space, and each gaussian component represents a\nword sense. Since the number of senses varies from word to word, we further\npropose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense\nnumber of words during training. Experiments on four benchmarks show the\neffectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:46:49 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Chen", "Xinchi", ""], ["Qiu", "Xipeng", ""], ["Jiang", "Jingxiang", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1511.06285", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Emilia Rejmund, Krzysztof Marasek", "title": "Harvesting comparable corpora and mining them for equivalent bilingual\n  sentences using statistical classification and analogy- based heuristics", "comments": "Springer p. 433-441, 2015", "journal-ref": null, "doi": "10.1007/978-3-319-25252-0_46", "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentences are a relatively scarce but extremely useful resource for\nmany applications including cross-lingual retrieval and statistical machine\ntranslation. This research explores our new methodologies for mining such data\nfrom previously obtained comparable corpora. The task is highly practical since\nnon-parallel multilingual data exist in far greater quantities than parallel\ncorpora, but parallel sentences are a much more useful resource. Here we\npropose a web crawling method for building subject-aligned comparable corpora\nfrom e.g. Wikipedia dumps and Euronews web page. The improvements in machine\ntranslation are shown on Polish-English language pair for various text domains.\nWe also tested another method of building parallel corpora based on comparable\ncorpora data. It lets automatically broad existing corpus of sentences from\nsubject of corpora based on analogies between them.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:26:06 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Rejmund", "Emilia", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1511.06303", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski and Armand Joulin and Tomas Mikolov", "title": "Alternative structures for character-level RNNs", "comments": "First revision. Updated Table 3, extended Sec. 5.3 and added a\n  paragraph to the conclusion,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are convenient and efficient models for language\nmodeling. However, when applied on the level of characters instead of words,\nthey suffer from several problems. In order to successfully model long-term\ndependencies, the hidden representation needs to be large. This in turn implies\nhigher computational costs, which can become prohibitive in practice. We\npropose two alternative structural modifications to the classical RNN model.\nThe first one consists on conditioning the character level representation on\nthe previous word representation. The other one uses the character history to\ncondition the output probability. We evaluate the performance of the two\nproposed modifications on challenging, multi-lingual real world data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 18:46:21 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 17:35:35 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1511.06312", "submitter": "James Cross", "authors": "James Cross, Bing Xiang, and Bowen Zhou", "title": "Good, Better, Best: Choosing Word Embedding Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose two methods of learning vector representations of words and\nphrases that each combine sentence context with structural features extracted\nfrom dependency trees. Using several variations of neural network classifier,\nwe show that these combined methods lead to improved performance when used as\ninput features for supervised term-matching.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:13:58 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Cross", "James", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1511.06341", "submitter": "Ramanathan Guha", "authors": "Ramanathan V Guha, Vineet Gupta", "title": "Communicating Semantics: Reference by Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Messages often refer to entities such as people, places and events. Correct\nidentification of the intended reference is an essential part of communication.\nLack of shared unique names often complicates entity reference. Shared\nknowledge can be used to construct uniquely identifying descriptive references\nfor entities with ambiguous names. We introduce a mathematical model for\n`Reference by Description', derive results on the conditions under which, with\nhigh probability, programs can construct unambiguous references to most\nentities in the domain of discourse and provide empirical validation of these\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:14:43 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:33:36 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 00:42:06 GMT"}, {"version": "v4", "created": "Mon, 7 Mar 2016 16:41:38 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Guha", "Ramanathan V", ""], ["Gupta", "Vineet", ""]]}, {"id": "1511.06349", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal\n  Jozefowicz, Samy Bengio", "title": "Generating Sentences from a Continuous Space", "comments": "First two authors contributed equally. Work was done when all authors\n  were at Google, Inc", "journal-ref": "SIGNLL Conference on Computational Natural Language Learning\n  (CONLL), 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard recurrent neural network language model (RNNLM) generates\nsentences one word at a time and does not work from an explicit global sentence\nrepresentation. In this work, we introduce and study an RNN-based variational\nautoencoder generative model that incorporates distributed latent\nrepresentations of entire sentences. This factorization allows it to explicitly\nmodel holistic properties of sentences such as style, topic, and high-level\nsyntactic features. Samples from the prior over these sentence representations\nremarkably produce diverse and well-formed sentences through simple\ndeterministic decoding. By examining paths through this latent space, we are\nable to generate coherent novel sentences that interpolate between known\nsentences. We present techniques for solving the difficult learning problem\npresented by this model, demonstrate its effectiveness in imputing missing\nwords, explore many interesting properties of the model's latent sentence\nspace, and present negative results on the use of the model in language\nmodeling.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:38:45 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 02:59:34 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 17:38:42 GMT"}, {"version": "v4", "created": "Thu, 12 May 2016 20:51:23 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Vilnis", "Luke", ""], ["Vinyals", "Oriol", ""], ["Dai", "Andrew M.", ""], ["Jozefowicz", "Rafal", ""], ["Bengio", "Samy", ""]]}, {"id": "1511.06361", "submitter": "Ivan Vendrov", "authors": "Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun", "title": "Order-Embeddings of Images and Language", "comments": "ICLR camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:56:14 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 21:19:30 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 04:32:53 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 04:58:08 GMT"}, {"version": "v5", "created": "Sun, 17 Jan 2016 03:08:20 GMT"}, {"version": "v6", "created": "Tue, 1 Mar 2016 08:23:50 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Vendrov", "Ivan", ""], ["Kiros", "Ryan", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1511.06379", "submitter": "Richard Searle Dr", "authors": "Richard Searle, Megan Bingham-Walker", "title": "Dynamic Adaptive Network Intelligence", "comments": "8 pages, 2 figures, 3 tables, ICLR 2016 conference paper submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate representational learning of both the explicit and implicit\nrelationships within data is critical to the ability of machines to perform\nmore complex and abstract reasoning tasks. We describe the efficient weakly\nsupervised learning of such inferences by our Dynamic Adaptive Network\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\nquestion answering tasks in the bAbI dataset that have proved difficult for\ncontemporary approaches to learning representation (Weston et al., 2015).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:07:27 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Searle", "Richard", ""], ["Bingham-Walker", "Megan", ""]]}, {"id": "1511.06388", "submitter": "Andrew Trask", "authors": "Andrew Trask, Phil Michalak, John Liu", "title": "sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In\n  Neural Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural word representations have proven useful in Natural Language Processing\n(NLP) tasks due to their ability to efficiently model complex semantic and\nsyntactic word relationships. However, most techniques model only one\nrepresentation per word, despite the fact that a single word can have multiple\nmeanings or \"senses\". Some techniques model words by using multiple vectors\nthat are clustered based on context. However, recent neural approaches rarely\nfocus on the application to a consuming NLP algorithm. Furthermore, the\ntraining process of recent word-sense models is expensive relative to\nsingle-sense embedding processes. This paper presents a novel approach which\naddresses these concerns by modeling multiple embeddings for each word based on\nsupervised disambiguation, which provides a fast and accurate way for a\nconsuming NLP model to select a sense-disambiguated embedding. We demonstrate\nthat these embeddings can disambiguate both contrastive senses such as nominal\nand verbal senses as well as nuanced senses such as sarcasm. We further\nevaluate Part-of-Speech disambiguated embeddings on neural dependency parsing,\nyielding a greater than 8% average error reduction in unlabeled attachment\nscores across 6 languages.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:22:42 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Trask", "Andrew", ""], ["Michalak", "Phil", ""], ["Liu", "John", ""]]}, {"id": "1511.06391", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Samy Bengio, Manjunath Kudlur", "title": "Order Matters: Sequence to sequence for sets", "comments": "Accepted as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequences have become first class citizens in supervised learning thanks to\nthe resurgence of recurrent neural networks. Many complex tasks that require\nmapping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to\nefficiently represent the joint probability of sequences. In many cases,\nhowever, variable sized inputs and/or outputs might not be naturally expressed\nas sequences. For instance, it is not clear how to input a set of numbers into\na model where the task is to sort them; similarly, we do not know how to\norganize outputs when they correspond to random variables and the task is to\nmodel their unknown joint probability. In this paper, we first show using\nvarious examples that the order in which we organize input and/or output data\nmatters significantly when learning an underlying model. We then discuss an\nextension of the seq2seq framework that goes beyond sequences and handles input\nsets in a principled way. In addition, we propose a loss which, by searching\nover possible orders during training, deals with the lack of structure of\noutput sets. We show empirical evidence of our claims regarding ordering, and\non the modifications to the seq2seq framework on benchmark language modeling\nand parsing tasks, as well as two artificial tasks -- sorting numbers and\nestimating the joint probability of unknown graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:31:26 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 16:50:35 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 17:03:38 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2016 22:25:12 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Vinyals", "Oriol", ""], ["Bengio", "Samy", ""], ["Kudlur", "Manjunath", ""]]}, {"id": "1511.06396", "submitter": "Patrick Verga", "authors": "Patrick Verga, David Belanger, Emma Strubell, Benjamin Roth, Andrew\n  McCallum", "title": "Multilingual Relation Extraction using Compositional Universal Schema", "comments": "Accepted to NAACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal schema builds a knowledge base (KB) of entities and relations by\njointly embedding all relation types from input KBs as well as textual patterns\nexpressing relations from raw text. In most previous applications of universal\nschema, each textual pattern is represented as a single embedding, preventing\ngeneralization to unseen patterns. Recent work employs a neural network to\ncapture patterns' compositional semantics, providing generalization to all\npossible input text. In response, this paper introduces significant further\nimprovements to the coverage and flexibility of universal schema relation\nextraction: predictions for entities unseen in training and multilingual\ntransfer learning to domains with no annotation. We evaluate our model through\nextensive experiments on the English and Spanish TAC KBP benchmark,\noutperforming the top system from TAC 2013 slot-filling using no handwritten\npatterns or additional annotation. We also consider a multilingual setting in\nwhich English training data entities overlap with the seed KB, but Spanish text\ndoes not. Despite having no annotation for Spanish data, we train an accurate\npredictor, with additional improvements obtained by tying word embeddings\nacross languages. Furthermore, we find that multilingual training improves\nEnglish relation extraction accuracy. Our approach is thus suited to\nbroad-coverage automated knowledge base construction in a variety of languages\nand domains.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:42:23 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 20:28:36 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Verga", "Patrick", ""], ["Belanger", "David", ""], ["Strubell", "Emma", ""], ["Roth", "Benjamin", ""], ["McCallum", "Andrew", ""]]}, {"id": "1511.06397", "submitter": "Martin Andrews", "authors": "Martin Andrews", "title": "Compressing Word Embeddings", "comments": "10 pages, 0 figures, submitted to ICONIP-2016. Previous experimental\n  results were submitted to ICLR-2016, but the paper has been significantly\n  updated, since a new experimental set-up worked much better", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for learning vector space representations of words have\nsucceeded in capturing fine-grained semantic and syntactic regularities using\nvector arithmetic. However, these vector space representations (created through\nlarge-scale text analysis) are typically stored verbatim, since their internal\nstructure is opaque. Using word-analogy tests to monitor the level of detail\nstored in compressed re-representations of the same vector space, the\ntrade-offs between the reduction in memory usage and expressiveness are\ninvestigated. A simple scheme is outlined that can reduce the memory footprint\nof a state-of-the-art embedding by a factor of 10, with only minimal impact on\nperformance. Then, using the same `bit budget', a binary (approximate)\nfactorisation of the same space is also explored, with the aim of creating an\nequivalent representation with better interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:42:47 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 17:19:51 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Andrews", "Martin", ""]]}, {"id": "1511.06407", "submitter": "Suyoun Kim", "authors": "Suyoun Kim, Ian Lane", "title": "Recurrent Models for Auditory Attention in Multi-Microphone Distance\n  Speech Recognition", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of multiple microphone data is one of the key ways to achieve\nrobust speech recognition in noisy environments or when the speaker is located\nat some distance from the input device. Signal processing techniques such as\nbeamforming are widely used to extract a speech signal of interest from\nbackground noise. These techniques, however, are highly dependent on prior\nspatial information about the microphones and the environment in which the\nsystem is being used. In this work, we present a neural attention network that\ndirectly combines multi-channel audio to generate phonetic states without\nrequiring any prior knowledge of the microphone layout or any explicit signal\npreprocessing for speech enhancement. We embed an attention mechanism within a\nRecurrent Neural Network (RNN) based acoustic model to automatically tune its\nattention to a more reliable input source. Unlike traditional multi-channel\npreprocessing, our system can be optimized towards the desired output in one\nstep. Although attention-based models have recently achieved impressive results\non sequence-to-sequence learning, no attention mechanisms have previously been\napplied to learn potentially asynchronous and non-stationary multiple inputs.\nWe evaluate our neural attention model on the CHiME-3 challenge task, and show\nthat the model achieves comparable performance to beamforming using a purely\ndata-driven method.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:56:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 22:16:54 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Kim", "Suyoun", ""], ["Lane", "Ian", ""]]}, {"id": "1511.06420", "submitter": "Ethan Caballero V", "authors": "Ethan Caballero", "title": "Skip-Thought Memory Networks", "comments": "Removed by arXiv administrators because submission violated the terms\n  of arXiv's license agreement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) is fundamental to natural language processing in that\nmost nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly\nsupervised memory network models that have been proposed so far struggle at\nanswering questions that involve relations among multiple entities (such as\nfacebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address\nthis problem of learning multi-argument multi-hop semantic relations for the\npurpose of QA, we propose a method that combines the jointly learned long-term\nread-write memory and attentive inference components of end-to-end memory\nnetworks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector\nrepresentations encoded by a Skip-Thought model (Kiros et al., 2015). This\nchoice to append Skip-Thought Vectors to the existing MemN2N framework is\nmotivated by the fact that Skip-Thought Vectors have been shown to accurately\nmodel multi-argument semantic relations (Kiros et al., 2015).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:15:46 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 02:30:16 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Caballero", "Ethan", ""]]}, {"id": "1511.06426", "submitter": "Moontae Lee", "authors": "Moontae Lee, Xiaodong He, Wen-tau Yih, Jianfeng Gao, Li Deng, Paul\n  Smolensky", "title": "Reasoning in Vector Space: An Exploratory Study of Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering tasks have shown remarkable progress with distributed\nvector representation. In this paper, we investigate the recently proposed\nFacebook bAbI tasks which consist of twenty different categories of questions\nthat require complex reasoning. Because the previous work on bAbI are all\nend-to-end models, errors could come from either an imperfect understanding of\nsemantics or in certain steps of the reasoning. For clearer analysis, we\npropose two vector space models inspired by Tensor Product Representation (TPR)\nto perform knowledge encoding and logical reasoning based on common-sense\ninference. They together achieve near-perfect accuracy on all categories\nincluding positional reasoning and path finding that have proved difficult for\nmost of the previous approaches. We hypothesize that the difficulties in these\ncategories are due to the multi-relations in contrast to uni-relational\ncharacteristic of other categories. Our exploration sheds light on designing\nmore sophisticated dataset and moving one step toward integrating transparent\nand interpretable formalism of TPR into existing learning paradigms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:30:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 22:30:01 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 11:16:46 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2016 18:49:34 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Lee", "Moontae", ""], ["He", "Xiaodong", ""], ["Yih", "Wen-tau", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""], ["Smolensky", "Paul", ""]]}, {"id": "1511.06438", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala, Alsuhaibani Mohammed, Takanori Maehara, Ken-ichi\n  Kawarabayashi", "title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon", "comments": "Accepted to AAAI-2016", "journal-ref": "Proceedings of the AAAI 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning word representations using large text corpora have\nreceived much attention lately due to their impressive performance in numerous\nnatural language processing (NLP) tasks such as, semantic similarity\nmeasurement, and word analogy detection. Despite their success, these\ndata-driven word representation learning methods do not consider the rich\nsemantic relational structure between words in a co-occurring context. On the\nother hand, already much manual effort has gone into the construction of\nsemantic lexicons such as the WordNet that represent the meanings of words by\ndefining the various relationships that exist among the words in a language. We\nconsider the question, can we improve the word representations learnt using a\ncorpora by integrating the knowledge from semantic lexicons?. For this purpose,\nwe propose a joint word representation learning method that simultaneously\npredicts the co-occurrences of two words in a sentence subject to the\nrelational constrains given by the semantic lexicon. We use relations that\nexist between words in the lexicon to regularize the word representations\nlearnt from the corpus. Our proposed method statistically significantly\noutperforms previously proposed methods for incorporating semantic lexicons\ninto word representations on several benchmark datasets for semantic similarity\nand word analogy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:58:10 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Bollegala", "Danushka", ""], ["Mohammed", "Alsuhaibani", ""], ["Maehara", "Takanori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1511.06591", "submitter": "Normunds Gruzitis", "authors": "Normunds Gruzitis and Guntis Barzdins", "title": "Polysemy in Controlled Natural Language Texts", "comments": null, "journal-ref": "Controlled Natural Language, Lecture Notes in Computer Science,\n  Vol. 5972, Springer, 2010, pp. 102-120", "doi": "10.1007/978-3-642-14418-9_7", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational semantics and logic-based controlled natural languages (CNL) do\nnot address systematically the word sense disambiguation problem of content\nwords, i.e., they tend to interpret only some functional words that are crucial\nfor construction of discourse representation structures. We show that\nmicro-ontologies and multi-word units allow integration of the rich and\npolysemous multi-domain background knowledge into CNL thus providing\ninterpretation for the content words. The proposed approach is demonstrated by\nextending the Attempto Controlled English (ACE) with polysemous and procedural\nconstructs resulting in a more natural CNL named PAO covering narrative\nmulti-domain texts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 13:41:31 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Gruzitis", "Normunds", ""], ["Barzdins", "Guntis", ""]]}, {"id": "1511.06674", "submitter": "Marius Leordeanu", "authors": "Anirudh Goyal and Marius Leordeanu", "title": "Stories in the Eye: Contextual Visual Interactions for Efficient Video\n  to Language Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating higher level visual and linguistic interpretations is at the\nheart of human intelligence. As automatic visual category recognition in images\nis approaching human performance, the high level understanding in the dynamic\nspatiotemporal domain of videos and its translation into natural language is\nstill far from being solved. While most works on vision-to-text translations\nuse pre-learned or pre-established computational linguistic models, in this\npaper we present an approach that uses vision alone to efficiently learn how to\ntranslate into language the video content. We discover, in simple form, the\nstory played by main actors, while using only visual cues for representing\nobjects and their interactions. Our method learns in a hierarchical manner\nhigher level representations for recognizing subjects, actions and objects\ninvolved, their relevant contextual background and their interaction to one\nanother over time. We have a three stage approach: first we take in\nconsideration features of the individual entities at the local level of\nappearance, then we consider the relationship between these objects and actions\nand their video background, and third, we consider their spatiotemporal\nrelations as inputs to classifiers at the highest level of interpretation.\nThus, our approach finds a coherent linguistic description of videos in the\nform of a subject, verb and object based on their role played in the overall\nvisual story learned directly from training data, without using a known\nlanguage model. We test the efficiency of our approach on a large scale dataset\ncontaining YouTube clips taken in the wild and demonstrate state-of-the-art\nperformance, often superior to current approaches that use more complex,\npre-learned linguistic knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:33:13 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Goyal", "Anirudh", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1511.06709", "submitter": "Rico Sennrich", "authors": "Rico Sennrich and Barry Haddow and Alexandra Birch", "title": "Improving Neural Machine Translation Models with Monolingual Data", "comments": "accepted to ACL 2016; new section on effect of back-translation\n  quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Machine Translation (NMT) has obtained state-of-the art performance\nfor several language pairs, while only using parallel data for training.\nTarget-side monolingual data plays an important role in boosting fluency for\nphrase-based statistical machine translation, and we investigate the use of\nmonolingual data for NMT. In contrast to previous work, which combines NMT\nmodels with separately trained language models, we note that encoder-decoder\nNMT architectures already have the capacity to learn the same information as a\nlanguage model, and we explore strategies to train with monolingual data\nwithout changing the neural network architecture. By pairing monolingual\ntraining data with an automatic back-translation, we can treat it as additional\nparallel training data, and we obtain substantial improvements on the WMT 15\ntask English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task\nTurkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We\nalso show that fine-tuning on in-domain monolingual and parallel data gives\nsubstantial improvements for the IWSLT 15 task English->German.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 17:58:37 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 14:52:55 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 19:54:58 GMT"}, {"version": "v4", "created": "Fri, 3 Jun 2016 15:09:54 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Sennrich", "Rico", ""], ["Haddow", "Barry", ""], ["Birch", "Alexandra", ""]]}, {"id": "1511.06732", "submitter": "Marc'Aurelio Ranzato", "authors": "Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba", "title": "Sequence Level Training with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language processing applications use language models to generate\ntext. These models are typically trained to predict the next word in a\nsequence, given the previous words and some context such as an image. However,\nat test time the model is expected to generate the entire sequence from\nscratch. This discrepancy makes generation brittle, as errors may accumulate\nalong the way. We address this issue by proposing a novel sequence level\ntraining algorithm that directly optimizes the metric used at test time, such\nas BLEU or ROUGE. On three different tasks, our approach outperforms several\nstrong baselines for greedy generation. The method is also competitive when\nthese baselines employ beam search, while being several times faster.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 19:25:54 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 16:11:27 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 16:51:31 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2016 06:24:58 GMT"}, {"version": "v5", "created": "Fri, 12 Feb 2016 16:05:32 GMT"}, {"version": "v6", "created": "Wed, 4 May 2016 13:43:39 GMT"}, {"version": "v7", "created": "Fri, 6 May 2016 21:18:46 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Ranzato", "Marc'Aurelio", ""], ["Chopra", "Sumit", ""], ["Auli", "Michael", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1511.06798", "submitter": "Luke Miratrix", "authors": "Luke Miratrix and Robin Ackerman", "title": "Conducting sparse feature selection on arbitrarily long phrases in text\n  corpora with a focus on interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for topic-specific summarization of large text\ncorpora, and illustrate how it can be used for analysis in two quite different\ncontexts: an OSHA database of fatality and catastrophe reports (to facilitate\nsurveillance for patterns in circumstances leading to injury or death) and\nlegal decisions on workers' compensation claims (to explore relevant case law).\nOur summarization framework, built on sparse classification methods, is a\ncompromise between simple word frequency based methods currently in wide use,\nand more heavyweight, model-intensive methods such as Latent Dirichlet\nAllocation (LDA). For a particular topic of interest (e.g., mental health\ndisability, or chemical reactions), we regress a labeling of documents onto the\nhigh-dimensional counts of all the other words and phrases in the documents.\nThe resulting small set of phrases found as predictive are then harvested as\nthe summary. Using a branch-and-bound approach, this method can be extended to\nallow for phrases of arbitrary length, which allows for potentially rich\nsummarization. We discuss how focus on the purpose of the summaries can inform\nchoices of regularization parameters and model constraints. We evaluate this\ntool by comparing computational time and summary statistics of the resulting\nword lists to three other methods in the literature. We also present a new R\npackage, textreg. Overall, we argue that sparse methods have much to offer text\nanalysis, and is a branch of research that should be considered further in this\ncontext.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 23:39:48 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 00:18:19 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Miratrix", "Luke", ""], ["Ackerman", "Robin", ""]]}, {"id": "1511.06833", "submitter": "Thenmalar S", "authors": "S. Thenmalar, J. Balaji, and T.V. Geetha", "title": "Semi-supervised Bootstrapping approach for Named Entity Recognition", "comments": "13 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of Named Entity Recognition (NER) is to identify references of named\nentities in unstructured documents, and to classify them into pre-defined\nsemantic categories. NER often aids from added background knowledge in the form\nof gazetteers. However using such a collection does not deal with name variants\nand cannot resolve ambiguities associated in identifying the entities in\ncontext and associating them with predefined categories. We present a\nsemi-supervised NER approach that starts with identifying named entities with a\nsmall set of training data. Using the identified named entities, the word and\nthe context features are used to define the pattern. This pattern of each named\nentity category is used as a seed pattern to identify the named entities in the\ntest set. Pattern scoring and tuple value score enables the generation of the\nnew patterns to identify the named entity categories. We have evaluated the\nproposed system for English language with the dataset of tagged (IEER) and\nuntagged (CoNLL 2003) named entity corpus and for Tamil language with the\ndocuments from the FIRE corpus and yield an average f-measure of 75% for both\nthe languages.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 04:11:44 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Thenmalar", "S.", ""], ["Balaji", "J.", ""], ["Geetha", "T. V.", ""]]}, {"id": "1511.06838", "submitter": "Stella Yu", "authors": "Takuya Narihira, Damian Borth, Stella X. Yu, Karl Ni, Trevor Darrell", "title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural\n  Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the visual sentiment task of mapping an image to an adjective\nnoun pair (ANP) such as \"cute baby\". To capture the two-factor structure of our\nANP semantics as well as to overcome annotation noise and ambiguity, we propose\na novel factorized CNN model which learns separate representations for\nadjectives and nouns but optimizes the classification performance over their\nproduct. Our experiments on the publicly available SentiBank dataset show that\nour model significantly outperforms not only independent ANP classifiers on\nunseen ANPs and on retrieving images of novel ANPs, but also image captioning\nmodels which capture word semantics from co-occurrence of natural text; the\nlatter turn out to be surprisingly poor at capturing the sentiment evoked by\npure visual experience. That is, our factorized ANP CNN not only trains better\nfrom noisy labels, generalizes better to new images, but can also expands the\nANP vocabulary on its own.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 04:58:46 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Narihira", "Takuya", ""], ["Borth", "Damian", ""], ["Yu", "Stella X.", ""], ["Ni", "Karl", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.06909", "submitter": "Shihao Ji", "authors": "Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson\n  and Pradeep Dubey", "title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very\n  Large Vocabularies", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BlackOut, an approximation algorithm to efficiently train massive\nrecurrent neural network language models (RNNLMs) with million word\nvocabularies. BlackOut is motivated by using a discriminative loss, and we\ndescribe a new sampling strategy which significantly reduces computation while\nimproving stability, sample efficiency, and rate of convergence. One way to\nunderstand BlackOut is to view it as an extension of the DropOut strategy to\nthe output layer, wherein we use a discriminative training loss and a weighted\nsampling scheme. We also establish close connections between BlackOut,\nimportance sampling, and noise contrastive estimation (NCE). Our experiments,\non the recently released one billion word language modeling benchmark,\ndemonstrate scalability and accuracy of BlackOut; we outperform the\nstate-of-the art, and achieve the lowest perplexity scores on this dataset.\nMoreover, unlike other established methods which typically require GPUs or CPU\nclusters, we show that a carefully implemented version of BlackOut requires\nonly 1-10 days on a single machine to train a RNNLM with a million word\nvocabulary and billions of parameters on one billion words. Although we\ndescribe BlackOut in the context of RNNLM training, it can be used to any\nnetworks with large softmax output layers.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 17:49:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 07:09:16 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 06:08:54 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 04:40:55 GMT"}, {"version": "v5", "created": "Wed, 6 Jan 2016 21:57:56 GMT"}, {"version": "v6", "created": "Sun, 21 Feb 2016 16:40:26 GMT"}, {"version": "v7", "created": "Thu, 31 Mar 2016 17:37:25 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Ji", "Shihao", ""], ["Vishwanathan", "S. V. N.", ""], ["Satish", "Nadathur", ""], ["Anderson", "Michael J.", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1511.06931", "submitter": "Jason  Weston", "authors": "Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra,\n  Alexander Miller, Arthur Szlam, Jason Weston", "title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-term goal of machine learning is to build intelligent conversational\nagents. One recent popular approach is to train end-to-end models on a large\namount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals\n& Le, 2015; Shang et al., 2015). However, this approach leaves many questions\nunanswered as an understanding of the precise successes and shortcomings of\neach model is hard to assess. A contrasting recent proposal are the bAbI tasks\n(Weston et al., 2015b) which are synthetic data that measure the ability of\nlearning machines at various reasoning tasks over toy language. Unfortunately,\nthose tests are very small and hence may encourage methods that do not scale.\nIn this work, we propose a suite of new tasks of a much larger scale that\nattempt to bridge the gap between the two regimes. Choosing the domain of\nmovies, we provide tasks that test the ability of models to answer factual\nquestions (utilizing OMDB), provide personalization (utilizing MovieLens),\ncarry short conversations about the two, and finally to perform on natural\ndialogs from Reddit. We provide a dataset covering 75k movie entities and with\n3.5M training examples. We present results of various models on these tasks,\nand evaluate their performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 22:26:49 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 09:31:59 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2016 04:51:54 GMT"}, {"version": "v4", "created": "Fri, 1 Apr 2016 06:22:44 GMT"}, {"version": "v5", "created": "Fri, 15 Apr 2016 20:22:13 GMT"}, {"version": "v6", "created": "Tue, 19 Apr 2016 15:30:29 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Dodge", "Jesse", ""], ["Gane", "Andreea", ""], ["Zhang", "Xiang", ""], ["Bordes", "Antoine", ""], ["Chopra", "Sumit", ""], ["Miller", "Alexander", ""], ["Szlam", "Arthur", ""], ["Weston", "Jason", ""]]}, {"id": "1511.06961", "submitter": "Lisa Lee", "authors": "Lisa Seung-Yeon Lee", "title": "On the Linear Algebraic Structure of Distributed Word Representations", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we leverage the linear algebraic structure of distributed word\nrepresentations to automatically extend knowledge bases and allow a machine to\nlearn new facts about the world. Our goal is to extract structured facts from\ncorpora in a simpler manner, without applying classifiers or patterns, and\nusing only the co-occurrence statistics of words. We demonstrate that the\nlinear algebraic structure of word embeddings can be used to reduce data\nrequirements for methods of learning facts. In particular, we demonstrate that\nwords belonging to a common category, or pairs of words satisfying a certain\nrelation, form a low-rank subspace in the projected space. We compute a basis\nfor this low-rank subspace using singular value decomposition (SVD), then use\nthis basis to discover new facts and to fit vectors for less frequent words\nwhich we do not yet have vectors for.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 04:28:39 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Lee", "Lisa Seung-Yeon", ""]]}, {"id": "1511.06995", "submitter": "Paolo Dragone", "authors": "Paolo Dragone", "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and\n  Interpretation", "comments": "Master thesis, 98 pages, ISBN: 9788887096057", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-sentential utterances (NSUs) are utterances that lack a complete\nsentential form but whose meaning can be inferred from the dialogue context,\nsuch as \"OK\", \"where?\", \"probably at his apartment\". The interpretation of\nnon-sentential utterances is an important problem in computational linguistics\nsince they constitute a frequent phenomena in dialogue and they are\nintrinsically context-dependent. The interpretation of NSUs is the task of\nretrieving their full semantic content from their form and the dialogue\ncontext. The first half of this thesis is devoted to the NSU classification\ntask. Our work builds upon Fern\\'andez et al. (2007) which present a series of\nmachine-learning experiments on the classification of NSUs. We extended their\napproach with a combination of new features and semi-supervised learning\ntechniques. The empirical results presented in this thesis show a modest but\nsignificant improvement over the state-of-the-art classification performance.\nThe consecutive, yet independent, problem is how to infer an appropriate\nsemantic representation of such NSUs on the basis of the dialogue context.\nFern\\'andez (2006) formalizes this task in terms of \"resolution rules\" built on\ntop of the Type Theory with Records (TTR). Our work is focused on the\nreimplementation of the resolution rules from Fern\\'andez (2006) with a\nprobabilistic account of the dialogue state. The probabilistic rules formalism\nLison (2014) is particularly suited for this task because, similarly to the\nframework developed by Ginzburg (2012) and Fern\\'andez (2006), it involves the\nspecification of update rules on the variables of the dialogue state to capture\nthe dynamics of the conversation. However, the probabilistic rules can also\nencode probabilistic knowledge, thereby providing a principled account of\nambiguities in the NSU resolution process.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 11:28:26 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Dragone", "Paolo", ""]]}, {"id": "1511.07001", "submitter": "Amelia Carolina Sparavigna", "authors": "A.C. Sparavigna and R. Marazzato", "title": "Analysis of a Play by Means of CHAPLIN, the Characters and Places\n  Interaction Network Software", "comments": null, "journal-ref": "International Journal of Sciences, 2015, 4(3):60-68", "doi": "10.18483/ijSci.662", "report-no": null, "categories": "cs.CY cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have developed a software able of gathering information on\nsocial networks from written texts. This software, the CHAracters and PLaces\nInteraction Network (CHAPLIN) tool, is implemented in Visual Basic. By means of\nit, characters and places of a literary work can be extracted from a list of\nraw words. The software interface helps users to select their names out of this\nlist. Setting some parameters, CHAPLIN creates a network where nodes represent\ncharacters/places and edges give their interactions. Nodes and edges are\nlabelled by performances. In this paper, we propose to use CHAPLIN for the\nanalysis a William Shakespeare's play, the famous 'Tragedy of Hamlet, Prince of\nDenmark'. Performances of characters in the play as a whole and in each act of\nit are given by graphs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 12:06:34 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Sparavigna", "A. C.", ""], ["Marazzato", "R.", ""]]}, {"id": "1511.07067", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Ramakrishna Vedantam, Jos\\'e M. F. Moura, Devi Parikh", "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings\n  Using Abstract Scenes", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model to learn visually grounded word embeddings (vis-w2v) to\ncapture visual notions of semantic relatedness. While word embeddings trained\nusing text have been extremely successful, they cannot uncover notions of\nsemantic relatedness implicit in our visual world. For instance, although\n\"eats\" and \"stares at\" seem unrelated in text, they share semantics visually.\nWhen people are eating something, they also tend to stare at the food.\nGrounding diverse relations like \"eats\" and \"stares at\" into vision remains\nchallenging, despite recent progress in vision. We note that the visual\ngrounding of words depends on semantics, and not the literal pixels. We thus\nuse abstract scenes created from clipart to provide the visual grounding. We\nfind that the embeddings we learn capture fine-grained, visually grounded\nnotions of semantic relatedness. We show improvements over text-only word\nembeddings (word2vec) on three tasks: common-sense assertion classification,\nvisual paraphrasing and text-based image retrieval. Our code and datasets are\navailable online.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 20:46:42 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 18:15:25 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Kottur", "Satwik", ""], ["Vedantam", "Ramakrishna", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Parikh", "Devi", ""]]}, {"id": "1511.07607", "submitter": "Rahul Anand Sharma Mr.", "authors": "Rahul Anand Sharma, Pramod Sankar K and CV Jawahar", "title": "Fine-Grain Annotation of Cricket Videos", "comments": "ACPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of human activities is one of the key problems in video\nunderstanding. Action recognition is challenging even for specific categories\nof videos, such as sports, that contain only a small set of actions.\nInterestingly, sports videos are accompanied by detailed commentaries available\nonline, which could be used to perform action annotation in a weakly-supervised\nsetting. For the specific case of Cricket videos, we address the challenge of\ntemporal segmentation and annotation of ctions with semantic descriptions. Our\nsolution consists of two stages. In the first stage, the video is segmented\ninto \"scenes\", by utilizing the scene category information extracted from\ntext-commentary. The second stage consists of classifying video-shots as well\nas the phrases in the textual description into various categories. The relevant\nphrases are then suitably mapped to the video-shots. The novel aspect of this\nwork is the fine temporal scale at which semantic information is assigned to\nthe video. As a result of our approach, we enable retrieval of specific actions\nthat last only a few seconds, from several hours of video. This solution yields\na large number of labeled exemplars, with no manual effort, that could be used\nby machine learning algorithms to learn complex actions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:34:20 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 10:48:11 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Sharma", "Rahul Anand", ""], ["K", "Pramod Sankar", ""], ["Jawahar", "CV", ""]]}, {"id": "1511.07788", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Marasek, {\\L}ukasz Brocki, Danijel Korzinek, Krzysztof\n  Wo{\\l}k, Ryszard Gubrynowicz", "title": "Spoken Language Translation for Polish", "comments": "Marasek K., Wo{\\l}k K., Korzinek D., Brocki {\\L}., Spoken Language\n  Translation for Polish, Proceedings of Forum Acuscticum 2014, Krak\\'ow. arXiv\n  admin note: substantial text overlap with arXiv:1509.08909", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language translation (SLT) is becoming more important in the\nincreasingly globalized world, both from a social and economic point of view.\nIt is one of the major challenges for automatic speech recognition (ASR) and\nmachine translation (MT), driving intense research activities in these areas.\nWhile past research in SLT, due to technology limitations, dealt mostly with\nspeech recorded under controlled conditions, today's major challenge is the\ntranslation of spoken language as it can be found in real life. Considered\napplication scenarios range from portable translators for tourists, lectures\nand presentations translation, to broadcast news and shows with live\ncaptioning. We would like to present PJIIT's experiences in the SLT gained from\nthe Eu-Bridge 7th framework project and the U-Star consortium activities for\nthe Polish/English language pair. Presented research concentrates on ASR\nadaptation for Polish (state-of-the-art acoustic models: DBN-BLSTM training,\nKaldi: LDA+MLLT+SAT+MMI), language modeling for ASR & MT (text normalization,\nRNN-based LMs, n-gram model domain interpolation) and statistical translation\ntechniques (hierarchical models, factored translation models, automatic casing\nand punctuation, comparable and bilingual corpora preparation). While results\nfor the well-defined domains (phrases for travelers, parliament speeches,\nmedical documentation, movie subtitling) are very encouraging, less defined\ndomains (presentation, lectures) still form a challenge. Our progress in the\nIWSLT TED task (MT only) will be presented, as well as current progress in the\nPolish ASR.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 16:28:16 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Marasek", "Krzysztof", ""], ["Brocki", "\u0141ukasz", ""], ["Korzinek", "Danijel", ""], ["Wo\u0142k", "Krzysztof", ""], ["Gubrynowicz", "Ryszard", ""]]}, {"id": "1511.07916", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Natural Language Understanding with Distributed Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a lecture note for the course DS-GA 3001 <Natural Language\nUnderstanding with Distributed Representation> at the Center for Data Science ,\nNew York University in Fall, 2015. As the name of the course suggests, this\nlecture note introduces readers to a neural network based approach to natural\nlanguage understanding/processing. In order to make it as self-contained as\npossible, I spend much time on describing basics of machine learning and neural\nnetworks, only after which how they are used for natural languages is\nintroduced. On the language front, I almost solely focus on language modelling\nand machine translation, two of which I personally find most fascinating and\nmost fundamental to natural language understanding.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 23:23:13 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1511.07972", "submitter": "Volker Tresp", "authors": "Volker Tresp and Crist\\'obal Esteban and Yinchong Yang and Stephan\n  Baier and Denis Krompa{\\ss}", "title": "Learning with Memory Embeddings", "comments": "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale\n  Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding learning, a.k.a. representation learning, has been shown to be able\nto model large-scale semantic knowledge graphs. A key concept is a mapping of\nthe knowledge graph to a tensor representation whose entries are predicted by\nmodels using latent representations of generalized entities. Latent variable\nmodels are well suited to deal with the high dimensionality and sparsity of\ntypical knowledge graphs. In recent publications the embedding models were\nextended to also consider time evolutions, time patterns and subsymbolic\nrepresentations. In this paper we map embedding models, which were developed\npurely as solutions to technical problems for modelling temporal knowledge\ngraphs, to various cognitive memory functions, in particular to semantic and\nconcept memory, episodic memory, sensory memory, short-term memory, and working\nmemory. We discuss learning, query answering, the path from sensory input to\nsemantic decoding, and the relationship between episodic memory and semantic\nmemory. We introduce a number of hypotheses on human memory that can be derived\nfrom the developed mathematical models.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 07:06:09 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 05:53:38 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 23:38:03 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 22:35:39 GMT"}, {"version": "v5", "created": "Mon, 25 Jan 2016 20:02:39 GMT"}, {"version": "v6", "created": "Wed, 13 Apr 2016 17:23:42 GMT"}, {"version": "v7", "created": "Thu, 21 Apr 2016 04:40:58 GMT"}, {"version": "v8", "created": "Thu, 5 May 2016 14:57:41 GMT"}, {"version": "v9", "created": "Sat, 7 May 2016 09:06:15 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Tresp", "Volker", ""], ["Esteban", "Crist\u00f3bal", ""], ["Yang", "Yinchong", ""], ["Baier", "Stephan", ""], ["Krompa\u00df", "Denis", ""]]}, {"id": "1511.08130", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Armand Joulin, Marco Baroni", "title": "A Roadmap towards Machine Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of intelligent machines is one of the biggest unsolved\nchallenges in computer science. In this paper, we propose some fundamental\nproperties these machines should have, focusing in particular on communication\nand learning. We discuss a simple environment that could be used to\nincrementally teach a machine the basics of natural-language-based\ncommunication, as a prerequisite to more complex interaction with human users.\nWe also present some conjectures on the sort of algorithms the machine should\nsupport in order to profitably learn from the environment.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 17:32:18 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 20:03:43 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Mikolov", "Tomas", ""], ["Joulin", "Armand", ""], ["Baroni", "Marco", ""]]}, {"id": "1511.08198", "submitter": "John Wieting", "authors": "John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu", "title": "Towards Universal Paraphrastic Sentence Embeddings", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings based on supervision from the Paraphrase Database (Ganitkevitch et\nal., 2013). We compare six compositional architectures, evaluating them on\nannotated textual similarity datasets drawn both from the same distribution as\nthe training data and from a wide range of other domains. We find that the most\ncomplex architectures, such as long short-term memory (LSTM) recurrent neural\nnetworks, perform best on the in-domain data. However, in out-of-domain\nscenarios, simple architectures such as word averaging vastly outperform LSTMs.\nOur simplest averaging model is even competitive with systems tuned for the\nparticular tasks while also being extremely efficient and easy to use.\n  In order to better understand how these architectures compare, we conduct\nfurther experiments on three supervised NLP tasks: sentence similarity,\nentailment, and sentiment classification. We again find that the word averaging\nmodels perform well for sentence similarity and entailment, outperforming\nLSTMs. However, on sentiment classification, we find that the LSTM performs\nvery strongly-even recording new state-of-the-art performance on the Stanford\nSentiment Treebank.\n  We then demonstrate how to combine our pretrained sentence embeddings with\nthese supervised tasks, using them both as a prior and as a black box feature\nextractor. This leads to performance rivaling the state of the art on the SICK\nsimilarity and entailment tasks. We release all of our resources to the\nresearch community with the hope that they can serve as the new baseline for\nfurther work on universal sentence embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 20:52:15 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 20:59:39 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 20:54:30 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Wieting", "John", ""], ["Bansal", "Mohit", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1511.08277", "submitter": "Shengxian Wan", "authors": "Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi\n  Cheng", "title": "A Deep Architecture for Semantic Matching with Multiple Positional\n  Sentence Representations", "comments": "Accepted by AAAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching natural language sentences is central for many applications such as\ninformation retrieval and question answering. Existing deep models rely on a\nsingle sentence representation or multiple granularity representations for\nmatching. However, such methods cannot well capture the contextualized local\ninformation in the matching process. To tackle this problem, we present a new\ndeep architecture to match two sentences with multiple positional sentence\nrepresentations. Specifically, each positional sentence representation is a\nsentence representation at this position, generated by a bidirectional long\nshort term memory (Bi-LSTM). The matching score is finally produced by\naggregating interactions between these different positional sentence\nrepresentations, through $k$-Max pooling and a multi-layer perceptron. Our\nmodel has several advantages: (1) By using Bi-LSTM, rich context of the whole\nsentence is leveraged to capture the contextualized local information in each\npositional sentence representation; (2) By matching with multiple positional\nsentence representations, it is flexible to aggregate different important\ncontextualized local information in a sentence to support the matching; (3)\nExperiments on different tasks such as question answering and sentence\ncompletion demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:57:54 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Wan", "Shengxian", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Pang", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1511.08299", "submitter": "Aditya Jami", "authors": "Matthew Long, Aditya Jami, Ashutosh Saxena", "title": "Hierarchical classification of e-commerce related social media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to classify tweets into root categories of the\nAmazon browse node hierarchy using a set of tweets with browse node ID labels,\na much larger set of tweets without labels, and a set of Amazon reviews.\nExamining twitter data presents unique challenges in that the samples are short\n(under 140 characters) and often contain misspellings or abbreviations that are\ntrivial for a human to decipher but difficult for a computer to parse. A\nvariety of query and document expansion techniques are implemented in an effort\nto improve information retrieval to modest success.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 06:57:06 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Long", "Matthew", ""], ["Jami", "Aditya", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1511.08308", "submitter": "Eric Nichols", "authors": "Jason P.C. Chiu and Eric Nichols", "title": "Named Entity Recognition with Bidirectional LSTM-CNNs", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition is a challenging task that has traditionally\nrequired large amounts of knowledge in the form of feature engineering and\nlexicons to achieve high performance. In this paper, we present a novel neural\nnetwork architecture that automatically detects word- and character-level\nfeatures using a hybrid bidirectional LSTM and CNN architecture, eliminating\nthe need for most feature engineering. We also propose a novel method of\nencoding partial lexicon matches in neural networks and compare it to existing\napproaches. Extensive evaluation shows that, given only tokenized text and\npublicly available word embeddings, our system is competitive on the CoNLL-2003\ndataset and surpasses the previously reported state of the art performance on\nthe OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed\nfrom publicly-available sources, we establish new state of the art performance\nwith an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing\nsystems that employ heavy feature engineering, proprietary lexicons, and rich\nentity linking information.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 07:40:33 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 09:23:52 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 06:25:57 GMT"}, {"version": "v4", "created": "Thu, 16 Jun 2016 06:15:49 GMT"}, {"version": "v5", "created": "Tue, 19 Jul 2016 05:02:51 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Chiu", "Jason P. C.", ""], ["Nichols", "Eric", ""]]}, {"id": "1511.08400", "submitter": "David Krueger", "authors": "David Krueger, Roland Memisevic", "title": "Regularizing RNNs by Stabilizing Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We stabilize the activations of Recurrent Neural Networks (RNNs) by\npenalizing the squared distance between successive hidden states' norms.\n  This penalty term is an effective regularizer for RNNs including LSTMs and\nIRNNs, improving performance on character-level language modeling and phoneme\nrecognition, and outperforming weight noise and dropout.\n  We achieve competitive performance (18.6\\% PER) on the TIMIT phoneme\nrecognition task for RNNs evaluated without beam search or an RNN transducer.\n  With this penalty term, IRNN can achieve similar performance to LSTM on\nlanguage modeling, although adding the penalty term to the LSTM results in\nsuperior performance.\n  Our penalty term also prevents the exponential growth of IRNN's activations\noutside of their training horizon, allowing them to generalize to much longer\nsequences.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:35:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 04:52:03 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2015 02:09:00 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 00:58:39 GMT"}, {"version": "v5", "created": "Fri, 5 Feb 2016 04:58:47 GMT"}, {"version": "v6", "created": "Wed, 2 Mar 2016 20:42:08 GMT"}, {"version": "v7", "created": "Tue, 26 Apr 2016 05:21:11 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Krueger", "David", ""], ["Memisevic", "Roland", ""]]}, {"id": "1511.08407", "submitter": "Ran Tian", "authors": "Ran Tian, Naoaki Okazaki, Kentaro Inui", "title": "The Mechanism of Additive Composition", "comments": "More explanations on theory and additional experiments added.\n  Accepted by Machine Learning Journal", "journal-ref": null, "doi": "10.1007/s10994-017-5634-8", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive composition (Foltz et al, 1998; Landauer and Dumais, 1997; Mitchell\nand Lapata, 2010) is a widely used method for computing meanings of phrases,\nwhich takes the average of vector representations of the constituent words. In\nthis article, we prove an upper bound for the bias of additive composition,\nwhich is the first theoretical analysis on compositional frameworks from a\nmachine learning point of view. The bound is written in terms of collocation\nstrength; we prove that the more exclusively two successive words tend to occur\ntogether, the more accurate one can guarantee their additive composition as an\napproximation to the natural phrase vector. Our proof relies on properties of\nnatural language data that are empirically verified, and can be theoretically\nderived from an assumption that the data is generated from a Hierarchical\nPitman-Yor Process. The theory endorses additive composition as a reasonable\noperation for calculating meanings of phrases, and suggests ways to improve\nadditive compositionality, including: transforming entries of distributional\nword vectors by a function that meets a specific condition, constructing a\nnovel type of vector representations to make additive composition sensitive to\nword order, and utilizing singular value decomposition to train word vectors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:58:17 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 23:34:40 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 04:28:21 GMT"}, {"version": "v4", "created": "Tue, 7 Mar 2017 02:39:58 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Tian", "Ran", ""], ["Okazaki", "Naoaki", ""], ["Inui", "Kentaro", ""]]}, {"id": "1511.08411", "submitter": "Mostafa Bayomi", "authors": "Mostafa Bayomi, Killian Levacher, M. Rami Ghorab, S\\'eamus Lawless", "title": "OntoSeg: a Novel Approach to Text Segmentation using Ontological\n  Similarity", "comments": "10 pages, IEEE ICDMW 2015 (SENTIRE Workshop)", "journal-ref": null, "doi": "10.1109/ICDMW.2015.6", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text segmentation (TS) aims at dividing long text into coherent segments\nwhich reflect the subtopic structure of the text. It is beneficial to many\nnatural language processing tasks, such as Information Retrieval (IR) and\ndocument summarisation. Current approaches to text segmentation are similar in\nthat they all use word-frequency metrics to measure the similarity between two\nregions of text, so that a document is segmented based on the lexical cohesion\nbetween its words. Various NLP tasks are now moving towards the semantic web\nand ontologies, such as ontology-based IR systems, to capture the\nconceptualizations associated with user needs and contents. Text segmentation\nbased on lexical cohesion between words is hence not sufficient anymore for\nsuch tasks. This paper proposes OntoSeg, a novel approach to text segmentation\nbased on the ontological similarity between text blocks. The proposed method\nuses ontological similarity to explore conceptual relations between text\nsegments and a Hierarchical Agglomerative Clustering (HAC) algorithm to\nrepresent the text as a tree-like hierarchy that is conceptually structured.\nThe rich structure of the created tree further allows the segmentation of text\nin a linear fashion at various levels of granularity. The proposed method was\nevaluated on a wellknown dataset, and the results show that using ontological\nsimilarity in text segmentation is very promising. Also we enhance the proposed\nmethod by combining ontological similarity with lexical similarity and the\nresults show an enhancement of the segmentation quality.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:10:18 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Bayomi", "Mostafa", ""], ["Levacher", "Killian", ""], ["Ghorab", "M. Rami", ""], ["Lawless", "S\u00e9amus", ""]]}, {"id": "1511.08417", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Chengyao Chen, Wenjie Li, Sujian Li, Furu Wei, Ming Zhou", "title": "TGSum: Build Tweet Guided Multi-Document Summarization Dataset", "comments": "7 pages, 1 figure in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of summarization research has been significantly hampered by\nthe costly acquisition of reference summaries. This paper proposes an effective\nway to automatically collect large scales of news-related multi-document\nsummaries with reference to social media's reactions. We utilize two types of\nsocial labels in tweets, i.e., hashtags and hyper-links. Hashtags are used to\ncluster documents into different topic sets. Also, a tweet with a hyper-link\noften highlights certain key points of the corresponding document. We\nsynthesize a linked document cluster to form a reference summary which can\ncover most key points. To this aim, we adopt the ROUGE metrics to measure the\ncoverage ratio, and develop an Integer Linear Programming solution to discover\nthe sentence set reaching the upper bound of ROUGE. Since we allow summary\nsentences to be selected from both documents and high-quality tweets, the\ngenerated reference summaries could be abstractive. Both informativeness and\nreadability of the collected summaries are verified by manual judgment. In\naddition, we train a Support Vector Regression summarizer on DUC generic\nmulti-document summarization benchmarks. With the collected data as extra\ntraining resource, the performance of the summarizer improves a lot on all the\ntest sets. We release this dataset for further research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:22:54 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Cao", "Ziqiang", ""], ["Chen", "Chengyao", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "1511.08629", "submitter": "Chunting Zhou", "authors": "Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C.M. Lau", "title": "Category Enhanced Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed word representations have been demonstrated to be effective in\ncapturing semantic and syntactic regularities. Unsupervised representation\nlearning from large unlabeled corpora can learn similar representations for\nthose words that present similar co-occurrence statistics. Besides local\noccurrence statistics, global topical information is also important knowledge\nthat may help discriminate a word from another. In this paper, we incorporate\ncategory information of documents in the learning of word representations and\nto learn the proposed models in a document-wise manner. Our models outperform\nseveral state-of-the-art models in word analogy and word similarity tasks.\nMoreover, we evaluate the learned word vectors on sentiment analysis and text\nclassification tasks, which shows the superiority of our learned word vectors.\nWe also learn high-quality category embeddings that reflect topical meanings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 11:38:57 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 07:33:09 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhou", "Chunting", ""], ["Sun", "Chonglin", ""], ["Liu", "Zhiyuan", ""], ["Lau", "Francis C. M.", ""]]}, {"id": "1511.08630", "submitter": "Chunting Zhou", "authors": "Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C.M. Lau", "title": "A C-LSTM Neural Network for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models have been demonstrated to be capable of achieving\nremarkable performance in sentence and document modeling. Convolutional neural\nnetwork (CNN) and recurrent neural network (RNN) are two mainstream\narchitectures for such modeling tasks, which adopt totally different ways of\nunderstanding natural languages. In this work, we combine the strengths of both\narchitectures and propose a novel and unified model called C-LSTM for sentence\nrepresentation and text classification. C-LSTM utilizes CNN to extract a\nsequence of higher-level phrase representations, and are fed into a long\nshort-term memory recurrent neural network (LSTM) to obtain the sentence\nrepresentation. C-LSTM is able to capture both local features of phrases as\nwell as global and temporal sentence semantics. We evaluate the proposed\narchitecture on sentiment classification and question classification tasks. The\nexperimental results show that the C-LSTM outperforms both CNN and LSTM and can\nachieve excellent performance on these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 11:44:17 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 07:20:46 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhou", "Chunting", ""], ["Sun", "Chonglin", ""], ["Liu", "Zhiyuan", ""], ["Lau", "Francis C. M.", ""]]}, {"id": "1511.08855", "submitter": "Francisco De Sousa Webber", "authors": "Francisco De Sousa Webber", "title": "Semantic Folding Theory And its Application in Semantic Fingerprinting", "comments": "59 pages, white paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language is recognized as a very complex domain since decades. No\ncomputer system has been able to reach human levels of performance so far. The\nonly known computational system capable of proper language processing is the\nhuman brain. While we gather more and more data about the brain, its\nfundamental computational processes still remain obscure. The lack of a sound\ncomputational brain theory also prevents the fundamental understanding of\nNatural Language Processing. As always when science lacks a theoretical\nfoundation, statistical modeling is applied to accommodate as many sampled\nreal-world data as possible. An unsolved fundamental issue is the actual\nrepresentation of language (data) within the brain, denoted as the\nRepresentational Problem. Starting with Jeff Hawkins' Hierarchical Temporal\nMemory (HTM) theory, a consistent computational theory of the human cortex, we\nhave developed a corresponding theory of language data representation: The\nSemantic Folding Theory. The process of encoding words, by using a topographic\nsemantic space as distributional reference frame into a sparse binary\nrepresentational vector is called Semantic Folding and is the central topic of\nthis document. Semantic Folding describes a method of converting language from\nits symbolic representation (text) into an explicit, semantically grounded\nrepresentation that can be generically processed by Hawkins' HTM networks. As\nit turned out, this change in representation, by itself, can solve many complex\nNLP problems by applying Boolean operators and a generic similarity function\nlike the Euclidian Distance. Many practical problems of statistical NLP\nsystems, like the high cost of computation, the fundamental incongruity of\nprecision and recall , the complex tuning procedures etc., can be elegantly\novercome by applying Semantic Folding.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 00:13:09 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 22:04:51 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Webber", "Francisco De Sousa", ""]]}, {"id": "1511.08952", "submitter": "Ndapa Nakashole", "authors": "Ndapandula Nakashole", "title": "Bootstrapping Ternary Relation Extractors", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary relation extraction methods have been widely studied in recent years.\nHowever, few methods have been developed for higher n-ary relation extraction.\nOne limiting factor is the effort required to generate training data. For\nbinary relations, one only has to provide a few dozen pairs of entities per\nrelation, as training data. For ternary relations (n=3), each training instance\nis a triplet of entities, placing a greater cognitive load on people. For\nexample, many people know that Google acquired Youtube but not the dollar\namount or the date of the acquisition and many people know that Hillary Clinton\nis married to Bill Clinton by not the location or date of their wedding. This\nmakes higher n-nary training data generation a time consuming exercise in\nsearching the Web. We present a resource for training ternary relation\nextractors. This was generated using a minimally supervised yet effective\napproach. We present statistics on the size and the quality of the dataset.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 00:49:13 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 02:52:34 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Nakashole", "Ndapandula", ""]]}, {"id": "1511.09107", "submitter": "K. Ch. Chatzisavvas", "authors": "Panagiotis Stalidis, Maria Giatsoglou, Konstantinos Diamantaras,\n  George Sarigiannidis, Konstantinos Ch. Chatzisavvas", "title": "Machine Learning Sentiment Prediction based on Hybrid Document\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sentiment analysis and opinion mining is a complex process\nconcerning the extraction of useful subjective information from text. The\nexplosion of user generated content on the Web, especially the fact that\nmillions of users, on a daily basis, express their opinions on products and\nservices to blogs, wikis, social networks, message boards, etc., render the\nreliable, automated export of sentiments and opinions from unstructured text\ncrucial for several commercial applications. In this paper, we present a novel\nhybrid vectorization approach for textual resources that combines a weighted\nvariant of the popular Word2Vec representation (based on Term Frequency-Inverse\nDocument Frequency) representation and with a Bag- of-Words representation and\na vector of lexicon-based sentiment values. The proposed text representation\napproach is assessed through the application of several machine learning\nclassification algorithms on a dataset that is used extensively in literature\nfor sentiment detection. The classification accuracy derived through the\nproposed hybrid vectorization approach is higher than when its individual\ncomponents are used for text represenation, and comparable with\nstate-of-the-art sentiment detection methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 22:41:43 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Stalidis", "Panagiotis", ""], ["Giatsoglou", "Maria", ""], ["Diamantaras", "Konstantinos", ""], ["Sarigiannidis", "George", ""], ["Chatzisavvas", "Konstantinos Ch.", ""]]}, {"id": "1511.09128", "submitter": "Haibing Wu", "authors": "Haibing Wu, Yiwei Gu, Shangdi Sun and Xiaodong Gu", "title": "Aspect-based Opinion Summarization with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers Aspect-based Opinion Summarization (AOS) of reviews on\nparticular products. To enable real applications, an AOS system needs to\naddress two core subtasks, aspect extraction and sentiment classification. Most\nexisting approaches to aspect extraction, which use linguistic analysis or\ntopic modeling, are general across different products but not precise enough or\nsuitable for particular products. Instead we take a less general but more\nprecise scheme, directly mapping each review sentence into pre-defined aspects.\nTo tackle aspect mapping and sentiment classification, we propose two\nConvolutional Neural Network (CNN) based methods, cascaded CNN and multitask\nCNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs\nat level 1 deal with aspect mapping task, and a single CNN at level 2 deals\nwith sentiment classification. Multitask CNN also contains multiple aspect CNNs\nand a sentiment CNN, but different networks share the same word embeddings.\nExperimental results indicate that both cascaded and multitask CNNs outperform\nSVM-based methods by large margins. Multitask CNN generally performs better\nthan cascaded CNN.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 01:46:15 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Yiwei", ""], ["Sun", "Shangdi", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1511.09173", "submitter": "Tingshao Zhu", "authors": "Aiqi Zhang, Ang Li and Tingshao Zhu", "title": "Recognizing Temporal Linguistic Expression Pattern of Individual with\n  Suicide Risk on Social Media", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suicide is a global public health problem. Early detection of individual\nsuicide risk plays a key role in suicide prevention. In this paper, we propose\nto look into individual suicide risk through time series analysis of personal\nlinguistic expression on social media (Weibo). We examined temporal patterns of\nthe linguistic expression of individuals on Chinese social media (Weibo). Then,\nwe used such temporal patterns as predictor variables to build classification\nmodels for estimating levels of individual suicide risk. Characteristics of\ntime sequence curves to linguistic features including parentheses, auxiliary\nverbs, personal pronouns and body words are reported to affect performance of\nsuicide most, and the predicting model has a accuracy higher than 0.60, shown\nby the results. This paper confirms the efficiency of the social media data in\ndetecting individual suicide risk. Results of this study may be insightful for\nimproving the performance of suicide prevention programs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 06:15:31 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhang", "Aiqi", ""], ["Li", "Ang", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1511.09376", "submitter": "Snigdha Chaturvedi", "authors": "Snigdha Chaturvedi, Shashank Srivastava, Hal Daume III and Chris Dyer", "title": "Modeling Dynamic Relationships Between Characters in Literary Novels", "comments": "9 pages, 1 figure. Accepted at AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying characters plays a vital role in computationally representing and\ninterpreting narratives. Unlike previous work, which has focused on inferring\ncharacter roles, we focus on the problem of modeling their relationships.\nRather than assuming a fixed relationship for a character pair, we hypothesize\nthat relationships are dynamic and temporally evolve with the progress of the\nnarrative, and formulate the problem of relationship modeling as a structured\nprediction problem. We propose a semi-supervised framework to learn\nrelationship sequences from fully as well as partially labeled data. We present\na Markovian model capable of accumulating historical beliefs about the\nrelationship and status changes. We use a set of rich linguistic and\nsemantically motivated features that incorporate world knowledge to investigate\nthe textual content of narrative. We empirically demonstrate that such a\nframework outperforms competitive baselines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 16:32:58 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Chaturvedi", "Snigdha", ""], ["Srivastava", "Shashank", ""], ["Daume", "Hal", "III"], ["Dyer", "Chris", ""]]}, {"id": "1511.09392", "submitter": "Krzysztof Wo{\\l}k", "authors": "Agnieszka Wo{\\l}k, Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Enhancements in statistical spoken language translation by\n  de-normalization of ASR results", "comments": "International Academy Publishing. arXiv admin note: text overlap with\n  arXiv:1510.04500", "journal-ref": "Journal of Computers, 2016 VOL 11, ISSN: 1796-203X, p. 33-40, 2016", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language translation (SLT) has become very important in an\nincreasingly globalized world. Machine translation (MT) for automatic speech\nrecognition (ASR) systems is a major challenge of great interest. This research\ninvestigates that automatic sentence segmentation of speech that is important\nfor enriching speech recognition output and for aiding downstream language\nprocessing. This article focuses on the automatic sentence segmentation of\nspeech and improving MT results. We explore the problem of identifying sentence\nboundaries in the transcriptions produced by automatic speech recognition\nsystems in the Polish language. We also experiment with reverse normalization\nof the recognized speech samples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:34:21 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wo\u0142k", "Agnieszka", ""], ["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1511.09460", "submitter": "Snigdha Chaturvedi", "authors": "Snigdha Chaturvedi, Dan Goldwasser, Hal Daume III", "title": "Ask, and shall you receive?: Understanding Desire Fulfillment in Natural\n  Language Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to comprehend wishes or desires and their fulfillment is\nimportant to Natural Language Understanding. This paper introduces the task of\nidentifying if a desire expressed by a subject in a given short piece of text\nwas fulfilled. We propose various unstructured and structured models that\ncapture fulfillment cues such as the subject's emotional state and actions. Our\nexperiments with two different datasets demonstrate the importance of\nunderstanding the narrative and discourse structure to address this task.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 20:37:03 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Chaturvedi", "Snigdha", ""], ["Goldwasser", "Dan", ""], ["Daume", "Hal", "III"]]}]