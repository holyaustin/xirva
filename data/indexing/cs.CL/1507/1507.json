[{"id": "1507.00133", "submitter": "Simone Faro", "authors": "Valeria Borz\\`i, Simone Faro, Arianna Pavone and Sabrina Sansone", "title": "Prior Polarity Lexical Resources for the Italian Language", "comments": "10 pages, Accepted to NLPCS 2015, the 12th International Workshop on\n  Natural Language Processing and Cognitive Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for\nItalian Natural language Applications) a manually annotated prior polarity\nlexical resource for Italian natural language applications in the field of\nopinion mining and sentiment induction. The resource consists in two different\nsets, an Italian dictionary of more than 277.000 words tagged with their prior\npolarity value, and a set of polarity modifiers, containing more than 200\nwords, which can be used in combination with non neutral terms of the\ndictionary in order to induce the sentiment of Italian compound terms. To the\nbest of our knowledge this is the first prior polarity manually annotated\nresource which has been developed for the Italian natural language.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 07:29:12 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Borz\u00ec", "Valeria", ""], ["Faro", "Simone", ""], ["Pavone", "Arianna", ""], ["Sansone", "Sabrina", ""]]}, {"id": "1507.00209", "submitter": "Hai Zhuge Mr", "authors": "Hai Zhuge", "title": "Dimensionality on Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization is one of the key features of human intelligence. It plays an\nimportant role in understanding and representation. With rapid and continual\nexpansion of texts, pictures and videos in cyberspace, automatic summarization\nbecomes more and more desirable. Text summarization has been studied for over\nhalf century, but it is still hard to automatically generate a satisfied\nsummary. Traditional methods process texts empirically and neglect the\nfundamental characteristics and principles of language use and understanding.\nThis paper summarizes previous text summarization approaches in a\nmulti-dimensional classification space, introduces a multi-dimensional\nmethodology for research and development, unveils the basic characteristics and\nprinciples of language use and understanding, investigates some fundamental\nmechanisms of summarization, studies the dimensions and forms of\nrepresentations, and proposes a multi-dimensional evaluation mechanisms.\nInvestigation extends to the incorporation of pictures into summary and to the\nsummarization of videos, graphs and pictures, and then reaches a general\nsummarization framework.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 12:39:50 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Zhuge", "Hai", ""]]}, {"id": "1507.00639", "submitter": "Daoud Clarke", "authors": "Daoud Clarke", "title": "Simple, Fast Semantic Parsing with a Tensor Kernel", "comments": "in CICLing 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple approach to semantic parsing based on a tensor product\nkernel. We extract two feature vectors: one for the query and one for each\ncandidate logical form. We then train a classifier using the tensor product of\nthe two vectors. Using very simple features for both, our system achieves an\naverage F1 score of 40.1% on the WebQuestions dataset. This is comparable to\nmore complex systems but is simpler to implement and runs faster.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 15:58:25 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Clarke", "Daoud", ""]]}, {"id": "1507.00955", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste", "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination", "comments": "32 pages, 5 figures", "journal-ref": "Handbook of Sentiment Analysis in Finance. Mitra, G. and Yu, X.\n  (Eds.). (2016). ISBN 1910571571", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:46:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 17:24:18 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 11:44:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Tharsis T. P.", ""], ["Treleaven", "Philip", ""], ["Aste", "Tomaso", ""]]}, {"id": "1507.01053", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho, Aaron Courville, Yoshua Bengio", "title": "Describing Multimedia Content using Attention-based Encoder--Decoder\n  Networks", "comments": "Submitted to IEEE Transactions on Multimedia Special Issue on Deep\n  Learning for Multimedia Computing", "journal-ref": null, "doi": "10.1109/TMM.2015.2477044", "report-no": null, "categories": "cs.NE cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas deep neural networks were first mostly used for classification tasks,\nthey are rapidly expanding in the realm of structured output problems, where\nthe observed target is composed of multiple random variables that have a rich\njoint distribution, given the input. We focus in this paper on the case where\nthe input also has a rich structure and the input and output structures are\nsomehow related. We describe systems that learn to attend to different places\nin the input, for each element of the output, for a variety of tasks: machine\ntranslation, image caption generation, video clip description and speech\nrecognition. All these systems are based on a shared set of building blocks:\ngated recurrent neural networks and convolutional neural networks, along with\ntrained attention mechanisms. We report on experimental results with these\nsystems, showing impressively good performance and the advantage of the\nattention mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 01:06:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cho", "Kyunghyun", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.01127", "submitter": "Sascha Rothe", "authors": "Sascha Rothe and Hinrich Sch\\\"utze", "title": "AutoExtend: Extending Word Embeddings to Embeddings for Synsets and\n  Lexemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\textit{AutoExtend}, a system to learn embeddings for synsets and\nlexemes. It is flexible in that it can take any word embeddings as input and\ndoes not need an additional training corpus. The synset/lexeme embeddings\nobtained live in the same vector space as the word embeddings. A sparse tensor\nformalization guarantees efficiency and parallelizability. We use WordNet as a\nlexical resource, but AutoExtend can be easily applied to other resources like\nFreebase. AutoExtend achieves state-of-the-art performance on word similarity\nand word sense disambiguation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2015 16:59:30 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Rothe", "Sascha", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1507.01193", "submitter": "Piotr Mirowski", "authors": "Piotr Mirowski, Andreas Vlachos", "title": "Dependency Recurrent Neural Language Models for Sentence Completion", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on language modelling has shifted focus from count-based models\nto neural models. In these works, the words in each sentence are always\nconsidered in a left-to-right order. In this paper we show how we can improve\nthe performance of the recurrent neural network (RNN) language model by\nincorporating the syntactic dependencies of a sentence, which have the effect\nof bringing relevant contexts closer to the word being predicted. We evaluate\nour approach on the Microsoft Research Sentence Completion Challenge and show\nthat the dependency RNN proposed improves over the RNN by about 10 points in\naccuracy. Furthermore, we achieve results comparable with the state-of-the-art\nmodels on this task.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 11:10:24 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Mirowski", "Piotr", ""], ["Vlachos", "Andreas", ""]]}, {"id": "1507.01526", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Ivo Danihelka, Alex Graves", "title": "Grid Long Short-Term Memory", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Grid Long Short-Term Memory, a network of LSTM cells\narranged in a multidimensional grid that can be applied to vectors, sequences\nor higher dimensional data such as images. The network differs from existing\ndeep LSTM architectures in that the cells are connected between network layers\nas well as along the spatiotemporal dimensions of the data. The network\nprovides a unified way of using LSTM for both deep and sequential computation.\nWe apply the model to algorithmic tasks such as 15-digit integer addition and\nsequence memorization, where it is able to significantly outperform the\nstandard LSTM. We then give results for two empirical tasks. We find that 2D\nGrid LSTM achieves 1.47 bits per character on the Wikipedia character\nprediction benchmark, which is state-of-the-art among neural approaches. In\naddition, we use the Grid LSTM to define a novel two-dimensional translation\nmodel, the Reencoder, and show that it outperforms a phrase-based reference\nsystem on a Chinese-to-English translation task.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 16:30:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 17:40:17 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:39:48 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Danihelka", "Ivo", ""], ["Graves", "Alex", ""]]}, {"id": "1507.01529", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Correspondence Factor Analysis of Big Data Sets: A Case Study of 30\n  Million Words; and Contrasting Analytics using Apache Solr and Correspondence\n  Analysis in R", "comments": "38 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large number of text data sets. These are cooking recipes. Term\ndistribution and other distributional properties of the data are investigated.\nOur aim is to look at various analytical approaches which allow for mining of\ninformation on both high and low detail scales. Metric space embedding is\nfundamental to our interest in the semantic properties of this data. We\nconsider the projection of all data into analyses of aggregated versions of the\ndata. We contrast that with projection of aggregated versions of the data into\nanalyses of all the data. Analogously for the term set, we look at analysis of\nselected terms. We also look at inherent term associations such as between\nsingular and plural. In addition to our use of Correspondence Analysis in R,\nfor latent semantic space mapping, we also use Apache Solr. Setting up the Solr\nserver and carrying out querying is described. A further novelty is that\nquerying is supported in Solr based on the principal factor plane mapping of\nall the data. This uses a bounding box query, based on factor projections.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 16:32:52 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1507.01636", "submitter": "Jiwei Li", "authors": "Jiwei Li and Eduard Hovy", "title": "Reflections on Sentiment/Opinion Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we described possible directions for deeper understanding,\nhelping bridge the gap between psychology / cognitive science and computational\napproaches in sentiment/opinion analysis literature. We focus on the opinion\nholder's underlying needs and their resultant goals, which, in a utilitarian\nmodel of sentiment, provides the basis for explaining the reason a sentiment\nvalence is held. While these thoughts are still immature, scattered,\nunstructured, and even imaginary, we believe that these perspectives might\nsuggest fruitful avenues for various kinds of future work.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 22:25:55 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Li", "Jiwei", ""], ["Hovy", "Eduard", ""]]}, {"id": "1507.01701", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn", "title": "A Survey and Classification of Controlled Natural Languages", "comments": null, "journal-ref": "Computational Linguistics, March 2014, Vol. 40, No. 1, pages\n  121-170", "doi": "10.1162/COLI_a_00168", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What is here called controlled natural language (CNL) has traditionally been\ngiven many different names. Especially during the last four decades, a wide\nvariety of such languages have been designed. They are applied to improve\ncommunication among humans, to improve translation, or to provide natural and\nintuitive representations for formal notations. Despite the apparent\ndifferences, it seems sensible to put all these languages under the same\numbrella. To bring order to the variety of languages, a general classification\nscheme is presented here. A comprehensive survey of existing English-based CNLs\nis given, listing and describing 100 languages from 1930 until today.\nClassification of these languages reveals that they form a single scattered\ncloud filling the conceptual space between natural languages such as English on\nthe one end and formal languages such as propositional logic on the other. The\ngoal of this article is to provide a common terminology and a common model for\nCNL, to contribute to the understanding of their general nature, to provide a\nstarting point for researchers interested in the area, and to help developers\nto make design decisions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 08:23:31 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Kuhn", "Tobias", ""]]}, {"id": "1507.01839", "submitter": "Mingbo Ma", "authors": "Mingbo Ma and Liang Huang and Bing Xiang and Bowen Zhou", "title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "comments": "this paper has been accepted by ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sentence modeling and classification, convolutional neural network\napproaches have recently achieved state-of-the-art results, but all such\nefforts process word vectors sequentially and neglect long-distance\ndependencies. To exploit both deep learning and linguistic structures, we\npropose a tree-based convolutional neural network model which exploit various\nlong-distance relationships between words. Our model improves the sequential\nbaselines on all three sentiment and question classification tasks, and\nachieves the highest published accuracy on TREC.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 15:20:36 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2015 15:36:45 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Ma", "Mingbo", ""], ["Huang", "Liang", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1507.02012", "submitter": "Akanksha Gehlot", "authors": "Akanksha Gehlot, Vaishali Sharma, Shashi Pal Singh, Ajai Kumar", "title": "Hindi to English Transfer Based Machine Translation System", "comments": "8 pages in International Journal of Advanced Computer ResearchISSN\n  (Print): 2249-7277 ISSN (Online): 2277-7970 Volume-5 Issue-19 (June-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large societies like India there is a huge demand to convert one human\nlanguage into another. Lots of work has been done in this area. Many transfer\nbased MTS have developed for English to other languages, as MANTRA CDAC Pune,\nMATRA CDAC Pune, SHAKTI IISc Bangalore and IIIT Hyderabad. Still there is a\nlittle work done for Hindi to other languages. Currently we are working on it.\nIn this paper we focus on designing a system, that translate the document from\nHindi to English by using transfer based approach. This system takes an input\ntext check its structure through parsing. Reordering rules are used to generate\nthe text in target language. It is better than Corpus Based MTS because Corpus\nBased MTS require large amount of word aligned data for translation that is not\navailable for many languages while Transfer Based MTS requires only knowledge\nof both the languages(source language and target language) to make transfer\nrules. We get correct translation for simple assertive sentences and almost\ncorrect for complex and compound sentences.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 03:50:47 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Gehlot", "Akanksha", ""], ["Sharma", "Vaishali", ""], ["Singh", "Shashi Pal", ""], ["Kumar", "Ajai", ""]]}, {"id": "1507.02020", "submitter": "Thierry Poibeau", "authors": "Thierry Poibeau (LaTTICe), Pablo Ruiz (LaTTICe)", "title": "Generating Navigable Semantic Maps from Social Sciences Corpora", "comments": "in Digital Humanities 2015, Jun 2015, Sydney, Australia. Actes de la\n  Conf{\\'e}rence Digital Humanities 2015. arXiv admin note: text overlap with\n  arXiv:1406.4211", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now commonplace to observe that we are facing a deluge of online\ninformation. Researchers have of course long acknowledged the potential value\nof this information since digital traces make it possible to directly observe,\ndescribe and analyze social facts, and above all the co-evolution of ideas and\ncommunities over time. However, most online information is expressed through\ntext, which means it is not directly usable by machines, since computers\nrequire structured, organized and typed information in order to be able to\nmanipulate it. Our goal is thus twofold: 1. Provide new natural language\nprocessing techniques aiming at automatically extracting relevant information\nfrom texts, especially in the context of social sciences, and connect these\npieces of information so as to obtain relevant socio-semantic networks; 2.\nProvide new ways of exploring these socio-semantic networks, thanks to tools\nallowing one to dynamically navigate these networks, de-construct and\nre-construct them interactively, from different points of view following the\nneeds expressed by domain experts.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:27:48 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Poibeau", "Thierry", "", "LaTTICe"], ["Ruiz", "Pablo", "", "LaTTICe"]]}, {"id": "1507.02045", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Mari Ostendorf", "title": "What Your Username Says About You", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usernames are ubiquitous on the Internet, and they are often suggestive of\nuser demographics. This work looks at the degree to which gender and language\ncan be inferred from a username alone by making use of unsupervised morphology\ninduction to decompose usernames into sub-units. Experimental results on the\ntwo tasks demonstrate the effectiveness of the proposed morphological features\ncompared to a character n-gram baseline.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 06:52:50 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2015 18:06:17 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Jaech", "Aaron", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1507.02062", "submitter": "Xiaojun Wan", "authors": "Xiaojun Wan, Ziqiang Cao, Furu Wei, Sujian Li and Ming Zhou", "title": "Multi-Document Summarization via Discriminative Summary Reranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-document summarization systems usually rely on a specific\nsummarization model (i.e., a summarization method with a specific parameter\nsetting) to extract summaries for different document sets with different\ntopics. However, according to our quantitative analysis, none of the existing\nsummarization models can always produce high-quality summaries for different\ndocument sets, and even a summarization model with good overall performance may\nproduce low-quality summaries for some document sets. On the contrary, a\nbaseline summarization model may produce high-quality summaries for some\ndocument sets. Based on the above observations, we treat the summaries produced\nby different summarization models as candidate summaries, and then explore\ndiscriminative reranking techniques to identify high-quality summaries from the\ncandidates for difference document sets. We propose to extract a set of\ncandidate summaries for each document set based on an ILP framework, and then\nleverage Ranking SVM for summary reranking. Various useful features have been\ndeveloped for the reranking process, including word-level features,\nsentence-level features and summary-level features. Evaluation results on the\nbenchmark DUC datasets validate the efficacy and robustness of our proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 08:26:23 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Wan", "Xiaojun", ""], ["Cao", "Ziqiang", ""], ["Wei", "Furu", ""], ["Li", "Sujian", ""], ["Zhou", "Ming", ""]]}, {"id": "1507.02086", "submitter": "Shashishekar Ramakrishna", "authors": "Shashishekar Ramakrishna and Lukasz Gorski and Adrian Paschke", "title": "The Role of Pragmatics in Legal Norm Representation", "comments": "International Workshop On Legal Domain And Semantic Web Applications\n  (LeDA-SWAn 2015), held during the 12th Extended Semantic Web Conference (ESWC\n  2015), June 1, 2015, Portoroz, Slovenia. in CEUR Workshop Proceedings 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the 'apparent clarity' of a given legal provision, its application\nmay result in an outcome that does not exactly conform to the semantic level of\na statute. The vagueness within a legal text is induced intentionally to\naccommodate all possible scenarios under which such norms should be applied,\nthus making the role of pragmatics an important aspect also in the\nrepresentation of a legal norm and reasoning on top of it. The notion of\npragmatics considered in this paper does not focus on the aspects associated\nwith judicial decision making. The paper aims to shed light on the aspects of\npragmatics in legal linguistics, mainly focusing on the domain of patent law,\nonly from a knowledge representation perspective. The philosophical discussions\npresented in this paper are grounded based on the legal theories from Grice and\nMarmor.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 10:04:14 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Ramakrishna", "Shashishekar", ""], ["Gorski", "Lukasz", ""], ["Paschke", "Adrian", ""]]}, {"id": "1507.02140", "submitter": "Xiaojun Wan", "authors": "Yue Hu and Xiaojun Wan", "title": "Mining and Analyzing the Future Works in Scientific Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future works in scientific articles are valuable for researchers and they can\nguide researchers to new research directions or ideas. In this paper, we mine\nthe future works in scientific articles in order to 1) provide an insight for\nfuture work analysis and 2) facilitate researchers to search and browse future\nworks in a research area. First, we study the problem of future work extraction\nand propose a regular expression based method to address the problem. Second,\nwe define four different categories for the future works by observing the data\nand investigate the multi-class future work classification problem. Third, we\napply the extraction method and the classification model to a paper dataset in\nthe computer science field and conduct a further analysis of the future works.\nFinally, we design a prototype system to search and demonstrate the future\nworks mined from the scientific papers. Our evaluation results show that our\nextraction method can get high precision and recall values and our\nclassification model can also get good results and it outperforms several\nbaseline models. Further analysis of the future work sentences also indicates\ninteresting results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:14:38 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Hu", "Yue", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1507.02145", "submitter": "Xiaojun Wan", "authors": "Xiaojiang Huang, Xiaojun Wan and Jianguo Xiao", "title": "Learning to Mine Chinese Coordinate Terms Using the Web", "comments": "This paper was written several years ago", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate relation refers to the relation between instances of a concept and\nthe relation between the directly hyponyms of a concept. In this paper, we\nfocus on the task of extracting terms which are coordinate with a user given\nseed term in Chinese, and grouping the terms which belong to different concepts\nif the seed term has several meanings. We propose a semi-supervised method that\nintegrates manually defined linguistic patterns and automatically learned\nsemi-structural patterns to extract coordinate terms in Chinese from web search\nresults. In addition, terms are grouped into different concepts based on their\nco-occurring terms and contexts. We further calculate the saliency scores of\nextracted terms and rank them accordingly. Experimental results demonstrate\nthat our proposed method generates results with high quality and wide coverage.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:27:43 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 04:28:47 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Huang", "Xiaojiang", ""], ["Wan", "Xiaojun", ""], ["Xiao", "Jianguo", ""]]}, {"id": "1507.02205", "submitter": "Aaron Jaech", "authors": "Aaron Jaech, Victoria Zayats, Hao Fang, Mari Ostendorf and Hannaneh\n  Hajishirzi", "title": "Talking to the crowd: What do people react to in online discussions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the question of how language use affects community\nreaction to comments in online discussion forums, and the relative importance\nof the message vs. the messenger. A new comment ranking task is proposed based\non community annotated karma in Reddit discussions, which controls for topic\nand timing of comments. Experimental work with discussion threads from six\nsubreddits shows that the importance of different types of language features\nvaries with the community of interest.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 15:55:18 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2015 18:09:26 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Jaech", "Aaron", ""], ["Zayats", "Victoria", ""], ["Fang", "Hao", ""], ["Ostendorf", "Mari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1507.02447", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari", "title": "Data Mining of Causal Relations from Text: Analysing Maritime Accident\n  Investigation Reports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining is a process of extracting information of interest from text.\nSuch a method includes techniques from various areas such as Information\nRetrieval (IR), Natural Language Processing (NLP), and Information Extraction\n(IE). In this study, text mining methods are applied to extract causal\nrelations from maritime accident investigation reports collected from the\nMarine Accident Investigation Branch (MAIB). These causal relations provide\ninformation on various mechanisms behind accidents, including human and\norganizational factors relating to the accident. The objective of this study is\nto facilitate the analysis of the maritime accident investigation reports, by\nmeans of extracting contributory causes with more feasibility. A careful\ninvestigation of contributory causes from the reports provide opportunity to\nimprove safety in future.\n  Two methods have been employed in this study to extract the causal relations.\nThey are 1) Pattern classification method and 2) Connectives method. The\nearlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers.\nThe latter simply searches for the words connecting cause and effect in\nsentences.\n  The causal patterns extracted using these two methods are compared to the\nmanual (human expert) extraction. The pattern classification method showed a\nfair and sensible performance with F-measure(average) = 65% when compared to\nconnectives method with F-measure(average) = 58%. This study is an evidence,\nthat text mining methods could be employed in extracting causal relations from\nmarine accident investigation reports.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 10:20:52 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Tirunagari", "Santosh", ""]]}, {"id": "1507.02628", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang and Abraham Ittycheriah", "title": "FAQ-based Question Answering via Word Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel word-alignment-based method to solve the\nFAQ-based question answering task. First, we employ a neural network model to\ncalculate question similarity, where the word alignment between two questions\nis used for extracting features. Second, we design a bootstrap-based feature\nextraction method to extract a small set of effective lexical features. Third,\nwe propose a learning-to-rank algorithm to train parameters more suitable for\nthe ranking tasks. Experimental results, conducted on three languages (English,\nSpanish and Japanese), demonstrate that the question similarity model is more\neffective than baseline systems, the sparse features bring 5% improvements on\ntop-1 accuracy, and the learning-to-rank algorithm works significantly better\nthan the traditional method. We further evaluate our method on the answer\nsentence selection task. Our method outperforms all the previous systems on the\nstandard TREC data set.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 18:11:03 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Wang", "Zhiguo", ""], ["Ittycheriah", "Abraham", ""]]}, {"id": "1507.02907", "submitter": "Luis Marujo", "authors": "Lu\\'is Marujo, Ricardo Ribeiro, David Martins de Matos, Jo\\~ao P.\n  Neto, Anatole Gershman, Jaime Carbonell", "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical\n  Approach", "comments": "6 pages, Please cite: Proceedings of *SEM: the 4th Joint Conference\n  on Lexical and Computational Semantics (bibtex:\n  http://aclweb.org/anthology/S/S15/S15-1020.bib)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of online content motivated the development of\nmulti-document summarization methods. In this work, we explore straightforward\napproaches to extend single-document summarization methods to multi-document\nsummarization. The proposed methods are based on the hierarchical combination\nof single-document summaries, and achieves state of the art results.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 13:59:00 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Marujo", "Lu\u00eds", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1507.03045", "submitter": "Tushar Khot", "authors": "Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish\n  Sabharwal, Peter Clark, Oren Etzioni", "title": "Markov Logic Networks for Natural Language Question Answering", "comments": "7 pages, 1 figure, StarAI workshop at UAI'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to answer elementary-level science questions using knowledge\nextracted automatically from science textbooks, expressed in a subset of\nfirst-order logic. Given the incomplete and noisy nature of these automatically\nextracted rules, Markov Logic Networks (MLNs) seem a natural model to use, but\nthe exact way of leveraging MLNs is by no means obvious. We investigate three\nways of applying MLNs to our task. In the first, we simply use the extracted\nscience rules directly as MLN clauses. Unlike typical MLN applications, our\ndomain has long and complex rules, leading to an unmanageable number of\ngroundings. We exploit the structure present in hard constraints to improve\ntractability, but the formulation remains ineffective. In the second approach,\nwe instead interpret science rules as describing prototypical entities, thus\nmapping rules directly to grounded MLN assertions, whose constants are then\nclustered using existing entity resolution methods. This drastically simplifies\nthe network, but still suffers from brittleness. Finally, our third approach,\ncalled Praline, uses MLNs to align the lexical elements as well as define and\ncontrol how inference should be performed in this task. Our experiments,\ndemonstrating a 15\\% accuracy boost and a 10x reduction in runtime, suggest\nthat the flexibility and different inference semantics of Praline are a better\nfit for the natural language question answering task.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 23:17:53 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Khot", "Tushar", ""], ["Balasubramanian", "Niranjan", ""], ["Gribkoff", "Eric", ""], ["Sabharwal", "Ashish", ""], ["Clark", "Peter", ""], ["Etzioni", "Oren", ""]]}, {"id": "1507.03077", "submitter": "Adel Rahimi", "authors": "Adel Rahimi", "title": "A new hybrid stemming algorithm for Persian", "comments": "8 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stemming has been an influential part in Information retrieval and search\nengines. There have been tremendous endeavours in making stemmer that are both\nefficient and accurate. Stemmers can have three method in stemming, Dictionary\nbased stemmer, statistical-based stemmers, and rule-based stemmers. This paper\naims at building a hybrid stemmer that uses both Dictionary based method and\nrule-based method for stemming. This ultimately helps the efficacy and\naccurateness of the stemmer.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 08:54:45 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 11:57:40 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Rahimi", "Adel", ""]]}, {"id": "1507.03223", "submitter": "Nisheeth Joshi", "authors": "Shruti Tyagi, Deepti Chopra, Iti Mathur, Nisheeth Joshi", "title": "Classifier-Based Text Simplification for Improved Machine Translation", "comments": "In Proceedings of International Conference on Advances in Computer\n  Engineering and Applications 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation is one of the research fields of Computational\nLinguistics. The objective of many MT Researchers is to develop an MT System\nthat produce good quality and high accuracy output translations and which also\ncovers maximum language pairs. As internet and Globalization is increasing day\nby day, we need a way that improves the quality of translation. For this\nreason, we have developed a Classifier based Text Simplification Model for\nEnglish-Hindi Machine Translation Systems. We have used support vector machines\nand Na\\\"ive Bayes Classifier to develop this model. We have also evaluated the\nperformance of these classifiers.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 12:14:19 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Tyagi", "Shruti", ""], ["Chopra", "Deepti", ""], ["Mathur", "Iti", ""], ["Joshi", "Nisheeth", ""]]}, {"id": "1507.03462", "submitter": "I\\~nigo Lopez-Gazpio", "authors": "Itziar Aldabe, Oier Lopez de Lacalle, I\\~nigo Lopez-Gazpio and Montse\n  Maritxalar", "title": "Supervised Hierarchical Classification for Student Answer Scoring", "comments": "5 pages with references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a hierarchical system that predicts one label at a time\nfor automated student response analysis. For the task, we build a\nclassification binary tree that delays more easily confused labels to later\nstages using hierarchical processes. In particular, the paper describes how the\nhierarchical classifier has been built and how the classification task has been\nbroken down into binary subtasks. It finally discusses the motivations and\nfundamentals of such an approach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 14:00:22 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Aldabe", "Itziar", ""], ["de Lacalle", "Oier Lopez", ""], ["Lopez-Gazpio", "I\u00f1igo", ""], ["Maritxalar", "Montse", ""]]}, {"id": "1507.03471", "submitter": "Lukas Zilka", "authors": "Lukas Zilka, Filip Jurcicek", "title": "Incremental LSTM-based Dialog State Tracker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dialog state tracker is an important component in modern spoken dialog\nsystems. We present an incremental dialog state tracker, based on LSTM\nnetworks. It directly uses automatic speech recognition hypotheses to track the\nstate. We also present the key non-standard aspects of the model that bring its\nperformance close to the state-of-the-art and experimentally analyze their\ncontribution: including the ASR confidence scores, abstracting scarcely\nrepresented values, including transcriptions in the training data, and model\naveraging.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 14:27:16 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Zilka", "Lukas", ""], ["Jurcicek", "Filip", ""]]}, {"id": "1507.03641", "submitter": "Greg Durrett", "authors": "Greg Durrett and Dan Klein", "title": "Neural CRF Parsing", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a parsing model that combines the exact dynamic\nprogramming of CRF parsing with the rich nonlinear featurization of neural net\napproaches. Our model is structurally a CRF that factors over anchored rule\nproductions, but instead of linear potential functions based on sparse\nfeatures, we use nonlinear potentials computed via a feedforward neural\nnetwork. Because potentials are still local to anchored rules, structured\ninference (CKY) is unchanged from the sparse case. Computing gradients during\nlearning involves backpropagating an error signal formed from standard CRF\nsufficient statistics (expected rule counts). Using only dense features, our\nneural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In\ncombination with sparse features, our system achieves 91.1 F1 on section 23 of\nthe Penn Treebank, and more generally outperforms the best prior single parser\nresults on a range of languages.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 22:23:51 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Durrett", "Greg", ""], ["Klein", "Dan", ""]]}, {"id": "1507.03934", "submitter": "Kai Sun", "authors": "Kai Sun, Qizhe Xie, Kai Yu", "title": "Recurrent Polynomial Network for Dialogue State Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue state tracking (DST) is a process to estimate the distribution of\nthe dialogue states as a dialogue progresses. Recent studies on constrained\nMarkov Bayesian polynomial (CMBP) framework take the first step towards\nbridging the gap between rule-based and statistical approaches for DST. In this\npaper, the gap is further bridged by a novel framework -- recurrent polynomial\nnetwork (RPN). RPN's unique structure enables the framework to have all the\nadvantages of CMBP including efficiency, portability and interpretability.\nAdditionally, RPN achieves more properties of statistical approaches than CMBP.\nRPN was evaluated on the data corpora of the second and the third Dialog State\nTracking Challenge (DSTC-2/3). Experiments showed that RPN can significantly\noutperform both traditional rule-based approaches and statistical approaches\nwith similar feature set. Compared with the state-of-the-art statistical DST\napproaches with a lot richer features, RPN is also competitive.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 17:18:30 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 10:56:09 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Sun", "Kai", ""], ["Xie", "Qizhe", ""], ["Yu", "Kai", ""]]}, {"id": "1507.04019", "submitter": "Pavan Kumar D S", "authors": "D. S. Pavan Kumar", "title": "Feature Normalisation for Robust Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Speech recognition system performance degrades in noisy environments. If the\nacoustic models are built using features of clean utterances, the features of a\nnoisy test utterance would be acoustically mismatched with the trained model.\nThis gives poor likelihoods and poor recognition accuracy. Model adaptation and\nfeature normalisation are two broad areas that address this problem. While the\nformer often gives better performance, the latter involves estimation of lesser\nnumber of parameters, making the system feasible for practical implementations.\n  This research focuses on the efficacies of various subspace, statistical and\nstereo based feature normalisation techniques. A subspace projection based\nmethod has been investigated as a standalone and adjunct technique involving\nreconstruction of noisy speech features from a precomputed set of clean speech\nbuilding-blocks. The building blocks are learned using non-negative matrix\nfactorisation (NMF) on log-Mel filter bank coefficients, which form a basis for\nthe clean speech subspace. The work provides a detailed study on how the method\ncan be incorporated into the extraction process of Mel-frequency cepstral\ncoefficients. Experimental results show that the new features are robust to\nnoise, and achieve better results when combined with the existing techniques.\n  The work also proposes a modification to the training process of SPLICE\nalgorithm for noise robust speech recognition. It is based on feature\ncorrelations, and enables this stereo-based algorithm to improve the\nperformance in all noise conditions, especially in unseen cases. Further, the\nmodified framework is extended to work for non-stereo datasets where clean and\nnoisy training utterances, but not stereo counterparts, are required. An\nMLLR-based computationally efficient run-time noise adaptation method in SPLICE\nframework has been proposed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 20:34:16 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Kumar", "D. S. Pavan", ""]]}, {"id": "1507.04116", "submitter": "Angelo Mariano", "authors": "Angelo Mariano, Giorgio Parisi, Saverio Pascazio", "title": "Language discrimination and clustering via a neural network approach", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.CL cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify twenty-one Indo-European languages starting from written text. We\nuse neural networks in order to define a distance among different languages,\nconstruct a dendrogram and analyze the ultrametric structure that emerges. Four\nor five subgroups of languages are identified, according to the \"cut\" of the\ndendrogram, drawn with an entropic criterion. The results and the method are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:19:14 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Mariano", "Angelo", ""], ["Parisi", "Giorgio", ""], ["Pascazio", "Saverio", ""]]}, {"id": "1507.04214", "submitter": "\\\"Umit Mersinli", "authors": "Umit Mersinli", "title": "Associative Measures and Multi-word Unit Extraction in Turkish", "comments": null, "journal-ref": "Associative Measures and Multi-word Unit Extraction in Turkish.\n  Dil ve Edebiyat Dergisi. 12(1). 43-61", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Associative measures are \"mathematical formulas determining the strength of\nassociation between two or more words based on their occurrences and\ncooccurrences in a text corpus\" (Pecina, 2010, p. 138). The purpose of this\npaper is to test the 12 associative measures that Text-NSP (Banerjee &\nPedersen, 2003) contains on a 10-million-word subcorpus of Turkish National\nCorpus (TNC) (Aksan et.al., 2012). A statistical comparison of those measures\nis out of the scope of the study, and the measures will be evaluated according\nto the linguistic relevance of the rankings they provide. The focus of the\nstudy is basically on optimizing the corpus data, before applying the measures\nand then, evaluating the rankings produced by these measures as a whole, not on\nthe linguistic relevance of individual n-grams. The findings include\nintra-linguistically relevant associative measures for a comma delimited,\nsentence splitted, lower-cased, well-balanced, representative, 10-million-word\ncorpus of Turkish.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 13:40:28 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Mersinli", "Umit", ""]]}, {"id": "1507.04420", "submitter": "Morgan Sonderegger", "authors": "James Kirby, Morgan Sonderegger", "title": "Bias and population structure in the actuation of sound change", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do human languages change at some times, and not others? We address this\nlongstanding question from a computational perspective, focusing on the case of\nsound change. Sound change arises from the pronunciation variability ubiquitous\nin every speech community, but most such variability does not lead to change.\nHence, an adequate model must allow for stability as well as change. Existing\ntheories of sound change tend to emphasize factors at the level of individual\nlearners promoting one outcome or the other, such as channel bias (which favors\nchange) or inductive bias (which favors stability). Here, we consider how the\ninteraction of these biases can lead to both stability and change in a\npopulation setting. We find that population structure itself can act as a\nsource of stability, but that both stability and change are possible only when\nboth types of bias are active, suggesting that it is possible to understand why\nsound change occurs at some times and not others as the population-level result\nof the interplay between forces promoting each outcome in individual speakers.\nIn addition, if it is assumed that learners learn from two or more teachers,\nthe transition from stability to change is marked by a phase transition,\nconsistent with the abrupt transitions seen in many empirical cases of sound\nchange. The predictions of multiple-teacher models thus match empirical cases\nof sound change better than the predictions of single-teacher models,\nunderscoring the importance of modeling language change in a population\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 00:27:02 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Kirby", "James", ""], ["Sonderegger", "Morgan", ""]]}, {"id": "1507.04646", "submitter": "Yang Liu", "authors": "Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, Houfeng Wang", "title": "A Dependency-Based Neural Network for Relation Classification", "comments": "This preprint is the full version of a short paper accepted in the\n  annual meeting of the Association for Computational Linguistics (ACL) 2015\n  (Beijing, China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research on relation classification has verified the effectiveness\nof using dependency shortest paths or subtrees. In this paper, we further\nexplore how to make full use of the combination of these dependency\ninformation. We first propose a new structure, termed augmented dependency path\n(ADP), which is composed of the shortest dependency path between two entities\nand the subtrees attached to the shortest path. To exploit the semantic\nrepresentation behind the ADP structure, we develop dependency-based neural\nnetworks (DepNN): a recursive neural network designed to model the subtrees,\nand a convolutional neural network to capture the most important features on\nthe shortest path. Experiments on the SemEval-2010 dataset show that our\nproposed method achieves state-of-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 16:43:55 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Liu", "Yang", ""], ["Wei", "Furu", ""], ["Li", "Sujian", ""], ["Ji", "Heng", ""], ["Zhou", "Ming", ""], ["Wang", "Houfeng", ""]]}, {"id": "1507.04798", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist", "title": "Exploratory topic modeling with distributional semantics", "comments": "Conference: The Fourteenth International Symposium on Intelligent\n  Data Analysis (IDA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As we continue to collect and store textual data in a multitude of domains,\nwe are regularly confronted with material whose largely unknown thematic\nstructure we want to uncover. With unsupervised, exploratory analysis, no prior\nknowledge about the content is required and highly open-ended tasks can be\nsupported. In the past few years, probabilistic topic modeling has emerged as a\npopular approach to this problem. Nevertheless, the representation of the\nlatent topics as aggregations of semi-coherent terms limits their\ninterpretability and level of detail.\n  This paper presents an alternative approach to topic modeling that maps\ntopics as a network for exploration, based on distributional semantics using\nlearned word vectors. From the granular level of terms and their semantic\nsimilarity relations global topic structures emerge as clustered regions and\ngradients of concepts. Moreover, the paper discusses the visual interactive\nrepresentation of the topic map, which plays an important role in supporting\nits exploration.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 23:11:45 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""]]}, {"id": "1507.04808", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville\n  and Joelle Pineau", "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical\n  Neural Network Models", "comments": "8 pages with references; Published in AAAI 2016 (Special Track on\n  Cognitive Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 00:21:39 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 19:49:39 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 23:20:41 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sordoni", "Alessandro", ""], ["Bengio", "Yoshua", ""], ["Courville", "Aaron", ""], ["Pineau", "Joelle", ""]]}, {"id": "1507.04908", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic and Zoran N. Milivojevic and Alessia Amelio", "title": "Analysis of the South Slavic Scripts by Run-Length Features of the Image\n  Texture", "comments": "9 pages, 9 figures, In Electronics 2015, Elektronika IR\n  Elektrotechnika, ISSN 1392-1215 (in press)", "journal-ref": "Elektronika Ir Elektrotechnika, ISSN 1392-1215, VOL. 21, NO. 4,\n  2015", "doi": "10.5755/j01.eee.21.4.12785", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes an algorithm for the script recognition based on the\ntexture characteristics. The image texture is achieved by coding each letter\nwith the equivalent script type (number code) according to its position in the\ntext line. Each code is transformed into equivalent gray level pixel creating\nan 1-D image. Then, the image texture is subjected to the run-length analysis.\nThis analysis extracts the run-length features, which are classified to make a\ndistinction between the scripts under consideration. In the experiment, a\ncustom oriented database is subject to the proposed algorithm. The database\nconsists of some text documents written in Cyrillic, Latin and Glagolitic\nscripts. Furthermore, it is divided into training and test parts. The results\nof the experiment show that 3 out of 5 run-length features can be used for\neffective differentiation between the analyzed South Slavic scripts.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 10:34:23 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Brodic", "Darko", ""], ["Milivojevic", "Zoran N.", ""], ["Amelio", "Alessia", ""]]}, {"id": "1507.05134", "submitter": "Matilde Marcolli", "authors": "Alexander Port, Iulia Gheorghita, Daniel Guth, John M.Clark, Crystal\n  Liang, Shival Dasu, Matilde Marcolli", "title": "Persistent Topology of Syntax", "comments": "15 pages, 25 jpg figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the persistent homology of the data set of syntactic parameters of\nthe world languages. We show that, while homology generators behave erratically\nover the whole data set, non-trivial persistent homology appears when one\nrestricts to specific language families. Different families exhibit different\npersistent homology. We focus on the cases of the Indo-European and the\nNiger-Congo families, for which we compare persistent homology over different\ncluster filtering values. We investigate the possible significance, in\nhistorical linguistic terms, of the presence of persistent generators of the\nfirst homology. In particular, we show that the persistent first homology\ngenerator we find in the Indo-European family is not due (as one might guess)\nto the Anglo-Norman bridge in the Indo-European phylogenetic network, but is\nrelated to the position of Ancient Greek and the Hellenic branch within the\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 00:07:52 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Port", "Alexander", ""], ["Gheorghita", "Iulia", ""], ["Guth", "Daniel", ""], ["Clark", "John M.", ""], ["Liang", "Crystal", ""], ["Dasu", "Shival", ""], ["Marcolli", "Matilde", ""]]}, {"id": "1507.05523", "submitter": "Siwei Lai", "authors": "Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao", "title": "How to Generate a Good Word Embedding?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze three critical components of word embedding training: the model,\nthe corpus, and the training parameters. We systematize existing\nneural-network-based word embedding algorithms and compare them using the same\ncorpus. We evaluate each word embedding in three ways: analyzing its semantic\nproperties, using it as a feature for supervised tasks and using it to\ninitialize neural networks. We also provide several simple guidelines for\ntraining word embeddings. First, we discover that corpus domain is more\nimportant than corpus size. We recommend choosing a corpus in a suitable domain\nfor the desired task, after that, using a larger corpus yields better results.\nSecond, we find that faster models provide sufficient performance in most\ncases, and more complex models can be used if the training corpus is\nsufficiently large. Third, the early stopping metric for iterating should rely\non the development set of the desired task rather than the validation loss of\ntraining embedding.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 15:07:53 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Lai", "Siwei", ""], ["Liu", "Kang", ""], ["Xu", "Liheng", ""], ["Zhao", "Jun", ""]]}, {"id": "1507.05630", "submitter": "Matteo Grella", "authors": "Matteo Grella", "title": "Notes About a More Aware Dependency Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I explain the reasons that led me to research and conceive a\nnovel technology for dependency parsing, mixing together the strengths of\ndata-driven transition-based and constraint-based approaches. In particular I\nhighlight the problem to infer the reliability of the results of a data-driven\ntransition-based parser, which is extremely important for high-level processes\nthat expect to use correct parsing results. I then briefly introduce a number\nof notes about a new parser model I'm working on, capable to proceed with the\nanalysis in a \"more aware\" way, with a more \"robust\" concept of robustness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 20:01:44 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Grella", "Matteo", ""]]}, {"id": "1507.05910", "submitter": "Sarath Chandar", "authors": "Alex Auvolat, Sarath Chandar, Pascal Vincent, Hugo Larochelle, Yoshua\n  Bengio", "title": "Clustering is Efficient for Approximate Maximum Inner Product Search", "comments": "10 pages, Under review at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Maximum Inner Product Search (MIPS) is an important task that has a\nwide applicability in recommendation systems and classification with a large\nnumber of classes. Solutions based on locality-sensitive hashing (LSH) as well\nas tree-based solutions have been investigated in the recent literature, to\nperform approximate MIPS in sublinear time. In this paper, we compare these to\nanother extremely simple approach for solving approximate MIPS, based on\nvariants of the k-means clustering algorithm. Specifically, we propose to train\na spherical k-means, after having reduced the MIPS problem to a Maximum Cosine\nSimilarity Search (MCSS). Experiments on two standard recommendation system\nbenchmarks as well as on large vocabulary word embeddings, show that this\nsimple approach yields much higher speedups, for the same retrieval precision,\nthan current state-of-the-art hashing-based and tree-based methods. This simple\nmethod also yields more robust retrievals when the query is corrupted by noise.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 16:53:12 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 16:36:09 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 02:26:44 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Auvolat", "Alex", ""], ["Chandar", "Sarath", ""], ["Vincent", "Pascal", ""], ["Larochelle", "Hugo", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1507.06020", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze", "title": "Practical Selection of SVM Supervised Parameters with Different Feature\n  Representations for Vowel Recognition", "comments": "07 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the classification performance of Support Vector Machine\n(SVM) can be conveniently affected by the different parameters of the kernel\ntricks and the regularization parameter, C. Thus, in this article, we propose a\nstudy in order to find the suitable kernel with which SVM may achieve good\ngeneralization performance as well as the parameters to use. We need to analyze\nthe behavior of the SVM classifier when these parameters take very small or\nvery large values. The study is conducted for a multi-class vowel recognition\nusing the TIMIT corpus. Furthermore, for the experiments, we used different\nfeature representations such as MFCC and PLP. Finally, a comparative study was\ndone to point out the impact of the choice of the parameters, kernel trick and\nfeature representations on the performance of the SVM classifier\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 00:27:11 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1507.06021", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze", "title": "An Empirical Comparison of SVM and Some Supervised Learning Algorithms\n  for Vowel recognition", "comments": "08 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we conduct a study on the performance of some supervised\nlearning algorithms for vowel recognition. This study aims to compare the\naccuracy of each algorithm. Thus, we present an empirical comparison between\nfive supervised learning classifiers and two combined classifiers: SVM, KNN,\nNaive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms\nwere tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral\ncoefficients (MFCCs).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 00:34:15 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1507.06023", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Ghaith Manita, Abir Smiti", "title": "Robust speech recognition using consensus function based on multi-layer\n  networks", "comments": "06 pages", "journal-ref": "9th Iberian Conference on Information Systems and Technologies\n  (CISTI), Barcelona 18-21 June, 2014, pgs 1-6", "doi": "10.1109/CISTI.2014.6877093", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering ensembles mingle numerous partitions of a specified data into\na single clustering solution. Clustering ensemble has emerged as a potent\napproach for ameliorating both the forcefulness and the stability of\nunsupervised classification results. One of the major problems in clustering\nensembles is to find the best consensus function. Finding final partition from\ndifferent clustering results requires skillfulness and robustness of the\nclassification algorithm. In addition, the major problem with the consensus\nfunction is its sensitivity to the used data sets quality. This limitation is\ndue to the existence of noisy, silence or redundant data. This paper proposes a\nnovel consensus function of cluster ensembles based on Multilayer networks\ntechnique and a maintenance database method. This maintenance database approach\nis used in order to handle any given noisy speech and, thus, to guarantee the\nquality of databases. This can generates good results and efficient data\npartitions. To show its effectiveness, we support our strategy with empirical\nevaluation using distorted speech from Aurora speech databases.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 00:49:48 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Manita", "Ghaith", ""], ["Smiti", "Abir", ""]]}, {"id": "1507.06025", "submitter": "Rimah  Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Nouerddine Ellouze", "title": "Incorporating Belief Function in SVM for Phoneme Recognition", "comments": "9th International Conference, Hybrid Artificial Intelligence Systems,\n  Salamanca, Spain, June 11-13, 2014", "journal-ref": "Lecture Notes in Computer Science Volume 8480, 2014, pp 191-199", "doi": "10.1007/978-3-319-07617-1_17", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Support Vector Machine (SVM) method has been widely used in numerous\nclassification tasks. The main idea of this algorithm is based on the principle\nof the margin maximization to find an hyperplane which separates the data into\ntwo different classes.In this paper, SVM is applied to phoneme recognition\ntask. However, in many real-world problems, each phoneme in the data set for\nrecognition problems may differ in the degree of significance due to noise,\ninaccuracies, or abnormal characteristics; All those problems can lead to the\ninaccuracies in the prediction phase. Unfortunately, the standard formulation\nof SVM does not take into account all those problems and, in particular, the\nvariation in the speech input. This paper presents a new formulation of SVM\n(B-SVM) that attributes to each phoneme a confidence degree computed based on\nits geometric position in the space. Then, this degree is used in order to\nstrengthen the class membership of the tested phoneme. Hence, we introduce a\nreformulation of the standard SVM that incorporates the degree of belief.\nExperimental performance on TIMIT database shows the effectiveness of the\nproposed method B-SVM on a phoneme recognition problem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 01:03:28 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Nouerddine", ""]]}, {"id": "1507.06028", "submitter": "Rimah Amami", "authors": "Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze", "title": "The challenges of SVM optimization using Adaboost on a phoneme\n  recognition problem", "comments": null, "journal-ref": "IEEE 4th International Conference on Cognitive Infocommunications\n  (CogInfoCom), Budapest 2-5 Dec. 2013, pgs 463-468", "doi": "10.1109/CogInfoCom.2013.6719292", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of digital technology is growing at a very fast pace which led to the\nemergence of systems based on the cognitive infocommunications. The expansion\nof this sector impose the use of combining methods in order to ensure the\nrobustness in cognitive systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 01:27:05 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Amami", "Rimah", ""], ["Ayed", "Dorra Ben", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1507.06073", "submitter": "Hao Tang", "authors": "Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu", "title": "Discriminative Segmental Cascades for Feature-Rich Phone Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative segmental models, such as segmental conditional random fields\n(SCRFs) and segmental structured support vector machines (SSVMs), have had\nsuccess in speech recognition via both lattice rescoring and first-pass\ndecoding. However, such models suffer from slow decoding, hampering the use of\ncomputationally expensive features, such as segment neural networks or other\nhigh-order features. A typical solution is to use approximate decoding, either\nby beam pruning in a single pass or by beam pruning to generate a lattice\nfollowed by a second pass. In this work, we study discriminative segmental\nmodels trained with a hinge loss (i.e., segmental structured SVMs). We show\nthat beam search is not suitable for learning rescoring models in this\napproach, though it gives good approximate decoding performance when the model\nis already well-trained. Instead, we consider an approach inspired by\nstructured prediction cascades, which use max-marginal pruning to generate\nlattices. We obtain a high-accuracy phonetic recognition system with several\nexpensive feature types: a segment neural network, a second-order language\nmodel, and second-order phone boundary features.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 06:54:09 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 02:58:29 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Weiran", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1507.06711", "submitter": "Shitao Weng", "authors": "Shitao Weng, Shushan Chen, Lei Yu, Xuewei Wu, Weicheng Cai, Zhi Liu,\n  Ming Li", "title": "The SYSU System for the Interspeech 2015 Automatic Speaker Verification\n  Spoofing and Countermeasures Challenge", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing speaker verification systems are reported to be vulnerable\nagainst different spoofing attacks, for example speaker-adapted speech\nsynthesis, voice conversion, play back, etc. In order to detect these spoofed\nspeech signals as a countermeasure, we propose a score level fusion approach\nwith several different i-vector subsystems. We show that the acoustic level\nMel-frequency cepstral coefficients (MFCC) features, the phase level modified\ngroup delay cepstral coefficients (MGDCC) and the phonetic level phoneme\nposterior probability (PPP) tandem features are effective for the\ncountermeasure. Furthermore, feature level fusion of these features before\ni-vector modeling also enhance the performance. A polynomial kernel support\nvector machine is adopted as the supervised classifier. In order to enhance the\ngeneralizability of the countermeasure, we also adopted the cosine similarity\nand PLDA scoring as one-class classifications methods. By combining the\nproposed i-vector subsystems with the OpenSMILE baseline which covers the\nacoustic and prosodic information further improves the final performance. The\nproposed fusion system achieves 0.29% and 3.26% EER on the development and test\nset of the database provided by the INTERSPEECH 2015 automatic speaker\nverification spoofing and countermeasures challenge.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 00:36:01 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 15:41:04 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Weng", "Shitao", ""], ["Chen", "Shushan", ""], ["Yu", "Lei", ""], ["Wu", "Xuewei", ""], ["Cai", "Weicheng", ""], ["Liu", "Zhi", ""], ["Li", "Ming", ""]]}, {"id": "1507.06829", "submitter": "Lisa Posch", "authors": "Lisa Posch, Arnim Bleier, Philipp Schaer, Markus Strohmaier", "title": "The Polylingual Labeled Topic Model", "comments": "Accepted for publication at KI 2015 (38th edition of the German\n  Conference on Artificial Intelligence)", "journal-ref": null, "doi": "10.1007/978-3-319-24489-1_26", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Polylingual Labeled Topic Model, a model which\ncombines the characteristics of the existing Polylingual Topic Model and\nLabeled LDA. The model accounts for multiple languages with separate topic\ndistributions for each language while restricting the permitted topics of a\ndocument to a set of predefined labels. We explore the properties of the model\nin a two-language setting on a dataset from the social science domain. Our\nexperiments show that our model outperforms LDA and Labeled LDA in terms of\ntheir held-out perplexity and that it produces semantically coherent topics\nwhich are well interpretable by human subjects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 13:01:20 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Posch", "Lisa", ""], ["Bleier", "Arnim", ""], ["Schaer", "Philipp", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1507.06837", "submitter": "Jeremy Fix", "authors": "Jeremy Fix and Herve Frezza-buet", "title": "YARBUS : Yet Another Rule Based belief Update System", "comments": "Source code at : https://github.com/jeremyfix/dstc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new rule based system for belief tracking in dialog systems.\nDespite the simplicity of the rules being considered, the proposed belief\ntracker ranks favourably compared to the previous submissions on the second and\nthird Dialog State Tracking challenges. The results of this simple tracker\nallows to reconsider the performances of previous submissions using more\nelaborate techniques.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 13:21:59 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Fix", "Jeremy", ""], ["Frezza-buet", "Herve", ""]]}, {"id": "1507.06947", "submitter": "Hasim Sak", "authors": "Ha\\c{s}im Sak, Andrew Senior, Kanishka Rao, Fran\\c{c}oise Beaufays", "title": "Fast and Accurate Recurrent Neural Network Acoustic Models for Speech\n  Recognition", "comments": "To be published in the INTERSPEECH 2015 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently shown that deep Long Short-Term Memory (LSTM) recurrent\nneural networks (RNNs) outperform feed forward deep neural networks (DNNs) as\nacoustic models for speech recognition. More recently, we have shown that the\nperformance of sequence trained context dependent (CD) hidden Markov model\n(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained\nphone models initialized with connectionist temporal classification (CTC). In\nthis paper, we present techniques that further improve performance of LSTM RNN\nacoustic models for large vocabulary speech recognition. We show that frame\nstacking and reduced frame rate lead to more accurate models and faster\ndecoding. CD phone modeling leads to further improvements. We also present\ninitial results for LSTM RNN models outputting words directly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 18:28:32 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Sak", "Ha\u015fim", ""], ["Senior", "Andrew", ""], ["Rao", "Kanishka", ""], ["Beaufays", "Fran\u00e7oise", ""]]}, {"id": "1507.07636", "submitter": "Sarath Chandar", "authors": "Sridhar Mahadevan, Sarath Chandar", "title": "Reasoning about Linguistic Regularities in Word Embeddings using Matrix\n  Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has explored methods for learning continuous vector space word\nrepresentations reflecting the underlying semantics of words. Simple vector\nspace arithmetic using cosine distances has been shown to capture certain types\nof analogies, such as reasoning about plurals from singulars, past tense from\npresent tense, etc. In this paper, we introduce a new approach to capture\nanalogies in continuous word representations, based on modeling not just\nindividual word vectors, but rather the subspaces spanned by groups of words.\nWe exploit the property that the set of subspaces in n-dimensional Euclidean\nspace form a curved manifold space called the Grassmannian, a quotient subgroup\nof the Lie group of rotations in n- dimensions. Based on this mathematical\nmodel, we develop a modified cosine distance model based on geodesic kernels\nthat captures relation-specific distances across word categories. Our\nexperiments on analogy tasks show that our approach performs significantly\nbetter than the previous approaches for the given task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 03:51:43 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Mahadevan", "Sridhar", ""], ["Chandar", "Sarath", ""]]}, {"id": "1507.07826", "submitter": "Diego Amancio", "authors": "Henrique F. de Arruda, Luciano da F. Costa and Diego R. Amancio", "title": "Classifying informative and imaginative prose using complex networks", "comments": null, "journal-ref": "Europhysics Letters (EPL) 113 (2016) 28007", "doi": "10.1209/0295-5075/113/28007", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods have been widely employed in recent years to grasp many\nlanguage properties. The application of such techniques have allowed an\nimprovement of several linguistic applications, which encompasses machine\ntranslation, automatic summarization and document classification. In the\nlatter, many approaches have emphasized the semantical content of texts, as it\nis the case of bag-of-word language models. This approach has certainly yielded\nreasonable performance. However, some potential features such as the structural\norganization of texts have been used only on a few studies. In this context, we\nprobe how features derived from textual structure analysis can be effectively\nemployed in a classification task. More specifically, we performed a supervised\nclassification aiming at discriminating informative from imaginative documents.\nUsing a networked model that describes the local topological/dynamical\nproperties of function words, we achieved an accuracy rate of up to 95%, which\nis much higher than similar networked approaches. A systematic analysis of\nfeature relevance revealed that symmetry and accessibility measurements are\namong the most prominent network measurements. Our results suggest that these\nmeasurements could be used in related language applications, as they play a\ncomplementary role in characterizing texts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 15:59:39 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["de Arruda", "Henrique F.", ""], ["Costa", "Luciano da F.", ""], ["Amancio", "Diego R.", ""]]}, {"id": "1507.07998", "submitter": "Andrew Dai", "authors": "Andrew M. Dai and Christopher Olah and Quoc V. Le", "title": "Document Embedding with Paragraph Vectors", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 01:04:28 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Dai", "Andrew M.", ""], ["Olah", "Christopher", ""], ["Le", "Quoc V.", ""]]}, {"id": "1507.08240", "submitter": "Yajie Miao", "authors": "Yajie Miao, Mohammad Gowayyed, Florian Metze", "title": "EESEN: End-to-End Speech Recognition using Deep RNN Models and\n  WFST-based Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of automatic speech recognition (ASR) has improved\ntremendously due to the application of deep neural networks (DNNs). Despite\nthis progress, building a new ASR system remains a challenging task, requiring\nvarious resources, multiple training stages and significant expertise. This\npaper presents our Eesen framework which drastically simplifies the existing\npipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen\ninvolves learning a single recurrent neural network (RNN) predicting\ncontext-independent targets (phonemes or characters). To remove the need for\npre-generated frame labels, we adopt the connectionist temporal classification\n(CTC) objective function to infer the alignments between speech and label\nsequences. A distinctive feature of Eesen is a generalized decoding approach\nbased on weighted finite-state transducers (WFSTs), which enables the efficient\nincorporation of lexicons and language models into CTC decoding. Experiments\nshow that compared with the standard hybrid DNN systems, Eesen achieves\ncomparable word error rates (WERs), while at the same time speeding up decoding\nsignificantly.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 17:53:50 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 21:03:34 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2015 20:35:52 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Miao", "Yajie", ""], ["Gowayyed", "Mohammad", ""], ["Metze", "Florian", ""]]}, {"id": "1507.08396", "submitter": "Shuangyin Li", "authors": "Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, and Rong Pan", "title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 06:44:37 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Li", "Shuangyin", ""], ["Li", "Jiefei", ""], ["Huang", "Guan", ""], ["Tan", "Ruiyang", ""], ["Pan", "Rong", ""]]}, {"id": "1507.08449", "submitter": "David Vilares", "authors": "David Vilares and Carlos G\\'omez-Rodr\\'iguez and Miguel A. Alonso", "title": "One model, two languages: training bilingual parsers with harmonized\n  treebanks", "comments": "7 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to train lexicalized parsers using bilingual corpora\nobtained by merging harmonized treebanks of different languages, producing\nparsers that can analyze sentences in either of the learned languages, or even\nsentences that mix both. We test the approach on the Universal Dependency\nTreebanks, training with MaltParser and MaltOptimizer. The results show that\nthese bilingual parsers are more than competitive, as most combinations not\nonly preserve accuracy, but some even achieve significant improvements over the\ncorresponding monolingual parsers. Preliminary experiments also show the\napproach to be promising on texts with code-switching and when more languages\nare added.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 10:53:11 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 14:57:45 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Vilares", "David", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""], ["Alonso", "Miguel A.", ""]]}, {"id": "1507.08452", "submitter": "Shashi Narayan", "authors": "Shashi Narayan and Claire Gardent", "title": "Unsupervised Sentence Simplification Using Deep Semantics", "comments": "10 pages, INLG 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to sentence simplification which departs from\nprevious work in two main ways. First, it requires neither hand written rules\nnor a training corpus of aligned standard and simplified sentences. Second,\nsentence splitting operates on deep semantic structure. We show (i) that the\nunsupervised framework we propose is competitive with four state-of-the-art\nsupervised systems and (ii) that our semantic based approach allows for a\nprincipled and effective handling of sentence splitting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 11:05:01 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 12:20:35 GMT"}, {"version": "v3", "created": "Wed, 7 Sep 2016 16:43:38 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Narayan", "Shashi", ""], ["Gardent", "Claire", ""]]}, {"id": "1507.08539", "submitter": "Ana Me\\v{s}trovi\\'c", "authors": "Domagoj Margan, Ana Me\\v{s}trovi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Multilayer Network of Language: a Unified Framework for Structural\n  Analysis of Linguistic Subsystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the focus of complex networks research has shifted from the\nanalysis of isolated properties of a system toward a more realistic modeling of\nmultiple phenomena - multilayer networks. Motivated by the prosperity of\nmultilayer approach in social, transport or trade systems, we propose the\nintroduction of multilayer networks for language. The multilayer network of\nlanguage is a unified framework for modeling linguistic subsystems and their\nstructural properties enabling the exploration of their mutual interactions.\nVarious aspects of natural language systems can be represented as complex\nnetworks, whose vertices depict linguistic units, while links model their\nrelations. The multilayer network of language is defined by three aspects: the\nnetwork construction principle, the linguistic subsystem and the language of\ninterest. More precisely, we construct a word-level (syntax, co-occurrence and\nits shuffled counterpart) and a subword level (syllables and graphemes) network\nlayers, from five variations of original text (in the modeled language). The\nobtained results suggest that there are substantial differences between the\nnetworks structures of different language subsystems, which are hidden during\nthe exploration of an isolated layer. The word-level layers share structural\nproperties regardless of the language (e.g. Croatian or English), while the\nsyllabic subword level expresses more language dependent structural properties.\nThe preserved weighted overlap quantifies the similarity of word-level layers\nin weighted and directed networks. Moreover, the analysis of motifs reveals a\nclose topological structure of the syntactic and syllabic layers for both\nlanguages. The findings corroborate that the multilayer network framework is a\npowerful, consistent and systematic approach to model several linguistic\nsubsystems simultaneously and hence to provide a more unified view on language.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 15:11:00 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Margan", "Domagoj", ""], ["Me\u0161trovi\u0107", "Ana", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}]