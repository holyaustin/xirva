[{"id": "1602.00104", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution", "title": "Extracting Keyword for Disambiguating Name Based on the Overlap\n  Principle", "comments": "7 pages, Proceeding of International Conference on Information\n  Technology and Engineering Application (4-th ICIBA), Book 1, 119-125,\n  February 20-21, 2015. arXiv admin note: text overlap with arXiv:1212.3023", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Name disambiguation has become one of the main themes in the Semantic Web\nagenda. The semantic web is an extension of the current Web in which\ninformation is not only given well-defined meaning, but also has many purposes\nthat contain the ambiguous naturally or a lot of thing came with the overlap,\nmainly deals with the persons name. Therefore, we develop an approach to\nextract keywords from web snippet with utilizing the overlap principle, a\nconcept to understand things with ambiguous, whereby features of person are\ngenerated for dealing with the variety of web, the web is steadily gaining\nground in the semantic research.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 10:53:05 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""]]}, {"id": "1602.00293", "submitter": "Suman Kalyan Maity", "authors": "Suman Kalyan Maity, Chaitanya Sarda, Anshit Chaudhary, Abhijeet Patil,\n  Shraman Kumar, Akash Mondal and Animesh Mukherjee", "title": "WASSUP? LOL : Characterizing Out-of-Vocabulary Words in Twitter", "comments": "4 pages, 1 figure, CSCW '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language in social media is mostly driven by new words and spellings that are\nconstantly entering the lexicon thereby polluting it and resulting in high\ndeviation from the formal written version. The primary entities of such\nlanguage are the out-of-vocabulary (OOV) words. In this paper, we study various\nsociolinguistic properties of the OOV words and propose a classification model\nto categorize them into at least six categories. We achieve 81.26% accuracy\nwith high precision and recall. We observe that the content features are the\nmost discriminative ones followed by lexical and context features.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 18:12:46 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Maity", "Suman Kalyan", ""], ["Sarda", "Chaitanya", ""], ["Chaudhary", "Anshit", ""], ["Patil", "Abhijeet", ""], ["Kumar", "Shraman", ""], ["Mondal", "Akash", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1602.00367", "submitter": "Yijun Xiao", "authors": "Yijun Xiao and Kyunghyun Cho", "title": "Efficient Character-level Document Classification by Combining\n  Convolution and Recurrent Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document classification tasks were primarily tackled at word level. Recent\nresearch that works with character-level inputs shows several benefits over\nword-level approaches such as natural incorporation of morphemes and better\nhandling of rare words. We propose a neural network architecture that utilizes\nboth convolution and recurrent layers to efficiently encode character inputs.\nWe validate the proposed model on eight large scale document classification\ntasks and compare with character-level convolution-only models. It achieves\ncomparable performances with much less parameters.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 02:53:41 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Xiao", "Yijun", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1602.00426", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Chia-Hsiang Liu,\n  Hung-yi Lee and Lin-shan Lee", "title": "An Iterative Deep Learning Framework for Unsupervised Discovery of\n  Speech Features and Linguistic Units with Applications on Spoken Term\n  Detection", "comments": "arXiv admin note: text overlap with arXiv:1506.02327", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we aim to discover high quality speech features and linguistic\nunits directly from unlabeled speech data in a zero resource scenario. The\nresults are evaluated using the metrics and corpora proposed in the Zero\nResource Speech Challenge organized at Interspeech 2015. A Multi-layered\nAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets\nof acoustic tokens from the given corpus. Each acoustic token set is specified\nby a set of hyperparameters that describe the model configuration. These sets\nof acoustic tokens carry different characteristics fof the given corpus and the\nlanguage behind, thus can be mutually reinforced. The multiple sets of token\nlabels are then used as the targets of a Multi-target Deep Neural Network\n(MDNN) trained on low-level acoustic features. Bottleneck features extracted\nfrom the MDNN are then used as the feedback input to the MAT and the MDNN\nitself in the next iteration. We call this iterative deep learning framework\nthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which\ngenerates both high quality speech features for the Track 1 of the Challenge\nand acoustic tokens for the Track 2 of the Challenge. In addition, we performed\nextra experiments on the same corpora on the application of query-by-example\nspoken term detection. The experimental results showed the iterative deep\nlearning framework of MAT-DNN improved the detection performance due to better\nunderlying speech features and acoustic tokens.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 08:37:56 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Tsai", "Cheng-Yu", ""], ["Lu", "Hsiang-Hung", ""], ["Liu", "Chia-Hsiang", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1602.00515", "submitter": "Nikola Milo\\v{s}evi\\'c MSc", "authors": "Nikola Milosevic", "title": "Marvin: Semantic annotation using multiple knowledge sources", "comments": "9 pages, 4 figures, keywords: Semantic annotation, text\n  normalization, semantic web, linked data, information management, text\n  mining, information extraction, data curation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  People are producing more written material then anytime in the history. The\nincrease is so high that professionals from the various fields are no more able\nto cope with this amount of publications. Text mining tools can offer tools to\nhelp them and one of the tools that can aid information retrieval and\ninformation extraction is semantic text annotation. In this report we present\nMarvin, a text annotator written in Java, which can be used as a command line\ntool and as a Java library. Marvin is able to annotate text using multiple\nsources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 13:27:34 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 11:31:17 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Milosevic", "Nikola", ""]]}, {"id": "1602.00812", "submitter": "Richard Moot", "authors": "Richard Moot (LaBRI, CNRS)", "title": "The Grail theorem prover: Type theory for syntax and semantics", "comments": null, "journal-ref": "Modern Perspectives in Type Theoretical Semantics, Springer, 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the name suggests, type-logical grammars are a grammar formalism based on\nlogic and type theory. From the prespective of grammar design, type-logical\ngrammars develop the syntactic and semantic aspects of linguistic phenomena\nhand-in-hand, letting the desired semantics of an expression inform the\nsyntactic type and vice versa. Prototypical examples of the successful\napplication of type-logical grammars to the syntax-semantics interface include\ncoordination, quantifier scope and extraction.This chapter describes the Grail\ntheorem prover, a series of tools for designing and testing grammars in various\nmodern type-logical grammars which functions as a tool . All tools described in\nthis chapter are freely available.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 07:35:02 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 07:04:29 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Moot", "Richard", "", "LaBRI, CNRS"]]}, {"id": "1602.01103", "submitter": "Chenhao Tan", "authors": "Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, Lillian\n  Lee", "title": "Winning Arguments: Interaction Dynamics and Persuasion Strategies in\n  Good-faith Online Discussions", "comments": "12 pages, 10 figures, to appear in Proceedings of WWW 2016, data and\n  more at https://chenhaot.com/pages/changemyview.html (v2 made a minor\n  correction on submission rules in ChangeMyView.)", "journal-ref": null, "doi": "10.1145/2872427.2883081", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changing someone's opinion is arguably one of the most important challenges\nof social interaction. The underlying process proves difficult to study: it is\nhard to know how someone's opinions are formed and whether and how someone's\nviews shift. Fortunately, ChangeMyView, an active community on Reddit, provides\na platform where users present their own opinions and reasoning, invite others\nto contest them, and acknowledge when the ensuing discussions change their\noriginal views. In this work, we study these interactions to understand the\nmechanisms behind persuasion.\n  We find that persuasive arguments are characterized by interesting patterns\nof interaction dynamics, such as participant entry-order and degree of\nback-and-forth exchange. Furthermore, by comparing similar counterarguments to\nthe same opinion, we show that language factors play an essential role. In\nparticular, the interplay between the language of the opinion holder and that\nof the counterargument provides highly predictive cues of persuasiveness.\nFinally, since even in this favorable setting people may not be persuaded, we\ninvestigate the problem of determining whether someone's opinion is susceptible\nto being changed at all. For this more difficult task, we show that stylistic\nchoices in how the opinion is expressed carry predictive power.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 21:00:11 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 20:13:55 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Tan", "Chenhao", ""], ["Niculae", "Vlad", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Lee", "Lillian", ""]]}, {"id": "1602.01208", "submitter": "Akira Taniguchi", "authors": "Akira Taniguchi, Tadahiro Taniguchi and Tetsunari Inamura", "title": "Spatial Concept Acquisition for a Mobile Robot that Integrates\n  Self-Localization and Unsupervised Word Discovery from Spoken Sentences", "comments": "This paper was accepted in the IEEE Transactions on Cognitive and\n  Developmental Systems. (04-May-2016)", "journal-ref": null, "doi": "10.1109/TCDS.2016.2565542", "report-no": null, "categories": "cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised learning method for the\nlexical acquisition of words related to places visited by robots, from human\ncontinuous speech signals. We address the problem of learning novel words by a\nrobot that has no prior knowledge of these words except for a primitive\nacoustic model. Further, we propose a method that allows a robot to effectively\nuse the learned words and their meanings for self-localization tasks. The\nproposed method is nonparametric Bayesian spatial concept acquisition method\n(SpCoA) that integrates the generative model for self-localization and the\nunsupervised word segmentation in uttered sentences via latent variables\nrelated to the spatial concept. We implemented the proposed method SpCoA on\nSIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile\nrobot in a real environment. Further, we conducted experiments for evaluating\nthe performance of SpCoA. The experimental results showed that SpCoA enabled\nthe robot to acquire the names of places from speech sentences. They also\nrevealed that the robot could effectively utilize the acquired spatial concepts\nand reduce the uncertainty in self-localization.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 06:56:51 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 12:17:46 GMT"}, {"version": "v3", "created": "Sat, 7 May 2016 11:59:51 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Taniguchi", "Akira", ""], ["Taniguchi", "Tadahiro", ""], ["Inamura", "Tetsunari", ""]]}, {"id": "1602.01248", "submitter": "Spyros Sioutas SS", "authors": "Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis and Giannis\n  Tzimas", "title": "Using Hadoop for Large Scale Analysis on Twitter: A Technical Report", "comments": "8 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (or opinion mining) on Twitter data has attracted much\nattention recently. One of the system's key features, is the immediacy in\ncommunication with other users in an easy, user-friendly and fast way.\nConsequently, people tend to express their feelings freely, which makes Twitter\nan ideal source for accumulating a vast amount of opinions towards a wide\ndiversity of topics. This amount of information offers huge potential and can\nbe harnessed to receive the sentiment tendency towards these topics. However,\nsince none can invest an infinite amount of time to read through these tweets,\nan automated decision making approach is necessary. Nevertheless, most existing\nsolutions are limited in centralized environments only. Thus, they can only\nprocess at most a few thousand tweets. Such a sample, is not representative to\ndefine the sentiment polarity towards a topic due to the massive number of\ntweets published daily. In this paper, we go one step further and develop a\nnovel method for sentiment learning in the MapReduce framework. Our algorithm\nexploits the hashtags and emoticons inside a tweet, as sentiment labels, and\nproceeds to a classification procedure of diverse sentiment types in a parallel\nand distributed manner. Moreover, we utilize Bloom filters to compact the\nstorage size of intermediate data and boost the performance of our algorithm.\nThrough an extensive experimental evaluation, we prove that our solution is\nefficient, robust and scalable and confirm the quality of our sentiment\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 10:19:19 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Nodarakis", "Nikolaos", ""], ["Sioutas", "Spyros", ""], ["Tsakalidis", "Athanasios", ""], ["Tzimas", "Giannis", ""]]}, {"id": "1602.01428", "submitter": "Jason Dou", "authors": "Jason Dou, Ni Sun, Xiaojun Zou", "title": "\"Draw My Topics\": Find Desired Topics fast from large scale of Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the \"Draw My Topics\" toolkit, which provides a fast way to\nincorporate social scientists' interest into standard topic modelling. Instead\nof using raw corpus with primitive processing as input, an algorithm based on\nVector Space Model and Conditional Entropy are used to connect social\nscientists' willingness and unsupervised topic models' output. Space for users'\nadjustment on specific corpus of their interest is also accommodated. We\ndemonstrate the toolkit's use on the Diachronic People's Daily Corpus in\nChinese.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 19:44:37 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Dou", "Jason", ""], ["Sun", "Ni", ""], ["Zou", "Xiaojun", ""]]}, {"id": "1602.01576", "submitter": "Anantharaman Palacode Narayana Iyer", "authors": "Anantharaman Palacode Narayana Iyer", "title": "A Factorized Recurrent Neural Network based architecture for medium to\n  large vocabulary Language Modelling", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/ICSC.2016.37", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical language models are central to many applications that use\nsemantics. Recurrent Neural Networks (RNN) are known to produce state of the\nart results for language modelling, outperforming their traditional n-gram\ncounterparts in many cases. To generate a probability distribution across a\nvocabulary, these models require a softmax output layer that linearly increases\nin size with the size of the vocabulary. Large vocabularies need a\ncommensurately large softmax layer and training them on typical laptops/PCs\nrequires significant time and machine resources. In this paper we present a new\ntechnique for implementing RNN based large vocabulary language models that\nsubstantially speeds up computation while optimally using the limited memory\nresources. Our technique, while building on the notion of factorizing the\noutput layer by having multiple output layers, improves on the earlier work by\nsubstantially optimizing on the individual output layer size and also\neliminating the need for a multistep prediction process.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 07:53:11 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Iyer", "Anantharaman Palacode Narayana", ""]]}, {"id": "1602.01595", "submitter": "Waleed Ammar", "authors": "Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, Noah A.\n  Smith", "title": "Many Languages, One Parser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train one multilingual model for dependency parsing and use it to parse\nsentences in several languages. The parsing model uses (i) multilingual word\nclusters and embeddings; (ii) token-level language information; and (iii)\nlanguage-specific features (fine-grained POS tags). This input representation\nenables the parser not only to parse effectively in multiple languages, but\nalso to generalize across languages based on linguistic universals and\ntypological similarities, making it more effective to learn from limited\nannotations. Our parser's performance compares favorably to strong baselines in\na range of data scenarios, including when the target language has a large\ntreebank, a small treebank, or no treebank for training.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 08:51:18 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 07:14:47 GMT"}, {"version": "v3", "created": "Sat, 14 May 2016 03:07:40 GMT"}, {"version": "v4", "created": "Tue, 26 Jul 2016 06:30:50 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Ammar", "Waleed", ""], ["Mulcaire", "George", ""], ["Ballesteros", "Miguel", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1602.01635", "submitter": "Jules Hedges", "authors": "Jules Hedges, Mehrnoosh Sadrzadeh", "title": "A Generalised Quantifier Theory of Natural Language in Categorical\n  Compositional Distributional Semantics with Bialgebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical compositional distributional semantics is a model of natural\nlanguage; it combines the statistical vector space models of words with the\ncompositional models of grammar. We formalise in this model the generalised\nquantifier theory of natural language, due to Barwise and Cooper. The\nunderlying setting is a compact closed category with bialgebras. We start from\na generative grammar formalisation and develop an abstract categorical\ncompositional semantics for it, then instantiate the abstract setting to sets\nand relations and to finite dimensional vector spaces and linear maps. We prove\nthe equivalence of the relational instantiation to the truth theoretic\nsemantics of generalised quantifiers. The vector space instantiation formalises\nthe statistical usages of words and enables us to, for the first time, reason\nabout quantified phrases and sentences compositionally in distributional\nsemantics.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 11:15:28 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 10:55:58 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Hedges", "Jules", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1602.01895", "submitter": "Shijian Tang", "authors": "Shijian Tang, Song Han", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for\n  Images Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural language descriptions for images is a challenging task.\nThe traditional way is to use the convolutional neural network (CNN) to extract\nimage features, followed by recurrent neural network (RNN) to generate\nsentences. In this paper, we present a new model that added memory cells to\ngate the feeding of image features to the deep neural network. The intuition is\nenabling our model to memorize how much information from images should be fed\nat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed\nthat our model outperforms other state-of-the-art models with higher BLEU\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 00:17:18 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Tang", "Shijian", ""], ["Han", "Song", ""]]}, {"id": "1602.01925", "submitter": "Waleed Ammar", "authors": "Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris\n  Dyer, Noah A. Smith", "title": "Massively Multilingual Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new methods for estimating and evaluating embeddings of words in\nmore than fifty languages in a single shared embedding space. Our estimation\nmethods, multiCluster and multiCCA, use dictionaries and monolingual data; they\ndo not require parallel data. Our new evaluation method, multiQVEC-CCA, is\nshown to correlate better than previous ones with two downstream tasks (text\ncategorization and parsing). We also describe a web portal for evaluation that\nwill facilitate further research in this area, along with open-source releases\nof all our methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 04:26:38 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 08:08:21 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Ammar", "Waleed", ""], ["Mulcaire", "George", ""], ["Tsvetkov", "Yulia", ""], ["Lample", "Guillaume", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1602.01929", "submitter": "Aleksandr Sizov", "authors": "Kong Aik Lee, Ville Hautam\\\"aki, Anthony Larcher, Wei Rao, Hanwu Sun,\n  Trung Hieu Nguyen, Guangsen Wang, Aleksandr Sizov, Ivan Kukanov, Amir\n  Poorjam, Trung Ngo Trong, Xiong Xiao, Cheng-Lin Xu, Hai-Hua Xu, Bin Ma,\n  Haizhou Li, Sylvain Meignier", "title": "Fantastic 4 system for NIST 2015 Language Recognition Evaluation", "comments": "Technical report for NIST LRE 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the systems jointly submitted by Institute for\nInfocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\\'e du Maine\n(LIUM), Nanyang Technology University (NTU) and the University of Eastern\nFinland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The\nsubmitted system is a fusion of nine sub-systems based on i-vectors extracted\nfrom different types of features. Given the i-vectors, several classifiers are\nadopted for the language detection task including support vector machines\n(SVM), multi-class logistic regression (MCLR), Probabilistic Linear\nDiscriminant Analysis (PLDA) and Deep Neural Networks (DNN).\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 06:02:51 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Lee", "Kong Aik", ""], ["Hautam\u00e4ki", "Ville", ""], ["Larcher", "Anthony", ""], ["Rao", "Wei", ""], ["Sun", "Hanwu", ""], ["Nguyen", "Trung Hieu", ""], ["Wang", "Guangsen", ""], ["Sizov", "Aleksandr", ""], ["Kukanov", "Ivan", ""], ["Poorjam", "Amir", ""], ["Trong", "Trung Ngo", ""], ["Xiao", "Xiong", ""], ["Xu", "Cheng-Lin", ""], ["Xu", "Hai-Hua", ""], ["Ma", "Bin", ""], ["Li", "Haizhou", ""], ["Meignier", "Sylvain", ""]]}, {"id": "1602.02047", "submitter": "Elvys Linhares Pontes", "authors": "Elvys Linhares Pontes", "title": "Utiliza\\c{c}\\~ao de Grafos e Matriz de Similaridade na Sumariza\\c{c}\\~ao\n  Autom\\'atica de Documentos Baseada em Extra\\c{c}\\~ao de Frases", "comments": "Dissertation, 83 pages, in Portuguese. in Disserta\\c{c}\\~ao de\n  Mestrado, Universidade Federal do Cear\\'a, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet increased the amount of information available. However, the\nreading and understanding of this information are costly tasks. In this\nscenario, the Natural Language Processing (NLP) applications enable very\nimportant solutions, highlighting the Automatic Text Summarization (ATS), which\nproduce a summary from one or more source texts. Automatically summarizing one\nor more texts, however, is a complex task because of the difficulties inherent\nto the analysis and generation of this summary. This master's thesis describes\nthe main techniques and methodologies (NLP and heuristics) to generate\nsummaries. We have also addressed and proposed some heuristics based on graphs\nand similarity matrix to measure the relevance of judgments and to generate\nsummaries by extracting sentences. We used the multiple languages (English,\nFrench and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA\n(French) corpus to evaluate the developped systems. The results obtained were\nquite interesting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 14:54:57 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Pontes", "Elvys Linhares", ""]]}, {"id": "1602.02068", "submitter": "Ram\\'on Fernandez Astudillo", "authors": "Andr\\'e F. T. Martins and Ram\\'on Fernandez Astudillo", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label\n  Classification", "comments": "Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 15:49:02 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 09:41:36 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Martins", "Andr\u00e9 F. T.", ""], ["Astudillo", "Ram\u00f3n Fernandez", ""]]}, {"id": "1602.02089", "submitter": "Martha Lewis", "authors": "Martha Lewis, Bob Coecke", "title": "Harmonic Grammar in a DisCo Model of Meaning", "comments": "Abstract, Advances in Distributional Semantics, IWCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of cognition developed in (Smolensky and Legendre, 2006) seeks to\nunify two levels of description of the cognitive process: the connectionist and\nthe symbolic. The theory developed brings together these two levels into the\nIntegrated Connectionist/Symbolic Cognitive architecture (ICS). Clark and\nPulman (2007) draw a parallel with semantics where meaning may be modelled on\nboth distributional and symbolic levels, developed by Coecke et al, 2010 into\nthe Distributional Compositional (DisCo) model of meaning. In the current work,\nwe revisit Smolensky and Legendre (S&L)'s model. We describe the DisCo\nframework, summarise the key ideas in S&L's architecture, and describe how\ntheir description of harmony as a graded measure of grammaticality may be\napplied in the DisCo model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 16:40:44 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Lewis", "Martha", ""], ["Coecke", "Bob", ""]]}, {"id": "1602.02133", "submitter": "Issa Atoum", "authors": "Issa Atoum and Ahmed Otoom", "title": "Mining Software Quality from Software Reviews: Research Trends and Open\n  Issues", "comments": "11 pages", "journal-ref": "International Journal of Computer Trends and Technology,Vol. 31,\n  No. 2, Jan 2016", "doi": "10.14445/22312803/IJCTT-V31P114", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software review text fragments have considerably valuable information about\nusers experience. It includes a huge set of properties including the software\nquality. Opinion mining or sentiment analysis is concerned with analyzing\ntextual user judgments. The application of sentiment analysis on software\nreviews can find a quantitative value that represents software quality.\nAlthough many software quality methods are proposed they are considered\ndifficult to customize and many of them are limited. This article investigates\nthe application of opinion mining as an approach to extract software quality\nproperties. We found that the major issues of software reviews mining using\nsentiment analysis are due to software lifecycle and the diverse users and\nteams.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:42:24 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Atoum", "Issa", ""], ["Otoom", "Ahmed", ""]]}, {"id": "1602.02215", "submitter": "Chris Waterson", "authors": "Noam Shazeer, Ryan Doherty, Colin Evans, Chris Waterson", "title": "Swivel: Improving Embeddings by Noticing What's Missing", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Submatrix-wise Vector Embedding Learner (Swivel), a method for\ngenerating low-dimensional feature embeddings from a feature co-occurrence\nmatrix. Swivel performs approximate factorization of the point-wise mutual\ninformation matrix via stochastic gradient descent. It uses a piecewise loss\nwith special handling for unobserved co-occurrences, and thus makes use of all\nthe information in the matrix. While this requires computation proportional to\nthe size of the entire matrix, we make use of vectorized multiplication to\nprocess thousands of rows and columns at once to compute millions of predicted\nvalues. Furthermore, we partition the matrix into shards in order to\nparallelize the computation across many nodes. This approach results in more\naccurate embeddings than can be achieved with methods that consider only\nobserved co-occurrences, and can scale to much larger corpora than can be\nhandled with sampling methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 04:39:41 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Shazeer", "Noam", ""], ["Doherty", "Ryan", ""], ["Evans", "Colin", ""], ["Waterson", "Chris", ""]]}, {"id": "1602.02332", "submitter": "Antti Puurula", "authors": "Antti Puurula", "title": "Scalable Text Mining with Sparse Generative Models", "comments": "PhD Thesis, Computer Science, University of Waikato, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The information age has brought a deluge of data. Much of this is in text\nform, insurmountable in scope for humans and incomprehensible in structure for\ncomputers. Text mining is an expanding field of research that seeks to utilize\nthe information contained in vast document collections. General data mining\nmethods based on machine learning face challenges with the scale of text data,\nposing a need for scalable text mining methods.\n  This thesis proposes a solution to scalable text mining: generative models\ncombined with sparse computation. A unifying formalization for generative text\nmodels is defined, bringing together research traditions that have used\nformally equivalent models, but ignored parallel developments. This framework\nallows the use of methods developed in different processing tasks such as\nretrieval and classification, yielding effective solutions across different\ntext mining tasks. Sparse computation using inverted indices is proposed for\ninference on probabilistic models. This reduces the computational complexity of\nthe common text mining operations according to sparsity, yielding probabilistic\nmodels with the scalability of modern search engines.\n  The proposed combination provides sparse generative models: a solution for\ntext mining that is general, effective, and scalable. Extensive experimentation\non text classification and ranked retrieval datasets are conducted, showing\nthat the proposed solution matches or outperforms the leading task-specific\nmethods in effectiveness, with a order of magnitude decrease in classification\ntimes for Wikipedia article categorization with a million classes. The\ndeveloped methods were further applied in two 2014 Kaggle data mining prize\ncompetitions with over a hundred competing teams, earning first and second\nplaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 02:49:27 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Puurula", "Antti", ""]]}, {"id": "1602.02373", "submitter": "Rie Johnson", "authors": "Rie Johnson, Tong Zhang", "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-hot CNN (convolutional neural network) has been shown to be effective for\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\ngeneral framework which jointly trains a linear model with a non-linear feature\ngenerator consisting of `text region embedding + pooling'. Under this\nframework, we explore a more sophisticated region embedding method using Long\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\neffective and efficient use of LSTM for this purpose in the supervised and\nsemi-supervised settings. The best results were obtained by combining region\nembeddings in the form of LSTM and convolution layers trained on unlabeled\ndata. The results indicate that on this task, embeddings of text regions, which\ncan convey complex concepts, are more useful than embeddings of single words in\nisolation. We report performances exceeding the previous best results on four\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 14:05:58 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 15:26:34 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1602.02410", "submitter": "Oriol Vinyals", "authors": "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui\n  Wu", "title": "Exploring the Limits of Language Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 19:11:17 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 23:01:48 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Jozefowicz", "Rafal", ""], ["Vinyals", "Oriol", ""], ["Schuster", "Mike", ""], ["Shazeer", "Noam", ""], ["Wu", "Yonghui", ""]]}, {"id": "1602.02499", "submitter": "David van Leeuwen", "authors": "David A. van Leeuwen and Rosemary Orr", "title": "The \"Sprekend Nederland\" project and its application to accent location", "comments": "Accepted to Speaker and Language Recognition Odyssey 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the data collection effort that is part of the project\nSprekend Nederland (The Netherlands Talking), and discusses its potential use\nin Automatic Accent Location. We define Automatic Accent Location as the task\nto describe the accent of a speaker in terms of the location of the speaker and\nits history. We discuss possible ways of describing accent location, the\nconsequence these have for the task of automatic accent location, and potential\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 09:23:19 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 13:17:20 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["van Leeuwen", "David A.", ""], ["Orr", "Rosemary", ""]]}, {"id": "1602.02665", "submitter": "Bruno Gon\\c{c}alves", "authors": "Johan Bollen, Bruno Gon\\c{c}alves, Ingrid van de Leemput, Guangchen\n  Ruan", "title": "The happiness paradox: your friends are happier than you", "comments": "15 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most individuals in social networks experience a so-called Friendship\nParadox: they are less popular than their friends on average. This effect may\nexplain recent findings that widespread social network media use leads to\nreduced happiness. However the relation between popularity and happiness is\npoorly understood. A Friendship paradox does not necessarily imply a Happiness\nparadox where most individuals are less happy than their friends. Here we\nreport the first direct observation of a significant Happiness Paradox in a\nlarge-scale online social network of $39,110$ Twitter users. Our results reveal\nthat popular individuals are indeed happier and that a majority of individuals\nexperience a significant Happiness paradox. The magnitude of the latter effect\nis shaped by complex interactions between individual popularity, happiness, and\nthe fact that users cluster assortatively by level of happiness. Our results\nindicate that the topology of online social networks and the distribution of\nhappiness in some populations can cause widespread psycho-social effects that\naffect the well-being of billions of individuals.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:46:18 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bollen", "Johan", ""], ["Gon\u00e7alves", "Bruno", ""], ["van de Leemput", "Ingrid", ""], ["Ruan", "Guangchen", ""]]}, {"id": "1602.02850", "submitter": "Bo Tang", "authors": "Bo Tang, Steven Kay, and Haibo He", "title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization", "comments": "This paper has been submitted to the IEEE Trans. Knowledge and Data\n  Engineering. 14 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TKDE.2016.2563436", "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 03:43:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tang", "Bo", ""], ["Kay", "Steven", ""], ["He", "Haibo", ""]]}, {"id": "1602.03001", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Hao Peng, Charles Sutton", "title": "A Convolutional Attention Network for Extreme Summarization of Source\n  Code", "comments": "Code, data and visualization at\n  http://groups.inf.ed.ac.uk/cup/codeattention/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms in neural networks have proved useful for problems in\nwhich the input and output do not have fixed dimension. Often there exist\nfeatures that are locally translation invariant and would be valuable for\ndirecting the model's attention, but previous attentional architectures are not\nconstructed to learn such features specifically. We introduce an attentional\nneural network that employs convolution on the input tokens to detect local\ntime-invariant and long-range topical attention features in a context-dependent\nway. We apply this architecture to the problem of extreme summarization of\nsource code snippets into short, descriptive function name-like summaries.\nUsing those features, the model sequentially generates a summary by\nmarginalizing over two attention mechanisms: one that predicts the next summary\ntoken based on the attention weights of the input tokens and another that is\nable to copy a code token as-is directly into the summary. We demonstrate our\nconvolutional attention neural network's performance on 10 popular Java\nprojects showing that it achieves better performance compared to previous\nattentional mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:36:49 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:18:28 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Peng", "Hao", ""], ["Sutton", "Charles", ""]]}, {"id": "1602.03265", "submitter": "Aida Nematzadeh", "authors": "Aida Nematzadeh and Filip Miscevic and Suzanne Stevenson", "title": "Simple Search Algorithms on Semantic Networks Learned from Language Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical and modeling research has focused on the semantic fluency\ntask because it is informative about semantic memory. An interesting interplay\narises between the richness of representations in semantic memory and the\ncomplexity of algorithms required to process it. It has remained an open\nquestion whether representations of words and their relations learned from\nlanguage use can enable a simple search algorithm to mimic the observed\nbehavior in the fluency task. Here we show that it is plausible to learn rich\nrepresentations from naturalistic data for which a very simple search algorithm\n(a random walk) can replicate the human patterns. We suggest that explicitly\nstructuring knowledge about words into a semantic network plays a crucial role\nin modeling human behavior in memory search and retrieval; moreover, this is\nthe case across a range of semantic information sources.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 04:54:15 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 04:46:21 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Nematzadeh", "Aida", ""], ["Miscevic", "Filip", ""], ["Stevenson", "Suzanne", ""]]}, {"id": "1602.03426", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Pushpak Bhattacharyya, Mark James Carman", "title": "Automatic Sarcasm Detection: A Survey", "comments": "This paper is likely to be submitted to ACM CSUR. This copy on arXiv\n  is to obtain feedback from stakeholders", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic sarcasm detection is the task of predicting sarcasm in text. This\nis a crucial step to sentiment analysis, considering prevalence and challenges\nof sarcasm in sentiment-bearing text. Beginning with an approach that used\nspeech-based features, sarcasm detection has witnessed great interest from the\nsentiment analysis community. This paper is the first known compilation of past\nwork in automatic sarcasm detection. We observe three milestones in the\nresearch so far: semi-supervised pattern extraction to identify implicit\nsentiment, use of hashtag-based supervision, and use of context beyond target\ntext. In this paper, we describe datasets, approaches, trends and issues in\nsarcasm detection. We also discuss representative performance values, shared\ntasks and pointers to future work, as given in prior works. In terms of\nresources that could be useful for understanding state-of-the-art, the survey\npresents several useful illustrations - most prominently, a table that\nsummarizes past papers along different dimensions such as features, annotation\ntechniques, data forms, etc.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 16:02:46 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 22:15:52 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Joshi", "Aditya", ""], ["Bhattacharyya", "Pushpak", ""], ["Carman", "Mark James", ""]]}, {"id": "1602.03483", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen", "title": "Learning Distributed Representations of Sentences from Unlabelled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:49:58 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Hill", "Felix", ""], ["Cho", "Kyunghyun", ""], ["Korhonen", "Anna", ""]]}, {"id": "1602.03551", "submitter": "Stephanie L. Hyland", "authors": "Stephanie L. Hyland, Theofanis Karaletsos, Gunnar R\\\"atsch", "title": "Knowledge Transfer with Medical Language Embeddings", "comments": "6 pages, 2 figures, to appear at SDM-DMMH 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying relationships between concepts is a key aspect of scientific\nknowledge synthesis. Finding these links often requires a researcher to\nlaboriously search through scien- tific papers and databases, as the size of\nthese resources grows ever larger. In this paper we describe how distributional\nsemantics can be used to unify structured knowledge graphs with unstructured\ntext to predict new relationships between medical concepts, using a\nprobabilistic generative model. Our approach is also designed to ameliorate\ndata sparsity and scarcity issues in the medical domain, which make language\nmodelling more challenging. Specifically, we integrate the medical relational\ndatabase (SemMedDB) with text from electronic health records (EHRs) to perform\nknowledge graph completion. We further demonstrate the ability of our model to\npredict relationships between tokens not appearing in the relational database.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 22:02:29 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Hyland", "Stephanie L.", ""], ["Karaletsos", "Theofanis", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1602.03606", "submitter": "Federico Barrios", "authors": "Federico Barrios, Federico L\\'opez, Luis Argerich, Rosa Wachenchauzer", "title": "Variations of the Similarity Function of TextRank for Automated\n  Summarization", "comments": "8 pages, 2 figures. Presented at the Argentine Symposium on\n  Artificial Intelligence (ASAI) 2015 - 44 JAIIO (September 2015)", "journal-ref": "44 JAIIO - ASAI 2015 - ISSN: 2451-7585, pages 65-72", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents new alternatives to the similarity function for the\nTextRank algorithm for automatic summarization of texts. We describe the\ngeneralities of the algorithm and the different functions we propose. Some of\nthese variants achieve a significative improvement using the same metrics and\ndataset as the original publication.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 02:39:21 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Barrios", "Federico", ""], ["L\u00f3pez", "Federico", ""], ["Argerich", "Luis", ""], ["Wachenchauzer", "Rosa", ""]]}, {"id": "1602.03609", "submitter": "Cicero dos Santos", "authors": "Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou", "title": "Attentive Pooling Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Attentive Pooling (AP), a two-way attention\nmechanism for discriminative model training. In the context of pair-wise\nranking or classification with neural networks, AP enables the pooling layer to\nbe aware of the current input pair, in a way that information from the two\ninput items can directly influence the computation of each other's\nrepresentations. Along with such representations of the paired inputs, AP\njointly learns a similarity measure over projected segments (e.g. trigrams) of\nthe pair, and subsequently, derives the corresponding attention vector for each\ninput to guide the pooling. Our two-way attention mechanism is a general\nframework independent of the underlying representation learning, and it has\nbeen applied to both convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) in our studies. The empirical results, from three very\ndifferent benchmark tasks of question answering/answer selection, demonstrate\nthat our proposed models outperform a variety of strong baselines and achieve\nstate-of-the-art performance in all the benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 03:06:33 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Santos", "Cicero dos", ""], ["Tan", "Ming", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1602.03661", "submitter": "Vito Domenico Pietro Servedio", "authors": "Vittorio Loreto, Pietro Gravino, Vito D.P. Servedio, Francesca Tria", "title": "On the emergence of syntactic structures: quantifying and modelling\n  duality of patterning", "comments": "11 pages, to appear in the Proceedings of the Workshop on Origins of\n  Communication Systems: Modeling and Ethologically-based Theory, Konrad Lorenz\n  Institute for Evolution and Cognition Research, Altenberg, Austria (2013).\n  Special Issue of topiCS on New frontiers in language evolution and\n  development", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex organization of syntax in hierarchical structures is one of the\ncore design features of human language. Duality of patterning refers for\ninstance to the organization of the meaningful elements in a language at two\ndistinct levels: a combinatorial level where meaningless forms are combined\ninto meaningful forms and a compositional level where meaningful forms are\ncomposed into larger lexical units. The question remains wide open regarding\nhow such a structure could have emerged. Furthermore a clear mathematical\nframework to quantify this phenomenon is still lacking. The aim of this paper\nis that of addressing these two aspects in a self-consistent way. First, we\nintroduce suitable measures to quantify the level of combinatoriality and\ncompositionality in a language, and present a framework to estimate these\nobservables in human natural languages. Second, we show that the theoretical\npredictions of a multi-agents modeling scheme, namely the Blending Game, are in\nsurprisingly good agreement with empirical data. In the Blending Game a\npopulation of individuals plays language games aiming at success in\ncommunication. It is remarkable that the two sides of duality of patterning\nemerge simultaneously as a consequence of a pure cultural dynamics in a\nsimulated environment that contains meaningful relations, provided a simple\nconstraint on message transmission fidelity is also considered.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 10:15:03 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Loreto", "Vittorio", ""], ["Gravino", "Pietro", ""], ["Servedio", "Vito D. P.", ""], ["Tria", "Francesca", ""]]}, {"id": "1602.03960", "submitter": "Sujay Kumar Jauhar", "authors": "Sujay Kumar Jauhar, Peter Turney and Eduard Hovy", "title": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice\n  Questions", "comments": "Keywords: Data, General Knowledge, Tables, Question Answering, MCQ,\n  Crowd-sourcing, Mechanical Turk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two new related resources that facilitate modelling of general\nknowledge reasoning in 4th grade science exams. The first is a collection of\ncurated facts in the form of tables, and the second is a large set of\ncrowd-sourced multiple-choice questions covering the facts in the tables.\nThrough the setup of the crowd-sourced annotation task we obtain implicit\nalignment information between questions and tables. We envisage that the\nresources will be useful not only to researchers working on question answering,\nbut also to people investigating a diverse range of other applications such as\ninformation extraction, question parsing, answer type identification, and\nlexical semantic modelling.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 03:54:43 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Jauhar", "Sujay Kumar", ""], ["Turney", "Peter", ""], ["Hovy", "Eduard", ""]]}, {"id": "1602.04101", "submitter": "Tai Wang", "authors": "Tai Wang, Xiangen Hu, Keith Shubeck, Zhiqiang Cai, Jie Tang", "title": "An Empirical Study on Academic Commentary and Its Implications on\n  Reading and Writing", "comments": "22 pages, 5 figures, 6 tables. An extension of an oral presentation\n  \"a simple model for social media\" in 45th Annual Mathematical Psychology\n  Meeting,2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between reading and writing (RRW) is one of the major themes\nin learning science. One of its obstacles is that it is difficult to define or\nmeasure the latent background knowledge of the individual. However, in an\nacademic research setting, scholars are required to explicitly list their\nbackground knowledge in the citation sections of their manuscripts. This unique\nopportunity was taken advantage of to observe RRW, especially in the published\nacademic commentary scenario. RRW was visualized under a proposed topic process\nmodel by using a state of the art version of latent Dirichlet allocation (LDA).\nThe empirical study showed that the academic commentary is modulated both by\nits target paper and the author's background knowledge. Although this\nconclusion was obtained in a unique environment, we suggest its implications\ncan also shed light on other similar interesting areas, such as dialog and\nconversation, group discussion, and social media.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:18:00 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wang", "Tai", ""], ["Hu", "Xiangen", ""], ["Shubeck", "Keith", ""], ["Cai", "Zhiqiang", ""], ["Tang", "Jie", ""]]}, {"id": "1602.04278", "submitter": "Taehwan Kim", "authors": "Taehwan Kim, Weiran Wang, Hao Tang, Karen Livescu", "title": "Signer-independent Fingerspelling Recognition with Deep Neural Network\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognition of fingerspelled letter sequences in\nAmerican Sign Language in a signer-independent setting. Fingerspelled sequences\nare both challenging and important to recognize, as they are used for many\ncontent words such as proper nouns and technical terms. Previous work has shown\nthat it is possible to achieve almost 90% accuracies on fingerspelling\nrecognition in a signer-dependent setting. However, the more realistic\nsigner-independent setting presents challenges due to significant variations\namong signers, coupled with the dearth of available training data. We\ninvestigate this problem with approaches inspired by automatic speech\nrecognition. We start with the best-performing approaches from prior work,\nbased on tandem models and segmental conditional random fields (SCRFs), with\nfeatures based on deep neural network (DNN) classifiers of letters and\nphonological features. Using DNN adaptation, we find that it is possible to\nbridge a large part of the gap between signer-dependent and signer-independent\nperformance. Using only about 115 transcribed words for adaptation from the\ntarget signer, we obtain letter accuracies of up to 82.7% with frame-level\nadaptation labels and 69.7% with only word labels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:30:34 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Kim", "Taehwan", ""], ["Wang", "Weiran", ""], ["Tang", "Hao", ""], ["Livescu", "Karen", ""]]}, {"id": "1602.04341", "submitter": "Wenpeng Yin", "authors": "Wenpeng Yin, Sebastian Ebert, Hinrich Sch\\\"utze", "title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding open-domain text is one of the primary challenges in natural\nlanguage processing (NLP). Machine comprehension benchmarks evaluate the\nsystem's ability to understand text based on the text content only. In this\nwork, we investigate machine comprehension on MCTest, a question answering (QA)\nbenchmark. Prior work is mainly based on feature engineering approaches. We\ncome up with a neural network framework, named hierarchical attention-based\nconvolutional neural network (HABCNN), to address this task without any\nmanually designed features. Specifically, we explore HABCNN for this task by\ntwo routes, one is through traditional joint modeling of passage, question and\nanswer, one is through textual entailment. HABCNN employs an attention\nmechanism to detect key phrases, key sentences and key snippets that are\nrelevant to answering the question. Experiments show that HABCNN outperforms\nprior deep learning approaches by a big margin.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 14:38:47 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Yin", "Wenpeng", ""], ["Ebert", "Sebastian", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1602.04375", "submitter": "Mrinmaya Sachan", "authors": "Mrinmaya Sachan, Avinava Dubey, Eric P. Xing", "title": "Science Question Answering using Instructional Materials", "comments": "Corrected that the science QA dataset is NOT freely available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a solution for elementary science test using instructional\nmaterials. We posit that there is a hidden structure that explains the\ncorrectness of an answer given the question and instructional materials and\npresent a unified max-margin framework that learns to find these hidden\nstructures (given a corpus of question-answer pairs and instructional\nmaterials), and uses what it learns to answer novel elementary science\nquestions. Our evaluation shows that our framework outperforms several strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 20:13:48 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 01:17:56 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Sachan", "Mrinmaya", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1602.04427", "submitter": "Zheng Xu", "authors": "Zheng Xu and Douglas Burdick and Louiqa Raschid", "title": "Exploiting Lists of Names for Named Entity Identification of Financial\n  Institutions from Unstructured Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a wealth of information about financial systems that is embedded in\ndocument collections. In this paper, we focus on a specialized text extraction\ntask for this domain. The objective is to extract mentions of names of\nfinancial institutions, or FI names, from financial prospectus documents, and\nto identify the corresponding real world entities, e.g., by matching against a\ncorpus of such entities. The tasks are Named Entity Recognition (NER) and\nEntity Resolution (ER); both are well studied in the literature. Our\ncontribution is to develop a rule-based approach that will exploit lists of FI\nnames for both tasks; our solution is labeled Dict-based NER and Rank-based ER.\nSince the FI names are typically represented by a root, and a suffix that\nmodifies the root, we use these lists of FI names to create specialized root\nand suffix dictionaries. To evaluate the effectiveness of our specialized\nsolution for extracting FI names, we compare Dict-based NER with a general\npurpose rule-based NER solution, ORG NER. Our evaluation highlights the\nbenefits and limitations of specialized versus general purpose approaches, and\npresents additional suggestions for tuning and customization for FI name\nextraction. To our knowledge, our proposed solutions, Dict-based NER and\nRank-based ER, and the root and suffix dictionaries, are the first attempt to\nexploit specialized knowledge, i.e., lists of FI names, for rule-based NER and\nER.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 07:31:28 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 06:33:34 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Xu", "Zheng", ""], ["Burdick", "Douglas", ""], ["Raschid", "Louiqa", ""]]}, {"id": "1602.04709", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti, Amir A. Delay, Fatemeh Seyedmendhi", "title": "Identifying Structures in Social Conversations in NSCLC Patients through\n  the Semi-Automatic extraction of Topical Taxonomies", "comments": "7 pages, 7 figures, 1 table", "journal-ref": "Journal of Engineering Research and Applications, Vol. 6, Issue 1,\n  January 2016, pp.20-26", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration of social conversations for addressing patient's needs is an\nimportant analytical task in which many scholarly publications are contributing\nto fill the knowledge gap in this area. The main difficulty remains the\ninability to turn such contributions into pragmatic processes the\npharmaceutical industry can leverage in order to generate insight from social\nmedia data, which can be considered as one of the most challenging source of\ninformation available today due to its sheer volume and noise. This study is\nbased on the work by Scott Spangler and Jeffrey Kreulen and applies it to\nidentify structure in social media through the extraction of a topical taxonomy\nable to capture the latent knowledge in social conversations in health-related\nsites. The mechanism for automatically identifying and generating a taxonomy\nfrom social conversations is developed and pressured tested using public data\nfrom media sites focused on the needs of cancer patients and their families.\nMoreover, a novel method for generating the category's label and the\ndetermination of an optimal number of categories is presented which extends\nScott and Jeffrey's research in a meaningful way. We assume the reader is\nfamiliar with taxonomies, what they are and how they are used.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 19:56:49 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Crocetti", "Giancarlo", ""], ["Delay", "Amir A.", ""], ["Seyedmendhi", "Fatemeh", ""]]}, {"id": "1602.04853", "submitter": "Vasyl Palchykov", "authors": "Yurij Holovatch and Vasyl Palchykov", "title": "Complex Networks of Words in Fables", "comments": "16 pages, 4 figures and 2 tables. To appear in: \"Maths Meets Myths:\n  Complexity-science approaches to folktales, myths, sagas, and histories.\"\n  Editors: R. Kenna, M. Mac Carron, P. Mac Carron. (Springer, 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we give an overview of the application of complex network\ntheory to quantify some properties of language. Our study is based on two\nfables in Ukrainian, Mykyta the Fox and Abu-Kasym's slippers. It consists of\ntwo parts: the analysis of frequency-rank distributions of words and the\napplication of complex-network theory. The first part shows that the text sizes\nare sufficiently large to observe statistical properties. This supports their\nselection for the analysis of typical properties of the language networks in\nthe second part of the chapter. In describing language as a complex network,\nwhile words are usually associated with nodes, there is more variability in the\nchoice of links and different representations result in different networks.\nHere, we examine a number of such representations of the language network and\nperform a comparative analysis of their characteristics. Our results suggest\nthat, irrespective of link representation, the Ukrainian language network used\nin the selected fables is a strongly correlated, scale-free, small world. We\ndiscuss how such empirical approaches may help form a useful basis for a\ntheoretical description of language evolution and how they may be used in\nanalyses of other textual narratives.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 14:32:57 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Holovatch", "Yurij", ""], ["Palchykov", "Vasyl", ""]]}, {"id": "1602.04874", "submitter": "Yushi Yao", "authors": "Yushi Yao, Zheng Huang", "title": "Bi-directional LSTM Recurrent Neural Network for Chinese Word\n  Segmentation", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network(RNN) has been broadly applied to natural language\nprocessing(NLP) problems. This kind of neural network is designed for modeling\nsequential data and has been testified to be quite efficient in sequential\ntagging tasks. In this paper, we propose to use bi-directional RNN with long\nshort-term memory(LSTM) units for Chinese word segmentation, which is a crucial\npreprocess task for modeling Chinese sentences and articles. Classical methods\nfocus on designing and combining hand-craft features from context, whereas\nbi-directional LSTM network(BLSTM) does not need any prior knowledge or\npre-designing, and it is expert in keeping the contextual information in both\ndirections. Experiment result shows that our approach gets state-of-the-art\nperformance in word segmentation on both traditional Chinese datasets and\nsimplified Chinese datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 00:45:19 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Yao", "Yushi", ""], ["Huang", "Zheng", ""]]}, {"id": "1602.04930", "submitter": "Hai-Jun Zhou", "authors": "Yi-Zhi Xu and Hai-Jun Zhou", "title": "Generalized minimum dominating set and application in automatic text\n  summarization", "comments": "11 pages, including 4 figures and 2 tables. To be published in\n  Journal of Physics: Conference Series", "journal-ref": null, "doi": "10.1088/1742-6596/699/1/012014", "report-no": null, "categories": "cs.IR cond-mat.stat-mech cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph formed by vertices and weighted edges, a generalized minimum\ndominating set (MDS) is a vertex set of smallest cardinality such that the\nsummed weight of edges from each outside vertex to vertices in this set is\nequal to or larger than certain threshold value. This generalized MDS problem\nreduces to the conventional MDS problem in the limiting case of all the edge\nweights being equal to the threshold value. We treat the generalized MDS\nproblem in the present paper by a replica-symmetric spin glass theory and\nderive a set of belief-propagation equations. As a practical application we\nconsider the problem of extracting a set of sentences that best summarize a\ngiven input text document. We carry out a preliminary test of the statistical\nphysics-inspired method to this automatic text summarization problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 07:43:29 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Xu", "Yi-Zhi", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1602.04983", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario\n  Fritz", "title": "Contextual Media Retrieval Using Natural Language Queries", "comments": "8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread integration of cameras in hand-held and head-worn devices as\nwell as the ability to share content online enables a large and diverse visual\ncapture of the world that millions of users build up collectively every day. We\nenvision these images as well as associated meta information, such as GPS\ncoordinates and timestamps, to form a collective visual memory that can be\nqueried while automatically taking the ever-changing context of mobile users\ninto account. As a first step towards this vision, in this work we present\nXplore-M-Ego: a novel media retrieval system that allows users to query a\ndynamic database of images and videos using spatio-temporal natural language\nqueries. We evaluate our system using a new dataset of real user queries as\nwell as through a usability study. One key finding is that there is a\nconsiderable amount of inter-user variability, for example in the resolution of\nspatial relations in natural language utterances. We show that our retrieval\nsystem can cope with this variability using personalisation through an online\nlearning-based retrieval formulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 11:04:29 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Malinowski", "Mateusz", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1602.05292", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Yufang Sun and Mark J. T. Smith", "title": "Authorship Attribution Using a Neural Network Language Model", "comments": "Proceedings of the 30th AAAI Conference on Artificial Intelligence\n  (AAAI'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, training language models for individual authors is often\nexpensive because of limited data resources. In such cases, Neural Network\nLanguage Models (NNLMs), generally outperform the traditional non-parametric\nN-gram models. Here we investigate the performance of a feed-forward NNLM on an\nauthorship attribution problem, with moderate author set size and relatively\nlimited data. We also consider how the text topics impact performance. Compared\nwith a well-constructed N-gram baseline method with Kneser-Ney smoothing, the\nproposed method achieves nearly 2:5% reduction in perplexity and increases\nauthor classification accuracy by 3:43% on average, given as few as 5 test\nsentences. The performance is very competitive with the state of the art in\nterms of accuracy and demand on test data. The source code, preprocessed\ndatasets, a detailed description of the methodology and results are available\nat https://github.com/zge/authorship-attribution.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 04:06:28 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sun", "Yufang", ""], ["Smith", "Mark J. T.", ""]]}, {"id": "1602.05307", "submitter": "Xiang Ren", "authors": "Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Jiawei Han", "title": "Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label\n  Embedding", "comments": "Submitted to KDD 2016. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current systems of fine-grained entity typing use distant supervision in\nconjunction with existing knowledge bases to assign categories (type labels) to\nentity mentions. However, the type labels so obtained from knowledge bases are\noften noisy (i.e., incorrect for the entity mention's local context). We define\na new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic\nidentification of correct type labels (type-paths) for training examples, given\nthe set of candidate type labels obtained by distant supervision with a given\ntype hierarchy. The unknown type labels for individual entity mentions and the\nsemantic similarity between entity types pose unique challenges for solving the\nLNR task. We propose a general framework, called PLE, to jointly embed entity\nmentions, text features and entity types into the same low-dimensional space\nwhere, in that space, objects whose types are semantically close have similar\nrepresentations. Then we estimate the type-path for each training example in a\ntop-down manner using the learned embeddings. We formulate a global objective\nfor learning the embeddings from text corpora and knowledge bases, which adopts\na novel margin-based loss that is robust to noisy labels and faithfully models\ntype correlation derived from knowledge bases. Our experiments on three public\ntyping datasets demonstrate the effectiveness and robustness of PLE, with an\naverage of 25% improvement in accuracy compared to next best method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 05:26:47 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Ren", "Xiang", ""], ["He", "Wenqi", ""], ["Qu", "Meng", ""], ["Voss", "Clare R.", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "1602.05388", "submitter": "Muhammad Imran", "authors": "Muhammad Imran, Prasenjit Mitra, Jaideep Srivastava", "title": "Cross-Language Domain Adaptation for Classifying Crisis-Related Short\n  Messages", "comments": "ISCRAM 2016, 10 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid crisis response requires real-time analysis of messages. After a\ndisaster happens, volunteers attempt to classify tweets to determine needs,\ne.g., supplies, infrastructure damage, etc. Given labeled data, supervised\nmachine learning can help classify these messages. Scarcity of labeled data\ncauses poor performance in machine training. Can we reuse old tweets to train\nclassifiers? How can we choose labeled tweets for training? Specifically, we\nstudy the usefulness of labeled data of past events. Do labeled tweets in\ndifferent language help? We observe the performance of our classifiers trained\nusing different combinations of training sets obtained from past disasters. We\nperform extensive experimentation on real crisis datasets and show that the\npast labels are useful when both source and target events are of the same type\n(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),\ncross-language domain adaptation was useful, however, when for different\nlanguages (e.g., Italian and English), the performance decreased.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 12:29:56 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 07:18:43 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Imran", "Muhammad", ""], ["Mitra", "Prasenjit", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "1602.05753", "submitter": "Toma\\v{z} Erjavec", "authors": "Mark A. Finlayson and Toma\\v{z} Erjavec", "title": "Overview of Annotation Creation: Processes & Tools", "comments": "To appear in: James Pustejovsky and Nancy Ide (eds.) \"Handbook of\n  Linguistic Annotation.\" 2016. New York: Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating linguistic annotations requires more than just a reliable annotation\nscheme. Annotation can be a complex endeavour potentially involving many\npeople, stages, and tools. This chapter outlines the process of creating\nend-to-end linguistic annotations, identifying specific tasks that researchers\noften perform. Because tool support is so central to achieving high quality,\nreusable annotations with low cost, the focus is on identifying capabilities\nthat are necessary or useful for annotation tools, as well as common problems\nthese tools present that reduce their utility. Although examples of specific\ntools are provided in many cases, this chapter concentrates more on abstract\ncapabilities and problems because new tools appear continuously, while old\ntools disappear into disuse or disrepair. The two core capabilities tools must\nhave are support for the chosen annotation scheme and the ability to work on\nthe language under study. Additional capabilities are organized into three\ncategories: those that are widely provided; those that often useful but found\nin only a few tools; and those that have as yet little or no available tool\nsupport.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 10:56:46 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Finlayson", "Mark A.", ""], ["Erjavec", "Toma\u017e", ""]]}, {"id": "1602.05765", "submitter": "Steven Schockaert", "authors": "Shoaib Jameel, Steven Schockaert", "title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible\n  Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceptual spaces are geometric representations of conceptual knowledge, in\nwhich entities correspond to points, natural properties correspond to convex\nregions, and the dimensions of the space correspond to salient features. While\nconceptual spaces enable elegant models of various cognitive phenomena, the\nlack of automated methods for constructing such representations have so far\nlimited their application in artificial intelligence. To address this issue, we\npropose a method which learns a vector-space embedding of entities from\nWikipedia and constrains this embedding such that entities of the same semantic\ntype are located in some lower-dimensional subspace. We experimentally\ndemonstrate the usefulness of these subspaces as (approximate) conceptual space\nrepresentations by showing, among others, that important features can be\nmodelled as directions and that natural properties tend to correspond to convex\nregions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 11:37:50 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 13:48:21 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Jameel", "Shoaib", ""], ["Schockaert", "Steven", ""]]}, {"id": "1602.05772", "submitter": "Stefan Gerdjikov", "authors": "Stefan Gerdjikov and Klaus U. Schulz", "title": "Corpus analysis without prior linguistic knowledge - unsupervised mining\n  of phrases and subphrase structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When looking at the structure of natural language, \"phrases\" and \"words\" are\ncentral notions. We consider the problem of identifying such \"meaningful\nsubparts\" of language of any length and underlying composition principles in a\ncompletely corpus-based and language-independent way without using any kind of\nprior linguistic knowledge. Unsupervised methods for identifying \"phrases\",\nmining subphrase structure and finding words in a fully automated way are\ndescribed. This can be considered as a step towards automatically computing a\n\"general dictionary and grammar of the corpus\". We hope that in the long run\nvariants of our approach turn out to be useful for other kind of sequence data\nas well, such as, e.g., speech, genom sequences, or music annotation. Even if\nwe are not primarily interested in immediate applications, results obtained for\na variety of languages show that our methods are interesting for many practical\ntasks in text mining, terminology extraction and lexicography, search engine\ntechnology, and related fields.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 12:08:05 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Gerdjikov", "Stefan", ""], ["Schulz", "Klaus U.", ""]]}, {"id": "1602.05875", "submitter": "Gil Keren", "authors": "Gil Keren and Bj\\\"orn Schuller", "title": "Convolutional RNN: an Enhanced Model for Extracting Features from\n  Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional convolutional layers extract features from patches of data by\napplying a non-linearity on an affine function of the input. We propose a model\nthat enhances this feature extraction process for the case of sequential data,\nby feeding patches of the data into a recurrent neural network and using the\noutputs or hidden states of the recurrent units to compute the extracted\nfeatures. By doing so, we exploit the fact that a window containing a few\nframes of the sequential data is a sequence itself and this additional\nstructure might encapsulate valuable information. In addition, we allow for\nmore steps of computation in the feature extraction process, which is\npotentially beneficial as an affine function followed by a non-linearity can\nresult in too simple features. Using our convolutional recurrent layers we\nobtain an improvement in performance in two audio classification tasks,\ncompared to traditional convolutional layers. Tensorflow code for the\nconvolutional recurrent layers is publicly available in\nhttps://github.com/cruvadom/Convolutional-RNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 16:55:30 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 10:01:25 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 14:03:16 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Keren", "Gil", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1602.05944", "submitter": "Erin Grant", "authors": "Erin Grant, Aida Nematzadeh, and Suzanne Stevenson", "title": "The Interaction of Memory and Attention in Novel Word Generalization: A\n  Computational Investigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People exhibit a tendency to generalize a novel noun to the basic-level in a\nhierarchical taxonomy -- a cognitively salient category such as \"dog\" -- with\nthe degree of generalization depending on the number and type of exemplars.\nRecently, a change in the presentation timing of exemplars has also been shown\nto have an effect, surprisingly reversing the prior observed pattern of\nbasic-level generalization. We explore the precise mechanisms that could lead\nto such behavior by extending a computational model of word learning and word\ngeneralization to integrate cognitive processes of memory and attention. Our\nresults show that the interaction of forgetting and attention to novelty, as\nwell as sensitivity to both type and token frequencies of exemplars, enables\nthe model to replicate the empirical results from different presentation\ntimings. Our results reinforce the need to incorporate general cognitive\nprocesses within word learning models to better understand the range of\nobserved behaviors in vocabulary acquisition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 20:53:26 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Grant", "Erin", ""], ["Nematzadeh", "Aida", ""], ["Stevenson", "Suzanne", ""]]}, {"id": "1602.06023", "submitter": "Ramesh Nallapati", "authors": "Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar\n  Gulcehre, Bing Xiang", "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and\n  Beyond", "comments": null, "journal-ref": "The SIGNLL Conference on Computational Natural Language Learning\n  (CoNLL), 2016", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we model abstractive text summarization using Attentional\nEncoder-Decoder Recurrent Neural Networks, and show that they achieve\nstate-of-the-art performance on two different corpora. We propose several novel\nmodels that address critical problems in summarization that are not adequately\nmodeled by the basic architecture, such as modeling key-words, capturing the\nhierarchy of sentence-to-word structure, and emitting words that are rare or\nunseen at training time. Our work shows that many of our proposed models\ncontribute to further improvement in performance. We also propose a new dataset\nconsisting of multi-sentence summaries, and establish performance benchmarks\nfor further research.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:04:18 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 22:50:03 GMT"}, {"version": "v3", "created": "Sat, 23 Apr 2016 02:38:01 GMT"}, {"version": "v4", "created": "Wed, 10 Aug 2016 22:56:10 GMT"}, {"version": "v5", "created": "Fri, 26 Aug 2016 16:13:13 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Nallapati", "Ramesh", ""], ["Zhou", "Bowen", ""], ["santos", "Cicero Nogueira dos", ""], ["Gulcehre", "Caglar", ""], ["Xiang", "Bing", ""]]}, {"id": "1602.06025", "submitter": "Yong Ren", "authors": "Yong Ren, Yining Wang, Jun Zhu", "title": "Spectral Learning for Supervised Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised topic models simultaneously model the latent topic structure of\nlarge collections of documents and a response variable associated with each\ndocument. Existing inference methods are based on variational approximation or\nMonte Carlo sampling, which often suffers from the local minimum defect.\nSpectral methods have been applied to learn unsupervised topic models, such as\nlatent Dirichlet allocation (LDA), with provable guarantees. This paper\ninvestigates the possibility of applying spectral methods to recover the\nparameters of supervised LDA (sLDA). We first present a two-stage spectral\nmethod, which recovers the parameters of LDA followed by a power update method\nto recover the regression model parameters. Then, we further present a\nsingle-phase spectral algorithm to jointly recover the topic distribution\nmatrix as well as the regression weights. Our spectral algorithms are provably\ncorrect and computationally efficient. We prove a sample complexity bound for\neach algorithm and subsequently derive a sufficient condition for the\nidentifiability of sLDA. Thorough experiments on synthetic and real-world\ndatasets verify the theory and demonstrate the practical effectiveness of the\nspectral algorithms. In fact, our results on a large-scale review rating\ndataset demonstrate that our single-phase spectral algorithm alone gets\ncomparable or even better performance than state-of-the-art methods, while\nprevious work on spectral methods has rarely reported such promising\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:07:20 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ren", "Yong", ""], ["Wang", "Yining", ""], ["Zhu", "Jun", ""]]}, {"id": "1602.06064", "submitter": "Tianxing He", "authors": "Tianxing He, Yu Zhang, Jasha Droppo, Kai Yu", "title": "On Training Bi-directional Neural Network Language Model with Noise\n  Contrastive Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to train bi-directional neural network language model(NNLM) with\nnoise contrastive estimation(NCE). Experiments are conducted on a rescore task\non the PTB data set. It is shown that NCE-trained bi-directional NNLM\noutperformed the one trained by conventional maximum likelihood training. But\nstill(regretfully), it did not out-perform the baseline uni-directional NNLM.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 07:27:49 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 02:51:34 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 02:00:23 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["He", "Tianxing", ""], ["Zhang", "Yu", ""], ["Droppo", "Jasha", ""], ["Yu", "Kai", ""]]}, {"id": "1602.06289", "submitter": "Stanis{\\l}aw Jastrz\\k{e}bski", "authors": "Stanis{\\l}aw Jastrz\\k{e}bski, Damian Le\\'sniak, Wojciech Marian\n  Czarnecki", "title": "Learning to SMILE(S)", "comments": "Accepted as a workshop contribution to ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how one can directly apply natural language processing (NLP)\nmethods to classification problems in cheminformatics. Connection between these\nseemingly separate fields is shown by considering standard textual\nrepresentation of compound, SMILES. The problem of activity prediction against\na target protein is considered, which is a crucial part of computer aided drug\ndesign process. Conducted experiments show that this way one can not only\noutrank state of the art results of hand crafted representations but also gets\ndirect structural insights into the way decisions are made.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 20:48:19 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 18:45:32 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Le\u015bniak", "Damian", ""], ["Czarnecki", "Wojciech Marian", ""]]}, {"id": "1602.06291", "submitter": "Shalini Ghosh", "authors": "Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, Larry\n  Heck", "title": "Contextual LSTM (CLSTM) models for Large scale NLP tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Documents exhibit sequential structure at multiple levels of abstraction\n(e.g., sentences, paragraphs, sections). These abstractions constitute a\nnatural hierarchy for representing the context in which to infer the meaning of\nwords and larger fragments of text. In this paper, we present CLSTM (Contextual\nLSTM), an extension of the recurrent neural network LSTM (Long-Short Term\nMemory) model, where we incorporate contextual features (e.g., topics) into the\nmodel. We evaluate CLSTM on three specific NLP tasks: word prediction, next\nsentence selection, and sentence topic prediction. Results from experiments run\non two corpora, English documents in Wikipedia and a subset of articles from a\nrecent snapshot of English Google News, indicate that using both words and\ntopics as features improves performance of the CLSTM models over baseline LSTM\nmodels for these tasks. For example on the next sentence selection task, we get\nrelative accuracy improvements of 21% for the Wikipedia dataset and 18% for the\nGoogle News dataset. This clearly demonstrates the significant benefit of using\ncontext appropriately in natural language (NL) tasks. This has implications for\na wide variety of NL applications like question answering, sentence completion,\nparaphrase generation, and next utterance prediction in dialog systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 20:52:08 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 17:19:09 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ghosh", "Shalini", ""], ["Vinyals", "Oriol", ""], ["Strope", "Brian", ""], ["Roy", "Scott", ""], ["Dean", "Tom", ""], ["Heck", "Larry", ""]]}, {"id": "1602.06359", "submitter": "Liang Pang", "authors": "Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, Xueqi\n  Cheng", "title": "Text Matching as Image Recognition", "comments": "Accepted by AAAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching two texts is a fundamental problem in many natural language\nprocessing tasks. An effective way is to extract meaningful matching patterns\nfrom words, phrases, and sentences to produce the matching score. Inspired by\nthe success of convolutional neural network in image recognition, where neurons\ncan capture many complicated patterns based on the extracted elementary visual\npatterns such as oriented edges and corners, we propose to model text matching\nas the problem of image recognition. Firstly, a matching matrix whose entries\nrepresent the similarities between words is constructed and viewed as an image.\nThen a convolutional neural network is utilized to capture rich matching\npatterns in a layer-by-layer way. We show that by resembling the compositional\nhierarchies of patterns in image recognition, our model can successfully\nidentify salient signals such as n-gram and n-term matchings. Experimental\nresults demonstrate its superiority against the baselines.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 02:55:11 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Wan", "Shengxian", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1602.06727", "submitter": "Zhizheng Wu", "authors": "Zhizheng Wu, Simon King", "title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using\n  Stacked Bottleneck Features and Minimum Generation Error Training", "comments": "submitted to IEEE/ACM Transactions on Audio, Speech and Language\n  Processing 2016 (AQ)", "journal-ref": null, "doi": "10.1109/TASLP.2016.2551865", "report-no": null, "categories": "cs.SD cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose two novel techniques --- stacking bottleneck features and minimum\ngeneration error training criterion --- to improve the performance of deep\nneural network (DNN)-based speech synthesis. The techniques address the related\nissues of frame-by-frame independence and ignorance of the relationship between\nstatic and dynamic features, within current typical DNN-based synthesis\nframeworks. Stacking bottleneck features, which are an acoustically--informed\nlinguistic representation, provides an efficient way to include more detailed\nlinguistic context at the input. The minimum generation error training\ncriterion minimises overall output trajectory error across an utterance, rather\nthan minimising the error per frame independently, and thus takes into account\nthe interaction between static and dynamic features. The two techniques can be\neasily combined to further improve performance. We present both objective and\nsubjective results that demonstrate the effectiveness of the proposed\ntechniques. The subjective results show that combining the two techniques leads\nto significantly more natural synthetic speech than from conventional DNN or\nlong short-term memory (LSTM) recurrent neural network (RNN) systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 11:11:04 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 11:18:07 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 11:31:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wu", "Zhizheng", ""], ["King", "Simon", ""]]}, {"id": "1602.06797", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Haitao Mi and Abraham Ittycheriah", "title": "Semi-supervised Clustering for Short Text via Deep Representation\n  Learning", "comments": "In Proceedings of CoNLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a semi-supervised method for short text clustering,\nwhere we represent texts as distributed vectors with neural networks, and use a\nsmall amount of labeled data to specify our intention for clustering. We design\na novel objective to combine the representation learning process and the\nk-means clustering process together, and optimize the objective with both\nlabeled data and unlabeled data iteratively until convergence through three\nsteps: (1) assign each short text to its nearest centroid based on its\nrepresentation from the current neural networks; (2) re-estimate the cluster\ncentroids based on cluster assignments from step (1); (3) update neural\nnetworks according to the objective by keeping centroids and cluster\nassignments fixed. Experimental results on four datasets show that our method\nworks significantly better than several other text clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 14:55:26 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 19:52:33 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wang", "Zhiguo", ""], ["Mi", "Haitao", ""], ["Ittycheriah", "Abraham", ""]]}, {"id": "1602.06967", "submitter": "Danila Doroshin", "authors": "Danila Doroshin, Nikolay Lubimov, Marina Nastasenko and Mikhail Kotov", "title": "Blind score normalization method for PLDA based speaker recognition", "comments": "4 pages, 1 figure, presented at the Interspeech 2015. In Sixteenth\n  Annual Conference of the International Speech Communication Association 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art\nmethod for modeling $i$-vector space in speaker recognition task. However the\nperformance degradation is observed if enrollment data size differs from one\nspeaker to another. This paper presents a solution to such problem by\nintroducing new PLDA scoring normalization technique. Normalization parameters\nare derived in a blind way, so that, unlike traditional \\textit{ZT-norm}, no\nextra development data is required. Moreover, proposed method has shown to be\noptimal in terms of detection cost function. The experiments conducted on NIST\nSRE 2014 database demonstrate an improved accuracy in a mixed enrollment number\ncondition.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 21:22:49 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Doroshin", "Danila", ""], ["Lubimov", "Nikolay", ""], ["Nastasenko", "Marina", ""], ["Kotov", "Mikhail", ""]]}, {"id": "1602.06979", "submitter": "Ethan Fast", "authors": "Ethan Fast, Binbin Chen, Michael Bernstein", "title": "Empath: Understanding Topic Signals in Large-Scale Text", "comments": "CHI: ACM Conference on Human Factors in Computing Systems 2016", "journal-ref": null, "doi": "10.1145/2858036.2858535", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language is colored by a broad range of topics, but existing text\nanalysis tools only focus on a small number of them. We present Empath, a tool\nthat can generate and validate new lexical categories on demand from a small\nset of seed terms (like \"bleed\" and \"punch\" to generate the category violence).\nEmpath draws connotations between words and phrases by deep learning a neural\nembedding across more than 1.8 billion words of modern fiction. Given a small\nset of seed words that characterize a category, Empath uses its neural\nembedding to discover new related terms, then validates the category with a\ncrowd-powered filter. Empath also analyzes text across 200 built-in,\npre-validated categories we have generated from common topics in our web\ndataset, like neglect, government, and social media. We show that Empath's\ndata-driven, human validated categories are highly correlated (r=0.906) with\nsimilar categories in LIWC.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 21:47:43 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Fast", "Ethan", ""], ["Chen", "Binbin", ""], ["Bernstein", "Michael", ""]]}, {"id": "1602.07019", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Haitao Mi and Abraham Ittycheriah", "title": "Sentence Similarity Learning by Lexical Decomposition and Composition", "comments": "In Proceedings of Coling 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most conventional sentence similarity methods only focus on similar parts of\ntwo input sentences, and simply ignore the dissimilar parts, which usually give\nus some clues and semantic meanings about the sentences. In this work, we\npropose a model to take into account both the similarities and dissimilarities\nby decomposing and composing lexical semantics over sentences. The model\nrepresents each word as a vector, and calculates a semantic matching vector for\neach word based on all words in the other sentence. Then, each word vector is\ndecomposed into a similar component and a dissimilar component based on the\nsemantic matching vector. After this, a two-channel CNN model is employed to\ncapture features by composing the similar and dissimilar components. Finally, a\nsimilarity score is estimated over the composed feature vectors. Experimental\nresults show that our model gets the state-of-the-art performance on the answer\nsentence selection task, and achieves a comparable result on the paraphrase\nidentification task.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 03:08:50 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 19:51:10 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wang", "Zhiguo", ""], ["Mi", "Haitao", ""], ["Ittycheriah", "Abraham", ""]]}, {"id": "1602.07236", "submitter": "Clayton Norris", "authors": "Clayton Norris", "title": "Petrarch 2 : Petrarcher", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PETRARCH 2 is the fourth generation of a series of Event-Data coders stemming\nfrom research by Phillip Schrodt. Each iteration has brought new functionality\nand usability, and this is no exception.Petrarch 2 takes much of the power of\nthe original Petrarch's dictionaries and redirects it into a faster and smarter\ncore logic. Earlier iterations handled sentences largely as a list of words,\nincorporating some syntactic information here and there. Petrarch 2 now views\nthe sentence entirely on the syntactic level. It receives the syntactic parse\nof a sentence from the Stanford CoreNLP software, and stores this data as a\ntree structure of linked nodes, where each node is a Phrase object.\nPrepositional, noun, and verb phrases each have their own version of this\nPhrase class, which deals with the logic particular to those kinds of phrases.\nSince this is an event coder, the core of the logic focuses around the verbs:\nwho is acting, who is being acted on, and what is happening. The theory behind\nthis new structure and its logic is founded in Generative Grammar, Information\nTheory, and Lambda-Calculus Semantics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 17:05:06 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Norris", "Clayton", ""]]}, {"id": "1602.07275", "submitter": "Silvio R. Dahmen", "authors": "Sandra D. Prado, Silvio R. Dahmen, Ana L.C. Bazzan, Padraig Mac Carron\n  and Ralph Kenna", "title": "Temporal Network Analysis of Literary Texts", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study temporal networks of characters in literature focusing on \"Alice's\nAdventures in Wonderland\" (1865) by Lewis Carroll and the anonymous \"La Chanson\nde Roland\" (around 1100). The former, one of the most influential pieces of\nnonsense literature ever written, describes the adventures of Alice in a\nfantasy world with logic plays interspersed along the narrative. The latter, a\nsong of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. during\nCharlemagne's campaign on the Iberian Peninsula. We apply methods recently\ndeveloped by Taylor and coworkers \\cite{Taylor+2015} to find time-averaged\neigenvector centralities, Freeman indices and vitalities of characters. We show\nthat temporal networks are more appropriate than static ones for studying\nstories, as they capture features that the time-independent approaches fail to\nyield.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 17:55:04 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Prado", "Sandra D.", ""], ["Dahmen", "Silvio R.", ""], ["Bazzan", "Ana L. C.", ""], ["Mac Carron", "Padraig", ""], ["Kenna", "Ralph", ""]]}, {"id": "1602.07291", "submitter": "Omid Sadjadi", "authors": "Seyed Omid Sadjadi, Sriram Ganapathy, Jason W. Pelecanos", "title": "The IBM 2016 Speaker Recognition System", "comments": "Submitted to Odyssey 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the recent advancements made in the IBM i-vector\nspeaker recognition system for conversational speech. In particular, we\nidentify key techniques that contribute to significant improvements in\nperformance of our system, and quantify their contributions. The techniques\ninclude: 1) a nearest-neighbor discriminant analysis (NDA) approach that is\nformulated to alleviate some of the limitations associated with the\nconventional linear discriminant analysis (LDA) that assumes Gaussian\nclass-conditional distributions, 2) the application of speaker- and\nchannel-adapted features, which are derived from an automatic speech\nrecognition (ASR) system, for speaker recognition, and 3) the use of a deep\nneural network (DNN) acoustic model with a large number of output units (~10k\nsenones) to compute the frame-level soft alignments required in the i-vector\nestimation process. We evaluate these techniques on the NIST 2010 speaker\nrecognition evaluation (SRE) extended core conditions involving telephone and\nmicrophone trials. Experimental results indicate that: 1) the NDA is more\neffective (up to 35% relative improvement in terms of EER) than the traditional\nparametric LDA for speaker recognition, 2) when compared to raw acoustic\nfeatures (e.g., MFCCs), the ASR speaker-adapted features provide gains in\nspeaker recognition performance, and 3) increasing the number of output units\nin the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)\nprovides consistent improvements in performance (for example from 37% to 57%\nrelative EER gains over our baseline GMM i-vector system). To our knowledge,\nresults reported in this paper represent the best performances published to\ndate on the NIST SRE 2010 extended core tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 20:39:40 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Sadjadi", "Seyed Omid", ""], ["Ganapathy", "Sriram", ""], ["Pelecanos", "Jason W.", ""]]}, {"id": "1602.07393", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge and Yufang Sun", "title": "Domain Specific Author Attribution Based on Feedforward Neural Network\n  Language Models", "comments": "International Conference on Pattern Recognition Application and\n  Methods (ICPRAM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution refers to the task of automatically determining the\nauthor based on a given sample of text. It is a problem with a long history and\nhas a wide range of application. Building author profiles using language models\nis one of the most successful methods to automate this task. New language\nmodeling methods based on neural networks alleviate the curse of dimensionality\nand usually outperform conventional N-gram methods. However, there have not\nbeen much research applying them to authorship attribution. In this paper, we\npresent a novel setup of a Neural Network Language Model (NNLM) and apply it to\na database of text samples from different authors. We investigate how the NNLM\nperforms on a task with moderate author set size and relatively limited\ntraining and test data, and how the topics of the text samples affect the\naccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of\nfitness of a trained language model to the test data. Given 5 random test\nsentences, it also increases the author classification accuracy by 3.43% on\naverage, compared with the N-gram methods using SRILM tools. An open source\nimplementation of our methodology is freely available at\nhttps://github.com/zge/authorship-attribution/.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 04:32:34 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sun", "Yufang", ""]]}, {"id": "1602.07394", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge", "title": "Improved Accent Classification Combining Phonetic Vowels with Acoustic\n  Features", "comments": "International Congress on Image and Signal Processing (CISP) 2015", "journal-ref": null, "doi": "10.1109/CISP.2015.7408064", "report-no": null, "categories": "cs.SD cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches have shown accent classification can be improved by integrating\nsemantic information into pure acoustic approach. In this work, we combine\nphonetic knowledge, such as vowels, with enhanced acoustic features to build an\nimproved accent classification system. The classifier is based on Gaussian\nMixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual\nLinear Predictive (PLP) features. The features are further optimized by\nPrinciple Component Analysis (PCA) and Hetroscedastic Linear Discriminant\nAnalysis (HLDA). Using 7 major types of accented speech from the Foreign\nAccented English (FAE) corpus, the system achieves classification accuracy 54%\nwith input test data as short as 20 seconds, which is competitive to the state\nof the art in this field.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 04:33:49 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ge", "Zhenhao", ""]]}, {"id": "1602.07563", "submitter": "Igor Mozeti\\v{c}", "authors": "Igor Mozetic, Miha Grcar, Jasmina Smailovic", "title": "Multilingual Twitter Sentiment Classification: The Role of Human\n  Annotators", "comments": null, "journal-ref": "PLoS ONE 11(5): e0155036, 2016", "doi": "10.1371/journal.pone.0155036", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What are the limits of automated Twitter sentiment classification? We analyze\na large set of manually labeled tweets in different languages, use them as\ntraining data, and construct automated classification models. It turns out that\nthe quality of classification models depends much more on the quality and size\nof training data than on the type of the model trained. Experimental results\nindicate that there is no statistically significant difference between the\nperformance of the top classification models. We quantify the quality of\ntraining data by applying various annotator agreement measures, and identify\nthe weakest points of different datasets. We show that the model performance\napproaches the inter-annotator agreement when the size of the training set is\nsufficiently large. However, it is crucial to regularly monitor the self- and\ninter-annotator agreements since this improves the training datasets and\nconsequently the model performance. Finally, we show that there is strong\nevidence that humans perceive the sentiment classes (negative, neutral, and\npositive) as ordered.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 15:34:22 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 07:05:52 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Mozetic", "Igor", ""], ["Grcar", "Miha", ""], ["Smailovic", "Jasmina", ""]]}, {"id": "1602.07572", "submitter": "Sascha Rothe", "authors": "Sascha Rothe, Sebastian Ebert, Hinrich Sch\\\"utze", "title": "Ultradense Word Embeddings by Orthogonal Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings are generic representations that are useful for many NLP tasks. In\nthis paper, we introduce DENSIFIER, a method that learns an orthogonal\ntransformation of the embedding space that focuses the information relevant for\na task in an ultradense subspace of a dimensionality that is smaller by a\nfactor of 100 than the original space. We show that ultradense embeddings\ngenerated by DENSIFIER reach state of the art on a lexicon creation task in\nwhich words are annotated with three types of lexical information - sentiment,\nconcreteness and frequency. On the SemEval2015 10B sentiment analysis task we\nshow that no information is lost when the ultradense subspace is used, but\ntraining is an order of magnitude more efficient due to the compactness of the\nultradense space.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 16:06:25 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 08:50:11 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Rothe", "Sascha", ""], ["Ebert", "Sebastian", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1602.07618", "submitter": "Bob Coecke", "authors": "Bob Coecke", "title": "From quantum foundations via natural language meaning to a theory of\n  everything", "comments": "Invited contribution to: `The Incomputable'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue for a paradigmatic shift from `reductionism' to\n`togetherness'. In particular, we show how interaction between systems in\nquantum theory naturally carries over to modelling how word meanings interact\nin natural language. Since meaning in natural language, depending on the\nsubject domain, encompasses discussions within any scientific discipline, we\nobtain a template for theories such as social interaction, animal behaviour,\nand many others.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 16:17:54 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Coecke", "Bob", ""]]}, {"id": "1602.07749", "submitter": "Thien Nguyen", "authors": "Thien Huu Nguyen, Avirup Sil, Georgiana Dinu and Radu Florian", "title": "Toward Mention Detection Robustness with Recurrent Neural Networks", "comments": "13 pages, 11 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in natural language processing (NLP) is to yield\ngood performance across application domains and languages. In this work, we\ninvestigate the robustness of the mention detection systems, one of the\nfundamental tasks in information extraction, via recurrent neural networks\n(RNNs). The advantage of RNNs over the traditional approaches is their capacity\nto capture long ranges of context and implicitly adapt the word embeddings,\ntrained on a large corpus, into a task-specific word representation, but still\npreserve the original semantic generalization to be helpful across domains. Our\nsystematic evaluation for RNN architectures demonstrates that RNNs not only\noutperform the best reported systems (up to 9\\% relative error reduction) in\nthe general setting but also achieve the state-of-the-art performance in the\ncross-domain setting for English. Regarding other languages, RNNs are\nsignificantly better than the traditional methods on the similar task of named\nentity recognition for Dutch (up to 22\\% relative error reduction).\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 23:14:01 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Nguyen", "Thien Huu", ""], ["Sil", "Avirup", ""], ["Dinu", "Georgiana", ""], ["Florian", "Radu", ""]]}, {"id": "1602.07776", "submitter": "Adhiguna Kuncoro", "authors": "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith", "title": "Recurrent Neural Network Grammars", "comments": "Proceedings of NAACL 2016 (contains corrigendum)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce recurrent neural network grammars, probabilistic models of\nsentences with explicit phrase structure. We explain efficient inference\nprocedures that allow application to both parsing and language modeling.\nExperiments show that they provide better parsing in English than any single\npreviously published supervised generative model and better language modeling\nthan state-of-the-art sequential RNNs in English and Chinese.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 02:42:58 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 23:28:08 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 14:22:02 GMT"}, {"version": "v4", "created": "Wed, 12 Oct 2016 04:47:45 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Dyer", "Chris", ""], ["Kuncoro", "Adhiguna", ""], ["Ballesteros", "Miguel", ""], ["Smith", "Noah A.", ""]]}, {"id": "1602.07803", "submitter": "Md. Tarek Habib", "authors": "Md. Masudul Haque, Md. Tarek Habib and Md. Mokhlesur Rahman", "title": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models", "comments": "in International Journal in Foundations of Computer Science &\n  Technology (IJFCST) Vol.5, No.6, November 2015", "journal-ref": null, "doi": "10.5121/ijfcst.2015.5607", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:35:16 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Haque", "Md. Masudul", ""], ["Habib", "Md. Tarek", ""], ["Rahman", "Md. Mokhlesur", ""]]}, {"id": "1602.07807", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly\n  Detection", "comments": "8 pages, 4 figures, 5 tables; published in Proceedings of the 2016\n  IEEE Tenth International Conference on Semantic Computing (ICSC), Laguna\n  Hills, CA, USA, pages 79-86, February 2016", "journal-ref": "In Proceedings of the 2016 IEEE Tenth International Conference on\n  Semantic Computing (ICSC), pages 79-86, Laguna Hills, CA, USA, February 2016.\n  IEEE", "doi": "10.1109/ICSC.2016.38", "report-no": null, "categories": "cs.DB cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important forms of data are stored digitally in XML format. Errors can\noccur in the textual content of the data in the fields of the XML. Fixing these\nerrors manually is time-consuming and expensive, especially for large amounts\nof data. There is increasing interest in the research, development, and use of\nautomated techniques for assisting with data cleaning. Electronic dictionaries\nare an important form of data frequently stored in XML format that frequently\nhave errors introduced through a mixture of manual typographical entry errors\nand optical character recognition errors. In this paper we describe methods for\nflagging statistical anomalies as likely errors in electronic dictionaries\nstored in XML format. We describe six systems based on different sources of\ninformation. The systems detect errors using various signals in the data\nincluding uncommon characters, text length, character-based language models,\nword-based language models, tied-field length ratios, and tied-field\ntransliteration models. Four of the systems detect errors based on expectations\nautomatically inferred from content within elements of a single field type. We\ncall these single-field systems. Two of the systems detect errors based on\ncorrespondence expectations automatically inferred from content within elements\nof multiple related field types. We call these tied-field systems. For each\nsystem, we provide an intuitive analysis of the type of error that it is\nsuccessful at detecting. Finally, we describe two larger-scale evaluations\nusing crowdsourcing with Amazon's Mechanical Turk platform and using the\nannotations of a domain expert. The evaluations consistently show that the\nsystems are useful for improving the efficiency with which errors in XML\nelectronic dictionaries can be detected.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:49:36 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 04:01:43 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1602.08128", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith", "title": "PCA Method for Automated Detection of Mispronounced Words", "comments": "SPIE Defense, Security, and Sensing", "journal-ref": null, "doi": "10.1117/12.884155", "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for detecting mispronunciations with the aim of\nimproving Computer Assisted Language Learning (CALL) tools used by foreign\nlanguage learners. The algorithm is based on Principle Component Analysis\n(PCA). It is hierarchical with each successive step refining the estimate to\nclassify the test word as being either mispronounced or correct. Preprocessing\nbefore detection, like normalization and time-scale modification, is\nimplemented to guarantee uniformity of the feature vectors input to the\ndetection system. The performance using various features including spectrograms\nand Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.\nBest results were obtained using MFCCs, achieving up to 99% accuracy in word\nverification and 93% in native/non-native classification. Compared with Hidden\nMarkov Models (HMMs) which are used pervasively in recognition application,\nthis particular approach is computational efficient and effective when training\ndata is limited.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 21:48:56 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sharma", "Sudhendu R.", ""], ["Smith", "Mark J. T.", ""]]}, {"id": "1602.08657", "submitter": "Luc Herren", "authors": "Luc Herren", "title": "QuotationFinder - Searching for Quotations and Allusions in Greek and\n  Latin Texts and Establishing the Degree to Which a Quotation or Allusion\n  Matches Its Source", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages, Project\n  presentations (August 2, 2017) jdmdh:3825", "doi": "10.46298/jdmdh.1389", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The software programs generally used with the TLG (Thesaurus Linguae Graecae)\nand the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well\nsuited for finding quotations and allusions. QuotationFinder uses more\nsophisticated criteria as it ranks search results based on how closely they\nmatch the source text, listing search results with literal quotations first and\nloose verbal parallels last.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 02:25:22 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 17:19:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Herren", "Luc", ""]]}, {"id": "1602.08715", "submitter": "Avi Shmidman", "authors": "Avi Shmidman, Moshe Koppel, Ely Porat", "title": "Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus", "comments": "Submission to the Journal of Data Mining and Digital Humanities\n  (Special Issue on Computer-Aided Processing of Intertextuality in Ancient\n  Languages)", "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages, Towards a\n  Digital Ecosystem: NLP. Corpus infrastructure. Methods for Retrieving Texts\n  and Computing Text Similarities (March 11, 2018) jdmdh:4175", "doi": "10.46298/jdmdh.1388", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for efficiently finding all parallel passages in a large\ncorpus, even if the passages are not quite identical due to rephrasing and\northographic variation. The key ideas are the representation of each word in\nthe corpus by its two most infrequent letters, finding matched pairs of strings\nof four or five words that differ by at most one word and then identifying\nclusters of such matched pairs. Using this method, over 4600 parallel pairs of\npassages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of\nover 1.8 million words, in just over 30 seconds. Empirical comparisons on\nsample data indicate that the coverage obtained by our method is essentially\nthe same as that obtained using slow exhaustive methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 13:43:33 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 00:58:28 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Shmidman", "Avi", ""], ["Koppel", "Moshe", ""], ["Porat", "Ely", ""]]}, {"id": "1602.08741", "submitter": "Nikolay N. Vasiliev", "authors": "Nikolay N. Vasiliev", "title": "Gibberish Semantics: How Good is Russian Twitter in Word Semantic\n  Similarity Task?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most studied and most successful language models were developed and\nevaluated mainly for English and other close European languages, such as\nFrench, German, etc. It is important to study applicability of these models to\nother languages. The use of vector space models for Russian was recently\nstudied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models\nwere evaluated against word semantic similarity task. For our knowledge Twitter\nwas not considered as a corpus for this task, with this work we fill the gap.\nResults for vectors trained on Twitter corpus are comparable in accuracy with\nother single-corpus trained models, although the best performance is currently\nachieved by combination of multiple corpora.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 16:58:01 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Vasiliev", "Nikolay N.", ""]]}, {"id": "1602.08742", "submitter": "James Loach", "authors": "James C. Loach and Jinzhao Wang", "title": "Optimizing the Learning Order of Chinese Characters Using a Novel\n  Topological Sort Algorithm", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0163623", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for optimizing the order in which Chinese\ncharacters are learned, one that incorporates the benefits of learning them in\norder of usage frequency and in order of their hierarchal structural\nrelationships. We show that our work outperforms previously published orders\nand algorithms. Our algorithm is applicable to any scheduling task where nodes\nhave intrinsic differences in importance and must be visited in topological\norder.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 16:59:46 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 05:08:04 GMT"}, {"version": "v3", "created": "Sat, 24 Sep 2016 17:35:29 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Loach", "James C.", ""], ["Wang", "Jinzhao", ""]]}, {"id": "1602.08761", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama", "title": "Resource Constrained Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of structured prediction under test-time budget\nconstraints. We propose a novel approach applicable to a wide range of\nstructured prediction problems in computer vision and natural language\nprocessing. Our approach seeks to adaptively generate computationally costly\nfeatures during test-time in order to reduce the computational cost of\nprediction while maintaining prediction performance. We show that training the\nadaptive feature generation system can be reduced to a series of structured\nlearning problems, resulting in efficient training using existing structured\nlearning algorithms. This framework provides theoretical justification for\nseveral existing heuristic approaches found in literature. We evaluate our\nproposed adaptive system on two structured prediction tasks, optical character\nrecognition (OCR) and dependency parsing and show strong performance in\nreduction of the feature costs without degrading accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 19:44:57 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 01:31:01 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1602.08844", "submitter": "Pramit Chaudhuri", "authors": "Pramit Chaudhuri, Joseph P. Dexter", "title": "Bioinformatics and Classical Literary Study", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, Special Issue on\n  Computer-Aided Processing of Intertextuality in Ancient Languages, Project\n  presentations (August 19, 2017) jdmdh:3807", "doi": "10.46298/jdmdh.1386", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Quantitative Criticism Lab, a collaborative\ninitiative between classicists, quantitative biologists, and computer\nscientists to apply ideas and methods drawn from the sciences to the study of\nliterature. A core goal of the project is the use of computational biology,\nnatural language processing, and machine learning techniques to investigate\nauthorial style, intertextuality, and related phenomena of literary\nsignificance. As a case study in our approach, here we review the use of\nsequence alignment, a common technique in genomics and computational\nlinguistics, to detect intertextuality in Latin literature. Sequence alignment\nis distinguished by its ability to find inexact verbal similarities, which\nmakes it ideal for identifying phonetic echoes in large corpora of Latin texts.\nAlthough especially suited to Latin, sequence alignment in principle can be\nextended to many other languages.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 07:20:59 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 05:04:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chaudhuri", "Pramit", ""], ["Dexter", "Joseph P.", ""]]}, {"id": "1602.08952", "submitter": "\\'Akos K\\'ad\\'ar", "authors": "\\'Akos K\\'ad\\'ar, Grzegorz Chrupa{\\l}a, Afra Alishahi", "title": "Representation of linguistic form and function in recurrent neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel methods for analyzing the activation patterns of RNNs from a\nlinguistic point of view and explore the types of linguistic structure they\nlearn. As a case study, we use a multi-task gated recurrent network\narchitecture consisting of two parallel pathways with shared word embeddings\ntrained on predicting the representations of the visual scene corresponding to\nan input sentence, and predicting the next word in the same sentence. Based on\nour proposed method to estimate the amount of contribution of individual tokens\nin the input to the final prediction of the networks we show that the image\nprediction pathway: a) is sensitive to the information structure of the\nsentence b) pays selective attention to lexical categories and grammatical\nfunctions that carry semantic information c) learns to treat the same input\ntoken differently depending on its grammatical functions in the sentence. In\ncontrast the language model is comparatively more sensitive to words with a\nsyntactic function. Furthermore, we propose methods to ex- plore the function\nof individual hidden units in RNNs and show that the two pathways of the\narchitecture in our case study contain specialized units tuned to patterns\ninformative for the task, some of which can carry activations to later time\nsteps to encode long-term dependencies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 13:31:17 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 12:30:12 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Alishahi", "Afra", ""]]}]