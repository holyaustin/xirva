[{"id": "1409.0314", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama", "title": "Empirical Evaluation of Tree distances for Parser Evaluation", "comments": "Submitted to satisfy partial requirements for Statistical Parsing\n  course", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this empirical study, I compare various tree distance measures --\noriginally developed in computational biology for the purpose of tree\ncomparison -- for the purpose of parser evaluation. I will control for the\nparser setting by comparing the automatically generated parse trees from the\nstate-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The\narticle describes two different tree distance measures (RF and QD) along with\nits variants (GRF and GQD) for the purpose of parser evaluation. The article\nwill argue that RF measure captures similar information as the standard EvalB\nmetric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,\n1989) applied by Tsarfaty et al. (2011). Finally, the article also provides\nempirical evidence by reporting high correlations between the different tree\ndistances and EvalB metric's scores.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 08:01:54 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 20:07:58 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Rama", "Taraka", ""]]}, {"id": "1409.0473", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "comments": "Accepted at ICLR 2015 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 16:33:02 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 18:32:00 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 18:10:39 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 21:39:11 GMT"}, {"version": "v5", "created": "Sun, 22 Mar 2015 17:08:39 GMT"}, {"version": "v6", "created": "Fri, 24 Apr 2015 13:25:33 GMT"}, {"version": "v7", "created": "Thu, 19 May 2016 21:53:22 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.0915", "submitter": "H. Hernan Moraldo", "authors": "H. Hernan Moraldo", "title": "An Approach for Text Steganography Based on Markov Chains", "comments": "Presented at 41 JAIIO - WSegI 2012", "journal-ref": "41 JAIIO - WSegI 2012, ISSN: 2313-9110, pages 21 - 35", "doi": null, "report-no": null, "categories": "cs.MM cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A text steganography method based on Markov chains is introduced, together\nwith a reference implementation. This method allows for information hiding in\ntexts that are automatically generated following a given Markov model. Other\nMarkov - based systems of this kind rely on big simplifications of the language\nmodel to work, which produces less natural looking and more easily detectable\ntexts. The method described here is designed to generate texts within a good\napproximation of the original language model provided.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 22:59:52 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Moraldo", "H. Hernan", ""]]}, {"id": "1409.1257", "submitter": "KyungHyun Cho", "authors": "Jean Pouget-Abadie and Dzmitry Bahdanau and Bart van Merrienboer and\n  Kyunghyun Cho and Yoshua Bengio", "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation\n  using Automatic Segmentation", "comments": "Eighth Workshop on Syntax, Semantics and Structure in Statistical\n  Translation (SSST-8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors of (Cho et al., 2014a) have shown that the recently introduced\nneural network translation systems suffer from a significant drop in\ntranslation quality when translating long sentences, unlike existing\nphrase-based translation systems. In this paper, we propose a way to address\nthis issue by automatically segmenting an input sentence into phrases that can\nbe easily translated by the neural network translation model. Once each segment\nhas been independently translated by the neural machine translation model, the\ntranslated clauses are concatenated to form a final translation. Empirical\nresults show a significant improvement in translation quality for long\nsentences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 21:00:49 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:09:37 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Bahdanau", "Dzmitry", ""], ["van Merrienboer", "Bart", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1259", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua\n  Bengio", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches", "comments": "Eighth Workshop on Syntax, Semantics and Structure in Statistical\n  Translation (SSST-8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation is a relatively new approach to statistical\nmachine translation based purely on neural networks. The neural machine\ntranslation models often consist of an encoder and a decoder. The encoder\nextracts a fixed-length representation from a variable-length input sentence,\nand the decoder generates a correct translation from this representation. In\nthis paper, we focus on analyzing the properties of the neural machine\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\nrecursive convolutional neural network. We show that the neural machine\ntranslation performs relatively well on short sentences without unknown words,\nbut its performance degrades rapidly as the length of the sentence and the\nnumber of unknown words increase. Furthermore, we find that the proposed gated\nrecursive convolutional network learns a grammatical structure of a sentence\nautomatically.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 21:03:41 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:08:30 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Cho", "Kyunghyun", ""], ["van Merrienboer", "Bart", ""], ["Bahdanau", "Dzmitry", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1612", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov", "title": "Semantic clustering of Russian web search results: possibilities and\n  problems", "comments": "Presented at Russian Summer School in Information Retrieval (RuSSIR\n  2014). To be published in Springer Communications in Computer and Information\n  Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper deals with word sense induction from lexical co-occurrence graphs.\nWe construct such graphs on large Russian corpora and then apply this data to\ncluster Mail.ru Search results according to meanings of the query. We compare\ndifferent methods of performing such clustering and different source corpora.\nModels of applying distributional semantics to big linguistic data are\ndescribed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 21:09:26 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 13:27:11 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Kutuzov", "Andrey", ""]]}, {"id": "1409.1744", "submitter": "Vincent A Traag", "authors": "V.A. Traag, R. Reinanda and G. van Klinken", "title": "Structure of a media co-occurrence network", "comments": null, "journal-ref": "Proceedings of ECCS 2014, 2016, pp. 81-91", "doi": "10.1007/978-3-319-29228-1_8", "report-no": null, "categories": "physics.soc-ph cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks have been of much interest in recent years. We here focus on\na network structure derived from co-occurrences of people in traditional\nnewspaper media. We find three clear deviations from what can be expected in a\nrandom graph. First, the average degree in the empirical network is much lower\nthan expected, and the average weight of a link much higher than expected.\nSecondly, high degree nodes attract disproportionately much weight. Thirdly,\nrelatively much of the weight seems to concentrate between high degree nodes.\nWe believe this can be explained by the fact that most people tend to co-occur\nrepeatedly with the same people. We create a model that replicates these\nobservations qualitatively based on two self-reinforcing processes: (1) more\nfrequently occurring persons are more likely to occur again; and (2) if two\npeople co-occur frequently, they are more likely to co-occur again. This\nsuggest that the media tends to focus on people that are already in the news,\nand that they reinforce existing co-occurrences.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 11:42:26 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 10:53:41 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Traag", "V. A.", ""], ["Reinanda", "R.", ""], ["van Klinken", "G.", ""]]}, {"id": "1409.2073", "submitter": "Tobias Kortkamp", "authors": "Tobias Kortkamp", "title": "An NLP Assistant for Clide", "comments": "Bachelor Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This report describes an NLP assistant for the collaborative development\nenvironment Clide, that supports the development of NLP applications by\nproviding easy access to some common NLP data structures. The assistant\nvisualizes text fragments and their dependencies by displaying the semantic\ngraph of a sentence, the coreference chain of a paragraph and mined triples\nthat are extracted from a paragraph's semantic graphs and linked using its\ncoreference chain. Using this information and a logic programming library, we\ncreate an NLP database which is used by a series of queries to mine the\ntriples. The algorithm is tested by translating a natural language text\ndescribing a graph to an actual graph that is shown as an annotation in the\ntext editor.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 02:31:03 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Kortkamp", "Tobias", ""]]}, {"id": "1409.2195", "submitter": "Daniel Fried", "authors": "Daniel Fried, Mihai Surdeanu, Stephen Kobourov, Melanie Hingle, Dane\n  Bell", "title": "Analyzing the Language of Food on Social Media", "comments": "An extended abstract of this paper will appear in IEEE Big Data 2014", "journal-ref": null, "doi": "10.1109/BigData.2014.7004305", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the predictive power behind the language of food on social\nmedia. We collect a corpus of over three million food-related posts from\nTwitter and demonstrate that many latent population characteristics can be\ndirectly predicted from this data: overweight rate, diabetes rate, political\nleaning, and home geographical location of authors. For all tasks, our\nlanguage-based models significantly outperform the majority-class baselines.\nPerformance is further improved with more complex natural language processing,\nsuch as topic modeling. We analyze which textual features have most predictive\npower for these datasets, providing insight into the connections between the\nlanguage of food, geographic locale, and community characteristics. Lastly, we\ndesign and implement an online system for real-time query and visualization of\nthe dataset. Visualization tools, such as geo-referenced heatmaps,\nsemantics-preserving wordclouds and temporal histograms, allow us to discover\nmore complex, global patterns mirrored in the language of food.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 03:07:54 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 17:35:02 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Fried", "Daniel", ""], ["Surdeanu", "Mihai", ""], ["Kobourov", "Stephen", ""], ["Hingle", "Melanie", ""], ["Bell", "Dane", ""]]}, {"id": "1409.2433", "submitter": "Antonina Kolokolova", "authors": "Antonina Kolokolova, Renesa Nizamee", "title": "Approximating solution structure of the Weighted Sentence Alignment\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximating solution structure of the bijective\nweighted sentence alignment problem of DeNero and Klein (2008). In particular,\nwe consider the complexity of finding an alignment that has a significant\noverlap with an optimal alignment. We discuss ways of representing the solution\nfor the general weighted sentence alignment as well as phrases-to-words\nalignment problem, and show that computing a string which agrees with the\noptimal sentence partition on more than half (plus an arbitrarily small\npolynomial fraction) positions for the phrases-to-words alignment is NP-hard.\nFor the general weighted sentence alignment we obtain such bound from the\nagreement on a little over 2/3 of the bits. Additionally, we generalize the\nHamming distance approximation of a solution structure to approximating it with\nrespect to the edit distance metric, obtaining similar lower bounds.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 17:19:11 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Kolokolova", "Antonina", ""], ["Nizamee", "Renesa", ""]]}, {"id": "1409.2450", "submitter": "Robert West", "authors": "Robert West, Hristo S. Paskov, Jure Leskovec, Christopher Potts", "title": "Exploiting Social Network Structure for Person-to-Person Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person-to-person evaluations are prevalent in all kinds of discourse and\nimportant for establishing reputations, building social bonds, and shaping\npublic opinion. Such evaluations can be analyzed separately using signed social\nnetworks and textual sentiment analysis, but this misses the rich interactions\nbetween language and social context. To capture such interactions, we develop a\nmodel that predicts individual A's opinion of individual B by synthesizing\ninformation from the signed social network in which A and B are embedded with\nsentiment analysis of the evaluative texts relating A to B. We prove that this\nproblem is NP-hard but can be relaxed to an efficiently solvable hinge-loss\nMarkov random field, and we show that this implementation outperforms text-only\nand network-only versions in two very different datasets involving\ncommunity-level decision-making: the Wikipedia Requests for Adminship corpus\nand the Convote U.S. Congressional speech corpus.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 18:14:16 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["West", "Robert", ""], ["Paskov", "Hristo S.", ""], ["Leskovec", "Jure", ""], ["Potts", "Christopher", ""]]}, {"id": "1409.2944", "submitter": "Hao Wang", "authors": "Hao Wang and Naiyan Wang and Dit-Yan Yeung", "title": "Collaborative Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 03:05:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 09:23:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Naiyan", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1409.2993", "submitter": "Jian Tang", "authors": "Jian Tang, Ming Zhang, Qiaozhu Mei", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a burden to the users of statistical topic models to\npredetermine the right number of topics, which is a key parameter of most topic\nmodels. Conventionally, automatic selection of this parameter is done through\neither statistical model selection (e.g., cross-validation, AIC, or BIC) or\nBayesian nonparametric models (e.g., hierarchical Dirichlet process). These\nmethods either rely on repeated runs of the inference algorithm to search\nthrough a large range of parameter values which does not suit the mining of big\ndata, or replace this parameter with alternative parameters that are less\nintuitive and still hard to be determined. In this paper, we explore to\n\"eliminate\" this parameter from a new perspective. We first present a\nnonparametric treatment of the PLSA model named nonparametric probabilistic\nlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows for\nthe exploration and comparison of different numbers of topics within a single\nexecution, yet remains as simple as that of PLSA. This is achieved by\nsubstituting the parameter of the number of topics with an alternative\nparameter that is the minimal goodness of fit of a document. We show that the\nnew parameter can be further eliminated by two parameter-free treatments:\neither by monitoring the diversity among the discovered topics or by a weak\nsupervision from users in the form of an exemplar topic. The parameter-free\ntopic model finds the appropriate number of topics when the diversity among the\ndiscovered topics is maximized, or when the granularity of the discovered\ntopics matches the exemplar topic. Experiments on both synthetic and real data\nprove that the parameter-free topic model extracts topics with a comparable\nquality comparing to classical topic models with \"manual transmission\". The\nquality of the topics outperforms those extracted through classical Bayesian\nnonparametric models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 08:41:35 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Tang", "Jian", ""], ["Zhang", "Ming", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1409.3005", "submitter": "Abdelkader El Mahdaouy", "authors": "Abdelkader El Mahdaouy, Sa\\\"id EL Alaoui Ouatik and Eric Gaussier", "title": "A Study of Association Measures and their Combination for Arabic MWT\n  Extraction", "comments": "This paper have been presented and published in 10th International\n  Conference on Terminology and Artificial Intelligence Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Multi-Word Term (MWT) extraction is a very important issue to many\napplications, such as information retrieval, question answering, and text\ncategorization. Although many methods have been used for MWT extraction in\nEnglish and other European languages, few studies have been applied to Arabic.\nIn this paper, we propose a novel, hybrid method which combines linguistic and\nstatistical approaches for Arabic Multi-Word Term extraction. The main\ncontribution of our method is to consider contextual information and both\ntermhood and unithood for association measures at the statistical filtering\nstep. In addition, our technique takes into account the problem of MWT\nvariation in the linguistic filtering step. The performance of the proposed\nstatistical measure (NLC-value) is evaluated using an Arabic environment corpus\nby comparing it with some existing competitors. Experimental results show that\nour NLC-value measure outperforms the other ones in term of precision for both\nbi-grams and tri-grams.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 09:52:41 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Mahdaouy", "Abdelkader El", ""], ["Ouatik", "Sa\u00efd EL Alaoui", ""], ["Gaussier", "Eric", ""]]}, {"id": "1409.3215", "submitter": "Ilya Sutskever", "authors": "Ilya Sutskever and Oriol Vinyals and Quoc V. Le", "title": "Sequence to Sequence Learning with Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 19:55:35 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 12:13:17 GMT"}, {"version": "v3", "created": "Sun, 14 Dec 2014 20:59:51 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Sutskever", "Ilya", ""], ["Vinyals", "Oriol", ""], ["Le", "Quoc V.", ""]]}, {"id": "1409.3512", "submitter": "Udaya Raj Dhungana", "authors": "Udaya Raj Dhungana, Subarna Shakya, Kabita Baral and Bharat Sharma", "title": "Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model of WordNet that is used to disambiguate the\ncorrect sense of polysemy word based on the clue words. The related words for\neach sense of a polysemy word as well as single sense word are referred to as\nthe clue words. The conventional WordNet organizes nouns, verbs, adjectives and\nadverbs together into sets of synonyms called synsets each expressing a\ndifferent concept. In contrast to the structure of WordNet, we developed a new\nmodel of WordNet that organizes the different senses of polysemy words as well\nas the single sense words based on the clue words. These clue words for each\nsense of a polysemy word as well as for single sense word are used to\ndisambiguate the correct meaning of the polysemy word in the given context\nusing knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word\ncan be a noun, verb, adjective or adverb.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 19:01:18 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Dhungana", "Udaya Raj", ""], ["Shakya", "Subarna", ""], ["Baral", "Kabita", ""], ["Sharma", "Bharat", ""]]}, {"id": "1409.3813", "submitter": "Yannick Versley", "authors": "Yannick Versley", "title": "Incorporating Semi-supervised Features into Discontinuous Easy-First\n  Constituent Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper describes adaptations for EaFi, a parser for easy-first parsing of\ndiscontinuous constituents, to adapt it to multiple languages as well as make\nuse of the unlabeled data that was provided as part of the SPMRL shared task\n2014.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 18:37:35 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Versley", "Yannick", ""]]}, {"id": "1409.3870", "submitter": "Jake Williams", "authors": "Jake Ryland Williams, James P. Bagrow, Christopher M. Danforth, and\n  Peter Sheridan Dodds", "title": "Text mixing shapes the anatomy of rank-frequency distributions: A modern\n  Zipfian mechanics for natural language", "comments": "9 pages, 6 figures, and 1 table", "journal-ref": "Phys. Rev. E 91, 052811 (2015)", "doi": "10.1103/PhysRevE.91.052811", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural languages are full of rules and exceptions. One of the most famous\nquantitative rules is Zipf's law which states that the frequency of occurrence\nof a word is approximately inversely proportional to its rank. Though this\n`law' of ranks has been found to hold across disparate texts and forms of data,\nanalyses of increasingly large corpora over the last 15 years have revealed the\nexistence of two scaling regimes. These regimes have thus far been explained by\na hypothesis suggesting a separability of languages into core and non-core\nlexica. Here, we present and defend an alternative hypothesis, that the two\nscaling regimes result from the act of aggregating texts. We observe that text\nmixing leads to an effective decay of word introduction, which we show provides\naccurate predictions of the location and severity of breaks in scaling. Upon\nexamining large corpora from 10 languages in the Project Gutenberg eBooks\ncollection (eBooks), we find emphatic empirical support for the universality of\nour claim.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 21:34:28 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 22:01:50 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 23:56:27 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Williams", "Jake Ryland", ""], ["Bagrow", "James P.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1409.3881", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "An Approach to Reducing Annotation Costs for BioNLP", "comments": "2 pages, 1 figure, 5 tables; appeared in Proceedings of the Workshop\n  on Current Trends in Biomedical Natural Language Processing at ACL\n  (Association for Computational Linguistics) 2008", "journal-ref": "In Proceedings of the Workshop on Current Trends in Biomedical\n  Natural Language Processing, pages 104-105, Columbus, Ohio, June 2008.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a broad range of BioNLP tasks for which active learning (AL) can\nsignificantly reduce annotation costs and a specific AL algorithm we have\ndeveloped is particularly effective in reducing annotation costs for these\ntasks. We have previously developed an AL algorithm called ClosestInitPA that\nworks best with tasks that have the following characteristics: redundancy in\ntraining material, burdensome annotation costs, Support Vector Machines (SVMs)\nwork well for the task, and imbalanced datasets (i.e. when set up as a binary\nclassification problem, one class is substantially rarer than the other). Many\nBioNLP tasks have these characteristics and thus our AL algorithm is a natural\napproach to apply to BioNLP tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 22:40:38 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.3942", "submitter": "Richa Sharma", "authors": "Richa Sharma, Shweta Nigam, Rekha Jain", "title": "Polarity detection movie reviews in hindi language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays peoples are actively involved in giving comments and reviews on\nsocial networking websites and other websites like shopping websites, news\nwebsites etc. large number of people everyday share their opinion on the web,\nresults is a large number of user data is collected .users also find it trivial\ntask to read all the reviews and then reached into the decision. It would be\nbetter if these reviews are classified into some category so that the user\nfinds it easier to read. Opinion Mining or Sentiment Analysis is a natural\nlanguage processing task that mines information from various text forms such as\nreviews, news, and blogs and classify them on the basis of their polarity as\npositive, negative or neutral. But, from the last few years, user content in\nHindi language is also increasing at a rapid rate on the Web. So it is very\nimportant to perform opinion mining in Hindi language as well. In this paper a\nHindi language opinion mining system is proposed. The system classifies the\nreviews as positive, negative and neutral for Hindi language. Negation is also\nhandled in the proposed system. Experimental results using reviews of movies\nshow the effectiveness of the system\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 12:36:56 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Sharma", "Richa", ""], ["Nigam", "Shweta", ""], ["Jain", "Rekha", ""]]}, {"id": "1409.4169", "submitter": "Meenakshi Lakshmanan", "authors": "Rama N. and Meenakshi Lakshmanan", "title": "An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech\n  Synthesis of Sanskrit Verse", "comments": "International Journal of Computer Science and Network Security,\n  Vol.10, No. 1, January 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rendering of Sanskrit poetry from text to speech is a problem that has\nnot been solved before. One reason may be the complications in the language\nitself. We present unique algorithms based on extensive empirical analysis, to\nsynthesize speech from a given text input of Sanskrit verses. Using a\npre-recorded audio units database which is itself tremendously reduced in size\ncompared to the colossal size that would otherwise be required, the algorithms\nwork on producing the best possible, tunefully rendered chanting of the given\nverse. His would enable the visually impaired and those with reading\ndisabilities to easily access the contents of Sanskrit verses otherwise\navailable only in writing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 08:05:36 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["N.", "Rama", ""], ["Lakshmanan", "Meenakshi", ""]]}, {"id": "1409.4354", "submitter": "Meenakshi Lakshmanan", "authors": "S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan", "title": "A Binary Schema and Computational Algorithms to Process Vowel-based\n  Euphonic Conjunctions for Word Searches", "comments": null, "journal-ref": "International Journal of Applied Engineering Research, ISSN\n  0973-4562, Vol. 9, No. 20, 2014, pp 7127-7142", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensively searching for words in Sanskrit E-text is a non-trivial\nproblem because words could change their forms in different contexts. One such\ncontext is sandhi or euphonic conjunctions, which cause a word to change owing\nto the presence of adjacent letters or words. The change wrought by these\npossible conjunctions can be so significant in Sanskrit that a simple search\nfor the word in its given form alone can significantly reduce the success level\nof the search. This work presents a representational schema that represents\nletters in a binary format and reduces Paninian rules of euphonic conjunctions\nto simple bit set-unset operations. The work presents an efficient algorithm to\nprocess vowel-based sandhis using this schema. It further presents another\nalgorithm that uses the sandhi processor to generate the possible transformed\nword forms of a given word to use in a comprehensive word search.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 18:08:33 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Raja", "S. V. Kasmir", ""], ["Rajitha", "V.", ""], ["Lakshmanan", "Meenakshi", ""]]}, {"id": "1409.4364", "submitter": "Meenakshi Lakshmanan", "authors": "S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan", "title": "Computational Algorithms Based on the Paninian System to Process\n  Euphonic Conjunctions for Word Searches", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Security, Vol. 12, No. 8, 2014, pp. 64-76", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for words in Sanskrit E-text is a problem that is accompanied by\ncomplexities introduced by features of Sanskrit such as euphonic conjunctions\nor sandhis. A word could occur in an E-text in a transformed form owing to the\noperation of rules of sandhi. Simple word search would not yield these\ntransformed forms of the word. Further, there is no search engine in the\nliterature that can comprehensively search for words in Sanskrit E-texts taking\neuphonic conjunctions into account. This work presents an optimal binary\nrepresentational schema for letters of the Sanskrit alphabet along with\nalgorithms to efficiently process the sandhi rules of Sanskrit grammar. The\nwork further presents an algorithm that uses the sandhi processing algorithm to\nperform a comprehensive word search on E-text.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 18:19:06 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Raja", "S. V. Kasmir", ""], ["Rajitha", "V.", ""], ["Lakshmanan", "Meenakshi", ""]]}, {"id": "1409.4504", "submitter": "Tao Wang", "authors": "Tao Wang and Hua Zhu", "title": "Voting for Deceptive Opinion Spam Detection", "comments": "arXiv admin note: text overlap with arXiv:1204.2804 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers' purchase decisions are increasingly influenced by user-generated\nonline reviews. Accordingly, there has been growing concern about the potential\nfor posting deceptive opinion spam fictitious reviews that have been\ndeliberately written to sound authentic, to deceive the readers. Existing\napproaches mainly focus on developing automatic supervised learning based\nmethods to help users identify deceptive opinion spams.\n  This work, we used the LSI and Sprinkled LSI technique to reduce the\ndimension for deception detection. We make our contribution to demonstrate what\nLSI is capturing in latent semantic space and reveal how deceptive opinions can\nbe recognized automatically from truthful opinions. Finally, we proposed a\nvoting scheme which integrates different approaches to further improve the\nclassification performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 05:12:50 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Wang", "Tao", ""], ["Zhu", "Hua", ""]]}, {"id": "1409.4614", "submitter": "Bilal Ahmed", "authors": "Bilal Ahmed", "title": "Lexical Normalisation of Twitter Data", "comments": "Removed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter with over 500 million users globally, generates over 100,000 tweets\nper minute . The 140 character limit per tweet, perhaps unintentionally,\nencourages users to use shorthand notations and to strip spellings to their\nbare minimum \"syllables\" or elisions e.g. \"srsly\". The analysis of twitter\nmessages which typically contain misspellings, elisions, and grammatical\nerrors, poses a challenge to established Natural Language Processing (NLP)\ntools which are generally designed with the assumption that the data conforms\nto the basic grammatical structure commonly used in English language. In order\nto make sense of Twitter messages it is necessary to first transform them into\na canonical form, consistent with the dictionary or grammar. This process,\nperformed at the level of individual tokens (\"words\"), is called lexical\nnormalisation. This paper investigates various techniques for lexical\nnormalisation of Twitter data and presents the findings as the techniques are\napplied to process raw data from Twitter.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 12:59:07 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 02:48:53 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 04:56:13 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2015 01:11:53 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Ahmed", "Bilal", ""]]}, {"id": "1409.4617", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter and Christoph Waldhauser", "title": "The Role of Emotions in Propagating Brands in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of word of mouth marketing are emotions. Emotions in texts help\npropagating messages in conventional advertising. In word of mouth scenarios,\nemotions help to engage consumers and incite to propagate the message further.\nWhile the function of emotions in offline marketing in general and word of\nmouth marketing in particular is rather well understood, online marketing can\nonly offer a limited view on the function of emotions. In this contribution we\nseek to close this gap. We therefore investigate how emotions function in\nsocial media. To do so, we collected more than 30,000 brand marketing messages\nfrom the Google+ social networking site. Using state of the art computational\nlinguistics classifiers, we compute the sentiment of these messages. Starting\nout with Poisson regression-based baseline models, we seek to replicate earlier\nfindings using this large data set. We extend upon earlier research by\ncomputing multi-level mixed effects models that compare the function of\nemotions across different industries. We find that while the well known notion\nof activating emotions propagating messages holds in general for our data as\nwell. But there are significant differences between the observed industries.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 13:03:51 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hochreiter", "Ronald", ""], ["Waldhauser", "Christoph", ""]]}, {"id": "1409.4714", "submitter": "Andrzej Kulig", "authors": "Andrzej Kulig, Stanislaw Drozdz, Jaroslaw Kwapien, Pawel Oswiecimka", "title": "Modeling the average shortest path length in growth of word-adjacency\n  networks", "comments": "Accepted for publication in Physical Review E", "journal-ref": "Phys. Rev. E. 91, 032810 (2015)", "doi": "10.1103/PhysRevE.91.032810", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate properties of evolving linguistic networks defined by the\nword-adjacency relation. Such networks belong to the category of networks with\naccelerated growth but their shortest path length appears to reveal the network\nsize dependence of different functional form than the ones known so far. We\nthus compare the networks created from literary texts with their artificial\nsubstitutes based on different variants of the Dorogovtsev-Mendes model and\nobserve that none of them is able to properly simulate the novel asymptotics of\nthe shortest path length. Then, we identify the local chain-like linear growth\ninduced by grammar and style as a missing element in this model and extend it\nby incorporating such effects. It is in this way that a satisfactory agreement\nwith the empirical result is obtained.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 17:49:31 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 15:36:48 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Kulig", "Andrzej", ""], ["Drozdz", "Stanislaw", ""], ["Kwapien", "Jaroslaw", ""], ["Oswiecimka", "Pawel", ""]]}, {"id": "1409.4835", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "Taking into Account the Differences between Actively and Passively\n  Acquired Data: The Case of Active Learning with Support Vector Machines for\n  Imbalanced Datasets", "comments": "4 pages, 5 figures; appeared in Proceedings of Human Language\n  Technologies: The 2009 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics, Companion Volume: Short Papers,\n  pages 137-140, Boulder, Colorado, June 2009. Association for Computational\n  Linguistics", "journal-ref": "Proceedings of HLT: The 2009 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics, Short\n  Papers, pages 137-140, Boulder, Colorado, June 2009. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actively sampled data can have very different characteristics than passively\nsampled data. Therefore, it's promising to investigate using different\ninference procedures during AL than are used during passive learning (PL). This\ngeneral idea is explored in detail for the focused case of AL with\ncost-weighted SVMs for imbalanced data, a situation that arises for many HLT\ntasks. The key idea behind the proposed InitPA method for addressing imbalance\nis to base cost models during AL on an estimate of overall corpus imbalance\ncomputed via a small unbiased sample rather than the imbalance in the labeled\ntraining data, which is the leading method used during PL.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 00:00:11 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.5165", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "A Method for Stopping Active Learning Based on Stabilizing Predictions\n  and the Need for User-Adjustable Stopping", "comments": "9 pages, 3 figures, 5 tables; appeared in Proceedings of the\n  Thirteenth Conference on Computational Natural Language Learning\n  (CoNLL-2009), June 2009", "journal-ref": "In Proceedings of the Thirteenth Conference on Computational\n  Natural Language Learning (CoNLL-2009), pages 39-47, Boulder, Colorado, June\n  2009. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey of existing methods for stopping active learning (AL) reveals the\nneeds for methods that are: more widely applicable; more aggressive in saving\nannotations; and more stable across changing datasets. A new method for\nstopping AL based on stabilizing predictions is presented that addresses these\nneeds. Furthermore, stopping methods are required to handle a broad range of\ndifferent annotation/performance tradeoff valuations. Despite this, the\nexisting body of work is dominated by conservative methods with little (if any)\nattention paid to providing users with control over the behavior of stopping\nmethods. The proposed method is shown to fill a gap in the level of\naggressiveness available for stopping AL and supports providing users with\ncontrol over stopping behavior.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 23:28:59 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.5502", "submitter": "Alexander Kalinin", "authors": "Alexander Kalinin, George Savchenko", "title": "Using crowdsourcing system for creating site-specific statistical\n  machine translation engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crowdsourcing translation approach is an effective tool for globalization\nof site content, but it is also an important source of parallel linguistic\ndata. For the given site, processed with a crowdsourcing system, a\nsentence-aligned corpus can be fetched, which covers a very narrow domain of\nterminology and language patterns - a site-specific domain. These data can be\nused for training and estimation of site-specific statistical machine\ntranslation engine\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 02:50:04 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Kalinin", "Alexander", ""], ["Savchenko", "George", ""]]}, {"id": "1409.5623", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Xiaolu Wang, Peter Sarlin", "title": "Interactive Visual Exploration of Topic Models using Graphs", "comments": "Online demo at http://risklab.fi/demo/topics/. appears in Proceedings\n  of the 2014 Eurographics Conference on Visualization (EuroVis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Probabilistic topic modeling is a popular and powerful family of tools for\nuncovering thematic structure in large sets of unstructured text documents.\nWhile much attention has been directed towards the modeling algorithms and\ntheir various extensions, comparatively few studies have concerned how to\npresent or visualize topic models in meaningful ways. In this paper, we present\na novel design that uses graphs to visually communicate topic structure and\nmeaning. By connecting topic nodes via descriptive keyterms, the graph\nrepresentation reveals topic similarities, topic meaning and shared, ambiguous\nkeyterms. At the same time, the graph can be used for information retrieval\npurposes, to find documents by topic or topic subsets. To exemplify the utility\nof the design, we illustrate its use for organizing and exploring corpora of\nfinancial patents.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 12:26:39 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 12:31:03 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Wang", "Xiaolu", ""], ["Sarlin", "Peter", ""]]}, {"id": "1409.7085", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J.\n  Dorr, Nathaniel W. Filardo, Lori Levin, Scott Miller and Christine Piatko", "title": "Semantically-Informed Syntactic Machine Translation: A Tree-Grafting\n  Approach", "comments": "10 pages, 7 figures, 3 tables; appeared in Proceedings of the Ninth\n  Conference of the Association for Machine Translation in the Americas (AMTA),\n  October 2010", "journal-ref": "In Proceedings of the Ninth Conference of the Association for\n  Machine Translation in the Americas (AMTA), Denver, Colorado, October 2010", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a unified and coherent syntactic framework for supporting a\nsemantically-informed syntactic approach to statistical machine translation.\nSemantically enriched syntactic tags assigned to the target-language training\ntexts improved translation quality. The resulting system significantly\noutperformed a linguistically naive baseline model (Hiero), and reached the\nhighest scores yet reported on the NIST 2009 Urdu-English translation task.\nThis finding supports the hypothesis (posed by many researchers in the MT\ncommunity, e.g., in DARPA GALE) that both syntactic and semantic information\nare critical for improving translation quality---and further demonstrates that\nlarge gains can be achieved for low-resource languages with different word\norder than English.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:16:49 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""], ["Dorr", "Bonnie J.", ""], ["Filardo", "Nathaniel W.", ""], ["Levin", "Lori", ""], ["Miller", "Scott", ""], ["Piatko", "Christine", ""]]}, {"id": "1409.7275", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "The meaning-frequency law in Zipfian optimization models of\n  communication", "comments": null, "journal-ref": "Glottometrics 35, 28-37 (2016)", "doi": null, "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Zipf's meaning-frequency law, words that are more frequent tend\nto have more meanings. Here it is shown that a linear dependency between the\nfrequency of a form and its number of meanings is found in a family of models\nof Zipf's law for word frequencies. This is evidence for a weak version of the\nmeaning-frequency law. Interestingly, that weak law (a) is not an inevitable of\nproperty of the assumptions of the family and (b) is found at least in the\nnarrow regime where those models exhibit Zipf's law for word frequencies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 14:42:02 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 10:27:24 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1409.7336", "submitter": "Juan Pablo C\\'ardenas", "authors": "Juan Pablo C\\'ardenas, Iv\\'an Gonz\\'alez, Gerardo Vidal, Miguel\n  Fuentes", "title": "Does network complexity help organize Babel's library?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL nlin.AO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study properties of texts from the perspective of complex\nnetwork theory. Words in given texts are linked by co-occurrence and\ntransformed into networks, and we observe that these display topological\nproperties common to other complex systems. However, there are some properties\nthat seem to be exclusive to texts; many of these properties depend on the\nfrequency of words in the text, while others seem to be strictly determined by\nthe grammar. Precisely, these properties allow for a categorization of texts as\neither with a sense and others encoded or senseless.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 16:36:21 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 16:49:51 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["C\u00e1rdenas", "Juan Pablo", ""], ["Gonz\u00e1lez", "Iv\u00e1n", ""], ["Vidal", "Gerardo", ""], ["Fuentes", "Miguel", ""]]}, {"id": "1409.7386", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams", "title": "Performance of Stanford and Minipar Parser on Biomedical Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the performance of two dependency parsers, namely Stanford and\nMinipar, on biomedical texts has been reported. The performance of te parsers\nto assignm dependencies between two biomedical concepts that are already proved\nto be connected is not satisfying. Both Stanford and Minipar, being statistical\nparsers, fail to assign dependency relation between two connected concepts if\nthey are distant by at least one clause. Minipar's performance, in terms of\nprecision, recall and the F-score of the attachment score (e.g., correctly\nidentified head in a dependency), to parse biomedical text is also measured\ntaking the Stanford's as a gold standard. The results suggest that Minipar is\nnot suitable yet to parse biomedical texts. In addition, a qualitative\ninvestigation reveals that the difference between working principles of the\nparsers also play a vital role for Minipar's degraded performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:35:27 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Shams", "Rushdi", ""]]}, {"id": "1409.7591", "submitter": "Arun Maiya", "authors": "Arun S. Maiya and Robert M. Rolfe", "title": "Topic Similarity Networks: Visual Analytics for Large Document Sets", "comments": "9 pages; 2014 IEEE International Conference on Big Data (IEEE BigData\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate ways in which to improve the interpretability of LDA topic\nmodels by better analyzing and visualizing their outputs. We focus on examining\nwhat we refer to as topic similarity networks: graphs in which nodes represent\nlatent topics in text collections and links represent similarity among topics.\nWe describe efficient and effective approaches to both building and labeling\nsuch networks. Visualizations of topic models based on these networks are shown\nto be a powerful means of exploring, characterizing, and summarizing large\ncollections of unstructured text documents. They help to \"tease out\"\nnon-obvious connections among different sets of documents and provide insights\ninto how topics form larger themes. We demonstrate the efficacy and\npracticality of these approaches through two case studies: 1) NSF grants for\nbasic research spanning a 14 year period and 2) the entire English portion of\nWikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 15:11:57 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Maiya", "Arun S.", ""], ["Rolfe", "Robert M.", ""]]}, {"id": "1409.7612", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams", "title": "Semi-supervised Classification for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised classification is an interesting idea where classification\nmodels are learned from both labeled and unlabeled data. It has several\nadvantages over supervised classification in natural language processing\ndomain. For instance, supervised classification exploits only labeled data that\nare expensive, often difficult to get, inadequate in quantity, and require\nhuman experts for annotation. On the other hand, unlabeled data are inexpensive\nand abundant. Despite the fact that many factors limit the wide-spread use of\nsemi-supervised classification, it has become popular since its level of\nperformance is empirically as good as supervised classification. This study\nexplores the possibilities and achievements as well as complexity and\nlimitations of semi-supervised classification for several natural langue\nprocessing tasks like parsing, biomedical information processing, text\nclassification, and summarization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:18:44 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Shams", "Rushdi", ""]]}, {"id": "1409.7619", "submitter": "Ekaterina Ovchinnikova", "authors": "Ekaterina Ovchinnikova, Vladimir Zaytsev, Suzanne Wertheim, Ross\n  Israel", "title": "Generating Conceptual Metaphors from Proposition Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary research on computational processing of linguistic metaphors is\ndivided into two main branches: metaphor recognition and metaphor\ninterpretation. We take a different line of research and present an automated\nmethod for generating conceptual metaphors from linguistic data. Given the\ngenerated conceptual metaphors, we find corresponding linguistic metaphors in\ncorpora. In this paper, we describe our approach and its evaluation using\nEnglish and Russian data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 13:54:37 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Ovchinnikova", "Ekaterina", ""], ["Zaytsev", "Vladimir", ""], ["Wertheim", "Suzanne", ""], ["Israel", "Ross", ""]]}, {"id": "1409.7985", "submitter": "Yanchuan Sim", "authors": "Yanchuan Sim and Bryan Routledge and Noah A. Smith", "title": "The Utility of Text: The Case of Amicus Briefs and the Supreme Court", "comments": "Working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.GT cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We explore the idea that authoring a piece of text is an act of maximizing\none's expected utility. To make this idea concrete, we consider the societally\nimportant decisions of the Supreme Court of the United States. Extensive past\nwork in quantitative political science provides a framework for empirically\nmodeling the decisions of justices and how they relate to text. We incorporate\ninto such a model texts authored by amici curiae (\"friends of the court\"\nseparate from the litigants) who seek to weigh in on the decision, then\nexplicitly model their goals in a random utility model. We demonstrate the\nbenefits of this approach in improved vote prediction and the ability to\nperform counterfactual analysis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 03:04:26 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 14:54:53 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 18:47:43 GMT"}, {"version": "v4", "created": "Fri, 10 Oct 2014 00:20:33 GMT"}, {"version": "v5", "created": "Tue, 25 Nov 2014 21:29:15 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Sim", "Yanchuan", ""], ["Routledge", "Bryan", ""], ["Smith", "Noah A.", ""]]}, {"id": "1409.8008", "submitter": "Arjun Das", "authors": "Arjun Das, Utpal Garain", "title": "CRF-based Named Entity Recognition @ICON 2013", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes performance of CRF based systems for Named Entity\nRecognition (NER) in Indian language as a part of ICON 2013 shared task. In\nthis task we have considered a set of language independent features for all the\nlanguages. Only for English a language specific feature, i.e. capitalization,\nhas been added. Next the use of gazetteer is explored for Bengali, Hindi and\nEnglish. The gazetteers are built from Wikipedia and other sources. Test\nresults show that the system achieves the highest F measure of 88% for English\nand the lowest F measure of 69% for both Tamil and Telugu. Note that for the\nleast performing two languages no gazetteer was used. NER in Bengali and Hindi\nfinds accuracy (F measure) of 87% and 79%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 07:11:30 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Das", "Arjun", ""], ["Garain", "Utpal", ""]]}, {"id": "1409.8152", "submitter": "Yelena Mejova", "authors": "Yelena Mejova, Amy X. Zhang, Nicholas Diakopoulos, Carlos Castillo", "title": "Controversy and Sentiment in Online News", "comments": "Computation+Journalism Symposium 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  How do news sources tackle controversial issues? In this work, we take a\ndata-driven approach to understand how controversy interplays with emotional\nexpression and biased language in the news. We begin by introducing a new\ndataset of controversial and non-controversial terms collected using\ncrowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare\nmillions of articles discussing controversial and non-controversial issues over\na span of 7 months. We find that in general, when it comes to controversial\nissues, the use of negative affect and biased language is prevalent, while the\nuse of strong emotion is tempered. We also observe many differences across news\nsources. Using these findings, we show that we can indicate to what extent an\nissue is controversial, by comparing it with other issues in terms of how they\nare portrayed across different media.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 15:23:50 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Mejova", "Yelena", ""], ["Zhang", "Amy X.", ""], ["Diakopoulos", "Nicholas", ""], ["Castillo", "Carlos", ""]]}, {"id": "1409.8309", "submitter": "Youssef Hassan", "authors": "Youssef Hassan, Mohamed Aly and Amir Atiya", "title": "Arabic Spelling Correction using Supervised Learning", "comments": "System description paper that is submitted in the EMNLP 2014\n  conference shared task \"Automatic Arabic Error Correction\" (Mohit et al.,\n  2014) in the Arabic NLP workshop. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of spelling correction in the Arabic\nlanguage utilizing the new corpus provided by QALB (Qatar Arabic Language Bank)\nproject which is an annotated corpus of sentences with errors and their\ncorrections. The corpus contains edit, add before, split, merge, add after,\nmove and other error types. We are concerned with the first four error types as\nthey contribute more than 90% of the spelling errors in the corpus. The\nproposed system has many models to address each error type on its own and then\nintegrating all the models to provide an efficient and robust system that\nachieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58\nincluding all the error types on the development set. Our system participated\nin the QALB 2014 shared task \"Automatic Arabic Error Correction\" and achieved\nan F1 score of 0.6, earning the sixth place out of nine participants.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 20:18:12 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Hassan", "Youssef", ""], ["Aly", "Mohamed", ""], ["Atiya", "Amir", ""]]}, {"id": "1409.8484", "submitter": "Christian Napoli", "authors": "Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana", "title": "An agent-driven semantical identifier using radial basis neural networks\n  and reinforcement learning", "comments": "Published on: Proceedings of the XV Workshop \"Dagli Oggetti agli\n  Agenti\" (WOA 2014), Catania, Italy, Sepember. 25-26, 2014", "journal-ref": "Proceedings of the XV Workshop \"Dagli Oggetti agli Agenti\" (WOA\n  2014), on CEUR-WS, volume 1260, ISSN: 1613-073, Catania, Italy, Sepember.\n  25-26, 2014. http://ceur-ws.org/Vol-1260/", "doi": "10.13140/2.1.1446.7843", "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the huge availability of documents in digital form, and the deception\npossibility raise bound to the essence of digital documents and the way they\nare spread, the authorship attribution problem has constantly increased its\nrelevance. Nowadays, authorship attribution,for both information retrieval and\nanalysis, has gained great importance in the context of security, trust and\ncopyright preservation. This work proposes an innovative multi-agent driven\nmachine learning technique that has been developed for authorship attribution.\nBy means of a preprocessing for word-grouping and time-period related analysis\nof the common lexicon, we determine a bias reference level for the recurrence\nfrequency of the words within analysed texts, and then train a Radial Basis\nNeural Networks (RBPNN)-based classifier to identify the correct author. The\nmain advantage of the proposed approach lies in the generality of the semantic\nanalysis, which can be applied to different contexts and lexical domains,\nwithout requiring any modification. Moreover, the proposed system is able to\nincorporate an external input, meant to tune the classifier, and then\nself-adjust by means of continuous learning reinforcement.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:10:23 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Napoli", "Christian", ""], ["Pappalardo", "Giuseppe", ""], ["Tramontana", "Emiliano", ""]]}, {"id": "1409.8558", "submitter": "Prasanna Kumar Muthukumar", "authors": "Prasanna Kumar Muthukumar and Alan W. Black", "title": "A Deep Learning Approach to Data-driven Parameterizations for\n  Statistical Parametric Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral\ncoefficients as the vocal tract parameterization of the speech signal. Mel\nCepstral coefficients were never intended to work in a parametric speech\nsynthesis framework, but as yet, there has been little success in creating a\nbetter parameterization that is more suited to synthesis. In this paper, we use\ndeep learning algorithms to investigate a data-driven parameterization\ntechnique that is designed for the specific requirements of synthesis. We\ncreate an invertible, low-dimensional, noise-robust encoding of the Mel Log\nSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA is\nthen unwrapped and used as the initialization for a Multi-Layer Perceptron\n(MLP). The MLP is fine-tuned by training it to reconstruct the input at the\noutput layer. This MLP is then split down the middle to form encoding and\ndecoding networks. These networks produce a parameterization of the Mel Log\nSpectrum that is intended to better fulfill the requirements of synthesis.\nResults are reported for experiments conducted using this resulting\nparameterization with the ClusterGen speech synthesizer.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:20:29 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Muthukumar", "Prasanna Kumar", ""], ["Black", "Alan W.", ""]]}, {"id": "1409.8581", "submitter": "Anand M Kumar", "authors": "M. Anand Kumar, V. Dhanalakshmi, K. P. Soman and V. Sharmiladevi", "title": "Improving the Performance of English-Tamil Statistical Machine\n  Translation System using Source-Side Pre-Processing", "comments": "Proc. of Int. Conf. on Advances in Computer Science, AETACS - 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Translation is one of the major oldest and the most active research\narea in Natural Language Processing. Currently, Statistical Machine Translation\n(SMT) dominates the Machine Translation research. Statistical Machine\nTranslation is an approach to Machine Translation which uses models to learn\ntranslation patterns directly from data, and generalize them to translate a new\nunseen text. The SMT approach is largely language independent, i.e. the models\ncan be applied to any language pair. Statistical Machine Translation (SMT)\nattempts to generate translations using statistical methods based on bilingual\ntext corpora. Where such corpora are available, excellent results can be\nattained translating similar texts, but such corpora are still not available\nfor many language pairs. Statistical Machine Translation systems, in general,\nhave difficulty in handling the morphology on the source or the target side\nespecially for morphologically rich languages. Errors in morphology or syntax\nin the target language can have severe consequences on meaning of the sentence.\nThey change the grammatical function of words or the understanding of the\nsentence through the incorrect tense information in verb. Baseline SMT also\nknown as Phrase Based Statistical Machine Translation (PBSMT) system does not\nuse any linguistic information and it only operates on surface word form.\nRecent researches shown that adding linguistic information helps to improve the\naccuracy of the translation with less amount of bilingual corpora. Adding\nlinguistic information can be done using the Factored Statistical Machine\nTranslation system through pre-processing steps. This paper investigates about\nhow English side pre-processing is used to improve the accuracy of\nEnglish-Tamil SMT system.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 04:54:03 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Kumar", "M. Anand", ""], ["Dhanalakshmi", "V.", ""], ["Soman", "K. P.", ""], ["Sharmiladevi", "V.", ""]]}]