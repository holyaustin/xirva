[{"id": "1411.0007", "submitter": "Michael Bloodgood", "authors": "John E. Miller, Michael Bloodgood, Manabu Torii and K. Vijay-Shanker", "title": "Rapid Adaptation of POS Tagging for Domain Specific Uses", "comments": "2 pages, 2 tables; appeared in Proceedings of the HLT-NAACL BioNLP\n  Workshop on Linking Natural Language and Biology, June 2006", "journal-ref": "In Proceedings of the HLT-NAACL BioNLP Workshop on Linking Natural\n  Language and Biology, pages 118-119, New York, New York, June 2006.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-of-speech (POS) tagging is a fundamental component for performing\nnatural language tasks such as parsing, information extraction, and question\nanswering. When POS taggers are trained in one domain and applied in\nsignificantly different domains, their performance can degrade dramatically. We\npresent a methodology for rapid adaptation of POS taggers to new domains. Our\ntechnique is unsupervised in that a manually annotated corpus for the new\ndomain is not necessary. We use suffix information gathered from large amounts\nof raw text as well as orthographic information to increase the lexical\ncoverage. We present an experiment in the Biological domain where our POS\ntagger achieves results comparable to POS taggers specifically trained to this\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 20:04:09 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Miller", "John E.", ""], ["Bloodgood", "Michael", ""], ["Torii", "Manabu", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1411.0129", "submitter": "Stevan Harnad", "authors": "Philippe Vincent-Lamarre, Alexandre Blondin Mass\\'e, Marcos Lopes,\n  M\\'elanie Lord, Odile Marcotte, Stevan Harnad", "title": "The Latent Structure of Dictionaries", "comments": "38 pages, 10 figures, 2 tables, 73 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many words (and which ones) are sufficient to define all other words?\nWhen dictionaries are analyzed as directed graphs with links from defining\nwords to defined words, they reveal a latent structure. Recursively removing\nall words that are reachable by definition but that do not define any further\nwords reduces the dictionary to a Kernel of about 10%. This is still not the\nsmallest number of words that can define all the rest. About 75% of the Kernel\nturns out to be its Core, a Strongly Connected Subset of words with a\ndefinitional path to and from any pair of its words and no word's definition\ndepending on a word outside the set. But the Core cannot define all the rest of\nthe dictionary. The 25% of the Kernel surrounding the Core consists of small\nstrongly connected subsets of words: the Satellites. The size of the smallest\nset of words that can define all the rest (the graph's Minimum Feedback Vertex\nSet or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core,\nhalf-Satellite. But every dictionary has a huge number of MinSets. The Core\nwords are learned earlier, more frequent, and less concrete than the\nSatellites, which in turn are learned earlier and more frequent but more\nconcrete than the rest of the Dictionary. In principle, only one MinSet's words\nwould need to be grounded through the sensorimotor capacity to recognize and\ncategorize their referents. In a dual-code sensorimotor-symbolic model of the\nmental lexicon, the symbolic code could do all the rest via re-combinatory\ndefinition.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 15:52:05 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2016 13:49:33 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Vincent-Lamarre", "Philippe", ""], ["Mass\u00e9", "Alexandre Blondin", ""], ["Lopes", "Marcos", ""], ["Lord", "M\u00e9lanie", ""], ["Marcotte", "Odile", ""], ["Harnad", "Stevan", ""]]}, {"id": "1411.0588", "submitter": "Grigor Iliev", "authors": "Nadezhda Borisova, Grigor Iliev, Elena Karashtranova", "title": "On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using\n  GATE", "comments": "8 pages", "journal-ref": "FMNS2013 (2013) 180-187", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this article, we describe an approach for automatic detection of\nnoun-adjective agreement errors in Bulgarian texts by explaining the necessary\nsteps required to develop a simple Java-based language processing application.\nFor this purpose, we use the GATE language processing framework, which is\ncapable of analyzing texts in Bulgarian language and can be embedded in\nsoftware applications, accessed through a set of Java APIs. In our example\napplication we also demonstrate how to use the functionality of GATE to perform\nregular expressions over annotations for detecting agreement errors in simple\nnoun phrases formed by two words - attributive adjective and a noun, where the\nattributive adjective precedes the noun. The provided code samples can also be\nused as a starting point for implementing natural language processing\nfunctionalities in software applications related to language processing tasks\nlike detection, annotation and retrieval of word groups meeting a specific set\nof criteria.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:05:54 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Borisova", "Nadezhda", ""], ["Iliev", "Grigor", ""], ["Karashtranova", "Elena", ""]]}, {"id": "1411.0778", "submitter": "Tingshao Zhu", "authors": "Xiaolei Huang, Lei Zhang, Tianli Liu, David Chiu, Tingshao Zhu, Xin Li", "title": "Detecting Suicidal Ideation in Chinese Microblogs with Psychological\n  Lexicons", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Suicide is among the leading causes of death in China. However, technical\napproaches toward preventing suicide are challenging and remaining under\ndevelopment. Recently, several actual suicidal cases were preceded by users who\nposted microblogs with suicidal ideation to Sina Weibo, a Chinese social media\nnetwork akin to Twitter. It would therefore be desirable to detect suicidal\nideations from microblogs in real-time, and immediately alert appropriate\nsupport groups, which may lead to successful prevention. In this paper, we\npropose a real-time suicidal ideation detection system deployed over Weibo,\nusing machine learning and known psychological techniques. Currently, we have\nidentified 53 known suicidal cases who posted suicide notes on Weibo prior to\ntheir deaths.We explore linguistic features of these known cases using a\npsychological lexicon dictionary, and train an effective suicidal Weibo post\ndetection model. 6714 tagged posts and several classifiers are used to verify\nthe model. By combining both machine learning and psychological knowledge, SVM\nclassifier has the best performance of different classifiers, yielding an\nF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 03:48:20 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Huang", "Xiaolei", ""], ["Zhang", "Lei", ""], ["Liu", "Tianli", ""], ["Chiu", "David", ""], ["Zhu", "Tingshao", ""], ["Li", "Xin", ""]]}, {"id": "1411.0861", "submitter": "Tingshao Zhu", "authors": "Lei Zhang, Xiaolei Huang, Tianli Liu, Zhenxiang Chen, Tingshao Zhu", "title": "Using Linguistic Features to Estimate Suicide Probability of Chinese\n  Microblog Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If people with high risk of suicide can be identified through social media\nlike microblog, it is possible to implement an active intervention system to\nsave their lives. Based on this motivation, the current study administered the\nSuicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a\nleading microblog service provider in China. Two NLP (Natural Language\nProcessing) methods, the Chinese edition of Linguistic Inquiry and Word Count\n(LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract\nlinguistic features from the Sina Weibo data. We trained predicting models by\nmachine learning algorithm based on these two types of features, to estimate\nsuicide probability based on linguistic features. The experiment results\nindicate that LDA can find topics that relate to suicide probability, and\nimprove the performance of prediction. Our study adds value in prediction of\nsuicidal probability of social network users with their behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 11:06:01 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Zhang", "Lei", ""], ["Huang", "Xiaolei", ""], ["Liu", "Tianli", ""], ["Chen", "Zhenxiang", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1411.0895", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2014.2313410", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic models using probabilistic linear discriminant analysis (PLDA)\ncapture the correlations within feature vectors using subspaces which do not\nvastly expand the model. This allows high dimensional and correlated feature\nspaces to be used, without requiring the estimation of multiple high dimension\ncovariance matrices. In this letter we extend the recently presented PLDA\nmixture model for speech recognition through a tied PLDA approach, which is\nbetter able to control the model size to avoid overfitting. We carried out\nexperiments using the Switchboard corpus, with both mel frequency cepstral\ncoefficient features and bottleneck feature derived from a deep neural network.\nReductions in word error rate were obtained by using tied PLDA, compared with\nthe PLDA mixture model, subspace Gaussian mixture models, and deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 13:11:06 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1411.1006", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi and Azadeh Shakery and Heshaam Faili", "title": "A Probabilistic Translation Method for Dictionary-based Cross-lingual\n  Information Retrieval in Agglutinative Languages", "comments": "The 3rd conference of Computational Linguistic, Sharif University of\n  Technology, November 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translation ambiguity, out of vocabulary words and missing some translations\nin bilingual dictionaries make dictionary-based Cross-language Information\nRetrieval (CLIR) a challenging task. Moreover, in agglutinative languages which\ndo not have reliable stemmers, missing various lexical formations in bilingual\ndictionaries degrades CLIR performance. This paper aims to introduce a\nprobabilistic translation model to solve the ambiguity problem, and also to\nprovide most likely formations of a dictionary candidate. We propose Minimum\nEdit Support Candidates (MESC) method that exploits a monolingual corpus and a\nbilingual dictionary to translate users' native language queries to documents'\nlanguage. Our experiments show that the proposed method outperforms\nstate-of-the-art dictionary-based English-Persian CLIR.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 19:15:59 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 06:55:11 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Shakery", "Azadeh", ""], ["Faili", "Heshaam", ""]]}, {"id": "1411.1147", "submitter": "Waleed Ammar", "authors": "Waleed Ammar, Chris Dyer, Noah A. Smith", "title": "Conditional Random Field Autoencoders for Unsupervised Structured\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for unsupervised learning of structured predictors\nwith overlapping, global features. Each input's latent representation is\npredicted conditional on the observable data using a feature-rich conditional\nrandom field. Then a reconstruction of the input is (re)generated, conditional\non the latent structure, using models for which maximum likelihood estimation\nhas a closed-form. Our autoencoder formulation enables efficient learning\nwithout making unrealistic independence assumptions or restricting the kinds of\nfeatures that can be used. We illustrate insightful connections to traditional\nautoencoders, posterior regularization and multi-view learning. We show\ncompetitive results with instantiations of the model for two canonical NLP\ntasks: part-of-speech induction and bitext word alignment, and show that\ntraining our model can be substantially more efficient than comparable\nfeature-rich baselines.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 04:49:38 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 05:58:04 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Ammar", "Waleed", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1411.1243", "submitter": "Stylianos Kampakis", "authors": "Stylianos Kampakis, Andreas Adamides", "title": "Using Twitter to predict football outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has been proven to be a notable source for predictive modelling on\nvarious domains such as the stock market, the dissemination of diseases or\nsports outcomes. However, such a study has not been conducted in football\n(soccer) so far. The purpose of this research was to study whether data mined\nfrom Twitter can be used for this purpose. We built a set of predictive models\nfor the outcome of football games of the English Premier League for a 3 month\nperiod based on tweets and we studied whether these models can overcome\npredictive models which use only historical data and simple football\nstatistics. Moreover, combined models are constructed using both Twitter and\nhistorical data. The final results indicate that data mined from Twitter can\nindeed be a useful source for predicting games in the Premier League. The final\nTwitter-based model performs significantly better than chance when measured by\nCohen's kappa and is comparable to the model that uses simple statistics and\nhistorical data. Combining both models raises the performance higher than it\nwas achieved by each individual model. Thereby, this study provides evidence\nthat Twitter derived features can indeed provide useful information for the\nprediction of football (soccer) outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 11:50:15 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Kampakis", "Stylianos", ""], ["Adamides", "Andreas", ""]]}, {"id": "1411.1999", "submitter": "Hossam   Ishkewy", "authors": "Hossam Ishkewy, Hany Harb and Hassan Farahat", "title": "Azhary: An Arabic Lexical Ontology", "comments": "appears in International Journal of Web & Semantic Technology\n  (IJWesT) Vol.5, No.4, October 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic language is the most spoken languages in the Semitic languages group,\nand one of the most common languages in the world spoken by more than 422\nmillion. It is also of paramount importance to Muslims, it is a sacred language\nof the Islamic Holly Book (Quran) and prayer (and other acts of worship) in\nIslam is performed only by mastering some of Arabic words. Arabic is also a\nmajor ritual language of a number of Christian churches in the Arab world and\nit is also used in writing several intellectual and religious Jewish books in\nthe Middle Ages. Despite this, there is no semantic Arabic lexicon which\nresearchers can depend on. In this paper we introduce Azhary as a lexical\nontology for the Arabic language. It groups Arabic words into sets of synonyms\ncalled synsets, and records a number of relationships between words such as\nsynonym, antonym, hypernym, hyponym, meronym, holonym and association\nrelations. The ontology contains 26,195 words organized in 13,328 synsets. It\nhas been developed and contrasted against AWN which is the most common\navailable Arabic lexical ontology.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 18:23:00 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Ishkewy", "Hossam", ""], ["Harb", "Hany", ""], ["Farahat", "Hassan", ""]]}, {"id": "1411.2328", "submitter": "Xun Wang", "authors": "Xun Wang", "title": "Modeling Word Relatedness in Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard LDA model suffers the problem that the topic assignment of each word\nis independent and word correlation hence is neglected. To address this\nproblem, in this paper, we propose a model called Word Related Latent Dirichlet\nAllocation (WR-LDA) by incorporating word correlation into LDA topic models.\nThis leads to new capabilities that standard LDA model does not have such as\nestimating infrequently occurring words or multi-language topic modeling.\nExperimental results demonstrate the effectiveness of our model compared with\nstandard LDA.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 05:24:41 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Wang", "Xun", ""]]}, {"id": "1411.2539", "submitter": "Ryan Kiros", "authors": "Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models", "comments": "13 pages. NIPS 2014 deep learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 19:09:41 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kiros", "Ryan", ""], ["Salakhutdinov", "Ruslan", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1411.2645", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Non-crossing dependencies: least effort, not grammar", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-47238-5_10", "report-no": null, "categories": "cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of null hypotheses (in a statistical sense) is common in hard\nsciences but not in theoretical linguistics. Here the null hypothesis that the\nlow frequency of syntactic dependency crossings is expected by an arbitrary\nordering of words is rejected. It is shown that this would require star\ndependency structures, which are both unrealistic and too restrictive. The\nhypothesis of the limited resources of the human brain is revisited. Stronger\nnull hypotheses taking into account actual dependency lengths for the\nlikelihood of crossings are presented. Those hypotheses suggests that crossings\nare likely to reduce when dependencies are shortened. A hypothesis based on\npressure to reduce dependency lengths is more parsimonious than a principle of\nminimization of crossings or a grammatical ban that is totally dissociated from\nthe general and non-linguistic principle of economy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 22:12:56 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1411.2674", "submitter": "Fangjian Guo", "authors": "Fangjian Guo, Charles Blundell, Hanna Wallach and Katherine Heller", "title": "The Bayesian Echo Chamber: Modeling Social Influence via Linguistic\n  Accommodation", "comments": "14 pages, 7 figures, to appear in AISTATS 2015. Fixed minor\n  formatting issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Bayesian Echo Chamber, a new Bayesian generative model for\nsocial interaction data. By modeling the evolution of people's language usage\nover time, this model discovers latent influence relationships between them.\nUnlike previous work on inferring influence, which has primarily focused on\nsimple temporal dynamics evidenced via turn-taking behavior, our model captures\nmore nuanced influence relationships, evidenced via linguistic accommodation\npatterns in interaction content. The model, which is based on a discrete analog\nof the multivariate Hawkes process, permits a fully Bayesian inference\nalgorithm. We validate our model's ability to discover latent influence\npatterns using transcripts of arguments heard by the US Supreme Court and the\nmovie \"12 Angry Men.\" We showcase our model's capabilities by using it to infer\nlatent influence patterns from Federal Open Market Committee meeting\ntranscripts, demonstrating state-of-the-art performance at uncovering social\ndynamics in group discussions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:02:20 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 18:39:13 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 19:37:09 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Guo", "Fangjian", ""], ["Blundell", "Charles", ""], ["Wallach", "Hanna", ""], ["Heller", "Katherine", ""]]}, {"id": "1411.2679", "submitter": "Jiwei Li", "authors": "Jiwei Li, Alan Ritter and Dan Jurafsky", "title": "Inferring User Preferences by Probabilistic Logical Reasoning over\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for inferring the latent attitudes or preferences of\nusers by performing probabilistic first-order logical reasoning over the social\nnetwork graph. Our method answers questions about Twitter users like {\\em Does\nthis user like sushi?} or {\\em Is this user a New York Knicks fan?} by building\na probabilistic model that reasons over user attributes (the user's location or\ngender) and the social network (the user's friends and spouse), via inferences\nlike homophily (I am more likely to like sushi if spouse or friends like sushi,\nI am more likely to like the Knicks if I live in New York). The algorithm uses\ndistant supervision, semi-supervised data harvesting and vector space models to\nextract user attributes (e.g. spouse, education, location) and preferences\n(likes and dislikes) from text. The extracted propositions are then fed into a\nprobabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft\nLogic). Our experiments show that probabilistic logical reasoning significantly\nimproves the performance on attribute and relation extraction, and also\nachieves an F-score of 0.791 at predicting a users likes or dislikes,\nsignificantly better than two strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:53:21 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Li", "Jiwei", ""], ["Ritter", "Alan", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1411.2738", "submitter": "Xin Rong", "authors": "Xin Rong", "title": "word2vec Parameter Learning Explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The word2vec model and application by Mikolov et al. have attracted a great\namount of attention in recent two years. The vector representations of words\nlearned by word2vec models have been shown to carry semantic meanings and are\nuseful in various NLP tasks. As an increasing number of researchers would like\nto experiment with word2vec or similar techniques, I notice that there lacks a\nmaterial that comprehensively explains the parameter learning process of word\nembedding models in details, thus preventing researchers that are non-experts\nin neural networks from understanding the working mechanism of such models.\n  This note provides detailed derivations and explanations of the parameter\nupdate equations of the word2vec models, including the original continuous\nbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization\ntechniques, including hierarchical softmax and negative sampling. Intuitive\ninterpretations of the gradient equations are also provided alongside\nmathematical derivations.\n  In the appendix, a review on the basics of neuron networks and\nbackpropagation is provided. I also created an interactive demo, wevi, to\nfacilitate the intuitive understanding of the model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 09:24:00 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 19:33:04 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2016 21:35:51 GMT"}, {"version": "v4", "created": "Sun, 5 Jun 2016 07:17:40 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Rong", "Xin", ""]]}, {"id": "1411.3146", "submitter": "Karl Moritz Hermann", "authors": "Karl Moritz Hermann", "title": "Distributed Representations for Compositional Semantics", "comments": "DPhil Thesis, University of Oxford, Submitted and accepted in 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical representation of semantics is a key issue for Natural\nLanguage Processing (NLP). A lot of research has been devoted to finding ways\nof representing the semantics of individual words in vector spaces.\nDistributional approaches --- meaning distributed representations that exploit\nco-occurrence statistics of large corpora --- have proved popular and\nsuccessful across a number of tasks. However, natural language usually comes in\nstructures beyond the word level, with meaning arising not only from the\nindividual words but also the structure they are contained in at the phrasal or\nsentential level. Modelling the compositional process by which the meaning of\nan utterance arises from the meaning of its parts is an equally fundamental\ntask of NLP.\n  This dissertation explores methods for learning distributed semantic\nrepresentations and models for composing these into representations for larger\nlinguistic units. Our underlying hypothesis is that neural models are a\nsuitable vehicle for learning semantically rich representations and that such\nrepresentations in turn are suitable vehicles for solving important tasks in\nnatural language processing. The contribution of this thesis is a thorough\nevaluation of our hypothesis, as part of which we introduce several new\napproaches to representation learning and compositional semantics, as well as\nmultiple state-of-the-art models which apply distributed semantic\nrepresentations to various tasks in NLP.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 11:26:51 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Hermann", "Karl Moritz", ""]]}, {"id": "1411.3315", "submitter": "Rami Al-Rfou", "authors": "Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena", "title": "Statistically Significant Detection of Linguistic Change", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 20:37:08 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Al-Rfou", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1411.3561", "submitter": "Prabhsimran Singh", "authors": "Prabhsimran Singh and Amritpal Singh", "title": "A Text to Speech (TTS) System with English to Punjabi Conversion", "comments": "5 pages, 8 figures, 3 tables", "journal-ref": "International Journal of Computer and Communication System\n  Engineering, Volume 1, Issue 04, December 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper aims to show how an application can be developed that converts the\nEnglish language into the Punjabi Language, and the same application can\nconvert the Text to Speech(TTS) i.e. pronounce the text. This application can\nbe really beneficial for those with special needs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 14:44:00 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Singh", "Prabhsimran", ""], ["Singh", "Amritpal", ""]]}, {"id": "1411.3827", "submitter": "EPTCS", "authors": "Antonin Delpeuch (University of Oxford)", "title": "Autonomization of Monoidal Categories", "comments": "In Proceedings ACT 2019, arXiv:2009.06334", "journal-ref": "EPTCS 323, 2020, pp. 24-43", "doi": "10.4204/EPTCS.323.3", "report-no": null, "categories": "math.CT cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that contrary to common belief in the DisCoCat community, a monoidal\ncategory is all that is needed to define a categorical compositional model of\nnatural language. This relies on a construction which freely adds adjoints to a\nmonoidal category. In the case of distributional semantics, this broadens the\nrange of available models, to include non-linear maps and cartesian products\nfor instance. We illustrate the applications of this principle to various\ndistributional models of meaning.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 08:44:22 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 17:09:47 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 13:34:58 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 02:13:15 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Delpeuch", "Antonin", "", "University of Oxford"]]}, {"id": "1411.4072", "submitter": "Bishan Yang", "authors": "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng", "title": "Learning Multi-Relational Semantics Using Neural-Embedding Models", "comments": "7 pages, 2 figures, NIPS 2014 workshop on Learning Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a unified framework for modeling multi-relational\nrepresentations, scoring, and learning, and conduct an empirical study of\nseveral recent multi-relational embedding models under the framework. We\ninvestigate the different choices of relation operators based on linear and\nbilinear transformations, and also the effects of entity representations by\nincorporating unsupervised vectors pre-trained on extra textual resources. Our\nresults show several interesting findings, enabling the design of a simple\nembedding model that achieves the new state-of-the-art performance on a popular\nknowledge base completion task evaluated on Freebase.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:08:01 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Yang", "Bishan", ""], ["Yih", "Wen-tau", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1411.4109", "submitter": "Glenn Hofford", "authors": "Glenn R. Hofford", "title": "Resolution of Difficult Pronouns Using the ROSS Method", "comments": "106 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new natural language understanding method for disambiguation of difficult\npronouns is described. Difficult pronouns are those pronouns for which a level\nof world or domain knowledge is needed in order to perform anaphoral or other\ntypes of resolution. Resolution of difficult pronouns may in some cases require\na prior step involving the application of inference to a situation that is\nrepresented by the natural language text. A general method is described: it\nperforms entity resolution and pronoun resolution. An extension to the general\npronoun resolution method performs inference as an embedded commonsense\nreasoning method. The general method and the embedded method utilize features\nof the ROSS representational scheme; in particular the methods use ROSS\nontology classes and the ROSS situation model. The overall method is a working\nsolution that solves the following Winograd schemas: a) trophy and suitcase, b)\nperson lifts person, c) person pays detective, and d) councilmen and\ndemonstrators.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 03:43:01 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Hofford", "Glenn R.", ""]]}, {"id": "1411.4114", "submitter": "Jongwon Ha", "authors": "Ha Jong Won, Li Gwang Chol, Kim Hyok Chol, Li Kum Song (College of\n  Computer Science, Kim Il Sung University)", "title": "Definition of Visual Speech Element and Research on a Method of\n  Extracting Feature Vector for Korean Lip-Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we defined the viseme (visual speech element) and described\nabout the method of extracting visual feature vector. We defined the 10 visemes\nbased on vowel by analyzing of Korean utterance and proposed the method of\nextracting the 20-dimensional visual feature vector, combination of static\nfeatures and dynamic features. Lastly, we took an experiment in recognizing\nwords based on 3-viseme HMM and evaluated the efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 05:44:10 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Won", "Ha Jong", "", "College of\n  Computer Science, Kim Il Sung University"], ["Chol", "Li Gwang", "", "College of\n  Computer Science, Kim Il Sung University"], ["Chol", "Kim Hyok", "", "College of\n  Computer Science, Kim Il Sung University"], ["Song", "Li Kum", "", "College of\n  Computer Science, Kim Il Sung University"]]}, {"id": "1411.4116", "submitter": "Jack Cheng J", "authors": "Jianpeng Cheng, Dimitri Kartsaklis, Edward Grefenstette", "title": "Investigating the Role of Prior Disambiguation in Deep-learning\n  Compositional Models of Meaning", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to explore the effect of prior disambiguation on neural\nnetwork- based compositional models, with the hope that better semantic\nrepresentations for text compounds can be produced. We disambiguate the input\nword vectors before they are fed into a compositional deep net. A series of\nevaluations shows the positive effect of prior disambiguation for such deep\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 06:32:49 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Kartsaklis", "Dimitri", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1411.4166", "submitter": "Manaal Faruqui", "authors": "Manaal Faruqui and Jesse Dodge and Sujay K. Jauhar and Chris Dyer and\n  Eduard Hovy and Noah A. Smith", "title": "Retrofitting Word Vectors to Semantic Lexicons", "comments": "Proceedings of NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector space word representations are learned from distributional information\nof words in large corpora. Although such statistics are semantically\ninformative, they disregard the valuable information that is contained in\nsemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This\npaper proposes a method for refining vector space representations using\nrelational information from semantic lexicons by encouraging linked words to\nhave similar vector representations, and it makes no assumptions about how the\ninput vectors were constructed. Evaluated on a battery of standard lexical\nsemantic evaluation tasks in several languages, we obtain substantial\nimprovements starting with a variety of word vector models. Our refinement\nmethod outperforms prior techniques for incorporating semantic lexicons into\nthe word vector training algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 17:34:20 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 04:16:50 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 03:13:17 GMT"}, {"version": "v4", "created": "Sun, 22 Mar 2015 17:55:20 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Faruqui", "Manaal", ""], ["Dodge", "Jesse", ""], ["Jauhar", "Sujay K.", ""], ["Dyer", "Chris", ""], ["Hovy", "Eduard", ""], ["Smith", "Noah A.", ""]]}, {"id": "1411.4194", "submitter": "Glenn Hofford", "authors": "Glenn R. Hofford", "title": "ROSS User's Guide and Reference Manual (Version 1.0)", "comments": "128 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROSS method is a new approach in the area of knowledge representation\nthat is useful for many artificial intelligence and natural language\nunderstanding representation and reasoning tasks. (ROSS stands for\n\"Representation\", \"Ontology\", \"Structure\", \"Star\" language). ROSS is a physical\nsymbol-based representational scheme. ROSS provides a complex model for the\ndeclarative representation of physical structure and for the representation of\nprocesses and causality. From the metaphysical perspective, the ROSS view of\nexternal reality involves a 4D model, wherein discrete single-time-point\nunit-sized locations with states are the basis for all objects, processes and\naspects that can be modeled. ROSS includes a language called \"Star\" for the\nspecification of ontology classes. The ROSS method also includes a formal\nscheme called the \"instance model\". Instance models are used in the area of\nnatural language meaning representation to represent situations. This document\nis an in-depth specification of the ROSS method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 22:47:35 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Hofford", "Glenn R.", ""]]}, {"id": "1411.4455", "submitter": "Miao Fan", "authors": "Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng,\n  Edward Y. Chang", "title": "Errata: Distant Supervision for Relation Extraction with Matrix\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essence of distantly supervised relation extraction is that it is an\nincomplete multi-label classification problem with sparse and noisy features.\nTo tackle the sparsity and noise challenges, we propose solving the\nclassification problem using matrix completion on factorized matrix of\nminimized rank. We formulate relation classification as completing the unknown\nlabels of testing items (entity pairs) in a sparse matrix that concatenates\ntraining and testing textual features with training labels. Our algorithmic\nframework is based on the assumption that the rank of item-by-feature and\nitem-by-label joint matrix is low. We apply two optimization models to recover\nthe underlying low-rank matrix leveraging the sparsity of feature-label matrix.\nThe matrix completion problem is then solved by the fixed point continuation\n(FPC) algorithm, which can find the global optimum. Experiments on two widely\nused datasets with different dimensions of textual features demonstrate that\nour low-rank matrix completion approach significantly outperforms the baseline\nand the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 12:43:30 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Fan", "Miao", ""], ["Zhao", "Deli", ""], ["Zhou", "Qiang", ""], ["Liu", "Zhiyuan", ""], ["Zheng", "Thomas Fang", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1411.4472", "submitter": "Andrej Gajduk", "authors": "Andrej Gajduk and Ljupco Kocarev", "title": "Opinion mining of text documents written in Macedonian language", "comments": "In press, MASA proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to extract public opinion from web portals such as review sites,\nsocial networks and blogs will enable companies and individuals to form a view,\nan attitude and make decisions without having to do lengthy and costly\nresearches and surveys. In this paper machine learning techniques are used for\ndetermining the polarity of forum posts on kajgana which are written in\nMacedonian language. The posts are classified as being positive, negative or\nneutral. We test different feature metrics and classifiers and provide detailed\nevaluation of their participation in improving the overall performance on a\nmanually generated dataset. By achieving 92% accuracy, we show that the\nperformance of systems for automated opinion mining is comparable to a human\nevaluator, thus making it a viable option for text data analysis. Finally, we\npresent a few statistics derived from the forum posts using the developed\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 13:36:49 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Gajduk", "Andrej", ""], ["Kocarev", "Ljupco", ""]]}, {"id": "1411.4614", "submitter": "Pascal Vaillant", "authors": "Pascal Vaillant, Jean-Baptiste Lamy", "title": "Using graph transformation algorithms to generate natural language\n  equivalents of icons expressing medical concepts", "comments": "Presented at the TSD 2014 conference: Text, Speech and Dialogue, 17th\n  international conference. Brno, Czech Republic, September 8-12, 2014. 10\n  pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical language addresses the need to communicate medical information in\na synthetic way. Medical concepts are expressed by icons conveying fast visual\ninformation about patients' current state or about the known effects of drugs.\nIn order to increase the visual language's acceptance and usability, a natural\nlanguage generation interface is currently developed. In this context, this\npaper describes the use of an informatics method ---graph transformation--- to\nprepare data consisting of concepts in an OWL-DL ontology for use in a natural\nlanguage generation component. The OWL concept may be considered as a\nstar-shaped graph with a central node. The method transforms it into a graph\nrepresenting the deep semantic structure of a natural language phrase. This\nwork may be of future use in other contexts where ontology concepts have to be\nmapped to half-formalized natural language expressions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 05:09:40 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Vaillant", "Pascal", ""], ["Lamy", "Jean-Baptiste", ""]]}, {"id": "1411.4618", "submitter": "Christopher Burges", "authors": "Christopher J.C. Burges, Erin Renshaw, and Andrzej Pastusiak", "title": "Relations World: A Possibilistic Graphical Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the idea of using a \"possibilistic graphical model\" as the basis\nfor a world model that drives a dialog system. As a first step we have\ndeveloped a system that uses text-based dialog to derive a model of the user's\nfamily relations. The system leverages its world model to infer relational\ntriples, to learn to recover from upstream coreference resolution errors and\nambiguities, and to learn context-dependent paraphrase models. We also explore\nsome theoretical aspects of the underlying graphical model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:15:00 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Burges", "Christopher J. C.", ""], ["Renshaw", "Erin", ""], ["Pastusiak", "Andrzej", ""]]}, {"id": "1411.4825", "submitter": "Claudia Schon", "authors": "Ulrich Furbach and Claudia Schon and Frieder Stolzenburg", "title": "Cognitive Systems and Question Answering", "comments": null, "journal-ref": "Industrie 4.0 Management, 31(1):29-32, 2015", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper briefly characterizes the field of cognitive computing. As an\nexemplification, the field of natural language question answering is introduced\ntogether with its specific challenges. A possibility to master these challenges\nis illustrated by a detailed presentation of the LogAnswer system, which is a\nsuccessful representative of the field of natural language question answering.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 12:37:00 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Furbach", "Ulrich", ""], ["Schon", "Claudia", ""], ["Stolzenburg", "Frieder", ""]]}, {"id": "1411.4925", "submitter": "Alejandro Ramos Soto", "authors": "A. Ramos-Soto and A. Bugar\\'in and S. Barro and J. Taboada", "title": "Linguistic Descriptions for Automatic Generation of Textual Short-Term\n  Weather Forecasts on Real Prediction Data", "comments": "13 pages, 20 figures, IEEE Transactions on Fuzzy Systems, 2014", "journal-ref": null, "doi": "10.1109/TFUZZ.2014.2328011", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an application which automatically generates textual\nshort-term weather forecasts for every municipality in Galicia (NW Spain),\nusing the real data provided by the Galician Meteorology Agency (MeteoGalicia).\nThis solution combines in an innovative way computing with perceptions\ntechniques and strategies for linguistic description of data together with a\nnatural language generation (NLG) system. The application, named GALiWeather,\nextracts relevant information from weather forecast input data and encodes it\ninto intermediate descriptions using linguistic variables and temporal\nreferences. These descriptions are later translated into natural language texts\nby the natural language generation system. The obtained forecast results have\nbeen thoroughly validated by an expert meteorologist from MeteoGalicia using a\nquality assessment methodology which covers two key dimensions of a text: the\naccuracy of its content and the correctness of its form. Following this\nvalidation GALiWeather will be released as a real service offering custom\nforecasts for a wide public.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 17:35:59 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Ramos-Soto", "A.", ""], ["Bugar\u00edn", "A.", ""], ["Barro", "S.", ""], ["Taboada", "J.", ""]]}, {"id": "1411.4952", "submitter": "Piotr Doll\\'ar", "authors": "Hao Fang and Saurabh Gupta and Forrest Iandola and Rupesh Srivastava\n  and Li Deng and Piotr Doll\\'ar and Jianfeng Gao and Xiaodong He and Margaret\n  Mitchell and John C. Platt and C. Lawrence Zitnick and Geoffrey Zweig", "title": "From Captions to Visual Concepts and Back", "comments": "version corresponding to CVPR15 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatically generating image\ndescriptions: visual detectors, language models, and multimodal similarity\nmodels learnt directly from a dataset of image captions. We use multiple\ninstance learning to train visual detectors for words that commonly occur in\ncaptions, including many different parts of speech such as nouns, verbs, and\nadjectives. The word detector outputs serve as conditional inputs to a\nmaximum-entropy language model. The language model learns from a set of over\n400,000 image descriptions to capture the statistics of word usage. We capture\nglobal semantics by re-ranking caption candidates using sentence-level features\nand a deep multimodal similarity model. Our system is state-of-the-art on the\nofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When\nhuman judges compare the system captions to ones written by other people on our\nheld-out test set, the system captions have equal or better quality 34% of the\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 18:23:45 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 20:19:56 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2015 18:05:07 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Fang", "Hao", ""], ["Gupta", "Saurabh", ""], ["Iandola", "Forrest", ""], ["Srivastava", "Rupesh", ""], ["Deng", "Li", ""], ["Doll\u00e1r", "Piotr", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Mitchell", "Margaret", ""], ["Platt", "John C.", ""], ["Zitnick", "C. Lawrence", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "1411.4960", "submitter": "Ana Me\\v{s}trovi\\'c", "authors": "Hana Rizvi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c, Ana Me\\v{s}trovi\\'c", "title": "Network Motifs Analysis of Croatian Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse network motifs in the co-occurrence directed\nnetworks constructed from five different texts (four books and one portal) in\nthe Croatian language. After preparing the data and network construction, we\nperform the network motif analysis. We analyse the motif frequencies and\nZ-scores in the five networks. We present the triad significance profile for\nfive datasets. Furthermore, we compare our results with the existing results\nfor the linguistic networks. Firstly, we show that the triad significance\nprofile for the Croatian language is very similar with the other languages and\nall the networks belong to the same family of networks. However, there are\ncertain differences between the Croatian language and other analysed languages.\nWe conclude that this is due to the free word-order of the Croatian language.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 18:46:36 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Rizvi\u0107", "Hana", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""], ["Me\u0161trovi\u0107", "Ana", ""]]}, {"id": "1411.5379", "submitter": "Kai Zhao", "authors": "Kai Zhao, Liang Huang", "title": "Type-Driven Incremental Semantic Parsing with Polymorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic parsing has made significant progress, but most current semantic\nparsers are extremely slow (CKY-based) and rather primitive in representation.\nWe introduce three new techniques to tackle these problems. First, we design\nthe first linear-time incremental shift-reduce-style semantic parsing algorithm\nwhich is more efficient than conventional cubic-time bottom-up semantic\nparsers. Second, our parser, being type-driven instead of syntax-driven, uses\ntype-checking to decide the direction of reduction, which eliminates the need\nfor a syntactic grammar such as CCG. Third, to fully exploit the power of\ntype-driven semantic parsing beyond simple types (such as entities and truth\nvalues), we borrow from programming language theory the concepts of subtype\npolymorphism and parametric polymorphism to enrich the type system in order to\nbetter guide the parsing. Our system learns very accurate parses in GeoQuery,\nJobs and Atis domains.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 21:06:15 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 21:35:32 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 16:41:48 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Zhao", "Kai", ""], ["Huang", "Liang", ""]]}, {"id": "1411.5595", "submitter": "Tianze Shi", "authors": "Tianze Shi, Zhiyuan Liu", "title": "Linking GloVe with word2vec", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Global Vectors for word representation (GloVe), introduced by Jeffrey\nPennington et al. is reported to be an efficient and effective method for\nlearning vector representations of words. State-of-the-art performance is also\nprovided by skip-gram with negative-sampling (SGNS) implemented in the word2vec\ntool. In this note, we explain the similarities between the training objectives\nof the two models, and show that the objective of SGNS is similar to the\nobjective of a specialized form of GloVe, though their cost functions are\ndefined differently.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 16:39:28 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 06:46:18 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Shi", "Tianze", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1411.5654", "submitter": "Xinlei Chen", "authors": "Xinlei Chen and C. Lawrence Zitnick", "title": "Learning a Recurrent Visual Representation for Image Caption Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the bi-directional mapping between images and their\nsentence-based descriptions. We propose learning this mapping using a recurrent\nneural network. Unlike previous approaches that map both sentences and images\nto a common embedding, we enable the generation of novel sentences given an\nimage. Using the same model, we can also reconstruct the visual features\nassociated with an image given its visual description. We use a novel recurrent\nvisual memory that automatically learns to remember long-term visual concepts\nto aid in both sentence generation and visual feature reconstruction. We\nevaluate our approach on several tasks. These include sentence generation,\nsentence retrieval and image retrieval. State-of-the-art results are shown for\nthe task of generating novel image descriptions. When compared to human\ngenerated captions, our automatically generated captions are preferred by\nhumans over $19.8\\%$ of the time. Results are better than or comparable to\nstate-of-the-art results on the image and sentence retrieval tasks for methods\nusing similar visual features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:50:27 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Chen", "Xinlei", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "1411.5726", "submitter": "Ramakrishna  Vedantam", "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick and Devi Parikh", "title": "CIDEr: Consensus-based Image Description Evaluation", "comments": "To appear in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing an image with a sentence is a long-standing\nchallenge in computer vision and natural language processing. Due to recent\nprogress in object detection, attribute classification, action recognition,\netc., there is renewed interest in this area. However, evaluating the quality\nof descriptions has proven to be challenging. We propose a novel paradigm for\nevaluating image descriptions that uses human consensus. This paradigm consists\nof three main parts: a new triplet-based method of collecting human annotations\nto measure consensus, a new automated metric (CIDEr) that captures consensus,\nand two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences\ndescribing each image. Our simple metric captures human judgment of consensus\nbetter than existing metrics across sentences generated by various sources. We\nalso evaluate five state-of-the-art image description approaches using this new\nprotocol and provide a benchmark for future comparisons. A version of CIDEr\nnamed CIDEr-D is available as a part of MS COCO evaluation server to enable\nsystematic evaluation and benchmarking.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 23:54:35 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 01:42:20 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1411.5732", "submitter": "Suleyman Cetintas", "authors": "Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park,\n  Ron Tzur", "title": "A Joint Probabilistic Classification Model of Relevant and Irrelevant\n  Sentences in Mathematical Word Problems", "comments": "appears in Journal of Educational Data Mining (JEDM, 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the difficulty level of math word problems is an important task\nfor many educational applications. Identification of relevant and irrelevant\nsentences in math word problems is an important step for calculating the\ndifficulty levels of such problems. This paper addresses a novel application of\ntext categorization to identify two types of sentences in mathematical word\nproblems, namely relevant and irrelevant sentences. A novel joint probabilistic\nclassification model is proposed to estimate the joint probability of\nclassification decisions for all sentences of a math word problem by utilizing\nthe correlation among all sentences along with the correlation between the\nquestion sentence and other sentences, and sentence text. The proposed model is\ncompared with i) a SVM classifier which makes independent classification\ndecisions for individual sentences by only using the sentence text and ii) a\nnovel SVM classifier that considers the correlation between the question\nsentence and other sentences along with the sentence text. An extensive set of\nexperiments demonstrates the effectiveness of the joint probabilistic\nclassification model for identifying relevant and irrelevant sentences as well\nas the novel SVM classifier that utilizes the correlation between the question\nsentence and other sentences. Furthermore, empirical results and analysis show\nthat i) it is highly beneficial not to remove stopwords and ii) utilizing part\nof speech tagging does not make a significant improvement although it has been\nshown to be effective for the related task of math word problem type\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:53:02 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Cetintas", "Suleyman", ""], ["Si", "Luo", ""], ["Xin", "Yan Ping", ""], ["Zhang", "Dake", ""], ["Park", "Joo Young", ""], ["Tzur", "Ron", ""]]}, {"id": "1411.5796", "submitter": "Rajveer Kaur", "authors": "Rajveer Kaur, Saurabh Sharma", "title": "Pre-processing of Domain Ontology Graph Generation System in Punjabi", "comments": "6 pages, 17 figures, 1 table, \"Published with International Journal\n  of Engineering Trends and Technology (IJETT)\"", "journal-ref": "International Journal of Engineering Trends and Technology\n  (IJETT), V17(3),141-146, Nov 2014. Published by Seventh Sense Research Group", "doi": "10.14445/22315381/IJETT-V17P229", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes pre-processing phase of ontology graph generation system\nfrom Punjabi text documents of different domains. This research paper focuses\non pre-processing of Punjabi text documents. Pre-processing is structured\nrepresentation of the input text. Pre-processing of ontology graph generation\nincludes allowing input restrictions to the text, removal of special symbols\nand punctuation marks, removal of duplicate terms, removal of stop words,\nextract terms by matching input terms with dictionary and gazetteer lists\nterms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 08:50:30 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Kaur", "Rajveer", ""], ["Sharma", "Saurabh", ""]]}, {"id": "1411.6699", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Jacob Eisenstein", "title": "One Vector is Not Enough: Entity-Augmented Distributional Semantics for\n  Discourse Relations", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relations bind smaller linguistic units into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked arguments. A more subtle\nchallenge is that it is not enough to represent the meaning of each argument of\na discourse relation, because the relation may depend on links between\nlower-level components, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted from the\ndistributional representations of the arguments, and also of their coreferent\nentity mentions. The resulting system obtains substantial improvements over the\nprevious state-of-the-art in predicting implicit discourse relations in the\nPenn Discourse Treebank.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 01:25:56 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Ji", "Yangfeng", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1411.6718", "submitter": "Mohamed Aly", "authors": "Mahmoud Nabil, Mohamed Aly, Amir Atiya", "title": "LABR: A Large Scale Arabic Sentiment Analysis Benchmark", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LABR, the largest sentiment analysis dataset to-date for the\nArabic language. It consists of over 63,000 book reviews, each rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset, and present its\nstatistics. We explore using the dataset for two tasks: (1) sentiment polarity\nclassification; and (2) ratings classification. Moreover, we provide standard\nsplits of the dataset into training, validation and testing, for both polarity\nand ratings classification, in both balanced and unbalanced settings. We extend\nour previous work by performing a comprehensive analysis on the dataset. In\nparticular, we perform an extended survey of the different classifiers\ntypically used for the sentiment polarity classification problem. We also\nconstruct a sentiment lexicon from the dataset that contains both single and\ncompound sentiment words and we explore its effectiveness. We make the dataset\nand experimental details publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 03:48:56 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 08:35:59 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Nabil", "Mahmoud", ""], ["Aly", "Mohamed", ""], ["Atiya", "Amir", ""]]}, {"id": "1411.7820", "submitter": "Vivi Nastase", "authors": "Vivi Nastase and Angela Fahrni", "title": "Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic\n  Models and Encyclopedic Knowledge", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for coarse-grained cross-lingual alignment of comparable\ntexts: segments consisting of contiguous paragraphs that discuss the same theme\n(e.g. history, economy) are aligned based on induced multilingual topics. The\nmethod combines three ideas: a two-level LDA model that filters out words that\ndo not convey themes, an HMM that models the ordering of themes in the\ncollection of documents, and language-independent concept annotations to serve\nas a cross-language bridge and to strengthen the connection between paragraphs\nin the same segment through concept relations. The method is evaluated on\nEnglish and French data previously used for monolingual alignment. The results\nshow state-of-the-art performance in both monolingual and cross-lingual\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 11:33:02 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Nastase", "Vivi", ""], ["Fahrni", "Angela", ""]]}, {"id": "1411.7942", "submitter": "Tamara Polajnar", "authors": "Tamara Polajnar, Laura Rimell, Stephen Clark", "title": "Using Sentence Plausibility to Learn the Semantics of Transitive Verbs", "comments": "Full updated paper for NIPS learning semantics workshop, with some\n  minor errata fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional approach to compositional distributional semantics considers\ntransitive verbs to be linear maps that transform the distributional vectors\nrepresenting nouns into a vector representing a sentence. We conduct an initial\ninvestigation that uses a matrix consisting of the parameters of a logistic\nregression classifier trained on a plausibility task as a transitive verb\nfunction. We compare our method to a commonly used corpus-based method for\nconstructing a verb matrix and find that the plausibility training may be more\neffective for disambiguation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:57:34 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 14:19:29 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Polajnar", "Tamara", ""], ["Rimell", "Laura", ""], ["Clark", "Stephen", ""]]}]