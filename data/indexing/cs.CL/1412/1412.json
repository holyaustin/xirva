[{"id": "1412.0696", "submitter": "Shuyang Gao", "authors": "Shuyang Gao, Greg Ver Steeg and Aram Galstyan", "title": "Understanding confounding effects in linguistic coordination: an\n  information-theoretic approach", "comments": null, "journal-ref": "PLoS ONE 10(6): e0130167, 2015", "doi": "10.1371/journal.pone.0130167", "report-no": null, "categories": "cs.CL cs.IT cs.SI math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest an information-theoretic approach for measuring stylistic\ncoordination in dialogues. The proposed measure has a simple predictive\ninterpretation and can account for various confounding factors through proper\nconditioning. We revisit some of the previous studies that reported strong\nsignatures of stylistic accommodation, and find that a significant part of the\nobserved coordination can be attributed to a simple confounding effect - length\ncoordination. Specifically, longer utterances tend to be followed by longer\nresponses, which gives rise to spurious correlations in the other stylistic\nfeatures. We propose a test to distinguish correlations in length due to\ncontextual factors (topic of conversation, user verbosity, etc.) and\nturn-by-turn coordination. We also suggest a test to identify whether stylistic\ncoordination persists even after accounting for length coordination and\ncontextual factors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 21:27:18 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 17:31:07 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Gao", "Shuyang", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1412.0751", "submitter": "John Wieting", "authors": "John Wieting", "title": "Tiered Clustering to Improve Lexical Entailment", "comments": "Paper for course project for Advanced NLP Spring 2013. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many tasks in Natural Language Processing involve recognizing lexical\nentailment. Two different approaches to this problem have been proposed\nrecently that are quite different from each other. The first is an asymmetric\nsimilarity measure designed to give high scores when the contexts of the\nnarrower term in the entailment are a subset of those of the broader term. The\nsecond is a supervised approach where a classifier is learned to predict\nentailment given a concatenated latent vector representation of the word. Both\nof these approaches are vector space models that use a single context vector as\na representation of the word. In this work, I study the effects of clustering\nwords into senses and using these multiple context vectors to infer entailment\nusing extensions of these two algorithms. I find that this approach offers some\nimprovement to these entailment algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 00:53:35 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Wieting", "John", ""]]}, {"id": "1412.0879", "submitter": "Sean Gallagher", "authors": "Sean Gallagher, Wlodek Zadrozny, Walid Shalaby, Adarsh Avadhani", "title": "Watsonsim: Overview of a Question Answering Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the project is to design and run a system similar to Watson,\ndesigned to answer Jeopardy questions. In the course of a semester, we\ndeveloped an open source question answering system using the Indri, Lucene,\nBing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among\nadditional modules. By the end of the semester, we achieved 18% accuracy on\nJeopardy questions, and work has not stopped since then.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 12:15:18 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Gallagher", "Sean", ""], ["Zadrozny", "Wlodek", ""], ["Shalaby", "Walid", ""], ["Avadhani", "Adarsh", ""]]}, {"id": "1412.1058", "submitter": "Rie Johnson", "authors": "Rie Johnson and Tong Zhang", "title": "Effective Use of Word Order for Text Categorization with Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) is a neural network that can make use of\nthe internal structure of data such as the 2D structure of image data. This\npaper studies CNN on text categorization to exploit the 1D structure (namely,\nword order) of text data for accurate prediction. Instead of using\nlow-dimensional word vectors as input as is often done, we directly apply CNN\nto high-dimensional text data, which leads to directly learning embedding of\nsmall text regions for use in classification. In addition to a straightforward\nadaptation of CNN from image to text, a simple but new variation which employs\nbag-of-word conversion in the convolution layer is proposed. An extension to\ncombine multiple convolution layers is also explored for higher accuracy. The\nexperiments demonstrate the effectiveness of our approach in comparison with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 16:19:51 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 12:59:35 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1412.1215", "submitter": "Odile Piton", "authors": "H\\'el\\`ene Pignot (SAMM), Odile Piton (SAMM)", "title": "Mary Astell's words in A Serious Proposal to the Ladies (part I), a\n  lexicographic inquiry with NooJ", "comments": "Zoe Gavriilidou, Elina Chadjipapa, Lena Papadopoulou, Max\n  Silberztein. Nooj 2010 International Conference and Workshop, May 2010,\n  Komotini, Greece. University of Thrace, Proceedings of the Nooj 2010\n  International Conference and Workshop, pp.232-244,\n  http://synmorphose.compulaw.gr/joomlatools-files/docman-files/Zoe-Gav\\_BOOK\\_7.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we elected to study with NooJ the lexis of a 17 th\ncentury text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,\npart I, published in 1694. We first focused on the semantics to see how Astell\nbuilds her vindication of the female sex, which words she uses to sensitise\nwomen to their alienated condition and promote their education. Then we studied\nthe morphology of the lexemes (which is different from contemporary English)\nused by the author, thanks to the NooJ tools we have devised for this purpose.\nNooJ has great functionalities for lexicographic work. Its commands and graphs\nprove to be most efficient in the spotting of archaic words or variants in\nspelling. Introduction In our previous articles, we have studied the\nsingularities of 17 th century English within the framework of a diachronic\nanalysis thanks to syntactical and morphological graphs and thanks to the\ndictionaries we have compiled from a corpus that may be expanded overtime. Our\nearly work was based on a limited corpus of English travel literature to Greece\nin the 17 th century. This article deals with a late seventeenth century text\nwritten by a woman philosopher and essayist, Mary Astell (1666--1731),\nconsidered as one of the first English feminists. Astell wrote her essay at a\ntime in English history when women were \"the weaker vessel\" and their main\nbusiness in life was to charm and please men by their looks and submissiveness.\nIn this essay we will see how NooJ can help us analyse Astell's rhetoric (what\npoint of view does she adopt, does she speak in her own name, in the name of\nall women, what is her representation of men and women and their relationships\nin the text, what are the goals of education?). Then we will turn our attention\nto the morphology of words in the text and use NooJ commands and graphs to\ncarry out a lexicographic inquiry into Astell's lexemes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 07:16:04 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Pignot", "H\u00e9l\u00e8ne", "", "SAMM"], ["Piton", "Odile", "", "SAMM"]]}, {"id": "1412.1342", "submitter": "Diego Amancio", "authors": "Diego R. Amancio", "title": "A perspective on the advancement of natural language processing tasks\n  via topological analysis of complex networks", "comments": null, "journal-ref": "Physics of Life Reviews, v. 11, p. 641-643, 2014", "doi": "10.1016/j.plrev.2014.07.010", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on \"Approaching human language with complex networks\" by Cong and Liu\n(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 14:37:36 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Amancio", "Diego R.", ""]]}, {"id": "1412.1454", "submitter": "Ciprian Chelba", "authors": "Noam Shazeer, Joris Pelemans, Ciprian Chelba", "title": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": "Google Research Publication Id: 43222", "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel family of language model (LM) estimation techniques named\nSparse Non-negative Matrix (SNM) estimation. A first set of experiments\nempirically evaluating it on the One Billion Word Benchmark shows that SNM\n$n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN)\nmodels. When using skip-gram features the models are able to match the\nstate-of-the-art recurrent neural network (RNN) LMs; combining the two modeling\ntechniques yields the best known result on the benchmark. The computational\nadvantages of SNM over both maximum entropy and RNN LM estimation are probably\nits main strength, promising an approach that has the same flexibility in\ncombining arbitrary features effectively and yet should scale to very large\namounts of data as gracefully as $n$-gram LMs do.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 19:42:12 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 20:35:52 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Shazeer", "Noam", ""], ["Pelemans", "Joris", ""], ["Chelba", "Ciprian", ""]]}, {"id": "1412.1632", "submitter": "Lei Yu", "authors": "Lei Yu, Karl Moritz Hermann, Phil Blunsom and Stephen Pulman", "title": "Deep Learning for Answer Sentence Selection", "comments": "9 pages, accepted by NIPS deep learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer sentence selection is the task of identifying sentences that contain\nthe answer to a given question. This is an important problem in its own right\nas well as in the larger context of open domain question answering. We propose\na novel approach to solving this task via means of distributed representations,\nand learn to match questions with answers by considering their semantic\nencoding. This contrasts prior work on this task, which typically relies on\nclassifiers with large numbers of hand-crafted syntactic and semantic features\nand various external resources. Our approach does not require any feature\nengineering nor does it involve specialist linguistic data, making this model\neasily applicable to a wide range of domains and languages. Experimental\nresults on a standard benchmark dataset from TREC demonstrate that---despite\nits simplicity---our model matches state of the art performance on the answer\nsentence selection task.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 11:53:02 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Yu", "Lei", ""], ["Hermann", "Karl Moritz", ""], ["Blunsom", "Phil", ""], ["Pulman", "Stephen", ""]]}, {"id": "1412.1820", "submitter": "Daniel Gillick", "authors": "Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, David Huynh", "title": "Context-Dependent Fine-Grained Entity Type Tagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity type tagging is the task of assigning category labels to each mention\nof an entity in a document. While standard systems focus on a small set of\ntypes, recent work (Ling and Weld, 2012) suggests that using a large\nfine-grained label set can lead to dramatic improvements in downstream tasks.\nIn the absence of labeled training data, existing fine-grained tagging systems\nobtain examples automatically, using resolved entities and their types\nextracted from a knowledge base. However, since the appropriate type often\ndepends on context (e.g. Washington could be tagged either as city or\ngovernment), this procedure can result in spurious labels, leading to poorer\ngeneralization. We propose the task of context-dependent fine type tagging,\nwhere the set of acceptable labels for a mention is restricted to only those\ndeducible from the local context (e.g. sentence or document). We introduce new\nresources for this task: 12,017 mentions annotated with their context-dependent\nfine types, and we provide baseline experimental results on this data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 23:26:33 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 20:14:36 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Gillick", "Dan", ""], ["Lazic", "Nevena", ""], ["Ganchev", "Kuzman", ""], ["Kirchner", "Jesse", ""], ["Huynh", "David", ""]]}, {"id": "1412.1841", "submitter": "Paul Tupper", "authors": "P. F. Tupper", "title": "Exemplar Dynamics and Sound Merger in Language", "comments": "23 pages. Considerably abbreviated version of this work appeared\n  (without mathematical details) in the Proceedings of the 36th Annual\n  Conference of the Cognitive Science Society (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model of phonological contrast in natural language.\nSpecifically, the model describes the maintenance of contrast between different\nwords in a language, and the elimination of such contrast when sounds in the\nwords merge. An example of such a contrast is that provided by the two vowel\nsounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' in\nmost dialects of English. We model language users' knowledge of the\npronunciation of a word as consisting of collections of labeled exemplars\nstored in memory. Each exemplar is a detailed memory of a particular utterance\nof the word in question. In our model an exemplar is represented by one or two\nphonetic variables along with a weight indicating how strong the memory of the\nutterance is. Starting from an exemplar-level model we derive\nintegro-differential equations for the evolution of exemplar density fields in\nphonetic space. Using these latter equations we investigate under what\nconditions two sounds merge, thus eliminating the contrast. Our main conclusion\nis that for the preservation of phonological contrast, it is necessary that\nanomalous utterances of a given word are discarded, and not merely stored in\nmemory as an exemplar of another word.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 22:30:53 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 21:34:02 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Tupper", "P. F.", ""]]}, {"id": "1412.1866", "submitter": "Jakub Mare\\v{c}ek", "authors": "Catherine Kerr, Terri Hoare, Paula Carroll, Jakub Marecek", "title": "Integer-Programming Ensemble of Temporal-Relations Classifiers", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, 2020", "doi": "10.1007/s10618-019-00671-x", "report-no": null, "categories": "cs.CL cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction and understanding of temporal events and their relations are\nmajor challenges in natural language processing. Processing text on a\nsentence-by-sentence or expression-by-expression basis often fails, in part due\nto the challenge of capturing the global consistency of the text. We present an\nensemble method, which reconciles the outputs of multiple classifiers of\ntemporal expressions across the text using integer programming. Computational\nexperiments show that the ensemble improves upon the best individual results\nfrom two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and\nSemEval-2016 Task 12 (Clinical TempEval).\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 00:30:09 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 17:35:34 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 22:53:55 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 22:15:58 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Kerr", "Catherine", ""], ["Hoare", "Terri", ""], ["Carroll", "Paula", ""], ["Marecek", "Jakub", ""]]}, {"id": "1412.2007", "submitter": "KyungHyun Cho", "authors": "S\\'ebastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation, a recently proposed approach to machine\ntranslation based purely on neural networks, has shown promising results\ncompared to the existing approaches such as phrase-based statistical machine\ntranslation. Despite its recent success, neural machine translation has its\nlimitation in handling a larger vocabulary, as training complexity as well as\ndecoding complexity increase proportionally to the number of target words. In\nthis paper, we propose a method that allows us to use a very large target\nvocabulary without increasing training complexity, based on importance\nsampling. We show that decoding can be efficiently done even with the model\nhaving a very large target vocabulary by selecting only a small subset of the\nwhole target vocabulary. The models trained by the proposed approach are\nempirically found to outperform the baseline models with a small vocabulary as\nwell as the LSTM-based neural machine translation models. Furthermore, when we\nuse the ensemble of a few models with very large target vocabularies, we\nachieve the state-of-the-art translation performance (measured by BLEU) on the\nEnglish->German translation and almost as high performance as state-of-the-art\nEnglish->French translation system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 14:26:27 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 19:41:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Jean", "S\u00e9bastien", ""], ["Cho", "Kyunghyun", ""], ["Memisevic", "Roland", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.2197", "submitter": "Liangliang Cao", "authors": "Liangliang Cao and Chang Wang", "title": "Practice in Synonym Extraction at Large Scale", "comments": "This paper has been withdrawn by the author since the experimental\n  results are not good enough", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synonym extraction is an important task in natural language processing and\noften used as a submodule in query expansion, question answering and other\napplications. Automatic synonym extractor is highly preferred for large scale\napplications. Previous studies in synonym extraction are most limited to small\nscale datasets. In this paper, we build a large dataset with 3.4 million\nsynonym/non-synonym pairs to capture the challenges in real world scenarios. We\nproposed (1) a new cost function to accommodate the unbalanced learning\nproblem, and (2) a feature learning based deep neural network to model the\ncomplicated relationships in synonym pairs. We compare several different\napproaches based on SVMs and neural networks, and find out a novel feature\nlearning based neural network outperforms the methods with hand-assigned\nfeatures. Specifically, the best performance of our model surpasses the SVM\nbaseline with a significant 97\\% relative improvement.\n", "versions": [{"version": "v1", "created": "Sat, 6 Dec 2014 04:40:18 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 16:49:44 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 19:55:17 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Cao", "Liangliang", ""], ["Wang", "Chang", ""]]}, {"id": "1412.2378", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala and Takanori Maehara and Yuichi Yoshida and\n  Ken-ichi Kawarabayashi", "title": "Learning Word Representations from Relational Graphs", "comments": "AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes of words and relations between two words are central to numerous\ntasks in Artificial Intelligence such as knowledge representation, similarity\nmeasurement, and analogy detection. Often when two words share one or more\nattributes in common, they are connected by some semantic relations. On the\nother hand, if there are numerous semantic relations between two words, we can\nexpect some of the attributes of one of the words to be inherited by the other.\nMotivated by this close connection between attributes and relations, given a\nrelational graph in which words are inter- connected via numerous semantic\nrelations, we propose a method to learn a latent representation for the\nindividual words. The proposed method considers not only the co-occurrences of\nwords as done by existing approaches for word representation learning, but also\nthe semantic relations in which two words co-occur. To evaluate the accuracy of\nthe word representations learnt using the proposed method, we use the learnt\nword representations to solve semantic word analogy problems. Our experimental\nresults show that it is possible to learn better word representations by using\nsemantic semantics between words.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 17:49:53 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Bollegala", "Danushka", ""], ["Maehara", "Takanori", ""], ["Yoshida", "Yuichi", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1412.2442", "submitter": "Asaad Kaadan", "authors": "M. Yahia Kaadan and Asaad Kaadan", "title": "Rediscovering the Alphabet - On the Innate Universal Grammar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Grammar (UG) theory has been one of the most important research\ntopics in linguistics since introduced five decades ago. UG specifies the\nrestricted set of languages learnable by human brain, and thus, many\nresearchers believe in its biological roots. Numerous empirical studies of\nneurobiological and cognitive functions of the human brain, and of many natural\nlanguages, have been conducted to unveil some aspects of UG. This, however,\nresulted in different and sometimes contradicting theories that do not indicate\na universally unique grammar. In this research, we tackle the UG problem from\nan entirely different perspective. We search for the Unique Universal Grammar\n(UUG) that facilitates communication and knowledge transfer, the sole purpose\nof a language. We formulate this UG and show that it is unique, intrinsic, and\ncosmic, rather than humanistic. Initial analysis on a widespread natural\nlanguage already showed some positive results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 04:14:05 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Kaadan", "M. Yahia", ""], ["Kaadan", "Asaad", ""]]}, {"id": "1412.2486", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Optimization models of natural communication", "comments": null, "journal-ref": "Journal of Quantitative Linguistics 25 (3), 207-237 (2018)", "doi": "10.1080/09296174.2017.1366095", "report-no": null, "categories": "physics.soc-ph cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of information theoretic models of communication was introduced more\nthan a decade ago to explain the origins of Zipf's law for word frequencies.\nThe family is a based on a combination of two information theoretic principles:\nmaximization of mutual information between forms and meanings and minimization\nof form entropy. The family also sheds light on the origins of three other\npatterns: the principle of contrast, a related vocabulary learning bias and the\nmeaning-frequency law. Here two important components of the family, namely the\ninformation theoretic principles and the energy function that combines them\nlinearly, are reviewed from the perspective of psycholinguistics, language\nlearning, information theory and synergetic linguistics. The minimization of\nthis linear function is linked to the problem of compression of standard\ninformation theory and might be tuned by self-organization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 09:05:40 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 09:23:06 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1412.2487", "submitter": "Richard A. Blythe", "authors": "Richard A. Blythe, Andrew D. M. Smith and Kenny Smith", "title": "Word learning under infinite uncertainty", "comments": "30 pages, 4 figures, contains considerable extra discussion and\n  relaxation of original model assumptions. Version to appear in Cognition", "journal-ref": "Cognition (2016) v151 pp18-27", "doi": "10.1016/j.cognition.2016.02.017", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language learners must learn the meanings of many thousands of words, despite\nthose words occurring in complex environments in which infinitely many meanings\nmight be inferred by the learner as a word's true meaning. This problem of\ninfinite referential uncertainty is often attributed to Willard Van Orman\nQuine. We provide a mathematical formalisation of an ideal cross-situational\nlearner attempting to learn under infinite referential uncertainty, and\nidentify conditions under which word learning is possible. As Quine's\nintuitions suggest, learning under infinite uncertainty is in fact possible,\nprovided that learners have some means of ranking candidate word meanings in\nterms of their plausibility; furthermore, our analysis shows that this ranking\ncould in fact be exceedingly weak, implying that constraints which allow\nlearners to infer the plausibility of candidate word meanings could themselves\nbe weak. This approach lifts the burden of explanation from `smart' word\nlearning constraints in learners, and suggests a programme of research into\nweak, unreliable, probabilistic constraints on the inference of word meaning in\nreal word learners.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 09:08:32 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 11:01:51 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Blythe", "Richard A.", ""], ["Smith", "Andrew D. M.", ""], ["Smith", "Kenny", ""]]}, {"id": "1412.2812", "submitter": "Ehsan Khoddam", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Unsupervised Induction of Semantic Roles within a Reconstruction-Error\n  Minimization Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to unsupervised estimation of feature-rich\nsemantic role labeling models. Our model consists of two components: (1) an\nencoding component: a semantic role labeling model which predicts roles given a\nrich set of syntactic and lexical features; (2) a reconstruction component: a\ntensor factorization model which relies on roles to predict argument fillers.\nWhen the components are estimated jointly to minimize errors in argument\nreconstruction, the induced roles largely correspond to roles defined in\nannotated resources. Our method performs on par with most accurate role\ninduction methods on English and German, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 23:40:41 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.2821", "submitter": "Xiu-Li Wang", "authors": "Xiuli Wang", "title": "Zipf's Law and the Frequency of Characters or Words of Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article discusses the frequency of characters of Oracle,concluding that\nthe frequency and the rank of a word or character is fit to Zipf-Mandelboit Law\nor Zipf's law with three parameters,and figuring out the parameters based on\nthe frequency,and pointing out that what some researchers of Oracle call the\nassembling on the two ends is just a description by their impression about the\nOracle data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 00:39:16 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Wang", "Xiuli", ""]]}, {"id": "1412.3336", "submitter": "Damian H. Zanette", "authors": "Dami\\'an H. Zanette", "title": "Statistical Patterns in Written Language", "comments": "Some authors of work reviewed in this article have claimed rights on\n  its graphical material. This material cannot be eliminated from the article\n  without jeopardizing its coherence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Quantitative linguistics has been allowed, in the last few decades, within\nthe admittedly blurry boundaries of the field of complex systems. A growing\nhost of applied mathematicians and statistical physicists devote their efforts\nto disclose regularities, correlations, patterns, and structural properties of\nlanguage streams, using techniques borrowed from statistics and information\ntheory. Overall, results can still be categorized as modest, but the prospects\nare promising: medium- and long-range features in the organization of human\nlanguage -which are beyond the scope of traditional linguistics- have already\nemerged from this kind of analysis and continue to be reported, contributing a\nnew perspective to our understanding of this most complex communication system.\nThis short book is intended to review some of these recent contributions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 15:22:08 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 16:59:39 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Zanette", "Dami\u00e1n H.", ""]]}, {"id": "1412.3714", "submitter": "Jiwei Li", "authors": "Jiwei Li", "title": "Feature Weight Tuning for Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 16:35:27 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 00:57:57 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Li", "Jiwei", ""]]}, {"id": "1412.4021", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Dai Quoc Nguyen, Dang Duc Pham, Son Bao Pham", "title": "A Robust Transformation-Based Learning Approach Using Ripple Down Rules\n  for Part-of-Speech Tagging", "comments": "Version 1: 13 pages. Version 2: Submitted to AI Communications - the\n  European Journal on Artificial Intelligence. Version 3: Resubmitted after\n  major revisions. Version 4: Resubmitted after minor revisions. Version 5: to\n  appear in AI Communications (accepted for publication on 3/12/2015)", "journal-ref": null, "doi": "10.3233/AIC-150698", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach to construct a system of\ntransformation rules for the Part-of-Speech (POS) tagging task. Our approach is\nbased on an incremental knowledge acquisition method where rules are stored in\nan exception structure and new rules are only added to correct the errors of\nexisting rules; thus allowing systematic control of the interaction between the\nrules. Experimental results on 13 languages show that our approach is fast in\nterms of training time and tagging speed. Furthermore, our approach obtains\nvery competitive accuracy in comparison to state-of-the-art POS and\nmorphological taggers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 15:26:43 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 06:03:22 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2015 19:03:01 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2015 02:41:55 GMT"}, {"version": "v5", "created": "Sat, 19 Dec 2015 11:06:15 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Nguyen", "Dai Quoc", ""], ["Pham", "Dang Duc", ""], ["Pham", "Son Bao", ""]]}, {"id": "1412.4160", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham", "title": "Ripple Down Rules for Question Answering", "comments": "V1: 21 pages, 7 figures, 10 tables. V2: 8 figures, 10 tables; shorten\n  section 2; change sections 4.3 and 5.1.2. V3: Accepted for publication in the\n  Semantic Web journal. V4 (Author's manuscript): camera ready version,\n  available from the Semantic Web journal at\n  http://www.semantic-web-journal.net", "journal-ref": null, "doi": "10.3233/SW-150204", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a new trend of building ontology-based question\nanswering systems. These systems use semantic web information to produce more\nprecise answers to users' queries. However, these systems are mostly designed\nfor English. In this paper, we introduce an ontology-based question answering\nsystem named KbQAS which, to the best of our knowledge, is the first one made\nfor Vietnamese. KbQAS employs our question analysis approach that\nsystematically constructs a knowledge base of grammar rules to convert each\ninput question into an intermediate representation element. KbQAS then takes\nthe intermediate representation element with respect to a target ontology and\napplies concept-matching techniques to return an answer. On a wide range of\nVietnamese questions, experimental results show that the performance of KbQAS\nis promising with accuracies of 84.1% and 82.4% for analyzing input questions\nand retrieving output answers, respectively. Furthermore, our question analysis\napproach can easily be applied to new domains and new languages, thus saving\ntime and human effort.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 23:30:06 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 14:20:55 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2015 14:14:09 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2015 23:39:58 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Nguyen", "Dai Quoc", ""], ["Pham", "Son Bao", ""]]}, {"id": "1412.4314", "submitter": "Joseph Chee Chang", "authors": "Joseph Chee Chang and Chu-Cheng Lin", "title": "Recurrent-Neural-Network for Language Detection on Twitter\n  Code-Switching Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed language data is one of the difficult yet less explored domains of\nnatural language processing. Most research in fields like machine translation\nor sentiment analysis assume monolingual input. However, people who are capable\nof using more than one language often communicate using multiple languages at\nthe same time. Sociolinguists believe this \"code-switching\" phenomenon to be\nsocially motivated. For example, to express solidarity or to establish\nauthority. Most past work depend on external tools or resources, such as\npart-of-speech tagging, dictionary look-up, or named-entity recognizers to\nextract rich features for training machine learning models. In this paper, we\ntrain recurrent neural networks with only raw features, and use word embedding\nto automatically learn meaningful representations. Using the same\nmixed-language Twitter corpus, our system is able to outperform the best\nSVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% in\naccuracy, or by 17% in error rate reduction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 05:34:25 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 15:53:22 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Chang", "Joseph Chee", ""], ["Lin", "Chu-Cheng", ""]]}, {"id": "1412.4369", "submitter": "Kevin Duh", "authors": "Daniel Fried and Kevin Duh", "title": "Incorporating Both Distributional and Relational Semantics in Word\n  Representations", "comments": "This is the long version of a short paper accepted as a workshop\n  contribution at ICLR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 15:18:18 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 12:44:01 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2015 13:21:20 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Fried", "Daniel", ""], ["Duh", "Kevin", ""]]}, {"id": "1412.4385", "submitter": "Yi Yang", "authors": "Yi Yang and Jacob Eisenstein", "title": "Unsupervised Domain Adaptation with Feature Embeddings", "comments": "For more details, please refer to the long version of this paper:\n  http://www.cc.gatech.edu/~jeisenst/papers/yang-naacl-2015.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is the dominant technique for unsupervised domain\nadaptation, but existing approaches often require the specification of \"pivot\nfeatures\" that generalize across domains, which are selected by task-specific\nheuristics. We show that a novel but simple feature embedding approach provides\nbetter performance, by exploiting the feature template structure common in NLP\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 17:44:58 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 19:35:12 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 01:44:48 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Yang", "Yi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1412.4401", "submitter": "Chantal Enguehard", "authors": "C. Enguehard (LINA), B. Daille, E. Morin", "title": "Tools for Terminology Processing", "comments": null, "journal-ref": "R. K. Arora, M. Kulkarni, H. Darbari. The Indo-European Conference\n  on Multilingual Communications Technologies (IEMCT), Jun 2002, Pune, India.\n  Tata McGraw-Hill, pp.218 - 229", "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic terminology processing appeared 10 years ago when electronic\ncorpora became widely available. Such processing may be statistically or\nlinguistically based and produces terminology resources that can be used in a\nnumber of applications : indexing, information retrieval, technology watch,\netc. We present the tools that have been developed in the IRIN Institute. They\nall take as input texts (or collection of texts) and reflect different states\nof terminology processing: term acquisition, term recognition and term\nstructuring.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 20:03:36 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Enguehard", "C.", "", "LINA"], ["Daille", "B.", ""], ["Morin", "E.", ""]]}, {"id": "1412.4616", "submitter": "Felix Weninger", "authors": "Felix Weninger, Bj\\\"orn Schuller, Florian Eyben, Martin W\\\"ollmer,\n  Gerhard Rigoll", "title": "A Broadcast News Corpus for Evaluation and Tuning of German LVCSR\n  Systems", "comments": "submitted to INTERSPEECH 2010 on May 3, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Transcription of broadcast news is an interesting and challenging application\nfor large-vocabulary continuous speech recognition (LVCSR). We present in\ndetail the structure of a manually segmented and annotated corpus including\nover 160 hours of German broadcast news, and propose it as an evaluation\nframework of LVCSR systems. We show our own experimental results on the corpus,\nachieved with a state-of-the-art LVCSR decoder, measuring the effect of\ndifferent feature sets and decoding parameters, and thereby demonstrate that\nreal-time decoding of our test set is feasible on a desktop PC at 9.2% word\nerror rate.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 14:34:38 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Weninger", "Felix", ""], ["Schuller", "Bj\u00f6rn", ""], ["Eyben", "Florian", ""], ["W\u00f6llmer", "Martin", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1412.4682", "submitter": "Mykola  Pechenizkiy", "authors": "Erik Tromp and Mykola Pechenizkiy", "title": "Rule-based Emotion Detection on Social Media: Putting Tweets on\n  Plutchik's Wheel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sentiment analysis beyond the typical granularity of polarity and\ninstead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an\nextension to the Rule-Based Emission Model algorithm to deduce such emotions\nfrom human-written messages. We evaluate our approach on two different datasets\nand compare its performance with the current state-of-the-art techniques for\nemotion detection, including a recursive auto-encoder. The results of the\nexperimental study suggest that RBEM-Emo is a promising approach advancing the\ncurrent state-of-the-art in emotion detection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 17:20:47 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Tromp", "Erik", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1412.4729", "submitter": "Subhashini Venugopalan", "authors": "Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach,\n  Raymond Mooney, Kate Saenko", "title": "Translating Videos to Natural Language Using Deep Recurrent Neural\n  Networks", "comments": "NAACL-HLT 2015 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving the visual symbol grounding problem has long been a goal of\nartificial intelligence. The field appears to be advancing closer to this goal\nwith recent breakthroughs in deep learning for natural language grounding in\nstatic images. In this paper, we propose to translate videos directly to\nsentences using a unified deep neural network with both convolutional and\nrecurrent structure. Described video datasets are scarce, and most existing\nmethods have been applied to toy domains with a small vocabulary of possible\nwords. By transferring knowledge from 1.2M+ images with category labels and\n100,000+ images with captions, our method is able to create sentence\ndescriptions of open-domain videos with large vocabularies. We compare our\napproach with recent work using language generation metrics, subject, verb, and\nobject prediction accuracy, and a human evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:21:50 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 00:58:38 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 04:22:06 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Venugopalan", "Subhashini", ""], ["Xu", "Huijuan", ""], ["Donahue", "Jeff", ""], ["Rohrbach", "Marcus", ""], ["Mooney", "Raymond", ""], ["Saenko", "Kate", ""]]}, {"id": "1412.4846", "submitter": "Chunhua Bian", "authors": "Ruokuang Lin, Qianli D.Y. Ma and Chunhua Bian", "title": "Scaling laws in human speech, decreasing emergence of new words and a\n  generalized model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language, as a typical complex system, its organization and evolution\nis an attractive topic for both physical and cultural researchers. In this\npaper, we present the first exhaustive analysis of the text organization of\nhuman speech. Two important results are that: (i) the construction and\norganization of spoken language can be characterized as Zipf's law and Heaps'\nlaw, as observed in written texts; (ii) word frequency vs. rank distribution\nand the growth of distinct words with the increase of text length shows\nsignificant differences between book and speech. In speech word frequency\ndistribution are more concentrated on higher frequency words, and the emergence\nof new words decreases much rapidly when the content length grows. Based on\nthese observations, a new generalized model is proposed to explain these\ncomplex dynamical behaviors and the differences between speech and book.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 00:34:37 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 16:16:51 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Lin", "Ruokuang", ""], ["Ma", "Qianli D. Y.", ""], ["Bian", "Chunhua", ""]]}, {"id": "1412.4930", "submitter": "R\\'emi Lebret", "authors": "R\\'emi Lebret and Ronan Collobert", "title": "Rehabilitation of Count-based Models for Word Vector Representations", "comments": "A. Gelbukh (Ed.), Springer International Publishing Switzerland", "journal-ref": "CICLing 2015, Part I, LNCS 9041, pp. 417-429, 2015", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on word representations mostly rely on predictive models.\nDistributed word representations (aka word embeddings) are trained to optimally\npredict the contexts in which the corresponding words tend to appear. Such\nmodels have succeeded in capturing word similarties as well as semantic and\nsyntactic regularities. Instead, we aim at reviving interest in a model based\non counts. We present a systematic study of the use of the Hellinger distance\nto extract semantic representations from the word co-occurence statistics of\nlarge text corpora. We show that this distance gives good performance on word\nsimilarity and analogy tasks, with a proper type and size of context, and a\ndimensionality reduction based on a stochastic low-rank approximation. Besides\nbeing both simple and intuitive, this method also provides an encoding function\nwhich can be used to infer unseen words or phrases. This becomes a clear\nadvantage compared to predictive models which must train these new words.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 09:43:56 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 18:35:17 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Lebret", "R\u00e9mi", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.5212", "submitter": "Micha{\\l} {\\L}opuszy\\'nski", "authors": "Micha{\\l} {\\L}opuszy\\'nski", "title": "Application of Topic Models to Judgments from Public Procurement Domain", "comments": "\"Legal Knowledge and Information Systems, JURIX 2014: The\n  Twenty-Seventh Annual Conference\", series Frontiers in Artificial\n  Intelligence and Applications, Volume 271, edited by Rinke Hoekstra,\n  IOSPress, 2014", "journal-ref": null, "doi": "10.3233/978-1-61499-468-8-131", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, automatic analysis of themes contained in a large corpora of\njudgments from public procurement domain is performed. The employed technique\nis unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,\nto use LDA in conjunction with recently developed method of unsupervised\nkeyword extraction. Such an approach improves the interpretability of the\nautomatically obtained topics and allows for better computational performance.\nThe described analysis illustrates a potential of the method in detecting\nrecurring themes and discovering temporal trends in lodged contract appeals.\nThese results may be in future applied to improve information retrieval from\nrepositories of legal texts or as auxiliary material for legal analyses carried\nout by human experts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 22:00:52 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["\u0141opuszy\u0144ski", "Micha\u0142", ""]]}, {"id": "1412.5335", "submitter": "Gr\\'egoire Mesnil", "authors": "Gr\\'egoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 11:02:04 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:17:16 GMT"}, {"version": "v3", "created": "Fri, 19 Dec 2014 11:36:14 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 20:03:35 GMT"}, {"version": "v5", "created": "Wed, 4 Feb 2015 05:17:55 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 14:26:14 GMT"}, {"version": "v7", "created": "Wed, 27 May 2015 06:40:09 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Mesnil", "Gr\u00e9goire", ""], ["Mikolov", "Tomas", ""], ["Ranzato", "Marc'Aurelio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.5404", "submitter": "Jichang Zhao", "authors": "Yuan Zuo, Jichang Zhao, Ke Xu", "title": "Word Network Topic Model: A Simple but General Solution for Short and\n  Imbalanced Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The short text has been the prevalent format for information of Internet in\nrecent decades, especially with the development of online social media, whose\nmillions of users generate a vast number of short messages everyday. Although\nsophisticated signals delivered by the short text make it a promising source\nfor topic modeling, its extreme sparsity and imbalance brings unprecedented\nchallenges to conventional topic models like LDA and its variants. Aiming at\npresenting a simple but general solution for topic modeling in short texts, we\npresent a word co-occurrence network based model named WNTM to tackle the\nsparsity and imbalance simultaneously. Different from previous approaches, WNTM\nmodels the distribution over topics for each word instead of learning topics\nfor each document, which successfully enhance the semantic density of data\nspace without importing too much time or space complexity. Meanwhile, the rich\ncontextual information preserved in the word-word space also guarantees its\nsensitivity in identifying rare topics with convincing quality. Furthermore,\nemploying the same Gibbs sampling with LDA makes WNTM easily to be extended to\nvarious application scenarios. Extensive validations on both short and normal\ntexts testify the outperformance of WNTM as compared to baseline methods. And\nfinally we also demonstrate its potential in precisely discovering newly\nemerging topics or unexpected events in Weibo at pretty early stages.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 14:18:52 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Zuo", "Yuan", ""], ["Zhao", "Jichang", ""], ["Xu", "Ke", ""]]}, {"id": "1412.5448", "submitter": "Micka\\\"el Poussevin", "authors": "Micka\\\"el Poussevin and Vincent Guigue and Patrick Gallinari", "title": "Extended Recommendation Framework: Generating the Text of a User Review\n  as a Personalized Summary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to augment rating based recommender systems by providing the user\nwith additional information which might help him in his choice or in the\nunderstanding of the recommendation. We consider here as a new task, the\ngeneration of personalized reviews associated to items. We use an extractive\nsummary formulation for generating these reviews. We also show that the two\ninformation sources, ratings and items could be used both for estimating\nratings and for generating summaries, leading to improved performance for each\nsystem compared to the use of a single source. Besides these two contributions,\nwe show how a personalized polarity classifier can integrate the rating and\ntextual aspects. Overall, the proposed system offers the user three\npersonalized hints for a recommendation: rating, text and polarity. We evaluate\nthese three components on two datasets using appropriate measures for each\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 15:46:28 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Poussevin", "Micka\u00ebl", ""], ["Guigue", "Vincent", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1412.5477", "submitter": "Meenakshi Lakshmanan", "authors": "S V Kasmir Raja, V Rajitha and Lakshmanan Meenakshi", "title": "Computational Model to Generate Case-Inflected Forms of Masculine Nouns\n  for Word Search in Sanskrit E-Text", "comments": null, "journal-ref": "Journal of Computer Science (ISSN Print: 1549-3636, ISSN Online:\n  1552-6607), December 2014, Volume 10, Issue 11, Pages 2269-2283", "doi": "10.3844/jcssp.2014.2260.2268", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of word search in Sanskrit is inseparable from complexities that\ninclude those caused by euphonic conjunctions and case-inflections. The\ncase-inflectional forms of a noun normally number 24 owing to the fact that in\nSanskrit there are eight cases and three numbers-singular, dual and plural. The\ntraditional method of generating these inflectional forms is rather elaborate\nowing to the fact that there are differences in the forms generated between\neven very similar words and there are subtle nuances involved. Further, it\nwould be a cumbersome exercise to generate and search for 24 forms of a word\nduring a word search in a large text, using the currently available\ncase-inflectional form generators. This study presents a new approach to\ngenerating case-inflectional forms that is simpler to compute. Further, an\noptimized model that is sufficient for generating only those word forms that\nare required in a word search and is more than 80% efficient compared to the\ncomplete case-inflectional forms generator, is presented in this study for the\nfirst time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 16:56:43 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Raja", "S V Kasmir", ""], ["Rajitha", "V", ""], ["Meenakshi", "Lakshmanan", ""]]}, {"id": "1412.5567", "submitter": "Awni Hannun", "authors": "Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos,\n  Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates and\n  Andrew Y. Ng", "title": "Deep Speech: Scaling up end-to-end speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a state-of-the-art speech recognition system developed using\nend-to-end deep learning. Our architecture is significantly simpler than\ntraditional speech systems, which rely on laboriously engineered processing\npipelines; these traditional systems also tend to perform poorly when used in\nnoisy environments. In contrast, our system does not need hand-designed\ncomponents to model background noise, reverberation, or speaker variation, but\ninstead directly learns a function that is robust to such effects. We do not\nneed a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our\napproach is a well-optimized RNN training system that uses multiple GPUs, as\nwell as a set of novel data synthesis techniques that allow us to efficiently\nobtain a large amount of varied data for training. Our system, called Deep\nSpeech, outperforms previously published results on the widely studied\nSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech\nalso handles challenging noisy environments better than widely used,\nstate-of-the-art commercial speech systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 20:39:45 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 21:36:13 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Hannun", "Awni", ""], ["Case", "Carl", ""], ["Casper", "Jared", ""], ["Catanzaro", "Bryan", ""], ["Diamos", "Greg", ""], ["Elsen", "Erich", ""], ["Prenger", "Ryan", ""], ["Satheesh", "Sanjeev", ""], ["Sengupta", "Shubho", ""], ["Coates", "Adam", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1412.5659", "submitter": "Nicholas Dronen", "authors": "Nicholas Dronen, Peter W. Foltz, Kyle Habermehl", "title": "Effective sampling for large-scale automated writing evaluation systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated writing evaluation (AWE) has been shown to be an effective\nmechanism for quickly providing feedback to students. It has already seen wide\nadoption in enterprise-scale applications and is starting to be adopted in\nlarge-scale contexts. Training an AWE model has historically required a single\nbatch of several hundred writing examples and human scores for each of them.\nThis requirement limits large-scale adoption of AWE since human-scoring essays\nis costly. Here we evaluate algorithms for ensuring that AWE models are\nconsistently trained using the most informative essays. Our results show how to\nminimize training set sizes while maximizing predictive performance, thereby\nreducing cost without unduly sacrificing accuracy. We conclude with a\ndiscussion of how to integrate this approach into large-scale AWE systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:41:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Dronen", "Nicholas", ""], ["Foltz", "Peter W.", ""], ["Habermehl", "Kyle", ""]]}, {"id": "1412.5673", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Jacob Eisenstein", "title": "Entity-Augmented Distributional Semantics for Discourse Relations", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relations bind smaller linguistic elements into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked sentences. A more subtle\nchallenge is that it is not enough to represent the meaning of each sentence of\na discourse relation, because the relation may depend on links between\nlower-level elements, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted not only\nfrom the distributional representations of the sentences, but also of their\ncoreferent entity mentions. The resulting system obtains substantial\nimprovements over the previous state-of-the-art in predicting implicit\ndiscourse relations in the Penn Discourse Treebank.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 23:26:48 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 23:17:48 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 14:14:44 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Ji", "Yangfeng", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1412.5836", "submitter": "Kevin Duh", "authors": "Daniel Fried and Kevin Duh", "title": "Incorporating Both Distributional and Relational Semantics in Word\n  Representations", "comments": "Accepted as a workshop contribution at ICLR2015. Long version at:\n  arXiv:1412.4369", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the hypothesis that word representations ought to incorporate\nboth distributional and relational semantics. To this end, we employ the\nAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes a\ndistributional objective on raw text and a relational objective on WordNet.\nPreliminary results on knowledge base completion, analogy tests, and parsing\nshow that word representations trained on both objectives can give improvements\nin some cases.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 12:30:55 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 17:18:34 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2015 13:27:28 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Fried", "Daniel", ""], ["Duh", "Kevin", ""]]}, {"id": "1412.6045", "submitter": "Luis Nieto Pi\\~na", "authors": "Luis Nieto Pi\\~na and Richard Johansson", "title": "A Simple and Efficient Method To Generate Word Sense Representations", "comments": "5 pages, submission to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of words have boosted the performance of many\nNatural Language Processing tasks. However, usually only one representation per\nword is obtained, not acknowledging the fact that some words have multiple\nmeanings. This has a negative effect on the individual word representations and\nthe language model as a whole. In this paper we present a simple model that\nenables recent techniques for building word vectors to represent distinct\nsenses of polysemic words. In our assessment of this model we show that it is\nable to effectively discriminate between words' senses and to do so in a\ncomputationally efficient manner.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 20:14:10 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 20:35:09 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Pi\u00f1a", "Luis Nieto", ""], ["Johansson", "Richard", ""]]}, {"id": "1412.6069", "submitter": "Dirk Roorda", "authors": "Dirk Roorda, Charles van den Heuvel", "title": "Annotation as a New Paradigm in Research Archiving", "comments": "http://depot.knaw.nl/13026/", "journal-ref": "Proceedings of the American Society for Information Science and\n  Technology; Volume 49, Issue 1, pages 1-10, 2012", "doi": "10.1002/meet.14504901084", "report-no": null, "categories": "cs.DL cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We outline a paradigm to preserve results of digital scholarship, whether\nthey are query results, feature values, or topic assignments. This paradigm is\ncharacterized by using annotations as multifunctional carriers and making them\nportable. The testing grounds we have chosen are two significant enterprises,\none in the history of science, and one in Hebrew scholarship. The first one\n(CKCC) focuses on the results of a project where a Dutch consortium of\nuniversities, research institutes, and cultural heritage institutions\nexperimented for 4 years with language techniques and topic modeling methods\nwith the aim to analyze the emergence of scholarly debates. The data: a complex\nset of about 20.000 letters. The second one (DTHB) is a multi-year effort to\nexpress the linguistic features of the Hebrew bible in a text database, which\nis still growing in detail and sophistication. Versions of this database are\npackaged in commercial bible study software. We state that the results of these\nforms of scholarship require new knowledge management and archive practices.\nOnly when researchers can build efficiently on each other's (intermediate)\nresults, they can achieve the aggregations of quality data by which new\nquestions can be answered, and hidden patterns visualized. Archives are\nrequired to find a balance between preserving authoritative versions of sources\nand supporting collaborative efforts in digital scholarship. Annotations are\npromising vehicles for preserving and reusing research results. Keywords\nannotation, portability, archiving, queries, features, topics, keywords,\nRepublic of Letters, Hebrew text databases.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 08:17:03 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Roorda", "Dirk", ""], ["Heuvel", "Charles van den", ""]]}, {"id": "1412.6211", "submitter": "Qiang Wu", "authors": "Xianfeng Hu, Yang Wang and Qiang Wu", "title": "Multiple Authors Detection: A Quantitative Analysis of Dream of the Red\n  Chamber", "comments": null, "journal-ref": "Advances in Adaptive Data Analysis, Article ID 1450012 (18 pages),\n  2014", "doi": "10.1142/S1793536914500125", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the authorship controversy of Dream of the Red Chamber and the\napplication of machine learning in the study of literary stylometry, we develop\na rigorous new method for the mathematical analysis of authorship by testing\nfor a so-called chrono-divide in writing styles. Our method incorporates some\nof the latest advances in the study of authorship attribution, particularly\ntechniques from support vector machines. By introducing the notion of relative\nfrequency as a feature ranking metric our method proves to be highly effective\nand robust.\n  Applying our method to the Cheng-Gao version of Dream of the Red Chamber has\nled to convincing if not irrefutable evidence that the first $80$ chapters and\nthe last $40$ chapters of the book were written by two different authors.\nFurthermore, our analysis has unexpectedly provided strong support to the\nhypothesis that Chapter 67 was not the work of Cao Xueqin either.\n  We have also tested our method to the other three Great Classical Novels in\nChinese. As expected no chrono-divides have been found. This provides further\nevidence of the robustness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 04:31:11 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Hu", "Xianfeng", ""], ["Wang", "Yang", ""], ["Wu", "Qiang", ""]]}, {"id": "1412.6264", "submitter": "Taraka Rama Kasicheyanula", "authors": "Taraka Rama K", "title": "Supertagging: Introduction, learning, and application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Supertagging is an approach originally developed by Bangalore and Joshi\n(1999) to improve the parsing efficiency. In the beginning, the scholars used\nsmall training datasets and somewhat na\\\"ive smoothing techniques to learn the\nprobability distributions of supertags. Since its inception, the applicability\nof Supertags has been explored for TAG (tree-adjoining grammar) formalism as\nwell as other related yet, different formalisms such as CCG. This article will\ntry to summarize the various chapters, relevant to statistical parsing, from\nthe most recent edited book volume (Bangalore and Joshi, 2010). The chapters\nwere selected so as to blend the learning of supertags, its integration into\nfull-scale parsing, and in semantic parsing.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 09:53:57 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["K", "Taraka Rama", ""]]}, {"id": "1412.6277", "submitter": "R\\'emi Lebret", "authors": "R\\'emi Lebret and Ronan Collobert", "title": "N-gram-Based Low-Dimensional Representation for Document Classification", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag-of-words (BOW) model is the common approach for classifying\ndocuments, where words are used as feature for training a classifier. This\ngenerally involves a huge number of features. Some techniques, such as Latent\nSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been\ndesigned to summarize documents in a lower dimension with the least semantic\ninformation loss. Some semantic information is nevertheless always lost, since\nonly words are considered. Instead, we aim at using information coming from\nn-grams to overcome this limitation, while remaining in a low-dimension space.\nMany approaches, such as the Skip-gram model, provide good word vector\nrepresentations very quickly. We propose to average these representations to\nobtain representations of n-grams. All n-grams are thus embedded in a same\nsemantic space. A K-means clustering can then group them into semantic\nconcepts. The number of features is therefore dramatically reduced and\ndocuments can be represented as bag of semantic concepts. We show that this\nmodel outperforms LSA and LDA on a sentiment classification task, and yields\nsimilar results than a traditional BOW-model with far less features.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 10:29:33 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 13:53:40 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Lebret", "R\u00e9mi", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.6334", "submitter": "Hubert Soyer", "authors": "Hubert Soyer and Pontus Stenetorp and Akiko Aizawa", "title": "Leveraging Monolingual Data for Crosslingual Compositional Word\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a novel neural network based architecture for\ninducing compositional crosslingual word representations. Unlike previously\nproposed methods, our method fulfills the following three criteria; it\nconstrains the word-level representations to be compositional, it is capable of\nleveraging both bilingual and monolingual data, and it is scalable to large\nvocabularies and large quantities of data. The key component of our approach is\nwhat we refer to as a monolingual inclusion criterion, that exploits the\nobservation that phrases are more closely semantically related to their\nsub-phrases than to other randomly sampled phrases. We evaluate our method on a\nwell-established crosslingual document classification task and achieve results\nthat are either comparable, or greatly improve upon previous state-of-the-art\nmethods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for\nthe English to German and German to English sub-tasks respectively. The former\nadvances the state of the art by 0.9% points of accuracy, the latter is an\nabsolute improvement upon the previous state of the art by 7.7% points of\naccuracy and an improvement of 33.0% in error reduction.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 13:23:35 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 07:44:39 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 08:03:57 GMT"}, {"version": "v4", "created": "Sat, 22 Aug 2015 15:22:26 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Soyer", "Hubert", ""], ["Stenetorp", "Pontus", ""], ["Aizawa", "Akiko", ""]]}, {"id": "1412.6418", "submitter": "Ehsan Khoddam Mohammadi", "authors": "Ivan Titov and Ehsan Khoddam", "title": "Inducing Semantic Representation from Text by Jointly Predicting and\n  Factorizing Relations", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new method to integrate two recent lines of work:\nunsupervised induction of shallow semantics (e.g., semantic roles) and\nfactorization of relations in text and knowledge bases. Our model consists of\ntwo components: (1) an encoding component: a semantic role labeling model which\npredicts roles given a rich set of syntactic and lexical features; (2) a\nreconstruction component: a tensor factorization model which relies on roles to\npredict argument fillers. When the components are estimated jointly to minimize\nerrors in argument reconstruction, the induced roles largely correspond to\nroles defined in annotated resources. Our method performs on par with most\naccurate role induction methods on English, even though, unlike these previous\napproaches, we do not incorporate any prior linguistic knowledge about the\nlanguage.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:30:33 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 12:16:56 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 10:24:27 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Titov", "Ivan", ""], ["Khoddam", "Ehsan", ""]]}, {"id": "1412.6448", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin and Yoshua\n  Bengio", "title": "Embedding Word Similarity with Neural Machine Translation", "comments": "arXiv admin note: text overlap with arXiv:1410.0718", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models learn word representations, or embeddings, that\ncapture rich linguistic and conceptual information. Here we investigate the\nembeddings learned by neural machine translation models, a recently-developed\nclass of neural language model. We show that embeddings from translation models\noutperform those learned by monolingual models at tasks that require knowledge\nof both conceptual similarity and lexical-syntactic role. We further show that\nthese effects hold when translating from both English to French and English to\nGerman, and argue that the desirable properties of translation embeddings\nshould emerge largely independently of the source and target languages.\nFinally, we apply a new method for training neural translation models with very\nlarge vocabularies, and show that this vocabulary expansion algorithm results\nin minimal degradation of embedding quality. Our embedding spaces can be\nqueried in an online demo and downloaded from our web page. Overall, our\nanalyses indicate that translation-based embeddings should be used in\napplications that require concepts to be organised according to similarity\nand/or lexical function, while monolingual embeddings are better suited to\nmodelling (nonspecific) inter-word relatedness.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 17:22:03 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 16:55:46 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 18:30:44 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 18:11:54 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Hill", "Felix", ""], ["Cho", "Kyunghyun", ""], ["Jean", "Sebastien", ""], ["Devin", "Coline", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.6568", "submitter": "Georgiana Dinu", "authors": "Georgiana Dinu, Angeliki Lazaridou, Marco Baroni", "title": "Improving zero-shot learning by mitigating the hubness problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The zero-shot paradigm exploits vector-based word representations extracted\nfrom text corpora with unsupervised methods to learn general mapping functions\nfrom other feature spaces onto word space, where the words associated to the\nnearest neighbours of the mapped vectors are used as their linguistic labels.\nWe show that the neighbourhoods of the mapped elements are strongly polluted by\nhubs, vectors that tend to be near a high proportion of items, pushing their\ncorrect labels down the neighbour list. After illustrating the problem\nempirically, we propose a simple method to correct it by taking the proximity\ndistribution of potential neighbours across many mapped vectors into account.\nWe show that this correction leads to consistent improvements in realistic\nzero-shot experiments in the cross-lingual, image labeling and image retrieval\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:03:46 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 14:15:13 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2015 13:10:07 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Dinu", "Georgiana", ""], ["Lazaridou", "Angeliki", ""], ["Baroni", "Marco", ""]]}, {"id": "1412.6575", "submitter": "Bishan Yang", "authors": "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge\n  Bases", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning representations of entities and relations in KBs using\nthe neural-embedding approach. We show that most existing models, including NTN\n(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a unified learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models on\nthe link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach\nthat utilizes the learned relation embeddings to mine logical rules such as\n\"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that\nembeddings learned from the bilinear objective are particularly good at\ncapturing relational semantics and that the composition of relations is\ncharacterized by matrix multiplication. More interestingly, we demonstrate that\nour embedding-based rule extraction approach successfully outperforms a\nstate-of-the-art confidence-based rule mining approach in mining Horn rules\nthat involve compositional reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:37:16 GMT"}, {"version": "v2", "created": "Sat, 27 Dec 2014 00:18:17 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 15:24:59 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2015 15:08:45 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Yang", "Bishan", ""], ["Yih", "Wen-tau", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1412.6577", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Claire Cardie", "title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks", "comments": "10 pages, 2 figures, published at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 01:53:22 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 08:35:22 GMT"}, {"version": "v3", "created": "Sat, 2 May 2015 20:22:32 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Cardie", "Claire", ""]]}, {"id": "1412.6616", "submitter": "Abram Demski", "authors": "Abram Demski, Volkan Ustun, Paul Rosenbloom, Cody Kommers", "title": "Outperforming Word2Vec on Analogy Tasks with Random Projections", "comments": "This paper has been withdrawn due to problems pointed out in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed vector representation based on a simplification of\nthe BEAGLE system, designed in the context of the Sigma cognitive architecture.\nOur method does not require gradient-based training of neural networks, matrix\ndecompositions as with LSA, or convolutions as with BEAGLE. All that is\ninvolved is a sum of random vectors and their pointwise products. Despite the\nsimplicity of this technique, it gives state-of-the-art results on analogy\nproblems, in most cases better than Word2Vec. To explain this success, we\ninterpret it as a dimension reduction via random projection.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:07:29 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 17:31:58 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Demski", "Abram", ""], ["Ustun", "Volkan", ""], ["Rosenbloom", "Paul", ""], ["Kommers", "Cody", ""]]}, {"id": "1412.6623", "submitter": "Luke Vilnis", "authors": "Luke Vilnis, Andrew McCallum", "title": "Word Representations via Gaussian Embedding", "comments": "12 pages, published as conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:42:40 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 14:24:28 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 18:19:11 GMT"}, {"version": "v4", "created": "Fri, 1 May 2015 10:14:58 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Vilnis", "Luke", ""], ["McCallum", "Andrew", ""]]}, {"id": "1412.6632", "submitter": "Junhua Mao", "authors": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille", "title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "comments": "Add a simple strategy to boost the performance of image captioning\n  task significantly. More details are shown in Section 8 of the paper. The\n  code and related data are available at https://github.com/mjhucla/mRNN-CR ;.\n  arXiv admin note: substantial text overlap with arXiv:1410.1090", "journal-ref": "ICLR 2015", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability\ndistribution of generating a word given previous words and an image. Image\ncaptions are generated by sampling from this distribution. The model consists\nof two sub-networks: a deep recurrent neural network for sentences and a deep\nconvolutional network for images. These two sub-networks interact with each\nother in a multimodal layer to form the whole m-RNN model. The effectiveness of\nour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,\nFlickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In\naddition, we apply the m-RNN model to retrieval tasks for retrieving images or\nsentences, and achieves significant performance improvement over the\nstate-of-the-art methods which directly optimize the ranking objective function\nfor retrieval. The project page of this work is:\nwww.stat.ucla.edu/~junhua.mao/m-RNN.html .\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 08:10:04 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 08:24:11 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 04:17:48 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:03:35 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2015 15:26:58 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Wei", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Huang", "Zhiheng", ""], ["Yuille", "Alan", ""]]}, {"id": "1412.6645", "submitter": "Gabriel Synnaeve", "authors": "Gabriel Synnaeve, Emmanuel Dupoux", "title": "Weakly Supervised Multi-Embeddings Learning of Acoustic Models", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained a Siamese network with multi-task same/different information on a\nspeech dataset, and found that it was possible to share a network for both\ntasks without a loss in performance. The first task was to discriminate between\ntwo same or different words, and the second was to discriminate between two\nsame or different talkers.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 11:54:41 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 10:09:09 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 12:35:32 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Synnaeve", "Gabriel", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "1412.6650", "submitter": "Alex Ter-Sarkisov", "authors": "Aram Ter-Sarkisov, Holger Schwenk, Loic Barrault and Fethi Bougares", "title": "Incremental Adaptation Strategies for Neural Network Language Models", "comments": "accepted as workshop paper at ACL-IJCNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is today acknowledged that neural network language models outperform\nbackoff language models in applications like speech recognition or statistical\nmachine translation. However, training these models on large amounts of data\ncan take several days. We present efficient techniques to adapt a neural\nnetwork language model to new data. Instead of training a completely new model\nor relying on mixture approaches, we propose two new methods: continued\ntraining on resampled data or insertion of adaptation layers. We present\nexperimental results in an CAT environment where the post-edits of professional\ntranslators are used to improve an SMT system. Both methods are very fast and\nachieve significant improvements without overfitting the small adaptation data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 13:06:05 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 13:43:19 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 11:36:36 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 14:54:51 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Ter-Sarkisov", "Aram", ""], ["Schwenk", "Holger", ""], ["Barrault", "Loic", ""], ["Bougares", "Fethi", ""]]}, {"id": "1412.6815", "submitter": "Misha Denil", "authors": "Misha Denil and Alban Demiraj and Nando de Freitas", "title": "Extraction of Salient Sentences from Labelled Documents", "comments": "arXiv admin note: substantial text overlap with arXiv:1406.3830", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical convolutional document model with an architecture\ndesigned to support introspection of the document structure. Using this model,\nwe show how to use visualisation techniques from the computer vision literature\nto identify and extract topic-relevant sentences.\n  We also introduce a new scalable evaluation technique for automatic sentence\nextraction systems that avoids the need for time consuming human annotation of\nvalidation data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 17:38:19 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 23:57:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Denil", "Misha", ""], ["Demiraj", "Alban", ""], ["de Freitas", "Nando", ""]]}, {"id": "1412.6881", "submitter": "Jinseok Nam", "authors": "Jinseok Nam and Johannes F\\\"urnkranz", "title": "On Learning Vector Representations in Hierarchical Label Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in multi-label classification is to capture label\npatterns or underlying structures that have an impact on such patterns. This\npaper addresses one such problem, namely how to exploit hierarchical structures\nover labels. We present a novel method to learn vector representations of a\nlabel space given a hierarchy of labels and label co-occurrence patterns. Our\nexperimental results demonstrate qualitatively that the proposed method is able\nto learn regularities among labels by exploiting a label hierarchy as well as\nlabel co-occurrences. It highlights the importance of the hierarchical\ninformation in order to obtain regularities which facilitate analogical\nreasoning over a label space. We also experimentally illustrate the dependency\nof the learned representations on the label hierarchy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 06:12:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 17:12:04 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 19:23:23 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Nam", "Jinseok", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "1412.7004", "submitter": "Pranava Swaroop Madhyastha", "authors": "Pranava Swaroop Madhyastha, Xavier Carreras, Ariadna Quattoni", "title": "Tailoring Word Embeddings for Bilexical Predictions: An Experimental\n  Comparison", "comments": "Accepted as a workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We investigate the problem of inducing word embeddings that are tailored for\na particular bilexical relation. Our learning algorithm takes an existing\nlexical vector space and compresses it such that the resulting word embeddings\nare good predictors for a target bilexical relation. In experiments we show\nthat task-specific embeddings can benefit both the quality and efficiency in\nlexical prediction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 14:49:19 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 17:33:57 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Madhyastha", "Pranava Swaroop", ""], ["Carreras", "Xavier", ""], ["Quattoni", "Ariadna", ""]]}, {"id": "1412.7026", "submitter": "Aditya Joshi", "authors": "Aditya Joshi, Johan Halseth, Pentti Kanerva", "title": "Language Recognition using Random Indexing", "comments": "7 pages, 1 figures, 2 tables, ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Indexing is a simple implementation of Random Projections with a wide\nrange of applications. It can solve a variety of problems with good accuracy\nwithout introducing much complexity. Here we use it for identifying the\nlanguage of text samples. We present a novel method of generating language\nrepresentation vectors using letter blocks. Further, we show that the method is\neasily implemented and requires little computational power and space.\nExperiments on a number of model parameters illustrate certain properties about\nhigh dimensional sparse vector representations of data. Proof of statistically\nrelevant language vectors are shown through the extremely high success of\nvarious language recognition tasks. On a difficult data set of 21,000 short\nsentences from 21 different languages, our model performs a language\nrecognition task and achieves 97.8% accuracy, comparable to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:34:43 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 08:02:49 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Joshi", "Aditya", ""], ["Halseth", "Johan", ""], ["Kanerva", "Pentti", ""]]}, {"id": "1412.7028", "submitter": "Jo\\\"el Legrand", "authors": "Jo\\\"el Legrand and Ronan Collobert", "title": "Joint RNN-Based Greedy Parsing and Word Composition", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a greedy parser based on neural networks, which\nleverages a new compositional sub-tree representation. The greedy parser and\nthe compositional procedure are jointly trained, and tightly depends on\neach-other. The composition procedure outputs a vector representation which\nsummarizes syntactically (parsing tags) and semantically (words) sub-trees.\nComposition and tagging is achieved over continuous (word or tag)\nrepresentations, and recurrent neural networks. We reach F1 performance on par\nwith well-known existing parsers, while having the advantage of speed, thanks\nto the greedy nature of the parser. We provide a fully functional\nimplementation of the method described in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:40:31 GMT"}, {"version": "v2", "created": "Thu, 25 Dec 2014 17:39:39 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 15:04:34 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2015 21:57:49 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Legrand", "Jo\u00ebl", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.7063", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran", "title": "Diverse Embedding Neural Network Language Models", "comments": "Under review as workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Diverse Embedding Neural Network (DENN), a novel architecture for\nlanguage models (LMs). A DENNLM projects the input word history vector onto\nmultiple diverse low-dimensional sub-spaces instead of a single\nhigher-dimensional sub-space as in conventional feed-forward neural network\nLMs. We encourage these sub-spaces to be diverse during network training\nthrough an augmented loss function. Our language modeling experiments on the\nPenn Treebank data set show the performance benefit of using a DENNLM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 17:19:56 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 21:33:46 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 19:53:21 GMT"}, {"version": "v4", "created": "Wed, 25 Feb 2015 21:55:15 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 20:07:50 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1412.7091", "submitter": "Pascal Vincent", "authors": "Pascal Vincent, Alexandre de Br\\'ebisson, Xavier Bouthillier", "title": "Efficient Exact Gradient Update for training Deep Networks with Very\n  Large Sparse Targets", "comments": "15 pages technical report version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of problems involves training deep neural networks with\nsparse prediction targets of very high dimension D. These occur naturally in\ne.g. neural language models or the learning of word-embeddings, often posed as\npredicting the probability of next words among a vocabulary of size D (e.g. 200\n000). Computing the equally large, but typically non-sparse D-dimensional\noutput vector from a last hidden layer of reasonable dimension d (e.g. 500)\nincurs a prohibitive O(Dd) computational cost for each example, as does\nupdating the D x d output weight matrix and computing the gradient needed for\nbackpropagation to previous layers. While efficient handling of large sparse\nnetwork inputs is trivial, the case of large sparse targets is not, and has\nthus so far been sidestepped with approximate alternatives such as hierarchical\nsoftmax or sampling-based approximations during training. In this work we\ndevelop an original algorithmic approach which, for a family of loss functions\nthat includes squared error and spherical softmax, can compute the exact loss,\ngradient update for the output weights, and gradient for backpropagation, all\nin O(d^2) per example instead of O(Dd), remarkably without ever computing the\nD-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e.\ntwo orders of magnitude for typical sizes, for that critical part of the\ncomputations that often dominates the training time in this kind of network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 18:51:08 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 04:02:12 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2015 01:27:13 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Vincent", "Pascal", ""], ["de Br\u00e9bisson", "Alexandre", ""], ["Bouthillier", "Xavier", ""]]}, {"id": "1412.7110", "submitter": "Dimitri Palaz", "authors": "Dimitri Palaz, Mathew Magimai Doss and Ronan Collobert", "title": "Learning linearly separable features for speech recognition using\n  convolutional neural networks", "comments": "Final version for ICLR 2015 Workshop; Revisions according to reviews.\n  Revised Section 4.5. Add references and correct typos. Submitted for ICLR\n  2015 conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic speech recognition systems usually rely on spectral-based features,\nsuch as MFCC of PLP. These features are extracted based on prior knowledge such\nas, speech perception or/and speech production. Recently, convolutional neural\nnetworks have been shown to be able to estimate phoneme conditional\nprobabilities in a completely data-driven manner, i.e. using directly temporal\nraw speech signal as input. This system was shown to yield similar or better\nperformance than HMM/ANN based system on phoneme recognition task and on large\nscale continuous speech recognition task, using less parameters. Motivated by\nthese studies, we investigate the use of simple linear classifier in the\nCNN-based framework. Thus, the network learns linearly separable features from\nraw speech. We show that such system yields similar or better performance than\nMLP based system using cepstral-based features as input.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 19:46:01 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 13:46:10 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 10:44:21 GMT"}, {"version": "v4", "created": "Thu, 26 Feb 2015 19:51:35 GMT"}, {"version": "v5", "created": "Fri, 27 Feb 2015 16:31:32 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 08:29:14 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Palaz", "Dimitri", ""], ["Doss", "Mathew Magimai", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.7119", "submitter": "Paul Baltescu", "authors": "Paul Baltescu and Phil Blunsom", "title": "Pragmatic Neural Language Modelling in Machine Translation", "comments": "NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an in-depth investigation on integrating neural language\nmodels in translation systems. Scaling neural language models is a difficult\ntask, but crucial for real-world applications. This paper evaluates the impact\non end-to-end MT quality of both new and existing scaling techniques. We show\nwhen explicitly normalising neural models is necessary and what optimisation\ntricks one should use in such scenarios. We also focus on scalable training\nalgorithms and investigate noise contrastive estimation and diagonal contexts\nas sources for further speed improvements. We explore the trade-offs between\nneural models and back-off n-gram models and find that neural models make\nstrong candidates for natural language applications in memory constrained\nenvironments, yet still lag behind traditional models in raw translation\nquality. We conclude with a set of recommendations one should follow to build a\nscalable neural language model for MT.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:08:06 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 02:17:28 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2015 17:20:03 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Baltescu", "Paul", ""], ["Blunsom", "Phil", ""]]}, {"id": "1412.7180", "submitter": "Yishu Miao", "authors": "Yishu Miao, Ziyu Wang, Phil Blunsom", "title": "Bayesian Optimisation for Machine Translation", "comments": "Bayesian optimisation workshop, NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel Bayesian optimisation algorithms for minimum error\nrate training of statistical machine translation systems. We explore two\nclasses of algorithms for efficiently exploring the translation space, with the\nfirst based on N-best lists and the second based on a hypergraph representation\nthat compactly represents an exponential number of translation options. Our\nalgorithms exhibit faster convergence and are capable of obtaining lower error\nrates than the existing translation model specific approaches, all within a\ngeneric Bayesian optimisation framework. Further more, we also introduce a\nrandom embedding algorithm to scale our approach to sparse high dimensional\nfeature sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 21:44:00 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Miao", "Yishu", ""], ["Wang", "Ziyu", ""], ["Blunsom", "Phil", ""]]}, {"id": "1412.7186", "submitter": "Ramon Ferrer i Cancho", "authors": "Ramon Ferrer-i-Cancho", "title": "Reply to the commentary \"Be careful when assuming the obvious\", by P.\n  Alday", "comments": "Minor corrections (language improved)", "journal-ref": "Language Dynamics and Change 5 (1), 147-155 (2015)", "doi": "10.1163/22105832-00501009", "report-no": null, "categories": "cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we respond to some comments by Alday concerning headedness in linguistic\ntheory and the validity of the assumptions of a mathematical model for word\norder. For brevity, we focus only on two assumptions: the unit of measurement\nof dependency length and the monotonicity of the cost of a dependency as a\nfunction of its length. We also revise the implicit psychological bias in\nAlday's comments. Notwithstanding, Alday is indicating the path for linguistic\nresearch with his unusual concerns about parsimony from multiple dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:05:06 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 06:52:04 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "1412.7415", "submitter": "Jestin Joy", "authors": "Jestin Joy, Kannan Balakrishnan", "title": "A prototype Malayalam to Sign Language Automatic Translator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign language, which is a medium of communication for deaf people, uses\nmanual communication and body language to convey meaning, as opposed to using\nsound. This paper presents a prototype Malayalam text to sign language\ntranslation system. The proposed system takes Malayalam text as input and\ngenerates corresponding Sign Language. Output animation is rendered using a\ncomputer generated model. This system will help to disseminate information to\nthe deaf people in public utility places like railways, banks, hospitals etc.\nThis will also act as an educational tool in learning Sign Language.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:51:41 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 04:52:42 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Joy", "Jestin", ""], ["Balakrishnan", "Kannan", ""]]}, {"id": "1412.7449", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever,\n  Geoffrey Hinton", "title": "Grammar as a Foreign Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic constituency parsing is a fundamental problem in natural language\nprocessing and has been the subject of intensive research and engineering for\ndecades. As a result, the most accurate parsers are domain specific, complex,\nand inefficient. In this paper we show that the domain agnostic\nattention-enhanced sequence-to-sequence model achieves state-of-the-art results\non the most widely used syntactic constituency parsing dataset, when trained on\na large synthetic corpus that was annotated using existing parsers. It also\nmatches the performance of standard parsers when trained only on a small\nhuman-annotated dataset, which shows that this model is highly data-efficient,\nin contrast to sequence-to-sequence models without the attention mechanism. Our\nparser is also fast, processing over a hundred sentences per second with an\nunoptimized CPU implementation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 17:16:24 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 03:16:54 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 22:41:07 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Vinyals", "Oriol", ""], ["Kaiser", "Lukasz", ""], ["Koo", "Terry", ""], ["Petrov", "Slav", ""], ["Sutskever", "Ilya", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1412.7782", "submitter": "Roshan Ragel", "authors": "MAC Jiffriya, MAC Akmal Jahan, and Roshan G. Ragel", "title": "Plagiarism Detection on Electronic Text based Assignments using Vector\n  Space Model (ICIAfS14)", "comments": "appears in The 7th International Conference on Information and\n  Automation for Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism is known as illegal use of others' part of work or whole work as\none's own in any field such as art, poetry, literature, cinema, research and\nother creative forms of study. Plagiarism is one of the important issues in\nacademic and research fields and giving more concern in academic systems. The\nsituation is even worse with the availability of ample resources on the web.\nThis paper focuses on an effective plagiarism detection tool on identifying\nsuitable intra-corpal plagiarism detection for text based assignments by\ncomparing unigram, bigram, trigram of vector space model with cosine similarity\nmeasure. Manually evaluated, labelled dataset was tested using unigram, bigram\nand trigram vector. Even though trigram vector consumes comparatively more\ntime, it shows better results with the labelled data. In addition, the selected\ntrigram vector space model with cosine similarity measure is compared with\ntri-gram sequence matching technique with Jaccard measure. In the results,\ncosine similarity score shows slightly higher values than the other. Because,\nit focuses on giving more weight for terms that do not frequently exist in the\ndataset and cosine similarity measure using trigram technique is more\npreferable than the other. Therefore, we present our new tool and it could be\nused as an effective tool to evaluate text based electronic assignments and\nminimize the plagiarism among students.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 03:54:01 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Jiffriya", "MAC", ""], ["Jahan", "MAC Akmal", ""], ["Ragel", "Roshan G.", ""]]}, {"id": "1412.8010", "submitter": "Xuan-Son Vu", "authors": "Xuan-Son Vu and Seong-Bae Park", "title": "Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary", "comments": "accepted on April-9th-2014, best paper award", "journal-ref": "The 40th Conference of the Korea Information Processing Society,\n  pp. 745-748, April 2014, South Korea", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SentiWordNet is an important lexical resource supporting sentiment analysis\nin opinion mining applications. In this paper, we propose a novel approach to\nconstruct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated\nfrom WordNet in which each synset has numerical scores to indicate its opinion\npolarities. Many previous studies obtained these scores by applying a machine\nlearning method to WordNet. However, Vietnamese WordNet is not available\nunfortunately by the time of this paper. Therefore, we propose a method to\nconstruct VSWN from a Vietnamese dictionary, not from WordNet. We show the\neffectiveness of the proposed method by generating a VSWN with 39,561 synsets\nautomatically. The method is experimentally tested with 266 synsets with aspect\nof positivity and negativity. It attains a competitive result compared with\nEnglish SentiWordNet that is 0.066 and 0.052 differences for positivity and\nnegativity sets respectively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 01:54:15 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Vu", "Xuan-Son", ""], ["Park", "Seong-Bae", ""]]}, {"id": "1412.8079", "submitter": "Ayoub Bagheri", "authors": "Ayoub Bagheri, Mohamad Saraee", "title": "Persian Sentiment Analyzer: A Framework based on a Novel Feature\n  Selection Method", "comments": null, "journal-ref": "International Journal of Artificial Intelligence 12.2 (2014):\n  115-129", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent decade, with the enormous growth of digital content in internet\nand databases, sentiment analysis has received more and more attention between\ninformation retrieval and natural language processing researchers. Sentiment\nanalysis aims to use automated tools to detect subjective information from\nreviews. One of the main challenges in sentiment analysis is feature selection.\nFeature selection is widely used as the first stage of analysis and\nclassification tasks to reduce the dimension of problem, and improve speed by\nthe elimination of irrelevant and redundant features. Up to now as there are\nfew researches conducted on feature selection in sentiment analysis, there are\nvery rare works for Persian sentiment analysis. This paper considers the\nproblem of sentiment classification using different feature selection methods\nfor online customer reviews in Persian language. Three of the challenges of\nPersian text are using of a wide variety of declensional suffixes, different\nword spacing and many informal or colloquial words. In this paper we study\nthese challenges by proposing a model for sentiment classification of Persian\nreview documents. The proposed model is based on lemmatization and feature\nselection and is employed Naive Bayes algorithm for classification. We evaluate\nthe performance of the model on a manually gathered collection of cellphone\nreviews, where the results show the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 21:00:24 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Bagheri", "Ayoub", ""], ["Saraee", "Mohamad", ""]]}, {"id": "1412.8102", "submitter": "EPTCS", "authors": "Bob Coecke (University of Oxford), Ichiro Hasuo (The University of\n  Tokyo), Prakash Panangaden (McGill University)", "title": "Proceedings of the 11th workshop on Quantum Physics and Logic", "comments": null, "journal-ref": "EPTCS 172, 2014", "doi": "10.4204/EPTCS.172", "report-no": null, "categories": "cs.LO cs.CL cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 11th International Workshop on\nQuantum Physics and Logic (QPL 2014), which was held from the 4th to the 6th of\nJune, 2014, at Kyoto University, Japan.\n  The goal of the QPL workshop series is to bring together researchers working\non mathematical foundations of quantum physics, quantum computing and\nspatio-temporal causal structures, and in particular those that use logical\ntools, ordered algebraic and category-theoretic structures, formal languages,\nsemantic methods and other computer science methods for the study of physical\nbehavior in general. Over the past few years, there has been growing activity\nin these foundational approaches, together with a renewed interest in the\nfoundations of quantum theory, which complement the more mainstream research in\nquantum computation. Earlier workshops in this series, with the same acronym\nunder the name \"Quantum Programming Languages\", were held in Ottawa (2003),\nTurku (2004), Chicago (2005), and Oxford (2006). The first QPL under the new\nname Quantum Physics and Logic was held in Reykjavik (2008), followed by Oxford\n(2009 and 2010), Nijmegen (2011), Brussels (2012) and Barcelona (2013).\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 03:54:16 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Coecke", "Bob", "", "University of Oxford"], ["Hasuo", "Ichiro", "", "The University of\n  Tokyo"], ["Panangaden", "Prakash", "", "McGill University"]]}, {"id": "1412.8319", "submitter": "Pawe{\\l} O\\'swi\\k{e}cimka", "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Pawe{\\l} O\\'swi\\k{e}cimka, Andrzej Kulig,\n  Jaros{\\l}aw Kwapie\\'n, Katarzyna Bazarnik, Iwona Grabska-Gradzi\\'nska, Jan\n  Rybicki, Marek Stanuszek", "title": "Quantifying origin and character of long-range correlations in narrative\n  texts", "comments": "28 pages, 8 figures, accepted for publication in Information Sciences", "journal-ref": "Information Sciences 331 (2016) 32-44", "doi": null, "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language using short sentences is considered efficient for\ncommunication. However, a text composed exclusively of such sentences looks\ntechnical and reads boring. A text composed of long ones, on the other hand,\ndemands significantly more effort for comprehension. Studying characteristics\nof the sentence length variability (SLV) in a large corpus of world-famous\nliterary texts shows that an appealing and aesthetic optimum appears somewhere\nin between and involves selfsimilar, cascade-like alternation of various\nlengths sentences. A related quantitative observation is that the power spectra\nS(f) of thus characterized SLV universally develop a convincing `1/f^beta'\nscaling with the average exponent beta =~ 1/2, close to what has been\nidentified before in musical compositions or in the brain waves. An\noverwhelming majority of the studied texts simply obeys such fractal attributes\nbut especially spectacular in this respect are hypertext-like, \"stream of\nconsciousness\" novels. In addition, they appear to develop structures\ncharacteristic of irreducibly interwoven sets of fractals called multifractals.\nScaling of S(f) in the present context implies existence of the long-range\ncorrelations in texts and appearance of multifractality indicates that they\ncarry even a nonlinear component. A distinct role of the full stops in inducing\nthe long-range correlations in texts is evidenced by the fact that the above\nquantitative characteristics on the long-range correlations manifest themselves\nin variation of the full stops recurrence times along texts, thus in SLV, but\nto a much lesser degree in the recurrence times of the most frequent words. In\nthis latter case the nonlinear correlations, thus multifractality, disappear\neven completely for all the texts considered. Treated as one extra word, the\nfull stops at the same time appear to obey the Zipfian rank-frequency\ndistribution, however.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 12:00:25 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 10:47:34 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Dro\u017cd\u017c", "Stanis\u0142aw", ""], ["O\u015bwi\u0119cimka", "Pawe\u0142", ""], ["Kulig", "Andrzej", ""], ["Kwapie\u0144", "Jaros\u0142aw", ""], ["Bazarnik", "Katarzyna", ""], ["Grabska-Gradzi\u0144ska", "Iwona", ""], ["Rybicki", "Jan", ""], ["Stanuszek", "Marek", ""]]}, {"id": "1412.8419", "submitter": "Pedro O. Pinheiro", "authors": "Remi Lebret and Pedro O. Pinheiro and Ronan Collobert", "title": "Simple Image Description Generator via a Linear Phrase-Based Approach", "comments": "Accepted as a workshop paper at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 18:43:10 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 05:09:13 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2015 03:53:26 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Lebret", "Remi", ""], ["Pinheiro", "Pedro O.", ""], ["Collobert", "Ronan", ""]]}, {"id": "1412.8504", "submitter": "Diego Amancio", "authors": "Diego R. Amancio", "title": "Probing the topological properties of complex networks modeling short\n  written texts", "comments": null, "journal-ref": "PLoS ONE 10(2): e0118394, 2015", "doi": "10.1371/journal.pone.0118394", "report-no": null, "categories": "cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, graph theory has been widely employed to probe several\nlanguage properties. More specifically, the so-called word adjacency model has\nbeen proven useful for tackling several practical problems, especially those\nrelying on textual stylistic analysis. The most common approach to treat texts\nas networks has simply considered either large pieces of texts or entire books.\nThis approach has certainly worked well -- many informative discoveries have\nbeen made this way -- but it raises an uncomfortable question: could there be\nimportant topological patterns in small pieces of texts? To address this\nproblem, the topological properties of subtexts sampled from entire books was\nprobed. Statistical analyzes performed on a dataset comprising 50 novels\nrevealed that most of the traditional topological measurements are stable for\nshort subtexts. When the performance of the authorship recognition task was\nanalyzed, it was found that a proper sampling yields a discriminability similar\nto the one found with full texts. Surprisingly, the support vector machine\nclassification based on the characterization of short texts outperformed the\none performed with entire books. These findings suggest that a local\ntopological analysis of large documents might improve its global\ncharacterization. Most importantly, it was verified, as a proof of principle,\nthat short texts can be analyzed with the methods and concepts of complex\nnetworks. As a consequence, the techniques described here can be extended in a\nstraightforward fashion to analyze texts as time-varying complex networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 23:09:13 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Amancio", "Diego R.", ""]]}, {"id": "1412.8527", "submitter": "EPTCS", "authors": "Anne Preller (LIRMM, France)", "title": "From Logical to Distributional Models", "comments": "In Proceedings QPL 2013, arXiv:1412.7917", "journal-ref": "EPTCS 171, 2014, pp. 113-131", "doi": "10.4204/EPTCS.171.11", "report-no": null, "categories": "cs.LO cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper relates two variants of semantic models for natural language,\nlogical functional models and compositional distributional vector space models,\nby transferring the logic and reasoning from the logical to the distributional\nmodels.\n  The geometrical operations of quantum logic are reformulated as algebraic\noperations on vectors. A map from functional models to vector space models\nmakes it possible to compare the meaning of sentences word by word.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 01:43:39 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Preller", "Anne", "", "LIRMM, France"]]}]