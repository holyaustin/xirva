[{"id": "1403.0052", "submitter": "Laurent Romary", "authors": "Laurent Romary (IDSL, INRIA Saclay - Ile de France, CMB)", "title": "TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding\n  Initiative guidelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an attempt to customise the TEI (Text Encoding\nInitiative) guidelines in order to offer the possibility to incorporate TBX\n(TermBase eXchange) based terminological entries within any kind of TEI\ndocuments. After presenting the general historical, conceptual and technical\ncontexts, we describe the various design choices we had to take while creating\nthis customisation, which in turn have led to make various changes in the\nactual TBX serialisation. Keeping in mind the objective to provide the TEI\nguidelines with, again, an onomasiological model, we try to identify the best\ncomprise in maintaining both the isomorphism with the existing TBX Basic\nstandard and the characteristics of the TEI framework.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 06:46:11 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Romary", "Laurent", "", "IDSL, INRIA Saclay - Ile de France, CMB"]]}, {"id": "1403.0531", "submitter": "Josiah Zayner Ph.D.", "authors": "Josiah P. Zayner", "title": "We Tweet Like We Talk and Other Interesting Observations: An Analysis of\n  English Communication Modalities", "comments": "9 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modalities of communication for human beings are gradually increasing in\nnumber with the advent of new forms of technology. Many human beings can\nreadily transition between these different forms of communication with little\nor no effort, which brings about the question: How similar are these different\ncommunication modalities? To understand technology$\\text{'}$s influence on\nEnglish communication, four different corpora were analyzed and compared:\nWriting from Books using the 1-grams database from the Google Books project,\nTwitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices\nrevealed that Talking has the most similarity when compared to the other modes\nof communication, while 1-grams were the least similar form of communication\nanalyzed. Based on the analysis of word usage, word usage frequency\ndistributions, and word class usage, among other things, Talking is also the\nmost similar to Twitter and IRC Chat. This suggests that communicating using\nTwitter and IRC Chat evolved from Talking rather than Writing. When we\ncommunicate online, even though we are writing, we do not Tweet or Chat how we\nwrite books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing\nwere clearly differentiable from our analysis with Twitter and Chat being much\nmore similar to Fiction than Nonfiction writing. These hypotheses were then\ntested using author and journalists Cory Doctorow. Mr. Doctorow$\\text{'}$s\nWriting, Twitter usage, and Talking were all found to have very similar\nvocabulary usage patterns as the amalgamized populations, as long as the\nwriting was Fiction. However, Mr. Doctorow$\\text{'}$s Nonfiction writing is\ndifferent from 1-grams and other collected Nonfiction writings. This data could\nperhaps be used to create more entertaining works of Nonfiction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 19:27:23 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Zayner", "Josiah P.", ""]]}, {"id": "1403.0541", "submitter": "Saadat Anwar", "authors": "Saadat Anwar", "title": "Representing, reasoning and answering questions about biological\n  pathways - various applications", "comments": "thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological organisms are composed of numerous interconnected biochemical\nprocesses. Diseases occur when normal functionality of these processes is\ndisrupted. Thus, understanding these biochemical processes and their\ninterrelationships is a primary task in biomedical research and a prerequisite\nfor diagnosing diseases, and drug development. Scientists studying these\nprocesses have identified various pathways responsible for drug metabolism, and\nsignal transduction, etc.\n  Newer techniques and speed improvements have resulted in deeper knowledge\nabout these pathways, resulting in refined models that tend to be large and\ncomplex, making it difficult for a person to remember all aspects of it. Thus,\ncomputer models are needed to analyze them. We want to build such a system that\nallows modeling of biological systems and pathways in such a way that we can\nanswer questions about them.\n  Many existing models focus on structural and/or factoid questions, using\nsurface-level knowledge that does not require understanding the underlying\nmodel. We believe these are not the kind of questions that a biologist may ask\nsomeone to test their understanding of the biological processes. We want our\nsystem to answer the kind of questions a biologist may ask. Such questions\nappear in early college level text books.\n  Thus the main goal of our thesis is to develop a system that allows us to\nencode knowledge about biological pathways and answer such questions about them\ndemonstrating understanding of the pathway. To that end, we develop a language\nthat will allow posing such questions and illustrate the utility of our\nframework with various applications in the biological domain. We use some\nexisting tools with modifications to accomplish our goal.\n  Finally, we apply our system to real world applications by extracting pathway\nknowledge from text and answering questions related to drug development.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 19:45:41 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Anwar", "Saadat", ""]]}, {"id": "1403.0801", "submitter": "Derrick Higgins", "authors": "Derrick Higgins, Chris Brew, Michael Heilman, Ramon Ziai, Lei Chen,\n  Aoife Cahill, Michael Flor, Nitin Madnani, Joel Tetreault, Daniel Blanchard,\n  Diane Napolitano, Chong Min Lee and John Blackmore", "title": "Is getting the right answer just about choosing the right words? The\n  role of syntactically-informed features in short answer scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in the educational landscape have spurred greater interest in\nthe problem of automatically scoring short answer questions. A recent shared\ntask on this topic revealed a fundamental divide in the modeling approaches\nthat have been applied to this problem, with the best-performing systems split\nbetween those that employ a knowledge engineering approach and those that\nalmost solely leverage lexical information (as opposed to higher-level\nsyntactic information) in assigning a score to a given response. This paper\naims to introduce the NLP community to the largest corpus currently available\nfor short-answer scoring, provide an overview of methods used in the shared\ntask using this data, and explore the extent to which more\nsyntactically-informed features can contribute to the short answer scoring task\nin a way that avoids the question-specific manual effort of the knowledge\nengineering approach.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 14:45:56 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 14:50:06 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Higgins", "Derrick", ""], ["Brew", "Chris", ""], ["Heilman", "Michael", ""], ["Ziai", "Ramon", ""], ["Chen", "Lei", ""], ["Cahill", "Aoife", ""], ["Flor", "Michael", ""], ["Madnani", "Nitin", ""], ["Tetreault", "Joel", ""], ["Blanchard", "Daniel", ""], ["Napolitano", "Diane", ""], ["Lee", "Chong Min", ""], ["Blackmore", "John", ""]]}, {"id": "1403.1194", "submitter": "Minoru Sasaki", "authors": "Minoru Sasaki", "title": "Latent Semantic Word Sense Disambiguation Using Global Co-occurrence\n  Information", "comments": "6 pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I propose a novel word sense disambiguation method based on\nthe global co-occurrence information using NMF. When I calculate the dependency\nrelation matrix, the existing method tends to produce very sparse co-occurrence\nmatrix from a small training set. Therefore, the NMF algorithm sometimes does\nnot converge to desired solutions. To obtain a large number of co-occurrence\nrelations, I propose to use co-occurrence frequencies of dependency relations\nbetween word features in the whole training set. This enables us to solve data\nsparseness problem and induce more effective latent features. To evaluate the\nefficiency of the method of word sense disambiguation, I make some experiments\nto compare with the result of the two baseline methods. The results of the\nexperiments show this method is effective for word sense disambiguation in\ncomparison with the all baseline methods. Moreover, the proposed method is\neffective for obtaining a stable effect by analyzing the global co-occurrence\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 17:20:01 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Sasaki", "Minoru", ""]]}, {"id": "1403.1252", "submitter": "Rami Al-Rfou", "authors": "Bryan Perozzi, Rami Al-Rfou, Vivek Kulkarni, Steven Skiena", "title": "Inducing Language Networks from Continuous Space Word Representations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in unsupervised feature learning have developed powerful\nlatent representations of words. However, it is still not clear what makes one\nrepresentation better than another and how we can learn the ideal\nrepresentation. Understanding the structure of latent spaces attained is key to\nany future advancement in unsupervised learning. In this work, we introduce a\nnew view of continuous space word representations as language networks. We\nexplore two techniques to create language networks from learned features by\ninducing them for two popular word representation methods and examining the\nproperties of their resulting networks. We find that the induced networks\ndiffer from other methods of creating language networks, and that they contain\nmeaningful community structure.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:36:53 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 17:36:43 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Perozzi", "Bryan", ""], ["Al-Rfou", "Rami", ""], ["Kulkarni", "Vivek", ""], ["Skiena", "Steven", ""]]}, {"id": "1403.1310", "submitter": "Roshan Ragel", "authors": "M.A.C. Jiffriya, M.A.C. Akmal Jahan, R.G. Ragel and S. Deegalla", "title": "AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based\n  Assignments", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 376 - 380, 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6732013", "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism is one of the growing issues in academia and is always a concern\nin Universities and other academic institutions. The situation is becoming even\nworse with the availability of ample resources on the web. This paper focuses\non creating an effective and fast tool for plagiarism detection for text based\nelectronic assignments. Our plagiarism detection tool named AntiPlag is\ndeveloped using the tri-gram sequence matching technique. Three sets of text\nbased assignments were tested by AntiPlag and the results were compared against\nan existing commercial plagiarism detection tool. AntiPlag showed better\nresults in terms of false positives compared to the commercial tool due to the\npre-processing steps performed in AntiPlag. In addition, to improve the\ndetection latency, AntiPlag applies a data clustering technique making it four\ntimes faster than the commercial tool considered. AntiPlag could be used to\nisolate plagiarized text based assignments from non-plagiarised assignments\neasily. Therefore, we present AntiPlag, a fast and effective tool for\nplagiarism detection on text based electronic assignments.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:16:01 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Jiffriya", "M. A. C.", ""], ["Jahan", "M. A. C. Akmal", ""], ["Ragel", "R. G.", ""], ["Deegalla", "S.", ""]]}, {"id": "1403.1314", "submitter": "Roshan Ragel", "authors": "R. G. Ragel, P. Herath and U. Senanayake", "title": "Authorship detection of SMS messages using unigrams", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 387-392 , 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6732015", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SMS messaging is a popular media of communication. Because of its popularity\nand privacy, it could be used for many illegal purposes. Additionally, since\nthey are part of the day to day life, SMSes can be used as evidence for many\nlegal disputes. Since a cellular phone might be accessible to people close to\nthe owner, it is important to establish the fact that the sender of the message\nis indeed the owner of the phone. For this purpose, the straight forward\nsolutions seem to be the use of popular stylometric methods. However, in\ncomparison with the data used for stylometry in the literature, SMSes have\nunusual characteristics making it hard or impossible to apply these methods in\na conventional way. Our target is to come up with a method of authorship\ndetection of SMS messages that could still give a usable accuracy. We argue\nthat, considering the methods of author attribution, the best method that could\nbe applied to SMS messages is an n-gram method. To prove our point, we checked\ntwo different methods of distribution comparison with varying number of\ntraining and testing data. We specifically try to compare how well our\nalgorithms work under less amount of testing data and large number of candidate\nauthors (which we believe to be the real world scenario) against controlled\ntests with less number of authors and selected SMSes with large number of\nwords. To counter the lack of information in an SMS message, we propose the\nmethod of stacking together few SMSes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:33:55 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Ragel", "R. G.", ""], ["Herath", "P.", ""], ["Senanayake", "U.", ""]]}, {"id": "1403.1349", "submitter": "Sam Anzaroot", "authors": "Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum", "title": "Learning Soft Linear Constraints with Application to Citation Field\n  Extraction", "comments": "appears in Proc. the 52nd Annual Meeting of the Association for\n  Computational Linguistics (ACL2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately segmenting a citation string into fields for authors, titles, etc.\nis a challenging task because the output typically obeys various global\nconstraints. Previous work has shown that modeling soft constraints, where the\nmodel is encouraged, but not require to obey the constraints, can substantially\nimprove segmentation performance. On the other hand, for imposing hard\nconstraints, dual decomposition is a popular technique for efficient prediction\ngiven existing algorithms for unconstrained inference. We extend the technique\nto perform prediction subject to soft constraints. Moreover, with a technique\nfor performing inference given soft constraints, it is easy to automatically\ngenerate large families of constraints and learn their costs with a simple\nconvex optimization problem during training. This allows us to obtain\nsubstantial gains in accuracy on a new, challenging citation extraction\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:24:02 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 13:27:02 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Anzaroot", "Sam", ""], ["Passos", "Alexandre", ""], ["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1403.1451", "submitter": "Damiano Spina", "authors": "Arkaitz Zubiaga, Damiano Spina, Raquel Mart\\'inez, V\\'ictor Fresno", "title": "Real-Time Classification of Twitter Trends", "comments": "Pre-print of article accepted for publication in Journal of the\n  American Society for Information Science and Technology copyright @ 2013\n  (American Society for Information Science and Technology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media users give rise to social trends as they share about common\ninterests, which can be triggered by different reasons. In this work, we\nexplore the types of triggers that spark trends on Twitter, introducing a\ntypology with following four types: 'news', 'ongoing events', 'memes', and\n'commemoratives'. While previous research has analyzed trending topics in a\nlong term, we look at the earliest tweets that produce a trend, with the aim of\ncategorizing trends early on. This would allow to provide a filtered subset of\ntrends to end users. We analyze and experiment with a set of straightforward\nlanguage-independent features based on the social spread of trends to\ncategorize them into the introduced typology. Our method provides an efficient\nway to accurately categorize trending topics without need of external data,\nenabling news organizations to discover breaking news in real-time, or to\nquickly identify viral memes that might enrich marketing decisions, among\nothers. The analysis of social features also reveals patterns associated with\neach type of trend, such as tweets about ongoing events being shorter as many\nwere likely sent from mobile devices, or memes having more retweets originating\nfrom a few trend-setters.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 14:23:53 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Spina", "Damiano", ""], ["Mart\u00ednez", "Raquel", ""], ["Fresno", "V\u00edctor", ""]]}, {"id": "1403.1618", "submitter": "Mohammad Mahmoodi Varnamkhasti", "authors": "Maryam Mahmoodi, Mohammad Mahmoodi Varnamkhasti", "title": "Design a Persian Automated Plagiarism Detector (AMZPPD)", "comments": "3 pages, Published with International Journal of Engineering Trends\n  and Technology (IJETT)Published with International Journal of Engineering\n  Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology(IJETT),\n  V8(8),465-467 February 2014", "doi": "10.14445/22315381/IJETT-V8P280", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there are lots of plagiarism detection approaches. But few of them\nimplemented and adapted for Persian languages. In this paper, our work on\ndesigning and implementation of a plagiarism detection system based on\npre-processing and NLP technics will be described. And the results of testing\non a corpus will be presented.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 22:57:29 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Mahmoodi", "Maryam", ""], ["Varnamkhasti", "Mohammad Mahmoodi", ""]]}, {"id": "1403.1773", "submitter": "Fred Morstatter", "authors": "Fred Morstatter, Nichola Lubold, Heather Pon-Barry, J\\\"urgen Pfeffer,\n  and Huan Liu", "title": "Finding Eyewitness Tweets During Crises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disaster response agencies have started to incorporate social media as a\nsource of fast-breaking information to understand the needs of people affected\nby the many crises that occur around the world. These agencies look for tweets\nfrom within the region affected by the crisis to get the latest updates of the\nstatus of the affected region. However only 1% of all tweets are geotagged with\nexplicit location information. First responders lose valuable information\nbecause they cannot assess the origin of many of the tweets they collect. In\nthis work we seek to identify non-geotagged tweets that originate from within\nthe crisis region. Towards this, we address three questions: (1) is there a\ndifference between the language of tweets originating within a crisis region\nand tweets originating outside the region, (2) what are the linguistic patterns\nthat can be used to differentiate within-region and outside-region tweets, and\n(3) for non-geotagged tweets, can we automatically identify those originating\nwithin the crisis region in real-time?\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 15:13:13 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Morstatter", "Fred", ""], ["Lubold", "Nichola", ""], ["Pon-Barry", "Heather", ""], ["Pfeffer", "J\u00fcrgen", ""], ["Liu", "Huan", ""]]}, {"id": "1403.2004", "submitter": "Michael Stewart", "authors": "Michael Stewart", "title": "Natural Language Feature Selection via Cooccurrence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specificity is important for extracting collocations, keyphrases, multi-word\nand index terms [Newman et al. 2012]. It is also useful for tagging, ontology\nconstruction [Ryu and Choi 2006], and automatic summarization of documents\n[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and\ninverse-document frequency (TF-IDF) are typically used to do this, but fail to\ntake advantage of the semantic relationships between terms [Church and Gale\n1995]. The result is that general idiomatic terms are mistaken for specific\nterms. We demonstrate use of relational data for estimation of term\nspecificity. The specificity of a term can be learned from its distribution of\nrelations with other terms. This technique is useful for identifying relevant\nwords or terms for other natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 20:10:37 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Stewart", "Michael", ""]]}, {"id": "1403.2124", "submitter": "Saif Mohammad Dr.", "authors": "Hannah Davis and Saif M. Mohammad", "title": "Generating Music from Literature", "comments": null, "journal-ref": "In Proceedings of the EACL Workshop on Computational Linguistics\n  for Literature, April 2014, Gothenburg, Sweden", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system, TransProse, that automatically generates musical pieces\nfrom text. TransProse uses known relations between elements of music such as\ntempo and scale, and the emotions they evoke. Further, it uses a novel\nmechanism to determine sequences of notes that capture the emotional activity\nin the text. The work has applications in information visualization, in\ncreating audio-visual e-books, and in developing music apps.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 01:41:09 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Davis", "Hannah", ""], ["Mohammad", "Saif M.", ""]]}, {"id": "1403.2152", "submitter": "Robert Freeman", "authors": "Robert John Freeman", "title": "Parsing using a grammar of word association vectors", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper was was first drafted in 2001 as a formalization of the system\ndescribed in U.S. patent U.S. 7,392,174. It describes a system for implementing\na parser based on a kind of cross-product over vectors of contextually similar\nwords. It is being published now in response to nascent interest in vector\ncombination models of syntax and semantics. The method used aggressive\nsubstitution of contextually similar words and word groups to enable product\nvectors to stay in the same space as their operands and make entire sentences\ncomparable syntactically, and potentially semantically. The vectors generated\nhad sufficient representational strength to generate parse trees at least\ncomparable with contemporary symbolic parsers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 07:26:50 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Freeman", "Robert John", ""]]}, {"id": "1403.2345", "submitter": "Jalal Mahmud", "authors": "Jalal Mahmud, Jeffrey Nichols, Clemens Drews", "title": "Home Location Identification of Twitter Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for inferring the home location of Twitter users\nat different granularities, including city, state, time zone or geographic\nregion, using the content of users tweets and their tweeting behavior. Unlike\nexisting approaches, our algorithm uses an ensemble of statistical and\nheuristic classifiers to predict locations and makes use of a geographic\ngazetteer dictionary to identify place-name entities. We find that a\nhierarchical classification approach, where time zone, state or geographic\nregion is predicted first and city is predicted next, can improve prediction\naccuracy. We have also analyzed movement variations of Twitter users, built a\nclassifier to predict whether a user was travelling in a certain period of time\nand use that to further improve the location detection accuracy. Experimental\nevidence suggests that our algorithm works well in practice and outperforms the\nbest existing algorithms for predicting the home location of Twitter users.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 05:29:42 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Mahmud", "Jalal", ""], ["Nichols", "Jeffrey", ""], ["Drews", "Clemens", ""]]}, {"id": "1403.2837", "submitter": "Ayshe Rashidi", "authors": "Ayshe Rashidi and Mina Zolfy Lighvan", "title": "HPS: a hierarchical Persian stemming method", "comments": "10 pages, 6 tables, 2 figures, International Journal on Natural\n  Language Computing (IJNLC), International Journal on Natural Language\n  Computing (IJNLC) Vol. 3, No.1, February 2014", "journal-ref": null, "doi": "10.5121/ijnlc.2014.3102", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel hierarchical Persian stemming approach based on the\nPart-Of-Speech of the word in a sentence is presented. The implemented stemmer\nincludes hash tables and several deterministic finite automata in its different\nlevels of hierarchy for removing the prefixes and suffixes of the words. We had\ntwo intentions in using hash tables in our method. The first one is that the\nDFA don't support some special words, so hash table can partly solve the\naddressed problem. the second goal is to speed up the implemented stemmer with\nomitting the time that deterministic finite automata need. Because of the\nhierarchical organization, this method is fast and flexible enough. Our\nexperiments on test sets from Hamshahri collection and security news (istna.ir)\nshow that our method has the average accuracy of 95.37% which is even improved\nin using the method on a test set with common topics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 08:08:49 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Rashidi", "Ayshe", ""], ["Lighvan", "Mina Zolfy", ""]]}, {"id": "1403.3142", "submitter": "Shalini Ghosh", "authors": "Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick Lincoln, Natarajan\n  Shankar, Wilfried Steiner", "title": "ARSENAL: Automatic Requirements Specification Extraction from Natural\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": "SRI-CSL-13-01", "categories": "cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Requirements are informal and semi-formal descriptions of the expected\nbehavior of a complex system from the viewpoints of its stakeholders\n(customers, users, operators, designers, and engineers). However, for the\npurpose of design, testing, and verification for critical systems, we can\ntransform requirements into formal models that can be analyzed automatically.\nARSENAL is a framework and methodology for systematically transforming natural\nlanguage (NL) requirements into analyzable formal models and logic\nspecifications. These models can be analyzed for consistency and\nimplementability. The ARSENAL methodology is specialized to individual domains,\nbut the approach is general enough to be adapted to new domains.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 01:07:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 17:44:40 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 19:01:57 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Ghosh", "Shalini", ""], ["Elenius", "Daniel", ""], ["Li", "Wenchao", ""], ["Lincoln", "Patrick", ""], ["Shankar", "Natarajan", ""], ["Steiner", "Wilfried", ""]]}, {"id": "1403.3185", "submitter": "Md. Ansarul Haque", "authors": "Md. Ansarul Haque", "title": "Sentiment Analysis by Using Fuzzy Logic", "comments": "16 pages.\n  http://airccse.org/journal/ijcseit/papers/4114ijcseit04.pdf, February 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How could a product or service is reasonably evaluated by anyone in the\nshortest time? A million dollar question but it is having a simple answer:\nSentiment analysis. Sentiment analysis is consumers review on products and\nservices which helps both the producers and consumers (stakeholders) to take\neffective and efficient decision within a shortest period of time. Producers\ncan have better knowledge of their products and services through the sentiment\nanalysis (ex. positive and negative comments or consumers likes and dislikes)\nwhich will help them to know their products status (ex. product limitations or\nmarket status). Consumers can have better knowledge of their interested\nproducts and services through the sentiment analysis (ex. positive and negative\ncomments or consumers likes and dislikes) which will help them to know their\ndeserving products status (ex. product limitations or market status). For more\nspecification of the sentiment values, fuzzy logic could be introduced.\nTherefore, sentiment analysis with the help of fuzzy logic (deals with\nreasoning and gives closer views to the exact sentiment values) will help the\nproducers or consumers or any interested person for taking the effective\ndecision according to their product or service interest.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 07:48:15 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Haque", "Md. Ansarul", ""]]}, {"id": "1403.3351", "submitter": "Mehrnoosh Sadrzadeh", "authors": "Samson Abramsky and Mehrnoosh Sadrzadeh", "title": "Semantic Unification A sheaf theoretic approach to natural language", "comments": "12 pages", "journal-ref": "Categories and Types in Logic, Language, and Physics, A Festshrift\n  for Jim Lambek. Casadio, Coecke, Moortgat, Scott (eds.), Lecture Notes in\n  Computer Science, Volume 8222, pp. 1-13, 2014", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language is contextual and sheaf theory provides a high level mathematical\nframework to model contextuality. We show how sheaf theory can model the\ncontextual nature of natural language and how gluing can be used to provide a\nglobal semantics for a discourse by putting together the local logical\nsemantics of each sentence within the discourse. We introduce a presheaf\nstructure corresponding to a basic form of Discourse Representation Structures.\nWithin this setting, we formulate a notion of semantic unification --- gluing\nmeanings of parts of a discourse into a coherent whole --- as a form of\nsheaf-theoretic gluing. We illustrate this idea with a number of examples where\nit can used to represent resolutions of anaphoric references. We also discuss\nmultivalued gluing, described using a distributions functor, which can be used\nto represent situations where multiple gluings are possible, and where we may\nneed to rank them using quantitative measures.\n  Dedicated to Jim Lambek on the occasion of his 90th birthday.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 18:20:10 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Abramsky", "Samson", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1403.3460", "submitter": "Chi Wang", "authors": "Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han", "title": "Scalable and Robust Construction of Topical Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated generation of high-quality topical hierarchies for a text\ncollection is a dream problem in knowledge engineering with many valuable\napplications. In this paper a scalable and robust algorithm is proposed for\nconstructing a hierarchy of topics from a text collection. We divide and\nconquer the problem using a top-down recursive framework, based on a tensor\northogonal decomposition technique. We solve a critical challenge to perform\nscalable inference for our newly designed hierarchical topic model. Experiments\nwith various real-world datasets illustrate its ability to generate robust,\nhigh-quality hierarchies efficiently. Our method reduces the time of\nconstruction by several orders of magnitude, and its robust feature renders it\npossible for users to interactively revise the hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 23:22:21 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Wang", "Chi", ""], ["Liu", "Xueqing", ""], ["Song", "Yanglei", ""], ["Han", "Jiawei", ""]]}, {"id": "1403.3668", "submitter": "Arthur Merin", "authors": "Arthur Merin", "title": "Language Heedless of Logic - Philosophy Mindful of What? Failures of\n  Distributive and Absorption Laws", "comments": "33 pages. v2 differs from v1 thus: typos corrected, a sentence\n  adjusted for anaphoric reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of philosophical logic and all of philosophy of language make empirical\nclaims about the vernacular natural language. They presume semantics under\nwhich `and' and `or' are related by the dually paired distributive and\nabsorption laws. However, at least one of each pair of laws fails in the\nvernacular. `Implicature'-based auxiliary theories associated with the\nprogramme of H.P. Grice do not prove remedial. Conceivable alternatives that\nmight replace the familiar logics as descriptive instruments are briefly noted:\n(i) substructural logics and (ii) meaning composition in linear algebras over\nthe reals, occasionally constrained by norms of classical logic. Alternative\n(ii) locates the problem in violations of one of the idempotent laws. Reasons\nfor a lack of curiosity about elementary and easily testable implications of\nthe received theory are considered. The concept of `reflective equilibrium' is\ncritically examined for its role in reconciling normative desiderata and\ndescriptive commitments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 18:53:51 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 18:37:45 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Merin", "Arthur", ""]]}, {"id": "1403.4024", "submitter": "Uli Fahrenberg", "authors": "Uli Fahrenberg, Fabrizio Biondi, Kevin Corre, Cyrille Jegourel, Simon\n  Kongsh{\\o}j, Axel Legay", "title": "Measuring Global Similarity between Texts", "comments": "Submitted to SLSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new similarity measure between texts which, contrary to the\ncurrent state-of-the-art approaches, takes a global view of the texts to be\ncompared. We have implemented a tool to compute our textual distance and\nconducted experiments on several corpuses of texts. The experiments show that\nour methods can reliably identify different global types of texts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 08:22:54 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 09:19:59 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 11:14:16 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Fahrenberg", "Uli", ""], ["Biondi", "Fabrizio", ""], ["Corre", "Kevin", ""], ["Jegourel", "Cyrille", ""], ["Kongsh\u00f8j", "Simon", ""], ["Legay", "Axel", ""]]}, {"id": "1403.4362", "submitter": "Abderrahim Mohammed El Amine", "authors": "Mohammed El Amine Abderrahim", "title": "Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for\n  Information Retrieval System", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.3955 by\n  other authors", "journal-ref": "International Journal of Computational Linguistics Research, ISSN:\n  0976-416X, Volume 4, Issue 4, December, 2013, Pages 149-158", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article evaluates the performance of two techniques for query\nreformulation in a system for information retrieval, namely, the concept based\nand the pseudo relevance feedback reformulation. The experiments performed on a\ncorpus of Arabic text have allowed us to compare the contribution of these two\nreformulation techniques in improving the performance of an information\nretrieval system for Arabic texts.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 07:18:39 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Abderrahim", "Mohammed El Amine", ""]]}, {"id": "1403.4467", "submitter": "R\\'emi Dubot", "authors": "R\\'emi Dubot and Christophe Collet", "title": "A hybrid formalism to parse Sign Languages", "comments": "6 pages, 6 figures, Procedings of the 6th Workshop on the\n  Representation and Processing of Sign Languages: Beyond the manual channel,\n  Language Resources and Evaluation Conference (LREC) Reykjavik, Iceland, 31\n  May 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language (SL) linguistic is dependent on the expensive task of\nannotating. Some automation is already available for low-level information (eg.\nbody part tracking) and the lexical level has shown significant progresses. The\nsyntactic level lacks annotated corpora as well as complete and consistent\nmodels. This article presents a solution for the automatic annotation of SL\nsyntactic elements. It exposes a formalism able to represent both\nconstituency-based and dependency-based models. The first enable the\nrepresentation the structures one may want to annotate, the second aims at\nfulfilling the holes of the first. A parser is presented and used to conduct\ntwo experiments on the solution. One experiment is on a real corpus, the other\nis on a synthetic corpus.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 14:16:45 GMT"}, {"version": "v2", "created": "Wed, 25 Jun 2014 13:16:31 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Dubot", "R\u00e9mi", ""], ["Collet", "Christophe", ""]]}, {"id": "1403.4473", "submitter": "R\\'emi Dubot", "authors": "R\\'emi Dubot and Christophe Collet", "title": "Sign Language Gibberish for syntactic parsing evaluation", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language (SL) automatic processing slowly progresses bottom-up. The\nfield has seen proposition to handle the video signal, to recognize and\nsynthesize sublexical and lexical units. It starts to see the development of\nsupra-lexical processing. But the recognition, at this level, lacks data. The\nsyntax of SL appears very specific as it uses massively the multiplicity of\narticulators and its access to the spatial dimensions. Therefore new parsing\ntechniques are developed. However these need to be evaluated. The shortage on\nreal data restrains the corpus-based models to small sizes. We propose here a\nsolution to produce data-sets for the evaluation of parsers on the specific\nproperties of SL. The article first describes the general model used to\ngenerates dependency grammars and the phrase generation from these lasts. It\nthen discusses the limits of approach. The solution shows to be of particular\ninterest to evaluate the scalability of the techniques on big models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 14:31:51 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Dubot", "R\u00e9mi", ""], ["Collet", "Christophe", ""]]}, {"id": "1403.4759", "submitter": "Zeeshan Bhatti", "authors": "Zeeshan Bhatti, Imdad Ali Ismaili, Asad Ali Shaikh, Waseem Javaid", "title": "Spelling Error Trends and Patterns in Sindhi", "comments": "5 pages, 6 tables, Sindhi Languge", "journal-ref": "Journal of Emerging Trends in Computing and Information Sciences,\n  VOL. 3, NO.10 Oct, 2012, ISSN 2079-8407", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical error Correction technique is the most accurate and widely used\napproach today, but for a language like Sindhi which is a low resourced\nlanguage the trained corpora's are not available, so the statistical techniques\nare not possible at all. Instead a useful alternative would be to exploit\nvarious spelling error trends in Sindhi by using a Rule based approach. For\ndesigning such technique an essential prerequisite would be to study the\nvarious error patterns in a language. This pa per presents various studies of\nspelling error trends and their types in Sindhi Language. The research shows\nthat the error trends common to all languages are also encountered in Sindhi\nbut their do exist some error patters that are catered specifically to a Sindhi\nlanguage.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 10:45:29 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Bhatti", "Zeeshan", ""], ["Ismaili", "Imdad Ali", ""], ["Shaikh", "Asad Ali", ""], ["Javaid", "Waseem", ""]]}, {"id": "1403.4887", "submitter": "Andrew Warren", "authors": "Andrew Warren and Joao Setubal", "title": "Using Entropy Estimates for DAG-Based Ontologies", "comments": "in ISMB Bio-Ontologies, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Entropy measurements on hierarchical structures have been used in\nmethods for information retrieval and natural language modeling. Here we\nexplore its application to semantic similarity. By finding shared ontology\nterms, semantic similarity can be established between annotated genes. A common\nprocedure for establishing semantic similarity is to calculate the\ndescriptiveness (information content) of ontology terms and use these values to\ndetermine the similarity of annotations. Most often information content is\ncalculated for an ontology term by analyzing its frequency in an annotation\ncorpus. The inherent problems in using these values to model functional\nsimilarity motivates our work. Summary: We present a novel calculation for\nestablishing the entropy of a DAG-based ontology, which can be used in an\nalternative method for establishing the information content of its terms. We\nalso compare our IC metric to two others using semantic and sequence\nsimilarity.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 17:29:24 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 01:16:11 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Warren", "Andrew", ""], ["Setubal", "Joao", ""]]}, {"id": "1403.4928", "submitter": "Leon Derczynski", "authors": "Steven Bethard, Leon Derczynski, James Pustejovsky, Marc Verhagen", "title": "Clinical TempEval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Clinical TempEval task which is currently in preparation for\nthe SemEval-2015 evaluation exercise. This task involves identifying and\ndescribing events, times and the relations between them in clinical text. Six\ndiscrete subtasks are included, focusing on recognising mentions of times and\nevents, describing those mentions for both entity types, identifying the\nrelation between an event and the document creation time, and identifying\nnarrative container relations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 19:59:49 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Bethard", "Steven", ""], ["Derczynski", "Leon", ""], ["Pustejovsky", "James", ""], ["Verhagen", "Marc", ""]]}, {"id": "1403.5596", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "Tarek El-Shishtawy and Fatma El-Ghannam", "title": "A Lemma Based Evaluator for Semitic Language Text Summarization Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching texts in highly inflected languages such as Arabic by simple\nstemming strategy is unlikely to perform well. In this paper, we present a\nstrategy for automatic text matching technique for for inflectional languages,\nusing Arabic as the test case. The system is an extension of ROUGE test in\nwhich texts are matched on token's lemma level. The experimental results show\nan enhancement of detecting similarities between different sentences having\nsame semantics but written in different lexical forms..\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 00:13:03 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["El-Shishtawy", "Tarek", ""], ["El-Ghannam", "Fatma", ""]]}, {"id": "1403.6023", "submitter": "Luis Marujo", "authors": "Lu\\'is Marujo, Anatole Gershman, Jaime Carbonell, Jo\\~ao P. Neto,\n  David Martins de Matos", "title": "Ensemble Detection of Single & Multiple Events at Sentence-Level", "comments": "Preliminary version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event classification at sentence level is an important Information Extraction\ntask with applications in several NLP, IR, and personalization systems.\nMulti-label binary relevance (BR) are the state-of-art methods. In this work,\nwe explored new multi-label methods known for capturing relations between event\ntypes. These new methods, such as the ensemble Chain of Classifiers, improve\nthe F1 on average across the 6 labels by 2.8% over the Binary Relevance. The\nlow occurrence of multi-label sentences motivated the reduction of the hard\nimbalanced multi-label classification problem with low number of occurrences of\nmultiple labels per instance to an more tractable imbalanced multiclass problem\nwith better results (+ 4.6%). We report the results of adding new features,\nsuch as sentiment strength, rhetorical signals, domain-id (source-id and date),\nand key-phrases in both single-label and multi-label event classification\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 16:21:04 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Marujo", "Lu\u00eds", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Neto", "Jo\u00e3o P.", ""], ["de Matos", "David Martins", ""]]}, {"id": "1403.6173", "submitter": "Anna Senina", "authors": "Anna Senina and Marcus Rohrbach and Wei Qiu and Annemarie Friedrich\n  and Sikandar Amin and Mykhaylo Andriluka and Manfred Pinkal and Bernt Schiele", "title": "Coherent Multi-Sentence Video Description with Variable Level of Detail", "comments": "10 pages", "journal-ref": null, "doi": "10.1007/978-3-319-11752-2_15", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily describe what they see in a coherent way and at varying\nlevel of detail. However, existing approaches for automatic video description\nare mainly focused on single sentence generation and produce descriptions at a\nfixed level of detail. In this paper, we address both of these limitations: for\na variable level of detail we produce coherent multi-sentence descriptions of\ncomplex videos. We follow a two-step approach where we first learn to predict a\nsemantic representation (SR) from video and then generate natural language\ndescriptions from the SR. To produce consistent multi-sentence descriptions, we\nmodel across-sentence consistency at the level of the SR by enforcing a\nconsistent topic. We also contribute both to the visual recognition of objects\nproposing a hand-centric approach as well as to the robust generation of\nsentences using a word lattice. Human judges rate our multi-sentence\ndescriptions as more readable, correct, and relevant than related work. To\nunderstand the difference between more detailed and shorter descriptions, we\ncollect and analyze a video description corpus of three levels of detail.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 22:28:38 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Senina", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Qiu", "Wei", ""], ["Friedrich", "Annemarie", ""], ["Amin", "Sikandar", ""], ["Andriluka", "Mykhaylo", ""], ["Pinkal", "Manfred", ""], ["Schiele", "Bernt", ""]]}, {"id": "1403.6381", "submitter": "Sureka Krishnasamy", "authors": "K. Sureka, K.G. Srinivasagan, S. Suganthi", "title": "An efficiency dependency parser using hybrid approach for tamil language", "comments": "5 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Natural language processing is a prompt research area across the country.\nParsing is one of the very crucial tool in language analysis system which aims\nto forecast the structural relationship among the words in a given sentence.\nMany researchers have already developed so many language tools but the accuracy\nis not meet out the human expectation level, thus the research is still exists.\nMachine translation is one of the major application area under Natural Language\nProcessing. While translation between one language to another language, the\nstructure identification of a sentence play a key role. This paper introduces\nthe hybrid way to solve the identification of relationship among the given\nwords in a sentence. In existing system is implemented using rule based\napproach, which is not suited in huge amount of data. The machine learning\napproaches is suitable for handle larger amount of data and also to get better\naccuracy via learning and training the system. The proposed approach takes a\nTamil sentence as an input and produce the result of a dependency relation as a\ntree like structure using hybrid approach. This proposed tool is very helpful\nfor researchers and act as an odd-on improve the quality of existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 04:54:28 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Sureka", "K.", ""], ["Srinivasagan", "K. G.", ""], ["Suganthi", "S.", ""]]}, {"id": "1403.6392", "submitter": "Arturo Tlaca\\'elel Curiel D\\'iaz", "authors": "Arturo Curiel, Christophe Collet", "title": "Implementation of an Automatic Sign Language Lexical Annotation\n  Framework based on Propositional Dynamic Logic", "comments": "Accepted in the \"LREC 2014: 6th Workshop on the Representation and\n  Processing of Signed Languages\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the implementation of an automatic Sign Language\n(SL) sign annotation framework based on a formal logic, the Propositional\nDynamic Logic (PDL). Our system relies heavily on the use of a specific variant\nof PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets\nus describe SL signs as formulae and corpora videos as labeled transition\nsystems (LTSs). Here, we intend to show how a generic annotation system can be\nconstructed upon these underlying theoretical principles, regardless of the\ntracking technologies available or the input format of corpora. With this in\nmind, we generated a development framework that adapts the system to specific\nuse cases. Furthermore, we present some results obtained by our application\nwhen adapted to one distinct case, 2D corpora analysis with pre-processed\ntracking information. We also present some insights on how such a technology\ncan be used to analyze 3D real-time data, captured with a depth device.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 15:36:36 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 11:06:00 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Curiel", "Arturo", ""], ["Collet", "Christophe", ""]]}, {"id": "1403.6397", "submitter": "Michael R\\\"oder", "authors": "Frank Rosner, Alexander Hinneburg, Michael R\\\"oder, Martin Nettling,\n  Andreas Both", "title": "Evaluating topic coherence measures", "comments": "This work has been presented at the \"Topic Models: Computation,\n  Application and Evaluation\" workshop at the \"Neural Information Processing\n  Systems\" conference 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Topic models extract representative word sets - called topics - from word\ncounts in documents without requiring any semantic annotations. Topics are not\nguaranteed to be well interpretable, therefore, coherence measures have been\nproposed to distinguish between good and bad topics. Studies of topic coherence\nso far are limited to measures that score pairs of individual words. For the\nfirst time, we include coherence measures from scientific philosophy that score\npairs of more complex word subsets and apply them to topic scoring.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 15:44:14 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Rosner", "Frank", ""], ["Hinneburg", "Alexander", ""], ["R\u00f6der", "Michael", ""], ["Nettling", "Martin", ""], ["Both", "Andreas", ""]]}, {"id": "1403.6636", "submitter": "Arturo Tlaca\\'elel Curiel D\\'iaz", "authors": "Arturo Curiel, Christophe Collet", "title": "Sign Language Lexical Recognition With Propositional Dynamic Logic", "comments": "6 pages, Proceedings of the 51st Annual Meeting of the Association\n  for Computational Linguistics (Volume 2: Short Papers), August, 2013", "journal-ref": "In Proceedings of the 51st Annual Meeting of the Association for\n  Computational Linguistics, vol. 2, pp. 328-333. 2013", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper explores the use of Propositional Dynamic Logic (PDL) as a\nsuitable formal framework for describing Sign Language (SL), the language of\ndeaf people, in the context of natural language processing. SLs are visual,\ncomplete, standalone languages which are just as expressive as oral languages.\nSigns in SL usually correspond to sequences of highly specific body postures\ninterleaved with movements, which make reference to real world objects,\ncharacters or situations. Here we propose a formal representation of SL signs,\nthat will help us with the analysis of automatically-collected hand tracking\ndata from French Sign Language (FSL) video corpora. We further show how such a\nrepresentation could help us with the design of computer aided SL verification\ntools, which in turn would bring us closer to the development of an automatic\nrecognition system for these languages.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 11:47:37 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Curiel", "Arturo", ""], ["Collet", "Christophe", ""]]}, {"id": "1403.7335", "submitter": "Duyu Tang", "authors": "Duyu Tang, Bing Qin, Ting Liu, Qiuhui Shi", "title": "Emotion Analysis Platform on Chinese Microblog", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Weibo, as the largest social media service in China, has billions of messages\ngenerated every day. The huge number of messages contain rich sentimental\ninformation. In order to analyze the emotional changes in accordance with time\nand space, this paper presents an Emotion Analysis Platform (EAP), which\nexplores the emotional distribution of each province, so that can monitor the\nglobal pulse of each province in China. The massive data of Weibo and the\nreal-time requirements make the building of EAP challenging. In order to solve\nthe above problems, emoticons, emotion lexicon and emotion-shifting rules are\nadopted in EAP to analyze the emotion of each tweet. In order to verify the\neffectiveness of the platform, case study on the Sichuan earthquake is done,\nand the analysis result of the platform accords with the fact. In order to\nanalyze from quantity, we manually annotate a test set and conduct experiment\non it. The experimental results show that the macro-Precision of EAP reaches\n80% and the EAP works effectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 10:45:31 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Tang", "Duyu", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""], ["Shi", "Qiuhui", ""]]}, {"id": "1403.7455", "submitter": "Mahima Sharma", "authors": "Shruti Mathur, Varun Prakash Saxena", "title": "Hybrid Approach to English-Hindi Name Entity Transliteration", "comments": "Proceedings of IEEE Students' Conference on Electrical, Electronics\n  and Computer Sciences 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation (MT) research in Indian languages is still in its\ninfancy. Not much work has been done in proper transliteration of name entities\nin this domain. In this paper we address this issue. We have used English-Hindi\nlanguage pair for our experiments and have used a hybrid approach. At first we\nhave processed English words using a rule based approach which extracts\nindividual phonemes from the words and then we have applied statistical\napproach which converts the English into its equivalent Hindi phoneme and in\nturn the corresponding Hindi word. Through this approach we have attained\n83.40% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 17:30:32 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Mathur", "Shruti", ""], ["Saxena", "Varun Prakash", ""]]}]