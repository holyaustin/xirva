[{"id": "1505.00122", "submitter": "Richard A. Blythe", "authors": "Richard A. Blythe", "title": "Hierarchy of Scales in Language Dynamics", "comments": "Colloquium (short review paper) solicited by European Physical\n  Journal B. 18 pages, 3 figures. accepted v2 contains more text, figures,\n  references and coherence", "journal-ref": "EPJB (2015) v88 295", "doi": "10.1140/epjb/e2015-60347-3", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods and insights from statistical physics are finding an increasing\nvariety of applications where one seeks to understand the emergent properties\nof a complex interacting system. One such area concerns the dynamics of\nlanguage at a variety of levels of description, from the behaviour of\nindividual agents learning simple artificial languages from each other, up to\nchanges in the structure of languages shared by large groups of speakers over\nhistorical timescales. In this Colloquium, we survey a hierarchy of scales at\nwhich language and linguistic behaviour can be described, along with the main\nprogress in understanding that has been made at each of them---much of which\nhas come from the statistical physics community. We argue that future\ndevelopments may arise by linking the different levels of the hierarchy\ntogether in a more coherent fashion, in particular where this allows more\neffective use of rich empirical data sets.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 08:44:45 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 19:53:49 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Blythe", "Richard A.", ""]]}, {"id": "1505.00138", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis", "title": "Compositional Distributional Semantics with Compact Closed Categories\n  and Frobenius Algebras", "comments": "Ph.D. Dissertation, University of Oxford", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT math.QA quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis contributes to ongoing research related to the categorical\ncompositional model for natural language of Coecke, Sadrzadeh and Clark in\nthree ways: Firstly, I propose a concrete instantiation of the abstract\nframework based on Frobenius algebras (joint work with Sadrzadeh). The theory\nimproves shortcomings of previous proposals, extends the coverage of the\nlanguage, and is supported by experimental work that improves existing results.\nThe proposed framework describes a new class of compositional models that find\nintuitive interpretations for a number of linguistic phenomena. Secondly, I\npropose and evaluate in practice a new compositional methodology which\nexplicitly deals with the different levels of lexical ambiguity (joint work\nwith Pulman). A concrete algorithm is presented, based on the separation of\nvector disambiguation from composition in an explicit prior step. Extensive\nexperimental work shows that the proposed methodology indeed results in more\naccurate composite representations for the framework of Coecke et al. in\nparticular and every other class of compositional models in general. As a last\ncontribution, I formalize the explicit treatment of lexical ambiguity in the\ncontext of the categorical framework by resorting to categorical quantum\nmechanics (joint work with Coecke). In the proposed extension, the concept of a\ndistributional vector is replaced with that of a density matrix, which\ncompactly represents a probability distribution over the potential different\nmeanings of the specific word. Composition takes the form of quantum\nmeasurements, leading to interesting analogies between quantum physics and\nlinguistics.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:00:33 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Kartsaklis", "Dimitri", ""]]}, {"id": "1505.00161", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi", "title": "Embedding Semantic Relations into Word Representations", "comments": "International Joint Conferences in AI (IJCAI) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations for semantic relations is important for various\ntasks such as analogy detection, relational search, and relation\nclassification. Although there have been several proposals for learning\nrepresentations for individual words, learning word representations that\nexplicitly capture the semantic relations between words remains under\ndeveloped. We propose an unsupervised method for learning vector\nrepresentations for words such that the learnt representations are sensitive to\nthe semantic relations that exist between two words. First, we extract lexical\npatterns from the co-occurrence contexts of two words in a corpus to represent\nthe semantic relations that exist between those two words. Second, we represent\na lexical pattern as the weighted sum of the representations of the words that\nco-occur with that lexical pattern. Third, we train a binary classifier to\ndetect relationally similar vs. non-similar lexical pattern pairs. The proposed\nmethod is unsupervised in the sense that the lexical pattern pairs we use as\ntrain data are automatically sampled from a corpus, without requiring any\nmanual intervention. Our proposed method statistically significantly\noutperforms the current state-of-the-art word representations on three\nbenchmark datasets for proportional analogy detection, demonstrating its\nability to accurately capture the semantic relations among words.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 11:43:34 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Bollegala", "Danushka", ""], ["Maehara", "Takanori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1505.00277", "submitter": "Dana Movshovitz-Attias", "authors": "Dana Movshovitz-Attias, William W. Cohen", "title": "Grounded Discovery of Coordinate Term Relationships between Software\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the detection of coordinate-term relationships\nbetween entities from the software domain, that refer to Java classes. Usually,\nrelations are found by examining corpus statistics associated with text\nentities. In some technical domains, however, we have access to additional\ninformation about the real-world objects named by the entities, suggesting that\ncoupling information about the \"grounded\" entities with corpus statistics might\nlead to improved methods for relation discovery. To this end, we develop a\nsimilarity measure for Java classes using distributional information about how\nthey are used in software, which we combine with corpus statistics on the\ndistribution of contexts in which the classes appear in text. Using our\napproach, cross-validation accuracy on this dataset can be improved\ndramatically, from around 60% to 88%. Human labeling results show that our\nclassifier has an F1 score of 86% over the top 1000 predicted pairs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 20:40:00 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Movshovitz-Attias", "Dana", ""], ["Cohen", "William W.", ""]]}, {"id": "1505.00468", "submitter": "Aishwarya Agrawal", "authors": "Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C.\n  Lawrence Zitnick, Dhruv Batra, Devi Parikh", "title": "VQA: Visual Question Answering", "comments": "The first three authors contributed equally. International Conference\n  on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the task of free-form and open-ended Visual Question Answering\n(VQA). Given an image and a natural language question about the image, the task\nis to provide an accurate natural language answer. Mirroring real-world\nscenarios, such as helping the visually impaired, both the questions and\nanswers are open-ended. Visual questions selectively target different areas of\nan image, including background details and underlying context. As a result, a\nsystem that succeeds at VQA typically needs a more detailed understanding of\nthe image and complex reasoning than a system producing generic image captions.\nMoreover, VQA is amenable to automatic evaluation, since many open-ended\nanswers contain only a few words or a closed set of answers that can be\nprovided in a multiple-choice format. We provide a dataset containing ~0.25M\nimages, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the\ninformation it provides. Numerous baselines and methods for VQA are provided\nand compared with human performance. Our VQA demo is available on CloudCV\n(http://cloudcv.org/vqa).\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 20:07:39 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 16:59:52 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2015 02:47:20 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2015 16:43:33 GMT"}, {"version": "v5", "created": "Mon, 7 Mar 2016 20:55:28 GMT"}, {"version": "v6", "created": "Wed, 20 Apr 2016 03:09:33 GMT"}, {"version": "v7", "created": "Thu, 27 Oct 2016 03:50:19 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Agrawal", "Aishwarya", ""], ["Lu", "Jiasen", ""], ["Antol", "Stanislaw", ""], ["Mitchell", "Margaret", ""], ["Zitnick", "C. Lawrence", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1505.00863", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Yao Meng, Zhongguang Zheng, Jun Sun", "title": "A Feature-based Classification Technique for Answering Multi-choice\n  World History Questions", "comments": "5 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.\nIn this paper, we describe our system for solving real-world university\nentrance exam questions, which are related to world history. Wikipedia is used\nas the main external resource for our system. Since problems with choosing\nright/wrong sentence from multiple sentence choices account for about\ntwo-thirds of the total, we individually design a classification based model\nfor solving this type of questions. For other types of questions, we also\ndesign some simple methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:06:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Song", "Shuangyong", ""], ["Meng", "Yao", ""], ["Zheng", "Zhongguang", ""], ["Sun", "Jun", ""]]}, {"id": "1505.01072", "submitter": "Arun Maiya", "authors": "Arun S. Maiya, Dale Visser, Andrew Wan", "title": "Mining Measured Information from Text", "comments": "4 pages; 38th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval (SIGIR '15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to extract measured information from text (e.g., a\n1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Such\nextractions are critically important across a wide range of domains -\nespecially those involving search and exploration of scientific and technical\ndocuments. We first propose a rule-based entity extractor to mine measured\nquantities (i.e., a numeric value paired with a measurement unit), which\nsupports a vast and comprehensive set of both common and obscure measurement\nunits. Our method is highly robust and can correctly recover valid measured\nquantities even when significant errors are introduced through the process of\nconverting document formats like PDF to plain text. Next, we describe an\napproach to extracting the properties being measured (e.g., the property \"pixel\npitch\" in the phrase \"a pixel pitch as high as 352 {\\mu}m\"). Finally, we\npresent MQSearch: the realization of a search engine with full support for\nmeasured information.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 16:36:27 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Maiya", "Arun S.", ""], ["Visser", "Dale", ""], ["Wan", "Andrew", ""]]}, {"id": "1505.01121", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Marcus Rohrbach and Mario Fritz", "title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about\n  Images", "comments": "ICCV'15 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Neural-Image-QA, an end-to-end\nformulation to this problem for which all parts are trained jointly. In\ncontrast to previous efforts, we are facing a multi-modal problem where the\nlanguage output (answer) is conditioned on visual and natural language input\n(image and question). Our approach Neural-Image-QA doubles the performance of\nthe previous best approach on this problem. We provide additional insights into\nthe problem by analyzing how much information is contained only in the language\npart for which we provide a new human baseline. To study human consensus, which\nis related to the ambiguities inherent in this challenging task, we propose two\nnovel metrics and collect additional answers which extends the original DAQUAR\ndataset to DAQUAR-Consensus.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 18:39:29 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 08:10:01 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 12:13:20 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Rohrbach", "Marcus", ""], ["Fritz", "Mario", ""]]}, {"id": "1505.01393", "submitter": "Philipp Mayr", "authors": "Iana Atanassova, Marc Bertin, Philipp Mayr", "title": "Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of\n  Methods and Tools", "comments": "2 pages, paper accepted for the 15th International Society of\n  Scientometrics and Informetrics Conference (ISSI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Open Access movement in scientific publishing and search engines like\nGoogle Scholar have made scientific articles more broadly accessible. During\nthe last decade, the availability of scientific papers in full text has become\nmore and more widespread thanks to the growing number of publications on online\nplatforms such as ArXiv and CiteSeer. The efforts to provide articles in\nmachine-readable formats and the rise of Open Access publishing have resulted\nin a number of standardized formats for scientific papers (such as NLM-JATS,\nTEI, DocBook). Our aim is to stimulate research at the intersection of\nBibliometrics and Computational Linguistics in order to study the ways\nBibliometrics can benefit from large-scale text analytics and sense mining of\nscientific papers, thus exploring the interdisciplinarity of Bibliometrics and\nNatural Language Processing.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 15:18:04 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Atanassova", "Iana", ""], ["Bertin", "Marc", ""], ["Mayr", "Philipp", ""]]}, {"id": "1505.01504", "submitter": "Hui Jiang", "authors": "Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai", "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its\n  Application to Neural Network Language Models", "comments": "7 pages, 4 figures, Technical report (A shorter version will appear\n  in ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the new fixed-size ordinally-forgetting encoding\n(FOFE) method, which can almost uniquely encode any variable-length sequence of\nwords into a fixed-size representation. FOFE can model the word order in a\nsequence using a simple ordinally-forgetting mechanism according to the\npositions of words. In this work, we have applied FOFE to feedforward neural\nnetwork language models (FNN-LMs). Experimental results have shown that without\nusing any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform\nnot only the standard fixed-input FNN-LMs but also the popular RNN-LMs.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 20:14:25 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 18:41:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Zhang", "Shiliang", ""], ["Jiang", "Hui", ""], ["Xu", "Mingbin", ""], ["Hou", "Junfeng", ""], ["Dai", "Lirong", ""]]}, {"id": "1505.01757", "submitter": "Kazem Taghva", "authors": "Kazem Taghva", "title": "Contextual Analysis for Middle Eastern Languages with Hidden Markov\n  Models", "comments": null, "journal-ref": "International Journal on Natural Language Computing, vol. 4, No.\n  4, August 2015, pp. 1-11", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying a document in Middle Eastern languages requires contextual\nanalysis due to different presentational forms for each character of the\nalphabet. The words of the document will be formed by the joining of the\ncorrect positional glyphs representing corresponding presentational forms of\nthe characters. A set of rules defines the joining of the glyphs. As usual,\nthese rules vary from language to language and are subject to interpretation by\nthe software developers.\n  In this paper, we propose a machine learning approach for contextual analysis\nbased on the first order Hidden Markov Model. We will design and build a model\nfor the Farsi language to exhibit this technology. The Farsi model achieves 94\n\\% accuracy with the training based on a short list of 89 Farsi vocabularies\nconsisting of 2780 Farsi characters.\n  The experiment can be easily extended to many languages including Arabic,\nUrdu, and Sindhi. Furthermore, the advantage of this approach is that the same\nsoftware can be used to perform contextual analysis without coding complex\nrules for each specific language. Of particular interest is that the languages\nwith fewer speakers can have greater representation on the web, since they are\ntypically ignored by software developers due to lack of financial incentives.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 16:03:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Taghva", "Kazem", ""]]}, {"id": "1505.01809", "submitter": "Jacob Devlin", "authors": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong\n  He, Geoffrey Zweig, Margaret Mitchell", "title": "Language Models for Image Captioning: The Quirks and What Works", "comments": "See http://research.microsoft.com/en-us/projects/image_captioning for\n  project information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recent approaches have achieved state-of-the-art results in image\ncaptioning. The first uses a pipelined process where a set of candidate words\nis generated by a convolutional neural network (CNN) trained on images, and\nthen a maximum entropy (ME) language model is used to arrange these words into\na coherent sentence. The second uses the penultimate activation layer of the\nCNN as input to a recurrent neural network (RNN) that then generates the\ncaption sequence. In this paper, we compare the merits of these different\nlanguage modeling approaches for the first time by using the same\nstate-of-the-art CNN as input. We examine issues in the different approaches,\nincluding linguistic irregularities, caption repetition, and data set overlap.\nBy combining key aspects of the ME and RNN methods, we achieve a new record\nperformance over previously published results on the benchmark COCO dataset.\nHowever, the gains we see in BLEU do not translate to human judgments.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 18:36:14 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 22:10:49 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 22:03:40 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Devlin", "Jacob", ""], ["Cheng", "Hao", ""], ["Fang", "Hao", ""], ["Gupta", "Saurabh", ""], ["Deng", "Li", ""], ["He", "Xiaodong", ""], ["Zweig", "Geoffrey", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1505.02074", "submitter": "Mengye Ren", "authors": "Mengye Ren, Ryan Kiros, Richard Zemel", "title": "Exploring Models and Data for Image Question Answering", "comments": "12 pages. Conference paper at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to address the problem of image-based question-answering (QA)\nwith new models and datasets. In our work, we propose to use neural networks\nand visual semantic embeddings, without intermediate stages such as object\ndetection and image segmentation, to predict answers to simple questions about\nimages. Our model performs 1.8 times better than the only published results on\nan existing image QA dataset. We also present a question generation algorithm\nthat converts image descriptions, which are widely available, into QA form. We\nused this algorithm to produce an order-of-magnitude larger dataset, with more\nevenly distributed answers. A suite of baseline results on this new dataset are\nalso presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 15:59:44 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 19:55:07 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 06:44:44 GMT"}, {"version": "v4", "created": "Sun, 29 Nov 2015 22:45:12 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Ren", "Mengye", ""], ["Kiros", "Ryan", ""], ["Zemel", "Richard", ""]]}, {"id": "1505.02251", "submitter": "Aris Kosmopoulos", "authors": "Aris Kosmopoulos and Georgios Paliouras and Ion Androutsopoulos", "title": "Probabilistic Cascading for Large Scale Hierarchical Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies are frequently used for the organization of objects. Given a\nhierarchy of classes, two main approaches are used, to automatically classify\nnew instances: flat classification and cascade classification. Flat\nclassification ignores the hierarchy, while cascade classification greedily\ntraverses the hierarchy from the root to the predicted leaf. In this paper we\npropose a new approach, which extends cascade classification to predict the\nright leaf by estimating the probability of each root-to-leaf path. We provide\nexperimental results which indicate that, using the same classification\nalgorithm, one can achieve better results with our approach, compared to the\ntraditional flat and cascade classifications.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:39:04 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Kosmopoulos", "Aris", ""], ["Paliouras", "Georgios", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1505.02419", "submitter": "Mo Yu", "authors": "Matthew R. Gormley and Mo Yu and Mark Dredze", "title": "Improved Relation Extraction with Feature-Rich Compositional Embedding\n  Models", "comments": "12 pages for EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional embedding models build a representation (or embedding) for a\nlinguistic structure based on its component word embeddings. We propose a\nFeature-rich Compositional Embedding Model (FCM) for relation extraction that\nis expressive, generalizes to new domains, and is easy-to-implement. The key\nidea is to combine both (unlexicalized) hand-crafted features with learned word\nembeddings. The model is able to directly tackle the difficulties met by\ntraditional compositional embeddings models, such as handling arbitrary types\nof sentence annotations and utilizing global information for composition. We\ntest the proposed model on two relation extraction tasks, and demonstrate that\nour model outperforms both previous compositional models and traditional\nfeature rich models on the ACE 2005 relation extraction task, and the SemEval\n2010 relation classification task. The combination of our model and a\nlog-linear classifier with hand-crafted features gives state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 18:47:06 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 01:01:34 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 02:01:34 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Gormley", "Matthew R.", ""], ["Yu", "Mo", ""], ["Dredze", "Mark", ""]]}, {"id": "1505.02425", "submitter": "Michael Heilman", "authors": "Michael Heilman, Kenji Sagae", "title": "Fast Rhetorical Structure Theory Discourse Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, There has been a variety of research on discourse parsing,\nparticularly RST discourse parsing. Most of the recent work on RST parsing has\nfocused on implementing new types of features or learning algorithms in order\nto improve accuracy, with relatively little focus on efficiency, robustness, or\npractical use. Also, most implementations are not widely available. Here, we\ndescribe an RST segmentation and parsing system that adapts models and feature\nsets from various previous work, as described below. Its accuracy is near\nstate-of-the-art, and it was developed to be fast, robust, and practical. For\nexample, it can process short documents such as news articles or essays in less\nthan a second.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 19:26:31 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Heilman", "Michael", ""], ["Sagae", "Kenji", ""]]}, {"id": "1505.02973", "submitter": "Konstantinos Tserpes", "authors": "Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis\n  Anagnostopoulos, Theodora Varvarigou", "title": "Comparing methods for Twitter Sentiment Analysis", "comments": "5 pages, 1 figure, 6th Conference on Knowledge Discovery and\n  Information Retrieval 2014, Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends the set of works which deal with the popular problem of\nsentiment analysis in Twitter. It investigates the most popular document\n(\"tweet\") representation methods which feed sentiment evaluation mechanisms. In\nparticular, we study the bag-of-words, n-grams and n-gram graphs approaches and\nfor each of them we evaluate the performance of a lexicon-based and 7\nlearning-based classification algorithms (namely SVM, Na\\\"ive Bayesian\nNetworks, Logistic Regression, Multilayer Perceptrons, Best-First Trees,\nFunctional Trees and C4.5) as well as their combinations, using a set of 4451\nmanually annotated tweets. The results demonstrate the superiority of\nlearning-based methods and in particular of n-gram graphs approaches for\npredicting the sentiment of tweets. They also show that the combinatory\napproach has impressive effects on n-grams, raising the confidence up to 83.15%\non the 5-Grams, using majority vote and a balanced dataset (equal number of\npositive, negative and neutral tweets for training). In the n-gram graph cases\nthe improvement was small to none, reaching 94.52% on the 4-gram graphs, using\nOrthodromic distance and a threshold of 0.001.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 12:05:19 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Psomakelis", "Evangelos", ""], ["Tserpes", "Konstantinos", ""], ["Anagnostopoulos", "Dimosthenis", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1505.03081", "submitter": "AbdelRahim Elmadany", "authors": "AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith", "title": "Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and\n  Instance Messages", "comments": null, "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  4, No.2,April 2015", "doi": "10.5121/ijnlc.2015.4208", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text segmentation task is an essential processing task for many of Natural\nLanguage Processing (NLP) such as text summarization, text translation,\ndialogue language understanding, among others. Turns segmentation considered\nthe key player in dialogue understanding task for building automatic\nHuman-Computer systems. In this paper, we introduce a novel approach to turn\nsegmentation into utterances for Egyptian spontaneous dialogues and Instance\nMessages (IM) using Machine Learning (ML) approach as a part of automatic\nunderstanding Egyptian spontaneous dialogues and IM task. Due to the lack of\nEgyptian dialect dialogue corpus the system evaluated by our corpus includes\n3001 turns, which are collected, segmented, and annotated manually from\nEgyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of\n95.98%.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 16:33:03 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Elmadany", "AbdelRahim A.", ""], ["Abdou", "Sherif M.", ""], ["Gheith", "Mervat", ""]]}, {"id": "1505.03084", "submitter": "AbdelRahim Elmadany", "authors": "AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith", "title": "A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and\n  Instant Message", "comments": null, "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  4, No.2,April 2015", "doi": "10.5121/ijnlc.2015.4206", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building dialogues systems interaction has recently gained considerable\nattention, but most of the resources and systems built so far are tailored to\nEnglish and other Indo-European languages. The need for designing systems for\nother languages is increasing such as Arabic language. For this reasons, there\nare more interest for Arabic dialogue acts classification task because it a key\nplayer in Arabic language understanding to building this systems. This paper\nsurveys different techniques for dialogue acts classification for Arabic. We\ndescribe the main existing techniques for utterances segmentations and\nclassification, annotation schemas, and test corpora for Arabic dialogues\nunderstanding that have introduced in the literature\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 16:38:39 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Elmadany", "AbdelRahim A.", ""], ["Abdou", "Sherif M.", ""], ["Gheith", "Mervat", ""]]}, {"id": "1505.03085", "submitter": "Edwin Lunando", "authors": "Edwin Lunando and Ayu Purwarianti", "title": "Indonesian Social Media Sentiment Analysis With Sarcasm Detection", "comments": "4 pages; 3 figures", "journal-ref": null, "doi": "10.1109/ICACSIS.2013.6761575", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sarcasm is considered one of the most difficult problem in sentiment\nanalysis. In our ob-servation on Indonesian social media, for cer-tain topics,\npeople tend to criticize something using sarcasm. Here, we proposed two\nadditional features to detect sarcasm after a common sentiment analysis is\nconducted. The features are the negativity information and the number of\ninterjection words. We also employed translated SentiWordNet in the sentiment\nclassification. All the classifications were conducted with machine learning\nalgorithms. The experimental results showed that the additional features are\nquite effective in the sarcasm detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 16:45:52 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Lunando", "Edwin", ""], ["Purwarianti", "Ayu", ""]]}, {"id": "1505.03105", "submitter": "Hossam Ibrahim", "authors": "Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith", "title": "Sentiment Analysis For Modern Standard Arabic And Colloquial", "comments": "International Journal on Natural Language Computing (IJNLC) Vol. 4,\n  No.2,April 2015", "journal-ref": null, "doi": "10.5121/ijnlc.2015.4207", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of social media such as blogs and social networks has fueled\ninterest in sentiment analysis. With the proliferation of reviews, ratings,\nrecommendations and other forms of online expression, online opinion has turned\ninto a kind of virtual currency for businesses looking to market their\nproducts, identify new opportunities and manage their reputations, therefore\nmany are now looking to the field of sentiment analysis. In this paper, we\npresent a feature-based sentence level approach for Arabic sentiment analysis.\nOur approach is using Arabic idioms/saying phrases lexicon as a key importance\nfor improving the detection of the sentiment polarity in Arabic sentences as\nwell as a number of novels and rich set of linguistically motivated features\ncontextual Intensifiers, contextual Shifter and negation handling), syntactic\nfeatures for conflicting phrases which enhance the sentiment classification\naccuracy. Furthermore, we introduce an automatic expandable wide coverage\npolarity lexicon of Arabic sentiment words. The lexicon is built with\ngold-standard sentiment words as a seed which is manually collected and\nannotated and it expands and detects the sentiment orientation automatically of\nnew sentiment words using synset aggregation technique and free online Arabic\nlexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and\nEgyptian dialectal Arabic tweets and microblogs (hotel reservation, product\nreviews, etc.). The experimental results using our resources and techniques\nwith SVM classifier indicate high performance levels, with accuracies of over\n95%.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 18:10:53 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Ibrahim", "Hossam S.", ""], ["Abdou", "Sherif M.", ""], ["Gheith", "Mervat", ""]]}, {"id": "1505.03239", "submitter": "Sarika Hegde", "authors": "Sarika Hegde, K. K. Achary, Surendra Shetty", "title": "Feature selection using Fisher's ratio technique for automatic speech\n  recognition", "comments": "in International Journal on Cybernetics & Informatics (IJCI) Vol. 4,\n  No. 2, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Speech Recognition involves mainly two steps; feature extraction\nand classification . Mel Frequency Cepstral Coefficient is used as one of the\nprominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC\ncoefficients is used as the feature vector in the classification step. But the\nquestion is whether the same or improved classification accuracy can be\nachieved by using a subset of 12 MFCC as feature vector. In this paper,\nFisher's ratio technique is used for selecting a subset of 12 MFCC coefficients\nthat contribute more in discriminating a pattern. The selected coefficients are\nused in classification with Hidden Markov Model algorithm. The classification\naccuracies that we get by using 12 coefficients and by using the selected\ncoefficients are compared.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 04:50:27 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Hegde", "Sarika", ""], ["Achary", "K. K.", ""], ["Shetty", "Surendra", ""]]}, {"id": "1505.03783", "submitter": "Carlos Pineda", "authors": "Germinal Cocho, Jorge Flores, Carlos Gershenson, Carlos Pineda, Sergio\n  S\\'anchez", "title": "Rank diversity of languages: Generic behavior in computational\n  linguistics", "comments": null, "journal-ref": "PLoS ONE 10(4): e0121898 (2015)", "doi": "10.1371/journal.pone.0121898", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical studies of languages have focused on the rank-frequency\ndistribution of words. Instead, we introduce here a measure of how word ranks\nchange in time and call this distribution \\emph{rank diversity}. We calculate\nthis diversity for books published in six European languages since 1800, and\nfind that it follows a universal lognormal distribution. Based on the mean and\nstandard deviation associated with the lognormal distribution, we define three\ndifferent word regimes of languages: \"heads\" consist of words which almost do\nnot change their rank in time, \"bodies\" are words of general use, while \"tails\"\nare comprised by context-specific words and vary their rank considerably in\ntime. The heads and bodies reflect the size of language cores identified by\nlinguists for basic communication. We propose a Gaussian random walk model\nwhich reproduces the rank variation of words in time and thus the diversity.\nRank diversity of words can be understood as the result of random variations in\nrank, where the size of the variation depends on the rank itself. We find that\nthe core size is similar for all languages studied.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 16:21:02 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Cocho", "Germinal", ""], ["Flores", "Jorge", ""], ["Gershenson", "Carlos", ""], ["Pineda", "Carlos", ""], ["S\u00e1nchez", "Sergio", ""]]}, {"id": "1505.03823", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou and Thomas Fang Zheng", "title": "Distant Supervision for Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking is an indispensable operation of populating knowledge\nrepositories for information extraction. It studies on aligning a textual\nentity mention to its corresponding disambiguated entry in a knowledge\nrepository. In this paper, we propose a new paradigm named distantly supervised\nentity linking (DSEL), in the sense that the disambiguated entities that belong\nto a huge knowledge repository (Freebase) are automatically aligned to the\ncorresponding descriptive webpages (Wiki pages). In this way, a large scale of\nweakly labeled data can be generated without manual annotation and fed to a\nclassifier for linking more newly discovered entities. Compared with\ntraditional paradigms based on solo knowledge base, DSEL benefits more via\njointly leveraging the respective advantages of Freebase and Wikipedia.\nSpecifically, the proposed paradigm facilitates bridging the disambiguated\nlabels (Freebase) of entities and their textual descriptions (Wikipedia) for\nWeb-scale entities. Experiments conducted on a dataset of 140,000 items and\n60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze\nthe feature performance and improve the F1-measure to 0.545.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 18:15:49 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 14:45:19 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2015 01:25:26 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1505.04197", "submitter": "AbdelRahim Elmadany", "authors": "AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith", "title": "Arabic Inquiry-Answer Dialogue Acts Annotation Schema", "comments": "IOSR Journal of Engineering (IOSRJEN),Vol. 04, Issue 12 (December\n  2014),V2. arXiv admin note: text overlap with arXiv:1505.03084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an annotation schema as part of an effort to create a manually\nannotated corpus for Arabic dialogue language understanding including spoken\ndialogue and written \"chat\" dialogue for inquiry-answer domain. The proposed\nschema handles mainly the request and response acts that occurs frequently in\ninquiry-answer debate conversations expressing request services, suggests, and\noffers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 20:13:16 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Elmadany", "AbdelRahim A.", ""], ["Abdou", "Sherif M.", ""], ["Gheith", "Mervat", ""]]}, {"id": "1505.04313", "submitter": "Erkki Luuk", "authors": "Erkki Luuk", "title": "A type-theoretical approach to Universal Grammar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of Universal Grammar (UG) as the hypothetical linguistic structure\nshared by all human languages harkens back at least to the 13th century. The\nbest known modern elaborations of the idea are due to Chomsky. Following a\ndevastating critique from theoretical, typological and field linguistics, these\nelaborations, the idea of UG itself and the more general idea of language\nuniversals stand untenable and are largely abandoned. The proposal tackles the\nhypothetical contents of UG using dependent and polymorphic type theory in a\nframework very different from the Chomskyan ones. We introduce a type logic for\na precise, universal and parsimonious representation of natural language\nmorphosyntax and compositional semantics. The logic handles grammatical\nambiguity (with polymorphic types), selectional restrictions and diverse kinds\nof anaphora (with dependent types), and features a partly universal set of\nmorphosyntactic types (by the Curry-Howard isomorphism).\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 19:28:49 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Luuk", "Erkki", ""]]}, {"id": "1505.04342", "submitter": "Eric Clark Mr.", "authors": "Eric M. Clark, Jake Ryland Williams, Chris A. Jones, Richard A.\n  Galbraith, Christopher M. Danforth, Peter Sheridan Dodds", "title": "Sifting Robotic from Organic Text: A Natural Language Approach for\n  Detecting Automation on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter, a popular social media outlet, has evolved into a vast source of\nlinguistic data, rich with opinion, sentiment, and discussion. Due to the\nincreasing popularity of Twitter, its perceived potential for exerting social\ninfluence has led to the rise of a diverse community of automatons, commonly\nreferred to as bots. These inorganic and semi-organic Twitter entities can\nrange from the benevolent (e.g., weather-update bots, help-wanted-alert bots)\nto the malevolent (e.g., spamming messages, advertisements, or radical\nopinions). Existing detection algorithms typically leverage meta-data (time\nbetween tweets, number of followers, etc.) to identify robotic accounts. Here,\nwe present a powerful classification scheme that exclusively uses the natural\nlanguage text from organic users to provide a criterion for identifying\naccounts posting automated messages. Since the classifier operates on text\nalone, it is flexible and may be applied to any textual data beyond the\nTwitter-sphere.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 01:22:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 13:32:43 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2015 16:14:38 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2015 23:02:05 GMT"}, {"version": "v5", "created": "Wed, 24 Feb 2016 07:42:59 GMT"}, {"version": "v6", "created": "Tue, 14 Jun 2016 13:44:56 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Clark", "Eric M.", ""], ["Williams", "Jake Ryland", ""], ["Jones", "Chris A.", ""], ["Galbraith", "Richard A.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""]]}, {"id": "1505.04420", "submitter": "Miryam de Lhoneux", "authors": "Miryam de Lhoneux", "title": "CCG Parsing and Multiword Expressions", "comments": "MSc thesis, The University of Edinburgh, 2014, School of Informatics,\n  MSc Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents a study about the integration of information about\nMultiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar\n(CCG). We build on previous work which has shown the benefit of adding\ninformation about MWEs to syntactic parsing by implementing a similar pipeline\nwith CCG parsing. More specifically, we collapse MWEs to one token in training\nand test data in CCGbank, a corpus which contains sentences annotated with CCG\nderivations. Our collapsing algorithm however can only deal with MWEs when they\nform a constituent in the data which is one of the limitations of our approach.\n  We study the effect of collapsing training and test data. A parsing effect\ncan be obtained if collapsed data help the parser in its decisions and a\ntraining effect can be obtained if training on the collapsed data improves\nresults. We also collapse the gold standard and show that our model\nsignificantly outperforms the baseline model on our gold standard, which\nindicates that there is a training effect. We show that the baseline model\nperforms significantly better on our gold standard when the data are collapsed\nbefore parsing than when the data are collapsed after parsing which indicates\nthat there is a parsing effect. We show that these results can lead to improved\nperformance on the non-collapsed standard benchmark although we fail to show\nthat it does so significantly. We conclude that despite the limited settings,\nthere are noticeable improvements from using MWEs in parsing. We discuss ways\nin which the incorporation of MWEs into parsing can be improved and hypothesize\nthat this will lead to more substantial results.\n  We finally show that turning the MWE recognition part of the pipeline into an\nexperimental part is a useful thing to do as we obtain different results with\ndifferent recognizers.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 17:26:36 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["de Lhoneux", "Miryam", ""]]}, {"id": "1505.04630", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang and Zhiyong Zhang", "title": "Recurrent Neural Network Training with Dark Knowledge Transfer", "comments": "ICASSP 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472809", "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),\nhave gained much attention in automatic speech recognition (ASR). Although some\nsuccessful stories have been reported, training RNNs remains highly\nchallenging, especially with limited training data. Recent research found that\na well-trained model can be used as a teacher to train other child models, by\nusing the predictions generated by the teacher model as supervision. This\nknowledge transfer learning has been employed to train simple neural nets with\na complex one, so that the final performance can reach a level that is\ninfeasible to obtain by regular training. In this paper, we employ the\nknowledge transfer learning approach to train RNNs (precisely LSTM) using a\ndeep neural network (DNN) model as the teacher. This is different from most of\nthe existing research on knowledge transfer learning, since the teacher (DNN)\nis assumed to be weaker than the child (RNN); however, our experiments on an\nASR task showed that it works fairly well: without applying any tricks on the\nlearning scheme, this approach can train RNNs successfully even with limited\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 13:26:02 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:58:15 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 03:31:51 GMT"}, {"version": "v4", "created": "Sat, 12 Mar 2016 07:51:36 GMT"}, {"version": "v5", "created": "Sun, 8 May 2016 12:40:35 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1505.04657", "submitter": "Phong Minh Vu", "authors": "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen", "title": "Mining User Opinions in Mobile App Reviews: A Keyword-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User reviews of mobile apps often contain complaints or suggestions which are\nvaluable for app developers to improve user experience and satisfaction.\nHowever, due to the large volume and noisy-nature of those reviews, manually\nanalyzing them for useful opinions is inherently challenging. To address this\nproblem, we propose MARK, a keyword-based framework for semi-automated review\nanalysis. MARK allows an analyst describing his interests in one or some mobile\napps by a set of keywords. It then finds and lists the reviews most relevant to\nthose keywords for further analysis. It can also draw the trends over time of\nthose keywords and detect their sudden changes, which might indicate the\noccurrences of serious issues. To help analysts describe their interests more\neffectively, MARK can automatically extract keywords from raw reviews and rank\nthem by their associations with negative reviews. In addition, based on a\nvector-based semantic representation of keywords, MARK can divide a large set\nof keywords into more cohesive subsets, or suggest keywords similar to the\nselected ones.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 14:25:03 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 02:23:32 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Vu", "Phong Minh", ""], ["Nguyen", "Tam The", ""], ["Pham", "Hung Viet", ""], ["Nguyen", "Tung Thanh", ""]]}, {"id": "1505.04771", "submitter": "Eric Malmi", "authors": "Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides\n  Gionis", "title": "DopeLearning: A Computational Approach to Rap Lyrics Generation", "comments": "This is a pre-print of an article appearing at KDD'16", "journal-ref": null, "doi": "10.1145/2939672.2939679", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing rap lyrics requires both creativity to construct a meaningful,\ninteresting story and lyrical skills to produce complex rhyme patterns, which\nform the cornerstone of good flow. We present a rap lyrics generation method\nthat captures both of these aspects. First, we develop a prediction model to\nidentify the next line of existing lyrics from a set of candidate next lines.\nThis model is based on two machine-learning techniques: the RankSVM algorithm\nand a deep neural network model with a novel structure. Results show that the\nprediction model can identify the true next line among 299 randomly selected\nlines with an accuracy of 17%, i.e., over 50 times more likely than by random.\nSecond, we employ the prediction model to combine lines from existing songs,\nproducing lyrics with rhyme and a meaning. An evaluation of the produced lyrics\nshows that in terms of quantitative rhyme density, the method outperforms the\nbest human rappers by 21%. The rap lyrics generator has been deployed as an\nonline tool called DeepBeat, and the performance of the tool has been assessed\nby analyzing its usage logs. This analysis shows that machine-learned rankings\ncorrelate with user preferences.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:35:21 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 20:51:02 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Malmi", "Eric", ""], ["Takala", "Pyry", ""], ["Toivonen", "Hannu", ""], ["Raiko", "Tapani", ""], ["Gionis", "Aristides", ""]]}, {"id": "1505.04870", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo,\n  Julia Hockenmaier, and Svetlana Lazebnik", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for\n  Richer Image-to-Sentence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Flickr30k dataset has become a standard benchmark for sentence-based\nimage description. This paper presents Flickr30k Entities, which augments the\n158k captions from Flickr30k with 244k coreference chains, linking mentions of\nthe same entities across different captions for the same image, and associating\nthem with 276k manually annotated bounding boxes. Such annotations are\nessential for continued progress in automatic image description and grounded\nlanguage understanding. They enable us to define a new benchmark for\nlocalization of textual entity mentions in an image. We present a strong\nbaseline for this task that combines an image-text embedding, detectors for\ncommon objects, a color classifier, and a bias towards selecting larger\nobjects. While our baseline rivals in accuracy more complex state-of-the-art\nmodels, we show that its gains cannot be easily parlayed into improvements on\nsuch tasks as image-sentence retrieval, thus underlining the limitations of\ncurrent methods and the need for further research.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 04:46:03 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 22:17:45 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2016 14:58:37 GMT"}, {"version": "v4", "created": "Mon, 19 Sep 2016 20:20:42 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Wang", "Liwei", ""], ["Cervantes", "Chris M.", ""], ["Caicedo", "Juan C.", ""], ["Hockenmaier", "Julia", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1505.04891", "submitter": "Fei Tian", "authors": "Fei Tian, Bin Gao, Enhong Chen, Tie-Yan Liu", "title": "Learning Better Word Embedding by Asymmetric Low-Rank Projection of\n  Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding, which refers to low-dimensional dense vector representations\nof natural words, has demonstrated its power in many natural language\nprocessing tasks. However, it may suffer from the inaccurate and incomplete\ninformation contained in the free text corpus as training data. To tackle this\nchallenge, there have been quite a few works that leverage knowledge graphs as\nan additional information source to improve the quality of word embedding.\nAlthough these works have achieved certain success, they have neglected some\nimportant facts about knowledge graphs: (i) many relationships in knowledge\ngraphs are \\emph{many-to-one}, \\emph{one-to-many} or even \\emph{many-to-many},\nrather than simply \\emph{one-to-one}; (ii) most head entities and tail entities\nin knowledge graphs come from very different semantic spaces. To address these\nissues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet\nmodels the relationships between head and tail entities after transforming them\nwith different low-rank projection matrices. The low-rank projection can allow\nnon \\emph{one-to-one} relationships between entities, while different\nprojection matrices for head and tail entities allow them to originate in\ndifferent semantic spaces. The experimental results demonstrate that ProjectNet\nyields more accurate word embedding than previous works, thus leads to clear\nimprovements in various natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 07:08:10 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 08:21:24 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Tian", "Fei", ""], ["Gao", "Bin", ""], ["Chen", "Enhong", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1505.05008", "submitter": "Cicero dos Santos", "authors": "Cicero Nogueira dos Santos, Victor Guimar\\~aes", "title": "Boosting Named Entity Recognition with Neural Character Embeddings", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art named entity recognition (NER) systems rely on\nhandcrafted features and on the output of other NLP tasks such as\npart-of-speech (POS) tagging and text chunking. In this work we propose a\nlanguage-independent NER system that uses automatically learned features only.\nOur approach is based on the CharWNN deep neural network, which uses word-level\nand character-level representations (embeddings) to perform sequential\nclassification. We perform an extensive number of experiments using two\nannotated corpora in two different languages: HAREM I corpus, which contains\ntexts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in\nSpanish. Our experimental results shade light on the contribution of neural\ncharacter embeddings for NER. Moreover, we demonstrate that the same neural\nnetwork which has been successfully applied to POS tagging can also achieve\nstate-of-the-art results for language-independet NER, using the same\nhyperparameters, and without any handcrafted features. For the HAREM I corpus,\nCharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score\nfor the total scenario (ten NE classes), and by 7.2 points in the F1 for the\nselective scenario (five NE classes).\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:21:37 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 11:35:32 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Guimar\u00e3es", "Victor", ""]]}, {"id": "1505.05253", "submitter": "Jun Feng", "authors": "Jun Feng, Mantong Zhou, Yu Hao, Minlie Huang and Xiaoyan Zhu", "title": "Knowlege Graph Embedding by Flexible Translation", "comments": "This paper has been withdraw by the author due to an error in sec3.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding refers to projecting entities and relations in\nknowledge graph into continuous vector spaces. State-of-the-art methods, such\nas TransE, TransH, and TransR build embeddings by treating relation as\ntranslation from head entity to tail entity. However, previous models can not\ndeal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or\nlack of scalability and efficiency. Thus, we propose a novel method, flexible\ntranslation, named TransF, to address the above issues. TransF regards relation\nas translation between head entity vector and tail entity vector with flexible\nmagnitude. To evaluate the proposed model, we conduct link prediction and\ntriple classification on benchmark datasets. Experimental results show that our\nmethod remarkably improve the performance compared with several\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 05:57:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 03:48:55 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Feng", "Jun", ""], ["Zhou", "Mantong", ""], ["Hao", "Yu", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1505.05612", "submitter": "Junhua Mao", "authors": "Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image\n  Question Answering", "comments": "Dataset released on the project page, see\n  http://idl.baidu.com/FM-IQA.html ; NIPS 2015 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the mQA model, which is able to answer questions\nabout the content of an image. The answer can be a sentence, a phrase or a\nsingle word. Our model contains four components: a Long Short-Term Memory\n(LSTM) to extract the question representation, a Convolutional Neural Network\n(CNN) to extract the visual representation, an LSTM for storing the linguistic\ncontext in an answer, and a fusing component to combine the information from\nthe first three components and generate the answer. We construct a Freestyle\nMultilingual Image Question Answering (FM-IQA) dataset to train and evaluate\nour mQA model. It contains over 150,000 images and 310,000 freestyle Chinese\nquestion-answer pairs and their English translations. The quality of the\ngenerated answers of our mQA model on this dataset is evaluated by human judges\nthrough a Turing Test. Specifically, we mix the answers provided by humans and\nour model. The human judges need to distinguish our model from the human. They\nwill also provide a score (i.e. 0, 1, 2, the larger the better) indicating the\nquality of the answer. We propose strategies to monitor the quality of this\nevaluation process. The experiments show that in 64.7% of cases, the human\njudges cannot distinguish our model from humans. The average score is 1.454\n(1.918 for human). The details of this work, including the FM-IQA dataset, can\nbe found on the project page: http://idl.baidu.com/FM-IQA.html\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:09:36 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 07:45:46 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2015 21:12:15 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Gao", "Haoyuan", ""], ["Mao", "Junhua", ""], ["Zhou", "Jie", ""], ["Huang", "Zhiheng", ""], ["Wang", "Lei", ""], ["Xu", "Wei", ""]]}, {"id": "1505.05667", "submitter": "Xipeng Qiu", "authors": "Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang", "title": "A Re-ranking Model for Dependency Parser with Recursive Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem to model all the nodes (words or\nphrases) in a dependency tree with the dense representations. We propose a\nrecursive convolutional neural network (RCNN) architecture to capture syntactic\nand compositional-semantic representations of phrases and words in a dependency\ntree. Different with the original recursive neural network, we introduce the\nconvolution and pooling layers, which can model a variety of compositions by\nthe feature maps and choose the most informative compositions by the pooling\nlayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list\nof candidate dependency parsing trees. The experiments show that RCNN is very\neffective to improve the state-of-the-art dependency parsing on both English\nand Chinese datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 10:23:10 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zhu", "Chenxi", ""], ["Qiu", "Xipeng", ""], ["Chen", "Xinchi", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1505.05841", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and Benjamin Strauss", "title": "Translation Memory Retrieval Methods", "comments": "9 pages, 6 tables, 3 figures; appeared in Proceedings of the 14th\n  Conference of the European Chapter of the Association for Computational\n  Linguistics, April 2014", "journal-ref": "In Proceedings of the 14th Conference of the European Chapter of\n  the Association for Computational Linguistics, pages 202-210, Gothenburg,\n  Sweden, April 2014. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translation Memory (TM) systems are one of the most widely used translation\ntechnologies. An important part of TM systems is the matching algorithm that\ndetermines what translations get retrieved from the bank of available\ntranslations to assist the human translator. Although detailed accounts of the\nmatching algorithms used in commercial systems can't be found in the\nliterature, it is widely believed that edit distance algorithms are used. This\npaper investigates and evaluates the use of several matching algorithms,\nincluding the edit distance algorithm that is believed to be at the heart of\nmost modern commercial TM systems. This paper presents results showing how well\nvarious matching algorithms correlate with human judgments of helpfulness\n(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm\nbased on weighted n-gram precision that can be adjusted for translator length\npreferences consistently returns translations judged to be most helpful by\ntranslators for multiple domains and language pairs.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 18:57:34 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Bloodgood", "Michael", ""], ["Strauss", "Benjamin", ""]]}, {"id": "1505.05899", "submitter": "George Saon", "authors": "George Saon, Hong-Kwang J. Kuo, Steven Rennie and Michael Picheny", "title": "The IBM 2015 English Conversational Telephone Speech Recognition System", "comments": "Submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the latest improvements to the IBM English conversational\ntelephone speech recognition system. Some of the techniques that were found\nbeneficial are: maxout networks with annealed dropout rates; networks with a\nvery large number of outputs trained on 2000 hours of data; joint modeling of\npartially unfolded recurrent neural networks and convolutional nets by\ncombining the bottleneck and output layers and retraining the resulting model;\nand lastly, sophisticated language model rescoring with exponential and neural\nnetwork LMs. These techniques result in an 8.0% word error rate on the\nSwitchboard part of the Hub5-2000 evaluation test set which is 23% relative\nbetter than our previous best published result.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 20:49:32 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Saon", "George", ""], ["Kuo", "Hong-Kwang J.", ""], ["Rennie", "Steven", ""], ["Picheny", "Michael", ""]]}, {"id": "1505.06027", "submitter": "Piotr Bojanowski", "authors": "Piotr Bojanowski (WILLOW, LIENS), R\\'emi Lajugie (LIENS, SIERRA),\n  Edouard Grave (APAM), Francis Bach (LIENS, SIERRA), Ivan Laptev (WILLOW,\n  LIENS), Jean Ponce (WILLOW, LIENS), Cordelia Schmid (LEAR)", "title": "Weakly-Supervised Alignment of Video With Text", "comments": "ICCV 2015 - IEEE International Conference on Computer Vision, Dec\n  2015, Santiago, Chile", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given a set of videos, along with natural language\ndescriptions in the form of multiple sentences (e.g., manual annotations, movie\nscripts, sport summaries etc.), and that these sentences appear in the same\ntemporal order as their visual counterparts. We propose in this paper a method\nfor aligning the two modalities, i.e., automatically providing a time stamp for\nevery sentence. Given vectorial features for both video and text, we propose to\ncast this task as a temporal assignment problem, with an implicit linear\nmapping between the two feature modalities. We formulate this problem as an\ninteger quadratic program, and solve its continuous convex relaxation using an\nefficient conditional gradient algorithm. Several rounding procedures are\nproposed to construct the final integer solution. After demonstrating\nsignificant improvements over the state of the art on the related task of\naligning video with symbolic labels [7], we evaluate our method on a\nchallenging dataset of videos with associated textual descriptions [36], using\nboth bag-of-words and continuous representations for text.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 11:08:39 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 14:57:40 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Bojanowski", "Piotr", "", "WILLOW, LIENS"], ["Lajugie", "R\u00e9mi", "", "LIENS, SIERRA"], ["Grave", "Edouard", "", "APAM"], ["Bach", "Francis", "", "LIENS, SIERRA"], ["Laptev", "Ivan", "", "WILLOW,\n  LIENS"], ["Ponce", "Jean", "", "WILLOW, LIENS"], ["Schmid", "Cordelia", "", "LEAR"]]}, {"id": "1505.06169", "submitter": "Emma Strubell", "authors": "Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum", "title": "Learning Dynamic Feature Selection for Fast Sequential Prediction", "comments": "Appears in The 53rd Annual Meeting of the Association for\n  Computational Linguistics, Beijing, China, July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present paired learning and inference algorithms for significantly\nreducing computation and increasing speed of the vector dot products in the\nclassifiers that are at the heart of many NLP components. This is accomplished\nby partitioning the features into a sequence of templates which are ordered\nsuch that high confidence can often be reached using only a small fraction of\nall features. Parameter estimation is arranged to maximize accuracy and early\nconfidence in this sequence. Our approach is simpler and better suited to NLP\nthan other related cascade methods. We present experiments in left-to-right\npart-of-speech tagging, named entity recognition, and transition-based\ndependency parsing. On the typical benchmarking datasets we can preserve POS\ntagging accuracy above 97% and parsing LAS above 88.5% both with over a\nfive-fold reduction in run-time, and NER F1 above 88 with more than 2x increase\nin speed.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 18:28:21 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Strubell", "Emma", ""], ["Vilnis", "Luke", ""], ["Silverstein", "Kate", ""], ["McCallum", "Andrew", ""]]}, {"id": "1505.06228", "submitter": "Fatma Elghannam Rashad", "authors": "Fatma Elghannam, Tarek El-Shishtawy", "title": "Keyphrase Based Evaluation of Automatic Text Summarization", "comments": "4 pages, 1 figure, 3 tables", "journal-ref": "International Journal of Computer Applications 117(7):5-8, May\n  2015. ISBN : 973-93-80886-51-2", "doi": "10.5120/20564-2953", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of methods to deal with the informative contents of the text\nunits in the matching process is a major challenge in automatic summary\nevaluation systems that use fixed n-gram matching. The limitation causes\ninaccurate matching between units in a peer and reference summaries. The\npresent study introduces a new Keyphrase based Summary Evaluator KpEval for\nevaluating automatic summaries. The KpEval relies on the keyphrases since they\nconvey the most important concepts of a text. In the evaluation process, the\nkeyphrases are used in their lemma form as the matching text unit. The system\nwas applied to evaluate different summaries of Arabic multi-document data set\npresented at TAC2011. The results showed that the new evaluation technique\ncorrelates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4,\nand AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG\nMeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 21:12:35 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Elghannam", "Fatma", ""], ["El-Shishtawy", "Tarek", ""]]}, {"id": "1505.06256", "submitter": "Tong Shu Li", "authors": "Tong Shu Li, Benjamin M. Good, Andrew I. Su", "title": "Exposing ambiguities in a relation-extraction gold standard with\n  crowdsourcing", "comments": "4 pages, 3 figures In: Bio-Ontologies SIG, ISMB: 10 July 2015, Dublin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Semantic relation extraction is one of the frontiers of biomedical natural\nlanguage processing research. Gold standards are key tools for advancing this\nresearch. It is challenging to generate these standards because of the high\ncost of expert time and the difficulty in establishing agreement between\nannotators. We implemented and evaluated a microtask crowdsourcing approach\nthat can produce a gold standard for extracting drug-disease relations. The\naggregated crowd judgment agreed with expert annotations from a pre-existing\ncorpus on 43 of 60 sentences tested. The levels of crowd agreement varied in a\nsimilar manner to the levels of agreement among the original expert annotators.\nThis work rein-forces the power of crowdsourcing in the process of assembling\ngold standards for relation extraction. Further, it high-lights the importance\nof exposing the levels of agreement between human annotators, expert or crowd,\nin gold standard corpora as these are reproducible signals indicating\nambiguities in the data or in the annotation guidelines.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 00:32:33 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Li", "Tong Shu", ""], ["Good", "Benjamin M.", ""], ["Su", "Andrew I.", ""]]}, {"id": "1505.06289", "submitter": "Will Monroe", "authors": "Angel Chang, Will Monroe, Manolis Savva, Christopher Potts,\n  Christopher D. Manning", "title": "Text to 3D Scene Generation with Rich Lexical Grounding", "comments": "10 pages, 7 figures, 3 tables. To appear in ACL-IJCNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to map descriptions of scenes to 3D geometric representations has\nmany applications in areas such as art, education, and robotics. However, prior\nwork on the text to 3D scene generation task has used manually specified object\ncategories and language that identifies them. We introduce a dataset of 3D\nscenes annotated with natural language descriptions and learn from this data\nhow to ground textual descriptions to physical objects. Our method successfully\ngrounds a variety of lexical terms to concrete referents, and we show\nquantitatively that our method improves 3D scene generation over previous work\nusing purely rule-based methods. We evaluate the fidelity and plausibility of\n3D scenes generated with our grounding approach through human judgments. To\nease evaluation on this task, we also introduce an automated metric that\nstrongly correlates with human judgments.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 08:32:11 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 01:13:17 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Chang", "Angel", ""], ["Monroe", "Will", ""], ["Savva", "Manolis", ""], ["Potts", "Christopher", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1505.06294", "submitter": "Dimitri Kartsaklis", "authors": "Dimitri Kartsaklis and Mehrnoosh Sadrzadeh", "title": "A Frobenius Model of Information Structure in Categorical Compositional\n  Distributional Semantics", "comments": "Accepted for presentation in the 14th Meeting on Mathematics of\n  Language (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.CT math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical compositional distributional model of Coecke, Sadrzadeh and\nClark provides a linguistically motivated procedure for computing the meaning\nof a sentence as a function of the distributional meaning of the words therein.\nThe theoretical framework allows for reasoning about compositional aspects of\nlanguage and offers structural ways of studying the underlying relationships.\nWhile the model so far has been applied on the level of syntactic structures, a\nsentence can bring extra information conveyed in utterances via intonational\nmeans. In the current paper we extend the framework in order to accommodate\nthis additional information, using Frobenius algebraic structures canonically\ninduced over the basis of finite-dimensional vector spaces. We detail the\ntheory, provide truth-theoretic and distributional semantics for meanings of\nintonationally-marked utterances, and present justifications and extensive\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 09:08:30 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kartsaklis", "Dimitri", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "1505.06427", "submitter": "Lantian Li Mr.", "authors": "Lantian Li and Dong Wang and Zhiyong Zhang and Thomas Fang Zheng", "title": "Deep Speaker Vectors for Semi Text-independent Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research shows that deep neural networks (DNNs) can be used to extract\ndeep speaker vectors (d-vectors) that preserve speaker characteristics and can\nbe used in speaker verification. This new method has been tested on\ntext-dependent speaker verification tasks, and improvement was reported when\ncombined with the conventional i-vector method.\n  This paper extends the d-vector approach to semi text-independent speaker\nverification tasks, i.e., the text of the speech is in a limited set of short\nphrases. We explore various settings of the DNN structure used for d-vector\nextraction, and present a phone-dependent training which employs the posterior\nfeatures obtained from an ASR system. The experimental results show that it is\npossible to apply d-vectors on semi text-independent speaker recognition, and\nthe phone-dependent training improves system performance.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 11:22:40 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Li", "Lantian", ""], ["Wang", "Dong", ""], ["Zhang", "Zhiyong", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1505.06750", "submitter": "Peter Sheridan Dodds", "authors": "P. S. Dodds, E. M. Clark, S. Desu, M. R. Frank, A. J. Reagan, J. R.\n  Williams, L. Mitchell, K. D. Harris, I. M. Kloumann, J. P. Bagrow, K.\n  Megerdoomian, M. T. McMahon, B. F. Tivnan, and C. M. Danforth", "title": "Reply to Garcia et al.: Common mistakes in measuring frequency dependent\n  word characteristics", "comments": "5 pages, 2 figures, 1 table. Expanded version of reply appearing in\n  PNAS 2015", "journal-ref": null, "doi": "10.1073/pnas.1505647112", "report-no": null, "categories": "physics.soc-ph cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the concerns expressed by Garcia et al. are misplaced,\ndue to (1) a misreading of our findings in [1]; (2) a widespread failure to\nexamine and present words in support of asserted summary quantities based on\nword usage frequencies; and (3) a range of misconceptions about word usage\nfrequency, word rank, and expert-constructed word lists. In particular, we show\nthat the English component of our study compares well statistically with two\nrelated surveys, that no survey design influence is apparent, and that\nestimates of measurement error do not explain the positivity biases reported in\nour work and that of others. We further demonstrate that for the frequency\ndependence of positivity---of which we explored the nuances in great detail in\n[1]---Garcia et al. did not perform a reanalysis of our data---they instead\ncarried out an analysis of a different, statistically improper data set and\nintroduced a nonlinearity before performing linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 20:33:13 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 19:36:37 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Dodds", "P. S.", ""], ["Clark", "E. M.", ""], ["Desu", "S.", ""], ["Frank", "M. R.", ""], ["Reagan", "A. J.", ""], ["Williams", "J. R.", ""], ["Mitchell", "L.", ""], ["Harris", "K. D.", ""], ["Kloumann", "I. M.", ""], ["Bagrow", "J. P.", ""], ["Megerdoomian", "K.", ""], ["McMahon", "M. T.", ""], ["Tivnan", "B. F.", ""], ["Danforth", "C. M.", ""]]}, {"id": "1505.06816", "submitter": "I. Beltagy", "authors": "I. Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, Raymond J.\n  Mooney", "title": "Representing Meaning with a Combination of Logical and Distributional\n  Models", "comments": "Special issue of Computational Linguistics on Formal Distributional\n  Semantics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP tasks differ in the semantic information they require, and at this time\nno single se- mantic representation fulfills all requirements. Logic-based\nrepresentations characterize sentence structure, but do not capture the graded\naspect of meaning. Distributional models give graded similarity ratings for\nwords and phrases, but do not capture sentence structure in the same detail as\nlogic-based approaches. So it has been argued that the two are complementary.\nWe adopt a hybrid approach that combines logic-based and distributional\nsemantics through probabilistic logic inference in Markov Logic Networks\n(MLNs). In this paper, we focus on the three components of a practical system\nintegrating logical and distributional models: 1) Parsing and task\nrepresentation is the logic-based part where input problems are represented in\nprobabilistic logic. This is quite different from representing them in standard\nfirst-order logic. 2) For knowledge base construction we form weighted\ninference rules. We integrate and compare distributional information with other\nsources, notably WordNet and an existing paraphrase collection. In particular,\nwe use our system to evaluate distributional lexical entailment approaches. We\nuse a variant of Robinson resolution to determine the necessary inference\nrules. More sources can easily be added by mapping them to logical rules; our\nsystem learns a resource-specific weight that corrects for scaling differences\nbetween resources. 3) In discussing probabilistic inference, we show how to\nsolve the inference problems efficiently. To evaluate our approach, we use the\ntask of textual entailment (RTE), which can utilize the strengths of both\nlogic-based and distributional representations. In particular we focus on the\nSICK dataset, where we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 06:19:18 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 03:51:26 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 03:46:07 GMT"}, {"version": "v4", "created": "Tue, 7 Jun 2016 13:30:01 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2016 15:07:47 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Beltagy", "I.", ""], ["Roller", "Stephen", ""], ["Cheng", "Pengxiang", ""], ["Erk", "Katrin", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1505.07184", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala and Takanori Maehara and Ken-ichi Kawarabayashi", "title": "Unsupervised Cross-Domain Word Representation Learning", "comments": "53rd Annual Meeting of the Association for Computational Linguistics\n  and the 7th International Joint Conferences on Natural Language Processing of\n  the Asian Federation of Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meaning of a word varies from one domain to another. Despite this important\ndomain dependence in word semantics, existing word representation learning\nmethods are bound to a single domain. Given a pair of\n\\emph{source}-\\emph{target} domains, we propose an unsupervised method for\nlearning domain-specific word representations that accurately capture the\ndomain-specific aspects of word semantics. First, we select a subset of\nfrequent words that occur in both domains as \\emph{pivots}. Next, we optimize\nan objective function that enforces two constraints: (a) for both source and\ntarget domain documents, pivots that appear in a document must accurately\npredict the co-occurring non-pivots, and (b) word representations learnt for\npivots must be similar in the two domains. Moreover, we propose a method to\nperform domain adaptation using the learnt word representations. Our proposed\nmethod significantly outperforms competitive baselines including the\nstate-of-the-art domain-insensitive word representations, and reports best\nsentiment classification accuracies for all domain-pairs in a benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 04:02:56 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Bollegala", "Danushka", ""], ["Maehara", "Takanori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1505.07302", "submitter": "Derek Greene", "authors": "Derek Greene and James P. Cross", "title": "Unveiling the Political Agenda of the European Parliament Plenary: A\n  Topical Analysis", "comments": "Add link to implementation code on Github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study analyzes political interactions in the European Parliament (EP) by\nconsidering how the political agenda of the plenary sessions has evolved over\ntime and the manner in which Members of the European Parliament (MEPs) have\nreacted to external and internal stimuli when making Parliamentary speeches. It\ndoes so by considering the context in which speeches are made, and the content\nof those speeches. To detect latent themes in legislative speeches over time,\nspeech content is analyzed using a new dynamic topic modeling method, based on\ntwo layers of matrix factorization. This method is applied to a new corpus of\nall English language legislative speeches in the EP plenary from the period\n1999-2014. Our findings suggest that the political agenda of the EP has evolved\nsignificantly over time, is impacted upon by the committee structure of the\nParliament, and reacts to exogenous events such as EU Treaty referenda and the\nemergence of the Euro-crisis have a significant impact on what is being\ndiscussed in Parliament.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 13:17:35 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 10:25:16 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 18:12:15 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 10:13:04 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Greene", "Derek", ""], ["Cross", "James P.", ""]]}, {"id": "1505.07599", "submitter": "Xipeng Qiu", "authors": "Xipeng Qiu, Peng Qian, Liusong Yin, Shiyu Wu, Xuanjing Huang", "title": "Overview of the NLPCC 2015 Shared Task: Chinese Word Segmentation and\n  POS Tagging for Micro-blog Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give an overview for the shared task at the 4th CCF\nConference on Natural Language Processing \\& Chinese Computing (NLPCC 2015):\nChinese word segmentation and part-of-speech (POS) tagging for micro-blog\ntexts. Different with the popular used newswire datasets, the dataset of this\nshared task consists of the relatively informal micro-texts. The shared task\nhas two sub-tasks: (1) individual Chinese word segmentation and (2) joint\nChinese word segmentation and POS Tagging. Each subtask has three tracks to\ndistinguish the systems with different resources. We first introduce the\ndataset and task, then we characterize the different approaches of the\nparticipating systems, report the test results, and provide a overview analysis\nof these results. An online system is available for open registration and\nevaluation at http://nlp.fudan.edu.cn/nlpcc2015.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 08:54:13 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 02:45:24 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2015 18:44:59 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Qiu", "Xipeng", ""], ["Qian", "Peng", ""], ["Yin", "Liusong", ""], ["Wu", "Shiyu", ""], ["Huang", "Xuanjing", ""]]}, {"id": "1505.07712", "submitter": "Eric  Werner", "authors": "Eric Werner", "title": "A Category Theory of Communication Theory", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CL cs.LO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theory of how agents can come to understand a language is presented. If\nunderstanding a sentence $\\alpha$ is to associate an operator with $\\alpha$\nthat transforms the representational state of the agent as intended by the\nsender, then coming to know a language involves coming to know the operators\nthat correspond to the meaning of any sentence. This involves a higher order\noperator that operates on the possible transformations that operate on the\nrepresentational capacity of the agent. We formalize these constructs using\nconcepts and diagrams analogous to category theory.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 15:00:27 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Werner", "Eric", ""]]}, {"id": "1505.07909", "submitter": "Bin Gao", "authors": "Huazheng Wang, Fei Tian, Bin Gao, Jiang Bian, Tie-Yan Liu", "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered\n  Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligence Quotient (IQ) Test is a set of standardized questions designed\nto evaluate human intelligence. Verbal comprehension questions appear very\nfrequently in IQ tests, which measure human's verbal ability including the\nunderstanding of the words with multiple senses, the synonyms and antonyms, and\nthe analogies among words. In this work, we explore whether such tests can be\nsolved automatically by artificial intelligence technologies, especially the\ndeep learning technologies that are recently developed and successfully applied\nin a number of fields. However, we found that the task was quite challenging,\nand simply applying existing technologies (e.g., word embedding) could not\nachieve a good performance, mainly due to the multiple senses of words and the\ncomplex relations among words. To tackle these challenges, we propose a novel\nframework consisting of three components. First, we build a classifier to\nrecognize the specific type of a verbal question (e.g., analogy,\nclassification, synonym, or antonym). Second, we obtain distributed\nrepresentations of words and relations by leveraging a novel word embedding\nmethod that considers the multi-sense nature of words and the relational\nknowledge among words (or their senses) contained in dictionaries. Third, for\neach type of questions, we propose a specific solver based on the obtained\ndistributed word representations and relation representations. Experimental\nresults have shown that the proposed framework can not only outperform existing\nmethods for solving verbal comprehension questions but also exceed the average\nperformance of the Amazon Mechanical Turk workers involved in the study. The\nresults indicate that with appropriate uses of the deep learning technologies\nwe might be a further step closer to the human intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 02:46:44 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 13:29:41 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 08:42:22 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 11:37:32 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Wang", "Huazheng", ""], ["Tian", "Fei", ""], ["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1505.07931", "submitter": "Xuefeng Yang", "authors": "Xuefeng Yang, Kezhi Mao", "title": "Supervised Fine Tuning for Word Embedding with Integrated Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning vector representation for words is an important research field which\nmay benefit many natural language processing tasks. Two limitations exist in\nnearly all available models, which are the bias caused by the context\ndefinition and the lack of knowledge utilization. They are difficult to tackle\nbecause these algorithms are essentially unsupervised learning approaches.\nInspired by deep learning, the authors propose a supervised framework for\nlearning vector representation of words to provide additional supervised fine\ntuning after unsupervised learning. The framework is knowledge rich approacher\nand compatible with any numerical vectors word representation. The authors\nperform both intrinsic evaluation like attributional and relational similarity\nprediction and extrinsic evaluations like the sentence completion and sentiment\nanalysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show\nthat the proposed fine tuning framework may significantly improve the quality\nof the vector representation of words.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 06:11:00 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Yang", "Xuefeng", ""], ["Mao", "Kezhi", ""]]}, {"id": "1505.08075", "submitter": "Chris Dyer", "authors": "Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A.\n  Smith", "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "comments": "Proceedings of ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:58:12 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Dyer", "Chris", ""], ["Ballesteros", "Miguel", ""], ["Ling", "Wang", ""], ["Matthews", "Austin", ""], ["Smith", "Noah A.", ""]]}, {"id": "1505.08149", "submitter": "Pavlo Kapustin", "authors": "Michael Kapustin, Pavlo Kapustin", "title": "Modeling meaning: computational interpreting and understanding of\n  natural language fragments", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this introductory article we present the basics of an approach to\nimplementing computational interpreting of natural language aiming to model the\nmeanings of words and phrases. Unlike other approaches, we attempt to define\nthe meanings of text fragments in a composable and computer interpretable way.\nWe discuss models and ideas for detecting different types of semantic\nincomprehension and choosing the interpretation that makes most sense in a\ngiven context. Knowledge representation is designed for handling\ncontext-sensitive and uncertain / imprecise knowledge, and for easy\naccommodation of new information. It stores quantitative information capturing\nthe essence of the concepts, because it is crucial for working with natural\nlanguage understanding and reasoning. Still, the representation is general\nenough to allow for new knowledge to be learned, and even generated by the\nsystem. The article concludes by discussing some reasoning-related topics:\npossible approaches to generation of new abstract concepts, and describing\nsituations and concepts in words (e.g. for specifying interpretation\ndifficulties).\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 19:06:42 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 10:35:11 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 17:04:01 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kapustin", "Michael", ""], ["Kapustin", "Pavlo", ""]]}]