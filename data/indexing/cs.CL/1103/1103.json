[{"id": "1103.0398", "submitter": "Ronan Collobert", "authors": "Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray\n  Kavukcuoglu, Pavel Kuksa", "title": "Natural Language Processing (almost) from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 11:34:50 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Collobert", "Ronan", ""], ["Weston", "Jason", ""], ["Bottou", "Leon", ""], ["Karlen", "Michael", ""], ["Kavukcuoglu", "Koray", ""], ["Kuksa", "Pavel", ""]]}, {"id": "1103.0784", "submitter": "Johan Bollen", "authors": "Johan Bollen, Bruno Goncalves, Guangchen Ruan, and Huina Mao", "title": "Happiness is assortative in online social networks", "comments": "17 pages, 9 figures", "journal-ref": "Artificial Life 17(3), 237-251 (2011)", "doi": "10.1162/artl_a_00034", "report-no": null, "categories": "cs.SI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks tend to disproportionally favor connections between\nindividuals with either similar or dissimilar characteristics. This propensity,\nreferred to as assortative mixing or homophily, is expressed as the correlation\nbetween attribute values of nearest neighbour vertices in a graph. Recent\nresults indicate that beyond demographic features such as age, sex and race,\neven psychological states such as \"loneliness\" can be assortative in a social\nnetwork. In spite of the increasing societal importance of online social\nnetworks it is unknown whether assortative mixing of psychological states takes\nplace in situations where social ties are mediated solely by online networking\nservices in the absence of physical contact. Here, we show that general\nhappiness or Subjective Well-Being (SWB) of Twitter users, as measured from a 6\nmonth record of their individual tweets, is indeed assortative across the\nTwitter social network. To our knowledge this is the first result that shows\nassortative mixing in online networks at the level of SWB. Our results imply\nthat online social networks may be equally subject to the social mechanisms\nthat cause assortative mixing in real social networks and that such assortative\nmixing takes place at the level of SWB. Given the increasing prevalence of\nonline social networks, their propensity to connect users with similar levels\nof SWB may be an important instrument in better understanding how both positive\nand negative sentiments spread through online social ties. Future research may\nfocus on how event-specific mood states can propagate and influence user\nbehavior in \"real life\".\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 21:05:17 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Bollen", "Johan", ""], ["Goncalves", "Bruno", ""], ["Ruan", "Guangchen", ""], ["Mao", "Huina", ""]]}, {"id": "1103.0890", "submitter": "Qi Mao", "authors": "Qi Mao, Ivor W. Tsang", "title": "Efficient Multi-Template Learning for Structured Prediction", "comments": null, "journal-ref": "Efficient Multi-Template Learning for Structured Prediction. IEEE\n  Transactions on Neural Networks and Learning Systems, 24(2): 248 - 261, Feb\n  2013", "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional random field (CRF) and Structural Support Vector Machine\n(Structural SVM) are two state-of-the-art methods for structured prediction\nwhich captures the interdependencies among output variables. The success of\nthese methods is attributed to the fact that their discriminative models are\nable to account for overlapping features on the whole input observations. These\nfeatures are usually generated by applying a given set of templates on labeled\ndata, but improper templates may lead to degraded performance. To alleviate\nthis issue, in this paper, we propose a novel multiple template learning\nparadigm to learn structured prediction and the importance of each template\nsimultaneously, so that hundreds of arbitrary templates could be added into the\nlearning model without caution. This paradigm can be formulated as a special\nmultiple kernel learning problem with exponential number of constraints. Then\nwe introduce an efficient cutting plane algorithm to solve this problem in the\nprimal, and its convergence is presented. We also evaluate the proposed\nlearning paradigm on two widely-studied structured prediction tasks,\n\\emph{i.e.} sequence labeling and dependency parsing. Extensive experimental\nresults show that the proposed method outperforms CRFs and Structural SVMs due\nto exploiting the importance of each template. Our complexity analysis and\nempirical results also show that our proposed method is more efficient than\nOnlineMKL on very sparse and high-dimensional data. We further extend this\nparadigm for structured prediction using generalized $p$-block norm\nregularization with $p>1$, and experiments show competitive performances when\n$p \\in [1,2)$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 13:08:59 GMT"}, {"version": "v2", "created": "Sat, 4 May 2013 13:57:32 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mao", "Qi", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "1103.1898", "submitter": "Heather Pon-Barry", "authors": "Heather Pon-Barry and Stuart M. Shieber", "title": "Recognizing Uncertainty in Speech", "comments": "11 pages", "journal-ref": "EURASIP Journal on Advances in Signal Processing, Volume 2011,\n  Article ID 251753, 11 pages", "doi": "10.1155/2011/251753", "report-no": null, "categories": "cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We address the problem of inferring a speaker's level of certainty based on\nprosodic information in the speech signal, which has application in\nspeech-based dialogue systems. We show that using phrase-level prosodic\nfeatures centered around the phrases causing uncertainty, in addition to\nutterance-level prosodic features, improves our model's level of certainty\nclassification. In addition, our models can be used to predict which phrase a\nperson is uncertain about. These results rely on a novel method for eliciting\nutterances of varying levels of certainty that allows us to compare the utility\nof contextually-based feature sets. We elicit level of certainty ratings from\nboth the speakers themselves and a panel of listeners, finding that there is\noften a mismatch between speakers' internal states and their perceived states,\nand highlighting the importance of this distinction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 21:43:46 GMT"}], "update_date": "2011-03-11", "authors_parsed": [["Pon-Barry", "Heather", ""], ["Shieber", "Stuart M.", ""]]}, {"id": "1103.2325", "submitter": "Tsvi Tlusty", "authors": "David Levary, Jean-Pierre Eckmann, Elisha Moses and Tsvi Tlusty", "title": "Self reference in word definitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionaries are inherently circular in nature. A given word is linked to a\nset of alternative words (the definition) which in turn point to further\ndescendants. Iterating through definitions in this way, one typically finds\nthat definitions loop back upon themselves. The graph formed by such\ndefinitional relations is our object of study. By eliminating those links which\nare not in loops, we arrive at a core subgraph of highly connected nodes.\n  We observe that definitional loops are conveniently classified by length,\nwith longer loops usually emerging from semantic misinterpretation. By breaking\nthe long loops in the graph of the dictionary, we arrive at a set of\ndisconnected clusters. We find that the words in these clusters constitute\nsemantic units, and moreover tend to have been introduced into the English\nlanguage at similar times, suggesting a possible mechanism for language\nevolution.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 17:57:16 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Levary", "David", ""], ["Eckmann", "Jean-Pierre", ""], ["Moses", "Elisha", ""], ["Tlusty", "Tsvi", ""]]}, {"id": "1103.2681", "submitter": "Sebastian Bernhardsson", "authors": "Sebastian Bernhardsson, Seung Ki Baek and Petter Minnhagen", "title": "A Paradoxical Property of the Monkey Book", "comments": "5 pages, 4 figures", "journal-ref": "J. Stat. Mech. (2011) P07013", "doi": "10.1088/1742-5468/2011/07/P07013", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A \"monkey book\" is a book consisting of a random distribution of letters and\nblanks, where a group of letters surrounded by two blanks is defined as a word.\nWe compare the statistics of the word distribution for a monkey book with the\ncorresponding distribution for the general class of random books, where the\nlatter are books for which the words are randomly distributed. It is shown that\nthe word distribution statistics for the monkey book is different and quite\ndistinct from a typical sampled book or real book. In particular the monkey\nbook obeys Heaps' power law to an extraordinary good approximation, in contrast\nto the word distributions for sampled and real books, which deviate from Heaps'\nlaw in a characteristics way. The somewhat counter-intuitive conclusion is that\na \"monkey book\" obeys Heaps' power law precisely because its word-frequency\ndistribution is not a smooth power law, contrary to the expectation based on\nsimple mathematical arguments that if one is a power law, so is the other.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 14:54:10 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Bernhardsson", "Sebastian", ""], ["Baek", "Seung Ki", ""], ["Minnhagen", "Petter", ""]]}, {"id": "1103.2903", "submitter": "Finn {\\AA}rup Nielsen", "authors": "Finn {\\AA}rup Nielsen", "title": "A new ANEW: Evaluation of a word list for sentiment analysis in\n  microblogs", "comments": "6 pages, 4 figures, 1 table, Submitted to \"Making Sense of Microposts\n  (#MSM2011)\"", "journal-ref": "Proceedings of the ESWC2011 Workshop on 'Making Sense of\n  Microposts': Big things come in small packages (2011) 93-98", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sentiment analysis of microblogs such as Twitter has recently gained a fair\namount of attention. One of the simplest sentiment analysis approaches compares\nthe words of a posting against a labeled word list, where each word has been\nscored for valence, -- a 'sentiment lexicon' or 'affective word lists'. There\nexist several affective word lists, e.g., ANEW (Affective Norms for English\nWords) developed before the advent of microblogging and sentiment analysis. I\nwanted to examine how well ANEW and other word lists performs for the detection\nof sentiment strength in microblog posts in comparison with a new word list\nspecifically constructed for microblogs. I used manually labeled postings from\nTwitter scored for sentiment. Using a simple word matching I show that the new\nword list may perform better than ANEW, though not as good as the more\nelaborate approach found in SentiStrength.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 13:39:20 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Nielsen", "Finn \u00c5rup", ""]]}, {"id": "1103.2950", "submitter": "Wentian Li", "authors": "Wentian Li and Pedro Miramontes", "title": "Fitting Ranked English and Spanish Letter Frequency Distribution in U.S.\n  and Mexican Presidential Speeches", "comments": "7 figures", "journal-ref": "Journal of Quantitative Linguistics, 18(4):359-380 (2011)", "doi": "10.1080/09296174.2011.608606", "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limited range in its abscissa of ranked letter frequency distributions\ncauses multiple functions to fit the observed distribution reasonably well. In\norder to critically compare various functions, we apply the statistical model\nselections on ten functions, using the texts of U.S. and Mexican presidential\nspeeches in the last 1-2 centuries. Dispite minor switching of ranking order of\ncertain letters during the temporal evolution for both datasets, the letter\nusage is generally stable. The best fitting function, judged by either\nleast-square-error or by AIC/BIC model selection, is the Cocho/Beta function.\nWe also use a novel method to discover clusters of letters by their\nobserved-over-expected frequency ratios.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 16:21:24 GMT"}], "update_date": "2012-05-07", "authors_parsed": [["Li", "Wentian", ""], ["Miramontes", "Pedro", ""]]}, {"id": "1103.3585", "submitter": "Fredrik Sandin", "authors": "Fredrik Sandin, Blerim Emruli, Magnus Sahlgren", "title": "Incremental dimension reduction of tensors with random index", "comments": "36 pages, 9 figures", "journal-ref": "Revised version published in Knowl. Inf. Syst. 2016 (Open Access)", "doi": "10.1007/s10115-016-1012-2", "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an incremental, scalable and efficient dimension reduction\ntechnique for tensors that is based on sparse random linear coding. Data is\nstored in a compactified representation with fixed size, which makes memory\nrequirements low and predictable. Component encoding and decoding are performed\non-line without computationally expensive re-analysis of the data set. The\nrange of tensor indices can be extended dynamically without modifying the\ncomponent representation. This idea originates from a mathematical model of\nsemantic memory and a method known as random indexing in natural language\nprocessing. We generalize the random-indexing algorithm to tensors and present\nsignal-to-noise-ratio simulations for representations of vectors and matrices.\nWe present also a mathematical analysis of the approximate orthogonality of\nhigh-dimensional ternary vectors, which is a property that underpins this and\nother similar random-coding approaches to dimension reduction. To further\ndemonstrate the properties of random indexing we present results of a synonym\nidentification task. The method presented here has some similarities with\nrandom projection and Tucker decomposition, but it performs well at high\ndimensionality only (n>10^3). Random indexing is useful for a range of complex\npractical problems, e.g., in natural language processing, data mining, pattern\nrecognition, event detection, graph searching and search engines. Prototype\nsoftware is provided. It supports encoding and decoding of tensors of order >=\n1 in a unified framework, i.e., vectors, matrices and higher order tensors.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2011 10:07:13 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sandin", "Fredrik", ""], ["Emruli", "Blerim", ""], ["Sahlgren", "Magnus", ""]]}, {"id": "1103.3952", "submitter": "{\\L}ukasz D{\\ke}bowski", "authors": "{\\L}ukasz D\\k{e}bowski", "title": "Mixing, Ergodic, and Nonergodic Processes with Rapidly Growing\n  Information between Blocks", "comments": "21 pages", "journal-ref": "IEEE Transactions on Information Theory 58:3392-3401, 2012", "doi": "10.1109/TIT.2012.2190708", "report-no": null, "categories": "cs.IT cs.CL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct mixing processes over an infinite alphabet and ergodic processes\nover a finite alphabet for which Shannon mutual information between adjacent\nblocks of length $n$ grows as $n^\\beta$, where $\\beta\\in(0,1)$. The processes\nare a modification of nonergodic Santa Fe processes, which were introduced in\nthe context of natural language modeling. The rates of mutual information for\nthe latter processes are alike and also established in this paper. As an\nauxiliary result, it is shown that infinite direct products of mixing processes\nare also mixing.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 10:04:20 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 15:52:30 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["D\u0119bowski", "\u0141ukasz", ""]]}, {"id": "1103.4012", "submitter": "Francesca Tria", "authors": "Simone Pompei, Vittorio Loreto and Francesca Tria", "title": "On the accuracy of language trees", "comments": "36 pages, 14 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0020109", "report-no": null, "categories": "physics.soc-ph cs.CL q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical linguistics aims at inferring the most likely language\nphylogenetic tree starting from information concerning the evolutionary\nrelatedness of languages. The available information are typically lists of\nhomologous (lexical, phonological, syntactic) features or characters for many\ndifferent languages.\n  From this perspective the reconstruction of language trees is an example of\ninverse problems: starting from present, incomplete and often noisy,\ninformation, one aims at inferring the most likely past evolutionary history. A\nfundamental issue in inverse problems is the evaluation of the inference made.\nA standard way of dealing with this question is to generate data with\nartificial models in order to have full access to the evolutionary process one\nis going to infer. This procedure presents an intrinsic limitation: when\ndealing with real data sets, one typically does not know which model of\nevolution is the most suitable for them. A possible way out is to compare\nalgorithmic inference with expert classifications. This is the point of view we\ntake here by conducting a thorough survey of the accuracy of reconstruction\nmethods as compared with the Ethnologue expert classifications. We focus in\nparticular on state-of-the-art distance-based methods for phylogeny\nreconstruction using worldwide linguistic databases.\n  In order to assess the accuracy of the inferred trees we introduce and\ncharacterize two generalizations of standard definitions of distances between\ntrees. Based on these scores we quantify the relative performances of the\ndistance-based algorithms considered. Further we quantify how the completeness\nand the coverage of the available databases affect the accuracy of the\nreconstruction. Finally we draw some conclusions about where the accuracy of\nthe reconstructions in historical linguistics stands and about the leading\ndirections to improve it.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 13:59:43 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Pompei", "Simone", ""], ["Loreto", "Vittorio", ""], ["Tria", "Francesca", ""]]}, {"id": "1103.4090", "submitter": "Luis Rocha", "authors": "An\\'alia Louren\\c{c}o, Michael Conover, Andrew Wong, Azadeh\n  Nematzadeh, Fengxia Pan, Hagit Shatkay, Luis M. Rocha", "title": "A Linear Classifier Based on Entity Recognition Tools and a Statistical\n  Approach to Method Extraction in the Protein-Protein Interaction Literature", "comments": "BMC Bioinformatics. In Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participated, in the Article Classification and the Interaction Method\nsubtasks (ACT and IMT, respectively) of the Protein-Protein Interaction task of\nthe BioCreative III Challenge. For the ACT, we pursued an extensive testing of\navailable Named Entity Recognition and dictionary tools, and used the most\npromising ones to extend our Variable Trigonometric Threshold linear\nclassifier. For the IMT, we experimented with a primarily statistical approach,\nas opposed to employing a deeper natural language processing strategy. Finally,\nwe also studied the benefits of integrating the method extraction approach that\nwe have used for the IMT into the ACT pipeline. For the ACT, our linear article\nclassifier leads to a ranking and classification performance significantly\nhigher than all the reported submissions. For the IMT, our results are\ncomparable to those of other systems, which took very different approaches. For\nthe ACT, we show that the use of named entity recognition tools leads to a\nsubstantial improvement in the ranking and classification of articles relevant\nto protein-protein interaction. Thus, we show that our substantially expanded\nlinear classifier is a very competitive classifier in this domain. Moreover,\nthis classifier produces interpretable surfaces that can be understood as\n\"rules\" for human understanding of the classification. In terms of the IMT\ntask, in contrast to other participants, our approach focused on identifying\nsentences that are likely to bear evidence for the application of a PPI\ndetection method, rather than on classifying a document as relevant to a\nmethod. As BioCreative III did not perform an evaluation of the evidence\nprovided by the system, we have conducted a separate assessment; the evaluators\nagree that our tool is indeed effective in detecting relevant evidence for PPI\ndetection methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 17:33:32 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2011 17:46:37 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Louren\u00e7o", "An\u00e1lia", ""], ["Conover", "Michael", ""], ["Wong", "Andrew", ""], ["Nematzadeh", "Azadeh", ""], ["Pan", "Fengxia", ""], ["Shatkay", "Hagit", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1103.5676", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn", "title": "Codeco: A Grammar Notation for Controlled Natural Language in Predictive\n  Editors", "comments": null, "journal-ref": "In Pre-Proceedings of the Second Workshop on Controlled Natural\n  Languages (CNL 2010), CEUR Workshop Proceedings, Volume 622, 2010", "doi": null, "report-no": null, "categories": "cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing grammar frameworks do not work out particularly well for controlled\nnatural languages (CNL), especially if they are to be used in predictive\neditors. I introduce in this paper a new grammar notation, called Codeco, which\nis designed specifically for CNLs and predictive editors. Two different parsers\nhave been implemented and a large subset of Attempto Controlled English (ACE)\nhas been represented in Codeco. The results show that Codeco is practical,\nadequate and efficient.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 15:05:11 GMT"}], "update_date": "2011-03-30", "authors_parsed": [["Kuhn", "Tobias", ""]]}]